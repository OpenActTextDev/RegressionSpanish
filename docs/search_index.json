[["index.html", "Modelado de Regresión con Aplicaciones Actuariales y Financieras Prefacio Prólogo Dedicación", " Modelado de Regresión con Aplicaciones Actuariales y Financieras Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University Prefacio Prólogo Los actuarios y otros analistas financieros cuantifican situaciones usando datos; somos personas de ‘números’. Muchos de nuestros enfoques y modelos son estilizados, basados en años de experiencia e investigaciones realizadas por legiones de analistas. Sin embargo, el mundo financiero y de gestión de riesgos evoluciona rápidamente. Muchos analistas se enfrentan a nuevas situaciones en las que los métodos probados simplemente no funcionan. Aquí es donde entra un conjunto de herramientas como el análisis de regresión. La regresión es el estudio de las relaciones entre variables. Es una disciplina estadística genérica que no se limita al mundo financiero; tiene aplicaciones en campos de ciencias sociales, biológicas y físicas. Puedes usar técnicas de regresión para investigar conjuntos de datos grandes y complejos. Para familiarizarte con la regresión, este libro explora muchos ejemplos y conjuntos de datos basados en aplicaciones actuariales y financieras. Esto no quiere decir que no encontrarás aplicaciones fuera del mundo financiero (por ejemplo, un actuario puede necesitar entender la evidencia científica más reciente sobre pruebas genéticas para fines de suscripción). Sin embargo, al familiarizarte con este conjunto de herramientas, verás cómo la regresión puede aplicarse en muchas (y a veces nuevas) situaciones. ¿Para Quién Es Este Libro? Este libro está escrito para analistas financieros que enfrentan eventos inciertos y desean cuantificar estos eventos utilizando información empírica. No se asume conocimiento previo del sector, aunque los lectores encontrarán la lectura mucho más fácil si tienen interés en las aplicaciones discutidas aquí. Este libro está diseñado para estudiantes que están siendo introducidos al campo, así como para analistas de la industria que deseen repasar técnicas antiguas y (para los capítulos posteriores) obtener una introducción a nuevos desarrollos. Para leer este libro, asumo un conocimiento comparable a una introducción de un semestre a la probabilidad y estadística; el Apéndice A1 proporciona una breve revisión para refrescar conceptos si estás oxidado. Los estudiantes actuariales en Norteamérica tendrán una introducción de un año a la probabilidad y estadística; este tipo de introducción ayudará a los lectores a captar conceptos más rápidamente que una base de un semestre. Finalmente, los lectores encontrarán útil el álgebra matricial, o lineal, aunque no es un requisito previo para leer este texto. Los diferentes lectores están interesados en entender la estadística a diferentes niveles. Este libro está escrito para acomodar al ‘lector de sillón’, es decir, aquel que lee pasivamente y no se involucra intentando realizar los ejercicios del texto. Considera una analogía con el fútbol, o cualquier otro juego. Al igual que el mariscal de campo de sillón en el fútbol, hay mucho que puedes aprender sobre el juego solo con observar. Sin embargo, si quieres agudizar tus habilidades, tienes que salir y jugar el juego. Si realizas los ejercicios o reproduces los análisis estadísticos en el texto, te convertirás en un mejor jugador. Aún así, este texto está escrito entrelazando ejemplos con los principios básicos. Así, incluso el lector de sillón puede obtener una sólida comprensión de las técnicas de regresión a través de este texto. ¿De Qué Trata Este Libro? La Tabla de Contenidos proporciona una visión general de los temas tratados, organizados en cuatro partes. La primera parte introduce la regresión lineal. Este es el material central del libro, con refrescadores sobre estadísticas matemáticas, distribuciones y álgebra matricial entrelazados según sea necesario. La segunda parte se dedica a temas en series temporales. ¿Por qué integrar temas de series temporales en un libro de regresión? Las razones son simples, pero convincentes; la mayoría de los datos contables, financieros y económicos se vuelven disponibles a lo largo del tiempo. Aunque las inferencias transversales son útiles, las decisiones empresariales deben tomarse en tiempo real con los datos actualmente disponibles. Los Capítulos 7-10 introducen técnicas de series temporales que se pueden realizar fácilmente usando herramientas de regresión (y hay muchas). La regresión no lineal es el tema de la tercera parte. Muchas de las herramientas modernas de ‘modelado predictivo’ están basadas en regresión no lineal; estas son las herramientas de trabajo en las oficinas estadísticas de la industria financiera y de gestión de riesgos. La cuarta parte se refiere a ‘aplicaciones actuariales,’ temas que he encontrado relevantes en mi investigación y trabajo de consultoría en gestión de riesgos financieros. Los primeros cuatro capítulos de esta parte consisten en variaciones de modelos de regresión que son particularmente útiles en la gestión de riesgos. Los últimos dos capítulos se centran en las comunicaciones, específicamente, en la redacción de informes y el diseño de gráficos. Comunicar información es un aspecto importante de toda disciplina técnica y la estadística ciertamente no es una excepción. Aquí está la traducción al español del texto proporcionado, manteniendo la estructura en Markdown y utilizando un lenguaje simple cuando sea posible: ¿Cómo Transmite Este Libro Su Mensaje? Desarrollo de Capítulos. Cada capítulo tiene varios ejemplos entretejidos con teoría. En los capítulos donde se introduce un modelo, comienzo con un ejemplo y discuto el análisis de datos sin hacer referencia a la teoría. Este análisis se presenta a un nivel intuitivo, sin referencia a un modelo específico. Esto es sencillo, porque se trata de poco más que ajustar curvas. El objetivo es que los estudiantes resuman los datos de manera sensata sin que la noción de un modelo obscurezca un buen análisis de datos. Luego, se proporciona una introducción a la teoría en el contexto del ejemplo introductorio. Se siguen uno o más ejemplos adicionales que refuerzan la teoría ya introducida y proporcionan un contexto para explicar la teoría adicional. En los Capítulos 5 y 6, que no introducen modelos sino técnicas para el análisis, comienzo con una introducción de la técnica. Esta introducción es seguida por un ejemplo que refuerza la explicación. De esta manera, el análisis de datos se puede omitir fácilmente sin pérdida de continuidad, si el tiempo es una preocupación. Datos Reales. Muchos de los ejercicios piden al lector que trabaje con datos reales. La necesidad de trabajar con datos reales está bien documentada; por ejemplo, véase Hogg (1972) o Singer y Willett (1990). Algunos criterios de Singer y Willett para juzgar un buen conjunto de datos incluyen: (1) autenticidad, (2) disponibilidad de información de fondo, (3) interés y relevancia para el aprendizaje sustantivo, y (4) disponibilidad de elementos con los que los lectores puedan identificarse. Por supuesto, hay algunas desventajas importantes al trabajar con datos reales. Los conjuntos de datos pueden volverse obsoletos rápidamente. Además, el conjunto de datos ideal para ilustrar un problema estadístico específico es difícil de encontrar. Esto se debe a que, con datos reales, casi por definición, varios problemas ocurren simultáneamente. Esto hace que sea difícil aislar un aspecto específico. Particularmente disfruto trabajar con grandes conjuntos de datos. Cuanto mayor es el conjunto de datos, mayor es la necesidad de estadísticas para resumir el contenido informativo. Software Estadístico y Datos. Mi objetivo al escribir este texto es llegar a un amplio grupo de estudiantes y analistas de la industria. Por lo tanto, para evitar excluir grandes segmentos, elegí no integrar ningún paquete de software estadístico específico en el texto. Sin embargo, debido a la orientación hacia las aplicaciones, es crucial que la metodología presentada pueda lograrse fácilmente utilizando paquetes disponibles. Para el curso que enseño en la Universidad de Wisconsin, uso los paquetes estadísticos SAS y R. En el sitio web del libro, http://research.bus.wisc.edu/RegActuaries los usuarios encontrarán scripts escritos en SAS y R para el análisis presentado en el texto. Los datos están disponibles en formato de texto, permitiendo a los lectores usar cualquier paquete estadístico que deseen. Cuando veas una nota como esta en el margen, también podrás encontrar este conjunto de datos () en el sitio web del libro. Suplementos Técnicos. Los suplementos técnicos refuerzan y amplían los resultados en el cuerpo principal del texto al proporcionar un tratamiento más formal y matemático del material. Este tratamiento es en realidad un suplemento porque las aplicaciones y ejemplos están descritos en el cuerpo principal del texto. Para los lectores con suficiente fondo matemático, los suplementos proporcionan material adicional que es útil para comunicarse con audiencias técnicas. Los suplementos técnicos ofrecen una cobertura más profunda y amplia de la regresión aplicada. Creo que los analistas deberían tener una idea de ‘lo que ocurre bajo el capó,’ o ‘cómo funciona el motor.’ La mayoría de estos temas se omitirán en la primera lectura del material. Sin embargo, a medida que trabajes con regresión, te enfrentarás a preguntas sobre ‘¿Por qué?’ y necesitarás profundizar en los detalles para ver exactamente cómo funciona una técnica en particular. Además, los suplementos técnicos proporcionan un menú de elementos opcionales que un instructor puede desear cubrir. Cursos Sugeridos. Hay una amplia variedad de temas que pueden incluirse en un curso de regresión. Aquí están algunos cursos sugeridos. El curso que enseño en la Universidad de Wisconsin es el primero en la lista de la siguiente tabla. \\[ {\\small \\begin{array}{ll} \\begin{array}{lll} \\hline \\textbf{Audiencia} &amp; \\textbf{Naturaleza del Curso}&amp; \\textbf{Capítulos Sugeridos} \\\\ \\hline \\text{Fondo de un año en} &amp; \\text{Introducción a la regresión y} &amp; \\text{Capítulos } 1-8 ,11-13, 20-21,\\\\ ~~~\\text{probabilidad y estadística} &amp; ~~~\\text{modelos de series temporales} &amp; ~~~\\text{solo cuerpo principal del texto} \\\\ \\text{Fondo de un año en} &amp; \\text{Regresión y modelos de} &amp; \\text{ Capítulos } 1-8, 20-21, \\text{seleccionados} \\\\ ~~~\\text{probabilidad y estadística} &amp; ~~~\\text{series temporales} &amp; ~~~\\text{porciones de los suplementos técnicos} \\\\ \\text{Fondo de un año en} &amp; \\text{Modelado de regresión} &amp; \\text{Capítulos } 1-6, 11-13, 20-21, \\text{seleccionados} \\\\ ~~~\\text{probabilidad y estadística} &amp; &amp; ~~~ \\text{porciones de los suplementos técnicos} \\\\ \\text{Fondo en estadísticas} &amp; \\text{Regresión actuarial} &amp; \\text{ Capítulos } 10-21, \\text{seleccionados} \\\\ ~~~\\text{y regresión lineal} &amp; ~~~\\text{modelos} &amp; ~~~ \\text{porciones de los suplementos técnicos} \\\\ \\hline \\end{array} \\end{array} } \\] Además de estos cursos sugeridos, este libro está diseñado para lectura complementaria para un curso de series temporales, así como un libro de referencia para analistas de la industria. Mi esperanza es que los estudiantes universitarios que utilicen las primeras partes del libro en su curso universitario encuentren los capítulos posteriores útiles en sus posiciones en la industria. De esta manera, espero promover el aprendizaje continuo a lo largo de la vida. Agradecimientos Es apropiado comenzar la sección de agradecimientos agradeciendo a los estudiantes del programa actuarial aquí en la Universidad de Wisconsin; los estudiantes son socios importantes en el negocio de la creación y difusión del conocimiento en las universidades. A través de sus preguntas y comentarios, he aprendido una cantidad tremenda a lo largo de los años. También he beneficiado de la asistencia de quienes me han ayudado a reunir todas las piezas para este libro, específicamente, Missy Pinney, Peng Shi, Yunjie (Winnie) Sun y Ziyan Xie. He disfrutado trabajando con varios antiguos estudiantes y colegas en problemas de regresión en los últimos años, incluyendo a Katrien Antonio, Jie Gao, Paul Johnson, Margie Rosenberg, Jiafeng Sun, Emil Valdez y Ping Wang. Sus contribuciones están reflejadas indirectamente a lo largo del texto. Debido a mi larga asociación con la Universidad de Wisconsin-Madison, soy reacio a retroceder más en el tiempo y proporcionar una lista más extensa por temor a olvidar personas importantes. También he tenido la suerte de tener una asociación más reciente con el Insurance Services Office (ISO). Los colegas en ISO me han proporcionado importantes perspectivas sobre aplicaciones. A través de este texto que presenta aplicaciones de regresión en problemas actuariales y de la industria financiera, espero fomentar asociaciones adicionales entre la academia y la industria. Estoy encantado de reconocer las revisiones detalladas que he recibido de los colegas Tim Welnetz y Margie Rosenberg. También deseo agradecer a Bob Miller por permitirme incluir nuestro trabajo conjunto sobre diseño de gráficos efectivos en el Capítulo 21. Bob me ha enseñado mucho sobre regresión a lo largo de los años. Además, me alegra reconocer el apoyo financiero a través del Assurant Health Professorship en Ciencias Actuariales en la Universidad de Wisconsin-Madison. Guardando lo más importante para el final, agradezco a mi familia por su apoyo. Mil gracias a mi madre Mary, hermanos Randy, Guy y Joe, mi esposa Deirdre y nuestros hijos Nathan y Adam. Dedicación Hay un viejo dicho, atribuido a Sir Isaac Newton y que se puede encontrar en Google Scholar, Si he visto más lejos, es porque me he subido a los hombros de gigantes. Dedico este libro a la memoria de dos gigantes que me ayudaron, y a todos los que los conocieron, a ver más lejos y vivir mejor: James C. Hickman y Joseph P. Sullivan. "],["translation.html", "Translation", " Translation Under Construction!!! What? The purpose of this project is to develop a Spanish translation for the text Regression Modeling with Actuarial and Financial Application. This translation will be open and freely available - a resource for our community. Why? Regression provides the foundations for data science techniques such as machine learning. Although this book is a bit dated (written in 2009), it provides an easy introduction to statistical learning tools geared for applications of interest to actuaries and other financial analysts. Who? Jed Frees jfrees@bus.wisc.edu is the author of the text and has secured permission from Cambridge University Press to publish an online Spanish version. Jed is learning Spanish and is familiar with online publishing. He will take responsibility for uploading the translated version to the web. Carla Parodi and Armando Zarruk (Chair) of the Society of Actuaries´s Latin America Committee will be coordinating translation volunteers. It has been already confirmed the collaboration from the different Associations of Actuaries of Latam countries, including those from Argentina, Colombia, Ecuador, Mexico, Panama, and Peru. If you are interested in participating, please contact carla.parodi@carlaparodi.com and/or azarrukr@unal.edu.co . How? We are following a procedure similar to our successful Spanish version of Loss Data Analytics, available at https://openacttexts.github.io/LDASpanish/. Check out our Google Usage Data to see who is using this book. In short, Jed is converting his text, written in 2009 using latex, to R markdown, and then using ChatGPT for the initial translation into Spanish. Volunteers will review this translation. If the number of suggested changes are small, probably these easiest thing is to send them Carla and Armando. They will review for consistency among terminology, then forward on to Jed who will upload the changes to Github. If major changes are required, another possibility is to go to the Github site, download the .Rmd file, make the changes in the file, and send on to Jed. An “.Rmd” file is based on R’s version of a markdown file. You can open it with any text editor (e.g. Notepad - not Word), make changes, and save as text file. Date: 04 September 2024 "],["regresión-y-la-distribución-normal.html", "Chapter 1 Regresión y la Distribución Normal 1.1 ¿Qué es el Análisis de Regresión? 1.2 Ajuste de Datos a una Distribución Normal 1.3 Transformaciones de Potencia 1.4 Muestreo y el Papel de la Normalidad 1.5 Regresión y Diseños de Muestreo 1.6 Aplicaciones Actuariales de la Regresión 1.7 Lecturas Adicionales y Referencias 1.8 Ejercicios 1.9 Suplemento Técnico - Teorema del Límite Central", " Chapter 1 Regresión y la Distribución Normal Vista Previa del Capítulo. El análisis de regresión es un método estadístico que se utiliza ampliamente en muchos campos de estudio, y la ciencia actuarial no es una excepción. Este capítulo proporciona una introducción al papel de la distribución normal en la regresión, el uso de transformaciones logarítmicas para especificar relaciones de regresión y la base de muestreo que es crítica para inferir resultados de regresión a poblaciones amplias de interés. 1.1 ¿Qué es el Análisis de Regresión? La estadística trata sobre datos. Como disciplina, se ocupa de la recolección, resumen y análisis de datos para hacer afirmaciones sobre el mundo real. Cuando los analistas recolectan datos, realmente están recolectando información que se cuantifica, es decir, se transforma a una escala numérica. Existen reglas fáciles y bien entendidas para reducir los datos, utilizando medidas de resumen numéricas o gráficas. Estas medidas de resumen pueden luego vincularse a una representación teórica, o modelo, de los datos. Con un modelo que se calibra con datos, se pueden hacer afirmaciones sobre el mundo. Los métodos estadísticos han tenido un gran impacto en varios campos de estudio. En el área de recolección de datos, el diseño cuidadoso de encuestas por muestreo es crucial para los grupos de investigación de mercado y para los procedimientos de auditoría de las firmas de contabilidad. El diseño experimental es una segunda subdisciplina dedicada a la recolección de datos. El enfoque del diseño experimental es construir métodos de recolección de datos que extraigan información de la manera más eficiente posible. Esto es especialmente importante en campos como la agricultura y la ingeniería, donde cada observación es costosa, posiblemente costando millones de dólares. Otros métodos estadísticos aplicados se centran en la gestión y predicción de datos. El control de procesos se ocupa de monitorear un proceso a lo largo del tiempo y decidir cuándo la intervención es más fructífera. El control de procesos ayuda a gestionar la calidad de los bienes producidos por los fabricantes. La previsión se trata de extrapolar un proceso hacia el futuro, ya sea las ventas de un producto o los movimientos de una tasa de interés. El análisis de regresión es un método estadístico utilizado para analizar datos. Como veremos, la característica distintiva de este método es la capacidad de hacer afirmaciones sobre variables después de haber controlado los valores de variables explicativas conocidas. Aunque otros métodos son importantes, el análisis de regresión ha sido el más influyente. Para ilustrar, un índice de revistas de negocios, ABI/INFORM, enumera más de veinticuatro mil artículos que utilizan técnicas de regresión en el período de treinta años de 1978-2007. ¡Y estas son solo las aplicaciones que se consideraron lo suficientemente innovadoras como para ser publicadas en revistas académicas! El análisis de regresión de datos es tan omnipresente en los negocios modernos que es fácil pasar por alto el hecho de que la metodología tiene poco más de 120 años. Los estudiosos atribuyen el nacimiento de la regresión al discurso presidencial de 1885 de Sir Francis Galton en la sección antropológica de la Asociación Británica para el Avance de las Ciencias. En ese discurso, descrito en Stigler (1986), Galton proporcionó una descripción de la regresión y la vinculó a la teoría de la curva normal. Su descubrimiento surgió de sus estudios sobre las propiedades de la selección natural y la herencia. Para ilustrar un conjunto de datos que se puede analizar utilizando métodos de regresión, la Tabla 1.1 muestra algunos datos incluidos en el artículo de Galton de 1885. Esta tabla muestra las alturas de 928 hijos adultos, clasificados por un índice de la altura de sus padres. Aquí, todas las alturas femeninas se multiplicaron por 1.08, y el índice se creó tomando el promedio de la altura del padre y la altura reescalada de la madre. Galton era consciente de que tanto la altura de los padres como la del hijo adulto podían ser adecuadamente aproximadas por una curva normal. Al desarrollar el análisis de regresión, proporcionó un único modelo para la distribución conjunta de alturas. Table 1.1: Galtons 1885 Regression Data &lt;64.0 64.5 65.5 66.5 67.5 68.5 69.5 70.5 71.5 72.5 &gt;73.0 Total &gt;73.7 0 0 0 0 0 0 5 3 2 4 0 14 73.2 0 0 0 0 0 3 4 3 2 2 3 17 72.2 0 0 1 0 4 4 11 4 9 7 1 41 71.2 0 0 2 0 11 18 20 7 4 2 0 64 70.2 0 0 5 4 19 21 25 14 10 1 0 99 69.2 1 2 7 13 38 48 33 18 5 2 0 167 68.2 1 0 7 14 28 34 20 12 3 1 0 120 67.2 2 5 11 17 38 31 27 3 4 0 0 138 66.2 2 5 11 17 36 25 17 1 3 0 0 117 65.2 1 1 7 2 15 16 4 1 1 0 0 48 64.2 4 4 5 5 14 11 16 0 0 0 0 59 63.2 2 4 9 3 5 7 1 1 0 0 0 32 62.2 0 1 0 3 3 0 0 0 0 0 0 7 &lt;61.2 1 1 1 0 0 1 0 1 0 0 0 5 Total 14 23 66 78 211 219 183 68 43 19 4 928 Fuente: Stigler (1986) La Tabla 1.1 muestra que gran parte de la información sobre la altura de un hijo adulto puede atribuirse o ‘explicarse’ en términos de la altura de los padres. Por lo tanto, utilizamos el término variable explicativa para las mediciones que proporcionan información sobre una variable de interés. El análisis de regresión es un método para cuantificar la relación entre una variable de interés y las variables explicativas. La metodología utilizada para estudiar los datos en la Tabla 1.1 también se puede utilizar para estudiar problemas actuariales y de gestión de riesgos, la tesis de este libro. 1.2 Ajuste de Datos a una Distribución Normal Históricamente, la distribución normal tuvo un papel fundamental en el desarrollo del análisis de regresión. Continúa desempeñando un papel importante, aunque estaremos interesados en extender las ideas de regresión a datos altamente ‘no normales’. Formalmente, la curva normal se define por la función \\[\\begin{equation} \\mathrm{f}(y)=\\frac{1}{\\sigma \\sqrt{2\\pi }}\\exp \\left( -\\frac{1}{2\\sigma ^{2}% }\\left( y-\\mu \\right) ^{2}\\right) . \\tag{1.1} \\end{equation}\\] Esta curva es una función de densidad de probabilidad con toda la línea real como su dominio. De la ecuación (1.1), vemos que la curva es simétrica respecto a \\(\\mu\\) (la media y la mediana). El grado de agudeza está controlado por el parámetro \\(\\sigma ^{2}\\). Estos dos parámetros, \\(\\mu\\) y \\(\\sigma ^{2}\\), son conocidos como los parámetros de ubicación y escala, respectivamente. El Apéndice A3.1 proporciona detalles adicionales sobre esta curva, incluyendo un gráfico y tablas de su distribución acumulada que utilizaremos a lo largo del texto. La curva normal también se muestra en la Figura 1.1, una imagen de un billete de moneda alemana ahora fuera de circulación, el diez Deutsche Mark. Este billete contiene la imagen del alemán Carl Gauss, un eminente matemático cuyo nombre a menudo se asocia con la curva normal (a veces se refiere a ella como la curva Gaussiana). Gauss desarrolló la curva normal en relación con la teoría de los mínimos cuadrados para ajustar curvas a los datos en 1809, aproximadamente al mismo tiempo que el trabajo relacionado del científico francés Pierre LaPlace. Según Stigler (1986), ¡hubo bastante acritud entre estos dos científicos sobre la prioridad del descubrimiento! La curva normal se utilizó por primera vez como una aproximación a los histogramas de datos alrededor de 1835 por Adolph Quetelet, un matemático y científico social belga. Como muchas cosas buenas, la curva normal ha existido durante algún tiempo, desde aproximadamente 1720 cuando Abraham de Moivre la derivó para su trabajo sobre modelado de juegos de azar. La curva normal es popular porque es fácil de usar y ha demostrado ser exitosa en muchas aplicaciones. Figure 1.1: Diez Deutsche Mark. Moneda alemana con el científico Gauss y la curva normal. Ejemplo: Reclamaciones por Lesiones Corporales en Massachusetts. Para nuestra primera mirada al ajuste de la curva normal a un conjunto de datos, consideramos los datos de Rempala y Derrig (2005). Ellos consideraron las reclamaciones derivadas de coberturas de seguros de lesiones corporales por accidentes de automóvil. Estos son montos incurridos por tratamientos médicos ambulatorios que surgen de accidentes de automóvil, típicamente esguinces, fracturas de clavícula y similares. Los datos consisten en una muestra de 272 reclamaciones de Massachusetts que se cerraron en 2001 (por ‘cerradas,’ queremos decir que la reclamación está resuelta y no pueden surgir responsabilidades adicionales del mismo accidente). Rempala y Derrig estaban interesados en desarrollar procedimientos para manejar mezclas de reclamaciones ‘típicas’ y otras de proveedores que informaron reclamaciones fraudulentas. Para esta muestra, consideramos solo esas reclamaciones típicas, ignorando las potencialmente fraudulentas. La Tabla 1.2 proporciona varias estadísticas que resumen diferentes aspectos de la distribución. Los montos de las reclamaciones están en unidades de logaritmos de miles de dólares. La reclamación logarítmica promedio es 0.481, lo que corresponde a $1,617.77 (=1000 \\(\\exp(0.481)\\)). Las reclamaciones más pequeñas y más grandes son -3.101 (45 dólares) y 3.912 (50,000 dólares), respectivamente. Table 1.2: Summary Statistics of Massachusetts Automobile Bodily Injury Claims Number Mean Median Standard Deviation Minimum Maximum 25th Percentile 75th Percentile Claims 272 0.481 0.793 1.101 -3.101 3.912 -0.114 1.168 Para completar, aquí hay algunas definiciones. La muestra es el conjunto de datos disponibles para el análisis, denotado por \\(y_1,\\ldots,y_n\\). Aquí, \\(n\\) es el número de observaciones, \\(y_1\\) representa la primera observación, \\(y_2\\) la segunda, y así sucesivamente hasta \\(y_n\\) para la \\(n\\)-ésima observación. Aquí hay algunas estadísticas de resumen importantes. Estadísticas Básicas de Resumen La media es el promedio de las observaciones, es decir, la suma de las observaciones dividida por el número de unidades. Usando notación algebraica, la media es \\[ \\overline{y}=\\frac{1}{n}\\left( y_1 + \\cdots + y_n \\right) = \\frac{1}{n} \\sum_{i=1}^{n} y_i. \\] La mediana es la observación central cuando las observaciones están ordenadas por tamaño. Es decir, es la observación en la que el 50% está por debajo de ella (y el 50% está por encima de ella). La desviación estándar es una medida de la dispersión, o escala, de la distribución. Se calcula como \\[ s_y = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^{2}} . \\] Un percentil es un número en el que una fracción específica de las observaciones está por debajo de él, cuando las observaciones están ordenadas por tamaño. Por ejemplo, el percentil 25 es aquel número en el que el 25% de las observaciones están por debajo de él. Para ayudar a visualizar la distribución, la Figura 1.2 muestra un histograma de los datos. Aquí, la altura de cada rectángulo muestra la frecuencia relativa de observaciones que caen dentro del rango dado por su base. El histograma proporciona una impresión visual rápida de la distribución; muestra que el rango de los datos es aproximadamente (-4,4), la tendencia central es ligeramente mayor que cero y que la distribución es aproximadamente simétrica. Aproximación de la Curva Normal. La Figura 1.2 también muestra una curva normal superpuesta, utilizando \\(\\overline{y}\\) para \\(\\mu\\) y \\(s_y^{2}\\) para \\(\\sigma ^{2}\\). Con la curva normal, solo se requieren dos cantidades (\\(\\mu\\) y \\(\\sigma ^{2}\\)) para resumir toda la distribución. Por ejemplo, la Tabla 1.2 muestra que 1.168 es el percentil 75, que es aproximadamente la observación número 204 (\\(=0.75\\times 272\\)) más grande de toda la muestra. De la ecuación (1.1) de la distribución normal, tenemos que \\(z=(y-\\mu )/\\sigma\\) es una normal estándar, de la cual 0.675 es el percentil 75. Así, \\(\\overline{y}+0.675s_y=\\) \\(0.481+0.675\\times 1.101=1.224\\) es el percentil 75 usando la aproximación de la curva normal. Figure 1.2: Frecuencia Relativa de Lesiones Corporales con Curva Normal Superpuesta. Código R para Producir la Figura 1.2 injury &lt;- read.csv(&quot;CSVData/MassBodilyInjury.csv&quot;, header=TRUE) injury2&lt;-subset(injury, providerA != 0 ) LOGCLAIMS&lt;-log(injury2$claims) # FIGURA 1.2 x &lt;- seq(-4, 4, 0.01) y &lt;- dnorm(x, mean=mean(LOGCLAIMS), sd=sqrt(var(LOGCLAIMS))) hist(LOGCLAIMS, freq=FALSE, main=&quot;&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Densidad&quot;, side=2, at=.35,las=1, adj=.7,cex=1.4) lines(x,y) Diagrama de Caja. Una inspección visual rápida de la distribución de una variable puede revelar algunas características sorprendentes que están ocultas por estadísticas, medidas de resumen numéricas. El diagrama de caja, también conocido como diagrama de ‘caja y bigotes’, es uno de estos dispositivos gráficos. La Figura 1.3 ilustra un diagrama de caja para las reclamaciones por lesiones corporales. Aquí, la caja captura el 50% central de los datos, con las tres líneas horizontales que corresponden a los percentiles 75, 50 y 25, leyendo de arriba a abajo. Las líneas horizontales por encima y por debajo de la caja son los ‘bigotes’. El bigote superior es 1.5 veces el rango intercuartílico (la diferencia entre los percentiles 75 y 25) por encima del percentil 75. De manera similar, el bigote inferior es 1.5 veces el rango intercuartílico por debajo del percentil 25. Las observaciones individuales fuera de los bigotes se denotan con pequeños símbolos de trazado circulares, y se denominan ‘valores atípicos’. Figure 1.3: Diagrama de Caja de Reclamaciones por Lesiones Corporales. Código R para Producir la Figura 1.3 boxplot(LOGCLAIMS, boxwex=.7, las=1) text(1, .57, &quot;mediana&quot;, cex=1.2) text(1.36, -0.2, &quot;percentil 25&quot;, cex=1.2) text(1.36, 1.1, &quot;percentil 75&quot;, cex=1.2) arrows(1.05, -2, 1.05, -3.3, code=3, angle=20, length=0.1) text(1.15, -2.5, &quot;valores atípicos&quot;, cex=1.2) text(1.13, 3.9, &quot;valor atípico&quot;, cex=1.2) Los gráficos son herramientas poderosas; permiten a los analistas visualizar fácilmente relaciones no lineales que son difíciles de comprender cuando se expresan verbalmente o mediante fórmulas matemáticas. Sin embargo, debido a su gran flexibilidad, los gráficos también pueden engañar fácilmente al analista. El Capítulo 21 subrayará este punto. Por ejemplo, la Figura 1.4 es un re-dibujo de la Figura 1.2; la diferencia es que la Figura 1.4 usa más y más finos rectángulos. Este análisis más detallado revela la naturaleza asimétrica de la distribución de la muestra que no era evidente en la Figura 1.2. Figure 1.4: Re-dibujo de la Figura 1.2 con un número aumentado de rectángulos. Código R para Producir la Figura 1.4 hist(LOGCLAIMS, freq=FALSE, nclass=32, main=&quot;&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Densidad&quot;, side=2, at=.75,las=1, adj=.7,cex=1.1) lines(x,y) Gráficos Cuantiles-Cuantiles. Aumentar el número de rectángulos puede descubrir características que no eran evidentes antes; sin embargo, en general hay menos observaciones por rectángulo, lo que significa que la incertidumbre de la estimación de la frecuencia relativa aumenta. Esto representa un compromiso. En lugar de forzar al analista a tomar una decisión arbitraria sobre el número de rectángulos, una alternativa es usar un dispositivo gráfico para comparar una distribución con otra conocida, llamado gráfico de cuantiles-cuantiles, o qq. La Figura 1.5 ilustra un gráfico \\(qq\\) para los datos de lesiones corporales utilizando la curva normal como distribución de referencia. Para cada punto, el eje vertical da el cuantil usando la distribución de la muestra. El eje horizontal da la cantidad correspondiente usando la curva normal. Por ejemplo, anteriormente consideramos el punto del percentil 75. Este punto aparece como (1.168, 0.675) en el gráfico. Para interpretar un gráfico \\(qq\\), si los puntos de los cuantiles se alinean a lo largo de la línea superpuesta, entonces la muestra y la distribución de referencia normal tienen la misma forma. (Esta línea se define conectando los percentiles 75 y 25). En la Figura 1.5, los percentiles pequeños de la muestra son consistentemente más pequeños que los valores correspondientes de la normal estándar, lo que indica que la distribución está sesgada a la izquierda. La diferencia en los valores en los extremos de la distribución se debe a los valores atípicos mencionados anteriormente que también podrían interpretarse como que la distribución de la muestra tiene colas más grandes que la distribución de referencia normal. Figure 1.5: Un gráfico \\(qq\\) de Reclamaciones por Lesiones Corporales, usando una distribución de referencia normal. Código R para Producir la Figura 1.5 qqnorm(LOGCLAIMS, main=&quot;&quot;, las=1, ylab=&quot;&quot;) mtext(&quot;Cuantiles de la Muestra&quot;, side=2, at=4.5, las=1,cex=1.1,adj=.4) qqline(LOGCLAIMS) 1.3 Transformaciones de Potencia En el ejemplo de la Sección 1.2, consideramos las reclamaciones sin justificar el uso de la escala logarítmica. Al analizar variables como los activos de las empresas, los salarios de los individuos y los precios de las viviendas en aplicaciones empresariales y económicas, es común considerar logaritmos en lugar de las unidades originales. Una transformación logarítmica mantiene el orden original (por ejemplo, los salarios altos siguen siendo altos en la escala de logaritmos de los salarios) pero sirve para ‘acercar’ los valores extremos de la distribución. Para ilustrar, la Figura 1.6 muestra la distribución de las reclamaciones por lesiones corporales en (miles de) dólares. Para graficar los datos de manera significativa, se eliminó la observación más grande ($50,000) antes de hacer este gráfico. Incluso con esta observación eliminada, la Figura 1.6 muestra que la distribución está muy inclinada hacia la derecha, con varios valores grandes de reclamaciones apareciendo. Las distribuciones que están inclinadas en una dirección u otra se conocen como sesgadas. La Figura 1.6 es un ejemplo de una distribución sesgada a la derecha, o sesgada positivamente. Aquí, la cola de la distribución a la derecha es más larga y hay una mayor concentración de masa a la izquierda. En contraste, una distribución sesgada a la izquierda, o sesgada negativamente, tiene una cola más larga a la izquierda y una mayor concentración de masa a la derecha. Muchas distribuciones de reclamaciones de seguros están sesgadas a la derecha (ver el texto de Klugman, Panjer y Willmot, 2008, para discusiones extensas). Como vimos en las Figuras 1.4 y 1.5, una transformación logarítmica produce una distribución que está solo ligeramente sesgada a la izquierda. Figure 1.6: Distribución de Reclamaciones por Lesiones Corporales. Las observaciones están en (miles de) dólares con la observación más grande omitida. Código R para Producir la Figura 1.6 injury3 = subset(injury, claims &lt; 25 ) CLAIMS25 &lt;- injury3$claims par(mar=c(4.2,4,1.2,.2),cex=1.1) hist(CLAIMS25, freq=FALSE, main=&quot;&quot;, las=1, ylab=&quot;&quot;, xlab=&quot;CLAIMS&quot;) mtext(&quot;Densidad&quot;, side=2, at=.28, las=1,cex=1.1) Las transformaciones logarítmicas se usan extensamente en el trabajo de estadística aplicada. Una ventaja es que sirven para simetrizar distribuciones que están sesgadas. Más generalmente, consideramos transformaciones de potencia, también conocidas como la familia de transformaciones de Box-Cox. Dentro de esta familia de transformaciones, en lugar de usar la respuesta \\(y\\), usamos una versión transformada o reescalada, \\(y^{\\lambda}\\). Aquí, la potencia \\(\\lambda\\) (lambda, una ‘l’ griega) es un número que puede ser especificado por el usuario. Los valores típicos de \\(\\lambda\\) que se usan en la práctica son \\(\\lambda\\)=1, 1/2, 0 o -1. Cuando usamos \\(\\lambda =0\\), queremos decir \\(\\ln (y)\\), es decir, la transformación logarítmica natural. Más formalmente, la familia de Box-Cox puede expresarse como \\[ y^{(\\lambda )}=\\left\\{ \\begin{array}{ll} \\frac{y^{\\lambda }-1}{\\lambda } &amp; \\lambda \\neq 0 \\\\ \\ln (y) &amp; \\lambda =0 \\end{array} \\right. . \\] Como veremos, porque las estimaciones de regresión no se ven afectadas por desplazamientos de ubicación y escala, en la práctica no necesitamos restar uno ni dividir por \\(\\lambda\\) al reescalar la respuesta. La ventaja de la expresión anterior es que, si dejamos que \\(\\lambda\\) se acerque a 0, entonces \\(y^{(\\lambda )}\\) se acerca a \\(\\ln (y)\\), a partir de algunos argumentos de cálculo sencillos. Para ilustrar la utilidad de las transformaciones, simulamos 500 observaciones de una distribución chi-cuadrado con dos grados de libertad. El Apéndice A3.2 introduce esta distribución (que encontraremos nuevamente más adelante al estudiar el comportamiento de los estadísticos de prueba). El panel superior izquierdo de la Figura 1.7 muestra que la distribución original está muy sesgada hacia la derecha. Los otros paneles en la Figura 1.7 muestran los datos reescalados utilizando las transformaciones de raíz cuadrada, logarítmica y recíproca negativa. La transformación logarítmica, en el panel inferior izquierdo, proporciona la mejor aproximación a la simetría para este ejemplo. La transformación recíproca negativa se basa en \\(\\lambda =-1\\), y luego multiplicando las observaciones reescaladas por menos uno, de modo que las observaciones grandes sigan siendo grandes. Figure 1.7: 500 observaciones simuladas de una distribución chi-cuadrado. El panel superior izquierdo se basa en la distribución original. El superior derecho corresponde a la transformación de raíz cuadrada, el inferior izquierdo a la transformación logarítmica y el inferior derecho a la transformación recíproca negativa. Código R para Producir la Figura 1.7 set.seed(1237) X1 &lt;- 10000*rchisq(500*1, df=2) X2 &lt;- sqrt(X1) X3 &lt;- log(X1) X4 &lt;- -1/X1 par(mfrow=c(2, 2), cex=.75, mar=c(3,5,1.5,0)) hist(X1, freq=FALSE, nclass=16, main=&quot;&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, las=1, yaxt=&quot;n&quot;,xlim=c(0,200000),ylim=c(0,.00005)) axis(2, at=seq(0,.00005,.00001),las=1, cex=.3, labels=c(&quot;0&quot;, &quot;0.00001&quot;, &quot;0.00002&quot;,&quot;0.00003&quot;, &quot;0.00004&quot;, &quot;0.00005&quot;)) mtext(&quot;Densidad&quot;, side=2, at=.000055, las=1, cex=.75) mtext(&quot;y&quot;, side=1, cex=.75, line=2) par(mar=c(3,4,1.5,0.2)) hist(X2, freq=FALSE, nclass=16, main=&quot;&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, las=1,xlim=c(0,400), ylim=c(0,.008)) mtext(&quot;Densidad&quot;, side=2, at=.0088, las=1, cex=.75) mtext(&quot;Raíz cuadrada de y&quot;, side=1, cex=.75, line=2) par(mar=c(3.2,5,1,0)) hist(X3, freq=FALSE, nclass=16, main=&quot;&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, las=1, ylim=c(0,.4)) mtext(&quot;Densidad&quot;, side=2, at=.44, las=1, cex=.75) mtext(&quot;Logaritmo de y&quot;, side=1, cex=.75, line=2) par(mar=c(3.2,4,1,0.2)) hist(X4, freq=FALSE, nclass=16, main=&quot;&quot;,xlab=&quot;&quot;, ylab=&quot;&quot;, las=1, ylim=c(0,100)) mtext(&quot;Densidad&quot;, side=2, at=110, las=1, cex=.75) mtext(&quot;Recíproco negativo de y&quot;, side=1, cex=.75, line=2) 1.4 Muestreo y el Papel de la Normalidad Una estadística es una medida resumen de los datos, como una media, mediana o percentil. Las colecciones de estadísticas son muy útiles para analistas, tomadores de decisiones y consumidores cotidianos para comprender grandes cantidades de datos que representan situaciones complejas. Hasta este punto, nuestro enfoque ha sido introducir técnicas sensatas para resumir variables; técnicas que se usarán repetidamente a lo largo de este texto. Sin embargo, la verdadera utilidad de la disciplina de la estadística es su capacidad para decir algo sobre lo desconocido, no solo para resumir la información ya disponible. Con este fin, necesitamos hacer algunas suposiciones bastante formales sobre la manera en que se observan los datos. Como ciencia, una característica destacada de la estadística (como disciplina) es la capacidad de criticar estas suposiciones y ofrecer alternativas mejoradas en situaciones específicas. Es costumbre suponer que los datos se extraen de una población más grande que estamos interesados en describir. El proceso de extracción de los datos se conoce como muestreo, o proceso generador de datos. Denotamos esta muestra como \\(\\{y_1,\\ldots,y_n\\}\\). Para que podamos criticar y modificar estas suposiciones de muestreo, las enumeramos a continuación en detalle: \\[ \\begin{array}{l} \\hline \\textbf{Suposiciones Básicas de Muestreo} \\\\ \\hline 1. ~\\mathrm{E~}y_i=\\mu \\\\ 2. ~\\mathrm{Var~}y_i=\\sigma ^{2} \\\\ 3. ~\\{y_i\\} \\text{ son independientes} \\\\ 4. ~\\{y_i\\} \\text{ están distribuidos normalmente}. \\\\ \\hline \\end{array} \\] En esta configuración básica, \\(\\mu\\) y \\(\\sigma ^{2}\\) sirven como parámetros que describen la ubicación y escala de la población de origen. El objetivo es inferir algo sensato sobre ellos basándose en estadísticas como \\(\\overline{y}\\) y \\(s_y^{2}\\). Para la tercera suposición, asumimos independencia entre las extracciones. En un esquema de muestreo, esto puede ser garantizado tomando una muestra aleatoria simple de una población. La cuarta suposición no es necesaria para muchos procedimientos de inferencia estadística porque los teoremas del límite central proporcionan una normalidad aproximada para muchas estadísticas de interés. Sin embargo, una justificación formal de algunas estadísticas, como las t-estadísticas, requiere esta suposición adicional. La Sección 1.9 proporciona una declaración explícita de una versión del teorema del límite central, dando condiciones bajo las cuales \\(\\overline{y}\\) está aproximadamente distribuido normalmente. Esta sección también discute un resultado relacionado, conocido como aproximación de Edgeworth, que muestra que la calidad de la aproximación normal es mejor para poblaciones de origen simétricas en comparación con distribuciones sesgadas. ¿Cómo se aplica esta discusión al estudio del análisis de regresión? Después de todo, hasta ahora nos hemos centrado solo en el promedio aritmético simple, \\(\\overline{y}\\). En capítulos posteriores, enfatizaremos que la regresión lineal es el estudio de promedios ponderados; específicamente, muchos coeficientes de regresión pueden expresarse como promedios ponderados con pesos apropiadamente elegidos. Los teoremas de límite central y aproximación de Edgeworth están disponibles para promedios ponderados; estos resultados asegurarán la normalidad aproximada de los coeficientes de regresión. Para usar aproximaciones de la curva normal en un contexto de regresión, a menudo transformaremos variables para lograr una simetría aproximada. 1.5 Regresión y Diseños de Muestreo La aproximación a la normalidad será un tema importante en las aplicaciones prácticas de la regresión lineal. Las Partes I y II de este libro se centran en la regresión lineal, donde aprenderemos conceptos básicos de regresión y diseño de muestreo. La Parte III se centrará en la regresión no lineal, que involucra respuestas binarias, de conteo y de colas pesadas, donde la normalidad no es la distribución de referencia más útil. Las ideas sobre conceptos básicos y diseño también se usarán en el contexto no lineal. En el análisis de regresión, nos enfocamos en una medición de interés y la llamamos variable dependiente. Otras mediciones se usan como variables explicativas. Un objetivo es comparar las diferencias en la variable dependiente en términos de diferencias en las variables explicativas. Como se mencionó en la Sección 1.1, la regresión se usa extensamente en muchos campos científicos. Tabla 1.3 enumera términos alternativos que puedes encontrar al leer aplicaciones de regresión. Tabla 1.3. Terminología para Variables de Regresión \\[ {\\small \\begin{array}{ll}\\hline\\hline y-\\text{Variable} &amp; x-\\text{Variable} \\\\\\hline \\text{Resultado de interés} &amp; \\text{Variable explicativa} \\\\ \\text{Variable dependiente} &amp; \\text{Variable independiente} \\\\ \\text{Variable endógena} &amp; \\text{Variable exógena} \\\\ \\text{Respuesta} &amp; \\text{Tratamiento} \\\\ \\text{Regresando} &amp; \\text{Regresor} \\\\ \\text{Variable del lado izquierdo} &amp; \\text{Variable del lado derecho} \\\\ \\text{Variable explicada} &amp; \\text{Variable predictora} \\\\ \\text{Resultado} &amp; \\text{Entrada} \\\\ \\hline \\end{array} } \\] En la última parte del siglo XIX y principios del siglo XX, la estadística comenzó a tener un impacto importante en el desarrollo de la ciencia experimental. Las ciencias experimentales a menudo utilizan estudios diseñados, donde los datos están bajo el control de un analista. Los estudios diseñados se realizan en entornos de laboratorio, donde hay restricciones físicas estrictas en cada variable que un investigador considera importante. Los estudios diseñados también ocurren en experimentos de campo más grandes, donde los mecanismos de control son diferentes a los de los entornos de laboratorio. La agricultura y la medicina utilizan estudios diseñados. Los datos de un estudio diseñado se dicen que son datos experimentales. Para ilustrar, un ejemplo clásico es considerar el rendimiento de un cultivo como el maíz, donde cada uno de varios parcelas de tierra (las observaciones) se asigna a varios niveles de fertilizante. El objetivo es determinar el efecto del fertilizante (la variable explicativa) en el rendimiento del maíz (la variable de respuesta). Aunque los investigadores intentan hacer que las parcelas de tierra sean lo más similares posible, inevitablemente surgen diferencias. Los investigadores agrícolas utilizan técnicas de aleatorización para asignar diferentes niveles de fertilizante a cada parcela de tierra. De esta manera, los analistas pueden explicar la variación en los rendimientos de maíz en términos de la variación de los niveles de fertilizante. A través del uso de técnicas de aleatorización, los investigadores que utilizan estudios diseñados pueden inferir que el tratamiento tiene un efecto causal sobre la respuesta. El Capítulo 6 discute la causalidad más a fondo. Ejemplo: Experimento de Seguro de Salud Rand. ¿Cómo están relacionados los gastos en atención médica con la demanda de seguro? Muchos estudios han establecido una relación positiva entre la cantidad gastada en atención médica y la demanda de seguro de salud. Aquellos en mala salud anticipan usar más servicios médicos que las personas en buena o regular salud y buscarán niveles más altos de seguro de salud para compensar estos gastos anticipados. Obtienen este seguro adicional al (i) seleccionar un plan de seguro de salud más generoso de un empleador, (ii) elegir un empleador con un plan de seguro de salud más generoso o (iii) pagar más por un seguro de salud individual. Así, es difícil desenredar la relación causa-efecto de los gastos en atención médica y la disponibilidad de seguro de salud. Un estudio reportado por Manning et al. (1987) buscó responder a esta pregunta utilizando un experimento cuidadosamente diseñado. En este estudio, los hogares inscritos de seis ciudades, entre noviembre de 1974 y febrero de 1977, fueron asignados aleatoriamente a uno de 14 planes de seguro diferentes. Estos planes variaban según los elementos de participación en los costos, la tasa de coaseguro (el porcentaje pagado de los gastos de bolsillo que variaba entre 0, 25, 50 y 95%) así como el deducible (5, 10 o 15 por ciento del ingreso familiar, hasta un máximo de $1,000). Así, hubo una asignación aleatoria a los niveles del tratamiento, la cantidad de seguro de salud. El estudio encontró que los planes más favorables resultaron en mayores gastos totales, incluso después de controlar el estado de salud de los participantes. Para la ciencia actuarial y otras ciencias sociales, los estudios diseñados son la excepción más que la regla. Por ejemplo, si queremos estudiar los efectos del tabaquismo en la mortalidad, es muy poco probable que podamos conseguir que los participantes en el estudio acepten ser asignados aleatoriamente a grupos de fumadores/no fumadores durante varios años solo para observar sus patrones de mortalidad. Al igual que en el estudio de Galton de la Sección 1.1, los investigadores en ciencias sociales generalmente trabajan con datos observacionales. Los datos observacionales no están bajo el control del analista. Con datos observacionales, no podemos inferir relaciones causales, pero podemos introducir medidas de asociación. Para ilustrar, en los datos de Galton, es evidente que los padres ‘altos’ tienden a tener hijos ‘altos’ y, a la inversa, los padres ‘bajos’ tienden a tener hijos ‘bajos’. El Capítulo 2 introducirá la correlación y otras medidas de asociación. Sin embargo, no podemos inferir causalidad a partir de los datos. Por ejemplo, puede haber otra variable, como la dieta familiar, que esté relacionada con ambas variables. Una buena dieta en la familia podría estar asociada con alturas altas de los padres y de los hijos adultos, mientras que una dieta pobre limita el crecimiento. Si ese fuera el caso, llamaríamos a la dieta familiar una variable confusora. En experimentos diseñados como el Experimento de Seguro de Salud Rand, podemos controlar los efectos de variables como el estado de salud mediante métodos de asignación aleatoria. En estudios observacionales, usamos control estadístico, en lugar de control experimental. Para ilustrar, en los datos de Galton, podríamos dividir nuestras observaciones en dos grupos, uno para ‘buena dieta familiar’ y otro para ‘mala dieta familiar’, y examinar la relación entre la altura de los padres y la de los hijos para cada subgrupo. Esta es la esencia del método de regresión, comparar un \\(y\\) y un \\(x\\), ‘controlando’ los efectos de otras variables explicativas. Por supuesto, para usar control estadístico y métodos de regresión, uno debe registrar la dieta familiar y cualquier otra medida de altura que pueda confundir los efectos de la altura de los padres en la altura de su hijo adulto. La dificultad en diseñar estudios es tratar de imaginar todas las variables que podrían afectar una variable de respuesta, una tarea imposible en la mayoría de los problemas de interés en ciencias sociales. Para dar algunas orientaciones sobre cuándo ‘es suficiente’, el Capítulo 6 discutirá medidas de la importancia de una variable explicativa y su impacto en la selección del modelo. 1.6 Aplicaciones Actuariales de la Regresión Este libro introduce un método estadístico, el análisis de regresión. La introducción está organizada en torno a la tríada tradicional de la inferencia estadística: prueba de hipótesis, estimación y predicción. Además, este libro muestra cómo esta metodología puede ser utilizada en aplicaciones que probablemente serán de interés para los actuarios y otros analistas de riesgos. Como tal, es útil comenzar con las tres áreas tradicionales de aplicaciones actuariales: tarificación, reservas y pruebas de solvencia. Tarificación y selección adversa. El análisis de regresión puede utilizarse para determinar los precios de seguros para muchas líneas de negocio. Por ejemplo, en el seguro de automóviles de pasajeros privados, las reclamaciones esperadas varían según el género del asegurado, la edad, la ubicación (ciudad versus rural), el propósito del vehículo (trabajo o placer) y una serie de otras variables explicativas. La regresión puede utilizarse para identificar las variables que son determinantes importantes de las reclamaciones esperadas. En mercados competitivos, las compañías de seguros no usan el mismo precio para todos los asegurados. Si lo hicieran, los ‘buenos riesgos’, aquellos con reclamaciones esperadas inferiores a la media, pagarían de más y abandonarían la compañía. En contraste, los ‘malos riesgos’, aquellos con reclamaciones esperadas superiores a la media, permanecerían con la compañía. Si la compañía continuara con esta política de precios plana, las primas aumentarían (para compensar las reclamaciones de la creciente proporción de malos riesgos) y la participación en el mercado disminuiría a medida que la compañía pierde buenos riesgos. Este problema se conoce como ‘selección adversa’. Usando un conjunto apropiado de variables explicativas, se pueden desarrollar sistemas de clasificación para que cada asegurado pague su parte justa. Reservas y pruebas de solvencia. Tanto la constitución de reservas como las pruebas de solvencia se preocupan por predecir si las obligaciones asociadas con un grupo de pólizas excederán el capital destinado a cumplir con las obligaciones derivadas de las pólizas. La constitución de reservas implica determinar la cantidad apropiada de capital para cumplir con estas obligaciones. Las pruebas de solvencia se centran en evaluar la adecuación del capital para financiar las obligaciones de un bloque de negocio. En algunas áreas de práctica, la regresión puede usarse para pronosticar futuras obligaciones y ayudar a determinar las reservas (ver, por ejemplo, el Capítulo 19). La regresión también puede utilizarse para comparar las características de empresas saludables y financieramente angustiadas para pruebas de solvencia (ver, por ejemplo, el Capítulo 14). Otras aplicaciones en gestión de riesgos. El análisis de regresión es una herramienta cuantitativa que puede aplicarse en una amplia variedad de problemas de negocio, no solo en las áreas tradicionales de tarificación, reservas y pruebas de solvencia. Al familiarizarse con el análisis de regresión, los actuarios tendrán otra habilidad cuantitativa que puede aplicarse a problemas generales relacionados con la seguridad financiera de personas, empresas y organizaciones gubernamentales. Para ayudarte a desarrollar ideas, este libro proporciona muchos ejemplos de aplicaciones potenciales ‘no actuariales’ a través de viñetas destacadas etiquetadas como ‘ejemplos’ y conjuntos de datos ilustrativos. Para entender las posibles aplicaciones de la regresión, comienza revisando los varios conjuntos de datos presentados en los Ejercicios del Capítulo 1. Incluso si no completas los ejercicios para fortalecer tus habilidades de resumen de datos (que requieren el uso de una computadora), una revisión de las descripciones de los problemas te ayudará a familiarizarte con los tipos de aplicaciones en las que un actuario podría usar técnicas de regresión. 1.7 Lecturas Adicionales y Referencias Este libro introduce herramientas de regresión y series temporales que son más relevantes para los actuarios y otros analistas de riesgos financieros. Afortunadamente, existen otras fuentes que proporcionan excelentes introducciones a estos temas estadísticos (aunque no desde un punto de vista de gestión de riesgos). En particular, para los analistas que desean especializarse en estadística, es útil obtener otra perspectiva. Para regresión, recomiendo Weisburg (2005) y Faraway (2005). Para series temporales, Diebold (2004) es una buena fuente. Además, Klugman, Panjer y Willmot (2008) proporciona una buena introducción a las aplicaciones actuariales de la estadística; este libro está destinado a complementar el libro de Klugman et al. al centrarse en métodos de regresión y series temporales. Referencias del Capítulo Beard, Robert E., Teivo Pentik\"{a}inen and Erkki Pesonen (1984). Risk Theory: The Stochastic Basis of Insurance (Third Edition). Chapman &amp; Hall, New York. Diebold, Francis. X. (2004). Elements of Forecasting, Third Edition. Thomson, South-Western, Mason, Ohio. Faraway, Julian J. (2005). Linear Models in R. Chapman &amp; Hall/CRC, New York. Hogg, Robert V. (1972). On statistical education. The American Statistician 26, 8-11. Klugman, Stuart A, Harry H. Panjer and Gordon E. Willmot (2008). Loss Models: From Data to Decisions. John Wiley &amp; Sons, Hoboken, New Jersey. Manning, Willard G., Joseph P. Newhouse, Naihua Duan, Emmett B. Keeler, Arleen Leibowitz and M. Susan Marquis (1987). Health insurance and the demand for medical care: Evidence from a randomized experiment. American Economic Review 77, No. 3, 251-277. Rempala, Grzegorz A. and Richard A. Derrig (2005). Modeling hidden exposures in claim severity via the EM algorithm. North American Actuarial Journal 9, No. 2, 108-128. Singer, Judith D. and Willett, J. B. (1990). Improving the teaching of applied statistics: Putting the data back into data analysis. The American Statistician 44, 223-230. Stigler, Steven M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. The Belknap Press of Harvard University Press, Cambridge, MA. Weisberg, Sanford (2005). Applied Linear Regression, Third Edition. John Wiley &amp; Sons, New York. 1.8 Ejercicios 1.1 Gastos de Salud de MEPS. Este ejercicio considera datos de la Encuesta de Gastos Médicos (MEPS), realizada por la Agencia de Investigación y Calidad en Salud de EE.UU. (AHRQ). MEPS es una encuesta probabilística que proporciona estimaciones nacionalmente representativas del uso de atención médica, gastos, fuentes de pago y cobertura de seguros para la población civil de EE.UU. Esta encuesta recopila información detallada sobre episodios de atención médica de cada tipo de servicio, incluyendo visitas a consultorios médicos, visitas a salas de emergencia hospitalarias, visitas a hospitales ambulatorios, estancias hospitalarias, otras visitas a proveedores médicos y uso de medicamentos prescritos. Esta información detallada permite desarrollar modelos de utilización de atención médica para predecir futuros gastos. Puedes obtener más información sobre MEPS en http://www.meps.ahrq.gov/mepsweb/. Consideramos los datos de MEPS de los paneles 7 y 8 de 2003, que consisten en 18,735 individuos entre las edades de 18 y 65 años. De esta muestra, tomamos una muestra aleatoria de 2,000 individuos que aparecen en el archivo ‘HealthExpend’. De esta muestra, hay 157 individuos que tuvieron gastos hospitalarios positivos. También hay 1,352 que tuvieron gastos ambulatorios positivos. Analizaremos estas dos muestras por separado. Nuestras variables dependientes consisten en las cantidades de gastos para visitas hospitalarias (EXPENDIP) y ambulatorias (EXPENDOP). Para MEPS, los eventos ambulatorios incluyen visitas al departamento ambulatorio del hospital, visitas a proveedores en consultorios y visitas a salas de emergencia, excluyendo servicios dentales. (Los servicios dentales, en comparación con otros tipos de servicios de atención médica, son más predecibles y ocurren de manera más regular). Las estancias hospitalarias con la misma fecha de admisión y alta, conocidas como ‘estancias de cero noches’, se incluyeron en los recuentos y gastos ambulatorios. (Los pagos asociados con visitas a salas de emergencia que precedieron inmediatamente a una estancia hospitalaria se incluyeron en los gastos hospitalarios. Los medicamentos prescritos que se pueden vincular a las admisiones hospitalarias se incluyeron en los gastos hospitalarios, no en la utilización ambulatoria). Parte 1: Usa solo los 157 individuos que tuvieron gastos hospitalarios positivos y realiza el siguiente análisis. Calcula estadísticas descriptivas para los gastos hospitalarios (EXPENDIP). a(i). ¿Cuál es el gasto típico (media y mediana)? a(ii). ¿Cómo se compara la desviación estándar con la media? ¿Los datos parecen estar sesgados? Calcula un diagrama de caja, un histograma y un gráfico \\(qq\\) (normal) para EXPENDIP. Comenta sobre la forma de la distribución. Transformaciones. c(i). Realiza una transformación de raíz cuadrada de los gastos hospitalarios. Resume la distribución resultante usando un histograma y un gráfico \\(qq\\). ¿Parece estar aproximadamente distribuida normalmente? c(ii). Realiza una transformación logarítmica (natural) de los gastos hospitalarios. Resume la distribución resultante usando un histograma y un gráfico \\(qq\\). ¿Parece estar aproximadamente distribuida normalmente? Parte 2: Usa solo los 1,352 individuos que tuvieron gastos ambulatorios positivos. Repite la parte (a) y calcula histogramas para los gastos y los gastos logarítmicos. Comenta sobre la normalidad aproximada de cada histograma. 1.2 Utilización de Hogares de Cuidado de Ancianos. Este ejercicio considera datos de hogares de cuidado de ancianos proporcionados por el Departamento de Salud y Servicios Familiares de Wisconsin (DHFS). El programa Medicaid del Estado de Wisconsin financia el cuidado en hogares de cuidado de ancianos para individuos que califican en base a necesidades y estado financiero. Como parte de las condiciones de participación, los hogares de cuidado de ancianos certificados por Medicaid deben presentar un informe de costos anual al DHFS, resumiendo el volumen y costo del cuidado proporcionado a todos sus residentes, financiados por Medicaid y de otro tipo. Estos informes de costos son auditados por el personal del DHFS y forman la base para las tasas diarias de pago de Medicaid específicas para cada instalación para los períodos subsiguientes. Los datos están disponibles públicamente; consulta [http://dhs.wisconsin.gov] para más información. El DHFS está interesado en técnicas predictivas que proporcionen pronósticos de utilización confiables para actualizar su programa de tasas de financiamiento de Medicaid para instalaciones de cuidado de ancianos. En esta tarea, consideramos los datos en el archivo ‘WiscNursingHome’ en los años de informe de costos 2000 y 2001. Hay 362 instalaciones en 2000 y 355 instalaciones en 2001. Típicamente, la utilización del cuidado en hogares de cuidado de ancianos se mide en días de paciente (‘días de paciente’ es el número de días que cada paciente estuvo en la instalación, sumado sobre todos los pacientes). Para este ejercicio, definimos la variable de resultado como años totales de pacientes (TPY), el número total de días de pacientes en el período de informe de costos dividido por el número de días operativos de la instalación en el período de informe de costos (ver Rosenberg et al., 2007, Apéndice 1, para una discusión adicional sobre esta elección). El número de camas (NUMBED) y el metraje cuadrado (SQRFOOT) del hogar de cuidado de ancianos miden el tamaño de la instalación. No sorprende que estas variables sean importantes predictores de TPY. Parte 1: Usa los datos del año de informe de costos 2000 y realiza el siguiente análisis. Calcula estadísticas descriptivas para TPY, NUMBED y SQRFOOT. Resume la distribución de TPY usando un histograma y un gráfico \\(qq\\). ¿Parece estar aproximadamente distribuida normalmente? Transformaciones. Realiza una transformación logarítmica (natural) de TPY (LOGTPY). Resume la distribución resultante usando un histograma y un gráfico \\(qq\\). ¿Parece estar aproximadamente distribuida normalmente? Parte 2: Usa los datos del año de informe de costos 2001 y repite las partes (a) y (c). 1.3 Reclamaciones de Seguros de Automóviles. Como analista actuarial, estás trabajando con una gran compañía de seguros para ayudarles a entender la distribución de sus reclamaciones para sus pólizas de automóviles particulares. Tienes disponibles datos de reclamaciones para un año reciente, que consisten en: STATE CODE: códigos del 01 al 17 utilizados, con cada código asignado aleatoriamente a un estado real. CLASS: clase de calificación del operador, basada en edad, género, estado civil y uso del vehículo. GENDER: género del operador. AGE: edad del operador. PAID: monto pagado para resolver y cerrar una reclamación. Te estás enfocando en conductores mayores de 50 años, para los cuales hay \\(n = 6,773\\) reclamaciones disponibles. Examina el histograma del monto PAID y comenta sobre la simetría. Crea una nueva variable, las reclamaciones pagadas en logaritmo natural, LNPAID. Crea un histograma y un gráfico \\(qq\\) de LNPAID. Comenta sobre la simetría de esta variable. 1.4 Costos Hospitalarios. Supongamos que eres un actuario de beneficios para empleados trabajando con una empresa de tamaño mediano en Wisconsin. Esta empresa está considerando ofrecer, por primera vez en su industria, cobertura de seguro hospitalario para los hijos dependientes de sus empleados. Tienes acceso a los registros de la empresa y, por lo tanto, tienes disponibles el número, edad y género de los hijos dependientes, pero no tienes otra información sobre los costos hospitalarios de la empresa. En particular, ninguna empresa en esta industria ha ofrecido esta cobertura, por lo que tienes poca experiencia histórica en la industria sobre la cual puedas prever los reclamos esperados. Recopilas datos del Nationwide Inpatient Sample del Healthcare Cost and Utilization Project (NIS-HCUP), una encuesta nacional de costos hospitalarios realizada por la Agencia de Investigación y Calidad en Salud de EE.UU. (AHRQ). Restringes la consideración a hospitales de Wisconsin y analizas una muestra aleatoria de \\(n=500\\) reclamaciones de datos de 2003. Aunque los datos provienen de registros hospitalarios, están organizados por alta individual y por lo tanto tienes información sobre la edad y el género del paciente dado de alta. Específicamente, consideras pacientes de 0 a 17 años. En un proyecto separado, considerarás la frecuencia de hospitalización. Para este proyecto, el objetivo es modelar la gravedad de los cargos hospitalarios, por edad y género. Examina la distribución de la variable dependiente, TOTCHG. Haz esto creando un histograma y luego un gráfico \\(qq\\), comparando el empírico con una distribución normal. Realiza una transformación logarítmica natural y llama a la nueva variable LNTOTCHG. Examina la distribución de esta variable transformada. Para visualizar la relación logarítmica, traza LNTOTCHG frente a TOTCHG. 1.5 Reclamaciones de Seguros de Lesiones Automovilísticas. Consideramos datos de reclamaciones por lesiones en automóviles utilizando datos del Insurance Research Council (IRC), una división del American Institute for Chartered Property Casualty Underwriters y del Insurance Institute of America. Los datos, recopilados en 2002, contienen información sobre la demografía del reclamante, la participación del abogado y la pérdida económica (LOSS, en miles), entre otras variables. Aquí analizamos una muestra de \\(n=1,340\\) pérdidas de un solo estado. El estudio completo de 2002 contiene más de 70,000 reclamaciones cerradas basadas en datos de treinta y dos aseguradoras. El IRC realizó estudios similares en 1977, 1987, 1992 y 1997. Calcula las estadísticas descriptivas para la pérdida económica total (LOSS). ¿Cuál es la pérdida típica? Calcula un histograma y un gráfico \\(qq\\) (normal) para LOSS. Comenta sobre la forma de la distribución. Divide el conjunto de datos en dos submuestras, una correspondiente a las reclamaciones que involucraron a un ATTORNEY (=1) y otra en la que no se involucró un ATTORNEY (=2). c(i). Para cada submuestra, calcula la pérdida típica. ¿Parece haber una diferencia en las pérdidas típicas según la participación del abogado? c(ii) Para comparar las distribuciones, calcula un diagrama de caja (boxplot) por nivel de participación del abogado. c(iii). Para cada submuestra, calcula un histograma y un gráfico \\(qq\\). Compara las dos distribuciones entre sí. 1.6 Gastos de la Compañía de Seguros. Al igual que otros negocios, las compañías de seguros buscan minimizar los gastos asociados con la realización de negocios para mejorar la rentabilidad. Para estudiar los gastos, este ejercicio examina una muestra aleatoria de 500 compañías de seguros de la base de datos de la National Association of Insurance Commissioners (NAIC) de más de 3,000 compañías. La NAIC mantiene una de las bases de datos regulatorias de seguros más grandes del mundo; consideramos aquí datos basados en los informes anuales de 2005 para todas las compañías de seguros de propiedad y casualty en los Estados Unidos. Los informes anuales son estados financieros que utilizan principios contables estatutarios. Específicamente, nuestra variable dependiente es EXPENSES, los gastos no relacionados con reclamaciones para una compañía. Aunque no es necesario para este ejercicio, los gastos no relacionados con reclamaciones se basan en tres componentes: ajuste de pérdidas no asignadas, gastos de suscripción y gastos de inversión. El gasto por ajuste de pérdidas no asignadas es el gasto no directamente atribuible a una reclamación pero que está asociado indirectamente con la resolución de reclamaciones; incluye elementos como los salarios de los ajustadores de reclamaciones, honorarios legales, costos judiciales, testigos expertos y costos de investigación. Los gastos de suscripción consisten en costos de adquisición de pólizas, como comisiones, así como la parte de los gastos administrativos, generales y otros atribuibles a las operaciones de suscripción. Los gastos de inversión son aquellos gastos relacionados con las actividades de inversión del asegurador. Examina la distribución de la variable dependiente, EXPENSES. Haz esto creando un histograma y luego un gráfico \\(qq\\), comparando el empírico con una distribución normal. Realiza una transformación logarítmica natural y examina la distribución de esta variable transformada. ¿Ha ayudado la transformación a simetrizar la distribución? 1.7 Esperanzas de Vida Nacionales. ¿Quién está haciendo bien el cuidado de la salud? Las decisiones sobre la atención médica se toman a nivel individual, corporativo y gubernamental. Prácticamente todas las personas, corporaciones y gobiernos tienen su propia perspectiva sobre la atención médica; estas diferentes perspectivas dan lugar a una amplia variedad de sistemas para gestionar la atención médica. Comparar diferentes sistemas de atención médica nos ayuda a aprender sobre enfoques distintos al nuestro, lo que a su vez nos ayuda a tomar mejores decisiones al diseñar sistemas mejorados. Aquí, consideramos los sistemas de atención médica de \\(n=185\\) países en todo el mundo. Como medida de la calidad de la atención, utilizamos LIFEEXP, la esperanza de vida al nacer. Esta variable dependiente, con varias variables explicativas, se enumeran en [Tabla 1.4]. A partir de esta tabla, notarás que aunque hay 185 países considerados en este estudio, no todos los países proporcionaron información para cada variable. Los datos no disponibles se indican en la columna Num Miss. Los datos provienen del Informe sobre el Desarrollo Humano de las Naciones Unidas (UN). Examina la distribución de la variable dependiente, LIFEEXP. Haz esto creando un histograma y luego un gráfico \\(qq\\), comparando el empírico con una distribución normal. Realiza una transformación logarítmica natural y examina la distribución de esta variable transformada. ¿Ha ayudado la transformación a simetrizar la distribución? \\[ \\small{ \\begin{array}{ll|crrrrr} \\hline &amp; &amp; Num &amp; &amp; &amp; Std &amp; Mini- &amp; Maxi- \\\\ Variable &amp; Description &amp; Miss &amp; Mean &amp; Median &amp; Dev &amp; mum &amp; mum \\\\\\hline BIRTH &amp; \\text{ Births attended by skilled} &amp; 7 &amp; 78.25 &amp; 92.00 &amp; 26.42 &amp; 6.00 &amp; 100.00 \\\\ ~~ATTEND&amp; ~~ \\text{ health personnel} (\\%)\\\\ FEMALE &amp; \\text{ Legislators, senior officials} &amp; 87 &amp; 29.07 &amp; 30.00 &amp; 11.71 &amp; 2.00 &amp; 58.00 \\\\ ~~BOSS&amp; ~~ \\text{ and managers, % female }\\\\ FERTILITY &amp; \\text{ Total fertility rate,}&amp; 4 &amp; 3.19 &amp; 2.70 &amp; 1.71 &amp; 0.90 &amp; 7.50 \\\\ &amp; ~~ \\text{ births per woman }&amp; \\\\ GDP &amp; \\text{ Gross domestic product,} &amp; 7 &amp; 247.55 &amp; 14.20 &amp; 1,055.69 &amp; 0.10 &amp; 12,416.50 \\\\ &amp; ~~\\text{ in billions of USD} \\\\ HEALTH&amp; \\text{ 2004 Health expenditure} &amp; 5 &amp; 718.01 &amp; 297.50 &amp; 1,037.01 &amp; 15.00 &amp; 6,096.00 \\\\ ~~ EXPEND &amp; ~~ \\text{ per capita, PPP in USD} \\\\ ILLITERATE &amp; \\text{ Adult illiteracy rate,} &amp; 14 &amp; 17.69 &amp; 10.10 &amp; 19.86 &amp; 0.20 &amp; 76.40 \\\\ &amp; ~~ \\% \\text{ aged 15 and older} &amp; \\\\ PHYSICIAN &amp; \\text{ Physicians,}&amp; 3 &amp; 146.08 &amp; 107.50 &amp; 138.55 &amp; 2.00 &amp; 591.00 \\\\ &amp; ~~ \\text{ per 100,000 people} \\\\ POP &amp; \\text{ 2005 population,} &amp; 1 &amp; 35.36 &amp; 7.80 &amp; 131.70 &amp; 0.10 &amp; 1,313.00 \\\\ &amp; ~~\\text{ in millions }\\\\ PRIVATE &amp; \\text{ 2004 Private expenditure} &amp; 1 &amp; 2.52 &amp; 2.40 &amp; 1.33 &amp; 0.30 &amp; 8.50 \\\\ ~~HEALTH&amp; ~~\\text{on health, % of GDP} \\\\ PUBLIC &amp; \\text{ Public expenditure} &amp; 28 &amp; 4.69 &amp; 4.60 &amp; 2.05 &amp; 0.60 &amp; 13.40 \\\\ ~~EDUCATION&amp; ~~ \\text{ on education, % of GDP} \\\\ RESEAR &amp; \\text{ Researchers in R &amp; D,} &amp; 95 &amp; 2,034.66 &amp; 848.00 &amp; 4,942.93 &amp; 15.00 &amp; 45,454.00 \\\\ ~~CHERS&amp;~~ \\text{ per million people} &amp; \\\\ SMOKING &amp; \\text{ Prevalence of smoking,} &amp; 88 &amp; 35.09 &amp; 32.00 &amp; 14.40 &amp; 6.00 &amp; 68.00 \\\\ &amp; ~~\\text{ (male) % of adults } \\\\ \\hline LIFEEXP &amp; \\text{ Life expectancy at birth,}&amp; &amp; 67.05 &amp; 71.00 &amp; 11.08 &amp; 40.50 &amp; 82.30 \\\\ &amp; ~~ \\text{ in years } \\\\ \\hline \\end{array} } \\] 1.9 Suplemento Técnico - Teorema del Límite Central Los teoremas del límite central forman la base para gran parte de la inferencia estadística utilizada en el análisis de regresión. Por lo tanto, es útil proporcionar una declaración explícita de una versión del teorema del límite central. Teorema del Límite Central. Supongamos que \\(y_1, \\ldots, y_n\\) están distribuidos de manera independiente con media \\(\\mu\\), varianza finita \\(\\sigma^2\\) y \\(\\mathrm{E}|y|^3\\) es finito. Entonces, \\[ \\lim_{n \\rightarrow \\infty} \\Pr \\left( \\frac{\\sqrt{n}}{\\sigma }(\\overline{y} - \\mu ) \\leq x \\right) = \\Phi \\left( x \\right), \\] para cada \\(x\\), donde \\(\\Phi \\left( \\cdot \\right)\\) es la función de distribución normal estándar. Bajo las suposiciones de este teorema, la distribución reescalada de \\(\\overline{y}\\) se aproxima a una distribución normal estándar a medida que aumenta el tamaño de la muestra, \\(n\\). Interpretamos esto como que, para tamaños de muestra ‘grandes’, la distribución de \\(\\overline{y}\\) puede aproximarse mediante una distribución normal. Investigaciones empíricas han mostrado que tamaños de muestra de \\(n = 25\\) a 50 proporcionan aproximaciones adecuadas para la mayoría de los propósitos. ¿Cuándo no funciona bien el teorema del límite central? Algunos conocimientos se proporcionan mediante otro resultado de la estadística matemática. Aproximación de Edgeworth. Supongamos que \\(y_1, \\ldots, y_n\\) están distribuidos idéntica e independientemente con media \\(\\mu\\), varianza finita \\(\\sigma^2\\) y \\(\\mathrm{E}|y|^3\\) es finito. Entonces, \\[ \\Pr \\left( \\frac{\\sqrt{n}}{\\sigma }(\\overline{y} - \\mu ) \\leq x \\right) = \\Phi \\left( x \\right) + \\frac{1}{6}\\frac{1}{\\sqrt{2\\pi }} e^{-x^2/2} \\frac{\\mathrm{E}(y - \\mu)^3}{\\sigma^3 \\sqrt{n}} + \\frac{h_n}{\\sqrt{n}}, \\] para cada \\(x\\), donde \\(h_n \\rightarrow 0\\) a medida que \\(n \\rightarrow \\infty\\). Este resultado sugiere que la distribución de \\(\\bar{y}\\) se acerca más a una distribución normal a medida que la asimetría, \\(\\mathrm{E}(\\overline{y} - \\mu)^3\\), se acerca a cero. Esto es importante en aplicaciones de seguros porque muchas distribuciones tienden a ser asimétricas. Históricamente, los analistas utilizaban el segundo término en el lado derecho del resultado para proporcionar una ‘corrección’ para la aproximación de la curva normal. Véase, por ejemplo, Beard, Pentikäinen y Pesonen (1984) para una discusión adicional sobre las aproximaciones de Edgeworth en la ciencia actuarial. Una alternativa (utilizada en este libro) que vimos en la Sección 1.3 es transformar los datos, logrando así una aproximada simetría. Como sugiere el teorema de aproximación de Edgeworth, si nuestra población parental es cercana a simétrica, entonces la distribución de \\(\\overline{y}\\) será aproximadamente normal. "],["C2BasicLR.html", "Chapter 2 Regresión Lineal Básica 2.1 Correlaciones y Mínimos Cuadrados 2.2 Modelo Básico de Regresión Lineal 2.3 ¿Es Útil el Modelo? Algunas Medidas de Resumen Básicas 2.4 Propiedades de los Estimadores del Coeficiente de Regresión 2.5 Inferencia Estadística 2.6 Construyendo un Mejor Modelo: Análisis de Residuos 2.7 Aplicación: Modelo de Valoración de Activos Financieros 2.8 Salida Computacional Ilustrativa de Regresión 2.9 Lecturas Adicionales y Referencias 2.10 Ejercicios 2.11 Suplemento Técnico - Elementos del Álgebra de Matrices", " Chapter 2 Regresión Lineal Básica Vista previa del capítulo. Este capítulo considera la regresión en el caso de tener solo una variable explicativa. A pesar de esta aparente simplicidad, la mayoría de las ideas profundas de la regresión pueden desarrollarse en este marco. Al limitarnos al caso de una variable, podemos expresar muchos cálculos usando álgebra simple. Esto nos permitirá desarrollar nuestra intuición sobre las técnicas de regresión al reforzarla con demostraciones simples. Además, podemos ilustrar las relaciones entre dos variables gráficamente porque estamos trabajando en solo dos dimensiones. Las herramientas gráficas resultan ser importantes para desarrollar un vínculo entre los datos y un modelo. 2.1 Correlaciones y Mínimos Cuadrados La regresión trata sobre relaciones. Específicamente, estudiaremos cómo dos variables, una \\(x\\) y una \\(y\\), están relacionadas. Queremos poder responder preguntas como, si cambiamos el nivel de \\(x\\), ¿qué pasará con el nivel de \\(y\\)? Si comparamos dos “sujetos” que parecen similares excepto por la medición de \\(x\\), ¿cómo diferirán sus mediciones de \\(y\\)? Entender las relaciones entre variables es fundamental para la gestión cuantitativa, particularmente en ciencias actuariales donde la incertidumbre es tan prevalente. Es útil trabajar con un ejemplo específico para familiarizarnos con conceptos clave. El análisis de ventas de lotería no ha sido parte de la práctica actuarial tradicional, pero es un área de crecimiento en la que los actuarios podrían contribuir. Ejemplo: Ventas de la Lotería de Wisconsin. Los administradores de la lotería del estado de Wisconsin están interesados en evaluar los factores que afectan las ventas de lotería. Las ventas consisten en boletos de lotería en línea que se venden en establecimientos minoristas seleccionados en Wisconsin. Estos boletos generalmente tienen un precio de $1.00, por lo que el número de boletos vendidos equivale a los ingresos de la lotería. Analizamos las ventas promedio de lotería (SALES) durante un período de cuarenta semanas, de abril de 1998 a enero de 1999, en cincuenta áreas seleccionadas al azar identificadas por código postal (ZIP) dentro del estado de Wisconsin. Aunque muchas variables económicas y demográficas podrían influir en las ventas, nuestro primer análisis se centra en la población (POP) como un determinante clave. El Capítulo 3 mostrará cómo considerar variables explicativas adicionales. Intuitivamente, parece claro que las áreas geográficas con más personas tendrán mayores ventas. Entonces, otras cosas siendo iguales, un \\(x=POP\\) más grande significa un \\(y=SALES\\) más grande. Sin embargo, la lotería es una fuente importante de ingresos para el estado y queremos ser lo más precisos posible. Una notación adicional será útil posteriormente. En esta muestra, hay cincuenta áreas geográficas y usamos subíndices para identificar cada área. Por ejemplo, \\(y_1\\) = 1,285.4 representa las ventas para la primera área en la muestra que tiene una población de \\(x_1\\) = 435. Llamamos al par ordenado (\\(x_1\\), \\(y_1\\)) = (435, 1285.4) la primera observación. Extendiendo esta notación, la muestra completa que contiene cincuenta observaciones puede representarse por (\\(x_1\\), \\(y_1\\)), …, (\\(x_{50}\\), \\(y_{50}\\)). Los puntos suspensivos ( … ) significan que el patrón continúa hasta que se encuentra el último objeto. A menudo hablaremos de un miembro genérico de la muestra, refiriéndonos a (\\(x_i\\), \\(y_i\\)) como la \\(i\\)-ésima observación. Los conjuntos de datos pueden complicarse, por lo que será útil si comienza trabajando con cada variable por separado. Los dos paneles en la Figura 2.1 muestran histogramas que dan una impresión visual rápida de la distribución de cada variable de forma aislada. La Tabla 2.1 proporciona resúmenes numéricos correspondientes. Para ilustrar, para la variable población (POP), vemos que el área con el menor número contenía 280 personas, mientras que la más grande contenía 39,098. El promedio, sobre 50 códigos postales, fue de 9,311.04. Para nuestra segunda variable, las ventas fueron tan bajas como 189 y tan altas como 33,181. Figure 2.1: Histogramas de Población y Ventas. Cada distribución está sesgada a la derecha, lo que indica que hay muchas áreas pequeñas en comparación con unas pocas áreas con mayores ventas y poblaciones. Table 2.1: Estadísticas Resumen de Cada Variable Promedio Mediana Desviación Estándar Mínimo Máximo POP 9,311 4,406 11,098 280 39,098 SALES 6,495 2,426 8,103 189 33,181 Fuente: Frees y Miller (2003) Código R para producir la Figura 2.1 y la Tabla 2.1 Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURA 2.1 par(mfrow=c(1, 2), cex=1.3, mar=c(4.1,3.1,1.2,1)) hist(Lot$POP, main=&quot;&quot;, ylab=&quot;&quot;, las=1, xlab = &quot;POP&quot;) mtext(&quot;Frecuencia&quot;, side=2, at=30, las=1, cex=1.3, adj=.6) hist(Lot$SALES, main=&quot;&quot;, ylab=&quot;&quot;, las=1, xlab = &quot;SALES&quot;) mtext(&quot;Frecuencia&quot;, side=2, at=34, las=1, cex=1.3, adj=.6) # TABLA 2.1 ESTADÍSTICAS RESUMEN BookSummStats &lt;- function(Xymat){ meanSummary &lt;- sapply(Xymat, mean, na.rm=TRUE) sdSummary &lt;- sapply(Xymat, sd, na.rm=TRUE) minSummary &lt;- sapply(Xymat, min, na.rm=TRUE) maxSummary &lt;- sapply(Xymat, max, na.rm=TRUE) medSummary &lt;- sapply(Xymat, median,na.rm=TRUE) tableMat &lt;- cbind(meanSummary, medSummary, sdSummary, minSummary, maxSummary) return(tableMat) } Xymat &lt;- data.frame(cbind(Lot$POP,Lot$SALES)) tableMat &lt;- BookSummStats(Xymat) colnames(tableMat) &lt;- c(&quot;Promedio&quot; , &quot;Mediana&quot; , &quot;Desviación Estándar&quot; , &quot;Mínimo&quot; , &quot;Máximo&quot;) rownames(tableMat) &lt;- c(&quot;POP&quot;, &quot;SALES&quot;) tableMat1 &lt;- format(round(tableMat, digits=0), big.mark = &#39;,&#39;) TableGen1(TableData=tableMat1, TextTitle=&#39;Estadísticas Resumen de Cada Variable&#39;, Align=&#39;r&#39;, Digits=0, ColumnSpec=1:5, ColWidth = ColWidth5) %&gt;% footnote(general = &quot;Frees y Miller (2003)&quot;, general_title = &quot;Fuente:&quot;, footnote_as_chunk = TRUE) Como muestra la Tabla 2.1, las estadísticas resumen básicas dan ideas útiles de la estructura de las características clave de los datos. Después de entender la información en cada variable de forma aislada, podemos comenzar a explorar la relación entre las dos variables. Gráfico de Dispersión y Coeficientes de Correlación - Herramientas Básicas de Resumen La herramienta gráfica básica utilizada para investigar la relación entre dos variables es un gráfico de dispersión, como se muestra en la Figura 2.2. Aunque podemos perder los valores exactos de las observaciones al graficar los datos, ganamos una impresión visual de la relación entre la población y las ventas. En la Figura 2.2 vemos que las áreas con poblaciones más grandes tienden a comprar más boletos de lotería. ¿Qué tan fuerte es esta relación? ¿Puede el conocimiento de la población del área ayudarnos a anticipar los ingresos por ventas de lotería? Exploramos estas dos preguntas a continuación. Figure 2.2: Un gráfico de dispersión de los datos de la lotería. Cada uno de los 50 símbolos de la gráfica corresponde a un código postal en el estudio. Esta figura sugiere que las áreas postales con poblaciones más grandes tienen mayores ingresos de lotería. Código R para Producir la Figura 2.2 #Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURA 2.2, CON CORRELACIONES par(mar=c(4.1,3.8,2,1),cex=1.1) plot(Lot$POP, Lot$SALES, ylab=&quot;&quot;, las=1, xlab = &quot;POP&quot;) mtext(&quot;SALES&quot;,side=2, at=36000, las=1, cex=1.1) Una forma de resumir la fuerza de la relación entre dos variables es a través de una estadística de correlación. Definición. El coeficiente de correlación ordinario, o de Pearson se define como \\[\\begin{equation*} r=\\frac{1}{(n-1)s_xs_y}\\sum_{i=1}^{n}\\left( x_{i}-\\overline{x}\\right) \\left( y_{i}-\\overline{y}\\right) . \\end{equation*}\\] Aquí, usamos la desviación estándar de la muestra \\(s_y = \\sqrt{(n-1)^{-1} \\sum_{i=1}^{n}\\left( y_i - \\overline{y}\\right)^{2}}\\) definida en la Sección 1.2, con una notación similar para \\(s_x\\). Aunque existen otras estadísticas de correlación, el coeficiente de correlación ideado por Pearson (1895) tiene varias propiedades deseables. Una propiedad importante es que, para cualquier conjunto de datos, \\(r\\) está acotado entre -1 y 1, es decir, \\(-1\\leq r\\leq 1\\). (El Ejercicio 2.3 proporciona pasos para comprobar esta propiedad.) Si \\(r\\) es mayor que cero, se dice que las variables están correlacionadas positivamente. Si \\(r\\) es menor que cero, se dice que las variables están correlacionadas negativamente. Cuanto mayor sea el coeficiente en valor absoluto, más fuerte será la relación. De hecho, si \\(r=1\\), entonces las variables están perfectamente correlacionadas. En este caso, todos los datos se encuentran en una línea recta que pasa por los cuadrantes inferior izquierdo y superior derecho. Si \\(r=-1\\), entonces todos los datos se encuentran en una línea que pasa por los cuadrantes superior izquierdo e inferior derecho. El coeficiente \\(r\\) es una medida de una relación lineal entre dos variables. Se dice que el coeficiente de correlación es invariante a la ubicación y la escala. Así, el centro de ubicación de cada variable no importa en el cálculo de \\(r\\). Por ejemplo, si agregamos $100 a las ventas de cada código postal, cada \\(y_i\\) aumentará en 100. Sin embargo, \\(\\overline{y}\\), el precio de compra promedio, también aumentará en 100 de modo que la desviación \\(y_i - \\overline{y}\\) permanece sin cambios, o invariante. Además, la escala de cada variable no importa en el cálculo de \\(r\\). Por ejemplo, supongamos que dividimos cada población entre 1000, de modo que \\(x_i\\) ahora representa la población en miles. Así, \\(\\overline{x}\\) también se divide entre 1000 y usted debería verificar que \\(s_x\\) también se divide entre 1000. Así, la versión estandarizada de \\(x_i\\), \\(\\left( x_i-\\overline{x}\\right) /s_x\\), permanece sin cambios, o invariante. Muchos paquetes estadísticos calculan una versión estandarizada de una variable restando el promedio y dividiendo por la desviación estándar. Ahora, usemos \\(y_{i,std}=\\left( y_i- \\overline{y}\\right) /s_y\\) y \\(x_{i,std}=\\left( x_i-\\overline{x} \\right) /s_x\\) para que sean las versiones estandarizadas de \\(y_i\\) y \\(x_i\\), respectivamente. Con esta notación, podemos expresar el coeficiente de correlación como \\(r=(n-1)^{-1}\\sum_{i=1}^{n}x_{i,std}\\times y_{i,std}.\\) Se dice que el coeficiente de correlación es una medida adimensional. Esto se debe a que hemos eliminado dólares, y todas las demás unidades de medida, considerando las variables estandarizadas \\(x_{i,std}\\) y \\(y_{i,std}\\). Debido a que el coeficiente de correlación no depende de las unidades de medida, es una estadística que puede compararse fácilmente entre diferentes conjuntos de datos. En el mundo de los negocios, el término “correlación” se usa a menudo como sinónimo del término “relación.” Para los propósitos de este texto, utilizamos el término correlación cuando nos referimos únicamente a relaciones lineales. La relación no lineal clásica es \\(y=x^{2}\\), una relación cuadrática. Considere esta relación y el conjunto de datos ficticios para \\(x\\), \\(\\{-2,1,0,1,2\\}\\). Ahora, como ejercicio (2.2), produzca un gráfico aproximado del conjunto de datos: \\[ \\begin{array}{l|rrrrr} \\hline i &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\ \\hline x_i &amp; -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 \\\\ y_i &amp; 4 &amp; 1 &amp; 0 &amp; 1 &amp; 4 \\\\ \\hline \\end{array} \\] El coeficiente de correlación para este conjunto de datos resulta ser \\(r=0\\) (verifíquelo). Por lo tanto, a pesar de que hay una relación perfecta entre \\(x\\) y \\(y\\) (\\(=x^{2}\\)), hay una correlación cero. Recuerde que los cambios de ubicación y escala no son relevantes en las discusiones sobre correlación, por lo que podríamos cambiar fácilmente los valores de \\(x\\) y \\(y\\) para que sean más representativos de un conjunto de datos de negocios. ¿Qué tan fuerte es la relación entre \\(y\\) y \\(x\\) para los datos de la lotería? Gráficamente, la respuesta es un gráfico de dispersión, como en la Figura 2.2. Numéricamente, la respuesta principal es el coeficiente de correlación, que resulta ser \\(r\\) = 0.886 para este conjunto de datos. Interpretamos esta estadística diciendo que SALES y POP están correlacionados (positivamente). La fuerza de la relación es fuerte porque \\(r\\) = 0.886 está cerca de uno. En resumen, podemos describir esta relación diciendo que hay una fuerte correlación entre SALES y POP. Método de Mínimos Cuadrados Ahora comenzamos a explorar la pregunta: “¿Puede el conocimiento de la población ayudarnos a entender las ventas?” Para responder a esta pregunta, identificamos las ventas como la variable de respuesta, o dependiente. La variable de población, que se usa para ayudar a entender las ventas, se llama la variable explicativa, o independiente. Supongamos que tenemos disponibles los datos de muestra de cincuenta ventas \\(\\{y_1, \\ldots, y_{50} \\}\\) y tu trabajo es predecir las ventas de un código postal seleccionado al azar. Sin conocimiento de la variable de población, un predictor sensato es simplemente \\(\\overline{y}=6,495\\), el promedio de la muestra disponible. Naturalmente, anticipas que las áreas con mayores poblaciones tendrán mayores ventas. Es decir, si también tienes conocimiento de la población, ¿puede mejorarse esta estimación? Si es así, ¿cuánto? Para responder a estas preguntas, el primer paso asume una relación lineal aproximada entre \\(x\\) y \\(y\\). Para ajustar una línea a nuestro conjunto de datos, usamos el método de mínimos cuadrados. Necesitamos una técnica general para que, si diferentes analistas están de acuerdo en los datos y en la técnica de ajuste, entonces estarán de acuerdo en la línea. Si diferentes analistas ajustan un conjunto de datos usando aproximaciones a ojo, en general llegarán a diferentes líneas, incluso usando el mismo conjunto de datos. El método comienza con la línea \\(y=b_0^{\\ast}+b_1^{\\ast}x\\), donde la intersección y la pendiente, \\(b_0^{\\ast}\\) y \\(b_1^{\\ast}\\), son meramente valores genéricos. Para la \\(i\\)-ésima observación, \\(y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right)\\) representa la desviación del valor observado \\(y_i\\) de la línea en \\(x_i\\). La cantidad \\[\\begin{equation*} SS(b_0^{\\ast},b_1^{\\ast})=\\sum_{i=1}^{n}\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) ^{2} \\end{equation*}\\] representa la suma de desviaciones cuadradas para esta línea candidata. El método de mínimos cuadrados consiste en determinar los valores de \\(b_0^{\\ast}\\) y \\(b_1^{\\ast}\\) que minimizan \\(SS(b_0^{\\ast},b_1^{\\ast})\\). Este es un problema fácil que puede resolverse mediante cálculo, de la siguiente manera. Tomando derivadas parciales con respecto a cada argumento obtenemos \\[\\begin{equation*} \\frac{\\partial }{\\partial b_0^{\\ast}}SS(b_0^{\\ast},b_1^{\\ast})=\\sum_{i=1}^{n}(-2)\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) \\end{equation*}\\] y \\[\\begin{equation*} \\frac{\\partial }{\\partial b_1^{\\ast}}SS(b_0^{\\ast},b_1^{\\ast})=\\sum_{i=1}^{n}(-2x_i)\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) . \\end{equation*}\\] Se invita al lector a tomar las segundas derivadas parciales para asegurarse de que estamos minimizando, no maximizando, esta función. Igualando estas cantidades a cero y cancelando términos constantes obtenemos \\[\\begin{equation*} \\sum_{i=1}^{n}\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) =0 \\end{equation*}\\] y \\[\\begin{equation*} \\sum_{i=1}^{n}x_i\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) =0, \\end{equation*}\\] que son conocidas como las ecuaciones normales. Resolver estas ecuaciones proporciona los valores de \\(b_0^{\\ast}\\) y \\(b_1^{\\ast}\\) que minimizan la suma de cuadrados. Definición. Las estimaciones de intersección y pendiente de mínimos cuadrados son \\[\\begin{equation*} b_1=r\\frac{s_y}{s_x}~~~~~\\mathrm{y}~~~~~b_0=\\overline{y}-b_1 \\overline{x}. \\end{equation*}\\] La línea que determinan, \\(\\widehat{y}=b_0+b_1x\\), se llama la línea de regresión ajustada. Hemos eliminado la notación de asterisco, o estrella, porque \\(b_0\\) y \\(b_1\\) ya no son valores “candidatos”. ¿Proporciona este procedimiento una línea sensata para nuestras ventas de lotería de Wisconsin? Anteriormente, calculamos \\(r=0.886\\). A partir de esto y de las estadísticas básicas resumidas en la Tabla 2.1, tenemos \\(b_1 = 0.886 \\left( 8,103\\right) /11,098=0.647\\) y \\(b_0 = 6,495-(0.647)9,311 = 469.7\\). Esto produce la línea de regresión ajustada \\[\\begin{equation*} \\widehat{y} = 469.7 + (0.647)x. \\end{equation*}\\] El sombrero, o “gorro”, encima de la \\(y\\) nos recuerda que esta \\(\\widehat{y}\\), o \\(\\widehat{SALES}\\), es un valor ajustado. Una aplicación de la línea de regresión es estimar ventas para una población específica, digamos, \\(x=10,000\\). La estimación es la altura de la línea de regresión, que es \\(469.7 + (0.647)(10,000) = 6,939.7\\). Ejemplo: Resumiendo Simulaciones. El análisis de regresión es una herramienta para resumir datos complejos. En el trabajo práctico, los actuarios a menudo simulan escenarios financieros complicados; a menudo se pasa por alto que la regresión puede usarse para resumir relaciones de interés. Para ilustrar, Manistre y Hancock (2005) simularon muchas realizaciones de una opción put europea a 10 años y demostraron la relación entre dos medidas de riesgo actuarial, el valor en riesgo (VaR) y la expectativa de cola condicional (CTE). Para un ejemplo, estos autores examinaron rendimientos de acciones distribuidos logarítmicamente con un precio inicial de $100, de modo que en 10 años el precio de la acción estaría distribuido como \\[\\begin{equation*} S(Z)=100 \\exp \\left( (.08) 10 + .15 \\sqrt{10} Z \\right), \\end{equation*}\\] basado en un retorno medio anual del 8%, desviación estándar del 15% y el resultado de una variable aleatoria normal estándar \\(Z\\). La opción put paga la diferencia entre el precio de ejercicio, que se tomará como 110 para este ejemplo, y \\(S(Z)\\). El valor presente de esta opción es \\[\\begin{equation*} C(Z)= \\mathrm{e}^{-0.06(10)} \\mathrm{max} \\left(0, 110-S(Z) \\right), \\end{equation*}\\] basado en una tasa de descuento del 6%. Para estimar el VaR y el CTE, para cada \\(i\\), se simularon 1000 variables aleatorias normales estándar i.i.d. y se usaron para calcular 1000 valores presentes, \\(C_{i1}, \\ldots, C_{i,1000}.\\) El percentil 95 de estos valores presentes es la estimación del valor en riesgo, denotado como \\(VaR_i.\\) El promedio de los 50 valores presentes más altos (\\(= (1-.05) \\times 1000\\)) es la estimación de la expectativa de cola condicional, denotada como \\(CTE_i\\). Manistre y Hancock (2005) realizaron este cálculo \\(i=1, \\ldots, 1000\\) veces; el resultado se presenta en la Figura 2.3. El diagrama de dispersión muestra una relación fuerte pero no perfecta entre el \\(VaR\\) y el \\(CTE\\), el coeficiente de correlación resulta ser \\(r=0.782\\). Figure 2.3: Gráfico de la Expectativa de Cola Condicional (CTE) frente al Valor en Riesgo (VaR). Basado en \\(n=1,000\\) simulaciones de un bono put europeo a 10 años. Fuente: Manistre y Hancock (2005). Código R para producir la Figura 2.3 # FIGURA 2.3 # simulación S &lt;- vector(mode = &quot;numeric&quot;, length = 1000) C &lt;- vector(mode = &quot;numeric&quot;, length = 1000) Var &lt;- vector(mode = &quot;numeric&quot;, length = 1000) CTE &lt;- vector(mode = &quot;numeric&quot;, length = 1000) for (i in 1:1000){ for (j in 1:1000){ S[j] &lt;- 100 * exp(.08 * 10 + .15 * (10 ^ .5) * rnorm(1)) C[j] &lt;- exp(-.06 * 10) * max(0, 110 - S[j]) } C &lt;- sort(C) Var[i] &lt;- C[950] CTE[i] &lt;- mean(C[950:1000]) } model &lt;- lm(CTE ~ Var) b0 &lt;- round(model$coef[1], digits = 3) b1 &lt;- round(model$coef[2], digits = 3) R2 &lt;- round(summary(model)$r.squared, digits = 4) plot(Var, CTE, xlab = expression(paste(&quot;Estimaciones de VaR&quot;)), ylab = expression(paste(&quot;Estimaciones de CTE&quot;)), xlim = c(0, 12), ylim = c(8, 20), xaxs = &quot;i&quot;, yaxs = &quot;i&quot;, pch = 20, cex = 0.4) lines(Var, model$fitted, lwd = .5) abline(h = c(10, 12, 14, 16, 18, 20), col = &quot;grey&quot;) 2.2 Modelo Básico de Regresión Lineal El diagrama de dispersión, el coeficiente de correlación y la línea de regresión ajustada son herramientas útiles para resumir la relación entre dos variables para un conjunto de datos específico. Para inferir relaciones generales, necesitamos modelos para representar los resultados de poblaciones amplias. Este capítulo se centra en un modelo de “regresión lineal básica”. La parte de “regresión lineal” proviene del hecho de que ajustamos una línea a los datos. La parte de “básica” es porque usamos solo una variable explicativa, \\(x\\). Este modelo también se conoce como una “regresión lineal simple”. Este texto evita este lenguaje porque da la falsa impresión de que las ideas e interpretaciones de regresión con una variable explicativa son siempre sencillas. Ahora introducimos dos conjuntos de supuestos del modelo básico, las representaciones “observables” y de “error”. Son equivalentes, pero cada una nos ayudará a medida que extendamos los modelos de regresión más allá de lo básico. \\[ {\\small \\begin{array}{l} \\hline \\hline &amp;\\textbf{Modelo Básico de Regresión Lineal} \\\\ &amp;\\textbf{Supuestos de Muestreo de la Representación Observable} \\\\ \\hline \\text{F1}. &amp; \\mathrm{E}~y_i=\\beta_0 + \\beta_1 x_i . \\\\ \\text{F2}. &amp; \\{x_1,\\ldots ,x_n\\} \\text{son variables no estocásticas}. \\\\ \\text{F3}. &amp; \\mathrm{Var}~y_i=\\sigma ^{2}. \\\\ \\text{F4}. &amp; \\{ y_i\\} \\text{son variables aleatorias independientes}. \\\\ \\hline\\ \\end{array} } \\] La “representación observable” se enfoca en variables que podemos ver (u observar), \\((x_i,y_i)\\). La inferencia sobre la distribución de \\(y\\) es condicional a las variables explicativas observadas, de modo que podemos tratar \\(\\{x_1,\\ldots ,x_n\\}\\) como variables no estocásticas (supuesto F2). Al considerar tipos de mecanismos de muestreo para \\((x_i,y_i)\\), es conveniente pensar en un esquema de muestreo aleatorio estratificado, donde los valores de \\(\\{x_1,\\ldots ,x_n\\}\\) se tratan como los estratos, o grupos. Bajo el muestreo estratificado, para cada valor único de \\(x_i\\), tomamos una muestra aleatoria de una población. Para ilustrar, supongamos que se está extrayendo de una base de datos de empresas para comprender el rendimiento de las acciones (\\(y\\)) y desea estratificar según el tamaño de la empresa. Si la cantidad de activos es una variable continua, entonces podemos imaginar tomar una muestra de tamaño 1 para cada empresa. De esta manera, hipotetizamos una distribución de rendimientos de acciones condicional al tamaño de los activos de la empresa. Digresión: A menudo verá informes que resumen resultados para los “50 mejores gerentes” o las “100 mejores universidades”, medidos por alguna variable de resultado. En aplicaciones de regresión, asegúrese de no seleccionar observaciones basadas en una variable dependiente, como el rendimiento más alto de las acciones, porque esto es estratificar basado en el \\(y\\), no en el \\(x\\). El Capítulo 6 discutirá los procedimientos de muestreo con mayor detalle. El muestreo estratificado también proporciona motivación para el supuesto F4, la independencia entre respuestas. Se puede motivar el supuesto F1 pensando en \\((x_i,y_i)\\) como una extracción de una población, donde la media de la distribución condicional de \\(y_i\\) dado {\\(x_i\\)} es lineal en la variable explicativa. El supuesto F3 se conoce como homocedasticidad, que discutiremos ampliamente en la Sección 5.7. Ver Goldberger (1991) para más información sobre esta representación. Un quinto supuesto que a menudo se usa implícitamente es: \\[ \\text{F5}. \\{y_i\\} \\text{ están distribuidos normalmente}. \\] Este supuesto no es necesario para muchos procedimientos de inferencia estadística porque los teoremas del límite central proporcionan normalidad aproximada para muchas estadísticas de interés. Sin embargo, la justificación formal para algunas, como las estadísticas \\(t\\), requieren este supuesto adicional. En contraste con la representación observable, un conjunto alternativo de supuestos se enfoca en las desviaciones, o “errores”, en la regresión, definidos como \\(\\varepsilon_i=y_i-\\left( \\beta_0 + \\beta_1 x_i \\right)\\). \\[ {\\small \\begin{array}{l} \\hline \\hline &amp;\\textbf{Modelo Básico de Regresión Lineal} \\\\ &amp;\\textbf{Supuestos de Muestreo de la Representación de Error} \\\\ \\hline \\text{E1}. &amp; y_i=\\beta_0 + \\beta_1 x_i + \\varepsilon_i . \\\\ \\text{E2}. &amp; \\{x_1,\\ldots ,x_n\\} \\text{ son variables no estocásticas}. \\\\ \\text{E3}. &amp; \\mathrm{E}~\\varepsilon _i=0 \\text{ y } \\mathrm{Var}~\\varepsilon _i=\\sigma ^{2}. \\\\ \\text{E4}. &amp; \\{ \\varepsilon_i\\} \\text{ son variables aleatorias independientes}. \\\\ \\hline\\ \\end{array} } \\] La “representación de error” se basa en la teoría gaussiana de errores (ver Stigler, 1986, para un contexto histórico). El supuesto E1 asume que \\(y\\) es en parte debido a una función lineal de la variable explicativa observada, \\(x\\). Otras variables no observadas que influyen en la medición de \\(y\\) se interpretan como incluidas en el término de “error” \\(\\varepsilon _i\\), que también se conoce como el término de “perturbación”. La independencia de errores, E4, puede motivarse asumiendo que {\\(\\varepsilon _i\\)} se realizan a través de una muestra aleatoria simple de una población desconocida de errores. Los supuestos E1-E4 son equivalentes a F1-F4. La representación de error proporciona una base útil para motivar las medidas de ajuste (Sección 2.3). Sin embargo, una desventaja de la representación de error es que desvía la atención de las cantidades observables \\((x_i,y_i)\\) a una cantidad no observable, {\\(\\varepsilon _i\\)}. Para ilustrar, la base de muestreo, ver {\\(\\varepsilon _i\\)} como una muestra aleatoria simple, no es directamente verificable porque no se puede observar directamente la muestra {\\(\\varepsilon _i\\)}. Además, el supuesto de errores aditivos en E1 será problemático cuando consideremos modelos de regresión no lineales. La Figura 2.4 ilustra algunos de los supuestos del modelo básico de regresión lineal. Los datos (\\(x_1,y_1\\)), (\\(x_2,y_2\\)) y (\\(x_3,y_3\\)) son observados y se representan con los símbolos de trazado circulares opacos. Según el modelo, estas observaciones deben estar cerca de la línea de regresión \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x\\). Cada desviación de la línea es aleatoria. A menudo asumimos que la distribución de desviaciones puede representarse por una curva normal, como en la Figura 2.4. Figure 2.4: La distribución de la respuesta varía según el nivel de la variable explicativa. Los supuestos del modelo básico de regresión lineal describen la población subyacente. Tabla 2.2 destaca la idea de que las características de esta población pueden resumirse mediante los parámetros \\(\\beta_0\\), \\(\\beta_1\\) y \\(\\sigma ^{2}\\). En la Sección 2.1, resumimos datos de una muestra, introduciendo las estadísticas \\(b_0\\) y \\(b_1\\). La Sección 2.3 introducirá \\(s^{2}\\), la estadística correspondiente al parámetro \\(\\sigma ^{2}\\). Tabla 2.2. Medidas Resumen de la Población y la Muestra \\[ {\\small \\begin{array}{llccc}\\hline\\hline &amp; \\text{Resumen} \\\\ \\text{Datos} &amp; \\text{Medidas} &amp; \\text{Intercepto} &amp; \\text{Pendiente} &amp; \\text{Varianza} \\\\\\hline \\text{Población} &amp; \\text{Parámetros} &amp; \\beta_0 &amp; \\beta_1 &amp; \\sigma^2 \\\\ \\text{Muestra} &amp; \\text{Estadísticas} &amp; b_0 &amp; b_1 &amp; s^2 \\\\ \\hline \\end{array} } \\] 2.3 ¿Es Útil el Modelo? Algunas Medidas de Resumen Básicas Aunque la estadística es la ciencia de resumir datos, también es el arte de argumentar con datos. Esta sección desarrolla algunas de las herramientas básicas usadas para justificar el modelo de regresión lineal básica. Un diagrama de dispersión puede proporcionar una fuerte evidencia visual de que \\(x\\) influye en \\(y\\); desarrollar evidencia numérica nos permitirá cuantificar la fuerza de la relación. Además, la evidencia numérica será útil cuando consideremos otros conjuntos de datos donde la evidencia gráfica no sea convincente. 2.3.1 Particionando la Variabilidad Las desviaciones cuadradas, \\(\\left( y_i-\\overline{y}\\right) ^2\\), proporcionan una base para medir la dispersión de los datos. Si deseamos estimar la \\(i\\)-ésima variable dependiente sin conocimiento de \\(x\\), entonces \\(\\overline{y}\\) es una estimación adecuada y \\(y_i- \\overline{y}\\) representa la desviación de la estimación. Usamos \\(Total~SS=\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^2\\), la suma total de cuadrados, para representar la variación en todas las respuestas. Supongamos ahora que también tenemos conocimiento de \\(x\\), una variable explicativa. Usando la línea de regresión ajustada, para cada observación podemos calcular el valor ajustado correspondiente, \\(\\widehat{y}_i = b_0 + b_1x_i\\). El valor ajustado es nuestra estimación con conocimiento de la variable explicativa. Como antes, la diferencia entre la respuesta y el valor ajustado, \\(y_i- \\widehat{y}_i\\), representa la desviación de esta estimación. Ahora tenemos dos “estimaciones” de \\(y_i\\), que son \\(\\widehat{y}_i\\) y \\(\\overline{y}\\). Presumiblemente, si la línea de regresión es útil, entonces \\(\\widehat{y}_i\\) es una medida más precisa que \\(\\overline{y}\\). Para juzgar esta utilidad, descomponemos algebraicamente la desviación total como: \\[\\begin{equation} {\\small \\begin{array}{ccccc} \\underbrace{y_i-\\overline{y}} &amp; = &amp; \\underbrace{y_i-\\widehat{y}_i} &amp; + &amp; \\underbrace{\\widehat{y}_i-\\overline{y}} \\\\ \\text{desviación} &amp; = &amp; \\text{desviación} &amp; + &amp; \\text{desviación} \\\\ \\text{total} &amp; &amp; \\text{no explicada} &amp; &amp; \\text{explicada} \\\\ \\end{array} \\tag{2.1} } \\end{equation}\\] Interpreta esta ecuación como “la desviación sin conocimiento de \\(x\\) es igual a la desviación con conocimiento de \\(x\\) más la desviación explicada por \\(x\\).” La Figura 2.5 es una representación geométrica de esta descomposición. En la figura, se eligió una observación por encima de la línea, lo que da una desviación positiva de la línea de regresión ajustada, para hacer que el gráfico sea más fácil de leer. Un buen ejercicio es hacer un boceto aproximado correspondiente a la Figura 2.5 con una observación por debajo de la línea de regresión ajustada. Figure 2.5: Representación geométrica de la descomposición de la desviación. Ahora, a partir de la descomposición algebraica en la ecuación (2.1), eleva al cuadrado cada lado de la ecuación y suma sobre todas las observaciones. Después de un poco de manipulación algebraica, esto da como resultado \\[\\begin{equation} \\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^2=\\sum_{i=1}^{n}\\left( y_i-\\widehat{y}_i\\right) ^2+\\sum_{i=1}^{n}\\left( \\widehat{y}_i- \\overline{y}\\right) ^2. \\tag{2.2} \\end{equation}\\] Reescribimos esto como \\(Total~SS=Error~SS+Regression~SS\\) donde \\(SS\\) significa suma de cuadrados. Interpretamos: \\(Total~SS\\) como la variación total sin conocimiento de \\(x\\), \\(Error~SS\\) como la variación total que queda después de introducir \\(x\\), y \\(Regression~SS\\) como la diferencia entre el \\(Total~SS\\) y el \\(Error~SS\\), o la variación total “explicada” mediante el conocimiento de \\(x\\). Al elevar al cuadrado el lado derecho de la ecuación (2.1), tenemos el término de producto cruzado \\(2\\left(y_i-\\widehat{y}_i\\right) \\left( \\widehat{y}_i-\\overline{y}\\right)\\). Con la “manipulación algebraica”, se puede comprobar que la suma de los productos cruzados sobre todas las observaciones es cero. Este resultado no es cierto para todas las líneas ajustadas, pero es una propiedad especial de la línea ajustada por mínimos cuadrados. En muchos casos, la descomposición de la variabilidad se reporta a través de un solo estadístico. Definición. El coeficiente de determinación se denota por el símbolo \\(R^2\\), llamado “\\(R\\)-cuadrado”, y se define como \\[\\begin{equation*} R^2=\\frac{Regression~SS}{Total~SS}. \\end{equation*}\\] Interpretamos \\(R^2\\) como la proporción de variabilidad explicada por la línea de regresión. En un caso extremo donde la línea de regresión se ajusta perfectamente a los datos, tenemos \\(Error~SS=0\\) y \\(R^2=1\\). En el otro caso extremo donde la línea de regresión no proporciona ninguna información sobre la respuesta, tenemos \\(Regression~SS=0\\) y \\(R^2=0\\). El coeficiente de determinación está limitado por las desigualdades \\(0 \\leq R^2 \\leq 1\\) con valores mayores que implican un mejor ajuste. 2.3.2 El Tamaño de una Desviación Típica: s En el modelo de regresión lineal básica, la desviación de la respuesta de la línea de regresión, $y_i-( _0+_1x_i) $, no es una cantidad observable porque los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) no son observados. Sin embargo, usando los estimadores \\(b_0\\) y \\(b_1\\), podemos aproximar esta desviación usando \\[\\begin{equation*} e_i=y_i-\\widehat{y}_i=y_i-\\left( b_0+b_1x_i\\right) , \\end{equation*}\\] conocido como el residuo. Los residuos serán cruciales para desarrollar estrategias para mejorar la especificación del modelo en la Sección 2.6. Ahora mostramos cómo usar los residuos para estimar \\(\\sigma ^2\\). De un primer curso en estadística, sabemos que si se pudieran observar las desviaciones \\(\\varepsilon _i\\), entonces una estimación deseable de \\(\\sigma ^2\\) sería \\((n-1)^{-1}\\sum_{i=1}^{n}\\left( \\varepsilon _i-\\overline{\\varepsilon }\\right) ^2\\). Como \\(\\{\\varepsilon _i\\}\\) no se observan, usamos lo siguiente. Definición. Un estimador de \\(\\sigma ^2\\), el error cuadrático medio (MSE), se define como \\[\\begin{equation} s^2=\\frac{1}{n-2}\\sum_{i=1}^{n}e_i{}^2. \\tag{2.3} \\end{equation}\\] La raíz cuadrada positiva, \\(s=\\sqrt{s^2},\\) se llama la desviación estándar residual. Comparando las definiciones de \\(s^2\\) y \\((n-1)^{-1}\\sum_{i=1}^{n}\\left( \\varepsilon _i-\\overline{\\varepsilon }\\right) ^2\\), verá dos diferencias importantes. Primero, al definir \\(s^2\\) no hemos restado el residuo promedio de cada residuo antes de elevar al cuadrado. Esto se debe a que el residuo promedio es cero, una propiedad especial de la estimación de mínimos cuadrados (ver Ejercicio 2.14). Este resultado se puede mostrar usando álgebra y está garantizado para todos los conjuntos de datos. En segundo lugar, al definir \\(s^2\\) hemos dividido por \\(n-2\\) en lugar de \\(n-1\\). Intuitivamente, dividir por \\(n\\) o \\(n-1\\) tiende a subestimar \\(\\sigma ^2\\). La razón es que, al ajustar líneas a los datos, necesitamos al menos dos observaciones para determinar una línea. Por ejemplo, debemos tener al menos tres observaciones para que haya alguna variabilidad alrededor de una línea. ¿Cuánta “libertad” hay para la variabilidad alrededor de una línea? Diremos que los grados de libertad del error son el número de observaciones disponibles, \\(n\\), menos el número de observaciones necesarias para determinar una línea, 2 (con símbolos, \\(df=n-2\\)). Sin embargo, como vimos en la subsección de estimación de mínimos cuadrados, no necesitamos identificar dos observaciones reales para determinar una línea. La idea es que si un analista conoce la línea y \\(n-2\\) observaciones, entonces las dos observaciones restantes se pueden determinar, sin variabilidad. Al dividir por \\(n-2\\), se puede mostrar que \\(s^2\\) es un estimador insesgado de \\(\\sigma ^2\\). También podemos expresar \\(s^2\\) en términos de las sumas de cuadrados. Es decir, \\[\\begin{equation*} s^2=\\frac{1}{n-2}\\sum_{i=1}^{n}\\left( y_i-\\widehat{y}_i\\right) ^2= \\frac{Error~SS}{n-2}=MSE. \\end{equation*}\\] Esto nos lleva a la tabla de análisis de varianza o ANOVA: \\[ {\\small \\begin{array}{llcl} \\hline \\hline \\text{Tabla ANOVA} \\\\ \\hline \\text{Fuente} &amp; \\text{Suma de Cuadrados} &amp; df &amp; \\text{Cuadrado Medio} \\\\ \\hline \\text{Regresión} &amp; Regression~SS &amp; 1 &amp; Regression~MS \\\\ \\text{Error} &amp; Error~SS &amp; n-2 &amp; MSE \\\\ \\text{Total} &amp; Total~SS &amp; n-1 &amp; \\\\ \\hline \\hline \\end{array} } \\] La tabla ANOVA es simplemente un dispositivo de contabilidad utilizado para hacer un seguimiento de las fuentes de variabilidad; aparece rutinariamente en paquetes de software estadístico como parte de los resultados de la regresión. Las figuras de la columna de cuadrados medios se definen como las sumas de cuadrados (\\(SS\\)) divididas por sus respectivos grados de libertad (\\(df\\)). En particular, el cuadrado medio de los errores (\\(MSE\\)) es igual a \\(s^2\\) y la suma de cuadrados de la regresión es igual al cuadrado medio de la regresión. Esta última propiedad es específica para la regresión con una variable; no es cierta cuando consideramos más de una variable explicativa. Los grados de libertad del error en la tabla ANOVA son \\(n-2\\). Los grados de libertad totales son \\(n-1\\), lo que refleja el hecho de que la suma total de cuadrados se centra en la media (se requieren al menos dos observaciones para una variabilidad positiva). El grado de libertad único asociado con la parte de regresión significa que la pendiente, más una observación, es suficiente información para determinar la línea. Esto se debe a que se necesitan dos observaciones para determinar una línea y al menos tres observaciones para que haya alguna variabilidad alrededor de la línea. La tabla de análisis de varianza para los datos de la lotería es: Suma de Cuadrados \\(df\\) Cuadrado Medio Regresión 2,527,165,015 1 2,527,165,015 Error 690,116,755 48 14,377,432 Total 3,217,281,770 49 Código R para Producir la Tabla ANOVA de Lotería #Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) model.basiclinearreg&lt;-lm(Lot$SALES ~ Lot$POP) #summary(model.basiclinearreg) ANOVA &lt;- anova(model.basiclinearreg) row1 &lt;- c(ANOVA$`Sum Sq`[1], ANOVA$`Df`[1], ANOVA$`Mean Sq`[1]) row2 &lt;- c(ANOVA$`Sum Sq`[2], ANOVA$`Df`[2], ANOVA$`Mean Sq`[2]) row3 &lt;- c(ANOVA$`Sum Sq`[1]+ANOVA$`Sum Sq`[2], ANOVA$`Df`[1] +ANOVA$`Df`[2], NaN) ANOVATable &lt;- rbind(row1, row2, row3) ANOVATable1 &lt;- format(round(ANOVATable, digits = 0), big.mark = &#39;,&#39;) ANOVATable1[3,3] &lt;- &quot;&quot; rownames(ANOVATable1) &lt;- c(&quot;Regresión&quot;, &quot;Error&quot;, &quot;Total&quot;) colnames(ANOVATable1) &lt;- c(&quot;Suma de Cuadrados&quot;, &quot;$df$&quot;, &quot;Cuadrado Medio&quot;) kable(ANOVATable1, align = &#39;r&#39;) %&gt;% kable_styling(position = &quot;center&quot;, full_width = FALSE) %&gt;% kableExtra::kable_classic(font = 12, html_font = &quot;Cambria&quot;) De esta tabla, puede verificar que \\(R^2=78.5\\%\\) y \\(s=3,792.\\) 2.4 Propiedades de los Estimadores del Coeficiente de Regresión Las estimaciones de mínimos cuadrados se pueden expresar como una suma ponderada de las respuestas. Para ver esto, define los pesos \\[\\begin{equation*} w_i=\\frac{x_i-\\overline{x}}{s_x^2(n-1)}. \\end{equation*}\\] Como la suma de las desviaciones de \\(x\\) (\\(x_i-\\overline{x}\\)) es cero, vemos que \\(\\sum_{i=1}^{n}w_i=0\\). Así, podemos expresar la estimación de la pendiente \\[\\begin{equation} b_1=r\\frac{s_y}{s_x}=\\frac{1}{(n-1)s_x^2}\\sum_{i=1}^{n}\\left( x_i-\\overline{x}\\right) \\left( y_i-\\overline{y}\\right) =\\sum_{i=1}^{n}w_i\\left( y_i-\\overline{y}\\right) =\\sum_{i=1}^{n}w_iy_i. \\tag{2.4} \\end{equation}\\] Los ejercicios piden al lector verificar que \\(b_0\\) también puede expresarse como una suma ponderada de las respuestas, por lo que nuestra discusión se refiere a ambos coeficientes de regresión. Dado que los coeficientes de regresión son sumas ponderadas de respuestas, pueden verse afectados drásticamente por observaciones inusuales (ver Sección 2.6). Como \\(b_1\\) es una suma ponderada, es sencillo derivar la esperanza y la varianza de esta estadística. Por la linealidad de las esperanzas y la Suposición F1, tenemos \\[\\begin{equation*} \\mathrm{E}~b_1=\\sum_{i=1}^{n}w_i~\\mathrm{E}~y_i=\\beta_0\\sum_{i=1}^{n}w_i+\\beta_1\\sum_{i=1}^{n}w_ix_i=\\beta_1. \\end{equation*}\\] Es decir, \\(b_1\\) es un estimador imparcial de \\(\\beta_1\\). Aquí, la suma $ _{i=1}^{n}w_ix_i$ \\(=\\) \\(\\left[ s_x^2(n-1)\\right] ^{-1}\\sum_{i=1}^{n}\\left( x_i-\\overline{x}\\right) x_i\\) \\(=\\left[s_x^2(n-1)\\right] ^{-1}\\sum_{i=1}^{n}\\left( x_i-\\overline{x}\\right) ^2=1.\\) A partir de la definición de los pesos, una sencilla algebra también muestra que \\(\\sum_{i=1}^{n}w_i^2=1/\\left( s_x^2(n-1)\\right)\\). Además, la independencia de las respuestas implica que la varianza de la suma es la suma de las varianzas, y así tenemos \\[\\begin{equation*} \\mathrm{Var}~b_1 =\\sum_{i=1}^{n}w_i^2\\mathrm{Var}~y_i=\\frac{\\sigma^2}{s_x^2(n-1)}. \\end{equation*}\\] Sustituyendo \\(\\sigma ^2\\) por su estimador \\(s^2\\) y tomando raíces cuadradas se obtiene lo siguiente. Definición. El error estándar de \\(b_1\\), la desviación estándar estimada de \\(b_1\\), se define como \\[\\begin{equation} se(b_1)=\\frac{s}{s_x\\sqrt{n-1}}. \\tag{2.5} \\end{equation}\\] Esta es nuestra medida de la fiabilidad, o precisión, del estimador de la pendiente. Usando la ecuación (2.5), vemos que \\(se(b_1)\\) está determinado por tres cantidades: \\(n\\), \\(s\\) y \\(s_x\\), de la siguiente manera: Si tenemos más observaciones, de manera que \\(n\\) sea mayor, entonces \\(se(b_1)\\) será menor, manteniendo todo lo demás constante. Si las observaciones tienen una mayor tendencia a estar más cerca de la línea, de manera que \\(s\\) sea menor, entonces \\(se(b_1)\\) será menor, manteniendo todo lo demás constante. Si los valores de la variable explicativa están más dispersos, de manera que \\(s_x\\) aumenta, entonces \\(se(b_1)\\) será menor, manteniendo todo lo demás constante. Valores menores de \\(se(b_1)\\) ofrecen una mejor oportunidad para detectar relaciones entre \\(y\\) y \\(x\\). La Figura 2.6 ilustra estas relaciones. Aquí, el diagrama de dispersión en el medio tiene el valor más pequeño de \\(se(b_1)\\). Comparado con el gráfico del medio, el gráfico de la izquierda tiene un valor mayor de \\(s\\) y por lo tanto \\(se(b_1)\\). Comparado con el gráfico de la derecha, el gráfico del medio tiene un valor mayor de \\(s_x\\), y por lo tanto un valor menor de \\(se(b_1)\\). Figure 2.6: Estos tres diagramas de dispersión muestran la misma relación lineal entre \\(y\\) y \\(x\\). El gráfico a la izquierda muestra una mayor variabilidad alrededor de la línea que el gráfico del medio. El gráfico a la derecha muestra una desviación estándar menor en \\(x\\) que el gráfico del medio. Código R para producir la Figura 2.6 # FIGURA 2.6 AQUÍ par(mfrow=c(1, 3),mar=c(3.8,2.8,1,1), cex=1.3) x &lt;- c(1,2,2.3,2.5,1.5,1.7,2.6,2.8,.9,.88,.8,1.2,1.3,1.45,1.8,2.2,2.1) y &lt;- c(.5,2.2,2.6,2.5,.8,1.5,2.3,2.4,.75,.7,1.3,1.5,1.7,2.3,2.3,2.7,1.25) plot(x,y, xlim=c(0.5,3), ylim=c(0,3.5), bty=&quot;l&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab=&quot;&quot;) mtext(&quot;y&quot;, side=2, at=3.5, line=2, las=1, cex=1.3) mtext(&quot;x&quot;, side=1, line=2, cex=1.3) a &lt;- seq(.75,2.75, by = .001) b = a lines(a,b) x &lt;- c(1,2,2.3,2.5,1.5,1.7,2.6,2.8,.9,.88,.8,1.2,1.3,1.45,1.8,2.2,2.45) y &lt;- c(1,2,2.3,2.5,1.2,1.6,2.4,2.6,1.1,1.11,1.2,1.3,1.45,1.6,1.95,2.3,2.7) plot(x,y, xlim=c(0.5,3), ylim=c(0,3.5), bty=&quot;l&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab=&quot;&quot;) mtext(&quot;y&quot;, side=2, at=3.5, line=2, las=1, cex=1.3) mtext(&quot;x&quot;, side=1, line=2, cex=1.3) a &lt;- seq(.75,2.75, by = .001) b = a lines(a,b) x &lt;- c(1,2,2.3,2.5,1.5,1.7,2.6,2.8,2.6,1.5,2,1.2,1.3,1.45,1.8,2.2,2.45) y &lt;- c(1,2,2.3,2.5,1.2,1.6,2.4,2.6,2,2,2.4,1.3,1.55,1.6,1.95,1.6,2.7) plot(x,y, xlim=c(-1.5,5), ylim=c(-2,5.5), bty=&quot;l&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab=&quot;&quot;) mtext(&quot;y&quot;, side=2, at=5.1, line=2, las=1, cex=1.3) mtext(&quot;x&quot;, side=1, line=2, cex=1.3) a &lt;- seq(-.5,4.5, by = .001) b = a lines(a,b) La ecuación (2.4) también implica que el coeficiente de regresión \\(b_1\\) sigue una distribución normal. Es decir, recordemos de la estadística matemática que las combinaciones lineales de variables aleatorias normales también son normales. Así, si se cumple la Suposición F5, entonces \\(b_1\\) sigue una distribución normal. Además, existen varias versiones de los teoremas del límite central para sumas ponderadas (ver, por ejemplo, Serfling, 1980). Así, como se discute en la Sección 1.4, si las respuestas \\(y_i\\) están siquiera aproximadamente distribuidas normalmente, entonces será razonable usar una aproximación normal para la distribución muestral de \\(b_1\\). Usando \\(se(b_1)\\) como la desviación estándar estimada de \\(b_1\\), para valores grandes de \\(n\\) tenemos que \\(\\left( b_1-\\beta_1\\right) /se(b_1)\\) tiene una distribución normal estándar aproximada. Aunque no lo probaremos aquí, bajo la Suposición F5 \\(\\left( b_1-\\beta_1\\right) /se(b_1)\\) sigue una distribución \\(t\\) con grados de libertad \\(df=n-2\\). 2.5 Inferencia Estadística Una vez que hemos ajustado un modelo con un conjunto de datos, podemos hacer una serie de afirmaciones importantes. Generalmente, es útil pensar en estas afirmaciones en tres categorías: (i) pruebas de ideas hipotetizadas, (ii) estimaciones de parámetros del modelo y (iii) predicciones de nuevos resultados. 2.5.1 ¿Es Importante la Variable Explicativa?: La Prueba t Respondemos a la pregunta de si la variable explicativa es importante investigando si \\(\\beta_1=0\\). La lógica es que si \\(\\beta_1=0\\), entonces el modelo de regresión lineal básico ya no incluye una variable explicativa \\(x\\). Por lo tanto, traducimos nuestra pregunta sobre la importancia de la variable explicativa en una pregunta más específica que puede ser respondida utilizando el marco de pruebas de hipótesis. Esta pregunta más específica es: ¿es válida la hipótesis nula \\(H_0:\\beta_1=0\\)? Respondemos a esta pregunta observando la estadística de prueba: \\[ {\\small t-\\mathrm{ratio}=\\frac{\\mathrm{valor~estimado~del~parámetro~-~valor~hipotetizado}} {\\mathrm{error~estándar~del~estimador}}. } \\] En el caso de \\(H_0:\\beta_1=0\\), examinamos la razón t \\(t(b_1)=b_1/se(b_1)\\) porque el valor hipotetizado de \\(\\beta_1\\) es 0. Esta es la estandarización apropiada porque, bajo la hipótesis nula y las suposiciones del modelo descritas en la Sección 2.4, la distribución muestral de \\(t(b_1)\\) se puede demostrar que sigue una distribución t con \\(df=n-2\\) grados de libertad. Así, para probar la hipótesis nula \\(H_0\\) contra la alternativa \\(H_{a}:\\beta_1\\neq 0\\), rechazamos \\(H_0\\) a favor de \\(H_{a}\\) si \\(|t(b_1)|\\) excede un valor t. Aquí, este valor t es un percentil de la distribución t usando \\(df=n-2\\) grados de libertad. Denotamos el nivel de significancia como \\(\\alpha\\) y este valor t como \\(t_{n-2,1-\\alpha /2}\\). Ejemplo: Ventas de Lotería - Continuación. Para el ejemplo de ventas de lotería, la desviación estándar residual es \\(s=3,792\\). En la Tabla 2.1, tenemos \\(s_x = 11,098\\). Por lo tanto, el error estándar de la pendiente es \\(se(b_1) = 3792/(11098\\sqrt{50-1})=0.0488\\). Según la Sección 2.1, la estimación de la pendiente es \\(b_1=0.647\\). Por lo tanto, la estadística t es \\(t(b_1) = 0.647/0.0488 = 13.4\\). Interpretamos esto diciendo que la pendiente está 13.4 errores estándar por encima de cero. Para el nivel de significancia, usamos el valor habitual de \\(\\alpha\\) = 5%. El percentil 97.5 de una distribución t con \\(df=50-2=48\\) grados de libertad es \\(t_{48,0.975}=2.011\\). Dado que \\(|13.4|&gt;2.011\\), rechazamos la hipótesis nula de que la pendiente \\(\\beta_1 = 0\\) a favor de la alternativa de que \\(\\beta_1 \\neq 0\\). Tomar decisiones comparando una razón t con un valor t se llama una prueba t. Probar \\(H_0:\\beta_1=0\\) frente a \\(H_{a}:\\beta_1\\neq 0\\) es solo una de las muchas pruebas de hipótesis que se pueden realizar, aunque es la más común. Tabla 2.3 describe procedimientos alternativos para la toma de decisiones. Estos procedimientos son para probar \\(H_0:\\beta_1 = d\\) donde \\(d\\) es un valor prescrito por el usuario que puede ser igual a cero o cualquier otro valor conocido. Por ejemplo, en nuestro ejemplo de la Sección 2.7, usaremos \\(d=1\\) para probar teorías financieras sobre el mercado de valores. Tabla 2.3 Procedimientos de Toma de Decisiones para Probar \\(H_0:\\beta_1 = d\\) \\[ {\\small \\begin{array}{c|c} \\hline \\text{Hipótesis Alternativa} (H_{a}) &amp; \\text{Procedimiento: Rechazar } H_0 \\text{ a favor de } H_{a} \\text{ si} \\\\ \\hline \\beta_1&gt;d &amp; t-\\mathrm{ratio}&gt;t_{n-2,1-\\alpha }. \\\\ \\beta_1&lt;d &amp; t-\\mathrm{ratio}&lt;-t_{n-2,1-\\alpha }. \\\\ \\beta_1\\neq d &amp; |t-\\mathrm{ratio}\\mathit{|}&gt;t_{n-2,1-\\alpha /2}. \\\\ \\end{array} }\\\\ {\\small \\begin{array}{l} \\hline \\text{Notas: El nivel de significancia es } \\alpha . \\text{Aquí, }t_{n-2,1-\\alpha} \\text{ es el percentil } (1-\\alpha )\\\\ ~~\\text{de la distribución *t* con } df=n-2 \\text{ grados de libertad.}\\\\ ~~\\text{La estadística de prueba es }t-\\mathrm{ratio} = (b_1 -d)/se(b_1) . \\\\ \\hline \\end{array} } \\] Alternativamente, se pueden construir valores de probabilidad (\\(p\\)-) y compararlos con los niveles de significancia dados. El valor \\(p\\)- es una estadística resumen útil para el analista de datos ya que permite al lector del informe entender la fuerza de la desviación de la hipótesis nula. Tabla 2.4 resume el procedimiento para calcular los valores \\(p\\)-. Tabla 2.4 Valores de Probabilidad para Probar \\(H_0:\\beta_1 = d\\) \\[ {\\small \\begin{array}{c|ccc} \\hline \\text{Hipótesis} &amp; &amp; &amp; \\\\ \\text{Alternativa} (H_a) &amp; \\beta_1&gt;d &amp; \\beta_1&lt;d &amp; \\beta_1\\neq d \\\\ \\hline p-value &amp; \\Pr(t_{n-2}&gt;t-\\mathrm{ratio}) &amp; \\Pr(t_{n-2}&lt;t-\\mathrm{ratio}) &amp; \\Pr (|t_{n-2}|&gt;|t-\\mathrm{ratio}\\mathit{|}) \\\\\\hline \\end{array} }\\\\ {\\small \\begin{array}{l} \\hline \\text{Notas: Aquí, }t_{n-2} \\text{ es una variable aleatoria distribuida como *t* con } df=n-2 \\text{ grados de libertad.}\\\\ ~~\\text{La estadística de prueba es }t-\\mathrm{ratio} = (b_1 -d)/se(b_1) . \\\\ \\hline \\end{array} } \\] Otra forma interesante de abordar la cuestión de la importancia de una variable explicativa es a través del coeficiente de correlación. Recuerda que el coeficiente de correlación es una medida de la relación lineal entre \\(x\\) e \\(y\\). Denotemos esta estadística por \\(r(y,x)\\). Esta cantidad no se ve afectada por cambios de escala en ninguna de las variables. Por ejemplo, si multiplicamos la variable \\(x\\) por el número \\(b_1\\), entonces el coeficiente de correlación permanece sin cambios. Además, las correlaciones no cambian con los desplazamientos aditivos. Así, si agregamos un número, digamos \\(b_0\\), a cada variable \\(x\\), entonces el coeficiente de correlación permanece sin cambios. Usar un cambio de escala y un desplazamiento aditivo en la variable \\(x\\) puede utilizarse para producir el valor ajustado \\(\\widehat{y}=b_0+b_1x\\). Por lo tanto, usando la notación, tenemos \\(|r(y,x)|=r(y,\\widehat{y})\\). Así, podemos interpretar que la correlación entre las respuestas y la variable explicativa es igual a la correlación entre las respuestas y los valores ajustados. Esto lleva al siguiente hecho algebraico interesante: \\(R^2=r^2.\\) Es decir, el coeficiente de determinación es igual al cuadrado del coeficiente de correlación. Esto es mucho más fácil de interpretar si uno piensa en \\(r\\) como la correlación entre los valores observados y los ajustados. Consulta el Ejercicio 2.13 para los pasos útiles para confirmar este resultado. 2.5.2 Intervalos de Confianza Los investigadores a menudo citan el mecanismo formal de pruebas de hipótesis para responder a la pregunta: “¿Tiene la variable explicativa una influencia real en la respuesta?” Una pregunta de seguimiento natural es: “¿En qué medida afecta \\(x\\) a \\(y\\)?” Hasta cierto punto, se puede responder utilizando el tamaño del \\(t\\)-ratio o el valor de \\(p\\). Sin embargo, en muchos casos, un intervalo de confianza para la pendiente es más útil. Para introducir los intervalos de confianza para la pendiente, recordemos que \\(b_1\\) es nuestro estimador puntual de la verdadera pendiente desconocida \\(\\beta_1\\). La Sección 2.4 argumentó que este estimador tiene un error estándar \\(se(b_1)\\) y que \\(\\left( b_1-\\beta_1\\right) /se(b_1)\\) sigue una distribución \\(t\\) con \\(n-2\\) grados de libertad. Las declaraciones de probabilidad se pueden invertir para obtener intervalos de confianza. Usando esta lógica, tenemos el siguiente intervalo de confianza para la pendiente \\(\\beta_1\\). Definición. Un intervalo de confianza del \\(100(1-\\alpha)\\)% para la pendiente \\(\\beta_1\\) es \\[\\begin{equation} b_1\\pm t_{n-2,1-\\alpha /2} ~se(b_1). \\tag{2.6} \\end{equation}\\] Al igual que con las pruebas de hipótesis, \\(t_{n-2,1-\\alpha /2}\\) es el percentil (1-\\(\\alpha\\) /2) de la distribución \\(t\\) con \\(df=n-2\\) grados de libertad. Debido a la naturaleza bilateral de los intervalos de confianza, el percentil es 1 - (1 - nivel de confianza) / 2. En este texto, por simplicidad, generalmente usamos un intervalo de confianza del 95%, por lo que el percentil es 1-(1-0.95)/2 = 0.975. El intervalo de confianza proporciona un rango de confiabilidad que mide la utilidad de la estimación. En la Sección 2.1, establecimos que la estimación de la pendiente por mínimos cuadrados para el ejemplo de ventas de lotería es \\(b_1=0.647\\). La interpretación es que si la población de un código postal difiere en 1,000, entonces esperamos que las ventas promedio de lotería difieran en $647. ¿Qué tan confiable es esta estimación? Resulta que \\(se(b_1)=0.0488\\) y, por lo tanto, un intervalo de confianza aproximado del 95% para la pendiente es \\[\\begin{equation*} 0.647\\pm (2.011)(.0488), \\end{equation*}\\] o (0.549, 0.745). De manera similar, si la población difiere en 1,000, un intervalo de confianza del 95% para el cambio esperado en las ventas es (549, 745). Aquí, usamos el valor \\(t\\) \\(t_{48,0.975}=2.011\\) porque hay 48 (= \\(n\\)-2) grados de libertad y, para un intervalo de confianza del 95%, necesitamos el percentil 97.5. 2.5.3 Intervalos de Predicción En la Sección 2.1, mostramos cómo usar los estimadores de mínimos cuadrados para predecir las ventas de lotería para un código postal, fuera de nuestra muestra, con una población de 10,000. Dado que la predicción es una tarea tan importante para los actuarios, formalizamos el procedimiento para que pueda ser utilizado regularmente. Para predecir una observación adicional, asumimos que el nivel de la variable explicativa es conocido y se denota por \\(x_{\\ast}\\). Por ejemplo, en nuestro ejemplo anterior de ventas de lotería usamos \\(x_{\\ast} = 10,000\\). También asumimos que la observación adicional sigue el mismo modelo de regresión lineal que las observaciones en la muestra. Usando nuestros estimadores de mínimos cuadrados, nuestra predicción puntual es \\(\\widehat{y}_{\\ast} = b_0 + b_1 x_{\\ast}\\), la altura de la línea de regresión ajustada en \\(x_{\\ast}\\). Podemos descomponer el error de predicción en dos partes: \\[ \\begin{array}{ccccc} \\underbrace{y_{\\ast} - \\widehat{y}_{\\ast}} &amp; = &amp; \\underbrace{\\beta_0 - b_0 + \\left( \\beta_1 - b_1 \\right) x_{\\ast}} &amp; + &amp; \\underbrace{\\varepsilon_{\\ast}} \\\\ {\\small \\text{error de predicción}} &amp; {\\small =} &amp; {\\small \\text{error en la estimación de la }} &amp; {\\small +} &amp; {\\small \\text{desviación de la observación adicional}} \\\\ &amp; &amp; {\\small \\text{línea de regresión en } x}_{\\ast} &amp; &amp; {\\small \\text{respuesta de su media}} \\end{array} \\] Se puede demostrar que el error estándar de la predicción es \\[\\begin{equation*} se(pred) = s \\sqrt{1+\\frac{1}{n}+\\frac{\\left( x_{\\ast}-\\overline{x}\\right) ^2}{(n-1)s_x^2}}. \\end{equation*}\\] Al igual que con \\(se(b_1)\\), los términos \\(n^{-1}\\) y $( x_{}- ) ^2/$ se acercan a cero a medida que el tamaño de la muestra \\(n\\) se vuelve grande. Por lo tanto, para grandes \\(n\\), tenemos que \\(se(pred)\\approx s\\), lo que refleja que el error en la estimación de la línea de regresión en un punto se vuelve insignificante y la desviación de la respuesta adicional de su media se convierte en la única fuente de incertidumbre. Definición. Un intervalo de predicción del \\(100(1-\\alpha)\\)% en \\(x_{\\ast}\\) es \\[\\begin{equation} \\widehat{y}_{\\ast} \\pm t_{n-2,1-\\alpha /2} ~se(pred) \\tag{2.7} \\end{equation}\\] donde el valor \\(t\\) \\(t_{n-2,1-\\alpha /2}\\) es el mismo que se usa para la prueba de hipótesis y el intervalo de confianza. Por ejemplo, la predicción puntual en \\(x_{\\ast} = 10,000\\) es \\(\\widehat{y}_{\\ast}\\)= 469.7 + 0.647 (10000) = 6,939.7. El error estándar de esta predicción es \\[\\begin{equation*} se(pred) = 3,792 \\sqrt{1+\\frac{1}{50} + \\frac{\\left( 10,000-9,311\\right)^2}{(50-1)(11,098)^2}} = 3,829.6. \\end{equation*}\\] Con un valor \\(t\\) igual a 2.011, esto da lugar a un intervalo de predicción aproximado del 95% \\[\\begin{equation*} 6,939.7 \\pm (2.011)(3,829.6) = 6,939.7 \\pm 7,701.3 = (-761.6, ~14,641.0). \\end{equation*}\\] Interpretamos estos resultados señalando primero que nuestra mejor estimación de ventas de lotería para un código postal con una población de 10,000 es 6,939.70. Nuestro intervalo de predicción del 95% representa un rango de confiabilidad para esta predicción. Si pudiéramos observar muchos códigos postales, cada uno con una población de 10,000, en promedio esperaríamos que aproximadamente 19 de cada 20, o el 95%, tendrían ventas de lotería entre 0 y 14,641. Es habitual truncar el límite inferior del intervalo de predicción a cero si se considera que los valores negativos de la respuesta son inapropiados. Código R para producir los análisis de la Sección 2.5 # RESULTADOS DE LA SECCIÓN 2.1 model.basiclinearreg&lt;-lm(SALES ~ POP, data = Lot) summary(model.basiclinearreg) # SECCIÓN 2.5.2 INTERVALOS DE CONFIANZA confint(model.basiclinearreg) confint(model.basiclinearreg, level=.90) # SECCIÓN 2.5.3 INTERVALOS DE PREDICCIÓN newdata &lt;- data.frame(POP &lt;- 10000) predict(model.basiclinearreg, newdata, interval=&quot;prediction&quot;) predict(model.basiclinearreg, newdata, interval=&quot;prediction&quot;, level=.90) # PROPORCIONA EL PERCENTIL 97.5 DE UNA DISTRIBUCIÓN T, SOLO PARA COMPROBAR qt(.975, 48) 2.6 Construyendo un Mejor Modelo: Análisis de Residuos Las disciplinas cuantitativas calibran modelos con datos. La estadística lleva esto un paso más allá, utilizando las discrepancias entre las suposiciones y los datos para mejorar la especificación del modelo. Examinaremos las suposiciones del modelo de la Sección 2.2 a la luz de los datos y utilizaremos cualquier desajuste para especificar un mejor modelo; este proceso se conoce como verificación diagnóstica (como cuando vas al médico y él o ella realiza pruebas diagnósticas para revisar tu salud). Comenzaremos con la representación del error de la Sección 2.2. Bajo este conjunto de suposiciones, las desviaciones {\\(\\varepsilon _i\\)} son idénticamente e independientemente distribuidas (i.i.d), y bajo la suposición F5, distribuidas normalmente. Para evaluar la validez de estas suposiciones, se usan los residuos (observados) {\\(e_i\\)} como aproximaciones para las desviaciones (no observadas) {\\(\\varepsilon _i\\)}. El tema básico es que si los residuos están relacionados con una variable o muestran algún otro patrón reconocible, entonces deberíamos poder aprovechar esta información y mejorar la especificación de nuestro modelo. Los residuos deberían contener poca o ninguna información y representar solo la variación natural de la muestra que no se puede atribuir a ninguna fuente específica. Análisis de residuos es el ejercicio de verificar los residuos en busca de patrones. Existen cinco tipos de discrepancias en el modelo que los analistas comúnmente buscan. Si se detectan, las discrepancias pueden corregirse con los ajustes apropiados en la especificación del modelo. Problemas de Especificación del Modelo Falta de Independencia. Puede haber relaciones entre las desviaciones {\\(\\varepsilon _i\\)} de modo que no sean independientes. Heterocedasticidad. La suposición E3 indica que todas las observaciones tienen una variabilidad común (aunque desconocida), conocida como homocedasticidad. Heterocedasticidad es el término usado cuando la variabilidad varía según la observación. Relaciones entre Desviaciones del Modelo y Variables Explicativas. Si una variable explicativa tiene la capacidad de ayudar a explicar la desviación \\(\\varepsilon\\), entonces deberíamos poder usar esta información para predecir mejor \\(y\\). Distribuciones No Normales. Si la distribución de la desviación representa una desviación seria de la normalidad, entonces los procedimientos de inferencia usuales ya no son válidos. Puntos Inusuales. Las observaciones individuales pueden tener un gran efecto en el ajuste del modelo de regresión, lo que significa que los resultados pueden ser sensibles al impacto de una sola observación. Esta lista servirá al lector durante el estudio del análisis de regresión. Por supuesto, con solo una introducción a los modelos básicos aún no hemos visto modelos alternativos que podrían usarse cuando encontramos estas discrepancias en el modelo. En la Parte II de este libro sobre modelos de series temporales, estudiaremos la falta de independencia entre datos ordenados en el tiempo. El Capítulo 5 considerará la heterocedasticidad con más detalle. La introducción a la regresión lineal múltiple en el Capítulo 3 será nuestra primera vista sobre cómo manejar las relaciones entre {\\(\\varepsilon _i\\)} y variables explicativas adicionales. Sin embargo, ya hemos tenido una introducción al efecto de las distribuciones normales, viendo que los gráficos \\(qq\\) pueden detectar la no normalidad y que las transformaciones pueden ayudar a inducir la normalidad aproximada. En esta sección, discutimos los efectos de los puntos inusuales. Gran parte del análisis de residuos se realiza examinando un residuo estandarizado, que es un residuo dividido por su error estándar. Un error estándar aproximado del residuo es \\(s\\); en el Capítulo 3 daremos una definición matemática precisa. Hay dos razones por las que a menudo examinamos residuos estandarizados en lugar de residuos básicos. Primero, si las respuestas están distribuidas normalmente, entonces los residuos estandarizados son aproximadamente realizaciones de una distribución normal estándar. Esto proporciona una distribución de referencia para comparar los valores de los residuos estandarizados. Por ejemplo, si un residuo estandarizado supera dos en valor absoluto, esto se considera inusualmente grande y la observación se llama outlier (punto atípico). Segundo, dado que los residuos estandarizados son adimensionales, podemos transferir la experiencia de un conjunto de datos a otro. Esto es cierto independientemente de si la distribución de referencia normal es aplicable o no. Puntos Atípicos y Puntos de Alta Influencia. Otra parte importante del análisis de residuos es la identificación de observaciones inusuales en un conjunto de datos. Debido a que las estimaciones de regresión son promedios ponderados con pesos que varían según la observación, algunas observaciones son más importantes que otras. Esta ponderación es más importante de lo que muchos usuarios del análisis de regresión se dan cuenta. De hecho, el ejemplo a continuación demuestra que una sola observación puede tener un efecto dramático en un gran conjunto de datos. Hay dos direcciones en las que un punto de datos puede ser inusual: la dirección horizontal y la dirección vertical. Por “inusual”, nos referimos a que una observación bajo consideración parece estar lejos de la mayoría del conjunto de datos. Una observación que es inusual en la dirección vertical se llama punto atípico. Una observación que es inusual en la dirección horizontal se llama punto de alta influencia. Una observación puede ser tanto un punto atípico como un punto de alta influencia. Ejemplo: Puntos Atípicos y Puntos de Alta Influencia. Considera el conjunto de datos ficticio de 19 puntos más tres puntos, etiquetados como A, B y C, que se muestra en la Figura 2.7 y Tabla 2.5. Piensa en los primeros 19 puntos como observaciones “buenas” que representan algún tipo de fenómeno. Queremos investigar el efecto de agregar un solo punto aberrante. Tabla 2.5. 19 Puntos Base Más Tres Tipos de Observaciones Inusuales \\[ \\small{ \\begin{array}{c|cccccccccc|ccc} \\hline Variables &amp; &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp; &amp; A &amp; B &amp; C \\\\ \\hline x &amp; 1.5 &amp; 1.7 &amp; 2.0 &amp; 2.2 &amp; 2.5 &amp; 2.5 &amp; 2.7 &amp; 2.9 &amp; 3.0 &amp; 3.5 &amp; 3.4 &amp; 9.5 &amp; 9.5 \\\\ y &amp; 3.0 &amp; 2.5 &amp; 3.5 &amp; 3.0 &amp; 3.1 &amp; 3.6 &amp; 3.2 &amp; 3.9 &amp; 4.0 &amp; 4.0 &amp; 8.0 &amp; 8.0 &amp; 2.5 \\\\ \\hline x &amp; 3.8 &amp; 4.2 &amp; 4.3 &amp; 4.6 &amp; 4.0 &amp; 5.1 &amp; 5.1 &amp; 5.2 &amp; 5.5 &amp; &amp; &amp; &amp; \\\\ y &amp; 4.2 &amp; 4.1 &amp; 4.8 &amp; 4.2 &amp; 5.1 &amp; 5.1 &amp; 5.1 &amp; 4.8 &amp; 5.3 &amp; &amp; &amp; &amp; \\\\ \\hline \\end{array} } \\] Figure 2.7: Gráfico de dispersión de 19 puntos base más tres puntos inusuales, etiquetados A, B, y C. Código R para Producir la Figura 2.7 # EJEMPLO 2.6 PUNTO ATÍPICO OUTLR &lt;- read.csv(&quot;CSVData/OutlierExample.csv&quot;, header=TRUE) # FIGURA 2.7 par(mar=c(4.1,3.1,1.1,.1), cex=1.3) plot(OUTLR$X, OUTLR$Y, xlab=&quot;x&quot;, ylab=&quot;&quot;, xlim=c(0, 10), ylim=c(2, 9), las=1) mtext(&quot;y&quot;, at=5.5,side=2,las=1,cex=1.3, line=2.3) points(4.3, 8.0) text(4.7, 8.0, &quot;A&quot;, cex=1.3) points(9.5, 8.0) text(9.9, 8.0, &quot;B&quot;, cex=1.3) points(9.5, 2.5) text(9.9, 2.5, &quot;C&quot;, cex=1.3) Para investigar el efecto de cada tipo de punto aberrante, Tabla 2.6 resume los resultados de cuatro regresiones separadas. La primera regresión es para los diecinueve puntos base. Las otras tres regresiones utilizan los diecinueve puntos base más cada tipo de observación inusual. Tabla 2.6. Resultados de Cuatro Regresiones \\[ {\\small \\begin{array}{l|rrrrr} \\hline Datos &amp; b_0 &amp; b_1 &amp; s &amp; R^2(\\%) &amp; t(b_1) \\\\ \\hline 19 \\text{ Puntos Base} &amp; 1.869 &amp; 0.611 &amp; 0.288 &amp; 89.0 &amp; 11.71 \\\\ 19 \\text{ Puntos Base} ~+~ A &amp; 1.750 &amp; 0.693 &amp; 0.846 &amp; 53.7 &amp; 4.57 \\\\ 19 \\text{ Puntos Base} ~+~ B &amp; 1.775 &amp; 0.640 &amp; 0.285 &amp; 94.7 &amp; 18.01 \\\\ 19 \\text{ Puntos Base} ~+~ C &amp; 3.356 &amp; 0.155 &amp; 0.865 &amp; 10.3 &amp; 1.44 \\\\ \\hline \\end{array} } \\] Tabla 2.6 muestra que una línea de regresión proporciona un buen ajuste para los diecinueve puntos base. El coeficiente de determinación, \\(R^2\\), indica que alrededor del 89% de la variabilidad ha sido explicada por la línea. El tamaño del error típico, \\(s\\), es de aproximadamente 0.29, pequeño en comparación con la dispersión en los valores de \\(y\\). Además, el cociente \\(t\\) para el coeficiente de la pendiente es grande. Cuando se agrega el punto atípico A a los diecinueve puntos base, la situación empeora dramáticamente. El \\(R^2\\) baja del 89% al 53.7% y \\(s\\) aumenta de aproximadamente 0.29 a alrededor de 0.85. La línea de regresión ajustada en sí no cambia mucho, aunque nuestra confianza en las estimaciones ha disminuido. Un punto atípico es inusual en el valor de \\(y\\), pero “inusual en el valor de \\(y\\)” depende del valor de \\(x\\). Para ver esto, mantén el valor de \\(y\\) del Punto A igual, pero aumenta el valor de \\(x\\) y llama al punto B. Cuando se agrega el punto B a los diecinueve puntos base, la línea de regresión proporciona un ajuste mejor. El punto B está cerca de estar en la línea de ajuste de regresión generada por los diecinueve puntos base. Así, la línea de regresión ajustada y el tamaño del error típico, \\(s\\), no cambian mucho. Sin embargo, \\(R^2\\) aumenta del 89% a casi el 95%. Si pensamos en \\(R^2\\) como \\(1-(Error~SS)/(Total~SS)\\), al agregar el punto B hemos aumentado \\(Total~SS\\), la desviación total cuadrada en los \\(y\\), aunque el \\(Error~SS\\) se mantiene relativamente sin cambios. El punto B no es un punto atípico, pero es un punto de alta influencia. Para mostrar cuán influyente es este punto, reduce considerablemente el valor de \\(y\\) y llama a este el nuevo punto C. Cuando se agrega este punto a los diecinueve puntos base, la situación empeora dramáticamente. El coeficiente \\(R^2\\) baja del 89% al 10%, y el \\(s\\) más que se triplica, de 0.29 a 0.87. Además, los coeficientes de la línea de regresión cambian drásticamente. La mayoría de los usuarios de la regresión al principio no creen que un punto de veinte pueda tener un efecto tan dramático en el ajuste de la regresión. El ajuste de una línea de regresión siempre puede mejorarse eliminando un punto atípico. Si el punto es un punto de alta influencia y no un punto atípico, no está claro si el ajuste mejorará cuando el punto sea eliminado. ¡Simplemente porque puedes mejorar dramáticamente un ajuste de regresión omitiendo una observación no significa que siempre debas hacerlo! El objetivo del análisis de datos es comprender la información en los datos. A lo largo del texto, encontraremos muchos conjuntos de datos donde los puntos inusuales proporcionan alguna de la información más interesante sobre los datos. El objetivo de esta subsección es reconocer los efectos de los puntos inusuales; el Capítulo 5 proporcionará opciones para manejar puntos inusuales en tu análisis. Todas las disciplinas cuantitativas, como contabilidad, economía, programación lineal, etc., practican el arte del análisis de sensibilidad. El análisis de sensibilidad es una descripción de los cambios globales en un sistema debido a un pequeño cambio local en un elemento del sistema. Examinar los efectos de observaciones individuales en el ajuste de regresión es un tipo de análisis de sensibilidad. Ejemplo: Ventas de Lotería – Continuación. La Figura 2.8 muestra un valor atípico; el punto en la parte superior izquierda del gráfico representa un código postal que incluye a Kenosha, Wisconsin. Las ventas para este código postal son inusualmente altas dada su población. Kenosha está cerca de la frontera con Illinois; los residentes de Illinois probablemente participen en la lotería de Wisconsin, lo que aumenta efectivamente el potencial de ventas en Kenosha. Tabla 2.7 resume el ajuste de la regresión tanto con como sin este código postal. Tabla 2.7. Resultados de la Regresión con y sin Kenosha \\[ {\\small \\begin{array}{l|rrrrr} \\hline \\text{Datos} &amp; b_0 &amp; b_1 &amp; s &amp; R^2(\\%) &amp; t(b_1) \\\\ \\hline \\text{Con Kenosha} &amp; 469.7 &amp; 0.647 &amp; 3,792 &amp; 78.5 &amp; 13.26 \\\\ \\text{Sin Kenosha} &amp; -43.5 &amp; 0.662 &amp; 2,728 &amp; 88.3 &amp; 18.82 \\\\ \\hline \\end{array} } \\] Figure 2.8: Gráfico de dispersión de SALES versus POP, con el valor atípico correspondiente a Kenosha marcado. Código R para producir la Figura 2.8 y la Tabla 2.7 Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURA 2.8 par(mar=c(4.1,3.9,2,1),cex=1.1) plot(Lot$POP, Lot$SALES, ylab=&quot;&quot;, las=1, xlab = &quot;POP&quot;) mtext(&quot;SALES&quot;,side=2, at=36000, las=1, cex=1.1) text(5000, 24000, &quot;Kenosha&quot;) # TABLA 2.7 model.basiclinearreg&lt;-lm(SALES ~ POP, Lot) summary(model.basiclinearreg) model.Kenosha&lt;-lm(SALES ~ POP, Lot, subset=-c(9)) summary(model.Kenosha) Para los propósitos de inferencia sobre la pendiente, la presencia de Kenosha no altera los resultados de manera dramática. Ambas estimaciones de la pendiente son cualitativamente similares y los correspondientes valores \\(t\\) son muy altos, muy por encima de los umbrales para la significancia estadística. Sin embargo, hay diferencias notables al evaluar la calidad del ajuste. El coeficiente de determinación, \\(R^2\\), aumentó del 78.5% al 88.3% al eliminar Kenosha. Además, nuestro “desviación típica” \\(s\\) disminuyó en más de $1,000. Esto es particularmente importante si queremos ajustar nuestros intervalos de predicción. Para verificar la exactitud de nuestras suposiciones, también es común revisar la suposición de normalidad. Una forma de hacerlo es mediante el gráfico \\(qq\\), introducido en la Sección 1.2. Los dos paneles en las Figuras 2.9 son gráficos \\(qq\\) con y sin el código postal de Kenosha. Recuerda que los puntos “cercanos” a una línea indican normalidad aproximada. En el panel derecho de la Figura 2.9, la secuencia parece ser lineal, por lo que los residuos están aproximadamente distribuidos de manera normal. Este no es el caso en el panel izquierdo, donde la secuencia de puntos parece aumentar dramáticamente para grandes cuantiles. Lo interesante es que la no-normalidad de la distribución se debe a un solo valor atípico, no a un patrón de sesgo común a todas las observaciones. Figure 2.9: Gráficos \\(qq\\) de los residuos de la Lotería de Wisconsin. El panel izquierdo se basa en los 50 puntos. El panel derecho se basa en 49 puntos, residuos de una regresión después de eliminar Kenosha. Código R para producir la Figura 2.9 #Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURA 2.9 # TABLA 2.7 model.basiclinearreg&lt;-lm(SALES ~ POP, Lot) #summary(model.basiclinearreg) model.Kenosha&lt;-lm(SALES ~ POP, Lot, subset=-c(9)) #summary(model.Kenosha) par(mfrow=c(1, 2), mar=c(4.1,3.9,1.7,1),cex=1.1) qqnorm(residuals(model.basiclinearreg), main=&quot;&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Cuantiles Muestrales&quot;, side=2,at=20500,las=1,cex=1.1, adj=.5) qqnorm(residuals(model.Kenosha), main=&quot;&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Cuantiles Muestrales&quot;, side=2,at=9050,las=1,cex=1.1, adj=.5) 2.7 Aplicación: Modelo de Valoración de Activos Financieros En esta sección, estudiamos una aplicación financiera, el Modelo de Valoración de Activos Financieros, a menudo conocido por el acrónimo CAPM. El nombre es algo engañoso, ya que el modelo realmente trata sobre rendimientos basados en activos de capital, no sobre los precios en sí mismos. Los tipos de activos que examinamos son valores de acciones que se negocian en un mercado activo, como la Bolsa de Valores de Nueva York (NYSE). Para una acción en la bolsa, podemos relacionar los rendimientos con los precios mediante la siguiente expresión: \\[ {\\small \\mathrm{rendimiento =}\\frac{\\mathrm{precio~al~final~de~un~período+dividendos-precio~al~inicio~de~un~período}}{\\mathrm{precio~al~inicio~de~un~período}}. } \\] Si podemos estimar los rendimientos que genera una acción, entonces el conocimiento del precio al inicio de un período financiero genérico nos permite estimar el valor al final del período (precio final más dividendos). Por lo tanto, seguimos la práctica estándar y modelamos los rendimientos de una acción. Una idea intuitivamente atractiva, y una de las características básicas del CAPM, es que debería haber una relación entre el rendimiento de una acción y el mercado. Una justificación es simplemente que si las fuerzas económicas hacen que el mercado mejore, entonces esas mismas fuerzas deberían actuar sobre una acción individual, sugiriendo que también debería mejorar. Como se mencionó anteriormente, medimos el rendimiento de una acción a través del rendimiento. Para medir el rendimiento del mercado, existen varios índices de mercado que resumen el rendimiento de cada bolsa. Usaremos el índice “ponderado por igual” del Standard &amp; Poor’s 500. El Standard &amp; Poor’s 500 es la colección de las 500 empresas más grandes que se negocian en la NYSE, donde “grande” es identificado por Standard &amp; Poor’s, una organización de calificación de servicios financieros. El índice ponderado por igual se define asumiendo que se crea una cartera invirtiendo un dólar en cada una de las 500 empresas. Otra justificación para una relación entre los rendimientos de las acciones y el mercado proviene de la teoría de la economía financiera. Esta es la teoría CAPM, atribuida a Sharpe (1964) y Lintner (1965) y basada en las ideas de diversificación de cartera de Harry Markowitz (1959). Otros factores iguales, los inversionistas desearían seleccionar un rendimiento con un alto valor esperado y una baja desviación estándar, esta última siendo una medida de riesgo. Una de las propiedades deseables de usar desviaciones estándar como medida de riesgo es que es sencillo calcular la desviación estándar de una cartera. Solo es necesario conocer la desviación estándar de cada acción y las correlaciones entre acciones. Una acción notable es una libre de riesgo, es decir, una acción que teóricamente tiene una desviación estándar cero. Los inversionistas a menudo utilizan un bono del Tesoro de EE. UU. a 30 días como una aproximación de una acción libre de riesgo, argumentando que la probabilidad de default del gobierno de EE. UU. dentro de 30 días es insignificante. Positando la existencia de un activo libre de riesgo y algunas otras condiciones suaves, bajo la teoría CAPM existe una frontera eficiente llamada la línea de mercado de valores. Esta frontera especifica el rendimiento mínimo esperado que los inversionistas deberían exigir para un nivel específico de riesgo. Para estimar esta línea, podemos usar la ecuación: \\[\\begin{equation*} \\mathrm{E}~r = \\beta_0 + \\beta_1 r_m \\end{equation*}\\] donde \\(r\\) es el rendimiento de la acción y \\(r_m\\) es el rendimiento del mercado. Interpretamos \\(\\beta_1 r_m\\) como una medida de la cantidad de rendimiento de la acción que se atribuye al comportamiento del mercado. Probar la teoría económica, o modelos que surgen de cualquier disciplina, implica recolectar datos. La teoría CAPM trata sobre rendimientos ex-ante (antes del hecho), aunque solo podemos probar con rendimientos ex-post (después del hecho). Antes del hecho, los rendimientos son desconocidos y hay toda una distribución de rendimientos. Después del hecho, solo hay una realización única del rendimiento de la acción y del mercado. Debido a que se requieren al menos dos observaciones para determinar una línea, los modelos CAPM se estiman usando datos de acciones y del mercado recopilados a lo largo del tiempo. De esta manera, se pueden realizar varias observaciones. Para los propósitos de nuestras discusiones, seguimos la práctica estándar en la industria de valores y examinamos precios mensuales. Datos Para ilustrar, considere los rendimientos mensuales durante el período de cinco años desde enero de 1986 hasta diciembre de 1990, inclusive. Específicamente, usamos los rendimientos de la acción de Lincoln National Insurance Corporation como la variable dependiente (\\(y\\)) y los rendimientos del mercado del índice Standard &amp; Poor’s 500 como la variable explicativa (\\(x\\)). En ese momento, Lincoln era una gran compañía de seguros multirama, con sede en el medio oeste de EE. UU., específicamente en Fort Wayne, Indiana. Debido a que era bien conocida por su gestión prudente y estabilidad, es una buena compañía para comenzar nuestro análisis de la relación entre el mercado y una acción individual. Comenzamos interpretando algunas estadísticas básicas, en la Tabla 2.8, en términos de teoría financiera. Primero, un inversionista en Lincoln estará preocupado de que el rendimiento promedio de cinco años, \\(\\overline{y}=0.00510\\), esté por debajo del rendimiento del mercado, \\(\\overline{x}=0.00741\\). Los estudiantes de teoría de intereses reconocen que los rendimientos mensuales se pueden convertir a una base anual usando la capitalización geométrica. Por ejemplo, el rendimiento anual de Lincoln es \\((1.0051)^{12}-1=0.062946\\), o aproximadamente 6.29 por ciento. Esto se compara con un rendimiento anual de 9.26% (= (1\\(00((1.00741)^{12}-1\\))) para el mercado. Una medida de riesgo, o volatilidad, que se usa en finanzas es la desviación estándar. Así, interprete \\(s_y\\) = 0.0859 \\(&gt;\\) 0.05254 = \\(s_x\\) para significar que una inversión en Lincoln es más riesgosa que la del mercado. Otro aspecto interesante de la Tabla 2.8 es que el rendimiento más bajo del mercado, -0.22052, está 4.338 desviaciones estándar por debajo de su promedio ((-0.22052-0.00741)/0.05254 = -4.338). Esto es muy inusual con respecto a una distribución normal. knitr::kable(2, caption = &quot;Silly. Crear una tabla solo para actualizar el contador...&quot;) Table 2.2: Silly. Crear una tabla solo para actualizar el contador… x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Table 2.3: Silly. x 2 knitr::kable(2, caption = &quot;Silly. &quot;) Table 2.4: Silly. x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Table 2.5: Silly. x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Table 2.6: Silly. x 2 Table 2.7: Silly. x 2 Table 2.8: Estadísticas Resumen de 60 Observaciones Mensuales Promedio Mediana Desviación Estándar Mínimo Máximo LINCOLN 0.0051 0.0075 0.0859 -0.2803 0.3147 MARKET 0.0074 0.0142 0.0525 -0.2205 0.1275 Fuente: Center for Research on Security Prices, University of Chicago A continuación, examinamos los datos a lo largo del tiempo, como se muestra gráficamente en la Figura 2.10. Estos son gráficos de dispersión de los rendimientos versus el tiempo, llamados gráficos de series temporales. En la Figura 2.10, se puede ver claramente el rendimiento más bajo del mercado y un vistazo rápido al eje horizontal revela que este punto inusual está en octubre de 1987, el momento del conocido colapso del mercado. Figure 2.10: Gráfico de series temporales de los rendimientos de la Lincoln National Corporation y del mercado. Hay 60 rendimientos mensuales durante el período de enero de 1986 a diciembre de 1990. El gráfico de dispersión en la Figura 2.11 resume gráficamente la relación entre el rendimiento de Lincoln y el rendimiento del mercado. El colapso del mercado es claramente evidente en la Figura 2.11 y representa un punto de alta influencia. Con la línea de regresión (descrita a continuación) superpuesta, los dos puntos atípicos que se pueden ver en la Figura 2.10 también son evidentes. A pesar de estas anomalías, el gráfico en la Figura 2.11 sugiere que hay una relación lineal entre los rendimientos de Lincoln y del mercado. Figure 2.11: Gráfico de dispersión del rendimiento de Lincoln versus el rendimiento del índice S&amp;P 500. La línea de regresión está superpuesta, lo que nos permite identificar el colapso del mercado y dos puntos atípicos. Código R para producir la Tabla 2.8 y las Figuras 2.10 y 2.11 CAPM &lt;- read.csv(&quot;CSVData/CAPM.csv&quot;, header=TRUE) # TABLA 2.8 ESTADÍSTICAS RESUMEN Xymat &lt;- data.frame(cbind(CAPM$LINCOLN,CAPM$MARKET)) tableMat &lt;- BookSummStats(Xymat) colnames(tableMat) &lt;- c(&quot;Promedio&quot; , &quot;Mediana&quot; , &quot;Desviación Estándar&quot; , &quot;Mínimo&quot; , &quot;Máximo&quot;) rownames(tableMat) &lt;- c(&quot;LINCOLN&quot;, &quot;MARKET&quot;) #tableMat1 &lt;- format(round(tableMat, digits=0), big.mark = &#39;,&#39;) TableGen1(TableData=tableMat, TextTitle=&#39;Estadísticas Resumen de 60 Observaciones Mensuales&#39;, Align=&#39;r&#39;, Digits=4, ColumnSpec=1:5, ColWidth = ColWidth5) %&gt;% footnote(general = &quot;Center for Research on Security Prices, University of Chicago&quot;, general_title = &quot;Fuente:&quot;, footnote_as_chunk = TRUE) # FIGURA 2.10 par(mar=c(4.1,3.1,2,1),cex=1.1, las=1) foo &lt;- ts(CAPM, freq = 12, start = c(1986, 1)) ts.plot(foo[,2], foo[,3], xlab=&quot;Año&quot;, ylab=&quot;&quot;, type=&quot;o&quot;, lty=c(1, 2)) mtext(&quot;Rendimiento Mensual&quot;, side=2, at=.38,las=1,cex=1.1, adj=.5) legend(1986, 0.3, c(&quot;LINCOLN&quot;, &quot;MARKET&quot;), lty=1:2, cex=0.5) # FIGURA 2.11 par(mar=c(4.1,3.1,1.4,0.2),cex=1.1, las=1) plot(CAPM$MARKET, CAPM$LINCOLN, xlab=&quot;MARKET&quot;, ylab=&quot;&quot;, xlim=c(-0.3, 0.2), ylim=c(-0.3, 0.4),las=1) mtext(&quot;LINCOLN&quot;, side=2,at=0.46,las=1, cex=1.1, adj=.5) reg &lt;- lm(LINCOLN ~ MARKET, data = CAPM) abline(reg) arrows(-0.22, -0.1, -0.22, -0.22,length=0.1, angle = 10) text(-0.22, -0.08, &quot;COLAPSO DE OCTUBRE, 1987&quot;, cex=0.8) arrows(0.1, 0.02, 0, -0.27,length=0.1, angle = 10) arrows(0.1, 0.02, 0.06, 0.3,length=0.1, angle = 10) text(0.16, 0.02, &quot;PUNTOS ATÍPICOS DE 1990&quot;, cex=0.8) Puntos Inusuales Para resumir la relación entre el mercado y el rendimiento de Lincoln, se ajustó un modelo de regresión. La regresión ajustada es \\[\\begin{equation*} \\widehat{LINCOLN}=-0.00214+0.973 MARKET. \\end{equation*}\\] El error estándar estimado resultante, \\(s = 0.0696\\), es menor que la desviación estándar de los rendimientos de Lincoln, \\(s_y=0.0859\\). Por lo tanto, el modelo de regresión explica parte de la variabilidad de los rendimientos de Lincoln. Además, el estadístico \\(t\\) asociado con la pendiente \\(b_1\\) resulta ser \\(t(b_1)=5.64\\), lo cual es significativamente alto. Un aspecto decepcionante es que el estadístico \\(R^2=35.4\\%\\) se puede interpretar como que el mercado explica solo un poco más de un tercio de la variabilidad. Por lo tanto, aunque el mercado es claramente un determinante importante, como lo evidencian el alto estadístico \\(t\\), solo proporciona una explicación parcial del rendimiento de los rendimientos de Lincoln. En el contexto del modelo de mercado, podemos interpretar la desviación estándar del mercado, \\(s_x\\), como riesgo no diversificable. Por lo tanto, el riesgo de un valor puede descomponerse en dos componentes: el componente diversificable y el componente del mercado, que es no diversificable. La idea es que, al combinar varios valores, podemos crear una cartera de valores que, en la mayoría de los casos, reducirá el riesgo de nuestras inversiones en comparación con un solo valor. Nuevamente, la razón para tener un valor es que estamos compensados con rendimientos esperados más altos al tener un valor con mayor riesgo. Para cuantificar el riesgo relativo, no es difícil demostrar que \\[\\begin{equation} s_y^2 = b_1^2 s_x^2 + s^2 \\frac{n-2}{n-1}. \\tag{2.8} \\end{equation}\\] El riesgo de un valor se debe al riesgo del mercado más el riesgo de un componente diversificable. Tenga en cuenta que el riesgo del componente del mercado, \\(s_x^2\\), es mayor para los valores con pendientes más grandes. Por esta razón, los inversores consideran que los valores con pendientes \\(b_1\\) mayores que uno son “agresivos” y las pendientes menores que uno como “defensivos”. Análisis de Sensibilidad El resumen anterior plantea inmediatamente dos cuestiones adicionales. Primero, ¿cuál es el efecto del colapso de octubre de 1987 en la ecuación de regresión ajustada? Sabemos que las observaciones inusuales, como el colapso, pueden influir mucho en el ajuste. Con este fin, se volvió a ejecutar la regresión sin la observación correspondiente al colapso. La motivación para esto es que el colapso de octubre de 1987 representa una combinación de eventos altamente inusuales (la interacción de varios programas de comercio automatizado operados por grandes casas de corretaje de valores) que no deseamos representar con el mismo modelo que nuestras otras observaciones. Eliminando esta observación, la regresión ajustada es \\[\\begin{equation*} \\widehat{LINCOLN} = -0.00181 + 0.956 MARKET, \\end{equation*}\\] con \\(R^2=26.4\\%\\), \\(t(b_1)=4.52\\), \\(s=0.0702\\) y \\(s_y=0.0811\\). Interpretamos estas estadísticas de la misma manera que el modelo ajustado que incluye el colapso de octubre de 1987. Sin embargo, es interesante notar que la proporción de variabilidad explicada ha disminuido al excluir el punto influyente. Esto sirve para ilustrar un punto importante. Los puntos de alta influencia a menudo son temidos por los analistas de datos porque, por definición, son diferentes de otras observaciones en el conjunto de datos y requieren una atención especial. Sin embargo, al ajustar las relaciones entre variables, también representan una oportunidad porque permiten al analista de datos observar la relación entre variables en rangos más amplios que de otro modo serían posibles. La desventaja es que estas relaciones pueden ser no lineales o seguir un patrón completamente diferente en comparación con las relaciones observadas en la parte principal de los datos. La segunda pregunta planteada por el análisis de regresión es qué se puede decir sobre las circunstancias inusuales que dieron lugar al comportamiento inusual de los rendimientos de Lincoln en octubre y noviembre de 1990. Una característica útil del análisis de regresión es identificar y plantear la pregunta; no la resuelve. Debido a que el análisis señala claramente dos puntos altamente inusuales, sugiere al analista de datos que vuelva y haga algunas preguntas específicas sobre las fuentes de los datos. En este caso, la respuesta es directa. En octubre de 1990, la compañía Travelers’ Insurance, una competidora, anunció que tomaría una gran amortización en su cartera de bienes raíces debido a un número sin precedentes de incumplimientos hipotecarios. El mercado reaccionó rápidamente a esta noticia, y los inversores asumieron que otras grandes compañías de seguros de vida también anunciarían pronto grandes amortizaciones. Anticipando esta noticia, los inversores trataron de vender sus carteras de, por ejemplo, las acciones de Lincoln, lo que provocó una caída en el precio. Sin embargo, resultó que los inversores reaccionaron en exceso a esta noticia y que la cartera de bienes raíces de Lincoln estaba en realidad en buen estado. Así, los precios rápidamente volvieron a sus niveles históricos. 2.8 Salida Computacional Ilustrativa de Regresión Las computadoras y los paquetes de software estadístico que realizan cálculos especializados juegan un papel vital en los análisis estadísticos modernos. Las capacidades informáticas económicas han permitido a los analistas de datos centrarse en las relaciones de interés. Es mucho menos importante especificar modelos que sean atractivos únicamente por su simplicidad computacional en comparación con épocas anteriores a la disponibilidad generalizada de computación económica. Un tema importante de este texto es centrarse en las relaciones de interés y confiar en el software estadístico ampliamente disponible para estimar los modelos que especificamos. Con cualquier paquete de computadora, generalmente las partes más difíciles de operar el paquete son (i) la entrada, (ii) el uso de los comandos y (iii) la interpretación de la salida. Encontrarás que la mayoría de los paquetes estadísticos modernos aceptan archivos en formato de hoja de cálculo o texto, lo que facilita la entrada de datos. Los paquetes de software estadístico para computadoras personales tienen lenguajes de comando basados en menús con facilidades de ayuda en línea fácilmente accesibles. Una vez que decides qué hacer, encontrar los comandos correctos es relativamente fácil. Esta sección proporciona orientación para interpretar la salida de los paquetes estadísticos. La mayoría de los paquetes estadísticos generan salidas similares. A continuación, se presentan tres ejemplos de paquetes estadísticos estándar: EXCEL, SAS y R. El símbolo de anotación “[.]” marca una cantidad estadística que se describe en la leyenda. Así, esta sección proporciona un enlace entre la notación utilizada en el texto y la salida de algunos de los paquetes estadísticos estándar. Salida en EXCEL Regression Statistics Multiple R 0.886283[F] R Square 0.785497[k] Adjusted R Square 0.781028[l] Standard Error 3791.758[j] Observations 50[a] ANOVA df SS MS F Significance F Regression 1[m] 2527165015 [p] 2527165015 [s] 175.773[u] 1.15757E-17[v] Residual 48[n] 690116754.8[q] 14377432.39[t] Total 49[o] 3217281770 [r] Coefficients Standard Error t Stat P-value Intercept 469.7036[b] 702.9061896[d] 0.668230846[f] 0.507187[h] X Variable 1 0.647095[c] 0.048808085[e] 13.25794257[g] 1.16E-17[i] El Sistema SAS The REG Procedure Dependent Variable: SALES Analysis of Variance Sum of Mean Source DF Squares Square F Value Pr &gt; F Model 1[m] 2527165015[p] 2527165015[s] 175.77[u] &lt;.0001[v] Error 48[n] 690116755[q] 14377432[t] Corrected Total 49[o] 3217281770[r] Root MSE 3791.75848[j] R-Square 0.7855[k] Dependent Mean 6494.82900[H] Adj R-Sq 0.7810[l] Coeff Var 58.38119[I] Parameter Estimates Parameter Standard Variable Label DF Estimate Error t Value Pr &gt; |t| Intercept Intercept 1 469.70360[b] 702.90619[d] 0.67[f] 0.5072[h] POP POP 1 0.64709[c] 0.04881[e] 13.26[g] &lt;.0001[i] Salida en R Analysis of Variance Table Response: SALES Df Sum Sq Mean Sq F value Pr(&gt;F) POP 1[m] 2527165015[p] 2527165015[s] 175.77304[u] &lt;2.22e-16[v]*** Residuals 48[n] 690116755[q] 14377432[t] --- Call: lm(formula = SALES ~ POP) Residuals: Min 1Q Median 3Q Max -6047 -1461 -670 486 18229 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 469.7036[b] 702.9062[d] 0.67[f] 0.51 [h] POP 0.6471[c] 0.0488[e] 13.26[g] &lt;2e-16 ***[i] --- Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 Residual standard error: 3792[j] on 48[n] degrees of freedom Multiple R-Squared: 0.785[k], Adjusted R-squared: 0.781[l] F-statistic: 176[u] on 1[m] and 48[n] DF, p-value: &lt;2e-16[v] Definición de Anotación de Leyenda, Símbolo [a] Número de observaciones \\(n\\). [b] La intersección estimada \\(b_0\\). [c] La pendiente estimada \\(b_1\\). [d] El error estándar de la intersección, \\(se(b_0)\\). [e] El error estándar de la pendiente, \\(se(b_1)\\). [f] El valor del \\(t\\) asociado con la intersección, \\(t(b_0) = b_0/se(b_0)\\). [g] El valor del \\(t\\) asociado con la pendiente, \\(t(b_1) = b_1/se(b_1)\\). [h] El valor \\(p\\) asociado con la intersección; aquí, \\(p-value=Pr(|t_{n-2}|&gt;|t(b_0)|)\\), donde \\(t(b_0)\\) es el valor realizado (0.67 aquí) y \\(t_{n-2}\\) tiene una distribución \\(t\\) con \\(df=n-2\\). [i] El valor \\(p\\) asociado con la pendiente; aquí, \\(p-value=Pr(|t_{n-2}|&gt;|t(b_1)|)\\), donde \\(t(b_1)\\) es el valor realizado (13.26 aquí) y \\(t_{n-2}\\) tiene una distribución \\(t\\) con \\(df=n-2\\). [j] La desviación estándar residual, \\(s\\). [k] El coeficiente de determinación, \\(R^2\\). [l] El coeficiente de determinación ajustado por grados de libertad, \\(R_{a}^2\\). (Este término se definirá en el Capítulo 3.) [m] Grados de libertad para el componente de regresión. Esto es 1 para una variable explicativa. [n] Grados de libertad para el componente de error, \\(n-2\\), para la regresión con una variable explicativa. [o] Grados de libertad totales, \\(n-1\\). [p] La suma de cuadrados de la regresión, \\(Regression~SS\\). [q] La suma de cuadrados del error, \\(Error~SS\\). [r] La suma total de cuadrados, \\(Total~SS\\). [s] El cuadrado medio de la regresión, \\(Regression~MS = Regression~SS/1\\), para una variable explicativa. [t] El cuadrado medio del error, \\(s^2=Error~MS = Error~SS/(n-2)\\), para una variable explicativa. [u] El \\(F-ratio=(Regression~MS)/(Error~MS)\\). (Este término se definirá en el Capítulo 3.) [v] El valor \\(p\\) asociado con el \\(F-ratio\\). (Este término se definirá en el Capítulo 3.) [w] El número de observación, \\(i\\). [x] El valor de la variable explicativa para la \\(i\\)-ésima observación, \\(x_i\\). [y] La respuesta para la \\(i\\)-ésima observación, \\(y_i\\). [z] El valor ajustado para la \\(i\\)-ésima observación, \\(\\widehat{y}_i\\). [A] El error estándar del ajuste, \\(se(\\widehat{y}_i)\\). [B] El residual para la \\(i\\)-ésima observación, \\(e_i\\). [C] El residual estandarizado para la \\(i\\)-ésima observación, \\(e_i/se(e_i)\\). El error estándar \\(se(e_i)\\) se definirá en la Sección 5.3.1. [F] El coeficiente de correlación múltiple es la raíz cuadrada del coeficiente de determinación, \\(R=\\sqrt{R^2}\\). Esto se definirá en el Capítulo 3. [G] El coeficiente estandarizado es \\(b_1s_x/s_y\\). Para regresión con una variable explicativa, esto es equivalente a \\(r\\), el coeficiente de correlación. [H] La respuesta promedio, \\(\\overline{y}\\). [I] El coeficiente de variación de la respuesta es \\(s_y/\\overline{y}\\). SAS imprime \\(100s_y/\\overline{y}\\). 2.9 Lecturas Adicionales y Referencias Relativamente pocas aplicaciones de la regresión son básicas en el sentido de que usan solo una variable explicativa; el propósito del análisis de regresión es reducir las relaciones complejas entre muchas variables. La Sección 2.7 describe una excepción importante a esta regla general, el modelo financiero CAPM; consulta a Panjer et al. (1998) para descripciones actuariales adicionales de este modelo. Campbell et al. (1997) ofrece una perspectiva desde la econometría financiera. Referencias del Capítulo Anscombe, Frank (1973). Graphs in statistical analysis. The American Statistician 27, 17-21. Campbell, John Y., Andrew W. Lo and A. Craig MacKinlay (1997). The Econometrics of Financial Markets. Princeton University Press, Princeton, New Jersey. Frees, Edward W. and Tom W. Miller (2003). Sales forecasting using longitudinal data models. International Journal of Forecasting 20, 97-111. Goldberger, Arthur (1991). A Course in Econometrics. Harvard University Press, Cambridge. Koch, Gary J. (1985). A basic demonstration of the [-1, 1] range for the correlation coefficient. American Statistician 39, 201-202. Linter, J. (1965). The valuation of risky assets and the selection of risky investments in stock portfolios and capital budgets. Review of Economics and Statistics, 13-37. Manistre, B. John and Geoffrey H. Hancock (2005). Variance of the CTE estimator. North American Actuarial Journal 9(2), 129-156. Markowitz, Harry (1959). Portfolio Selection: Efficient Diversification of Investments. John Wiley, New York. Panjer, Harry H., Phelim P. Boyle, Samuel H. Cox, Daniel Dufresne, Hans U. Gerber, Heinz H. Mueller, Hal W. Pedersen, Stanley R. Pliska, Michael Sherris, Elias S. Shiu and Ken S. Tan (1998). Financial Economics: With Applications to Investment, Insurance and Pensions. Society of Actuaries, Schaumburg, Illinois. Pearson, Karl (1895). Royal Society Proceedings 58, 241. Serfling, Robert J. (1980). Approximation Theorems of Mathematical Statistics. John Wiley and Sons, New York. Sharpe, William F. (1964). Capital asset prices: A theory of market equilibrium under risk. Journal of Finance, 425-442. Stigler, Steven M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. Harvard University Press, Cambridge, MA. 2.10 Ejercicios Secciones 2.1-2.2 2.1 Considera el siguiente conjunto de datos \\[ \\begin{array}{l|ccc} \\hline i &amp; 1 &amp; 2 &amp; 3 \\\\ \\hline x_i &amp; 2 &amp; -6 &amp; 7 \\\\ y_i &amp; 3 &amp; 4 &amp; 6\\\\ \\hline \\end{array} \\] Ajusta una línea de regresión utilizando el método de mínimos cuadrados. Determina \\(r\\), \\(b_1\\) y \\(b_0\\). 2.2 Una relación perfecta, pero sin correlación. Considera la relación cuadrática \\(y=x^2\\), con datos \\[ \\begin{array}{l|ccccc} \\hline i &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5\\\\ \\hline x_i &amp; -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 \\\\ y_i &amp; 4 &amp; 1 &amp; 0 &amp; 1 &amp; 4\\\\ \\hline \\end{array} \\] Produce un gráfico aproximado para este conjunto de datos. Verifica que el coeficiente de correlación es \\(r=0\\). 2.3 Acotación del coeficiente de correlación. Utiliza los siguientes pasos para demostrar que \\(r\\) está acotado entre -1 y 1 (Estos pasos son de Koch, 1990). Deja que \\(a\\) y \\(c\\) sean constantes genéricas. Verifica \\[\\begin{eqnarray*} 0 &amp; \\leq &amp; \\frac{1}{n-1}\\sum_{i=1}^{n}\\left( a\\frac{x_i-\\overline{x}}{s_x}-c \\frac{y_i-\\overline{y}}{s_y}\\right) ^2 \\\\ &amp;=&amp; a^2+c^2-2acr. \\end{eqnarray*}\\] Utiliza los resultados del apartado (a) para demostrar que \\(2ac(r-1)\\leq (a-c)^2.\\) Al tomar \\(a=c\\), utiliza el resultado del apartado (b) para demostrar que \\(r\\leq 1\\). Al tomar \\(a=-c\\), utiliza los resultados del apartado (b) para demostrar que \\(r\\geq -1\\). ¿En qué condiciones es \\(r=-1\\)? ¿En qué condiciones es \\(r=1\\)? 2.4 Los coeficientes de regresión son sumas ponderadas. Demuestra que el término de intercepto, \\(b_0\\), puede expresarse como una suma ponderada de las variables dependientes. Es decir, demuestra que \\(b_0=\\sum_{i=1}^{n}w_{i,0}y_i.\\) Además, expresa los pesos en términos de los pesos de la pendiente, \\(w_i\\). 2.5 Otra expresión para la pendiente como una suma ponderada Utilizando álgebra, establece una expresión alternativa \\[\\begin{equation*} b_1=\\frac{\\sum_{i=1}^{n}weight_i~slope_i}{ \\sum_{i=1}^{n}weight_i}. \\end{equation*}\\] Aquí, \\(slope_i\\) es la pendiente entre \\((x_i,y_i)\\) y \\((\\bar{x},\\bar{y})\\). Da una forma precisa para el peso \\(weight_i\\) como una función de la variable explicativa \\(x\\). Supón que \\(\\bar{x} = 4, \\bar{y} = 3, x_1 = 2 \\text{ y } y_1= 6\\). Determina la pendiente y el peso para la primera observación, es decir, \\(slope_1\\) y \\(weight_1\\). 2.6 Considera dos variables, \\(y\\) y \\(x\\). Realiza una regresión de \\(y\\) sobre \\(x\\) para obtener un coeficiente de pendiente que llamaremos \\(b_{1,x,y}\\). Realiza otra regresión de \\(x\\) sobre \\(y\\) para obtener un coeficiente de pendiente que llamaremos \\(b_{1,y,x}\\). Demuestra que el coeficiente de correlación entre \\(x\\) y \\(y\\) es la media geométrica de los dos coeficientes de pendiente según el signo, es decir, demuestra que \\(|r|=\\sqrt{ b_{1,x,y}b_{1,y,x}}.\\) 2.7 Regresión a través del origen. Considera el modelo \\(y_i=\\beta_1 x_i + \\varepsilon _i\\), es decir, regresión con una variable explicativa sin el término de intercepto. Este modelo se llama regresión a través del origen porque la verdadera línea de regresión \\(\\mathrm{E}y = \\beta_1 x\\) pasa por el origen (el punto (0, 0)). Para este modelo, la estimación de mínimos cuadrados de \\(\\beta_1\\) es ese número \\(b_1\\) que minimiza la suma de cuadrados \\(\\mathrm{SS}(b_1^{\\ast} )=\\sum_{i=1}^{n}\\left( y_i - b_1^{\\ast}x_i\\right) ^2.\\) Verifica que \\[\\begin{equation*} b_1 = \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sum_{i=1}^{n}x_i^2}. \\end{equation*}\\] Considera el modelo \\(y_i=\\beta_1 z_i^2 + \\varepsilon _i\\), un modelo cuadrático que pasa por el origen. Utiliza el resultado del apartado (a) para determinar la estimación de mínimos cuadrados de \\(\\beta_1\\). 2.8 a. Demuestra que \\[\\begin{equation*} s_y^2=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^2= \\frac{1}{n-1}\\left( \\sum_{i=1}^{n}y_i^2-n\\overline{y}^2\\right) . \\end{equation*}\\] Sigue los mismos pasos para demostrar que \\(\\sum_{i=1}^{n}\\left( y_i - \\overline{y} \\right) \\left( x_i-\\overline{x}\\right) =\\sum_{i=1}^{n} x_i y_i - n \\overline{x}~\\overline{y}.\\) Demuestra que \\[ b_{1}=\\frac{\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) \\left( x_i- \\overline{x}\\right) }{\\sum_{i=1}^{n}\\left( x_i - \\overline{x} \\right) ^2} \\] Establece la fórmula comúnmente utilizada \\[ b_{1}= \\frac{\\sum_{i=1}^{n}x_iy_i-n\\overline{x}~\\overline{y}} {\\sum_{i=1}^{n}x_i^2 - n\\overline{x}^2}. \\] 2.9 Interpretación de los coeficientes asociados con una variable explicativa binaria. Supón que \\(x_i\\) solo toma los valores 0 y 1. De las \\(n\\) observaciones, \\(n_1\\) toman el valor \\(x=0\\). Estas \\(n_1\\) observaciones tienen un valor promedio \\(y\\) de \\(\\overline{y}_1\\). Las restantes \\(n-n_1\\) observaciones tienen el valor \\(x=1\\) y un valor promedio \\(y\\) de \\(\\overline{y}_2\\). Utiliza el Ejercicio 2.8 para demostrar que \\(b_1 = \\overline{y}_2 - \\overline{y}_1.\\) 2.10 Utilización de Hogares de Cuidado. Este ejercicio considera los datos de hogares de cuidado proporcionados por el Departamento de Salud y Servicios Familiares de Wisconsin (DHFS) y descritos en el Ejercicio 1.2. Parte 1: Utiliza los datos del año 2000 y realiza el siguiente análisis. Correlaciones a(i). Calcula la correlación entre TPY y LOGTPY. Comenta tu resultado. a(ii). Calcula la correlación entre TPY, NUMBED y SQRFOOT. ¿Parecen estas variables altamente correlacionadas? a(iii). Calcula la correlación entre TPY y NUMBED/10. Comenta tu resultado. Diagramas de dispersión. Grafica TPY versus NUMBED y TPY versus SQRFOOT. Comenta los gráficos. Regresión lineal básica. c(i). Ajusta un modelo de regresión lineal básico usando TPY como variable de resultado y NUMBED como variable explicativa. Resume el ajuste citando el coeficiente de determinación, \\(R^2\\), y el estadístico \\(t\\) para NUMBED. c(ii). Repite c(i), usando SQRFOOT en lugar de NUMBED. En términos de \\(R^2\\), ¿cuál modelo se ajusta mejor? c(iii). Repite c(i), usando LOGTPY como variable de resultado y LOG(NUMBED) como variable explicativa. c(iv). Repite c(iii), usando LOGTPY como variable de resultado y LOG(SQRFOOT) como variable explicativa. Parte 2: Ajusta el modelo en la Parte 1.c(i) usando datos de 2001. ¿Son los patrones estables a lo largo del tiempo? Secciones 2.3-2.4 2.11 Supón que, para un tamaño de muestra de \\(n\\) = 3, tienes \\(e_2\\) = 24 y \\(e_{3}\\) = -1. Determina \\(e_{1}\\). 2.12 Supón que \\(r=0\\), \\(n=15\\) y \\(s_y = 10\\). Determina \\(s\\). 2.13 El coeficiente de correlación y el coeficiente de determinación. Usa los siguientes pasos para establecer una relación entre el coeficiente de determinación y el coeficiente de correlación. Muestra que \\(\\widehat{y}_i-\\overline{y}=b_1(x_i-\\overline{x}).\\) Usa el apartado (a) para mostrar que \\(Regress~SS=\\sum_{i=1}^{n}\\left(\\widehat{y}_i - \\overline{y} \\right)^2 = b_1^2s_x^2(n-1).\\) Usa el apartado (b) para establecer que \\(R^2=r^2.\\) 2.14 Muestra que el residuo promedio es cero, es decir, muestra que \\(n^{-1}\\sum_{i=1}^{n} e_i=0.\\) 2.15 Correlación entre residuos y variables explicativas. Considera una secuencia genérica de pares de números \\((x_1,y_1)\\), …, \\((x_n,y_n)\\) con el coeficiente de correlación calculado como \\(r(y,x)=\\left[ (n-1)s_ys_x\\right] ^{-1}\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) \\left( x_i-\\overline{x}\\right) .\\) Supón que \\(\\overline{y}=0\\), \\(\\overline{x}=0\\) o ambos \\(\\overline{x}\\) y \\(\\overline{y}=0\\). Luego, verifica que \\(r(y,x)=0\\) implica \\(\\sum_{i=1}^{n}y_i x_i=0\\) y viceversa. Muestra que la correlación entre los residuos y las variables explicativas es cero. Haz esto usando la parte (a) del Ejercicio 2.13 para mostrar que \\(\\sum_{i=1}^{n} x_i e_i = 0\\) y luego aplica la parte (a). Muestra que la correlación entre los residuos y los valores ajustados es cero. Haz esto mostrando que \\(\\sum_{i=1}^n \\widehat{y}_i e_i = 0\\) y luego aplica la parte (a). 2.16 Correlación y estadísticas \\(t\\). Usa los siguientes pasos para establecer una relación entre el coeficiente de correlación y el estadístico \\(t\\) para la pendiente. Usa álgebra para verificar que \\[\\begin{equation*} R^2=1-\\frac{n-2}{n-1}\\frac{s^2}{s_y^2}. \\end{equation*}\\] Usa la parte (a) para establecer la siguiente fórmula rápida para \\(s\\), \\[\\begin{equation*}s = s_y \\sqrt{(1-r^2)\\frac{n-1}{n-2}}.\\end{equation*}\\] Usa la parte (b) para mostrar que \\[\\begin{equation*} t(b_1) = \\sqrt{n-2}\\frac{r}{\\sqrt{1-r^2}}. \\end{equation*}\\] 2.17 Efectos de un punto inusual. Estás analizando un conjunto de datos de tamaño \\(n=100\\). Has realizado un análisis de regresión usando una variable predictora y notas que el residuo para la décima observación es inusualmente grande. Supón que, de hecho, resulta que \\(e_{10}=8s\\). ¿Qué porcentaje de la suma de cuadrados de los errores, \\(Error~SS\\), se debe a la décima observación? Supón que \\(e_{10}=4s\\). ¿Qué porcentaje de la suma de cuadrados de errores, \\(Error~SS\\), se debe a la décima observación? Supón que reduces el conjunto de datos a tamaño \\(n=20\\). Después de realizar la regresión, resulta que todavía tenemos \\(e_{10}=4s\\). ¿Qué porcentaje de la suma de cuadrados de errores, \\(Error~SS\\), se debe a la décima observación? 2.18 Considera un conjunto de datos de 20 observaciones con las siguientes estadísticas resumen: \\(\\overline{x}=0\\), \\(\\overline{y}=9\\), \\(s_x=1\\) y \\(s_y=10\\). Realizas una regresión usando una variable y determinas que \\(s=7\\). Determina el error estándar de una predicción en \\(x_{\\ast}=1.\\) 2.19 Las estadísticas resumen pueden ocultar relaciones importantes. Los datos en Tabla 2.9 son de Anscombe (1973). El propósito de este ejercicio es demostrar cómo graficar los datos puede revelar información importante que no es evidente en las estadísticas numéricas resumen. Tabla 2.9. Datos de Anscombe (1973) \\[ {\\small \\begin{array}{c|rrrrrr} \\hline obs &amp; &amp; &amp; &amp; &amp; &amp; \\\\ num &amp; x_1 &amp; y_1 &amp; y_2 &amp; y_3 &amp; x_2 &amp; y_4 \\\\ \\hline 1 &amp; 10 &amp; 8.04 &amp; 9.14 &amp; 7.46 &amp; 8 &amp; 6.58 \\\\ 2 &amp; 8 &amp; 6.95 &amp; 8.14 &amp; 6.77 &amp; 8 &amp; 5.76 \\\\ 3 &amp; 13 &amp; 7.58 &amp; 8.74 &amp; 12.74 &amp; 8 &amp; 7.71 \\\\ 4 &amp; 9 &amp; 8.81 &amp; 8.77 &amp; 7.11 &amp; 8 &amp; 8.84 \\\\ 5 &amp; 11 &amp; 8.33 &amp; 9.26 &amp; 7.81 &amp; 8 &amp; 8.47 \\\\ 6 &amp; 14 &amp; 9.96 &amp; 8.10 &amp; 8.84 &amp; 8 &amp; 7.04 \\\\ 7 &amp; 6 &amp; 7.24 &amp; 6.13 &amp; 6.08 &amp; 8 &amp; 5.25 \\\\ 8 &amp; 4 &amp; 4.26 &amp; 3.10 &amp; 5.39 &amp; 8 &amp; 5.56 \\\\ 9 &amp; 12 &amp; 10.84 &amp; 9.13 &amp; 8.15 &amp; 8 &amp; 7.91 \\\\ 10 &amp; 7 &amp; 4.82 &amp; 7.26 &amp; 6.42 &amp; 8 &amp; 6.89 \\\\ 11 &amp; 5 &amp; 5.68 &amp; 4.74 &amp; 5.73 &amp; 19 &amp; 12.50 \\\\ \\hline \\end{array} } \\] Calcula los promedios y desviaciones estándar de cada columna de datos. Verifica que los promedios y desviaciones estándar de cada una de las columnas \\(x\\) son iguales, dentro de dos decimales, y de manera similar para cada una de las columnas \\(y\\). Realiza cuatro regresiones, (1) \\(y_{1}\\) sobre \\(x_{1}\\), (2) \\(y_2\\) sobre \\(x_{1}\\), (3) \\(y_{3}\\) sobre \\(x_{1}\\) y (4) \\(y_{4}\\) sobre \\(x_2\\). Verifica, para cada uno de los cuatro ajustes de regresión, que \\(b_0\\approx 3.0\\), \\(b_{1}\\approx 0.5\\), \\(s\\approx 1.237\\) y \\(R^2\\approx 0.677\\), dentro de dos decimales. Produce diagramas de dispersión para cada uno de los cuatro modelos de regresión que ajustaste en el apartado (b). Discute el hecho de que los modelos de regresión ajustados en el apartado (b) implican que los cuatro conjuntos de datos son similares, aunque los cuatro diagramas de dispersión producidos en el apartado (c) muestran una historia dramáticamente diferente. 2.20 Utilización de Hogares de Cuidado. Este ejercicio considera los datos de hogares de cuidado proporcionados por el Departamento de Salud y Servicios Familiares de Wisconsin (DHFS) y descritos en el Ejercicio 1.2 y 2.10. Decides examinar la relación entre los años totales de pacientes (LOGTPY) y el número de camas (LOGNUMBED), ambos en unidades logarítmicas, usando datos del año 2001. Estadísticas descriptivas. Crea estadísticas descriptivas básicas para cada variable. Resume la relación mediante un estadístico de correlación y un diagrama de dispersión. Ajusta el modelo lineal básico. Cita las estadísticas descriptivas básicas, incluye el coeficiente de determinación, el coeficiente de regresión para LOGNUMBED y el estadístico \\(t\\) correspondiente. Pruebas de hipótesis. Prueba las siguientes hipótesis al nivel de significancia del 5% usando un estadístico \\(t\\). También calcula el valor \\(p\\) correspondiente. c(i). Prueba \\(H_0: \\beta_1 = 0\\) frente a \\(H_a: \\beta_1 \\neq 0\\). c(ii). Prueba \\(H_0: \\beta_1 = 1\\) frente a \\(H_a: \\beta_1 \\neq 1\\). c(iii). Prueba \\(H_0: \\beta_1 = 1\\) frente a \\(H_a: \\beta_1 &gt; 1\\). c(iv). Prueba \\(H_0: \\beta_1 = 1\\) frente a \\(H_a: \\beta_1 &lt; 1\\). Estás interesado en el efecto que un cambio marginal en LOGNUMBED tiene sobre el valor esperado de LOGTPY. d(i). Supón que hay un cambio marginal en LOGNUMBED de 2. Proporciona una estimación puntual del cambio esperado en LOGTPY. d(ii). Proporciona un intervalo de confianza del 95% correspondiente a la estimación puntual en la parte d(i). d(iii). Proporciona un intervalo de confianza del 99% correspondiente a la estimación puntual en la parte d(i). En un número especificado de camas estimado en \\(x_{*} = 100\\), haz lo siguiente: e(i). Encuentra el valor predicho de LOGTPY. e(ii). Obtén el error estándar de la predicción. e(iii). Obtén un intervalo de predicción del 95% para tu predicción. e(iv). Convierte la predicción puntual en la parte e(i) y el intervalo de predicción obtenido en la parte e(iii) en años totales de personas (mediante exponenciación). e(v). Obtén un intervalo de predicción como en la parte e(iv), correspondiente a un nivel del 90% (en lugar del 95%). 2.21 Ofertas Públicas Iniciales. Como analista financiero, deseas convencer a un cliente de las ventajas de invertir en empresas que acaban de ingresar a una bolsa de valores, en una OPI (oferta pública inicial). Por lo tanto, reúnes datos de 116 empresas que fijaron precios durante el período de seis meses del 1 de enero de 1998 al 1 de junio de 1998. Al mirar estos datos históricos recientes, puedes calcular RETURN, el retorno de la empresa en un año (en porcentaje). También estás interesado en observar características financieras de la empresa que puedan ayudarte a entender (y predecir) el retorno. Inicialmente examinas REVENUE, los ingresos de la empresa en 1997 en millones de dólares. Desafortunadamente, esta variable no estaba disponible para seis empresas. Por lo tanto, las estadísticas a continuación son para las 110 empresas que tienen tanto REVENUE como RETURN. Además, la Tabla 2.9 proporciona información sobre los ingresos logarítmicos (naturales), denominados como LnREV, y el precio inicial de la acción, denominado PRICEIPO. Table 2.9: Estadísticas Resumen de Cada Variable Media Mediana Desviación Estándar Mínimo Máximo RETURN 0.106 -0.130 0.824 -0.938 4.333 REV 134.487 39.971 261.881 0.099 1455.761 LnREV 3.686 3.688 1.698 -2.316 7.283 PRICEIPO 13.195 13.000 4.694 4.000 29.000 Hipotetizas que las empresas más grandes, medida por ingresos, son más estables y, por lo tanto, deberían tener mayores retornos. Has determinado que la correlación entre RETURN y REVENUE es -0.0175. a(i). Calcula el ajuste de mínimos cuadrados usando REVENUE para predecir RETURN. Determina \\(b_0\\) y \\(b_1\\). a(ii). Para Hyperion Telecommunications, los ingresos son 95.55 (millones de dólares). Calcula el RETURN ajustado usando el ajuste de regresión en la parte a(i). Tabla 2.11. Resultados de la Regresión con Ingresos Logarítmicos \\[ {\\small \\begin{array}{l|rrr} \\hline &amp; &amp; \\text{Error} &amp; \\\\ \\text{Variable} &amp; \\text{Coeficiente} &amp; \\text{Estándar} &amp; t-\\text{estadístico} \\\\ \\hline \\text{INTERCEPTO} &amp; 0.438 &amp; 0.186 &amp; 2.35\\\\ \\text{LnREV} &amp; -0.090 &amp; 0.046 &amp; -1.97 \\\\ \\hline s = 0.8136, &amp; R^2 = 0.03452 \\\\ \\hline \\end{array} } \\] Ingresos logarítmicos y retornos. b(i). Supón que usas LnREV para predecir RETURN. Calcula el RETURN ajustado bajo este modelo de regresión. ¿Es igual a tu respuesta en la parte a(ii)? b(ii) ¿Afectan significativamente los ingresos logarítmicos a los retornos? Para ello, proporciona una prueba formal de hipótesis. Expón tus hipótesis nula y alternativa, el criterio de toma de decisiones y la regla de toma de decisiones. Usa un nivel de significancia del 10%. b(iii). Hipotetizas que, manteniendo todo constante, las empresas con mayores ingresos serán más estables y, por lo tanto, tendrán un mayor retorno inicial. Por lo tanto, deseas considerar la hipótesis nula de ninguna relación entre LnREV y RETURN frente a la hipótesis alternativa de que hay una relación positiva entre LnREV y RETURN. Para ello, proporciona una prueba formal de hipótesis. Expón tus hipótesis nula y alternativa, el criterio de toma de decisiones y la regla de toma de decisiones. Usa un nivel de significancia del 10%. Determina la correlación entre LnREV y RETURN. Asegúrate de indicar si esta correlación es positiva, negativa o cero. Estás considerando invertir en una empresa que tiene LnREV = 2 (por lo que los ingresos son \\(e^2\\) = 7.389 millones de dólares). d(i). Usando el modelo de regresión ajustado, determina la predicción puntual de mínimos cuadrados. d(ii). Determina el intervalo de predicción del 95% correspondiente a tu predicción en la parte d(i). El \\(R^2\\) del modelo de regresión ajustado es un decepcionante 3.5%. Parte de la dificultad se debe a la observación número 59, la Corporación Inktomi. Las ventas de Inktomi están en el 12º lugar más bajo del conjunto de datos, con LnREV = 1.76 (por lo que los ingresos son \\(e^{1.76} = 5.79\\) millones de dólares), pero tiene el mayor retorno en el primer año, con RETURN = 433.33. e(i). Calcula el residuo para esta observación. e(ii). ¿Qué proporción de la variabilidad no explicada (suma de cuadrados de errores) representa esta observación? e(iii). Define la idea de una observación de alto apalancamiento. e(iv). ¿Se consideraría esta observ ación como una observación de alto apalancamiento? Justifica tu respuesta. 2.22 Esperanzas de Vida Nacionales. Continuamos el análisis iniciado en el Ejercicio 1.7 examinando la relación entre \\(y= LIFEEXP\\) y \\(x=FERTILITY\\), mostrado en la Figura 2.12. Ajusta un modelo de regresión lineal de \\(LIFEEXP\\) usando la variable explicativa \\(x=FERTILITY\\). Figure 2.12: Gráfico de FERTILITY versus LIFEEXP. EE.UU. tiene una tasa de FERTILITY de 2.0. Determina la esperanza de vida ajustada. La nación insular Dominica no reportó una tasa de FERTILITY y, por lo tanto, no se incluyó en la regresión. Supón que su tasa de FERTILITY es 2.0. Proporciona un intervalo de predicción del 95% para la esperanza de vida en Dominica. China tiene una tasa de FERTILITY de 1.7 y una esperanza de vida de 72.5. Determina el residuo bajo el modelo. ¿Cuántos múltiplos de \\(s\\) está este residuo alejado de cero? Supón que tu hipótesis previa es que la pendiente de FERTILITY es -6.0 y deseas probar la hipótesis nula de que la pendiente ha aumentado (es decir, la pendiente es mayor que -6.0). Prueba esta hipótesis al nivel de significancia del 5%. También calcula un valor \\(p\\) aproximado. 2.11 Suplemento Técnico - Elementos del Álgebra de Matrices Los ejemplos son una herramienta excelente para introducir temas técnicos como la regresión. Sin embargo, este capítulo también ha utilizado álgebra, así como probabilidad y estadística básica, para darte una comprensión más profunda del análisis de regresión. A partir de ahora, estudiaremos relaciones multivariantes. Con muchas cosas ocurriendo simultáneamente en varias dimensiones, el álgebra ya no es útil para proporcionar información. En cambio, necesitaremos el álgebra de matrices. Este suplemento ofrece una breve introducción al álgebra de matrices para que puedas estudiar los capítulos de regresión lineal de este texto. El Apéndice A3 define conceptos adicionales de matrices. 2.11.1 Definiciones Básicas Una matriz es una tabla rectangular de números organizados en filas y columnas (el plural de matriz es matrices). Por ejemplo, considera los ingresos y la edad de 3 personas. \\[\\begin{equation*} \\mathbf{A}= \\begin{array}{c} Fila~1 \\\\ Fila~2 \\\\ Fila~3 \\end{array} \\overset{ \\begin{array}{cc} ~~~Col~1~ &amp; Col~2 \\end{array} }{\\left( \\begin{array}{cc} 6,000 &amp; 23 \\\\ 13,000 &amp; 47 \\\\ 11,000 &amp; 35 \\end{array} \\right) } \\end{equation*}\\] Aquí, la columna 1 representa el ingreso y la columna 2 representa la edad. Cada fila corresponde a un individuo. Por ejemplo, el primer individuo tiene 23 años y un ingreso de $6,000. El número de filas y columnas se llama la dimensión de la matriz. Por ejemplo, la dimensión de la matriz \\(\\mathbf{A}\\) anterior es \\(3\\times 2\\) (se lee 3 “por” 2). Esto significa 3 filas y 2 columnas. Si quisiéramos representar los ingresos y la edad de 100 personas, entonces la dimensión de la matriz sería \\(100\\times 2\\). Es conveniente representar una matriz usando la notación \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\\\ a_{31} &amp; a_{32} \\end{array} \\right) . \\end{equation*}\\] Aquí, \\(a_{ij}\\) es el símbolo para el número en la \\(i\\)-ésima fila y \\(j\\)-ésima columna de \\(\\mathbf{A}\\). En general, trabajamos con matrices de la forma \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nk} \\end{array} \\right) . \\end{equation*}\\] En este caso, la matriz \\(\\mathbf{A}\\) tiene dimensión \\(n\\times k\\). Un vector es una matriz especial. Un vector fila es una matriz que contiene solo 1 fila (\\(k=1\\)). Un vector columna es una matriz que contiene solo 1 columna (\\(n=1\\)). Por ejemplo, \\[\\begin{equation*} \\text{vector columna}\\rightarrow \\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\end{array} \\right) ~~~~\\text{vector fila}\\rightarrow \\left( \\begin{array}{ccccc} 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\end{array} \\right) . \\end{equation*}\\] Observa que el vector fila ocupa mucho menos espacio en una página impresa que el vector columna correspondiente. Una operación básica que relaciona estas dos cantidades es la transposición. La transposición de una matriz \\(\\mathbf{A}\\) se define intercambiando las filas y columnas y se denota por \\(\\mathbf{A }^{\\prime }\\) (o \\(\\mathbf{A}^{T}\\)). Por ejemplo, \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} 6,000 &amp; 23 \\\\ 13,000 &amp; 47 \\\\ 11,000 &amp; 35 \\end{array} \\right) ~~~\\mathbf{A}^{\\prime }=\\left( \\begin{array}{ccc} 6,000 &amp; 13,000 &amp; 11,000 \\\\ 23 &amp; 47 &amp; 35 \\end{array} \\right) . \\end{equation*}\\] Así, si \\(\\mathbf{A}\\) tiene dimensión \\(n\\times k\\), entonces \\(\\mathbf{A}^{\\prime }\\) tiene dimensiones \\(k\\times n\\). 2.11.2 Algunas Matrices Especiales Una matriz cuadrada es una matriz donde el número de filas es igual al número de columnas, es decir, \\(n=k\\). Los números diagonales de una matriz cuadrada son los números en una matriz donde el número de fila es igual al número de columna, por ejemplo, \\(a_{11}\\), \\(a_{22}\\), y así sucesivamente. Una matriz diagonal es una matriz cuadrada en la que todos los números no diagonales son iguales a 0. Por ejemplo, \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{ccc} -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{array} \\right) . \\end{equation*}\\] Una matriz identidad es una matriz diagonal donde todos los números diagonales son iguales a 1. Esta matriz especial se denota a menudo por \\(\\mathbf{I}\\). Una matriz simétrica es una matriz cuadrada \\(\\mathbf{A}\\) tal que la matriz permanece sin cambios si intercambiamos las filas y las columnas. Más formalmente, una matriz \\(\\mathbf{A}\\) es simétrica si \\(\\mathbf{A=A} ^{\\prime }\\). Por ejemplo, \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{ccc} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 10 \\end{array} \\right) \\mathbf{=A}^{\\prime }. \\end{equation*}\\] Observa que una matriz diagonal es una matriz simétrica. 2.11.3 Operaciones Básicas Multiplicación por un Escalar Sea \\(\\mathbf{A}\\) una matriz de \\(n\\times k\\) y sea \\(c\\) un número real. Es decir, un número real es una matriz de \\(1\\times 1\\) y también se llama escalar. Multiplicar un escalar \\(c\\) por una matriz \\(\\mathbf{A}\\) se denota por \\(c\\mathbf{A}\\) y se define por \\[\\begin{equation*} c\\mathbf{A}=\\left( \\begin{array}{cccc} ca_{11} &amp; ca_{12} &amp; \\cdots &amp; ca_{1k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ ca_{n1} &amp; ca_{n2} &amp; \\cdots &amp; ca_{nk} \\end{array} \\right) . \\end{equation*}\\] Por ejemplo, supongamos que \\(c=10\\) y \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} 1 &amp; 2 \\\\ 6 &amp; 8 \\end{array} \\right) ~~~~~\\text{entonces} ~~~~\\mathbf{B}=c\\mathbf{A}=\\left( \\begin{array}{cc} 10 &amp; 20 \\\\ 60 &amp; 80 \\end{array} \\right) . \\end{equation*}\\] Observa que \\(c\\mathbf{A}=\\mathbf{A}c\\). Suma y Resta de Matrices Sean \\(\\mathbf{A}\\) y \\(\\mathbf{B}\\) matrices con dimensiones \\(n\\times k\\). Utiliza \\(a_{ij}\\) y \\(b_{ij}\\) para denotar los números en la \\(i\\)-ésima fila y \\(j\\)-ésima columna de \\(\\mathbf{A}\\) y \\(\\mathbf{B}\\), respectivamente. Entonces, la matriz \\(\\mathbf{C}=\\mathbf{A}+\\mathbf{B}\\) se define como la matriz con \\((a_{ij}+b_{ij})\\) en la \\(i\\)-ésima fila y \\(j\\)-ésima columna. De manera similar, la matriz \\(\\mathbf{C}=\\mathbf{A}-\\mathbf{B}\\) se define como la matriz con \\((a_{ij}-b_{ij})\\) en la \\(i\\)-ésima fila y \\(j\\)-ésima columna. Simbólicamente, escribimos esto como sigue. \\[\\begin{equation*} \\text{Si }\\mathbf{A=}\\left( a_{ij}\\right) _{ij}\\text{ y } \\mathbf{B=}\\left( b_{ij}\\right) _{ij}\\text{, entonces} \\end{equation*}\\] \\[\\begin{equation*} \\mathbf{C}=\\mathbf{A}+\\mathbf{B=}\\left( a_{ij}+b_{ij}\\right) _{ij}\\text{ y }\\mathbf{C}=\\mathbf{A}-\\mathbf{B=}\\left( a_{ij}-b_{ij}\\right) _{ij}. \\end{equation*}\\] Por ejemplo, considera \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} 2 &amp; 5 \\\\ 4 &amp; 1 \\end{array} \\right) ~~~\\mathbf{B}=\\left( \\begin{array}{cc} 4 &amp; 6 \\\\ 8 &amp; 1 \\end{array} \\right). \\end{equation*}\\] Entonces \\[\\begin{equation*} \\mathbf{A}+\\mathbf{B}=\\left( \\begin{array}{cc} 6 &amp; 11 \\\\ 12 &amp; 2 \\end{array} \\right) ~~~\\mathbf{A}-\\mathbf{B}=\\left( \\begin{array}{cc} -2 &amp; -1 \\\\ -4 &amp; 0 \\end{array} \\right) . \\end{equation*}\\] Ejemplo Básico de Regresión Lineal de Suma y Resta. Ahora, recuerda que el modelo básico de regresión lineal puede escribirse como \\(n\\) ecuaciones: \\[\\begin{equation*} \\begin{array}{c} y_1=\\beta_0+\\beta_1x_1+\\varepsilon _1 \\\\ \\vdots \\\\ y_n=\\beta_0+\\beta_1x_n+\\varepsilon _n. \\end{array} \\end{equation*}\\] Podemos definir \\[\\begin{equation*} \\mathbf{y}=\\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) ~~~\\boldsymbol \\varepsilon = \\left( \\begin{array}{c} \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_n \\end{array} \\right) ~~~\\text{y}~~~ \\mathrm{E~}\\mathbf{y} =\\left( \\begin{array}{c} \\beta_0+\\beta_1 x_1 \\\\ \\vdots \\\\ \\beta_0 + \\beta_1 x_n \\end{array} \\right) . \\end{equation*}\\] Con esta notación, podemos expresar las \\(n\\) ecuaciones de manera más compacta como \\(\\mathbf{y} = \\mathrm{E~}\\mathbf{y}+\\boldsymbol \\varepsilon\\). Multiplicación de Matrices En general, si \\(\\mathbf{A}\\) es una matriz de dimensión \\(n\\times c\\) y \\(\\mathbf{B}\\) es una matriz de dimensión \\(c\\times k\\), entonces \\(\\mathbf{C}=\\mathbf{AB}\\) es una matriz de dimensión \\(n\\times k\\) y se define por \\[\\begin{equation*} \\mathbf{C}=\\mathbf{AB}=\\left( \\sum_{s=1}^{c}a_{is}b_{sj}\\right)_{ij}. \\end{equation*}\\] Por ejemplo, considera las matrices \\(2\\times 2\\) \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} 2 &amp; 5 \\\\ 4 &amp; 1 \\end{array} \\right) ~~~\\mathbf{B}=\\left( \\begin{array}{cc} 4 &amp; 6 \\\\ 8 &amp; 1 \\end{array} \\right) . \\end{equation*}\\] La matriz \\(\\mathbf{AB}\\) tiene dimensión \\(2\\times 2\\). Para ilustrar el cálculo, considera el número en la primera fila y segunda columna de \\(\\mathbf{AB}\\). Según la regla presentada arriba, con \\(i=1\\) y \\(j=2\\), el elemento correspondiente de \\(\\mathbf{AB}\\) es \\(\\sum_{s=1}^2a_{1s}b_{s2}=a_{11}b_{12}+a_{12}b_{22}=2(6)+5(1)=17\\). Los otros cálculos se resumen como \\[\\begin{equation*} \\mathbf{AB}=\\left( \\begin{array}{cc} 2(4)+5(8) &amp; 2(6)+5(1) \\\\ 4(4)+1(8) &amp; 4(6)+1(1) \\end{array} \\right) =\\left( \\begin{array}{cc} 48 &amp; 17 \\\\ 24 &amp; 25 \\end{array} \\right) . \\end{equation*}\\] Como otro ejemplo, supongamos \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{ccc} 1 &amp; 2 &amp; 4 \\\\ 0 &amp; 5 &amp; 8 \\end{array} \\right) ~~~\\mathbf{B}=\\left( \\begin{array}{c} 3 \\\\ 5 \\\\ 2 \\end{array} \\right) . \\end{equation*}\\] Como \\(\\mathbf{A}\\) tiene dimensión \\(2\\times 3\\) y \\(\\mathbf{B}\\) tiene dimensión \\(3\\times 1\\), esto significa que el producto \\(\\mathbf{AB}\\) tiene dimensión \\(2\\times 1\\). Los cálculos se resumen como \\[\\begin{equation*} \\mathbf{AB}=\\left( \\begin{array}{c} 1(3)+2(5)+4(2) \\\\ 0(3)+5(5)+8(2) \\end{array} \\right) =\\left( \\begin{array}{c} 21 \\\\ 41 \\end{array} \\right) . \\end{equation*}\\] Para algunos ejemplos adicionales, tenemos \\[\\begin{equation*} \\left( \\begin{array}{cc} 4 &amp; 2 \\\\ 5 &amp; 8 \\end{array} \\right) \\left( \\begin{array}{c} a_1 \\\\ a_2 \\end{array} \\right) =\\left( \\begin{array}{c} 4a_1+2a_2 \\\\ 5a_1+8a_2 \\end{array} \\right) . \\end{equation*}\\] \\[\\begin{equation*} \\left( \\begin{array}{ccc} 2 &amp; 3 &amp; 5 \\end{array} \\right) \\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 5 \\end{array} \\right) =2^2+3^2+5^2=38~~~\\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 5 \\end{array} \\right) \\left( \\begin{array}{ccc} 2 &amp; 3 &amp; 5 \\end{array} \\right) =\\left( \\begin{array}{ccc} 4 &amp; 6 &amp; 10 \\\\ 6 &amp; 9 &amp; 15 \\\\ 10 &amp; 15 &amp; 25 \\end{array} \\right) . \\end{equation*}\\] En general, observa que \\(\\mathbf{AB}\\neq \\mathbf{BA}\\) en la multiplicación de matrices, a diferencia de la multiplicación de escalares (números reales). Además, observamos que la matriz identidad cumple el papel de “uno” en la multiplicación de matrices, ya que \\(\\mathbf{AI=A}\\) y \\(\\mathbf{IA=A}\\) para cualquier matriz \\(\\mathbf{A}\\), siempre que las dimensiones sean compatibles para permitir la multiplicación de matrices. Ejemplo Básico de Regresión Lineal de Multiplicación de Matrices. Define \\[\\begin{equation*} \\mathbf{X}=\\left( \\begin{array}{cc} 1 &amp; x_1 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{array} \\right) \\text{ y } \\boldsymbol \\beta =\\left( \\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array} \\right) \\text{, para obtener } \\mathbf{X} \\boldsymbol{\\beta} =\\left( \\begin{array}{c} \\beta_0+\\beta_1x_1 \\\\ \\vdots \\\\ \\beta_0+\\beta_1x_n \\end{array} \\right) =\\mathbf{\\mathrm{E~}\\mathbf{y}}. \\end{equation*}\\] Así, se obtiene la expresión matricial familiar del modelo de regresión, \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\). Otras cantidades útiles incluyen \\[\\begin{equation*} \\mathbf{y}^{\\prime }\\mathbf{y}=\\left( \\begin{array}{ccc} y_1 &amp; \\cdots &amp; y_n \\end{array} \\right) \\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) =y_1^2+\\cdots +y_n^2=\\sum_{i=1}^{n}y_i^2, \\end{equation*}\\] \\[\\begin{equation*} \\mathbf{X}^{\\prime }\\mathbf{y}=\\left( \\begin{array}{ccc} 1 &amp; \\cdots &amp; 1 \\\\ x_1 &amp; \\cdots &amp; x_n \\end{array} \\right) \\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) =\\left( \\begin{array}{c} \\sum_{i=1}^{n}y_i \\\\ \\sum_{i=1}^{n}x_iy_i \\end{array} \\right) \\end{equation*}\\] y \\[\\begin{equation*} \\mathbf{X}^{\\prime }\\mathbf{X}=\\left( \\begin{array}{ccc} 1 &amp; \\cdots &amp; 1 \\\\ x_1 &amp; \\cdots &amp; x_n \\end{array} \\right) \\left( \\begin{array}{cc} 1 &amp; x_1 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{array} \\right) =\\left( \\begin{array}{cc} n &amp; \\sum_{i=1}^{n}x_i \\\\ \\sum_{i=1}^{n}x_i &amp; \\sum_{i=1}^{n} x_i^2 \\end{array} \\right) . \\end{equation*}\\] Observa que \\(\\mathbf{X}^{\\prime }\\mathbf{X}\\) es una matriz simétrica. Inversas de Matrices En álgebra de matrices, no existe el concepto de “división.” En su lugar, extendemos el concepto de “recíprocos” de los números reales. Para comenzar, supongamos que \\(\\mathbf{A}\\) es una matriz cuadrada de dimensión \\(k \\times k\\) y que \\(\\mathbf{I}\\) es la matriz identidad de dimensión \\(k \\times k\\). Si existe una matriz \\(k \\times k\\) llamada \\(\\mathbf{B}\\) tal que \\(\\mathbf{AB}=\\mathbf{I}=\\mathbf{BA}\\), entonces \\(\\mathbf{B}\\) se llama inversa de \\(\\mathbf{A}\\) y se escribe como \\[\\begin{equation*} \\mathbf{B}=\\mathbf{A}^{-1}. \\end{equation*}\\] No todas las matrices cuadradas tienen inversas. Además, incluso cuando existe una inversa, puede no ser fácil de calcular manualmente. Una excepción a esta regla son las matrices diagonales. Supongamos que \\(\\mathbf{A}\\) es una matriz diagonal de la forma \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{ccc} a_{11} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; a_{kk} \\end{array} \\right). \\text{ Entonces } \\mathbf{A}^{-1}=\\left( \\begin{array}{ccc} \\frac{1}{a_{11}} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; \\frac{1}{a_{kk}} \\end{array} \\right). \\end{equation*}\\] Por ejemplo, \\[\\begin{equation*} \\begin{array}{cccc} \\left( \\begin{array}{cc} 2 &amp; 0 \\\\ 0 &amp; -19 \\end{array} \\right) &amp; \\left( \\begin{array}{cc} \\frac{1}{2} &amp; 0 \\\\ 0 &amp; -\\frac{1}{19} \\end{array} \\right) &amp; = &amp; \\left( \\begin{array}{cc} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right) \\\\ \\mathbf{A} &amp; \\mathbf{A}^{-1} &amp; = &amp; \\mathbf{I} \\end{array} . \\end{equation*}\\] En el caso de una matriz de dimensión \\(2\\times 2\\), el procedimiento de inversión se puede realizar manualmente incluso cuando la matriz no es diagonal. En el caso de \\(2\\times 2\\), supongamos que si \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} a &amp; b \\\\ c &amp; d \\end{array} \\right), \\text{ entonces } \\mathbf{A}^{-1}=\\frac{1}{ad-bc}\\left( \\begin{array}{cc} d &amp; -b \\\\ -c &amp; a \\end{array} \\right) \\text{.} \\end{equation*}\\] Así, por ejemplo, si \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} 2 &amp; 2 \\\\ 3 &amp; 4 \\end{array} \\right) \\text{ entonces } \\mathbf{A}^{-1}=\\frac{1}{2(4)-2(3)} \\left( \\begin{array}{cc} 4 &amp; -2 \\\\ -3 &amp; 2 \\end{array} \\right) =\\left( \\begin{array}{cc} 2 &amp; -1 \\\\ -3/2 &amp; 1 \\end{array} \\right) \\text{.} \\end{equation*}\\] Como verificación, tenemos \\[\\begin{equation*} \\mathbf{A}\\mathbf{A}^{-1}=\\left( \\begin{array}{cc} 2 &amp; 2 \\\\ 3 &amp; 4 \\end{array} \\right) \\left( \\begin{array}{cc} 2 &amp; -1 \\\\ -3/2 &amp; 1 \\end{array} \\right) =\\left( \\begin{array}{cc} 2(2)-2(3/2) &amp; 2(-1)+2(1) \\\\ 3(2)-4(3/2) &amp; 3(-1)+4(1) \\end{array} \\right) =\\left( \\begin{array}{cc} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right) =\\mathbf{I}\\text{.} \\end{equation*}\\] Ejemplo Básico de Regresión Lineal de Inversas de Matrices. Con \\[\\begin{equation*} \\mathbf{X}^{\\prime }\\mathbf{X}=\\left( \\begin{array}{cc} n &amp; \\sum\\limits_{i=1}^{n}x_i \\\\ \\sum\\limits_{i=1}^{n}x_i &amp; \\sum\\limits_{i=1}^{n}x_i^2 \\end{array} \\right), \\end{equation*}\\] tenemos \\[\\begin{equation*} \\left( \\mathbf{X}^{\\prime }\\mathbf{X}\\right)^{-1}=\\frac{1}{n\\sum_{i=1}^{n}x_i^2-\\left( \\sum_{i=1}^{n}x_i\\right) ^2}\\left( \\begin{array}{cc} \\sum\\limits_{i=1}^{n}x_i^2 &amp; -\\sum\\limits_{i=1}^{n}x_i \\\\ -\\sum\\limits_{i=1}^{n}x_i &amp; n \\end{array} \\right). \\end{equation*}\\] Para simplificar esta expresión, recuerda que \\(\\overline{x}=n^{-1} \\sum_{i=1}^{n}x_i\\). Así, \\[\\begin{equation} \\left( \\mathbf{X}^{\\prime }\\mathbf{X}\\right)^{-1}=\\frac{1}{ \\sum_{i=1}^{n}x_i^2-n\\overline{x}^2}\\left( \\begin{array}{cc} n^{-1}\\sum\\limits_{i=1}^{n}x_i^2 &amp; -\\overline{x} \\\\ -\\overline{x} &amp; 1 \\end{array} \\right) . \\tag{2.9} \\end{equation}\\] La Sección 3.1 discutirá la relación \\(\\mathbf{b}=\\left( \\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{\\prime}\\mathbf{y}\\). Para ilustrar el cálculo, tenemos \\[\\begin{eqnarray*} \\mathbf{b} &amp;=&amp;\\left( \\mathbf{X}^{\\prime }\\mathbf{X}\\right)^{-1}\\mathbf{X} ^{\\prime }\\mathbf{y}=\\frac{1}{\\sum_{i=1}^{n}x_i^2-n\\overline{x}^2} \\left( \\begin{array}{cc} n^{-1}\\sum\\limits_{i=1}^{n}x_i^2 &amp; -\\overline{x} \\\\ -\\overline{x} &amp; 1 \\end{array} \\right) \\left( \\begin{array}{c} \\sum\\limits_{i=1}^{n}y_i \\\\ \\sum\\limits_{i=1}^{n}x_iy_i \\end{array} \\right) \\\\ &amp;=&amp;\\frac{1}{\\sum_{i=1}^{n}x_i^2-n\\overline{x}^2}\\left( \\begin{array}{c} \\sum\\limits_{i=1}^{n}\\left( \\overline{y}x_i^2-\\overline{x} x_iy_i\\right) \\\\ \\sum\\limits_{i=1}^{n}x_iy_i-n\\overline{x}\\overline{y} \\end{array} \\right) =\\left( \\begin{array}{c} b_0 \\\\ b_1 \\end{array} \\right) . \\end{eqnarray*}\\] De esta expresión, podemos ver \\[\\begin{equation*} b_1=\\frac{\\sum\\limits_{i=1}^{n}x_iy_i-n\\overline{x}\\overline{y}}{\\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x}^2} \\end{equation*}\\] y \\[\\begin{equation*} b_0=\\frac{\\overline{y}\\sum\\limits_{i=1}^{n}x_i^2-\\overline{x} \\sum\\limits_{i=1}^{n}x_iy_i}{\\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x}^2}=\\frac{\\overline{y}\\left( \\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x} ^2\\right) -\\overline{x}\\left( \\sum\\limits_{i=1}^{n} x_i y_i - n\\overline{x} \\overline{y}\\right) }{\\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x}^2}=\\overline{y}-b_1\\overline{x}. \\end{equation*}\\] Estas son las expresiones usuales para la pendiente \\(b_1\\) (Ejercicio 2A.8) y el intercepto \\(b_0\\). 2.11.4 Matrices Aleatorias Esperanzas. Consideremos una matriz de variables aleatorias \\[\\begin{equation*} \\mathbf{U=}\\left( \\begin{array}{cccc} u_{11} &amp; u_{12} &amp; \\cdots &amp; u_{1c} \\\\ u_{21} &amp; u_{22} &amp; \\cdots &amp; u_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ u_{n1} &amp; u_{n2} &amp; \\cdots &amp; u_{nc} \\end{array} \\right). \\end{equation*}\\] Cuando escribimos la esperanza de una matriz, esto es una forma abreviada para la matriz de esperanzas. Específicamente, supongamos que la función de probabilidad conjunta de \\({u_{11}, u_{12}, ..., u_{1c}, ..., u_{n1}, ..., u_{nc}}\\) está disponible para definir el operador de esperanza. Entonces definimos \\[\\begin{equation*} \\mathrm{E} ~ \\mathbf{U} = \\left( \\begin{array}{cccc} \\mathrm{E }u_{11} &amp; \\mathrm{E }u_{12} &amp; \\cdots &amp; \\mathrm{E }u_{1c} \\\\ \\mathrm{E }u_{21} &amp; \\mathrm{E }u_{22} &amp; \\cdots &amp; \\mathrm{E }u_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathrm{E }u_{n1} &amp; \\mathrm{E }u_{n2} &amp; \\cdots &amp; \\mathrm{E }u_{nc} \\end{array} \\right). \\end{equation*}\\] Como un caso especial importante, consideremos la función de probabilidad conjunta para las variables aleatorias \\(y_1, \\ldots, y_n\\) y el operador de expectativas correspondiente. Entonces \\[\\begin{equation*} \\mathrm{E}~ \\mathbf{y=} \\mathrm{E } \\left( \\begin{array}{cccc} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) = \\left( \\begin{array}{cccc} \\mathrm{E }y_1 \\\\ \\vdots \\\\ \\mathrm{E }y_n \\end{array} \\right). \\end{equation*}\\] Por la linealidad de las esperanzas, para una matriz no aleatoria A y un vector , tenemos \\(\\mathrm{E} (\\textbf{A y} + \\textbf{B}) = \\textbf{A} \\mathrm{E} \\textbf{y + B}\\). Varianzas. También podemos trabajar con los segundos momentos de vectores aleatorios. La varianza de un vector de variables aleatorias se llama matriz de varianza-covarianza. Se define como \\[\\begin{equation} \\mathrm{Var} ~ \\mathbf{y} = \\mathrm{E} ( (\\mathbf{y} - \\mathrm{E} \\mathbf{y})(\\mathbf{y} - \\mathrm{E} \\mathbf{y})^{\\prime} ). \\tag{2.10} \\end{equation}\\] Es decir, podemos expresar \\[\\begin{equation*} \\mathrm{Var}~\\mathbf{y=} \\mathrm{E } \\left( \\left( \\begin{array}{c} y_1 -\\mathrm{E } y_1 \\\\ \\vdots \\\\ y_n -\\mathrm{E } y_n \\end{array}\\right) \\left(\\begin{array}{ccc} y_1 - \\mathrm{E } y_1 &amp; \\cdots &amp; y_n - \\mathrm{E } y_n \\end{array}\\right) \\right) \\end{equation*}\\] \\[\\begin{equation*} = \\left( \\begin{array}{cccc} \\mathrm{Var}~y_1 &amp; \\mathrm{Cov}(y_1, y_2) &amp; \\cdots &amp;\\mathrm{Cov}(y_1, y_n) \\\\ \\mathrm{Cov}(y_2, y_1) &amp; \\mathrm{Var}~y_2 &amp; \\cdots &amp; \\mathrm{Cov}(y_2, y_n) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\mathrm{Cov}(y_n, y_1) &amp; \\mathrm{Cov}(y_n, y_2) &amp; \\cdots &amp; \\mathrm{Var}~y_n \\\\ \\end{array}\\right), \\end{equation*}\\] porque \\(\\mathrm{E} ( (y_i - \\mathrm{E} y_i)(y_j - \\mathrm{E} y_j) ) = \\mathrm{Cov}(y_i, y_j)\\) para \\(i \\neq j\\) y \\(\\mathrm{Cov}(y_i, y_i) = \\mathrm{Var}~y_i\\). En el caso de que \\(y_1, \\ldots, y_n\\) sean mutuamente no correlacionados, tenemos que \\(\\mathrm{Cov}(y_i, y_j)=0\\) para \\(i \\neq j\\) y así \\[\\begin{equation*} \\mathrm{Var}~\\mathbf{y=} \\left( \\begin{array}{cccc} \\mathrm{Var}~y_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\mathrm{Var}~y_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0 &amp; \\cdots &amp; \\mathrm{Var}~y_n \\\\ \\end{array}\\right). \\end{equation*}\\] Además, si las varianzas son idénticas, de modo que \\(\\mathrm{Var}~y_i=\\sigma ^2\\), entonces podemos escribir \\(\\mathrm{Var} ~\\mathbf{y} = \\sigma ^2 \\mathbf{I}\\), donde I es la matriz identidad \\(n \\times n\\). Por ejemplo, si \\(y_1, \\ldots, y_n\\) son i.i.d., entonces \\(\\mathrm{Var} ~\\mathbf{y} = \\sigma ^2 \\mathbf{I}\\). A partir de la ecuación (2.10), se puede demostrar que \\[\\begin{equation} \\mathrm{Var}\\left( \\mathbf{Ay +B} \\right) = \\mathrm{Var}\\left( \\mathbf{Ay} \\right) = \\mathbf{A} \\left( \\mathrm{Var}~\\mathbf{y} \\right) \\mathbf{A}^{\\prime}. \\tag{2.11} \\end{equation}\\] Por ejemplo, si \\(\\mathbf{A} = (a_1, a_2, \\ldots,a_n)= \\mathbf{a}^{\\prime}\\) y B = 0, entonces la ecuación (2.11) se reduce a \\[\\begin{equation*} \\mathrm{Var}\\left( \\sum_{i=1}^n a_i y_i \\right) = \\mathrm{Var} \\left( \\mathbf{a^{\\prime} y} \\right) = \\mathbf{a^{\\prime}} \\left( \\mathrm{Var} ~\\mathbf{y} \\right) \\mathbf{a} = (a_1, a_2, \\ldots,a_n) \\left( \\mathrm{Var} ~\\mathbf{y} \\right) \\left(\\begin{array}{c} a_1 \\\\ \\vdots \\\\ a_n \\end{array}\\right) \\end{equation*}\\] \\[\\begin{equation*} = \\sum_{i=1}^n a_i^2 \\mathrm{Var} ~y_i ~+~2 \\sum_{i=2}^n \\sum_{j=1}^{i-1} a_i a_j \\mathrm{Cov}(y_i, y_j). \\end{equation*}\\] Definición - Distribución Normal Multivariante. Un vector de variables aleatorias \\(\\mathbf{y} = \\left(y_1, \\ldots, y_n \\right)^{\\prime}\\) se dice que es normal multivariante si todas las combinaciones lineales de la forma \\(\\sum_{i=1}^n a_i y_i\\) están distribuidas normalmente. En este caso, escribimos \\(\\mathbf{y} \\sim N (\\mathbf{\\boldsymbol \\mu}, \\mathbf{\\Sigma} )\\), donde \\(\\mathbf{\\boldsymbol \\mu} = \\mathrm{E}~ \\mathbf{y}\\) es el valor esperado de y y \\(\\mathbf{\\Sigma}= \\mathrm{Var}~\\mathbf{y}\\) es la matriz de varianza-covarianza de y. Según la definición, tenemos que \\(\\mathbf{y}\\sim N (\\mathbf{\\boldsymbol \\mu}, \\mathbf{\\Sigma} )\\) implica que \\(\\mathbf{a^{\\prime}y}\\sim N (\\mathbf{a^{\\prime} \\boldsymbol \\mu}, \\mathbf{a^{\\prime}\\Sigma a})\\). Así, si \\(y_i\\) son i.i.d., entonces \\(\\sum_{i=1}^n a_i y_i\\) está distribuido normalmente con media \\(\\mu \\sum_{i=1}^n a_i\\) y varianza \\(\\sigma ^2 \\sum_{i=1}^n a_i ^2\\). "],["C3BasicMLR.html", "Chapter 3 Regresión Lineal Múltiple - I 3.1 Método de Mínimos Cuadrados 3.2 Modelo de Regresión Lineal y Propiedades de los Estimadores 3.3 Estimación y Bondad de Ajuste 3.4 Inferencia Estadística para un Coeficiente Único 3.5 Algunas Variables Explicativas Especiales 3.6 Lectura Adicional y Referencias 3.7 Ejercicios", " Chapter 3 Regresión Lineal Múltiple - I Vista previa del capítulo. Este capítulo introduce la regresión lineal en el caso de varios variables explicativas, conocida como regresión lineal múltiple. Muchos conceptos básicos de la regresión lineal se extienden directamente, incluyendo medidas de bondad de ajuste como \\(R^2\\) y la inferencia usando estadísticas \\(t\\). Los modelos de regresión lineal múltiple proporcionan un marco para resumir datos altamente complejos y multivariados. Debido a que este marco solo requiere linealidad en los parámetros, podemos ajustar modelos que son funciones no lineales de las variables explicativas, proporcionando así un amplio alcance de aplicaciones potenciales. 3.1 Método de Mínimos Cuadrados El Capítulo 2 trató sobre el problema de una respuesta que depende de una sola variable explicativa. Ahora extendemos el enfoque de ese capítulo y estudiamos cómo una respuesta puede depender de varias variables explicativas. Ejemplo: Seguro de Vida Temporal. Como todas las empresas, las compañías de seguros de vida buscan continuamente nuevas formas de llevar productos al mercado. Aquellos involucrados en el desarrollo de productos desean saber “¿quién compra seguro y cuánto compran?” En economía, esto se conoce como el lado de la demanda de un mercado de productos. Los analistas pueden obtener fácilmente información sobre las características de los clientes actuales a través de las bases de datos de la empresa. Los clientes potenciales, aquellos que no tienen seguro con la compañía, son a menudo el principal objetivo para expandir la cuota de mercado. En este ejemplo, examinamos la Encuesta de Finanzas del Consumidor (SCF), una muestra representativa a nivel nacional que contiene información extensa sobre activos, pasivos, ingresos y características demográficas de los encuestados (potenciales clientes en EE. UU.). Estudiamos una muestra aleatoria de 500 hogares con ingresos positivos que fueron entrevistados en la encuesta de 2004. Inicialmente, consideramos el subconjunto de \\(n=275\\) familias que compraron seguro de vida temporal. Deseamos abordar la segunda parte de la pregunta de la demanda y determinar las características de la familia que influyen en la cantidad de seguro comprado. El Capítulo 11 considerará la primera parte, es decir, si un hogar compra o no un seguro, a través de modelos donde la respuesta es una variable aleatoria binaria. Para el seguro de vida temporal, la cantidad de seguro se mide por el valor nominal de la póliza, FACE, la cantidad que la compañía pagará en caso de la muerte del asegurado. Las características que resultarán importantes incluyen los ingresos anuales, INCOME, el número de años de EDUCATION del encuestado y el número de miembros del hogar, NUMHH. En general, consideraremos conjuntos de datos donde hay \\(k\\) variables explicativas y una variable de respuesta en una muestra de tamaño \\(n\\). Es decir, los datos consisten en: \\[ \\left\\{ \\begin{aligned} x_{11},x_{12},\\ldots,x_{1k},y_1 \\\\ x_{21},x_{22},\\ldots,x_{2k},y_2 \\\\ \\vdots \\\\ x_{n1},x_{n2},\\ldots,x_{nk},y_n \\end{aligned} \\right\\}. \\] La \\(i\\)-ésima observación corresponde a la \\(i\\)-ésima fila, que consiste en \\((x_{i1},x_{i2},\\ldots,x_{ik},y_i)\\). Para este caso general, tomamos \\(k+1\\) mediciones en cada entidad. Para el ejemplo de demanda de seguros, \\(k=3\\) y los datos consisten en \\((x_{11},x_{12},x_{13}, y_1), \\ldots , (x_{275,1},x_{275,2},x_{275,3},y_{275})\\). Es decir, usamos cuatro mediciones de cada uno de los \\(n=275\\) hogares. Resumiendo los Datos Comenzamos el análisis de los datos examinando cada variable por separado. La Tabla 3.1 proporciona estadísticas descriptivas básicas de las cuatro variables. Para FACE e INCOME, vemos que la media es mucho mayor que la mediana, lo que sugiere que la distribución está sesgada hacia la derecha. Los histogramas (no reportados aquí) muestran que este es el caso. Será útil considerar también sus transformaciones logarítmicas, LNFACE y LNINCOME, respectivamente, que también se informan en la Tabla 3.1. Table 3.1: Estadísticas Descriptivas del Seguro de Vida Temporal Media Mediana Desviación Estándar Mínimo Máximo FACE 747,581 150,000 1,674,362 800 14,000,000 INCOME 208,975 65,000 824,010 260 10,000,000 EDUCATION 14.524 16 2.549 2 17 NUMHH 2.96 3 1.493 1 9 LNFACE 11.99 11.918 1.871 6.685 16.455 LNINCOME 11.149 11.082 1.295 5.561 16.118 El siguiente paso es medir la relación entre cada \\(x\\) sobre \\(y\\), comenzando con los diagramas de dispersión en la Figura 3.1. El panel de la izquierda es un gráfico de FACE versus INCOME; en este panel, vemos una gran concentración en la esquina inferior izquierda que corresponde a hogares con ingresos y cantidades de seguro pequeños. Ambas variables tienen distribuciones sesgadas y su efecto conjunto es altamente no lineal. El panel derecho presenta las mismas variables, pero utilizando transformaciones logarítmicas. Aquí, vemos una relación que se puede aproximar más fácilmente con una línea. Figure 3.1: Ingresos versus Monto Nominal del Seguro de Vida Temporal. El panel de la izquierda es un gráfico de monto nominal versus ingresos, mostrando un patrón altamente no lineal. En el panel derecho, el monto nominal versus ingresos está en unidades logarítmicas naturales, sugiriendo un patrón lineal (aunque variable). Código R para producir la Tabla 3.1 y la Figura 3.1 Term &lt;- read.csv(&quot;CSVData/TermLife.csv&quot;, header=TRUE) # SELECCIONAR EL SUBCONJUNTO DE DATOS CORRESPONDIENTE A LA COMPRA DE SEGURO Term2 &lt;-subset(Term, FACE &gt; 0) # TABLA 3.1 ESTADÍSTICAS DESCRIPTIVAS BookSummStats &lt;- function(Xymat){ meanSummary &lt;- sapply(Xymat, mean, na.rm=TRUE) sdSummary &lt;- sapply(Xymat, sd, na.rm=TRUE) minSummary &lt;- sapply(Xymat, min, na.rm=TRUE) maxSummary &lt;- sapply(Xymat, max, na.rm=TRUE) medSummary &lt;- sapply(Xymat, median,na.rm=TRUE) tableMat &lt;- cbind(meanSummary, medSummary, sdSummary, minSummary, maxSummary) return(tableMat) } LNFACE &lt;- log(Term2$FACE) LNINCOME &lt;- log(Term2$INCOME) Xymat &lt;- data.frame(cbind(Term2$FACE, Term2$INCOME,Term2$EDUCATION, Term2$NUMHH,LNFACE, LNINCOME) ) tableMat &lt;- BookSummStats(Xymat) colnames(tableMat) &lt;- c(&quot;Media&quot; , &quot;Mediana&quot; , &quot;Desviación Estándar&quot; , &quot;Mínimo&quot; , &quot;Máximo&quot;) rownames(tableMat) &lt;- c(&quot;FACE&quot;, &quot;INCOME&quot;, &quot;EDUCATION&quot;, &quot;NUMHH&quot;, &quot;LNFACE&quot;, &quot;LNINCOME&quot;) tableMat1 &lt;- tableMat tableMat1[3:6,] &lt;- round(tableMat1[3:6,], digits = 3) tableMat1[1:2,] &lt;- format(round(tableMat[1:2,], digits=0), big.mark = &#39;,&#39;) TableGen1(TableData=tableMat1, TextTitle=&#39;Estadísticas Descriptivas del Seguro de Vida Temporal&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:5, ColWidth = ColWidth5) Term &lt;- read.csv(&quot;CSVData/TermLife.csv&quot;, header=TRUE) # SELECCIONAR EL SUBCONJUNTO DE DATOS CORRESPONDIENTE A LA COMPRA DE SEGURO Term2 &lt;-subset(Term, FACE &gt; 0) Term2$LNFACE &lt;- log(Term2$FACE) Term2$LNINCOME &lt;- log(Term2$INCOME) # FIGURA 3.1 par(mfrow=c(1, 2), cex=1.1, mar=c(4.1,4,1.5,1)) plot(Term2$INCOME, Term2$FACE, ylab=&quot;&quot;, las=1, yaxt=&quot;n&quot;, xaxt=&quot;n&quot;, xlab=&quot;INCOME (en Millones)&quot;) mtext(&quot;FACE (en Millones)&quot;, side=2, at=15200000, las=1, cex=1.1, adj=.4) axis(2,at=seq(0,14000000,2000000), labels=c(&quot;0&quot;, &quot;2&quot;, &quot;4&quot;, &quot;6&quot;, &quot;8&quot;,&quot;10&quot;,&quot;12&quot;,&quot;14&quot;), las=1) axis(1,at=seq(0,11000000,1000000), labels=c(&quot;0&quot;,&quot;1&quot;, &quot;2&quot;,&quot;3 &quot;, &quot;4&quot;,&quot;5&quot;, &quot;6&quot;,&quot;7&quot;,&quot;8&quot;,&quot;9&quot;,&quot;10&quot;,&quot;11&quot;)) plot(Term2$LNINCOME, Term2$LNFACE, ylab=&quot;&quot;, xlab = &quot;LNINCOME&quot;, las=1) mtext(&quot;LNFACE&quot;, side=2, at=16.8, las=1, cex=1.1, adj=1.1) Los datos de Seguro de Vida Temporal son multivariados en el sentido de que se toman varias mediciones en cada hogar. Es difícil producir un gráfico de observaciones en tres o más dimensiones en una plataforma bidimensional, como una hoja de papel, que no sea confuso, engañoso o ambos. Para resumir gráficamente datos multivariados en aplicaciones de regresión, considere usar una matriz de diagramas de dispersión como en la Figura 3.2. Cada cuadrado de esta figura representa un gráfico simple de una variable contra otra. Para cada cuadrado, la variable de la fila da las unidades del eje vertical y la variable de la columna da las unidades del eje horizontal. La matriz a veces se llama una matriz de dispersión parcial porque solo se presentan los elementos en la parte inferior izquierda. Figure 3.2: Matriz de diagramas de dispersión de cuatro variables. Cada cuadrado es un diagrama de dispersión. La matriz de diagramas de dispersión se puede resumir numéricamente usando una matriz de correlación. Cada correlación en la Tabla 3.2 corresponde a un cuadrado de la matriz de diagramas de dispersión en la Figura 3.2. Los analistas a menudo presentan tablas de correlaciones porque son fáciles de interpretar. Sin embargo, recuerde que un coeficiente de correlación solo mide la magnitud de las relaciones lineales. Por lo tanto, una tabla de correlaciones proporciona una idea de las relaciones lineales, pero puede pasar por alto una relación no lineal que se puede revelar en una matriz de diagramas de dispersión. Table 3.2: Correlaciones del Seguro de Vida Temporal NUMHH EDUCATION LNINCOME EDUCATION -0.064 LNINCOME 0.179 0.343 LNFACE 0.288 0.383 0.482 La matriz de diagramas de dispersión y la correspondiente matriz de correlación son herramientas útiles para resumir datos multivariados. Son fáciles de producir e interpretar. Sin embargo, cada una captura solo relaciones entre pares de variables y no puede cuantificar relaciones entre varias variables. Código R para producir la Tabla 3.2 y la Figura 3.2 tableCor &lt;- cor(Term1) tableCor &lt;- round(tableCor, digits = 3) tableCor[upper.tri(tableCor, diag = TRUE)] &lt;- &quot;&quot; tablePrint &lt;- tableCor[-1,] tablePrint &lt;- tablePrint[,-4] TableGen1(TableData=tablePrint, TextTitle=&#39;Correlaciones del Seguro de Vida Temporal&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:3, ColWidth = ColWidth5) # FIGURA 3.2 par(mar=c(4.1,2.1,2.1,2.1), cex=1.1) varTerm &lt;- c(&quot;NUMHH&quot;,&quot;EDUCATION&quot;, &quot;LNINCOME&quot;, &quot;LNFACE&quot;) Term1 &lt;- Term2[varTerm] pairs(Term1,upper.panel=NULL, gap=0,cex.labels=1.25, las=1) Método de Mínimos Cuadrados Consideremos la pregunta: “¿Puede el conocimiento de la educación, el tamaño del hogar y el ingreso ayudarnos a entender la demanda de seguros?” Las correlaciones en la Tabla 3.2 y los gráficos en las Figuras 3.1 y 3.2 sugieren que cada variable, EDUCATION, NUMHH y LNINCOME, puede ser una variable explicativa útil de LNFACE cuando se consideran individualmente. Parece razonable investigar el efecto conjunto de estas variables en una respuesta. El concepto geométrico de un plano se utiliza para explorar la relación lineal entre una respuesta y varias variables explicativas. Recuerde que un plano extiende el concepto de una línea a más de dos dimensiones. Un plano puede definirse mediante una ecuación algebraica como \\[ y = b_0 + b_1 x_1 + \\ldots + b_k x_k. \\] Esta ecuación define un plano en \\(k+1\\) dimensiones. La Figura 3.3 muestra un plano en tres dimensiones. En esta figura, hay una variable de respuesta, LNFACE, y dos variables explicativas, EDUCATION y LNINCOME (NUMHH se mantiene fijo). Es difícil graficar más de tres dimensiones de una manera significativa. Figure 3.3: Un ejemplo de un plano tridimensional Necesitamos una manera de determinar un plano basado en los datos. La dificultad es que en la mayoría de las aplicaciones de análisis de regresión, el número de observaciones, \\(n\\), excede con creces el número de observaciones requeridas para ajustar un plano, \\(k+1\\). Por lo tanto, generalmente no es posible encontrar un solo plano que pase por todas las \\(n\\) observaciones. Como en el Capítulo 2, utilizamos el método de mínimos cuadrados para determinar un plano a partir de los datos. El método de mínimos cuadrados se basa en determinar los valores de \\(b_0^{\\ast},b_1^{\\ast},\\ldots,b_k^{\\ast}\\) que minimizan la cantidad \\[\\begin{equation} SS(b_0^{\\ast},b_1^{\\ast},\\ldots,b_k^{\\ast})=\\sum_{i=1}^{n}\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_{i1}+\\ldots+b_k^{\\ast}x_{ik}\\right) \\right) ^2. \\tag{3.1} \\end{equation}\\] Dejamos de usar la notación con asterisco, o estrella, y usamos \\(b_0, b_1, \\ldots, b_k\\) para denotar los mejores valores, conocidos como las estimaciones de mínimos cuadrados. Con las estimaciones de mínimos cuadrados, definimos el plano de regresión de mínimos cuadrados, o ajustado, como \\[ \\widehat{y} = b_0 + b_1 x_1 + \\ldots + b_k x_k. \\] Las estimaciones de mínimos cuadrados se determinan minimizando \\(SS(b_0^{\\ast},b_1^{\\ast},\\ldots,b_k^{\\ast})\\). Es difícil escribir los estimadores de mínimos cuadrados resultantes utilizando una fórmula simple a menos que se recurra a la notación matricial. Debido a su importancia en los modelos estadísticos aplicados, se proporciona una fórmula explícita para los estimadores a continuación. Sin embargo, estas fórmulas han sido programadas en una gran variedad de paquetes de software estadístico y de hojas de cálculo. La disponibilidad de estos paquetes permite a los analistas de datos concentrarse en las ideas del procedimiento de estimación en lugar de enfocarse en los detalles de los procedimientos de cálculo. Como ejemplo, se ajustó un plano de regresión a los datos de Seguro de Vida Temporal donde se utilizaron tres variables explicativas, \\(x_1\\) para EDUCATION, \\(x_2\\) para NUMHH y \\(x_3\\) para LNINCOME. El plano de regresión ajustado resultante es \\[\\begin{equation} \\widehat{y} = 2.584 + 0.206 x_1 + 0.306 x_2 + 0.494 x_3. \\tag{3.2} \\end{equation}\\] Notación Matricial Supongamos que los datos son de la forma \\((x_{i0}, x_{i1}, \\ldots, x_{ik}, y_i)\\), donde \\(i = 1, \\ldots, n\\). Aquí, la variable \\(x_{i0}\\) está asociada con el término “intercepto”. En la mayoría de las aplicaciones, suponemos que \\(x_{i0}\\) es idénticamente igual a 1 y, por lo tanto, no es necesario representarlo explícitamente. Sin embargo, hay aplicaciones importantes donde este no es el caso, y por lo tanto, para expresar el modelo en notación general, se incluye aquí. Los datos se representan en notación matricial usando: \\[ \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} ~~~~ \\mathbf{X} = \\begin{pmatrix} x_{10} &amp; x_{11} &amp; \\cdots &amp; x_{1k} \\\\ x_{20} &amp; x_{21} &amp; \\cdots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n0} &amp; x_{n1} &amp; \\cdots &amp; x_{nk} \\end{pmatrix}. \\] Aquí, \\(\\mathbf{y}\\) es el vector de respuestas de \\(n \\times 1\\) y \\(\\mathbf{X}\\) es la matriz de variables explicativas de \\(n \\times (k+1)\\). Usamos la convención de álgebra matricial de que las letras minúsculas y mayúsculas en negrita representan vectores y matrices, respectivamente. (Si necesita repasar sobre matrices, revise la Sección 2.11). Ejemplo: Seguro de Vida Temporal - Continuación. Recuerde que \\(y\\) representa el valor logarítmico del seguro, \\(x_1\\) para los años de educación, \\(x_2\\) para el número de miembros del hogar, y \\(x_3\\) para el ingreso logarítmico. Por lo tanto, hay \\(k = 3\\) variables explicativas y \\(n = 275\\) hogares. El vector de respuestas y la matriz de variables explicativas son: \\[ \\small{ \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{275} \\end{pmatrix} = \\begin{pmatrix} 9.904 \\\\ 11.775 \\\\ \\vdots \\\\ 9.210 \\end{pmatrix} \\\\ \\mathbf{X} = \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{275,1} &amp; x_{275,2} &amp; x_{275,3} \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 16 &amp; 3 &amp; 10.669 \\\\ 1 &amp; 9 &amp; 3 &amp; 9.393 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; 12 &amp; 1 &amp; 10.545 \\end{pmatrix}. } \\] Por ejemplo, para la primera observación en el conjunto de datos, la variable dependiente es \\(y_1 = 9.904\\) (correspondiente a \\(\\exp(9.904) = \\$20,000\\)), para un encuestado con 16 años de educación que vive en un hogar con 3 personas y un ingreso logarítmico de 10.669 (\\(\\exp(10.669) = \\$43,000\\)). Bajo el principio de estimación de mínimos cuadrados, nuestro objetivo es elegir los coeficientes \\(b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast}\\) para minimizar la función de suma de cuadrados \\(SS(b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast})\\). Usando cálculo, regresamos a la ecuación (3.1), tomamos derivadas parciales con respecto a cada coeficiente y establecemos estas cantidades igual a cero: \\[ \\begin{array}{ll} \\frac{\\partial }{\\partial b_j^{\\ast}}SS(b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast}) &amp;= \\sum_{i=1}^{n}\\left( -2x_{ij}\\right) \\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_{i1}+\\ldots+b_k^{\\ast}x_{ik}\\right) \\right) \\\\ &amp;= 0, ~~~ \\text{para} ~ j=0,1,\\ldots,k. \\end{array} \\] Este es un sistema de \\(k+1\\) ecuaciones y \\(k+1\\) incógnitas que se puede resolver fácilmente usando notación matricial, como sigue. Podemos expresar el vector de parámetros a minimizar como \\(\\mathbf{b}^{\\ast}=(b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast})^{\\prime}\\). Usando esto, la suma de cuadrados se puede escribir como \\(SS(\\mathbf{b}^{\\ast}) = (\\mathbf{y-Xb}^{\\ast})^{\\prime}(\\mathbf{y-Xb}^{\\ast})\\). Así, en forma matricial, la solución al problema de minimización se puede expresar como \\(\\frac{\\partial}{\\partial \\mathbf{b}^{\\ast}} SS(\\mathbf{b}^{\\ast}) = \\mathbf{0}\\). Esta solución satisface las ecuaciones normales: \\[\\begin{equation} \\mathbf{X^{\\prime}Xb} = \\mathbf{X}^{\\prime}\\mathbf{y}. \\tag{3.3} \\end{equation}\\] Aquí, se ha eliminado la notación de asterisco (*) para denotar el hecho de que \\(\\mathbf{b} = (b_0, b_1, \\ldots, b_k)^{\\prime}\\) representa el mejor vector de valores en el sentido de minimizar \\(SS(\\mathbf{b}^{\\ast})\\) sobre todas las opciones de \\(\\mathbf{b}^{\\ast}\\). El estimador de mínimos cuadrados \\(\\mathbf{b}\\) no necesita ser único. Sin embargo, suponiendo que las variables explicativas no son combinaciones lineales entre sí, tenemos que \\(\\mathbf{X^{\\prime}X}\\) es invertible. En este caso, podemos escribir la solución única como: \\[\\begin{equation} \\mathbf{b} = \\left( \\mathbf{X^{\\prime}X} \\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}. \\tag{3.4} \\end{equation}\\] Para ilustrar, para el ejemplo de Seguro de Vida Temporal, la ecuación (3.4) produce: \\[ \\small{ \\mathbf{b} = \\begin{pmatrix} b_0 \\\\ b_1 \\\\ b_2 \\\\ b_3 \\\\ \\end{pmatrix} = \\begin{pmatrix} 2.584 \\\\ 0.206 \\\\ 0.306 \\\\ 0.494 \\\\ \\end{pmatrix}. } \\] 3.2 Modelo de Regresión Lineal y Propiedades de los Estimadores En la sección anterior, aprendimos cómo utilizar el método de mínimos cuadrados para ajustar un plano de regresión con un conjunto de datos. Esta sección describe los supuestos que sustentan el modelo de regresión y algunas de las propiedades resultantes de los estimadores de los coeficientes de regresión. Con el modelo y los datos ajustados, podremos hacer inferencias sobre el conjunto de datos de la muestra a una población más grande. Además, más adelante utilizaremos estos supuestos del modelo de regresión para ayudarnos a mejorar la especificación del modelo en el Capítulo 5. 3.2.1 Función de Regresión La mayoría de los supuestos del modelo de regresión lineal múltiple se trasladarán directamente de los supuestos del modelo de regresión lineal básico introducidos en la Sección 2.2. La principal diferencia es que ahora resumimos la relación entre la variable respuesta y las variables explicativas a través de la función de regresión: \\[\\begin{equation} \\mathrm{E~}y = \\beta_0 x_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k, \\tag{3.5} \\end{equation}\\] que es lineal en los parámetros \\(\\beta_0,\\ldots,\\beta_k\\). De aquí en adelante, usaremos \\(x_0 = 1\\) para la variable asociada con el parámetro \\(\\beta_0\\); esto es lo predeterminado en la mayoría de los paquetes estadísticos, y la mayoría de las aplicaciones de regresión incluyen el término de intercepto \\(\\beta_0\\). El intercepto es el valor esperado de \\(y\\) cuando todas las variables explicativas son iguales a cero. Aunque rara vez es de interés, el término \\(\\beta_0\\) sirve para establecer la altura del plano de regresión ajustado. En cambio, los otros betas son típicamente parámetros importantes en un estudio de regresión. Para ayudar a interpretarlos, inicialmente asumimos que \\(x_j\\) varía de manera continua y no está relacionado con las otras variables explicativas. Entonces, podemos interpretar \\(\\beta_j\\) como el cambio esperado en \\(y\\) por unidad de cambio en \\(x_j\\) asumiendo que todas las demás variables explicativas se mantienen fijas. Es decir, desde el cálculo, reconocerás que \\(\\beta_j\\) puede interpretarse como una derivada parcial. Específicamente, usando la ecuación anterior, tenemos que \\[ \\beta_j = \\frac{\\partial }{\\partial x_j}\\mathrm{E}~y. \\] 3.2.2 Interpretación del Coeficiente de Regresión Examinemos las estimaciones de los coeficientes de regresión del ejemplo de Seguro de Vida Temporal y enfoquémonos inicialmente en el signo de los coeficientes. Por ejemplo, en la ecuación (3.4), el coeficiente asociado con NUMHH es \\(b_2 = 0.306 &gt; 0\\). Si consideramos dos hogares que tienen el mismo ingreso y el mismo nivel de educación, entonces se espera que el hogar más grande (en términos de NUMHH) demande más seguro de vida temporal bajo el modelo de regresión. Esta es una interpretación sensata; los hogares más grandes tienen más dependientes para los cuales el seguro de vida temporal puede proporcionar activos financieros necesarios en caso de la muerte prematura de un sostén de la familia. El coeficiente positivo asociado con el ingreso (\\(b_3 = 0.494\\)) también es plausible; los hogares con mayores ingresos tienen más dinero disponible para comprar seguros. El signo positivo asociado con EDUCATION (\\(b_1 = 0.206)\\) también es razonable; más educación sugiere que los encuestados son más conscientes de sus necesidades de seguro, otras cosas iguales. También necesitas interpretar la cantidad del coeficiente de regresión. Consideremos primero el coeficiente de EDUCATION. Usando la ecuación (3.4), se calcularon los valores ajustados de \\(\\widehat{\\mathrm{LNFACE}}\\) permitiendo que EDUCATION variara y manteniendo NUMHH y LNINCOME fijos en los promedios de la muestra. Los resultados son: Efectos de Pequeños Cambios en la Educación EDUCATION 14 14.1 14.2 14.3 \\(\\widehat{\\mathit{LNFACE}}\\) 11.883 11.904 11.924 11.945 \\(\\widehat{\\mathit{FACE}}\\) 144,803 147,817 150,893 154,034 \\(\\widehat{\\mathit{FACE}}\\) % Cambio 2.081 2.081 2.081 A medida que EDUCATION aumenta, \\(\\widehat{\\mathrm{LNFACE}}\\) también aumenta. Además, la cantidad de incremento en \\(\\widehat{\\mathrm{LNFACE}}\\) es un 0.0206 constante. Esto viene directamente de la ecuación (3.4); a medida que EDUCATION aumenta en 0.1 años, se espera que la demanda de seguros aumente en 0.0206 dólares logarítmicos, manteniendo NUMHH y LNINCOME fijos. Esta interpretación es correcta, pero la mayoría de los directores de desarrollo de productos no son muy partidarios de los dólares logarítmicos. Para volver a dólares, los valores ajustados pueden calcularse a través de la exponenciación como \\(\\widehat{\\mathrm{FACE}} = \\exp(\\widehat{\\mathrm{LNFACE}})\\). Además, se puede calcular el cambio porcentual; por ejemplo, \\(100 \\times (147,817/144,803 - 1) \\approx 2.08\\%\\). Esto proporciona otra interpretación del coeficiente de regresión; a medida que EDUCATION aumenta en 0.1 años, se espera que la demanda de seguros aumente en un 2.08%. Esta es una simple consecuencia del cálculo usando \\(\\partial \\ln y / \\partial x = \\left(\\partial y / \\partial x \\right) / y\\); es decir, un pequeño cambio en el valor logarítmico de \\(y\\) equivale a un pequeño cambio en \\(y\\) como una proporción de \\(y\\). Es debido a este resultado del cálculo que utilizamos logaritmos naturales en lugar de logaritmos comunes en el análisis de regresión. Dado que esta tabla utiliza un cambio discreto en EDUCATION, el 2.08% difiere ligeramente del resultado continuo \\(0.206 \\times (\\mathrm{cambio~en~EDUCATION}) = 2.06\\%\\). Sin embargo, esta proximidad generalmente se considera adecuada para fines de interpretación. Continuando con esta lógica, consideremos pequeños cambios en el ingreso logarítmico. Efectos de Pequeños Cambios en el Ingreso Logarítmico LNINCOME 11 11.1 11.2 11.3 INCOME 59,874 66,171 73,130 80,822 INCOME % Cambio 10.52 10.52 10.52 \\(\\widehat{\\mathit{LNFACE}}\\) 11.957 12.006 12.055 12.105 \\(\\widehat{\\mathit{FACE}}\\) 155,831 163,722 172,013 180,724 \\(\\widehat{\\mathit{FACE}}\\) % Cambio 5.06 5.06 5.06 \\(\\widehat{\\mathit{FACE}}\\) % Cambio / INCOME % Cambio 0.482 0.482 0.482 Podemos usar la misma lógica para interpretar el coeficiente LNINCOME en la ecuación (3.4). A medida que el ingreso logarítmico aumenta en 0.1 unidades, se espera que la demanda de seguros aumente en un 5.06%. Esto se refiere a las unidades logarítmicas en \\(y\\) pero no en \\(x\\). Podemos usar la misma lógica para decir que a medida que el ingreso logarítmico aumenta en 0.1 unidades, el INCOME aumenta en un 10.52%. Por lo tanto, un cambio del 10.52% en el INCOME corresponde a un cambio del 5.06% en el FACE. Resumiendo, decimos que, manteniendo NUMHH y EDUCATION fijos, esperamos que un aumento del 1% en el INCOME esté asociado con un aumento del 0.482% en \\(\\widehat{\\mathrm{FACE}}\\) (como antes, esto es cercano a la estimación del parámetro \\(b_3 = 0.494\\)). El coeficiente asociado con el ingreso se conoce como elasticidad en economía. En economía, la elasticidad es la relación entre el cambio porcentual en una variable y el cambio porcentual en otra variable. Matemáticamente, resumimos esto como: \\[ \\frac{\\partial \\ln y}{\\partial \\ln x} = \\left(\\frac{\\partial y}{y}\\right)/\\left(\\frac{\\partial x}{x}\\right). \\] 3.2.3 Suposiciones del Modelo Como en la Sección 2.2 para una sola variable explicativa, existen dos conjuntos de suposiciones que se pueden utilizar para la regresión lineal múltiple. Son conjuntos equivalentes, cada uno con ventajas comparativas a medida que avanzamos en nuestro estudio de la regresión. La representación de “observables” se centra en las variables de interés \\((x_{i1}, \\ldots, x_{ik}, y_i)\\). La “representación del error” proporciona una base para motivar nuestras medidas de bondad de ajuste y el estudio del análisis de residuales. Sin embargo, el segundo conjunto de suposiciones se centra en el caso de errores aditivos y oscurece la base de muestreo del modelo. \\[ \\textbf{Suposiciones de Muestreo del Modelo de Regresión Lineal Múltiple} \\\\ \\small{ \\begin{array}{ll} \\text{Representación de Observables} &amp; \\text{Representación de Error} \\\\ \\hline F1.~ \\mathrm{E}~y_i=\\beta_0+\\beta_1 x_{i1}+\\ldots+\\beta_k x_{ik}. &amp; E1.~ y_i=\\beta_0+\\beta_1 x_{i1}+\\ldots+\\beta_k x_{ik}+\\varepsilon_i. \\\\ F2.~ \\{x_{i1},\\ldots ,x_{ik}\\} &amp; E2.~ \\{x_{i1},\\ldots ,x_{ik}\\} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{son variables no estocásticas.} &amp; \\ \\ \\ \\ \\ \\ \\ \\ \\text{son variables no estocásticas.} \\\\ F3.~ \\mathrm{Var}~y_i=\\sigma^2. &amp; E3.~ \\mathrm{E}~\\varepsilon_i=0 \\text{ y } \\mathrm{Var}~\\varepsilon_i=\\sigma^2. \\\\ F4.~ \\{y_i\\} \\text{ son variables aleatorias independientes.} &amp; E4.~ \\{\\varepsilon_i\\} \\text{ son variables aleatorias independientes.} \\\\ F5.~ \\{y_i\\} \\text{ están distribuidos normalmente.} &amp; E5.~ \\{\\varepsilon_i\\} \\text{ están distribuidos normalmente.} \\\\ \\hline \\end{array} } \\] Para motivar aún más las Suposiciones F2 y F4, generalmente asumimos que nuestros datos han sido obtenidos como resultado de un esquema de muestreo estratificado, donde cada valor único de \\(\\{x_{i1}, \\ldots, x_{ik}\\}\\) se trata como un estrato. Es decir, para cada valor de \\(\\{x_{i1}, \\ldots, x_{ik}\\}\\), tomamos una muestra aleatoria de respuestas de una población. Así, las respuestas dentro de cada estrato son independientes entre sí, al igual que las respuestas de diferentes estratos. El Capítulo 6 discutirá esta base de muestreo en mayor detalle. 3.2.4 Propiedades de los Estimadores de los Coeficientes de Regresión La Sección 3.1 describió el método de mínimos cuadrados para estimar los coeficientes de regresión. Con las suposiciones del modelo de regresión, podemos establecer algunas propiedades básicas de estos estimadores. Para hacerlo, de la Sección 2.11.4, tenemos que la esperanza de un vector es el vector de esperanzas, de modo que \\[ \\small{ \\mathrm{E}~\\mathbf{y} = \\left( \\begin{array}{l} \\mathrm{E}~y_1 \\\\ \\mathrm{E}~y_2 \\\\ \\vdots \\\\ \\mathrm{E}~y_n \\end{array} \\right) . } \\] Además, la multiplicación básica de matrices muestra que \\[ \\small{ \\mathbf{X} \\boldsymbol \\beta = \\left( \\begin{array}{cccc} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; \\cdots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{nk} \\end{array} \\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) = \\left( \\begin{array}{c} \\beta_0 + \\beta_1 x_{11} + \\cdots + \\beta_k x_{1k} \\\\ \\beta_0 + \\beta_1 x_{21} + \\cdots + \\beta_k x_{2k} \\\\ \\vdots \\\\ \\beta_0 + \\beta_1 x_{n1} + \\cdots + \\beta_k x_{nk} \\end{array} \\right) . } \\] Dado que la \\(i\\)-ésima fila de la suposición F1 es \\(\\mathrm{E}~y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik}\\), podemos reescribir esta suposición en forma matricial como \\(\\mathrm{E}~\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta\\). Ahora estamos en condiciones de enunciar la primera propiedad importante de los estimadores de regresión por mínimos cuadrados. Propiedad 1. Consideremos un modelo de regresión y que se cumplan las Suposiciones F1-F4. Entonces, el estimador \\(\\mathbf{b}\\) definido en la ecuación (3.4) es un estimador insesgado del vector de parámetros \\(\\boldsymbol \\beta\\). Para establecer la Propiedad 1, tenemos que \\[ \\begin{array}{ll} \\mathrm{E}~\\mathbf{b} &amp; = \\mathrm{E}~\\left((\\mathbf{X^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathbf{y}\\right) = (\\mathbf{X^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathrm{E}~\\mathbf{y} \\\\ &amp;= (\\mathbf{X^{\\prime}X)}^{-1} \\mathbf{X}^{\\prime} \\left( \\mathbf{X} \\boldsymbol \\beta \\right) = \\boldsymbol \\beta, \\end{array} \\] utilizando reglas de multiplicación de matrices. Este capítulo asume que \\(\\mathbf{X^{\\prime}X}\\) es invertible. También se puede mostrar que el estimador de mínimos cuadrados solo necesita ser una solución de las ecuaciones normales para ser insesgado (sin requerir que \\(\\mathbf{X^{\\prime}X}\\) sea invertible, ver Sección 4.7.3). Así, se dice que \\(\\mathbf{b}\\) es un estimador insesgado de \\(\\boldsymbol \\beta\\). En particular, \\(\\mathrm{E}~b_j = \\beta_j\\) para \\(j = 0,1,\\ldots,k\\). Dado que la independencia implica covarianza cero, de la Suposición F4 tenemos que \\(\\mathrm{Cov}(y_i,y_j) = 0\\) para \\(i \\neq j\\). A partir de esto, de la Suposición F3 y de la definición de la varianza de un vector, tenemos que \\[ \\small{ \\begin{array}{ll} \\mathrm{Var~}\\mathbf{y} &amp;= \\left( \\begin{array}{cccc} \\mathrm{Var~}y_1 &amp; \\mathrm{Cov}(y_1,y_2) &amp; \\cdots &amp; \\mathrm{Cov}(y_1,y_n) \\\\ \\mathrm{Cov}(y_2,y_1) &amp; \\mathrm{Var~}y_2 &amp; \\cdots &amp; \\mathrm{Cov}(y_2,y_n) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathrm{Cov}(y_n,y_1) &amp; \\mathrm{Cov}(y_n,y_2) &amp; \\cdots &amp; \\mathrm{Var~}y_n \\end{array} \\right) \\\\ &amp;= \\left( \\begin{array}{cccc} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{array} \\right) = \\sigma^2 \\mathbf{I}, \\end{array} } \\] donde \\(\\mathbf{I}\\) es una matriz identidad de \\(n \\times n\\). Ahora estamos en condiciones de enunciar la segunda propiedad importante de los estimadores de regresión por mínimos cuadrados. Propiedad 2. Consideremos un modelo de regresión y que se cumplan las Suposiciones F1-F4. Entonces, el estimador \\(\\mathbf{b}\\) definido en la ecuación (3.4) tiene una varianza de \\(\\mathrm{Var~}\\mathbf{b} = \\sigma^2(\\mathbf{X^{\\prime}X)}^{-1}\\). Para establecer la Propiedad 2, a partir de la propiedad de la matriz de varianza, tenemos que \\[ \\small{ \\begin{array}{ll} \\mathrm{Var~}\\mathbf{b} &amp; = \\mathrm{Var~}\\left((\\mathbf{X^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathbf{y}\\right) = (\\mathbf{X^{\\prime}X)}^{-1} \\mathbf{X}^{\\prime} \\mathrm{Var~}\\mathbf{y} \\mathbf{X} (\\mathbf{X^{\\prime}X)}^{-1} \\\\ &amp;= (\\mathbf{X^{\\prime}X)}^{-1} \\mathbf{X}^{\\prime} \\sigma^2 \\mathbf{I} \\mathbf{X} (\\mathbf{X^{\\prime}X)}^{-1} = \\sigma^2 (\\mathbf{X^{\\prime}X)}^{-1}, \\end{array} } \\] como se requiere. Esta propiedad importante nos permitirá medir la precisión del estimador \\(\\mathbf{b}\\) cuando discutamos la inferencia estadística. Específicamente, por la definición de la varianza de un vector (ver Sección 2.11.4), \\[\\begin{equation} \\small{ \\mathrm{Var~}\\mathbf{b}= \\begin{pmatrix} \\mathrm{Var~}b_0 &amp; \\mathrm{Cov}(b_0,b_1) &amp; \\cdots &amp; \\mathrm{Cov}(b_0,b_k) \\\\ \\mathrm{Cov}(b_1,b_0) &amp; \\mathrm{Var~}b_1 &amp; \\cdots &amp; \\mathrm{Cov}(b_1,b_k) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathrm{Cov}(b_k,b_0) &amp; \\mathrm{Cov}(b_k,b_1) &amp; \\cdots &amp; \\mathrm{Var~}b_k \\end{pmatrix} = \\sigma^2 (\\mathbf{X^{\\prime}X)}^{-1}. } \\tag{3.6} \\end{equation}\\] Así, por ejemplo, \\(\\mathrm{Var~}b_j\\) es \\(\\sigma^2\\) veces la entrada diagonal \\((j+1)\\) de \\((\\mathbf{X^{\\prime}X)}^{-1}\\). Como otro ejemplo, \\(\\mathrm{Cov}(b_0,b_j)\\) es \\(\\sigma^2\\) veces el elemento en la primera fila y la columna \\((j+1)\\) de \\((\\mathbf{X^{\\prime}X)}^{-1}\\). Aunque existen métodos alternativos que son preferibles para aplicaciones específicas, los estimadores de mínimos cuadrados han demostrado ser efectivos para muchos análisis de datos rutinarios. Una característica deseable de los estimadores de regresión por mínimos cuadrados se resume en el siguiente resultado bien conocido. Teorema de Gauss-Markov: Consideremos el modelo de regresión y supongamos que se cumplen las Suposiciones F1-F4. Entonces, dentro de la clase de estimadores que son funciones lineales de las respuestas, el estimador de mínimos cuadrados \\(\\mathbf{b}\\) definido en la ecuación (3.4) es el estimador insesgado con la varianza mínima del vector de parámetros \\(\\boldsymbol{\\beta}\\). El teorema de Gauss-Markov establece que el estimador de mínimos cuadrados es el más preciso en el sentido de que tiene la menor varianza. Ya hemos visto en la Propiedad 1 que los estimadores de mínimos cuadrados son insesgados. El teorema de Gauss-Markov afirma que el estimador de mínimos cuadrados es el más preciso en el sentido de que tiene la menor varianza. (En un contexto matricial, “varianza mínima” significa que si \\(\\mathbf{b}^{\\ast}\\) es cualquier otro estimador, entonces la diferencia de las matrices de varianza, \\(\\mathrm{Var~} \\mathbf{b}^{\\ast} - \\mathrm{Var~}\\mathbf{b}\\), es semidefinida no negativa.) Una propiedad adicional importante se refiere a la distribución de los estimadores de regresión por mínimos cuadrados. Propiedad 3: Consideremos un modelo de regresión y supongamos que se cumplen las Suposiciones F1-F5. Entonces, el estimador de mínimos cuadrados \\(\\mathbf{b}\\) definido en la ecuación (3.4) está distribuido normalmente. Para establecer la Propiedad 3, definimos los vectores de peso, \\(\\mathbf{w}_i = (\\mathbf{X^{\\prime}X)}^{-1}(1, x_{i1}, \\ldots, x_{ik})^{\\prime}\\). Con esta notación, observamos que \\[ \\mathbf{b} = (\\mathbf{X^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathbf{y} = \\sum_{i=1}^{n} \\mathbf{w}_i y_i, \\] de modo que \\(\\mathbf{b}\\) es una combinación lineal de respuestas. Con la Suposición F5, las respuestas están distribuidas normalmente. Debido a que las combinaciones lineales de variables aleatorias normalmente distribuidas también están distribuidas normalmente, tenemos la conclusión de la Propiedad 3. Este resultado sustenta gran parte de la inferencia estadística que se presentará en las Secciones 3.4 y 4.2. 3.3 Estimación y Bondad de Ajuste Desviación Estándar Residual Se discutirán propiedades adicionales de los estimadores de los coeficientes de regresión cuando nos enfoquemos en la inferencia estadística. Ahora continuamos nuestra discusión sobre la estimación proporcionando un estimador del otro parámetro en el modelo de regresión lineal, \\(\\sigma^2\\). Nuestro estimador para \\(\\sigma^2\\) puede desarrollarse utilizando el principio de reemplazar expectativas teóricas por promedios muestrales. Examinando \\(\\sigma^2 = \\mathrm{E}\\left( y-\\mathrm{E~}y\\right)^2\\), al reemplazar la expectativa externa por un promedio muestral, se sugiere utilizar el estimador \\(n^{-1}\\sum_{i=1}^{n}(y_i-\\mathrm{E~}y_i)^2\\). Dado que no observamos \\(\\mathrm{E}~y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik}\\), utilizamos en su lugar la cantidad observada correspondiente \\(b_0 + b_1 x_{i1} + \\ldots + b_k x_{ik} = \\widehat{y}_i\\). Esto conduce a lo siguiente. Definición. Un estimador de \\(\\sigma^2\\), el error cuadrático medio (MSE), se define como \\[\\begin{equation} s^2 = \\frac{1}{n-(k+1)}\\sum_{i=1}^{n}\\left( y_i - \\widehat{y}_i \\right)^2. \\tag{3.7} \\end{equation}\\] La raíz cuadrada positiva, \\(s = \\sqrt{s^2}\\), se llama la desviación estándar residual. Esta expresión generaliza la definición en la ecuación (2.3), que es válida para \\(k=1\\). Resulta que, al usar \\(n-(k+1)\\) en lugar de \\(n\\) en el denominador de la ecuación (3.7), \\(s^2\\) es un estimador insesgado de \\(\\sigma^2\\). Esencialmente, al usar \\(\\widehat{y}_i\\) en lugar de \\(\\mathrm{E~}y_i\\) en la definición, hemos introducido algunas pequeñas dependencias entre las desviaciones de las respuestas \\(y_i - \\widehat{y}_i\\), reduciendo así la variabilidad total. Para compensar esta menor variabilidad, también reducimos el denominador en la definición de \\(s^2\\). Para proporcionar más intuición sobre la elección de \\(n-(k+1)\\) en la definición de \\(s^2\\), introducimos el concepto de residuales en el contexto de la regresión lineal múltiple. A partir de la Suposición E1, recordemos que los errores aleatorios pueden expresarse como \\(\\varepsilon_i = y_i - (\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik})\\). Dado que los parámetros \\(\\beta_0, \\ldots, \\beta_k\\) no se observan, los errores en sí mismos no se observan. En su lugar, examinamos los “errores estimados”, o residuales, definidos por \\(e_i = y_i - \\widehat{y}_i\\). A diferencia de los errores, existen ciertas dependencias entre los residuales. Una dependencia se debe al hecho algebraico de que el residual promedio es cero. Además, debe haber al menos \\(k+2\\) observaciones para que haya variación en el ajuste del plano. Si solo tenemos \\(k+1\\) observaciones, podríamos ajustar un plano a los datos perfectamente, resultando en ninguna variación en el ajuste. Por ejemplo, si \\(k=1\\), dado que dos observaciones determinan una línea, se requieren al menos tres observaciones para observar cualquier desviación de la línea. Debido a estas dependencias, solo tenemos \\(n-(k+1)\\) residuales libres, o no restringidos, para estimar la variabilidad en torno al plano de regresión. La raíz cuadrada positiva de \\(s^2\\) es nuestro estimador de \\(\\sigma\\). Usando los residuales, se puede expresar como \\[\\begin{equation} s = \\sqrt{\\frac{1}{n-(k+1)}\\sum_{i=1}^{n}e_i^2}. \\tag{3.8} \\end{equation}\\] Debido a que se basa en residuales, nos referimos a \\(s\\) como la desviación estándar residual. La cantidad \\(s\\) es una medida de nuestro “error típico”. Por esta razón, \\(s\\) también se llama el error estándar de la estimación. El Coeficiente de Determinación: \\(R^2\\) Para resumir la bondad de ajuste del modelo, como en el Capítulo 2, particionamos la variabilidad en partes que son “explicadas” y “no explicadas” por el ajuste de la regresión. Algebraicamente, los cálculos para la regresión utilizando muchas variables son similares al caso de usar solo una variable. Desafortunadamente, cuando se trata de muchas variables, perdemos la fácil interpretación gráfica como en la Figura 2.4. Comenzamos con la suma total de desviaciones cuadradas, \\(Total~SS = \\sum_{i=1}^{n}\\left( y_i - \\overline{y} \\right)^2\\), como nuestra medida de la variación total en el conjunto de datos. Como en la ecuación (2.1), podemos interpretar la ecuación \\[ \\small{ \\begin{array}{ccccc} \\underbrace{y_i - \\overline{y}} &amp; = &amp; \\underbrace{y_i - \\widehat{y}_i} &amp; + &amp; \\underbrace{\\widehat{y}_i - \\overline{y}} \\\\ \\text{desviación total} &amp; = &amp; \\text{desviación no explicada} &amp; + &amp; \\text{desviación explicada} \\\\ \\end{array} } \\] como “la desviación sin conocimiento de las variables explicativas es igual a la desviación no explicada por las variables explicativas más la desviación explicada por las variables explicativas”. Al cuadrar cada lado y sumar sobre todas las observaciones se obtiene \\[ Total~SS = Error~SS + Regression~SS \\] donde \\(Error~SS = \\sum_{i=1}^{n}\\left( y_i - \\widehat{y}_i \\right)^2\\) y \\(Regression~SS = \\sum_{i=1}^{n}\\left( \\widehat{y}_i - \\overline{y} \\right)^2\\). Como en la Sección 2.3 para el caso de una variable explicativa, la suma de los términos de producto cruzado resulta ser cero. Una estadística que resume esta relación es el coeficiente de determinación, \\[ R^2 = \\frac{Regression~SS}{Total~SS}. \\] Interpretamos \\(R^2\\) como la proporción de variabilidad explicada por la función de regresión. Si el modelo es adecuado para los datos, se esperaría una fuerte relación entre las respuestas observadas y las “esperadas” bajo el modelo, los valores ajustados. Un hecho algebraico interesante es el siguiente. Si uno eleva al cuadrado el coeficiente de correlación entre las respuestas y los valores ajustados, obtenemos el coeficiente de determinación, es decir, \\[ R^2 = \\left[ r \\left(y, \\widehat{y} \\right) \\right]^2. \\] Como resultado, \\(R\\), la raíz cuadrada positiva de \\(R^2\\), se llama el coeficiente de correlación múltiple. Se puede interpretar como la correlación entre la respuesta y la mejor combinación lineal de las variables explicativas, los valores ajustados. (Esta relación se desarrolla utilizando álgebra matricial en el apéndice técnico Sección 5.10.1.) La descomposición de la variabilidad también se resume utilizando la tabla de análisis de varianza, o ANOVA, como sigue. \\[ \\small{ \\begin{array}{l|lcl} \\hline \\text{Fuente} &amp; \\text{Suma de Cuadrados} &amp; df &amp; \\text{Cuadrado Medio} \\\\ \\hline \\text{Regresión} &amp; Regression~SS &amp; k &amp; Regression~MS \\\\ \\text{Error} &amp; Error~SS &amp; n - (k + 1) &amp; MSE \\\\ \\text{Total} &amp; Total~SS &amp; n - 1 &amp; \\\\ \\hline \\end{array} } \\] Las cifras de la columna de cuadrados medios se definen como las cifras de suma de cuadrados divididas por sus respectivos grados de libertad. Los grados de libertad del error denotan el número de residuales no restringidos. Es este número el que utilizamos en nuestra definición del “promedio” o error cuadrático medio. Es decir, definimos \\[ MSE = Error~MS = \\frac{Error~SS}{n - (k + 1)} = s^2. \\] De manera similar, los grados de libertad de la regresión son el número de variables explicativas. Esto da como resultado \\[ Regression~MS = \\frac{Regression~SS}{k}. \\] Al hablar del coeficiente de determinación, se puede establecer que siempre que se añade una variable explicativa al modelo, \\(R^2\\) nunca disminuye. Esto es cierto, independientemente de si la variable adicional es útil o no. Nos gustaría una medida de ajuste que disminuyera cuando se introducen variables inútiles en el modelo como variables explicativas. Para evitar esta anomalía, una estadística ampliamente utilizada es el coeficiente de determinación ajustado por grados de libertad, definido por \\[\\begin{equation} R_{a}^2 = 1 - \\frac{(Error~SS) / [n - (k + 1)]}{(Total~SS) / (n - 1)} = 1 - \\frac{s^2}{s_{y}^2}. \\tag{3.9} \\end{equation}\\] Para interpretar esta estadística, observe que \\(s_y^2\\) no depende del modelo ni de las variables del modelo. Por lo tanto, \\(s^2\\) y \\(R_a^2\\) son medidas equivalentes de ajuste del modelo. A medida que el ajuste del modelo mejora, \\(R_{a}^2\\) se hace más grande y \\(s^2\\) se hace más pequeño, y viceversa. Dicho de otro modo, elegir un modelo con el menor \\(s^2\\) es equivalente a elegir un modelo con el mayor \\(R_a^2\\). Ejemplo: Seguro de Vida a Término - Continuación. Para ilustrar, la Tabla 3.3 muestra las estadísticas resumen para la regresión de LNFACE sobre EDUCATION, NUMHH y LNINCOME. A partir de la columna de grados de libertad, recordamos que hay tres variables explicativas y 275 observaciones. Como medidas de ajuste del modelo, el coeficiente de determinación es \\(R^2 = 34.3\\%\\) (=\\(328.47 / 958.90\\)) y la desviación estándar residual es \\(s = 1.525\\) (=\\(\\sqrt{2.326}\\)). Si intentáramos estimar el monto nominal logarítmico sin conocimiento de las variables explicativas EDUCATION, NUMHH y LNINCOME, entonces el tamaño del error típico sería \\(s_y = 1.871\\) (=\\(\\sqrt{958.90 / 274}\\)). Así, al aprovechar nuestro conocimiento de las variables explicativas, hemos podido reducir el tamaño del error típico. La medida de ajuste del modelo que compara estas dos estimaciones de variabilidad es el coeficiente de determinación ajustado, \\(R_a^2 = 1 - 2.326 / 1.871^2 = 33.6\\%\\). Table 3.3: Tabla ANOVA para Seguro de Vida a Término Suma de Cuadrados \\(df\\) Cuadrado Medio Regresión 328.47 3 109.49 Error 630.43 271 2.326 Total 958.9 274 Ejemplo: ¿Por qué las Mujeres Viven Más que los Hombres? En un artículo con este título, Lemaire (2002) examinó lo que llamó la “ventaja femenina”, la diferencia en la esperanza de vida entre mujeres y hombres. Las esperanzas de vida son de interés porque se utilizan ampliamente como medidas de la salud de una nación. Lemaire examinó datos de \\(n = 169\\) países y encontró que la ventaja femenina promedio era de 4.51 años en todo el mundo. Buscó explicar esta diferencia basándose en 45 medidas de comportamiento, variables que capturan el grado de modernización económica de una nación, normas sociales/culturales/religiosas, posición geográfica y calidad de la atención médica disponible. Después de un análisis detallado, Lemaire reporta los coeficientes de un modelo de regresión que aparecen en la Tabla 3.4. Este modelo de regresión explica \\(R^2 = 61\\%\\) de la variabilidad. Es un modelo parsimonioso que consiste en solo \\(k = 4\\) de las 45 variables originales. Tabla 3.4. Coeficientes de Regresión de un Modelo de la Ventaja Femenina \\[ \\small{ \\begin{array}{l|rr} \\hline \\text{Variable} &amp; \\text{Coeficiente} &amp; t\\text{-estadística} \\\\ \\hline \\text{Intercepto} &amp; 9.904 &amp; 12.928 \\\\ \\text{Número Logarítmico de Personas por Médico} &amp; -0.473 &amp; -3.212 \\\\ \\text{Fertilidad} &amp; -0.444 &amp; -3.477 \\\\ \\text{Porcentaje de Hindúes y Budistas} &amp; -0.018 &amp; -3.196 \\\\ \\text{Indicador de la Unión Soviética} &amp; 4.922 &amp; 7.235 \\\\ \\hline \\end{array} } \\] Fuente: Lemaire (2002) Todas las variables fueron estadísticamente significativas. El número de personas por médico también estaba correlacionado con otras variables que capturan el grado de modernización económica de un país, como la urbanización, el número de automóviles y el porcentaje de personas que trabajan en la agricultura. La fertilidad, el número de nacimientos por mujer, estaba altamente correlacionada con las variables de educación en el estudio, incluyendo el analfabetismo femenino y la matriculación escolar femenina. El porcentaje de hindúes y budistas es una variable social/cultural/religiosa. El indicador de la Unión Soviética es una variable geográfica: caracteriza a los países de Europa del Este que pertenecían anteriormente a la Unión Soviética. Debido al alto grado de colinealidad entre las 45 variables candidatas, otros analistas podrían fácilmente elegir un conjunto alternativo de variables. No obstante, el punto importante de Lemaire fue que este modelo simple explica aproximadamente el 61% de la variabilidad basada únicamente en variables de comportamiento, no relacionadas con las diferencias biológicas entre sexos. 3.4 Inferencia Estadística para un Coeficiente Único 3.4.1 La Prueba t En muchas aplicaciones, una sola variable es de interés principal, y otras variables se incluyen en la regresión para controlar fuentes adicionales de variabilidad. Para ilustrar, un agente de ventas podría estar interesado en el efecto que tiene el ingreso sobre la cantidad de seguros demandados. En un análisis de regresión, también se podrían incluir otras variables explicativas como el género de un individuo, tipo de ocupación, edad, tamaño del hogar, nivel educativo, etc. Al incluir estas variables explicativas adicionales, esperamos obtener una mejor comprensión de la relación entre el ingreso y la demanda de seguros. Para llegar a conclusiones sensatas, necesitaremos algunas reglas para decidir si una variable es importante o no. Respondemos a la pregunta “¿Es \\(x_j\\) importante?” investigando si el parámetro de pendiente correspondiente, \\(\\beta_j\\), es igual a cero. La pregunta de si \\(\\beta_j\\) es cero se puede replantear en el marco de la prueba de hipótesis como “¿Es válida \\(H_0:\\beta_j=0\\)?” Examinamos la proximidad de \\(b_j\\) a cero para determinar si \\(\\beta_j\\) es cero o no. Dado que las unidades de \\(b_j\\) dependen de las unidades de \\(y\\) y \\(x_j\\), necesitamos estandarizar esta cantidad. A partir de la Propiedad 2 y la ecuación (3.6), vimos que \\(\\mathrm{Var~}b_j\\) es \\(\\sigma^2\\) multiplicado por el elemento diagonal \\((j+1)^{st}\\) de \\((\\mathbf{X^{\\prime}X})^{-1}\\). Reemplazando \\(\\sigma^2\\) por el estimador \\(s^2\\) y tomando raíces cuadradas, tenemos lo siguiente. Definición. El error estándar de \\(b_j\\) se puede expresar como \\[ se(b_j) = s \\sqrt{\\text{elemento diagonal (j+1)st de } (\\mathbf{X^{\\prime}X})^{-1}}. \\] Recuerda que un error estándar es una desviación estándar estimada. Para probar \\(H_0:\\beta_j=0\\), examinamos la razón \\(t\\), \\(t(b_j) = \\frac{b_j}{se(b_j)}\\). Interpretamos \\(t(b_j)\\) como el número de errores estándar que \\(b_j\\) está alejado de cero. Esta es la cantidad adecuada porque se puede demostrar que la distribución de muestreo de \\(t(b_j)\\) es la distribución \\(t\\) con \\(df=n-(k+1)\\) grados de libertad, bajo la hipótesis nula y con los supuestos del modelo de regresión lineal F1-F5. Esto nos permite construir pruebas de la hipótesis nula como el siguiente procedimiento: Procedimiento. La Prueba t para un Coeficiente de Regresión (\\(\\beta\\)). La hipótesis nula es \\(H_0:\\beta_j=0\\). La hipótesis alternativa es \\(H_{a}:\\beta_j \\neq 0\\). Establecer un nivel de significancia \\(\\alpha\\) (típicamente, pero no necesariamente, 5%). Construir la estadística, \\(t(b_j) = \\frac{b_j}{se(b_j)}\\). Procedimiento: Rechazar la hipótesis nula a favor de la alternativa si \\(|t(b_j)|\\) excede un valor \\(t\\). Aquí, este valor \\(t\\) es el percentil \\((1-\\alpha /2)^{th}\\) de la distribución \\(t\\) con \\(df=n-(k+1)\\) grados de libertad, denotado como \\(t_{n-(k+1),1-\\alpha /2}\\). En muchas aplicaciones, el tamaño de la muestra será lo suficientemente grande como para que podamos aproximar el valor \\(t\\) por el percentil correspondiente de la curva normal estándar. Al nivel de significancia del 5%, este percentil es 1.96. Así, como regla general, podemos interpretar que una variable es importante si su razón \\(t\\) excede dos en valor absoluto. Aunque es la más común, probar \\(H_0:\\beta_j=0\\) frente a \\(H_{a}:\\beta_j \\neq 0\\) es solo una de las muchas pruebas de hipótesis que se pueden realizar. Tabla 3.5 describe procedimientos alternativos para la toma de decisiones. Estos procedimientos son para probar \\(H_0:\\beta_j = d\\). Aquí, \\(d\\) es un valor prescrito por el usuario que puede ser igual a cero o cualquier otro valor conocido. Tabla 3.5. Procedimientos de Toma de Decisiones para Probar \\(H_0: \\beta_j = d\\) \\[ \\small{ \\begin{array}{cc} \\hline \\text{Hipótesis Alternativa }(H_{a}) &amp; \\text{Procedimiento: Rechazar } H_0 \\text{ en favor de } H_a \\text{ si }\\\\ \\hline \\beta_j &gt; d &amp; t-\\mathrm{razón}&gt;t_{n-(k+1),1-\\alpha } \\\\ \\beta_j &lt; d &amp; t-\\mathrm{razón}&lt;-t_{n-(k+1),1-\\alpha } \\\\ \\beta_j\\neq d &amp; |t-\\mathrm{razón}\\mathit{|}&gt;t_{n-(k+1),1-\\alpha/2} \\end{array} \\\\ \\begin{array}{ll}\\hline \\textit{Notas:} &amp;\\text{ El nivel de significancia es } \\alpha. \\text{ Aquí, } t_{n-(k+1),1-\\alpha}\\text{ es el }(1-\\alpha)^{th}\\text{ percentil} \\\\ &amp;~~\\text{de la distribución }t-\\text{utilizando } df=n-(k+1)\\text{ grados de libertad.} \\\\ &amp;~~\\text{La estadística de prueba es }t-\\mathrm{razón} = (b_j -d)/se(b_j) . \\\\ \\hline \\end{array} } \\] Alternativamente, se pueden construir valores \\(p\\) y compararlos con niveles de significancia dados. El valor \\(p\\) permite al lector del informe entender la fuerza de la desviación de la hipótesis nula. Tabla 3.6 resume el procedimiento para calcular los valores \\(p\\). Tabla 3.6. Valores de Probabilidad para Probar \\(H_0:\\beta_j =d\\) \\[ \\small{ \\begin{array}{cccc} \\hline \\text{Hipótesis} &amp; &amp; &amp; \\\\ \\text{Alternativa} (H_a ) &amp; \\beta_j &gt; d &amp; \\beta_j &lt; d &amp; \\beta_j \\neq d \\\\ \\hline p-valor &amp; \\Pr(t_{n-(k+1)}&gt;t-ratio) &amp; \\Pr(t_{n-(k+1)}&lt;t-ratio) &amp; \\Pr(|t_{n-(k+1)}|&gt;|t-ratio|) \\\\ \\end{array} \\\\ \\begin{array}{ll}\\hline \\textit{Notas:} &amp; \\text{ Aquí, } t_{n-(k+1)} \\text{ es una variable aleatoria con distribución }t\\text{ con }df=n-(k+1)\\text{ grados de libertad.} \\\\ &amp;~~\\text{La estadística de prueba es }t-\\mathrm{ratio} = (b_j -d)/se(b_j) . \\\\ \\hline \\end{array} } \\] Ejemplo: Seguro de Vida Temporal - Continuación. Una convención útil al reportar los resultados de un análisis estadístico es colocar el error estándar de una estadística entre paréntesis debajo de esa estadística. Así, por ejemplo, en nuestra regresión de LNFACE sobre EDUCATION, NUMHH, y LNINCOME, la ecuación de regresión estimada es: \\[ \\begin{array}{lccccc} \\widehat{LNFACE} = &amp;2.584 ~ + &amp;0.206~ \\text{EDUCATION} + &amp;0.306 ~\\text{NUMHH} + &amp;0.494 ~\\text{LNINCOME}. \\\\ \\text{error estándar} &amp;(0.846) &amp;(0.039) &amp;(0.063) &amp;(0.078). \\end{array} \\] Para ilustrar el cálculo de los errores estándar, primero notemos que, según la Tabla 3.3, tenemos que la desviación estándar residual es \\(s=1.525\\). Usando un paquete estadístico, tenemos \\[ \\small{ (\\mathbf{X^{\\prime}X})^{-1} = \\begin{pmatrix} 0.307975 &amp; -0.004633 &amp; -0.002131 &amp; -0.020697 \\\\ -0.004633 &amp; 0.000648 &amp; 0.000143 &amp; -0.000467 \\\\ -0.002131 &amp; 0.000143 &amp; 0.001724 &amp; -0.000453 \\\\ -0.020697 &amp; -0.000467 &amp; -0.000453 &amp; 0.002585 \\end{pmatrix}. } \\] Para ilustrar, podemos calcular \\(se(b_3)=s \\times \\sqrt{0.002585} = 0.078\\), como se indicó antes. El cálculo de los errores estándar, así como las correspondientes estadísticas \\(t\\), es parte del resultado estándar de los programas estadísticos y no necesita ser realizado por los usuarios. Nuestro objetivo aquí es ilustrar las ideas subyacentes a los cálculos rutinarios. Con esta información, podemos calcular inmediatamente las razones \\(t\\) para verificar si un coeficiente asociado con una variable individual es significativamente diferente de cero. Por ejemplo, la razón \\(t\\) para la variable LNINCOME es \\(t(b_3) = \\frac{0.494}{0.078} = 6.3\\). La interpretación es que \\(b_3\\) está más de cuatro errores estándar por encima de cero, y por lo tanto LNINCOME es una variable importante en el modelo. Más formalmente, podemos estar interesados en probar la hipótesis nula de que \\(H_0:\\beta_3 = 0\\) frente a \\(H_0:\\beta_3 \\neq 0\\). A un nivel de significancia del 5%, el valor \\(t\\) es 1.96, porque \\(df=275-(1+3)=271\\). Por lo tanto, rechazamos la hipótesis nula en favor de la hipótesis alternativa, que el ingreso logarítmico (LNINCOME) es importante para determinar el monto logarítmico del seguro. 3.4.2 Intervalos de Confianza Los intervalos de confianza para los parámetros son otra forma de describir la fuerza de la contribución de la \\(j\\)-ésima variable explicativa. La estadística \\(b_j\\) se llama una estimación puntual del parámetro \\(\\beta_j\\). Para proporcionar un rango de confianza, usamos el intervalo de confianza: \\[\\begin{equation} b_j \\pm t_{n-(k+1),1-\\alpha /2}~se(b_j). \\tag{3.10} \\end{equation}\\] Aquí, el valor \\(t\\) \\(t_{n-(k+1),1-\\alpha /2}\\) es un percentil de la distribución \\(t\\) con \\(df=n-(k+1)\\) grados de libertad. Usamos el mismo valor \\(t\\) que en la prueba de hipótesis bilateral. De hecho, hay una dualidad entre el intervalo de confianza y la prueba de hipótesis bilateral. Por ejemplo, no es difícil comprobar que si un valor hipotético cae fuera del intervalo de confianza, entonces \\(H_0\\) será rechazado en favor de \\(H_{a}\\). Además, el conocimiento del \\(p\\)-valor, la estimación puntual y el error estándar se puede utilizar para determinar un intervalo de confianza. 3.4.3 Gráficos de Variables Añadidas Para representar datos multivariados de forma gráfica, hemos visto que una matriz de dispersión es una herramienta útil. Sin embargo, la principal limitación de la matriz de dispersión es que solo captura relaciones entre pares de variables. Cuando los datos se pueden resumir utilizando un modelo de regresión, una herramienta gráfica que no tiene esta limitación es el gráfico de variables añadidas. El gráfico de variables añadidas también se llama gráfico de regresión parcial porque, como veremos, se construye en términos de los residuos de ciertos ajustes de regresión. También veremos que el gráfico de variables añadidas se puede resumir en términos de un coeficiente de correlación parcial, proporcionando así un vínculo entre correlación y regresión. Para introducir estas ideas, trabajamos en el contexto del siguiente ejemplo. Ejemplo: Precios de Refrigeradores. ¿Qué características de un refrigerador son importantes para determinar su precio (PRICE)? Aquí consideramos varias características de un refrigerador, incluyendo el tamaño del refrigerador en pies cúbicos (RSIZE), el tamaño del compartimento del congelador en pies cúbicos (FSIZE), la cantidad promedio de dinero gastado por año para operar el refrigerador (ECOST, por “costo de energía”), el número de estantes en las puertas del refrigerador y del congelador (SHELVES), y el número de características (FEATURES). La variable de características incluye estantes para latas, cajones transparentes, fabricantes de hielo, portahuevos, y así sucesivamente. Tanto los consumidores como los fabricantes están interesados en modelos de precios de refrigeradores. Otras cosas iguales, los consumidores generalmente prefieren refrigeradores más grandes con menores costos de energía y que tengan más características. Debido a las fuerzas de la oferta y la demanda, esperaríamos que los consumidores paguen más por estos refrigeradores. Un refrigerador más grande con menores costos de energía y que tiene más características a un precio similar se considera una ganga para el consumidor. ¿Cuánto estaría dispuesto a pagar el consumidor por este espacio adicional? Un modelo de precios para refrigeradores en el mercado proporciona algo de información sobre esta cuestión. Para este fin, analizamos datos de \\(n=37\\) refrigeradores. La Tabla 3.7 proporciona las estadísticas descriptivas básicas para la variable de respuesta PRICE y las cinco variables explicativas. De esta tabla, vemos que el precio promedio de los refrigeradores es \\(\\overline{y} = \\$626.40\\), con una desviación estándar de \\(s_{y} = \\$139.80\\). De manera similar, la cantidad promedio anual para operar un refrigerador, o ECOST promedio, es $70.51. knitr::kable(2, caption = &quot;Silly. Create a table just to update the counter...&quot;) Table 3.4: Silly. Create a table just to update the counter… x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Table 3.5: Silly. x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Table 3.6: Silly. x 2 Table 3.7: Estadísticas Descriptivas para cada variable para 37 Refrigeradores Media Mediana Desviación Estándar Mínimo Máximo ECOST 70.514 68.0 9.140 60.0 94.0 RSIZE 13.400 13.2 0.600 12.6 14.7 FSIZE 5.184 5.1 0.938 4.1 7.4 SHELVES 2.514 2.0 1.121 1.0 5.0 FEATURES 3.459 3.0 2.512 1.0 12.0 PRICE 626.351 590.0 139.790 460.0 1200.0 Para analizar las relaciones entre pares de variables, la Tabla 3.8 proporciona una matriz de coeficientes de correlación. En la tabla, vemos que hay relaciones lineales fuertes entre PRICE y tanto el espacio del congelador (FSIZE) como el número de FEATURES. Sorprendentemente, también hay una fuerte correlación positiva entre PRICE y ECOST. Recordemos que ECOST es el costo de energía; uno podría esperar que los refrigeradores de mayor precio deberían tener menores costos de energía. Table 3.8: Matriz de Coeficientes de Correlación ECOST RSIZE FSIZE SHELVES FEATURES RSIZE -0.033 FSIZE 0.855 -0.235 SHELVES 0.188 -0.363 0.251 FEATURES 0.334 -0.096 0.439 0.16 PRICE 0.522 -0.024 0.72 0.4 0.697 R Code to Produce Tables 3.7 and 3.8 Refrig &lt;- read.csv(&quot;CSVData/Refrigerator.csv&quot;, header=TRUE) # TABLE 3.7 SUMMARY STATISTICS varFrig &lt;- c(&quot;ECOST&quot;, &quot;RSIZE&quot;, &quot;FSIZE&quot;, &quot;SHELVES&quot;, &quot;FEATURES&quot;, &quot;PRICE&quot;) Refrig1 &lt;- Refrig[varFrig] tableMat &lt;- BookSummStats(Refrig1) colnames(tableMat) &lt;- c(&quot;Media&quot; , &quot;Mediana&quot; , &quot;Desviación Estándar&quot; , &quot;Mínimo&quot; , &quot;Máximo&quot;) rownames(tableMat) &lt;- varFrig # tableMat1[1:2,] &lt;- format(round(tableMat[1:2,], digits=0), big.mark = &#39;,&#39;) TableGen1(TableData=tableMat, TextTitle=&#39;Estadísticas Descriptivas para cada variable para 37 Refrigeradores&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:5, ColWidth = ColWidth5) tableCor &lt;- cor(Refrig1) tableCor &lt;- round(tableCor, digits = 3) tableCor[upper.tri(tableCor, diag = TRUE)] &lt;- &quot;&quot; tablePrint &lt;- tableCor[-1,] tablePrint &lt;- tablePrint[,-6] TableGen1(TableData=tablePrint, TextTitle=&#39;Matriz de Coeficientes de Correlación&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:5, ColWidth = ColWidth5) Se ajustó un modelo de regresión a los datos. La ecuación de regresión ajustada aparece en la Tabla 3.9, con \\(s=60.65\\) y \\(R^2=83.8\\%\\). Table 3.9: Modelo Ajustado de Precio de Refrigeradores Coeficiente Desviación Estándar \\(t\\)-Ratio Intercepto 798.00 271.400 -2.9 ECOST -6.96 2.275 -3.1 RSIZE 76.50 19.440 3.9 FSIZE 137.00 23.760 5.8 SHELVES 37.90 9.886 3.8 FEATURES 23.80 4.512 5.3 De la Tabla 3.9, las variables explicativas parecen ser buenos predictores de los precios de los refrigeradores. Juntas, estas variables explican el 83.8% de la variabilidad. Para entender los precios, el error típico ha disminuido de \\(s_{y} = \\$139.80\\) a \\(s = \\$60.65\\). Los \\(t\\)-ratios para cada una de las variables explicativas superan el valor absoluto de dos, lo que indica que cada variable es importante de manera individual. Lo que sorprende del ajuste de la regresión es el coeficiente negativo asociado con el costo de energía. Recordemos que podemos interpretar \\(b_{ECOST} = -6.96\\) como que, por cada aumento de un dólar en ECOST, esperamos que el PRICE disminuya en $6.96. Esta relación negativa está conforme con nuestra intuición económica. Sin embargo, es sorprendente que el mismo conjunto de datos nos haya mostrado que existe una relación positiva entre PRICE y ECOST. Esta aparente anomalía se debe a que la correlación solo mide las relaciones entre pares de variables, mientras que el ajuste de la regresión puede considerar varias variables simultáneamente. Para proporcionar más información sobre esta aparente anomalía, ahora introducimos el gráfico de variables añadidas. Producción de un Gráfico de Variables Añadidas El gráfico de variables añadidas proporciona vínculos adicionales entre la metodología de regresión y herramientas más fundamentales como los diagramas de dispersión y las correlaciones. Trabajamos en el contexto del Ejemplo del Precio de los Refrigeradores para demostrar la construcción de este gráfico. Procedimiento para producir un gráfico de variables añadidas. Realiza una regresión de PRICE sobre RSIZE, FSIZE, SHELVES y FEATURES, omitiendo ECOST. Calcula los residuos de esta regresión, los cuales etiquetamos como \\(e_1\\). Realiza una regresión de ECOST sobre RSIZE, FSIZE, SHELVES y FEATURES. Calcula los residuos de esta regresión, los cuales etiquetamos como \\(e_2\\). Grafica \\(e_1\\) versus \\(e_2\\). Este es el gráfico de variables añadidas de PRICE versus ECOST, controlando por los efectos de RSIZE, FSIZE, SHELVES y FEATURES. Este gráfico aparece en la Figura 3.4. Figure 3.4: Un gráfico de variables añadidas. Los residuos de la regresión de PRICE sobre las variables explicativas, omitiendo ECOST, están en el eje horizontal. En el eje vertical están los residuos de la regresión de ECOST sobre las demás variables explicativas. El coeficiente de correlación es -0.48. Código en R para producir la Figura 3.4 model.refrig1 &lt;- lm(PRICE ~ RSIZE + FSIZE + SHELVES + FEATURES, data = Refrig1) #summary(model.refrig1) model.refrig2 &lt;- lm(ECOST ~ RSIZE + FSIZE + SHELVES + FEATURES, data = Refrig1) #summary(model.refrig2) par(mar=c(4.1,4,.1,1), cex=1.1) plot(residuals(model.refrig2),residuals(model.refrig1), xlab=expression(e[1]),ylab=&quot;&quot;, las=1) mtext(expression(e[2]), side=2, at=0, line=3, las=1, cex=1.1) El error \\(\\varepsilon\\) puede interpretarse como la variación natural en una muestra. En muchas situaciones, esta variación natural es pequeña en comparación con los patrones evidentes en el componente de regresión no aleatorio. Por lo tanto, es útil pensar en el error, \\(\\varepsilon_i = y_i - \\left( \\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik} \\right)\\), como la respuesta después de controlar por los efectos de las variables explicativas. En la Sección 3.3, vimos que un error aleatorio puede aproximarse mediante un residuo, \\(e_i = y_i - \\left( b_0 + b_1 x_{i1} + \\cdots + b_k x_{ik} \\right)\\). De la misma manera, podemos pensar en un residuo como la respuesta después de “controlar” los efectos de las variables explicativas. Con esto en mente, podemos interpretar el eje vertical de la Figura 3.4 como el precio del refrigerador (PRICE) controlado por los efectos de RSIZE, FSIZE, SHELVES y FEATURES. De manera similar, podemos interpretar el eje horizontal como ECOST controlado por los efectos de RSIZE, FSIZE, SHELVES y FEATURES. El gráfico proporciona entonces una representación gráfica de la relación entre PRICE y ECOST, después de controlar por las demás variables explicativas. En comparación, un diagrama de dispersión de PRICE y ECOST (no mostrado aquí) no controla por las demás variables explicativas. Por lo tanto, es posible que la relación positiva entre PRICE y ECOST no se deba a una relación causal, sino más bien a una o más variables adicionales que hacen que ambas variables sean grandes. Por ejemplo, de la Tabla 3.8, vemos que el tamaño del congelador (FSIZE) está positivamente correlacionado tanto con ECOST como con PRICE. Ciertamente parece razonable que aumentar el tamaño de un congelador cause que tanto el costo de energía como el precio aumenten. Más bien, la correlación positiva puede deberse al hecho de que valores grandes de FSIZE significan valores grandes tanto de ECOST como de PRICE. Las variables omitidas en una regresión se llaman variables omitidas. Esta omisión podría causar un problema serio en el ajuste del modelo de regresión; los coeficientes de regresión podrían no solo ser significativamente fuertes cuando no deberían serlo, sino que también podrían tener el signo incorrecto. Seleccionar el conjunto adecuado de variables para incluir en el modelo de regresión es una tarea importante; es el tema de los Capítulos 5 y 6. 3.4.4 Coeficientes de Correlación Parcial Como vimos en el Capítulo 2, una estadística de correlación es una cantidad útil para resumir gráficos. La correlación en el gráfico de variables añadidas se llama coeficiente de correlación parcial. Se define como la correlación entre los residuos \\(e_1\\) y \\(e_2\\) y se denota por \\(r(y,x_j | x_1, \\ldots, x_{j-1}, x_{j+1}, \\ldots, x_k)\\). Debido a que resume un gráfico de variables añadidas, podemos interpretar \\(r(y,x_j | x_1, \\ldots, x_{j-1}, x_{j+1}, \\ldots, x_k)\\) como la correlación entre \\(y\\) y \\(x_j\\), en presencia de las otras variables explicativas. Por ejemplo, la correlación entre PRICE y ECOST en presencia de las otras variables explicativas es -0.48. El coeficiente de correlación parcial también se puede calcular utilizando \\[\\begin{equation} r(y,x_j | x_1, \\ldots, x_{j-1}, x_{j+1}, \\ldots, x_k) = \\frac{t(b_j)}{\\sqrt{t(b_j)^2 + n - (k + 1)}}. \\tag{3.11} \\end{equation}\\] Aquí, \\(t(b_j)\\) es el \\(t\\)-ratio para \\(b_j\\) de una regresión de \\(y\\) sobre \\(x_1, \\ldots, x_k\\) (incluyendo la variable \\(x_j\\)). Un aspecto importante de esta ecuación es que nos permite calcular coeficientes de correlación parcial ejecutando solo una regresión. Por ejemplo, en la Tabla 3.9, la correlación parcial entre PRICE y ECOST en presencia de las otras variables explicativas es \\(\\frac{-3.1}{\\sqrt{(-3.1)^2 + 37 - (5 + 1)}} \\approx -0.48\\). El cálculo de coeficientes de correlación parcial es más rápido cuando se usa la relación con el \\(t\\)-ratio, pero puede fallar en detectar relaciones no lineales. La información en la Tabla 3.9 nos permite calcular los cinco coeficientes de correlación parcial en el Ejemplo del Precio de los Refrigeradores después de ejecutar solo una regresión. El procedimiento de tres pasos para producir gráficos de variables añadidas requiere diez regresiones, dos para cada una de las cinco variables explicativas. Por supuesto, al producir gráficos de variables añadidas, podemos detectar relaciones no lineales que son omitidas por los coeficientes de correlación. Los coeficientes de correlación parcial proporcionan otra interpretación para los \\(t\\)-ratios. La ecuación muestra cómo calcular una estadística de correlación a partir de un \\(t\\)-ratio, proporcionando así otro vínculo entre la correlación y el análisis de regresión. Además, de la ecuación vemos que cuanto mayor es el \\(t\\)-ratio, mayor es el coeficiente de correlación parcial. Es decir, un \\(t\\)-ratio alto significa que existe una gran correlación entre la respuesta y la variable explicativa, controlando por las otras variables explicativas. Esto proporciona una respuesta parcial a la pregunta que suelen hacer los consumidores de análisis de regresión: “¿Cuál es la variable más importante?” 3.5 Algunas Variables Explicativas Especiales El modelo de regresión lineal es la base de una rica familia de modelos. Esta sección ofrece varios ejemplos para ilustrar la riqueza de esta familia. Estos ejemplos demuestran el uso de (i) variables binarias, (ii) transformación de variables explicativas y (iii) términos de interacción. Esta sección también sirve para subrayar el significado del adjetivo lineal en la frase “regresión lineal”; la función de regresión es lineal en los parámetros pero puede ser una función altamente no lineal de las variables explicativas. 3.5.1 Variables Binarias Las variables categóricas proporcionan una etiqueta numérica para mediciones de observaciones que caen en grupos distintos, o categorías. Debido a la agrupación, las variables categóricas son discretas y generalmente toman un número finito de valores. Comenzamos nuestra discusión con una variable categórica que puede tomar uno de solo dos valores, una variable binaria. Una discusión más detallada sobre las variables categóricas es el tema del Capítulo 4. Ejemplo: Seguro de Vida a Término - Continuación. Ahora consideramos el estado civil del encuestado. En la Encuesta de Finanzas del Consumidor, los encuestados pueden seleccionar entre varias opciones que describen su estado civil, incluyendo “casado”, “viviendo con una pareja”, “divorciado”, y así sucesivamente. El estado civil no se mide de manera continua, sino que toma valores que caen en grupos distintos. En este capítulo, agrupamos a los encuestados de acuerdo a si están solteros o no, definidos para incluir a aquellos que están separados, divorciados, viudos, nunca casados, y que no están casados ni viviendo con una pareja. El Capítulo 4 presentará un análisis más completo del estado civil incluyendo categorías adicionales. La variable binaria SINGLE se define como uno si el encuestado está soltero y 0 en caso contrario. La variable SINGLE también se conoce como una variable indicadora porque indica si el encuestado está soltero o no. Otro nombre para este tipo importante de variable es una variable dummy. Podríamos usar 0 y 100, o 20 y 36, o cualquier otro par de valores distintos. Sin embargo, 0 y 1 son convenientes para la interpretación de los valores de los parámetros, discutidos a continuación. Para simplificar la discusión, presentamos ahora un modelo utilizando solo LNINCOME y SINGLE como variables explicativas. Para nuestra muestra de \\(n = 275\\) hogares, 57 son solteros y los otros 218 no lo son. Para ver las relaciones entre LNFACE, LNINCOME y SINGLE, la Figura 3.5 introduce un gráfico de letras de LNFACE versus LNINCOME, con SINGLE como la variable de código. Podemos ver que la Figura 3.5 es un diagrama de dispersión de LNFACE versus LNINCOME, usando 50 hogares seleccionados aleatoriamente de nuestra muestra de 275 (para mayor claridad del gráfico). Sin embargo, en lugar de usar el mismo símbolo de gráfico para cada observación, hemos codificado los símbolos para que podamos entender fácilmente el comportamiento de una tercera variable, SINGLE. En otras aplicaciones, puede optar por usar otros símbolos de gráfico como \\(\\clubsuit\\), \\(\\heartsuit\\), \\(\\spadesuit\\), y así sucesivamente, o usar diferentes colores, para codificar información adicional. Para esta aplicación, se seleccionaron los códigos de letras “S” para solteros y “o” para otros porque recuerdan al lector la naturaleza del esquema de codificación. Independientemente del esquema de codificación, el punto importante es que un gráfico de letras es un dispositivo útil para representar gráficamente tres o más variables en dos dimensiones. La principal restricción es que la información adicional debe estar categorizada, como con las variables binarias, para que el esquema de codificación funcione. Figure 3.5: Gráfico de letras de LNFACE versus LNINCOME, con el código de letra ‘S’ para solteros y ‘o’ para otros. Las líneas de regresión ajustadas han sido superpuestas. La línea inferior es para solteros y la línea superior es para otros. La Figura 3.5 sugiere que LNFACE es más bajo para aquellos solteros que para otros para un nivel dado de ingreso. Por lo tanto, ahora consideramos un modelo de regresión, LNFACE = β_0 + β_1 LNINCOME + β_2 SINGLE + ϵ. La función de regresión se puede escribir como: \\[ \\text{E } y = \\begin{cases} \\beta_0 + \\beta_1 \\text{LNINCOME} &amp; \\text{para otros encuestados} \\\\ \\beta_0 + \\beta_2 + \\beta_1 \\text{LNINCOME} &amp; \\text{para encuestados solteros} \\end{cases} \\] La interpretación de los coeficientes del modelo difiere del caso de variables continuas. Para variables continuas como LNINCOME, interpretamos \\(\\beta_1\\) como el cambio esperado en \\(y\\) por unidad de cambio en el ingreso logarítmico, manteniendo fijas otras variables. Para variables binarias como SINGLE, interpretamos \\(\\beta_2\\) como el aumento esperado en \\(y\\) al pasar del nivel base de SINGLE (=0) al nivel alternativo. Así, aunque tenemos un modelo para ambos estados civiles, podemos interpretar el modelo usando dos ecuaciones de regresión, una para cada tipo de estado civil. Al escribir una ecuación separada para cada estado civil, hemos simplificado una complicada ecuación de regresión múltiple. A veces, es más fácil comunicar una serie de relaciones simples en comparación con una sola relación compleja. Aunque la interpretación para variables explicativas binarias difiere de la continua, el método de estimación de mínimos cuadrados ordinarios sigue siendo válido. Para ilustrar, la versión ajustada del modelo anterior es \\[ \\small{ \\begin{array}{cclll} \\widehat{LNFACE} &amp; = &amp; 5.09 &amp; + 0.634 \\text{LNINCOME} &amp; - 0.800 \\text{SINGLE} .\\\\ \\text{error estándar} &amp; &amp; (0.89) &amp; ~~(0.078) &amp; ~(0.248) \\\\ \\end{array} } \\] Para interpretar \\(b_2 = -0.800\\), decimos que esperamos que el logaritmo de la cara sea menor en 0.80 para un encuestado que es soltero en comparación con la otra categoría. Esto asume que otras cosas, como el ingreso, permanecen constantes. Para una interpretación gráfica, las dos líneas de regresión ajustadas están superpuestas en la Figura 3.5. 3.5.2 Transformación de Variables Explicativas Los modelos de regresión tienen la capacidad de representar relaciones complejas y no lineales entre la respuesta esperada y las variables explicativas. Por ejemplo, los textos tempranos sobre regresión, como Plackett (1960, Capítulo 6), dedican un capítulo completo al modelo de regresión polinómica, \\[\\begin{equation} \\text{E } y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_p x^p. \\tag{3.12} \\end{equation}\\] Aquí, la idea es que un polinomio de orden \\(p\\) en \\(x\\) puede usarse para aproximar funciones generales y desconocidas no lineales de \\(x\\). El tratamiento moderno de la regresión polinómica no requiere un capítulo completo porque el modelo en la ecuación (3.12) puede expresarse como un caso especial del modelo de regresión lineal. Es decir, con la función de regresión en la ecuación (3.5), \\(\\text{E } y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k\\), podemos elegir \\(k = p\\) y \\(x_1 =x\\), \\(x_2 = x^2\\), \\(\\ldots\\), \\(x_p = x^p\\). Así, con estas elecciones de variables explicativas, podemos modelar una función altamente no lineal de \\(x\\). No estamos restringidos a potencias de \\(x\\) en nuestra elección de transformaciones. Por ejemplo, el modelo \\(\\text{E } y = \\beta_0 + \\beta_1 \\ln x\\), proporciona otra forma de representar una curva suavemente inclinada en \\(x\\). Este modelo puede escribirse como un caso especial del modelo de regresión lineal básico usando \\(x^{\\ast} = \\ln x\\) como la versión transformada de \\(x\\). Las transformaciones de las variables explicativas no tienen que ser funciones suaves. Para ilustrar, en algunas aplicaciones, es útil categorizar una variable explicativa continua. Por ejemplo, supongamos que \\(x\\) representa el número de años de educación, que varía de 0 a 17. Si nos basamos en información auto-reportada por nuestra muestra de personas mayores, puede haber una cantidad considerable de error en la medición de \\(x\\). Podríamos optar por usar una transformación menos informativa, pero más confiable, de \\(x\\) como \\(x^{\\ast}\\), una variable binaria para haber completado 13 años de escolaridad (terminar la secundaria). Formalmente, codificaríamos $x^{} = 1$ si \\(x \\geq 13\\) y \\(x^{\\ast} = 0\\) si \\(x &lt; 13\\). Así, hay varias formas en que las funciones no lineales de las variables explicativas pueden usarse en el modelo de regresión. Un ejemplo de un modelo de regresión no lineal es \\(y = \\beta_0 + \\exp (\\beta_1 x) + \\varepsilon.\\) Estos típicamente surgen en aplicaciones científicas de regresiones donde hay principios científicos fundamentales que guían el desarrollo del modelo complejo. 3.5.3 Términos de Interacción Hasta ahora hemos discutido cómo las variables explicativas, digamos \\(x_1\\) y \\(x_2\\), afectan la respuesta media de manera aditiva, es decir, \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). Aquí, esperamos que \\(y\\) aumente en \\(\\beta_1\\) por cada unidad que aumente \\(x_1\\), manteniendo \\(x_2\\) fijo. ¿Qué pasa si la tasa marginal de aumento de \\(\\mathrm{E}~y\\) difiere para valores altos de \\(x_2\\) en comparación con valores bajos de \\(x_2\\)? Una forma de representar esto es crear una variable de interacción \\(x_3 = x_1 \\times x_2\\) y considerar el modelo \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\). Con este modelo, el cambio en el esperado \\(y\\) por cada unidad de cambio en \\(x_1\\) ahora depende de \\(x_2\\). Formalmente, podemos evaluar pequeños cambios en la función de regresión como: \\[ \\frac{\\partial \\mathrm{E}~y}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} \\left(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\right) = \\beta_1 + \\beta_3 x_2 . \\] De esta manera, podemos permitir funciones más complicadas de \\(x_1\\) y \\(x_2\\). La Figura 3.6 ilustra esta estructura compleja. A partir de esta figura y los cálculos anteriores, vemos que los cambios parciales de \\(\\mathrm{E}~y\\) debido al movimiento de \\(x_1\\) dependen del valor de \\(x_2\\). De esta manera, decimos que los cambios parciales debidos a cada variable no son independientes, sino que “se mueven juntos.” Figure 3.6: Gráfico de \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\) versus \\(x_1\\) y \\(x_2\\). De manera más general, un término de interacción es una variable que se crea como una función no lineal de dos o más variables explicativas. Estos términos especiales, aunque nos permiten explorar una familia rica de funciones no lineales, pueden considerarse casos especiales del modelo de regresión lineal. Para hacer esto, simplemente creamos la variable de interés y tratamos este nuevo término como otra variable explicativa. Por supuesto, no todas las variables que creamos serán útiles. En algunos casos, la variable creada será tan similar a las variables ya presentes en nuestro modelo que no nos proporcionará nueva información. Afortunadamente, podemos usar pruebas \\(t\\) para verificar si la nueva variable es útil. Además, el Capítulo 4 presentará una prueba para decidir si un grupo de variables es útil. La función que usamos para crear una variable de interacción debe ser más que una combinación lineal de otras variables explicativas. Por ejemplo, si usamos \\(x_3 = x_1 + x_2\\), no podremos estimar todos los parámetros. El Capítulo 5 presentará algunas técnicas para ayudar a evitar situaciones en las que una variable es una combinación lineal de las demás. Para darles una idea de la amplia variedad de aplicaciones potenciales de variables explicativas especiales, ahora presentamos una serie de ejemplos breves. Ejemplo: Seguro de Vida a Término - Continuación. ¿Cómo interpretamos la interacción de una variable binaria con una variable continua? Para ilustrar, consideremos un modelo de regresión para Seguro de Vida a Término, \\[ \\mathrm{LNFACE} = \\beta_0 + \\beta_1 \\mathrm{LNINCOME} + \\beta_2 \\mathrm{SINGLE} + \\beta_3 \\mathrm{LNINCOME*SINGLE} + \\varepsilon . \\] En este modelo, hemos creado una tercera variable explicativa a través de la interacción de LNINCOME y SINGLE. La función de regresión se puede escribir como: \\[ \\mathrm{E}~y = \\begin{cases} \\beta_0 + \\beta_1 \\mathrm{LNINCOME}, &amp; \\text{para otros encuestados}, \\\\ \\beta_0 + \\beta_2 + (\\beta_1 + \\beta_3) \\mathrm{LNINCOME}, &amp; \\text{para encuestados solteros}. \\end{cases} \\] Así, a través de este único modelo con cuatro parámetros, podemos crear dos líneas de regresión separadas, una para los solteros y otra para los demás. La Figura 3.7 muestra las dos líneas de regresión ajustadas para nuestros datos. Figure 3.7: Gráfico de LNFACE versus LNINCOME, con el código de letra S para solteros y o para otros. Las líneas de regresión ajustadas han sido superpuestas. La línea inferior es para solteros y la línea superior es para otros. Ejemplo: Gastos de Compañías de Seguros de Vida. En una industria de seguros de vida bien desarrollada, minimizar los gastos es crucial para la posición competitiva de una compañía. Segal (2002) analizó datos contables anuales de más de 100 empresas para el período 1995-1998, inclusive, utilizando una base de datos de la Asociación Nacional de Comisionados de Seguros (NAIC) y otra información reportada. Segal modeló los gastos generales de la compañía como una función de la producción de la empresa y el precio de los insumos. La producción consiste en la producción de seguros, medida por \\(x_1\\) a \\(x_5\\), descritos en Tabla 3.10. Segal también consideró el cuadrado de cada salida, así como un término de interacción con una variable binaria \\(D\\) que indica si la empresa utiliza o no una sucursal para distribuir sus productos. (En una sucursal, los gerentes de campo son empleados de la empresa, no agentes independientes.) Tabla 3.10.Veintitrés Coeficientes de Regresión de un Modelo de Costos de Gastos \\[ \\small{ \\begin{array}{l|rrrr} \\hline &amp; \\text{Variable} &amp; &amp;\\text{Variable}&amp;\\text{ Cuadrada} \\\\ &amp; \\text{Base} &amp; \\text{Interacción} &amp; \\text{Base} &amp; \\text{Interacción} \\\\ &amp; &amp; \\text{con}~~ &amp; &amp; \\text{con}~~ \\\\ \\text{Variable} &amp; (D=0) &amp; (D=1) &amp; (D=0) &amp; (D=1)\\\\ \\hline \\text{Número de Pólizas de Vida Emitidas } (x_1) &amp; -0.454 &amp; 0.152 &amp; 0.032 &amp; -0.007 \\\\ \\text{Cantidad de Seguro de Vida a Término Vendido } (x_2) &amp; 0.112 &amp; -0.206 &amp; 0.002 &amp; 0.005 \\\\ \\text{Cantidad de Seguro de Vida Entera Vendido } (x_3) &amp; -0.184 &amp; 0.173 &amp; 0.008 &amp; -0.007 \\\\ \\text{Total de Consideraciones de Anualidades } (x_4) &amp; 0.098 &amp; -0.169 &amp; -0.003 &amp; 0.009 \\\\ \\text{Total de Primas de Accidentes y Salud } (x_5) &amp;-0.171 &amp; 0.014 &amp; 0.010 &amp; 0.002 \\\\ \\text{Intercepto } &amp; 7.726 &amp; &amp; &amp; \\\\ \\text{Precio del Trabajo (PL)} &amp; 0.553 &amp; &amp; &amp; \\\\ \\text{Precio del Capital (PC)} &amp; 0.102 &amp; &amp; &amp; \\\\ \\hline \\end{array} \\\\ \\begin{array}{l} \\textit{Nota: } x_1 \\text{ a } x_5 \\text{ están en unidades logarítmicas}.\\\\ \\textit{Fuente: Segal (2002)} \\end{array} } \\] Para los insumos de precios, el precio del trabajo (\\(PL\\)) se define como el costo total de empleados y agentes dividido por su número, en unidades logarítmicas. El precio del capital (\\(PC\\)) se aproxima por la relación del gasto en capital con el número de empleados y agentes, también en unidades logarítmicas. El precio de los materiales consiste en gastos distintos al trabajo y al capital divididos por el número de pólizas vendidas y terminadas durante el año. No aparece directamente como una variable explicativa. Más bien, Segal tomó la variable dependiente (\\(y\\)) como los gastos totales de la compañía divididos por el precio de los materiales, nuevamente en unidades logarítmicas. Con estas definiciones de variables, Segal estimó la siguiente función de regresión: \\[ \\mathrm{E~}y=\\beta_0 + \\sum_{j=1}^5 \\left( \\beta_j x_j + \\beta_{j+5} D x_j + \\beta_{j+10} x_j^2 + \\beta_{j+15}D x_j^2 \\right) + \\beta_{21} PL + \\beta_{22} PC. \\] Las estimaciones de los parámetros aparecen en Tabla 3.10. Por ejemplo, el cambio marginal en \\(\\mathrm{E}~y\\) por unidad de cambio en \\(x_1\\) es: \\[ \\frac{\\partial ~ \\mathrm{E}~y}{\\partial x_1}= \\beta_1 + \\beta_{6} D + 2 \\beta_{11} x_1 + 2 \\beta_{16}D x_1, \\] que se estima como $ -0.454 + 0.152 D + (0.064 - 0.014 D) x_1$. Para estos datos, el número mediano de pólizas emitidas fue \\(x_1=15,944\\). En este valor de \\(x_1\\), el cambio marginal estimado es $ -0.454 + 0.152 D + (0.064 - 0.014 D) (15944) = 0.165 + 0.017 D,$ o 0.165 para la base \\((D=0)\\) y 0.182 para compañías de sucursales \\((D=1)\\). Estas estimaciones son elasticidades, como se define en la Sección 3.2.2. Para interpretar estos coeficientes más a fondo, dejemos que \\(COST\\) represente los gastos generales totales de la compañía y \\(NUMPOL\\) represente el número de pólizas de vida emitidas. Entonces, para compañías de sucursales \\((D=1)\\), tenemos: \\[ 0.182 \\approx \\frac{\\partial y }{\\partial x_1 } = \\frac{\\partial ~ \\mathrm{ln}~COST}{\\partial ~ \\mathrm{ln}~NUMPOL}= \\frac{ \\frac{\\partial ~ COST}{\\partial ~NUMPOL}} {\\frac{COST}{NUMPOL}}, \\] o \\(\\frac{\\partial ~ COST}{\\partial ~NUMPOL} \\approx 0.182 \\frac{COST}{NUMPOL}\\). El costo mediano es $15,992,000, por lo que el costo marginal por póliza en estos valores medianos es \\(0.182 \\times (15992000/15944) =\\) $182.55. Caso Especial: Funciones de Respuesta Curvilíneas Podemos expandir las funciones polinómicas de una variable explicativa para incluir varias variables explicativas. Por ejemplo, la respuesta esperada, o función de respuesta, para un modelo de segundo orden con dos variables explicativas es: \\[ \\mathrm{E} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2. \\] La Figura 3.8 ilustra esta función de respuesta. De manera similar, la función de respuesta para un modelo de segundo orden con tres variables explicativas es: \\[ \\mathrm{E} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{33} x_3^2 + \\beta_{12} x_1 x_2 + \\beta_{13} x_1 x_3 + \\beta_{23} x_2 x_3. \\] Cuando hay más de una variable explicativa, los modelos de tercer orden y de órdenes superiores rara vez se utilizan en aplicaciones. Figure 3.8: Gráfico de \\(\\mathrm{E}~y = \\beta_0 + \\beta_1~x_1 + \\beta_2~x_2 + \\beta_{11}~x_1^2 + \\beta_{22}~x_2^2 + \\beta_{12}~x_1~x_2\\) versus \\(x_1\\) y \\(x_2\\). Código R para producir la Figura 3.8 # FIGURA 3.8 X1 &lt;- seq(3, 16, length=15) X2 &lt;- seq(5, 15, length=15) f &lt;- function(X1,X2) {y &lt;- 50 + 2*X1 + 3*X2 + 3*(X1-10)*(X2-10)- .4*(X1-10)^2+.4*(X2-10)^2} y &lt;- outer(X1, X2, f) par(mar=c(0,2.1,0,.1)) persp(X1, X2, y, theta = 30, phi = 30, expand = 0.5, ticktype=&quot;detailed&quot;) Caso Especial: Funciones No Lineales de una Variable Continua En algunas aplicaciones, esperamos que la respuesta tenga cambios abruptos en el comportamiento en ciertos valores de una variable explicativa, incluso si la variable es continua. Por ejemplo, supongamos que estamos tratando de modelar las contribuciones benéficas de un individuo (\\(y\\)) en función de sus ingresos (\\(x\\)). Para los datos de 2007, un modelo simple que podríamos considerar es el que se muestra en la Figura 3.9. Figure 3.9: El cambio marginal en \\(\\mathrm{E}~y\\) es menor por debajo de $97,500. El parámetro \\(\\beta_2\\) representa la diferencia en las pendientes. Código R para producir la Figura 3.9 # FIGURA 3.9 x &lt;- seq(90000,105000,20) Ey &lt;- 20 + 1*x + 2*(x-97500)*(x&gt;97500) par(mar=c(4.1,3.1,.1,1), cex=1.3) plot(x,Ey, type=&quot;l&quot;,yaxt=&quot;n&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;E y&quot;, side=2,las=1, line=1.5,cex=1.1) text(102000,102000,expression(beta[1]+beta[2]),cex=1.1) text(94000,92000,expression(beta[1]),cex=1.1) Una justificación para este modelo es que, en 2007, los individuos pagaban un 7.65% de sus ingresos en impuestos de Seguridad Social hasta $97,500. No se aplican impuestos de Seguridad Social a los ingresos que superan los $97,500. Así, una teoría es que, para los ingresos superiores a $97,500, los individuos tienen más ingresos disponibles por dólar y, por lo tanto, deberían estar más dispuestos a hacer contribuciones benéficas. Para modelar esta relación, definamos la variable binaria \\(z\\) que sea cero si \\(x &lt; 97,500\\) y uno si \\(x \\ge 97,500\\). Definamos la función de regresión como \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x + \\beta_2 z (x - 97,500)\\). Esto se puede escribir como: \\[ \\mathrm{E}~y = \\begin{cases} \\beta_0 + \\beta_1 x &amp; x &lt; 97,500 \\\\ \\beta_0 - \\beta_2(97,500) + (\\beta_1+\\beta_2) x &amp; x \\geq 97,500 \\end{cases} \\] Para estimar este modelo, realizaríamos una regresión de \\(y\\) sobre dos variables explicativas, \\(x_1 = x\\) y \\(x_2 = z \\times (x - 97,500)\\). Si \\(\\beta_2 &gt; 0\\), entonces la tasa marginal de contribuciones benéficas es mayor para ingresos que superan los $97,500. La Figura 3.9 ilustra esta relación, conocida como regresión lineal por tramos o a veces un modelo de “barra rota”. El cambio abrupto en la Figura 3.9 en \\(x = 97,500\\) se llama “cambio de pendiente”. Tenemos relaciones lineales por encima y por debajo del cambio de pendiente y hemos usado una variable binaria para unir las dos partes. No estamos restringidos a un solo cambio de pendiente. Por ejemplo, supongamos que deseamos hacer un estudio histórico de los ingresos gravables federales para los contribuyentes solteros de 1992. Entonces, había tres tramos impositivos: la tasa marginal por debajo de 21,450 era del 15%, por encima de 51,900 era del 31%, y en medio era del 28%. Para este ejemplo, usaríamos dos cambios de pendiente, en 21,450 y 51,900. Además, la regresión lineal por tramos no está restringida a funciones de respuesta continuas. Por ejemplo, supongamos que estamos estudiando las comisiones pagadas a los corredores de bolsa (\\(y\\)) en función del número de acciones compradas por un cliente (\\(x\\)). Podríamos esperar ver la relación ilustrada en la Figura 3.10. Aquí, la discontinuidad en \\(x = 100\\) refleja los gastos administrativos de comerciar en lotes irregulares, ya que se llaman transacciones de menos de 100 acciones. El menor costo marginal para transacciones superiores a 100 acciones simplemente refleja las economías de escala al hacer negocios en volúmenes mayores. Un modelo de regresión para esto es \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 z x\\) donde \\(z = 0\\) si \\(x &lt; 100\\) y \\(z = 1\\) si \\(x \\geq 100\\). La función de regresión representada en la Figura 3.10 es: \\[ \\mathrm{E}~y = \\begin{cases} \\beta_0 + \\beta_1 x_1 &amp; x &lt; 100 \\\\ \\beta_0 + \\beta_2 + (\\beta_1+\\beta_3) x_1 &amp; x \\geq 100 \\end{cases} \\] Figure 3.10: Gráfico de comisiones esperadas (\\(\\mathrm{E}~y\\)) versus número de acciones negociadas (\\(x\\)). El cambio en \\(x=100\\) refleja ahorros en gastos administrativos. La pendiente más baja para \\(x \\ge 100\\) refleja economías de escala en gastos. Código R para producir la Figura 3.10 # FIGURA 3.10 par(mar=c(4.1,4.5,.1,.1)) x &lt;- seq(50,150,.1) Ey &lt;- 20 + 2*x - 1*(x-100)*(x&gt;100) - 20*(x&gt;100) plot(x,Ey, type=&quot;p&quot;,ylab=&quot;&quot;, font.lab=1, cex.lab=1.1, cex=.25, las=1) mtext(&quot;E y&quot;, side=2,las=1, line=2.8,cex=1.1) 3.6 Lectura Adicional y Referencias Para las pruebas de los resultados del Capítulo 3, remitimos al lector a Goldberger (1991). Los modelos de regresión no lineales se discuten, por ejemplo, en Bates y Watts (1988). El Capítulo 3 ha introducido los fundamentos de la regresión lineal múltiple. El Capítulo 4 ampliará el alcance al introducir variables categóricas y métodos de inferencia estadística para manejar varios coeficientes simultáneamente. El Capítulo 5 presentará técnicas para ayudarte a seleccionar variables apropiadas en un modelo de regresión lineal múltiple. El Capítulo 6 es un capítulo de síntesis, que discute la interpretación del modelo, la selección de variables y la recolección de datos. Referencias del Capítulo Bates, Douglas M. y Watts, D. G. (1988). Nonlinear Regression Analysis and its Applications. John Wiley &amp; Sons, Nueva York. Lemaire, Jean (2002). Why do females live longer than males? North American Actuarial Journal, 6(4), 21-37. Goldberger, Arthur (1991). A Course in Econometrics. Harvard University Press, Cambridge. Plackett, R.L. (1960). Regression Analysis. Clarendon Press, Oxford, Inglaterra. Segal, Dan (2002). An economic analysis of life insurance company expenses. North American Actuarial Journal, 6(4), 81-94. 3.7 Ejercicios 3.1. Considera un conjunto de datos ficticio de \\(n = 100\\) observaciones con \\(s_y = 100\\). Realizamos una regresión con tres variables explicativas para obtener \\(s = 50\\). Calcula el coeficiente de determinación ajustado, \\(R_a^2\\). Completa la tabla de ANOVA. \\[ \\small{ \\begin{array}{|l|l|c|l|} \\hline \\text{Tabla de ANOVA} \\\\ \\hline \\text{Fuente} &amp; \\text{Suma de Cuadrados} &amp; \\text{df} &amp; \\text{Cuadrado Medio} \\\\ \\hline \\text{Regresión} &amp; &amp; &amp; \\\\ \\text{Error} &amp; &amp; &amp; \\\\ \\text{Total} &amp; &amp; &amp; \\\\ \\hline \\end{array} } \\] Calcula el coeficiente de determinación (no ajustado), \\(R^2\\). 3.2. Considera un conjunto de datos ficticio de \\(n = 100\\) observaciones con \\(s_y = 80\\). Realizamos una regresión con tres variables explicativas para obtener \\(s = 50\\). También obtenemos \\[ \\small{ \\left(\\mathbf{X^{\\prime} X} \\right)^{-1} = \\begin{pmatrix} 100 &amp; 20 &amp; 20 &amp; 20 \\\\ 20 &amp; 90 &amp; 30 &amp; 40 \\\\ 20 &amp; 30 &amp; 80 &amp; 50 \\\\ 20 &amp; 40 &amp; 50 &amp; 70 \\\\ \\end{pmatrix}. } \\] Determina el error estándar de \\(b_3\\), \\(se(b_3)\\). Determina la covarianza estimada entre \\(b_2\\) y \\(b_3\\). Determina la correlación estimada entre \\(b_2\\) y \\(b_3\\). Determina la varianza estimada de \\(4b_2 + 3b_3\\). 3.3. Considera el siguiente pequeño conjunto de datos ficticio. Ajustarás un modelo de regresión a \\(y\\) usando dos variables explicativas, \\(x_1\\) y \\(x_2\\). \\[ \\small{ \\begin{array}{c|cccc} \\hline i &amp; 1 &amp; 2 &amp; 3 &amp; 4 \\\\ \\hline x_{i,1} &amp; -1 &amp; 2 &amp; 4 &amp; 6 \\\\ x_{i,2} &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ y_i &amp; 0 &amp; 1 &amp; 5 &amp; 8 \\\\ \\hline \\end{array} } \\] A partir del modelo de regresión ajustado, tenemos \\(s = 1.373\\) y \\[ \\small{ \\mathbf{b} = \\begin{pmatrix} 0.1538 \\\\ 0.6923 \\\\ 2.8846 \\end{pmatrix} ~~~\\text{y}~~~ \\left(\\mathbf{X^{\\prime} X} \\right)^{-1} = \\begin{pmatrix} 0.53846 &amp; -0.07692 &amp; -0.15385 \\\\ -0.07692 &amp; 0.15385 &amp; -0.69231 \\\\ -0.15385 &amp; -0.69231 &amp; 4.11538 \\\\ \\end{pmatrix}. } \\] Escribe el vector de variables dependientes, \\(\\mathbf{y}\\), y la matriz de variables explicativas, \\(\\mathbf{X}\\). Determina el valor numérico para \\(\\widehat{y}_3\\), el valor ajustado para la tercera observación. Determina el valor numérico para \\(se(b_2)\\). Determina el valor numérico para \\(t(b_1)\\). 3.4. Lotería de Wisconsin. La Sección 2.1 describió una muestra de \\(n=50\\) áreas geográficas (códigos postales) que contenían datos de ventas de la lotería estatal de Wisconsin (\\(y = \\text{SALES}\\)). En esa sección, las ventas se analizaron usando un modelo de regresión lineal básico con \\(x = \\text{POP}\\), la población del área, como la variable explicativa. Este ejercicio amplía ese análisis al introducir variables explicativas adicionales dadas en la Tabla 3.11. \\[ \\text{Tabla 3.11: Características de lotería, económicas y demográficas de cincuenta códigos postales de Wisconsin} \\] \\[ \\small{ \\begin{array}{ll} \\hline \\textbf{Características de la lotería} \\\\ \\hline \\text{SALES} &amp; \\text{Ventas de lotería en línea a consumidores individuales} \\\\ \\hline \\textbf{Características económicas y } \\\\ ~~~~~\\textbf{demográficas} \\\\\\hline \\text{PERPERHH} &amp; \\text{Personas por hogar} \\\\ \\text{MEDSCHYR} &amp; \\text{Años medianos de escolaridad} \\\\ \\text{MEDHVL} &amp; \\text{Valor medianos de viviendas en \\$1000s para viviendas ocupadas por sus propietarios} \\\\ \\text{PRCRENT} &amp; \\text{Porcentaje de viviendas ocupadas por arrendatarios} \\\\ \\text{PRC55P} &amp; \\text{Porcentaje de la población que tiene 55 años o más} \\\\ \\text{HHMEDAGE} &amp; \\text{Edad mediana del hogar} \\\\ \\text{MEDINC} &amp; \\text{Ingreso mediano estimado del hogar, en \\$1000s} \\\\ \\text{POP} &amp; \\text{Población} \\\\ \\hline \\end{array} } \\] Produce una tabla de estadísticas descriptivas para todas las variables. Un código postal (observación 11, código = 53211, Shorewood Wisconsin, un suburbio de Milwaukee) parece tener valores inusualmente grandes de MEDSCHYR y MEDHVL. Para esta observación, ¿cuántas desviaciones estándar está el valor de MEDSCHYR por encima de la media? Para esta observación, ¿cuántas desviaciones estándar está el valor de MEDHVL por encima de la media? Produce una tabla de correlaciones. ¿Cuáles son las tres variables más correlacionadas con SALES? Produce una matriz de gráficos de dispersión de todas las variables explicativas y SALES. En el gráfico de MEDSCHYR versus SALES, describe la posición de la observación 11. Ajusta un modelo lineal de SALES en las ocho variables explicativas. Resume el ajuste de este modelo citando la desviación estándar de los residuos, \\(s\\), el coeficiente de determinación, \\(R^2\\) y su versión ajustada, \\(R_a^2\\). Basado en el ajuste del modelo de la parte (d), ¿es MEDSCHYR una variable estadísticamente significativa? Para responder a esta pregunta, utiliza una prueba formal de hipótesis. Expón tus hipótesis nula y alternativa, criterio de decisión y regla de decisión. Ahora ajusta un modelo más parsimonioso, usando SALES como variable dependiente y MEDSCHYR, MEDHVL y POP como variables explicativas. Resume el ajuste de este modelo citando la desviación estándar de los residuos, \\(s\\), el coeficiente de determinación, \\(R^2\\) y su versión ajustada, \\(R_a^2\\). ¿Cómo se comparan estos valores con el ajuste del modelo en la parte (d)? Observa que el signo del coeficiente de regresión asociado con MEDSCHYR es negativo. Para ayudar a interpretar este coeficiente, calcula el coeficiente de correlación parcial correspondiente. ¿Cuál es la interpretación de este coeficiente? Para obtener más información sobre la relación entre MEDSCHYR y SALES, produce un gráfico de variable añadida controlando los efectos de MEDHVL y POP. Verifica que la correlación asociada con este gráfico concuerde con tu respuesta en la parte (g). Vuelve a ejecutar la regresión en la parte (f), después de eliminar la observación 11. Cita las estadísticas descriptivas básicas de esta regresión. Para este ajuste del modelo, ¿es MEDSCHYR una variable estadísticamente significativa? Para responder a esta pregunta, utiliza una prueba formal de hipótesis. Expón tus hipótesis nula y alternativa, criterio de decisión y regla de decisión. Vuelve a ejecutar la regresión en la parte (f), después de eliminar la observación 9. Cita las estadísticas descriptivas básicas de esta regresión. 3.5. Gastos de Compañías de Seguros. Este ejercicio considera los datos de compañías de seguros del NAIC y descritos en el Ejercicio 1.6. La Tabla 3.10 describe varias variables que pueden ser usadas para explicar los gastos. Al igual que en el estudio de Segal (2002) sobre los aseguradores de vida, los “outputs” de la empresa consisten en primas emitidas (para propiedad y accidentes, estas se subdividen en líneas personales y comerciales) así como pérdidas (subdivididas en líneas de corto y largo plazo). ASSETS y CASH son medidas comúnmente usadas del tamaño de una empresa. GROUP, STOCK y MUTUAL describen la estructura organizacional. Los “inputs” de la empresa se recopilaron del Bureau of Labor Statistics (BLS, del programa Occupational Employee Statistics). STAFFWAGE se calcula como el salario promedio en el estado donde está ubicada la compañía de seguros. AGENTWAGE se calcula como el salario promedio ponderado de la industria de corretaje, ponderado por el porcentaje de prima bruta escrita en cada estado. Table 3.10: Variables de Gastos de Aseguradoras Variable Descripción EXPENSES Gastos totales incurridos, en millones de dólares LOSSLONG Pérdidas incurridas para líneas de largo plazo, en millones de dólares LOSSSHORT Pérdidas incurridas para líneas de corto plazo, en millones de dólares GPWPERSONAL Prima bruta emitida para líneas personales, en millones de dólares GPWCOMM Prima bruta emitida para líneas comerciales, en millones de dólares ASSETS Activos netos admitidos, en millones de dólares CASH Efectivo y activos invertidos, en millones de dólares GROUP Indica si la empresa está afiliada STOCK Indica si la empresa es una compañía de acciones MUTUAL Indica si la empresa es una compañía mutua STAFFWAGE Salario promedio anual del personal administrativo de la aseguradora, en miles de dólares AGENTWAGE Salario promedio anual del agente de seguros, en miles de dólares Una inspección preliminar de los datos mostró que muchas empresas no informaron pérdidas de seguros en 2005. Para este ejercicio, consideramos las 384 compañías con algunas pérdidas en el archivo NAICExpense.csv. Produce estadísticas descriptivas de la variable de respuesta y las variables explicativas (no binarias). Nota el patrón de sesgo para cada variable. Observa que muchas variables tienen valores negativos. Transforma cada variable no binaria mediante la transformación logarítmica modificada, \\(\\ln(1+x)\\). Produce estadísticas descriptivas de estas variables explicativas no binarias modificadas. Denota LNEXPENSES (\\(= \\ln(1+\\text{EXPENSES})\\)) como la variable de gastos modificada. Para el análisis posterior, usa solo las variables modificadas descritas en la parte (b). Produce una tabla de correlaciones para las variables no binarias. ¿Cuáles son las tres variables más correlacionadas con LNEXPENSES? Proporciona un diagrama de caja de LNEXPENSES por nivel de GROUP. ¿Qué nivel de grupo tiene mayores gastos? Ajusta un modelo lineal de LNEXPENSES en las once variables explicativas. Resume el ajuste de este modelo citando la desviación estándar de los residuos, \\(s\\), el coeficiente de determinación, \\(R^2\\), y su versión ajustada, \\(R^2_a\\). Ajusta un modelo lineal de LNEXPENSES en un modelo reducido usando ocho variables explicativas, eliminando LNCASH, STOCK y MUTUAL. Para las variables explicativas, incluye ASSETS, GROUP, ambas versiones de pérdidas y primas brutas, así como las dos variables del BLS. f(i). Resume el ajuste de este modelo citando \\(s\\), \\(R^2\\), y \\(R^2_a\\). f(ii). Interpreta el coeficiente asociado con las primas brutas comerciales en la escala logarítmica. f(iii). Supón que GPWCOMM aumenta en $1, ¿cuánto esperamos que aumenten los EXPENSES? Usa tu respuesta en la parte f(ii) y los valores medianos de GPWCOMM y EXPENSES para esta pregunta. Eleva al cuadrado cada una de las dos variables de pérdidas y las dos de primas brutas. Ajusta un modelo lineal de LNEXPENSES en un modelo reducido usando doce variables explicativas, las ocho variables de la parte (f) y los cuatro términos cuadrados adicionales que acabas de crear. g(i). Resume el ajuste de este modelo citando \\(s\\), \\(R^2\\), y \\(R^2_a\\). g(ii). ¿Los términos cuadráticos parecen ser variables explicativas útiles? Ahora omite las dos variables del BLS, por lo que estás ajustando un modelo de LNEXPENSES en ASSETS, GROUP, ambas versiones de pérdidas y primas brutas, así como términos cuadráticos. Resume el ajuste de este modelo citando \\(s\\), \\(R^2\\), y \\(R^2_a\\). Comenta sobre el número de observaciones usadas para ajustar este modelo comparado con la parte (f). Términos de Interacción Elimina los términos cuadráticos de la parte (g) y agrega términos de interacción con la variable ficticia GROUP. Así, ahora hay once variables: ASSETS, GROUP, ambas versiones de pérdidas y primas brutas, así como interacciones de GROUP con ASSETS y ambas versiones de pérdidas y primas brutas. i(i). Resume el ajuste de este modelo citando \\(s\\), \\(R^2\\), y \\(R^2_a\\). i(ii). Supón que GPWCOMM aumenta en $1, ¿cuánto esperamos que aumenten los EXPENSES para las compañías GROUP=0? Usa los valores medianos de GPWCOMM y EXPENSES de las compañías GROUP=0 para esta pregunta. i(iii). Supón que GPWCOMM aumenta en $1, ¿cuánto esperamos que aumenten los EXPENSES para las compañías GROUP=1? Usa los valores medianos de GPWCOMM y EXPENSES de las compañías GROUP=1 para esta pregunta. 3.6 Expectativas de Vida Nacionales. Continuamos el análisis iniciado en los Ejercicios 1 y 2. Ahora ajusta un modelo de regresión en LIFEEXP usando tres variables explicativas: FERTILITY, PUBLICEDUCATION y lnHEALTH (la transformación logarítmica natural de PRIVATEHEALTH). Interpreta el coeficiente de regresión asociado con PUBLICEDUCATION. Interpreta el coeficiente de regresión asociado con los gastos en salud sin usar la escala logarítmica para los gastos. Basado en el ajuste del modelo, ¿es PUBLICEDUCATION una variable estadísticamente significativa? Para responder a esta pregunta, usa una prueba formal de hipótesis. Expón tus hipótesis nula y alternativa, criterio de decisión, y regla de decisión. El signo negativo del coeficiente de PUBLICEDUCATION es sorprendente, dado que el signo de la correlación entre PUBLICEDUCATION y LIFEEXP es positivo y la intuición sugiere una relación positiva. Para verificar este resultado, aparece un gráfico de variable añadida en la Figura 3.11. d(i). Para un gráfico de variable añadida, describe su propósito y un método para producirlo. d(ii). Calcula la correlación correspondiente al gráfico de variable añadida que aparece en la Figura 3.11. Figure 3.11: Gráfico de variable añadida de PUBLICEDUCATION versus LIFEEXP, controlando por FERTILITY y lnHEALTH Código R para producir la Figura 3.11 # FIGURA 3.11 LifeExp &lt;- read.csv(&quot;CSVData/UNLifeExpectancy.csv&quot;, header=TRUE) # ELIMINAR LIFEEXPs MISSING LifeExp3 &lt;- subset(LifeExp, !is.na(LIFEEXP) ) varLife &lt;- c(&quot;FERTILITY&quot;,&quot;PUBLICEDUCATION&quot;, &quot;REGION&quot;,&quot;COUNTRY&quot;,&quot;LIFEEXP&quot;, &quot;HEALTHEXPEND&quot;) LifeExp4 &lt;- LifeExp3[varLife] LifeExp4$lnHEALTH &lt;- log(LifeExp4$HEALTHEXPEND) LifeExp4.good &lt;- na.omit(LifeExp4) # GRÁFICO DE VARIABLE AÑADIDA model4a &lt;- lm(LIFEEXP ~ FERTILITY+lnHEALTH, data=LifeExp4.good) model4b &lt;- lm(PUBLICEDUCATION ~ FERTILITY+lnHEALTH, data=LifeExp4.good) plot(residuals(model4b),residuals(model4a), xlab=&quot;residuos(PUBLICEDUCATION)&quot;,ylab=&quot;residuos(LIFEEXP)&quot;) "],["C4MLRANOVA.html", "Chapter 4 Regresión Lineal Múltiple - II 4.1 El Papel de las Variables Binarias 4.2 Inferencia Estadística para Varios Coeficientes 4.3 Modelo ANOVA de Un Factor 4.4 Combinando Variables Explicativas Categóricas y Continuas 4.5 Lecturas Adicionales y Referencias 4.6 Ejercicios 4.7 Suplemento Técnico - Expresiones Matriciales", " Chapter 4 Regresión Lineal Múltiple - II Vista previa del capítulo. Este capítulo amplía la discusión sobre la regresión lineal múltiple al introducir la inferencia estadística para manejar varios coeficientes simultáneamente. Para motivar esta extensión, este capítulo considera los coeficientes asociados con variables categóricas. Estas variables nos permiten agrupar observaciones en distintas categorías. Este capítulo muestra cómo incorporar variables categóricas en funciones de regresión utilizando variables binarias, ampliando así considerablemente el alcance de las posibles aplicaciones del análisis de regresión. La inferencia estadística para varios coeficientes permite a los analistas tomar decisiones sobre variables categóricas, así como otras aplicaciones importantes. Las variables explicativas categóricas también proporcionan la base para un modelo de ANOVA, un tipo especial de modelo de regresión que permite un análisis e interpretación más sencillos. 4.1 El Papel de las Variables Binarias Las variables categóricas proporcionan etiquetas para observaciones, para denotar la pertenencia a grupos o categorías distintas. Una variable binaria es un caso especial de una variable categórica. Para ilustrar, una variable binaria puede indicarnos si alguien tiene o no seguro de salud. Una variable categórica podría indicarnos si alguien tiene: seguro privado grupal (ofrecido por empleadores y asociaciones), seguro privado individual (a través de compañías de seguros), seguro público (como Medicare o Medicaid), sin seguro de salud. Para las variables categóricas, puede o no existir un orden de los grupos. En el caso del seguro de salud, es difícil ordenar estas cuatro categorías y decir cuál es “mayor”: seguro privado grupal, seguro privado individual, seguro público o sin seguro de salud. En contraste, para la educación, podríamos agrupar a las personas en “baja,” “intermedia,” y “alta” según sus años de educación. En este caso, hay un orden entre los grupos basado en el nivel de logro educativo. Como veremos, este orden puede o no proporcionar información sobre la variable dependiente. Factor es otro término utilizado para una variable explicativa categórica no ordenada. Para las variables categóricas ordenadas, los analistas suelen asignar una puntuación numérica a cada resultado y tratar la variable como si fuera continua. Por ejemplo, si tuviéramos tres niveles de educación, podríamos emplear rangos y usar: \\[ \\small{ \\text{EDUCATION} = \\begin{cases} 1 &amp; \\text{para educación baja} \\\\ 2 &amp; \\text{para educación intermedia} \\\\ 3 &amp; \\text{para educación alta.} \\end{cases} } \\] Una alternativa sería utilizar una puntuación numérica que se aproxime a un valor subyacente de la categoría. Por ejemplo, podríamos usar: \\[ \\small{ \\text{EDUCATION} = \\begin{cases} 6 &amp; \\text{para educación baja} \\\\ 10 &amp; \\text{para educación intermedia} \\\\ 14 &amp; \\text{para educación alta.} \\end{cases} } \\] Esto da el número aproximado de años de escolaridad que completaron las personas en cada categoría. La asignación de puntuaciones numéricas y el tratamiento de la variable como continua tiene implicaciones importantes para la interpretación en la modelización de regresión. Recordemos que el coeficiente de regresión es el cambio marginal en la respuesta esperada; en este caso, el \\(\\beta\\) para EDUCATION evalúa el incremento en \\(\\mathrm{E }~y\\) por unidad de cambio en EDUCATION. Si registramos EDUCATION como un rango en un modelo de regresión, entonces el \\(\\beta\\) para EDUCATION corresponde al incremento en \\(\\mathrm{E }~y\\) al pasar de EDUCATION=1 a EDUCATION=2 (de baja a intermedia); este incremento es el mismo que al pasar de EDUCATION=2 a EDUCATION=3 (de intermedia a alta). ¿Queremos modelar este incremento como igual? Esta es una suposición que el analista hace con esta codificación de EDUCATION; puede ser o no válida, pero ciertamente necesita ser reconocida. Debido a esta interpretación de los coeficientes, los analistas rara vez usan rangos u otras puntuaciones numéricas para resumir variables categóricas no ordenadas. La forma más directa de manejar factores en la regresión es mediante el uso de variables binarias. Una variable categórica con \\(c\\) niveles puede representarse utilizando \\(c\\) variables binarias, una para cada categoría. Por ejemplo, supongamos que no estábamos seguros de la dirección del efecto de la educación y decidimos tratarla como un factor. Entonces, podríamos codificar \\(c=3\\) variables binarias: (1) una variable para indicar educación baja, (2) una para indicar educación intermedia, y (3) una para indicar educación alta. Estas variables binarias son a menudo conocidas como variables ficticias. En el análisis de regresión con un término de intercepción, utilizamos solo \\(c-1\\) de estas variables binarias; la variable restante entra implícitamente a través del término de intercepción. Al identificar una variable como un factor, la mayoría de los paquetes de software estadístico crearán automáticamente variables binarias para usted. A través del uso de variables binarias, no utilizamos el orden de las categorías dentro de un factor. Debido a que no se hace ninguna suposición sobre el orden de las categorías, para el ajuste del modelo no importa qué variable se omita con respecto al ajuste del modelo. Sin embargo, sí importa para la interpretación de los coeficientes de regresión. Consideremos el siguiente ejemplo. Ejemplo: Seguro de Vida Temporal - Continuado. Ahora volvemos al estado civil de los encuestados de la Encuesta de Finanzas del Consumidor (SCF). Recordemos que el estado civil no se mide de manera continua, sino que toma valores que caen en grupos distintos que tratamos como no ordenados. En el Capítulo 3, agrupamos a los encuestados según si eran o no “solteros”, donde ser soltero incluye nunca haberse casado, estar separado, divorciado, viudo, y no casado pero viviendo con una pareja. Ahora complementamos esto al considerar la variable categórica, MARSTAT, que representa el estado civil del encuestado. Esto puede ser: 1, para casado 2, para viviendo con una pareja 0, para otro (SCF desglosa aún más esta categoría en separado, divorciado, viudo, nunca casado, inaplicable, personas de 17 años o menos, y sin más personas). Como antes, la variable dependiente es \\(y =\\) LNFACE, la cantidad que la compañía pagará en caso de fallecimiento del asegurado nombrado (en dólares logarítmicos). La Tabla 4.1 resume la variable dependiente según el nivel de la variable categórica. Esta tabla muestra que el estado civil “casado” es el más prevalente en la muestra y que los casados eligen tener la mayor cobertura de seguro de vida. La Figura 4.1 da una visión más completa de la distribución de LNFACE para cada uno de los tres tipos de estado civil. La tabla y la figura también sugieren que aquellos que viven juntos tienen menos cobertura de seguro de vida que las otras dos categorías. Table 4.1: Estadísticas Resumidas de Logaritmo del Monto de Cobertura por Estado Civil MARSTAT Número Media Desviación Estándar Otro 0 57 10.958 1.566 Casado 1 208 12.329 1.822 Viviendo juntos 2 10 10.825 2.001 Total 275 11.990 1.871 Figure 4.1: Diagramas de Caja del Logaritmo del Monto de Cobertura, por Nivel de Estado Civil Código R para Producir la Tabla 4.1 y la Figura 4.1 tableout &lt;- data.frame( MARSTAT &lt;- c(0,1,2,&quot;&quot;), Número &lt;- c(57, 208, 10, 275), Media &lt;- c(10.958, 12.329, 10.825, 11.990), Desviación_Estándar &lt;- c(1.566, 1.822, 2.001, 1.871) ) colnames(tableout) &lt;- c(&quot;MARSTAT&quot;, &quot;Número&quot;, &quot;Media&quot;, &quot;Desviación Estándar&quot;) rownames(tableout) &lt;- c(&quot;Otro&quot;, &quot;Casado&quot;, &quot;Viviendo juntos&quot;, &quot;Total&quot;) TableGen1(TableData=tableout , TextTitle=&#39;Estadísticas Resumidas de Logaritmo del Monto de Cobertura por Estado Civil&#39;, Align=&#39;crrr&#39;, Digits=3, ColumnSpec=1:3, ColWidth = ColWidth4) %&gt;% kableExtra::column_spec(1, width = &quot;4cm&quot;) library(HH) Term &lt;- read.csv(&quot;CSVData/TermLife.csv&quot;, header=TRUE) Term2 &lt;- subset(Term, FACE &gt; 0) LNFACE &lt;- log(Term2$FACE) LNINCOME &lt;- log(Term2$INCOME) MAR0 &lt;- 1*(Term2$MARSTAT == 0) # FIGURA 4.1 par(mar=c(4.1,4,1,1), cex=1.1) boxplot(LNFACE ~ MARSTAT, data=Term2, ylab=&quot;&quot;, xlab=&quot;Estado Civil&quot;) mtext(&quot;LNFACE&quot;, side=2, at=17.2, las=1, cex=1.1, adj=.4) ¿Son las variables continuas y categóricas determinantes importantes de la respuesta? Para responder a esto, se realizó una regresión usando LNFACE como respuesta y cinco variables explicativas: tres continuas y dos binarias (para el estado civil). Recordemos que nuestras tres variables explicativas continuas son: LNINCOME (ingreso anual logarítmico), el número de años de EDUCATION del encuestado, y el número de miembros del hogar, NUMHH. Para las variables binarias, primero definimos MAR0 como la variable binaria que toma el valor de uno si MARSTAT=0 y cero en caso contrario. De manera similar, definimos MAR1 y MAR2 como variables binarias que indican si MARSTAT=1 y MARSTAT=2, respectivamente. Existe una dependencia lineal perfecta entre estas tres variables binarias, ya que MAR0 + MAR1 + MAR2 = 1 para cualquier encuestado. Por lo tanto, solo necesitamos dos de las tres. Sin embargo, no hay una dependencia perfecta entre cualquiera de dos de las tres. Resulta que cor(MAR0, MAR1) = -0.90, cor(MAR0, MAR2) = -0.10, y cor(MAR1, MAR2) = -0.34. Se realizó un modelo de regresión utilizando LNINCOME, EDUCATION, NUMHH, MAR0, y MAR2 como variables explicativas. La ecuación de regresión ajustada resulta ser: \\[ \\small{ \\widehat{y} = 3.395 + 0.452 \\text{ LNINCOME} + 0.205 \\text{ EDUCATION} + 0.248 \\text{ NUMHH} - 0.557 \\text{ MAR0} - 0.789 \\text{ MAR2}. } \\] Para interpretar los coeficientes de regresión asociados con el estado civil, consideremos un encuestado que está casado. En este caso, MAR0=0, MAR1=1, y MAR2=0, de manera que: \\[ \\small{ \\widehat{y}_m = 3.395 + 0.452 \\text{ LNINCOME} + 0.205 \\text{ EDUCATION} + 0.248 \\text{ NUMHH}. } \\] De manera similar, si el encuestado es codificado como viviendo juntos, entonces MAR0=0, MAR1=0, y MAR2=1, y: \\[ \\small{ \\widehat{y}_{lt} = 3.395 + 0.452 \\text{ LNINCOME} + 0.205 \\text{ EDUCATION} + 0.248 \\text{ NUMHH} - 0.789. } \\] La diferencia entre \\(\\widehat{y}_m\\) y \\(\\widehat{y}_{lt}\\) es \\(0.789.\\) Así, podemos interpretar el coeficiente de regresión asociado con MAR2, \\(-0.789\\), como la diferencia en los valores ajustados para alguien que vive juntos en comparación con una persona similar que está casada (la categoría omitida). De manera similar, podemos interpretar \\(-0.557\\) como la diferencia entre la categoría “otro” y la categoría de casados, manteniendo fijas las otras variables explicativas. Para la diferencia en los valores ajustados entre las categorías “otro” y “viviendo juntos”, podemos usar \\(-0.557 - (-0.789) = 0.232.\\) Aunque la regresión se realizó utilizando MAR0 y MAR2, cualquier dos de las tres producirían la misma Tabla ANOVA (Tabla 4.2). Sin embargo, la elección de las variables binarias sí afecta los coeficientes de regresión. La Tabla 4.3 muestra tres modelos, omitiendo MAR1, MAR2, y MAR0, respectivamente. Para cada ajuste, los coeficientes asociados con las variables continuas permanecen iguales. Como hemos visto, las interpretaciones de las variables binarias son con respecto a la categoría omitida, conocida como el nivel de referencia. Aunque cambian de un modelo a otro, su interpretación general sigue siendo la misma. Es decir, si queremos estimar la diferencia en la cobertura entre la categoría “otro” y la categoría “viviendo juntos”, la estimación sería \\(0.232\\), sin importar el modelo. Table 4.2: Seguro de Vida Temporal con Estado Civil - Tabla ANOVA Fuente Suma de Cuadrados \\(df\\) Cuadrado Medio Regresión 343.28 5 68.66 Error 615.62 269 2.29 Total 948.90 274 Aunque los tres modelos en la Tabla 4.3 son iguales excepto por diferentes elecciones de parámetros, parecen diferentes. En particular, los \\(t\\)-ratios difieren y muestran diferentes apariencias de significancia estadística. Por ejemplo, ambos \\(t\\)-ratios asociados con el estado civil en el Modelo 2 son menores que 2 en valor absoluto, lo que sugiere que el estado civil es poco importante. En contraste, tanto el Modelo 1 como el Modelo 3 tienen al menos una variable binaria de estado civil que supera 2 en valor absoluto, lo que sugiere significancia estadística. Por lo tanto, se puede influir en la apariencia de significancia estadística al alterar la elección del nivel de referencia. Para evaluar la importancia general del estado civil (no solo cada variable binaria), la Sección 4.2 introducirá pruebas de conjuntos de coeficientes de regresión. Table 4.3: Coeficientes de Regresión del Seguro de Vida Temporal con Estado Civil Coeficiente Modelo 1 Modelo 1 \\(t\\)-Ratio Coeficiente Modelo 2 Modelo 2 \\(t\\)-Ratio Coeficiente Modelo 3 Modelo 3 \\(t\\)-Ratio LNINCOME 0.452 5.74 0.452 5.74 0.452 5.74 EDUCATION 0.205 5.3 0.205 5.3 0.205 5.3 NUMHH 0.248 3.57 0.248 3.57 0.248 3.57 Intercepto 3.395 3.77 3.395 2.74 2.838 3.34 MAR0 -0.557 -2.15 0.232 0.44 MAR1 0.789 1.59 0.557 2.15 MAR2 -0.789 -1.59 -0.232 -0.44 Ejemplo: ¿Cómo afecta el Compartir Costos en Planes de Seguro Médico a los Gastos en Salud? En uno de los muchos estudios que resultaron del Experimento de Seguro de Salud de Rand (HIE) introducido en la Sección 1.5, Keeler y Rolph (1988) investigaron los efectos del compartir costos en los planes de seguro médico. Para este estudio, 14 planes de seguro médico fueron agrupados por la tasa de coaseguro (el porcentaje pagado como gastos de bolsillo que variaba en 0, 25, 50 y 95%). Uno de los planes con 95% limitaba los gastos anuales de bolsillo en atención ambulatoria a 150 por persona (450 por familia), proporcionando en efecto un deducible ambulatorio individual. Este plan se analizó como un grupo separado, de manera que había \\(c=5\\) categorías de planes de seguro. En la mayoría de los estudios de seguros, los individuos eligen planes de seguro, lo que hace difícil evaluar los efectos del compartir costos debido a la selección adversa. La selección adversa puede surgir porque los individuos con mala salud crónica son más propensos a elegir planes con menos compartir costos, lo que da la apariencia de que menos cobertura conduce a mayores gastos. En el HIE de Rand, los individuos fueron asignados aleatoriamente a los planes, eliminando así esta fuente potencial de sesgo. Keeler y Rolph (1988) organizaron los gastos de un individuo en episodios de tratamiento; cada episodio contiene gastos asociados con un determinado ataque de enfermedad, condición crónica o procedimiento. Los episodios se clasificaron como hospitalarios, dentales o ambulatorios; esta clasificación se basó principalmente en diagnósticos, no en la ubicación de los servicios. Así, por ejemplo, los servicios ambulatorios que preceden o siguen a una hospitalización, así como los medicamentos y pruebas relacionados, se incluyeron como parte de un episodio hospitalario. Para simplificar, aquí solo informamos resultados para episodios hospitalarios. Aunque las familias fueron asignadas aleatoriamente a los planes, Keeler y Rolph (1988) utilizaron métodos de regresión para controlar los atributos de los participantes y aislar los efectos del compartir costos en los planes. La Tabla 4.4 resume los coeficientes de regresión, basados en una muestra de \\(n=1,967\\) gastos por episodio. En esta regresión, el gasto logarítmico fue la variable dependiente. La variable categórica de compartir costos se descompuso en cinco variables binarias para que no se impusiera ninguna forma funcional en la respuesta al seguro. Estas variables son “Co-ins25,” “Co-ins50,” y “Co-ins95,” para tasas de coaseguro del 25, 50 y 95%, respectivamente, y “Deducible Indiv” para el plan con deducibles individuales. La variable omitida es el plan de seguro gratuito con 0% de coaseguro. El HIE se llevó a cabo en seis ciudades; una variable categórica para controlar la ubicación se representó con cinco variables binarias, Dayton, Fitchburg, Franklin, Charleston y Georgetown, siendo Seattle la variable omitida. Se utilizó un factor categórico con \\(c=6\\) niveles para la edad y el sexo; las variables binarias en el modelo consistieron en “Edad 0-2,” “Edad 3-5,” “Edad 6-17,” “Mujer edad 18-65,” y “Hombre edad 46-65,” siendo la categoría omitida “Hombre edad 18-45.” Otras variables de control incluyeron una escala de estado de salud, el estado socioeconómico, el número de visitas médicas en el año anterior al experimento en una escala logarítmica y la raza. La Tabla 4.4 resume los efectos de las variables. Como señalaron Keeler y Rolph, hubo grandes diferencias según la ubicación y la edad, aunque la regresión solo explicó \\(R^2=11\\%\\) de la variabilidad. Para las variables de compartir costos, solo “Co-ins95” fue estadísticamente significativa, y esto solo al nivel del 5%, no al nivel del 1%. El estudio de Keeler y Rolph (1988) examina otros tipos de gastos por episodio, así como la frecuencia de los gastos. Concluyeron que el compartir costos en los planes de seguro médico tiene poco efecto sobre la cantidad de gastos por episodio, aunque hay diferencias importantes en la frecuencia de episodios. Esto se debe a que un episodio de tratamiento está compuesto por dos decisiones. La cantidad de tratamiento es decidida conjuntamente entre el paciente y el médico y, en gran medida, no se ve afectada por el tipo de plan de seguro médico. La decisión de buscar tratamiento médico la toma el paciente; este proceso de toma de decisiones es más susceptible a los incentivos económicos en los aspectos de compartir costos de los planes de seguro médico. Table 4.4: Coeficientes de Gastos por Episodio del Rand HIE Variable Coeficiente de Regresión Variable Coeficiente de Regresión Intercepto 7.95 Dayton 0.13* Co-ins25 0.07 Fitchburg 0.12 Co-ins50 0.02 Franklin -0.01 Co-ins95 -0.13* Charleston 0.20* Deducible Indiv -0.03 Georgetown -0.18* Escala de Salud -0.02* Edad 0-2 -0.63** Estado Socioeconómico 0.03 Edad 3-5 -0.64** Visitas Médicas -0.03 Edad 6-17 -0.30** Examen -0.10* Mujer edad 18-65 0.11 Negro 0.14* Hombre edad 46-65 0.26 Nota: * significativo al 5%, ** significativo al 1% Fuente: Keeler y Rolph (1988) 4.2 Inferencia Estadística para Varios Coeficientes Puede ser útil examinar varios coeficientes de regresión al mismo tiempo. Por ejemplo, cuando se evalúa el efecto de una variable categórica con \\(c\\) niveles, necesitamos decir algo de manera conjunta sobre las \\(c-1\\) variables binarias que ingresan a la ecuación de regresión. Para hacer esto, la Sección 4.2.1 introduce un método para manejar combinaciones lineales de coeficientes de regresión. La Sección 4.2.2 muestra cómo probar varias combinaciones lineales, y la Sección 4.2.3 presenta otras aplicaciones de inferencia. 4.2.1 Conjuntos de Coeficientes de Regresión Recordemos que nuestros coeficientes de regresión se especifican como \\(\\boldsymbol{\\beta} = \\left( \\beta_0, \\beta_1, \\ldots, \\beta_k \\right)^{\\prime},\\) un vector de tamaño \\((k+1) \\times 1\\). Será conveniente expresar combinaciones lineales de los coeficientes de regresión utilizando la notación \\(\\mathbf{C} \\boldsymbol{\\beta},\\) donde \\(\\mathbf{C}\\) es una matriz de tamaño \\(p \\times (k+1)\\) que es especificada por el usuario y depende de la aplicación. Algunas aplicaciones involucran la estimación de \\(\\mathbf{C} \\boldsymbol{\\beta}\\). Otras involucran probar si \\(\\mathbf{C} \\boldsymbol{\\beta}\\) es igual a un valor específico conocido (denotado como \\(\\mathbf{d}\\)). Llamamos a \\(H_0:\\mathbf{C \\boldsymbol{\\beta} = d}\\) la hipótesis lineal general. Para demostrar la amplia variedad de aplicaciones en las que se pueden usar conjuntos de coeficientes de regresión, ahora presentamos una serie de casos especiales. Caso Especial 1: Un Coeficiente de Regresión. En la Sección 3.4, investigamos la importancia de un solo coeficiente, digamos \\(\\beta_j\\). Podemos expresar este coeficiente como \\(\\mathbf{C} \\boldsymbol{\\beta}\\) eligiendo \\(p=1\\) y \\(\\mathbf{C}\\) como un vector de \\(1 \\times (k+1)\\) con un uno en la columna \\((j+1)\\) y ceros en las demás posiciones. Estas elecciones resultan en \\[ \\mathbf{C \\boldsymbol{\\beta} =} \\left( 0~\\ldots~0~1~0~\\ldots~0\\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) = \\beta_j. \\] Caso Especial 2: Función de Regresión. Aquí, elegimos \\(p=1\\) y \\(\\mathbf{C}\\) como un vector de \\(1 \\times (k+1)\\) que representa la transpuesta de un conjunto de variables explicativas. Estas elecciones resultan en \\[ \\mathbf{C \\boldsymbol{\\beta} =} \\left( x_0, x_1, \\ldots, x_k \\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) = \\beta_0 x_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k = \\mathrm{E}~y, \\] que es la función de regresión. Caso Especial 3: Combinación Lineal de Coeficientes de Regresión. Cuando \\(p=1\\), usamos la convención de que las letras en minúscula en negrita son vectores y tomamos \\(\\mathbf{C = c^{\\prime}} = \\left( c_0, \\ldots, c_k \\right)^{\\prime}\\). En este caso, \\(\\mathbf{C} \\boldsymbol{\\beta}\\) es una combinación lineal genérica de los coeficientes de regresión \\[ \\mathbf{C} \\boldsymbol{\\beta} = \\mathbf{c}^{\\prime} {\\boldsymbol \\beta} = c_0 \\beta_0 + \\ldots + c_k \\beta_k . \\] Caso Especial 4: Prueba de Igualdad de Coeficientes de Regresión. Supongamos que el interés radica en probar \\(H_0: \\beta_1 = \\beta_2\\). Para este propósito, tomamos \\(p=1\\), \\(\\mathbf{c}^{\\prime} = \\left( 0, 1, -1, 0, \\ldots, 0\\right)\\), y \\(\\mathbf{d} = 0\\). Con estas elecciones, tenemos \\[ \\mathbf{C \\boldsymbol{\\beta} = c^{\\prime} \\boldsymbol{\\beta} =} \\left( 0, 1, -1, 0, \\ldots, 0\\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) = \\beta_1 - \\beta_2 = 0, \\] de modo que la hipótesis lineal general se reduce a \\(H_0: \\beta_1 = \\beta_2\\). Caso Especial 5: Adecuación del Modelo. Es costumbre en el análisis de regresión presentar una prueba de si alguna de las variables explicativas es útil para explicar la respuesta. Formalmente, esto es una prueba de la hipótesis nula \\(H_0:\\beta_1=\\beta_2=\\ldots=\\beta_k=0\\). Es importante notar que, como convención, no se prueba si el intercepto es cero. Para probar esto usando la hipótesis lineal general, elegimos \\(p=k\\), \\(\\mathbf{d}=\\left( 0~\\ldots~0\\right)^{\\prime}\\) como un vector de tamaño \\(k \\times 1\\) lleno de ceros y \\(\\mathbf{C}\\) como una matriz de tamaño \\(k \\times (k+1)\\) tal que \\[ \\small{ \\mathbf{C \\boldsymbol{\\beta} =}\\left( \\begin{array}{ccccc} 0 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{array} \\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) =\\left( \\begin{array}{c} \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) =\\left( \\begin{array}{c} 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right) =\\mathbf{d}. } \\] Caso Especial 6: Prueba de Partes del Modelo. Supongamos que estamos interesados en comparar una función de regresión completa \\[ \\mathrm{E~}y = \\beta_0 + \\beta_1 x_1 +\\ldots + \\beta_k x_k + \\beta_{k+1} x_{k+1} + \\ldots + \\beta_{k+p} x_{k+p} \\] con una función de regresión reducida, \\[ \\mathrm{E~}y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k. \\] Comenzando con la regresión completa, vemos que si se cumple la hipótesis nula \\(H_0:\\beta_{k+1} = \\ldots = \\beta_{k+p} = 0\\), entonces llegamos a la regresión reducida. Para ilustrar, las variables \\(x_{k+1}, \\ldots, x_{k+p}\\) pueden referirse a varias variables binarias que representan una variable categórica y nuestro interés radica en si la variable categórica es importante. Para probar la importancia de la variable categórica, queremos ver si las variables binarias \\(x_{k+1}, \\ldots, x_{k+p}\\) afectan conjuntamente a las variables dependientes. Para probar esto utilizando la hipótesis lineal general, elegimos \\(\\mathbf{d}\\) y \\(\\mathbf{C}\\) tal que \\[ \\small{ \\mathbf{C \\boldsymbol{\\beta} =}\\left( \\begin{array}{ccccccc} 0 &amp; \\cdots &amp; 0 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{array} \\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\\\ \\beta_{k+1} \\\\ \\vdots \\\\ \\beta_{k+p} \\end{array} \\right) =\\left( \\begin{array}{c} \\beta_{k+1} \\\\ \\vdots \\\\ \\beta_{k+p} \\end{array} \\right) =\\left( \\begin{array}{c} 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right) =\\mathbf{d}. } \\] De una lista de \\(k+p\\) variables \\(x_1, \\ldots, x_{k+p}\\), puedes eliminar cualquier \\(p\\) que consideres apropiado. Las variables adicionales no necesitan ser las últimas \\(p\\) en la especificación de la regresión. Eliminar \\(x_{k+1}, \\ldots, x_{k+p}\\) es solo por conveniencia notacional. 4.2.2 La Hipótesis Lineal General Para resumir, la hipótesis lineal general se puede expresar como \\(H_0:\\mathbf{C \\boldsymbol{\\beta} = d}\\). Aquí, \\(\\mathbf{C}\\) es una matriz de \\(p \\times (k+1)\\), \\(\\mathbf{d}\\) es un vector de \\(p \\times 1\\), y tanto \\(\\mathbf{C}\\) como \\(\\mathbf{d}\\) son especificados por el usuario y dependen de la aplicación en cuestión. Aunque \\(k+1\\) es el número de coeficientes de regresión, \\(p\\) es el número de restricciones bajo \\(H_0\\) sobre estos coeficientes. (Para aquellos lectores con conocimiento de álgebra matricial avanzada, \\(p\\) es el rango de \\(\\mathbf{C}\\)). Esta hipótesis nula se prueba contra la alternativa \\(H_a:\\mathbf{C \\boldsymbol{\\beta} \\neq d}\\). Esto puede ser obvio, pero requerimos que \\(p \\leq k+1\\) porque no podemos probar más restricciones que los parámetros libres. Para entender la base del procedimiento de prueba, primero recordemos algunas de las propiedades básicas de los estimadores de los coeficientes de regresión descritos en la Sección 3.3. Sin embargo, nuestro objetivo ahora es entender las propiedades de las combinaciones lineales de los coeficientes de regresión especificadas por \\(\\mathbf{C \\boldsymbol{\\beta}}\\). Un estimador natural de esta cantidad es \\(\\mathbf{Cb}\\). Es fácil ver que \\(\\mathbf{Cb}\\) es un estimador insesgado de \\(\\mathbf{C \\boldsymbol{\\beta}}\\), porque \\(\\mathrm{E~}\\mathbf{Cb = C}\\mathrm{E~}\\mathbf{b = C \\boldsymbol{\\beta}}\\). Además, la varianza es \\(\\mathrm{Var}\\left( \\mathbf{Cb}\\right) \\mathbf{= C}\\mathrm{Var}\\left( \\mathbf{b}\\right) \\mathbf{C}^{\\prime}\\) \\(=\\sigma^2 \\mathbf{C}\\left( \\mathbf{X^{\\prime}X}\\right)^{-1} \\mathbf{C}^{\\prime}\\). Para evaluar la diferencia entre \\(\\mathbf{d}\\), el valor hipotetizado de \\(\\mathbf{C \\boldsymbol{\\beta}}\\), y su valor estimado, \\(\\mathbf{Cb}\\), utilizamos la siguiente estadística: \\[ F-\\text{ratio}=\\frac{(\\mathbf{Cb-d)}^{\\prime}\\left( \\mathbf{C}\\left( \\mathbf{X^{\\prime}X} \\right)^{-1} \\mathbf{C}^{\\prime}\\right)^{-1}(\\mathbf{Cb-d)}}{ps_{full}^2}. \\tag{4.1} \\] Aquí, \\(s_{full}^2\\) es el error cuadrático medio del modelo de regresión completo. Usando la teoría de modelos lineales, se puede comprobar que la estadística \\(F\\)-ratio sigue una distribución \\(F\\) con grados de libertad del numerador \\(df_1=p\\) y grados de libertad del denominador \\(df_2=n-(k+1)\\). Tanto la estadística como la distribución teórica llevan el nombre de R. A. Fisher, un científico y estadístico renombrado que hizo mucho para avanzar la estadística como ciencia en la primera mitad del siglo XX. Al igual que la distribución normal y la distribución \\(t\\), la distribución \\(F\\) es una distribución continua. La distribución \\(F\\) es la distribución muestral para el \\(F\\)-ratio y es proporcional a la razón de dos sumas de cuadrados, cada una de las cuales es positiva o cero. Así, a diferencia de la distribución normal y la distribución \\(t\\), la distribución \\(F\\) solo toma valores no negativos. Recordemos que la distribución \\(t\\) está indexada por un único parámetro de grados de libertad. La distribución \\(F\\) está indexada por dos parámetros de grados de libertad: uno para el numerador, \\(df_1\\), y uno para el denominador, \\(df_2\\). El Apéndice A3.4 proporciona detalles adicionales. La estadística de prueba en la ecuación (4.1) es compleja en su forma. Afortunadamente, existe una alternativa que es más sencilla de implementar e interpretar; esta alternativa se basa en el principio de la suma de cuadrados adicional. Procedimiento para Probar la Hipótesis Lineal General Ejecuta la regresión completa y obtén la suma de cuadrados del error y el error cuadrático medio, los cuales etiquetamos como \\((Error~SS)_{full}\\) y \\(s_{full}^2\\), respectivamente. Considera el modelo asumiendo que la hipótesis nula es verdadera. Ejecuta una regresión con este modelo y obtén la suma de cuadrados del error, la cual etiquetamos como \\((Error~SS)_{reduced}\\). Calcula \\[ F-\\text{ratio}=\\frac{(Error~SS)_{reduced}-(Error~SS)_{full}}{ps_{full}^2}. \\tag{4.2} \\] Rechaza la hipótesis nula en favor de la alternativa si el \\(F\\)-ratio excede un valor \\(F\\). El valor \\(F\\) es un percentil de la distribución \\(F\\) con \\(df_1=p\\) y \\(df_2=n-(k+1)\\) grados de libertad. El percentil es uno menos el nivel de significancia de la prueba. Siguiendo nuestra notación con la distribución \\(t\\), denotamos este percentil como \\(F_{p,n-(k+1),1-\\alpha}\\), donde \\(\\alpha\\) es el nivel de significancia. Este procedimiento es comúnmente conocido como una prueba \\(F\\). La Sección 4.7.2 proporciona las bases matemáticas. Para entender el principio de la suma de cuadrados adicional, recordemos que la suma de cuadrados del error para el modelo completo se determina como el valor mínimo de \\[ SS(b_0^{\\ast}, \\ldots, b_k^{\\ast}) = \\sum_{i=1}^{n} \\left( y_i - \\left( b_0^{\\ast} + \\ldots + b_k^{\\ast} x_{i,k} \\right) \\right)^2. \\] Aquí, \\(SS(b_0^{\\ast}, \\ldots, b_k^{\\ast})\\) es una función de \\(b_0^{\\ast}, \\ldots, b_k^{\\ast}\\) y \\((Error~SS)_{full}\\) es el mínimo sobre todos los valores posibles de \\(b_0^{\\ast}, \\ldots, b_k^{\\ast}\\). De manera similar, \\((Error~SS)_{reduced}\\) es la mínima suma de cuadrados del error bajo las restricciones en la hipótesis nula. Debido a que hay menos posibilidades bajo la hipótesis nula, tenemos que \\[ (Error~SS)_{full} \\leq (Error~SS)_{reduced}. \\tag{4.3} \\] Para ilustrar, consideremos nuestro primer caso especial donde \\(H_0 : \\beta_j = 0\\). En este caso, la diferencia entre los modelos completo y reducido equivale a eliminar una variable. Una consecuencia de la ecuación (4.3) es que, al agregar variables a un modelo de regresión, la suma de cuadrados del error nunca aumenta (y, de hecho, generalmente disminuye). Por lo tanto, agregar variables a un modelo de regresión aumenta \\(R^2\\), el coeficiente de determinación. ¿Cuán grande debe ser una disminución en la suma de cuadrados del error para que sea estadísticamente significativa? Intuitivamente, se puede ver el cociente \\(F\\) como la diferencia en la suma de cuadrados del error dividida por el número de restricciones, \\(\\frac{(Error~SS)_{reduced}-(Error~SS)_{full}}{p}\\), y luego reescalada por la mejor estimación del término de varianza, el \\(s^2\\), del modelo completo. Bajo la hipótesis nula, esta estadística sigue una distribución \\(F\\) y podemos comparar la estadística de prueba con esta distribución para ver si es inusualmente grande. Usando la relación \\(Regression~SS = Total~SS - Error~SS\\), podemos re-expresar la diferencia en la suma de cuadrados del error como \\[ (Error~SS)_{reduced} - (Error~SS)_{full} = (Regression~SS)_{full} - (Regression~SS)_{reduced}. \\] Esta diferencia se conoce como una Suma de Cuadrados Tipo III. Al probar la importancia de un conjunto de variables explicativas, \\(x_{k+1}, \\ldots, x_{k+p}\\), en presencia de \\(x_1, \\ldots, x_k\\), encontrarás que muchos paquetes estadísticos calculan esta cantidad directamente en una sola ejecución de regresión. La ventaja de esto es que permite al analista realizar una prueba \\(F\\) usando una sola ejecución de regresión, en lugar de dos ejecuciones de regresión como en nuestro procedimiento de cuatro pasos descrito anteriormente. Ejemplo: Seguro de Vida Temporal - Continuación. Antes de discutir la lógica y las implicaciones de la prueba \\(F\\), vamos a ilustrar su uso. En el ejemplo del Seguro de Vida Temporal, supongamos que deseamos entender el impacto del estado civil. La Tabla 4.3 presentó un mensaje mixto en términos de cocientes \\(t\\); a veces eran estadísticamente significativos y otras veces no. Sería útil tener una prueba formal para dar una respuesta definitiva, al menos en términos de significancia estadística. Específicamente, consideramos un modelo de regresión utilizando LNINCOME, EDUCATION, NUMHH, MAR0, y MAR2 como variables explicativas. La ecuación del modelo es \\[ \\small{ \\begin{array}{ll} y &amp;= \\beta_0 + \\beta_1 \\text{LNINCOME} + \\beta_2 \\text{EDUCATION} + \\beta_3 \\text{NUMHH} \\\\ &amp; \\ \\ \\ \\ + \\beta_4 \\text{MAR0} + \\beta_5 \\text{MAR2}. \\end{array} } \\] Nuestro objetivo es probar \\(H_0: \\beta_4 = \\beta_5 = 0\\). Comenzamos ejecutando un modelo de regresión con todas las \\(k+p=5\\) variables. Los resultados se informaron en la Tabla 4.2, donde vimos que \\((Error~SS)_{full} = 615.62\\) y \\(s_{full}^2 = (1.513)^2 = 2.289\\). El siguiente paso es ejecutar el modelo reducido sin MAR0 y MAR2. Esto se hizo en la Tabla 3.3 del Capítulo 3, donde vimos que \\((Error~SS)_{reduced} = 630.43\\). Luego calculamos la estadística de prueba \\[ \\small{ F-\\text{ratio} = \\frac{(Error~SS)_{reduced} - (Error~SS)_{full}}{ps_{full}^2} = \\frac{630.43 - 615.62}{2 \\times 2.289} = 3.235. } \\] El cuarto paso compara la estadística de prueba con una distribución \\(F\\) con \\(df_1=p=2\\) y \\(df_2 = n-(k+p+1) = 269\\) grados de libertad. Usando un nivel de significancia del 5%, resulta que el percentil 95 es \\(F-\\text{ratio} \\approx 3.029\\). El valor \\(p\\) correspondiente es \\(\\Pr(F &gt; 3.235) = 0.0409\\). Al nivel de significancia del 5%, rechazamos la hipótesis nula \\(H_0: \\beta_4 = \\beta_5 = 0\\). Esto sugiere que es importante utilizar el estado civil para entender la cobertura de seguro de vida temporal, incluso en presencia de ingresos, educación y número de miembros del hogar. Algunos Casos Especiales La prueba de hipótesis lineales general está disponible cuando puedes expresar un modelo como un subconjunto de otro. Por esta razón, es útil pensar en ella como una herramienta para comparar modelos “más pequeños” con modelos “más grandes”. Sin embargo, el modelo más pequeño debe ser un subconjunto del modelo más grande. Por ejemplo, la prueba de hipótesis lineales general no se puede usar para comparar las funciones de regresión \\(\\mathrm{E~}y = \\beta_0 + \\beta_7 x_7\\) frente a \\(\\mathrm{E~}y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4\\). Esto se debe a que la primera, función más pequeña, no es un subconjunto de la segunda, función más grande. La prueba de hipótesis lineales general se puede usar en muchas ocasiones, aunque no siempre es necesaria. Por ejemplo, supongamos que deseamos probar \\(H_0:\\beta_k=0\\). Ya hemos visto que esta hipótesis nula se puede examinar usando la prueba del cociente \\(t\\). En este caso especial, resulta que \\((t-\\textrm{ratio})^2=F-\\textrm{ratio}\\). Así, estas pruebas son equivalentes para probar \\(H_0:\\beta_k=0\\) frente a \\(H_a:\\beta_k \\neq 0\\). La prueba \\(F\\) tiene la ventaja de que funciona para más de un predictor, mientras que la prueba \\(t\\) tiene la ventaja de que se pueden considerar alternativas unilaterales. Por lo tanto, ambas pruebas son consideradas útiles. Dividiendo el numerador y el denominador de la ecuación (4.2) por \\(Total~SS\\), la estadística de prueba también se puede escribir como: \\[ F-\\textrm{ratio}=\\frac{\\left( R_{full}^2-R_{reduced}^2\\right) /p}{\\left( 1-R_{full}^2\\right) / (n-(k+1))}. \\tag{4.4} \\] La interpretación de esta expresión es que el cociente \\(F\\) mide la disminución en el coeficiente de determinación, \\(R^2\\). La expresión en la ecuación (4.4) es particularmente útil para probar la adecuación del modelo, nuestro Caso Especial 5. En este caso, \\(p=k\\), y la suma de cuadrados de regresión bajo el modelo reducido es cero. Así, tenemos \\[ \\small{ F-\\textrm{ratio}=\\frac{\\left( (Regression~SS)_{full}\\right) /k}{s_{full}^2} =\\frac{(Regression~MS)_{full}}{(Error~SS)_{full}}. } \\] Esta estadística de prueba es una característica regular de la tabla ANOVA para muchos paquetes estadísticos. Por ejemplo, en nuestro ejemplo de Seguro de Vida Temporal, probar la adecuación del modelo significa evaluar \\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = 0\\). En la Tabla 4.2, el cociente \\(F\\) es 68.66 / 2.29 = 29.98. Con \\(df_1=5\\) y \\(df_2 = 269\\), el valor \\(F\\) es aproximadamente 2.248 y el valor \\(p\\) correspondiente es \\(\\Pr(F &gt; 29.98) \\approx 0\\). Esto nos lleva a rechazar firmemente la idea de que las variables explicativas no son útiles para entender la cobertura del seguro de vida temporal, reafirmando lo que aprendimos en el análisis gráfico y de correlación. Cualquier otro resultado sería sorprendente. Para otra expresión, dividiendo por \\(Total~SS\\), podemos escribir \\[ F-\\textrm{ratio}=\\frac{R^2}{1-R^2}\\frac{n-(k+1)}{k}. \\] Dado que tanto el cociente \\(F\\) como \\(R^2\\) son medidas del ajuste del modelo, parece intuitivamente plausible que estén relacionados de alguna manera. Una consecuencia de esta relación es que, a medida que \\(R^2\\) aumenta, también lo hace el cociente \\(F\\) y viceversa. El cociente \\(F\\) se usa porque su distribución muestral es conocida bajo una hipótesis nula, por lo que podemos hacer afirmaciones sobre significancia estadística. La medida \\(R^2\\) se usa debido a las interpretaciones fáciles asociadas con ella. 4.2.3 Estimando y Prediciendo Varios Coeficientes Estimación de Combinaciones Lineales de Coeficientes de Regresión En algunas aplicaciones, el principal interés es estimar una combinación lineal de los coeficientes de regresión. Para ilustrar, recordemos que en la Sección 3.5 desarrollamos una función de regresión para las contribuciones caritativas de un individuo (\\(y\\)) en términos de sus salarios (\\(x\\)). En esta función, hubo un cambio abrupto en la función en \\(x=97,500\\). Para modelarlo, definimos la variable binaria \\(z\\) para que sea cero si \\(x&lt;97,500\\) y uno si \\(x \\geq 97,500\\), y la función de regresión \\(\\mathrm{E~}y = \\beta_0 + \\beta_1 x + \\beta_2 z(x - 97,500)\\). Así, el cambio marginal esperado en las contribuciones por cambio en el salario para salarios superiores a \\(97,500\\) es \\(\\frac{\\partial \\left( \\mathrm{E~}y\\right)}{\\partial x} = \\beta_1 + \\beta_2\\). Para estimar \\(\\beta_1 + \\beta_2\\), un estimador razonable es \\(b_1 + b_2\\), el cual está disponible en el software estándar de regresión. Además, también nos gustaría calcular errores estándar para \\(b_1 + b_2\\) que se pueden utilizar, por ejemplo, para determinar un intervalo de confianza para \\(\\beta_1 + \\beta_2\\). Sin embargo, \\(b_1\\) y \\(b_2\\) suelen estar correlacionados, por lo que el cálculo del error estándar de \\(b_1 + b_2\\) requiere la estimación de la covarianza entre \\(b_1\\) y \\(b_2\\). La estimación de \\(\\beta_1 + \\beta_2\\) es un ejemplo de nuestro Caso Especial 3, que considera combinaciones lineales de coeficientes de regresión de la forma \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta = c_0 \\beta_0 + c_1 \\beta_1 + \\ldots + c_k \\beta_k\\). Para nuestro ejemplo de contribuciones caritativas, elegiríamos \\(c_1 = c_2 = 1\\) y los demás \\(c\\)’s igual a cero. Para estimar \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta\\), reemplazamos el vector de parámetros por el vector de estimadores y usamos \\(\\mathbf{c}^{\\prime} \\mathbf{b}\\). Para evaluar la fiabilidad de este estimador, como en la Sección 4.2.2, tenemos que \\(\\mathrm{Var}\\left( \\mathbf{c}^{\\prime} \\mathbf{b}\\right) = \\sigma^2 \\mathbf{c}^{\\prime}(\\mathbf{X^{\\prime} X})^{-1} \\mathbf{c}\\). Así, podemos definir la desviación estándar estimada, o error estándar, de \\(\\mathbf{c}^{\\prime} \\mathbf{b}\\) como \\[ se\\left( \\mathbf{c}^{\\prime} \\mathbf{b} \\right) = s \\sqrt{\\mathbf{c}^{\\prime} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{c}}. \\] Con esta cantidad, un intervalo de confianza del \\(100(1 - \\alpha) \\%\\) para \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta\\) es \\[ \\mathbf{c}^{\\prime} \\mathbf{b} \\pm t_{n - (k + 1), 1 - \\alpha / 2} ~ se(\\mathbf{c}^{\\prime} \\mathbf{b}). \\tag{4.5} \\] El intervalo de confianza en la ecuación (4.5) es válido bajo las Suposiciones F1-F5. Si elegimos que \\(\\mathbf{c}\\) tenga un “1” en la \\((j + 1)^{\\text{st}}\\) fila y ceros en los demás, entonces \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta = \\beta_j\\), \\(\\mathbf{c}^{\\prime} \\mathbf{b} = b_j\\), y \\[ se(b_j) = s \\sqrt{(j + 1)^{\\text{st}}~ \\textit{elemento diagonal de } (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1}}. \\] Por lo tanto, la ecuación (4.5) proporciona una base teórica para los intervalos de confianza de los coeficientes de regresión individuales introducidos en la ecuación (3.10) de la Sección 3.4 y lo generaliza a combinaciones lineales arbitrarias de los coeficientes de regresión. Otra aplicación importante de la ecuación (4.5) es la elección de \\(\\mathbf{c}\\) correspondiente a un conjunto de variables explicativas de interés, digamos, \\(\\mathbf{x}_{\\ast} = \\left( 1, x_{\\ast 1}, x_{\\ast 2}, \\ldots, x_{\\ast k} \\right)^{\\prime}\\). Estas pueden corresponder a una observación dentro del conjunto de datos o a un punto fuera de los datos disponibles. El parámetro de interés, \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta = \\mathbf{x}_{\\ast}^{\\prime} \\boldsymbol \\beta\\), es la respuesta esperada o la función de regresión en ese punto. Entonces, \\(\\mathbf{x}_{\\ast}^{\\prime} \\mathbf{b}\\) proporciona un estimador puntual y la ecuación (4.5) proporciona el intervalo de confianza correspondiente. Intervalos de Predicción La predicción es un objetivo inferencial que está estrechamente relacionado con la estimación de la función de regresión en un punto. Supongamos que, al considerar las contribuciones caritativas, conocemos los salarios de un individuo (y por lo tanto si los salarios superan los \\(97,500\\)) y deseamos predecir la cantidad de contribuciones caritativas. En general, asumimos que el conjunto de variables explicativas \\(\\mathbf{x}_{\\ast}\\) es conocido y deseamos predecir la respuesta correspondiente \\(y_{\\ast}\\). Esta nueva respuesta sigue las suposiciones descritas en la Sección 3.2. Específicamente, la respuesta esperada es \\(\\mathrm{E~}y_{\\ast} = \\mathbf{x}_{\\ast}^{\\prime} \\boldsymbol \\beta\\), \\(\\mathbf{x}_{\\ast}\\) es no estocástica, \\(\\mathrm{Var~}y_{\\ast} = \\sigma^2\\), \\(y_{\\ast}\\) es independiente de \\(\\{y_1, \\ldots, y_{n}\\}\\) y está distribuida normalmente. Bajo estas suposiciones, un intervalo de predicción del \\(100(1 - \\alpha)\\%\\) para \\(y_{\\ast}\\) es \\[ \\mathbf{x}_{\\ast}^{\\prime} \\mathbf{b} \\pm t_{n - (k + 1), 1 - \\alpha / 2} ~ s \\sqrt{1 + \\mathbf{x}_{\\ast}^{\\prime} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{x}_{\\ast}}. \\tag{4.6} \\] La ecuación (4.6) generaliza el intervalo de predicción introducido en la Sección 2.4. 4.3 Modelo ANOVA de Un Factor La Sección 4.1 mostró cómo incorporar variables categóricas no ordenadas, o factores, en un modelo de regresión lineal mediante el uso de variables binarias. Los factores son importantes en la investigación en ciencias sociales; pueden usarse para clasificar a las personas por género, etnia, estado civil, etc., o clasificar empresas por región geográfica, estructura organizativa, y así sucesivamente. En los estudios de seguros, las aseguradoras usan factores para categorizar a los asegurados según un “sistema de clasificación de riesgos.” Aquí, la idea es crear grupos de asegurados con características de riesgo similares que tendrán experiencias de reclamaciones similares. Estos grupos forman la base del precio del seguro, de modo que a cada asegurado se le cobra un monto adecuado a su categoría de riesgo. Este proceso a veces se conoce como “segmentación.” Aunque los factores pueden representarse como variables binarias en un modelo de regresión lineal, estudiamos los modelos de un factor como una unidad separada porque: El método de los mínimos cuadrados es mucho más simple, evitando la necesidad de invertir matrices de alta dimensión. Las interpretaciones resultantes de los coeficientes son más sencillas. El modelo de un factor sigue siendo un caso especial del modelo de regresión lineal. Por lo tanto, no se necesita teoría estadística adicional para establecer sus capacidades de inferencia estadística. Para establecer la notación para el modelo ANOVA de un factor, ahora consideramos el siguiente ejemplo. Ejemplo: Reclamaciones de Seguros de Automóviles. Examinamos la experiencia de reclamaciones de un gran asegurador de propiedades y accidentes del medio oeste de los Estados Unidos para seguros de automóviles particulares. La variable dependiente es la cantidad pagada en una reclamación cerrada, en dólares (reclamaciones que no se cerraron al final del año se manejan por separado). Las aseguradoras categorizan a los asegurados según un sistema de clasificación de riesgos. El sistema de clasificación de riesgos de esta aseguradora se basa en: Características del operador del automóvil (edad, género, estado civil y si es el conductor principal u ocasional de un automóvil). Características del vehículo (uso en la ciudad o en el campo, si el vehículo se usa para ir a la escuela o al trabajo, para negocios o placer, y si se usa para ir al trabajo, la distancia aproximada del trayecto). Estos factores se resumen en la variable categórica de clase de riesgo CLASS. La Tabla 4.5 muestra 18 clases de riesgo - no se proporciona información adicional de clasificación aquí para proteger los intereses propietarios de la aseguradora. La Tabla 4.5 resume los resultados de \\(n=6,773\\) reclamaciones para conductores de 50 años o más. Podemos ver que la reclamación mediana varía desde un mínimo de 707.40 (CLASE F7) hasta un máximo de 1,231.25 (CLASE C72). La distribución de las reclamaciones resulta ser asimétrica, por lo que consideramos \\(y\\) = reclamaciones logarítmicas. La tabla presenta medias, medianas y desviaciones estándar. Dado que la distribución de las reclamaciones logarítmicas es menos asimétrica, las medias están cerca de las medianas. La Figura 4.2 muestra la distribución de las reclamaciones logarítmicas por clase de riesgo. Table 4.5: Estadísticas Resumidas de Reclamaciones de Automóviles por Clase de Riesgo 1 2 3 4 5 6 Clase C1 C11 C1A C1B C1C C2 Número 726 1151 77 424 38 61 Mediana (dólares) 948.86 1,013.81 925.48 1,026.73 1,001.73 851.20 Mediana (en dólares log) 6.855 6.921 6.830 6.934 6.909 6.747 Media (en dólares log) 6.941 6.952 6.866 6.998 6.786 6.801 Desv. estándar (en dólares log) 1.064 1.074 1.072 1.068 1.110 0.948 Clase C6 C7 C71 C72 C7A C7B Número 911 913 1129 85 113 686 Mediana (dólares) 1,011.24 957.68 960.40 1,231.25 1,139.93 1,113.13 Mediana (en dólares log) 6.919 6.865 6.867 7.116 7.039 7.015 Media (en dólares log) 6.926 6.901 6.954 7.183 7.064 7.072 Desv. estándar (en dólares log) 1.115 1.058 1.038 0.988 1.021 1.103 Clase C7C F1 F11 F6 F7 F71 Número 81 29 40 157 59 93 Mediana (dólares) 1,200.00 1,078.04 774.79 1,105.04 707.40 1,118.73 Mediana (en dólares log) 7.090 6.983 6.652 7.008 6.562 7.020 Media (en dólares log) 7.244 7.004 6.804 6.910 6.577 6.935 Desv. estándar (en dólares log) 0.944 0.996 1.212 1.193 0.897 0.983 Figure 4.2: Diagramas de Caja de Reclamaciones Logarítmicas por Clase de Riesgo Código R para Producir la Tabla 4.5 y la Figura 4.2 AutoC &lt;- read.csv(&quot;CSVData/AutoClaims.csv&quot;, header=TRUE) # CREAR UNA TABLA DE MEDIAS Y DESVIACIONES ESTÁNDAR library(Hmisc) t1 &lt;- summarize(log(AutoC$PAID), AutoC$CLASS, length ) t2 &lt;- summarize(AutoC$PAID, AutoC$CLASS, median) t3 &lt;- summarize(log(AutoC$PAID), AutoC$CLASS, median) t4 &lt;- summarize(log(AutoC$PAID), AutoC$CLASS, mean) t5 &lt;- summarize(log(AutoC$PAID), AutoC$CLASS, sd) tablemat &lt;- cbind(t1, format(round(t2[2], digits = 2), big.mark = &#39;,&#39;), round(t3[2], digits = 3), round(t4[2], digits = 3), round(t5[2], digits = 3)) block1 &lt;- t(tablemat[1:6,]) block2 &lt;- t(tablemat[7:12,]) block3 &lt;- t(tablemat[13:18,]) bigblock &lt;- rbind(block1, block2, block3) temprow &lt;- c(&quot;Clase&quot;, &quot;Número&quot;, &quot;Mediana (dólares)&quot;, &quot;Mediana (en dólares log)&quot; ,&quot;Media (en dólares log)&quot;, &quot;Desv. estándar (en dólares log)&quot;) bigblock1 &lt;- cbind (c(temprow,temprow, temprow), bigblock) row.names(bigblock1) &lt;- NULL TableGen1(TableData=bigblock1 , TextTitle=&#39;Estadísticas Resumidas de Reclamaciones de Automóviles por Clase de Riesgo&#39;, Align=&#39;lrrrrrr&#39;, Digits=3, ColumnSpec=1:6, ColWidth = ColWidth4) %&gt;% kableExtra::column_spec(1, width = &quot;6cm&quot;) library(HH) AutoC &lt;- read.csv(&quot;CSVData/AutoClaims.csv&quot;, header=TRUE) # RECLAMACIONES DE AUTOMÓVILES # FIGURA 4.2 par(cex=0.6) boxplot(log(PAID) ~ CLASS,cex=.6, cex.labels=2, data = AutoC, xlab = &quot;&quot;, ylab = &quot;&quot;) Esta sección se centra en la clase de riesgo (CLASS) como la variable explicativa. Usamos la notación \\(y_{ij}\\) para referirnos a la \\(i\\)-ésima observación de la \\(j\\)-ésima clase de riesgo. Para la \\(j\\)-ésima clase de riesgo, asumimos que hay \\(n_j\\) observaciones. Existen \\(n=n_1+n_2+\\ldots +n_c\\) observaciones en total. Los datos son: \\[ \\begin{array}{cccccc} \\small{\\text{Datos para la clase de riesgo }}1 &amp; \\ \\ \\ \\ &amp; y_{11} &amp; y_{21} &amp; \\ldots &amp; y_{n_1,1} \\\\ \\small{\\text{Datos para la clase de riesgo }}2 &amp; &amp; y_{12} &amp; y_{22} &amp; \\ldots &amp; y_{n_2,1} \\\\ . &amp; &amp; . &amp; . &amp; \\ldots &amp; . \\\\ \\small{\\text{Datos para la clase de riesgo }} c &amp; &amp; y_{1c} &amp; y_{2c} &amp; \\ldots &amp; y_{n_c,c} \\end{array} \\] donde \\(c=18\\) es el número de niveles del factor CLASS. Debido a que cada nivel de un factor puede organizarse en una sola fila (o columna), otro término para este tipo de datos es una “clasificación de una vía.” Así, un modelo de una vía es otro término para un modelo de un factor. Una medida resumen importante para cada nivel del factor es el promedio de la muestra. Sea \\[ \\overline{y}_j=\\frac{1}{n_j}\\sum_{i=1}^{n_j}y_{ij} \\] el promedio de la \\(j\\)-ésima CLASS. Suposiciones del Modelo y Análisis La ecuación del modelo ANOVA de un factor es \\[ y_{ij}=\\mu_j+ \\varepsilon_{ij}\\ \\ \\ \\ \\ \\ i=1,\\ldots ,n_j,\\ \\ \\ \\ \\ j=1,\\ldots ,c. \\tag{4.7} \\] Al igual que con los modelos de regresión, se asume que las desviaciones aleatorias \\(\\{\\varepsilon_{ij} \\}\\) tienen una media cero con varianza constante (Suposición E3) y son independientes entre sí (Suposición E4). Dado que asumimos que el valor esperado de cada desviación es cero, tenemos \\(\\text{E}~y_{ij}=\\mu_j\\). Por lo tanto, interpretamos \\(\\mu_j\\) como el valor esperado de la respuesta \\(y_{ij}\\), es decir, la media \\(\\mu\\) varía según el nivel del factor \\(j\\). Para estimar los parámetros \\(\\{\\mu_j\\}\\), al igual que en la regresión, usamos el método de mínimos cuadrados, introducido en la Sección 2.1. Es decir, sea \\(\\mu^{\\ast}_j\\) una estimación “candidata” de \\(\\mu_j\\). La cantidad \\[ SS(\\mu^{\\ast}_1, \\ldots , \\mu^{\\ast}_{c}) = \\sum_{j=1}^{c} \\sum_{i=1}^{n_j} (y_{ij}-\\mu^{\\ast}_j)^2 \\] representa la suma de los cuadrados de las desviaciones de las respuestas respecto a estas estimaciones candidatas. A partir de algebra básica, el valor de \\(\\mu^{\\ast}_j\\) que minimiza esta suma de cuadrados es \\(\\bar{y}_j\\). Por lo tanto, \\(\\bar{y}_j\\) es la estimación por mínimos cuadrados de \\(\\mu_j\\). Para entender la confiabilidad de las estimaciones, podemos descomponer la variabilidad como en el caso de regresión, presentado en las Secciones 2.3.1 y 3.3. La suma mínima de los cuadrados de las desviaciones se llama suma de cuadrados del error y se define como \\[ Error ~SS = SS(\\bar{y}_1, \\ldots, \\bar{y}_{c}) = \\sum_{j=1}^{c} \\sum_{i=1}^{n_j} \\left(y_{ij}-\\bar{y}_j \\right)^2. \\] La variación total en el conjunto de datos se resume en la suma total de cuadrados, \\[ Total ~SS=\\sum_{j=1}^{c}\\sum_{i=1}^{n_j}(y_{ij}-\\bar{y})^2. \\] La diferencia, llamada suma de cuadrados del factor, se puede expresar como: \\[ \\begin{array}{ll} Factor~ SS &amp; = Total ~SS - Error ~SS \\\\ &amp; = \\sum_{j=1}^{c}\\sum_{i=1}^{n_j}(y_{ij}-\\bar{y})^2-\\sum_{j=1}^{c}\\sum_{i=1}^{n_j}(y_{ij}-\\bar{y}_j)^2 = \\sum_{j=1}^{c}\\sum_{i=1}^{n_j}(\\bar{y}_j-\\bar{y})^2 \\\\ &amp; = \\sum_{j=1}^{c}n_j(\\bar{y}_j-\\bar{y})^2. \\end{array} \\] Las dos últimas igualdades se derivan de la manipulación algebraica. El \\(Factor ~SS\\) desempeña el mismo papel que el \\(Regression ~SS\\) en los Capítulos 2 y 3. La descomposición de la variabilidad se resume en la Tabla 4.6. Table 4.6: Tabla ANOVA para el Modelo de Un Factor Fuente Suma de Cuadrados \\(df\\) Media Cuadrática Factor \\(Factor ~SS\\) \\(c-1\\) \\(Factor ~MS\\) Error \\(Error ~SS\\) \\(n-c\\) \\(Error ~MS\\) Total \\(Total ~SS\\) \\(n-1\\) Las convenciones para esta tabla son las mismas que en el caso de regresión. Es decir, la columna de media cuadrática (MS) se define dividiendo la columna de suma de cuadrados (SS) por la columna de grados de libertad (df). Por lo tanto, \\(Factor~MS \\equiv (Factor~SS)/(c-1)\\) y \\(Error~MS \\equiv (Error~SS)/(n-c)\\). Usamos \\[ s^2 = \\text{Error MS} = \\frac{1}{n-c} \\sum_{j=1}^{c}\\sum_{i=1}^{n_j} e_{ij}^2 \\] como nuestra estimación de \\(\\sigma^2\\), donde \\(e_{ij} = y_{ij} - \\bar{y}_j\\) es el residuo. Con este valor de \\(s\\), se puede mostrar que el intervalo de estimación para \\(\\mu_j\\) es \\[ \\bar{y}_j \\pm t_{n-c,1-\\alpha /2}\\frac{s}{\\sqrt{n_j}}. \\tag{4.8} \\] Aquí, el valor t \\(t_{n-c,1-\\alpha /2}\\) es un percentil de la distribución t con \\(df=n-c\\) grados de libertad. Ejemplo: Reclamaciones de Automóviles - Continuación. Para ilustrar, la tabla ANOVA que resume el ajuste para los datos de reclamaciones de automóviles se presenta en la Tabla 4.7. Aquí, vemos que la media cuadrática del error es \\(s^2 = 1.14.\\) Table 4.7: Tabla ANOVA para Reclamaciones de Automóviles Logarítmicas Fuente Suma de Cuadrados \\(df\\) Media Cuadrática CLASS 39.2 17 2.31 Error 7729.0 6755 1.14 Total 7768.2 6772 En la tarificación de automóviles, se usan los promedios de las reclamaciones para ayudar a fijar los precios de las coberturas de seguros. Como ejemplo, para la CLASS C72, el promedio de la reclamación logarítmica es 7.183. A partir de la ecuación (4.8), un intervalo de confianza del 95% es \\[ \\small{ 7.183 \\pm (1.96) \\frac{\\sqrt{1.14}}{\\sqrt{85}} = 7.183 \\pm 0.227 = (6.956 ,7.410). } \\] Cabe destacar que estas estimaciones están en unidades logarítmicas naturales. En dólares, nuestra estimación puntual es \\(e^{7.183} = 1,316.85\\) y nuestro intervalo de confianza del 95% es \\((e^{6.956} , e^{7.410}) \\text{ o } (\\$1,049.43, \\$1,652.43)\\). Una característica importante de la descomposición y estimación en un ANOVA de un factor es la facilidad de cálculo. Aunque la suma de cuadrados parece compleja, es importante señalar que no se requieren cálculos matriciales. En cambio, todos los cálculos se pueden realizar mediante promedios y sumas de cuadrados. Esto ha sido un aspecto importante históricamente, antes de la era de la computación de escritorio disponible. Además, las aseguradoras pueden segmentar sus carteras en cientos o incluso miles de clases de riesgo en lugar de las 18 utilizadas en nuestros datos de Reclamaciones de Automóviles. Por lo tanto, incluso hoy en día puede ser útil identificar una variable categórica como un factor y dejar que su software estadístico utilice técnicas de estimación ANOVA. Además, la estimación ANOVA también proporciona una interpretación directa de los resultados. Vínculo con la Regresión Esta subsección muestra cómo un modelo ANOVA de un factor se puede reescribir como un modelo de regresión. Para ello, hemos visto que tanto el modelo de regresión como el modelo ANOVA de un factor utilizan una estructura de error lineal con las Suposiciones E3 y E4 para errores idénticamente y distribuidos de manera independiente. De manera similar, ambos utilizan la suposición de normalidad E5 para resultados de inferencia seleccionados (como intervalos de confianza). Ambos emplean variables explicativas no estocásticas como en la Suposición E2. Ambos tienen un término de error aditivo (media cero), por lo que la principal diferencia aparente está en la respuesta esperada, \\(\\mathrm{E }~y\\). Para el modelo de regresión lineal, \\(\\mathrm{E }~y\\) es una combinación lineal de variables explicativas (Suposición F1). Para el modelo ANOVA de un factor, \\(\\mathrm{E}~y_] = \\mu_j\\) es una media que depende del nivel del factor. Para igualar estos dos enfoques, para el factor ANOVA con \\(c\\) niveles, definimos \\(c\\) variables binarias, \\(x_1, x_2, \\ldots, x_c\\). Aquí, \\(x_j\\) indica si una observación cae o no en el nivel \\(j\\)-ésimo. Con estas variables, podemos reescribir nuestro modelo ANOVA de un factor como \\[ y = \\mu_1 x_1 + \\mu_2 x_2 + \\ldots + \\mu_c x_c + \\varepsilon. \\tag{4.9} \\] Así, hemos reescrito la respuesta esperada del ANOVA de un factor como una función de regresión, aunque utilizando una forma sin intercepto (como en la ecuación (3.5)). El ANOVA de un factor es un caso especial de nuestro modelo de regresión habitual, utilizando variables binarias del factor como variables explicativas en la función de regresión. Como hemos visto, no se necesitan cálculos matriciales para la estimación por mínimos cuadrados. Sin embargo, siempre se pueden utilizar los procedimientos matriciales desarrollados en el Capítulo 3. La Sección 4.7.1 muestra cómo nuestra expresión matricial habitual para los coeficientes de regresión (\\(\\mathbf{b} = \\left(\\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{\\prime}\\mathbf{y}\\)) se reduce a las estimaciones simples \\(\\bar{y}_j\\) cuando se utiliza una sola variable categórica. Reparametrización Para incluir un término de intercepto, definimos \\(\\tau_j = \\mu_j - \\mu\\), donde \\(\\mu\\) es un parámetro aún no especificado. Como cada observación debe pertenecer a una de las \\(c\\) categorías, tenemos que \\(x_1 + x_2 + \\ldots + x_{c} = 1\\) para cada observación. Así, al usar \\(\\mu_j = \\tau_j + \\mu\\) en la ecuación (4.9), obtenemos \\[ y = \\mu + \\tau_1 x_1 + \\tau_2 x_2 + \\ldots + \\tau_{c} x_{c} + \\varepsilon, \\tag{4.10} \\] Así, hemos reescrito el modelo en lo que parece ser nuestro formato usual de regresión. Usamos \\(\\tau\\) en lugar de \\(\\beta\\) por razones históricas. Los modelos ANOVA fueron inventados por R.A. Fisher en relación con experimentos agrícolas. Aquí, la configuración típica es aplicar varios tratamientos a parcelas de tierra para cuantificar las respuestas de rendimiento de los cultivos. Así, la letra griega “t”, \\(\\tau\\), sugiere la palabra tratamiento, otro término utilizado para describir los niveles del factor de interés. Una versión más simple de la ecuación (4.10) se puede dar cuando identificamos el nivel del factor. Es decir, si sabemos que una observación pertenece al nivel \\(j\\)-ésimo, entonces solo \\(x_j\\) es uno y los otros \\(x\\) son 0. Por lo tanto, una expresión más simple de la ecuación (4.10) es \\[ y_{ij} = \\mu + \\tau_j + \\varepsilon_{ij}. \\] Al comparar las ecuaciones (4.9) y (4.10), vemos que el número de parámetros ha aumentado en uno. Es decir, en la ecuación (4.9) hay \\(c\\) parámetros, \\(\\mu_1, \\ldots, \\mu_c\\), mientras que en la ecuación (4.10) hay \\(c + 1\\) parámetros, \\(\\mu\\) y \\(\\tau_1, \\ldots, \\tau_c\\). Se dice que el modelo en la ecuación (4.10) está sobreparametrizado. Es posible estimar este modelo directamente, utilizando la teoría general de modelos lineales, resumida en la Sección 4.7.3. En esta teoría, los coeficientes de regresión no necesitan ser identificables. Alternativamente, se pueden hacer equivalentes estas dos expresiones restringiendo el movimiento de los parámetros en la ecuación (4.10). A continuación, presentamos dos formas de imponer restricciones. El primer tipo de restricción, generalmente utilizado en el contexto de regresión, es requerir que uno de los \\(\\tau\\) sea cero. Esto equivale a eliminar una de las variables explicativas. Por ejemplo, podríamos usar \\[ y = \\mu + \\tau_1 x_1 + \\tau_2 x_2 + \\ldots + \\tau_{c-1} x_{c-1} + \\varepsilon, \\tag{4.11} \\] eliminando \\(x_c\\). Con esta formulación, es fácil ajustar el modelo en la ecuación (4.11) utilizando rutinas de software de regresión, porque solo se necesita ejecutar la regresión con \\(c-1\\) variables explicativas. Sin embargo, se debe tener cuidado con la interpretación de los parámetros. Para igualar los modelos en las ecuaciones (4.9) y (4.10), necesitamos definir \\(\\mu \\equiv \\mu_c\\) y \\(\\tau_j = \\mu_j - \\mu_c\\) para \\(j=1,2,\\ldots,c-1\\). Es decir, el término de intercepto de la regresión es el nivel medio de la categoría eliminada, y cada coeficiente de regresión es la diferencia entre un nivel medio y el nivel medio eliminado. No es necesario eliminar el último nivel \\(c\\), y de hecho, se podría eliminar cualquier nivel. Sin embargo, la interpretación de los parámetros depende de la variable eliminada. Con esta restricción, los valores ajustados son \\(\\hat{\\mu} = \\hat{\\mu}_c = \\bar{y}_c\\) y \\(\\hat{\\tau}_j = \\hat{\\mu}_j - \\hat{\\mu}_c = \\bar{y}_j - \\bar{y}_c.\\) Recordemos que el símbolo de sombrero (\\(\\hat{\\cdot}\\)), o “hat,” representa un valor estimado o ajustado. El segundo tipo de restricción es interpretar \\(\\mu\\) como una media para toda la población. Para ello, el requisito usual es \\(\\mu \\equiv \\frac{1}{n} \\sum_{j=1}^c n_j \\mu_j\\), es decir, \\(\\mu\\) es un promedio ponderado de medias. Con esta definición, interpretamos \\(\\tau_j = \\mu_j - \\mu\\) como diferencias de tratamiento entre un nivel medio y la media poblacional. Otra forma de expresar esta restricción es \\(\\sum_{j=1}^{c} n_j \\tau_j = 0\\), es decir, la suma (ponderada) de las diferencias de tratamiento es cero. La desventaja de esta restricción es que no se puede implementar fácilmente con una rutina de regresión, y se necesita una rutina especial. La ventaja es que hay una simetría en las definiciones de los parámetros. No es necesario preocuparse por qué variable se está eliminando de la ecuación, lo cual es una consideración importante. Con esta restricción, los valores ajustados son \\[ \\hat{\\mu} = \\frac{1}{n} \\sum_{j=1}^{c} n_j \\hat{\\mu}_j = \\frac{1}{n} \\sum_{j=1}^{c} n_j \\bar{y}_j = \\bar{y} \\] y \\[ \\hat{\\tau}_j = \\hat{\\mu}_j - \\hat{\\mu} = \\bar{y}_j - \\bar{y}. \\] 4.4 Combinando Variables Explicativas Categóricas y Continuas Existen varias formas de combinar variables explicativas categóricas y continuas. Inicialmente, presentamos el caso de solo una variable categórica y una variable continua. Luego, presentamos brevemente el caso general, llamado modelo lineal general. Cuando se combinan modelos de variables categóricas y continuas, usamos la terminología factor para la variable categórica y covariable para la variable continua. Combinando un Factor y una Covariable Comencemos con los modelos más simples que utilizan un factor y una covariable. En la Sección 4.3, introdujimos el modelo de un solo factor \\(y_{ij} = \\mu_j + \\varepsilon_{ij}\\). En el Capítulo 2, introdujimos la regresión lineal básica en términos de una variable continua, o covariable, usando \\(y_{ij} = \\beta_0 + \\beta_1 x_{ij} + \\varepsilon_{ij}\\). La Tabla 4.8 resume diferentes enfoques que podrían usarse para representar combinaciones de un factor y una covariable. Table 4.8: Varios Modelos que Representan Combinaciones de Un Factor y Una Covariable Descripción del Modelo Notación ANOVA de un factor (modelo sin covariable) \\(y_{ij} = \\mu_j + \\varepsilon_{ij}\\) Regresión con intercepto y pendiente constante (modelo sin factor) \\(y_{ij} = \\beta_0 + \\beta_1 x_{ij} + \\varepsilon_{ij}\\) Regresión con intercepto variable y pendiente constante (modelo de análisis de covarianza) \\(y_{ij} = \\beta_{0j} + \\beta_1 x_{ij} + \\varepsilon_{ij}\\) Regresión con intercepto constante y pendiente variable \\(y_{ij} = \\beta_0 + \\beta_{1j} x_{ij} + \\varepsilon_{ij}\\) Regresión con intercepto y pendiente variable \\(y_{ij} = \\beta_{0j} + \\beta_{1j} x_{ij} + \\varepsilon_{ij}\\) Podemos interpretar la regresión con intercepto variable y pendiente constante como un modelo aditivo, porque estamos sumando el efecto del factor, \\(\\beta_{0j}\\), al efecto de la covariable, \\(\\beta_1 x_{ij}\\). Nótese que también se podría usar la notación \\(\\mu_j\\) en lugar de \\(\\beta_{0j}\\) para sugerir la presencia de un efecto de factor. Este también es conocido como un modelo de análisis de covarianza (ANCOVA). La regresión con intercepto y pendiente variables puede considerarse un modelo de interacción. Aquí, tanto el intercepto, \\(\\beta_{0j}\\), como la pendiente, \\(\\beta_{1j}\\), pueden variar según el nivel del factor. En este sentido, interpretamos que el factor y la covariable están “interactuando”. El modelo con intercepto constante y pendiente variable típicamente no se usa en la práctica; se incluye aquí por completitud. Con este modelo, el factor y la covariable interactúan solo a través de la pendiente variable. Las Figuras 4.3, 4.4 y 4.5 ilustran las respuestas esperadas de estos modelos. Figure 4.3: Gráfico de la respuesta esperada frente a la covariable para el modelo de regresión con intercepto variable y pendiente constante. Figure 4.4: Gráfico de la respuesta esperada frente a la covariable para el modelo de regresión con intercepto constante y pendiente variable. Figure 4.5: Gráfico de la respuesta esperada frente a la covariable para el modelo de regresión con intercepto variable y pendiente variable. Código R para Producir las Figuras 4.3, 4.4, y 4.5 ## Figura 4.3 x &lt;- seq(0, 100, length = 101) y &lt;- 0.15 * x par(mar = c(3.2, 3, .2, .2)) plot(x, y, type = &quot;l&quot;, xlim = c(10, 90), xaxt = &quot;n&quot;, ylim = c(1.5, 25), yaxt = &quot;n&quot;, ylab = &quot;&quot;, xlab = &quot;&quot;) mtext(&quot;y&quot;, side = 2, las = 1, line = 2, cex = 1.1) mtext(&quot;x&quot;, side = 1, line = 2, cex = 1.1) lines(x, y + 6) lines(x, y + 8) arrows(40, 5.5, 55, 5.5, code = 1, lwd = 2, angle = 15, length = 0.2) text(69, 5.5, expression(y == beta[&quot;0,3&quot;] + beta[1] * x), cex = 1.1) arrows(30, 13, 30, 16, code = 1, lwd = 2, angle = 15, length = 0.2) text(30, 17, expression(y == beta[&quot;0,2&quot;] + beta[1] * x), cex = 1.1) arrows(66, 15.5, 62, 12, code = 1, lwd = 2, angle = 15, length = 0.2) text(58, 11, expression(y == beta[&quot;0,1&quot;] + beta[1] * x), cex = 1.1) ## Figura 4.4 x &lt;- seq(0, 100, length = 101) y3 &lt;- 12 + 0.1 * x y2 &lt;- 12 - 0.1 * x y1 &lt;- 12 + 0.02 * x par(mar = c(3.2, 3, .2, .2)) plot(x, y1, type = &quot;l&quot;, xlim = c(3.5, 90), xaxt = &quot;n&quot;, ylim = c(1.5, 25), yaxt = &quot;n&quot;, ylab = &quot;&quot;, xlab = &quot;&quot;) mtext(&quot;y&quot;, side = 2, las = 1, line = 2, cex = 1.1) mtext(&quot;x&quot;, side = 1, line = 2, cex = 1.1) lines(x, y2) lines(x, y3) arrows(38, 8, 35, 4, code = 1, lwd = 2, angle = 15, length = 0.2) text(35, 3.5, expression(y == beta[0] + beta[&quot;1,2&quot;] * x), cex = 1.1) arrows(26, 15, 26, 18, code = 1, lwd = 2, angle = 15, length = 0.2) text(26, 19, expression(y == beta[0] + beta[&quot;1,3&quot;] * x), cex = 1.1) arrows(62, 13.5, 66, 16, code = 1, lwd = 2, angle = 15, length = 0.2) text(70, 16.5, expression(y == beta[0] + beta[&quot;1,1&quot;] * x), cex = 1.1) ## Figura 4.5 x1 &lt;- seq(50, 90, length = 51) y1 &lt;- -5 + 0.2 * x1 x2 &lt;- seq(30, 80, length = 41) y2 &lt;- 12 + 0.02 * x2 x3 &lt;- seq(10, 50, length = 41) y3 &lt;- 15 + 0.15 * x3 par(mar = c(3.2, 3, .2, .2)) plot(x1, y1, type = &quot;l&quot;, xlim = c(3.5, 90), xaxt = &quot;n&quot;, ylim = c(1.5, 25), ylab = &quot;&quot;, xlab = &quot;&quot;, yaxt = &quot;n&quot;) mtext(&quot;y&quot;, side = 2, las = 1, line = 2, cex = 1.1) mtext(&quot;x&quot;, side = 1, line = 2, cex = 1.1) lines(x2, y2) lines(x3, y3) arrows(68, 8, 65, 4, code = 1, lwd = 2, angle = 5, length = 0.2) text(65, 3.5, expression(y == beta[&quot;0,1&quot;] + beta[&quot;1,1&quot;] * x), cex = 1.1) arrows(50, 12.5, 30, 11.5, code = 1, lwd = 2, angle = 5, length = 0.2) text(30, 10.5, expression(y == beta[&quot;0,3&quot;] + beta[&quot;1,3&quot;] * x), cex = 1.1) arrows(30, 20, 20, 22, code = 1, lwd = 2, angle = 5, length = 0.2) text(20, 23, expression(y == beta[&quot;0,2&quot;] + beta[&quot;1,2&quot;] * x), cex = 1.1) Para cada modelo presentado en la Tabla 4.8, las estimaciones de los parámetros pueden calcularse utilizando el método de mínimos cuadrados. Como es habitual, esto significa escribir la respuesta esperada, \\(\\mathrm{E }~y_{ij}\\), como una función de variables conocidas y parámetros desconocidos. Para el modelo de regresión con intercepto variable y pendiente constante, las estimaciones de mínimos cuadrados pueden expresarse de manera compacta como: \\[ b_1 = \\frac{\\sum_{j=1}^{c}\\sum_{i=1}^{n_j} (x_{ij} - \\bar{x}_j) (y_{ij} - \\bar{y}_j)}{\\sum_{j=1}^{c}\\sum_{i=1}^{n_j} (x_{ij} - \\bar{x}_j)^2} \\] y \\(b_{0j} = \\bar{y}_j - b_1 \\bar{x}_j\\). De manera similar, las estimaciones de mínimos cuadrados para el modelo de regresión con intercepto y pendiente variables pueden expresarse como: \\[ b_{1j} = \\frac{\\sum_{i=1}^{n_j} (x_{ij} - \\bar{x}_j) (y_{ij} - \\bar{y}_j)}{\\sum_{i=1}^{n_j} (x_{ij} - \\bar{x}_j)^2} \\] y \\(b_{0j} = \\bar{y}_j - b_{1j} \\bar{x}_j\\). Con estas estimaciones de los parámetros, se pueden calcular los valores ajustados. Para cada modelo, los valores ajustados se definen como la respuesta esperada con los parámetros desconocidos reemplazados por sus estimaciones de mínimos cuadrados. Por ejemplo, para el modelo de regresión con intercepto variable y pendiente constante, los valores ajustados son \\(\\hat{y}_{ij} = b_{0j} + b_1 x_{ij}.\\) Ejemplo: Costos Hospitalarios en Wisconsin. Ahora estudiamos el impacto de varios predictores en los costos hospitalarios en el estado de Wisconsin. Identificar predictores de los costos hospitalarios puede proporcionar dirección a los hospitales, al gobierno, a las aseguradoras y a los consumidores en el control de estas variables, lo que a su vez lleva a un mejor control de los costos hospitalarios. Los datos para el año 1989 fueron obtenidos de la Oficina de Información de Salud, del Departamento de Salud y Servicios Humanos de Wisconsin. Se utilizan datos transversales, que detallan los costos de alta de 20 grupos relacionados con el diagnóstico (DRG) para hospitales en el estado de Wisconsin, desglosados en nueve áreas principales de servicios de salud y tres tipos de pagador (Pago por servicio, HMO y otros). Aunque hay 540 combinaciones potenciales de DRG, área y pagador (\\(20 \\times 9 \\times 3 = 540\\)), solo 526 combinaciones se realizaron realmente en el conjunto de datos de 1989. Otros predictores incluidos fueron el logaritmo del número total de altas (NO DSCHG) y el número total de camas hospitalarias (NUM BEDS) para cada combinación. La variable de respuesta es el logaritmo de los costos hospitalarios totales por número de altas (CHGNUM). Para simplificar la presentación, ahora consideramos solo los costos asociados con tres grupos relacionados con el diagnóstico (DRG): DRG #209, DRG #391, y DRG #430. La covariable, \\(x\\), es el logaritmo natural del número de altas. En entornos ideales, los hospitales con más pacientes disfrutan de menores costos debido a economías de escala. En entornos no ideales, los hospitales pueden no tener capacidad excedente y, por lo tanto, los hospitales con más pacientes tienen costos más altos. Uno de los propósitos de este análisis es investigar la relación entre los costos hospitalarios y la utilización hospitalaria. Recuerde que nuestra medida de los costos hospitalarios es el logaritmo de los costos por alta (\\(y\\)). El diagrama de dispersión en la Figura 4.6 da una idea preliminar de la relación entre \\(y\\) y \\(x\\). Notamos que parece haber una relación negativa entre \\(y\\) y \\(x\\). La relación negativa entre \\(y\\) y \\(x\\) sugerida por la Figura 4.6 es engañosa y está inducida por una variable omitida, la categoría del costo (DRG). Para ver el efecto conjunto de la variable categórica DRG y la variable continua \\(x\\), en la Figura 4.7 se muestra un gráfico de \\(y\\) versus \\(x\\) donde los símbolos de trazado son códigos para el nivel de la variable categórica. En este gráfico, vemos que el nivel de costo varía según el nivel del factor DRG. Además, para cada nivel de DRG, la pendiente entre \\(y\\) y \\(x\\) es cero o positiva. Las pendientes no son negativas, como sugiere la Figura 4.6. Figure 4.6: Gráfico del logaritmo natural del costo por alta versus logaritmo natural del número de altas. Este gráfico sugiere una relación negativa engañosa. Figure 4.7: Gráfico con letras del logaritmo natural del costo por alta versus logaritmo natural del número de altas según DRG. Aquí, A es para DRG #209, B es para DRG #391, y C es para DRG #430. Código R para Producir las Figuras 4.6 y 4.7 ## C5_HOSP.txt HospitalCosts &lt;- read.csv(&quot;CSVData/WiscHospCosts.csv&quot;, header=TRUE) # attach(HospitalCosts) # LNCHGNUM&lt;-log(CHG_NUM) ## Figura 4.6 par(mar=c(4.1,4,1,1), cex=1.1) plot(log(CHG_NUM) ~ log(NO_DSCHG), data=HospitalCosts, subset=DRG==209|DRG==391|DRG==430,type=&quot;p&quot;,pch=1,cex=0.6, xlim=c(2,9.2),xaxp=c(1.5,9,5),xlab=&quot;Número de Altas&quot;,ylim=c(5.9,9.8), yaxp=c(6.0,9.6,3),ylab=&quot;&quot;,las=1) mtext(&quot;CHGNUM&quot;, side=2, at=10.1, las=1, cex=1.1, adj=.4) ## Figura 4.7 DRGabc&lt;-character(length=length(HospitalCosts$DRG)) for (i in 1:length(HospitalCosts$DRG)) {DRGabc[i]&lt;-if (HospitalCosts$DRG[i]==209) &quot;A&quot; else if (HospitalCosts$DRG[i]==391) &quot;B&quot; else if (HospitalCosts$DRG[i]==430) &quot;C&quot; else &quot;D&quot;} par(mar=c(4.1,4,1,1), cex=1.1) plot(log(CHG_NUM) ~ log(NO_DSCHG), data=HospitalCosts, subset=DRG==209|DRG==391|DRG==430,type=&quot;p&quot;, pch=as.character(DRGabc),cex=0.8, xlim=c(2,9.2), xaxp=c(1.5,9,5),xlab=&quot;Número de Altas&quot;,ylim=c(5.9,9.8), yaxp=c(6.0,9.6,3),ylab=&quot;&quot;,las=1) mtext(&quot;CHGNUM&quot;, side=2, at=10.1, las=1, cex=1.1, adj=.4) Table 4.9: Bondad de Ajuste de los Modelos de Costos Hospitalarios en Wisconsin Descripción del Modelo Grados de libertad del modelo Grados de libertad del error Suma de cuadrados del error R-cuadrado (%) Media Cuadrática ANOVA de un factor 2 76 9.396 93.3 0.124 Regresión con intercepto y pendiente constantes 1 77 115.059 18.2 1.222 Regresión con intercepto variable y pendiente constante 3 75 7.482 94.7 0.100 Regresión con intercepto constante y pendiente variable 3 75 14.048 90.0 0.187 Regresión con intercepto y pendiente variables 5 73 5.458 96.1 0.075 Cada uno de los cinco modelos definidos en la Tabla 4.8 fue ajustado a este subconjunto del estudio de caso hospitalario. Las estadísticas resumen se encuentran en la Tabla 4.9. Para este conjunto de datos, hay \\(n = 79\\) observaciones y \\(c = 3\\) niveles del factor DRG. Para cada modelo, los grados de libertad del modelo son el número de parámetros del modelo menos uno. Los grados de libertad del error son el número de observaciones menos el número de parámetros del modelo. Usando variables binarias, cada uno de los modelos en la Tabla 4.8 puede escribirse en un formato de regresión. Como hemos visto en la Sección 4.2, cuando un modelo puede escribirse como un subconjunto de otro modelo más grande, tenemos procedimientos formales de prueba disponibles para decidir cuál modelo es más apropiado. Para ilustrar este procedimiento de prueba con nuestro ejemplo de DRG, a partir de la Tabla 4.9 y los gráficos asociados, parece claro que el factor DRG es importante. Además, una prueba \\(t\\), que no se presenta aquí, muestra que la covariable \\(x\\) es importante. Por lo tanto, comparemos el modelo completo \\(\\mathrm{E}~y_{ij} = \\beta_{0,j} + \\beta_{1,j}x\\) con el modelo reducido \\(\\mathrm{E}~y_{ij} = \\beta_{0,j} + \\beta_1x\\). En otras palabras, ¿hay una pendiente diferente para cada DRG? Usando la notación de la Sección 4.2, llamamos al intercepto y la pendiente variables el modelo completo. Bajo la hipótesis nula, \\(H_0: \\beta_{1,1} = \\beta_{1,2} = \\beta_{1,3}\\), obtenemos el modelo con intercepto variable y pendiente constante. Así, usando el cociente \\(F\\) en la ecuación (4.2), tenemos: \\[ \\small{ F\\text{-ratio} = \\frac{(Error~SS)_{reduced} - (Error~SS)_{full}}{ps_{full}^2} = \\frac{7.482 - 5.458}{2 \\times 0.075} = 13.535. } \\] El percentil 95 de la distribución \\(F\\) con \\(df_1 = p = 2\\) y \\(df_2 = (df)_{full} = 73\\) es aproximadamente 3.13. Por lo tanto, esta prueba nos lleva a rechazar la hipótesis nula y a declarar válida la alternativa, el modelo de regresión con intercepto variable y pendiente variable. Combinación de Dos Factores Hemos visto cómo combinar covariables, así como una covariable y un factor, tanto de manera aditiva como con interacciones. De la misma manera, supongamos que tenemos dos factores, como sexo (dos niveles: masculino/femenino) y edad (tres niveles: joven/mediana/anciana). Las variables binarias correspondientes serían \\(x_1\\) para indicar si la observación representa a una mujer, \\(x_2\\) para indicar si la observación representa a una persona joven, y \\(x_3\\) para indicar si la observación representa a una persona de mediana edad. Un modelo aditivo para estos dos factores puede usar la función de regresión \\[ \\mathrm{E }~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3. \\] Como hemos visto, este modelo es sencillo de interpretar. Por ejemplo, podemos interpretar \\(\\beta_1\\) como el efecto del sexo, manteniendo constante la edad. También podemos incorporar dos términos de interacción, \\(x_1 x_2\\) y \\(x_1 x_3\\). Usando las cinco variables explicativas, obtenemos la función de regresión \\[ \\mathrm{E }~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3. \\tag{4.12} \\] Aquí, las variables \\(x_1\\), \\(x_2\\), y \\(x_3\\) se conocen como los efectos principales. La Tabla 4.10 ayuda a interpretar esta ecuación. Específicamente, hay seis tipos de personas que podríamos encontrar: hombres y mujeres que son jóvenes, de mediana edad o ancianos. Tenemos seis parámetros en la ecuación (4.12). La Tabla 4.10 proporciona el enlace entre los parámetros y los tipos de personas. Al usar los términos de interacción, no imponemos ninguna especificación previa sobre los efectos aditivos de cada factor. De la Tabla 4.10, vemos que la interpretación de los coeficientes de regresión en la ecuación (4.12) no es directa. Sin embargo, usar el modelo aditivo con términos de interacción es equivalente a crear una nueva variable categórica con seis niveles, uno para cada tipo de persona. Si los términos de interacción son críticos en su estudio, puede ser conveniente crear un nuevo factor que incorpore los términos de interacción simplemente para facilitar la interpretación. Table 4.10: Función de Regresión para un Modelo de Dos Factores con Interacciones Sexo Edad \\(x_1\\) \\(x_2\\) \\(x_3\\) \\(x_4\\) \\(x_5\\) Función de Regresión Masculino Joven 0 1 0 0 0 \\(\\beta_0 + \\beta_2\\) Masculino Mediana 0 0 1 0 0 \\(\\beta_0 + \\beta_3\\) Masculino Anciana 0 0 0 0 0 \\(\\beta_0\\) Femenino Joven 1 1 0 1 0 \\(\\beta_0 + \\beta_1 + \\beta_2 + \\beta_4\\) Femenino Mediana 1 0 1 0 1 \\(\\beta_0 + \\beta_1 + \\beta_3 + \\beta_5\\) Femenino Anciana 1 0 0 0 0 \\(\\beta_0 + \\beta_1\\) Las extensiones a más de dos factores siguen de manera similar. Por ejemplo, supongamos que está examinando el comportamiento de empresas con sede en diez regiones geográficas, dos estructuras organizativas (con fines de lucro versus sin fines de lucro) con cuatro años de datos. Si decide tratar cada variable como un factor y desea modelar todos los términos de interacción, entonces esto es equivalente a un factor con \\(10 \\times 2 \\times 4 = 80\\) niveles. Los modelos con términos de interacción pueden tener un número considerable de parámetros y el analista debe ser prudente al especificar las interacciones a considerar. Modelo Lineal General El modelo lineal general extiende el modelo de regresión lineal de dos maneras. Primero, las variables explicativas pueden ser continuas, categóricas o una combinación. La única restricción es que entren linealmente de tal manera que la función de regresión resultante \\[ \\mathrm{E}~y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k \\tag{4.13} \\] sea una combinación lineal de coeficientes. Como hemos visto, podemos elevar al cuadrado las variables continuas o tomar otras transformaciones no lineales (como logaritmos) así como usar variables binarias para representar variables categóricas, por lo que esta “restricción”, como su nombre lo indica, permite una amplia clase de funciones generales para representar los datos. La segunda extensión es que las variables explicativas pueden ser combinaciones lineales unas de otras en el modelo lineal general. Debido a esto, en el caso del modelo lineal general, las estimaciones de los parámetros no tienen por qué ser únicas. Sin embargo, una característica importante del modelo lineal general es que los valores ajustados resultantes resultan ser únicos, utilizando el método de mínimos cuadrados. Por ejemplo, en la Sección 4.3 vimos que el modelo ANOVA de un factor podía expresarse como un modelo de regresión con \\(c\\) variables indicadoras. Sin embargo, si hubiéramos intentado estimar el modelo en la ecuación (4.10), el método de mínimos cuadrados no habría llegado a un conjunto único de estimaciones de los coeficientes de regresión. La razón es que, en la ecuación (4.10), cada variable explicativa puede expresarse como una combinación lineal de las otras. Por ejemplo, observe que \\(x_c = 1 - (x_1 + x_2 + \\ldots + x_{c-1})\\). El hecho de que las estimaciones de los parámetros no sean únicas es una desventaja, pero no una insuperable. La suposición de que las variables explicativas no sean combinaciones lineales unas de otras significa que podemos calcular estimaciones únicas de los coeficientes de regresión utilizando el método de mínimos cuadrados. En términos de matrices, porque las variables explicativas no son combinaciones lineales unas de otras, la matriz \\(\\mathbf{X}^{\\prime}\\mathbf{X}\\) no es invertible. Específicamente, supongamos que estamos considerando la función de regresión en la ecuación (4.13) y, utilizando el método de mínimos cuadrados, nuestras estimaciones de los coeficientes de regresión son \\(b_0^{o}, b_1^{o}, \\ldots, b_k^{o}\\). Este conjunto de estimaciones de los coeficientes de regresión minimiza nuestra suma de cuadrados de los errores, pero puede haber otros conjuntos de coeficientes que también minimicen la suma de cuadrados de los errores. Los valores ajustados se calculan como \\(\\hat{y}_i = b_0^{o} + b_1^{o} x_{i1} + \\ldots + b_k^{o} x_{ik}\\). Se puede demostrar que los valores ajustados resultantes son únicos, en el sentido de que cualquier conjunto de coeficientes que minimice la suma de cuadrados de los errores produce los mismos valores ajustados (ver Sección 4.7.3). Por lo tanto, para un conjunto de datos y un modelo lineal general especificado, los valores ajustados son únicos. Debido a que los residuos se calculan como las respuestas observadas menos los valores ajustados, tenemos que los residuos son únicos. Debido a que los residuos son únicos, tenemos que las sumas de cuadrados de los errores son únicas. Por lo tanto, parece razonable, y es cierto, que podemos usar la prueba general de hipótesis descrita en la Sección 4.2 para decidir si las colecciones de variables explicativas son importantes. En resumen, para los modelos lineales generales, las estimaciones de los parámetros pueden no ser únicas y, por lo tanto, no significativas. Una parte importante de los modelos de regresión es la interpretación de los coeficientes de regresión. Esta interpretación no está necesariamente disponible en el contexto del modelo lineal general. Sin embargo, para los modelos lineales generales, todavía podemos discutir la importancia de una variable individual o colección de variables a través de pruebas parciales F. Además, los valores ajustados, y el correspondiente ejercicio de predicción, funcionan en el contexto del modelo lineal general. La ventaja del contexto del modelo lineal general es que no necesitamos preocuparnos por el tipo de restricciones a imponer en los parámetros. Aunque no es el tema de este texto, esta ventaja es particularmente importante en los diseños experimentales complicados utilizados en las ciencias de la vida. El lector encontrará que las rutinas de estimación del modelo lineal general están ampliamente disponibles en los paquetes de software estadístico disponibles en el mercado hoy en día. 4.5 Lecturas Adicionales y Referencias Hay varios buenos libros sobre modelos lineales que se enfocan en variables categóricas y técnicas de análisis de variables. Hocking (2003) y Searle (1987) son buenos ejemplos. Referencias del Capítulo Hocking, Ronald R. (2003). Methods and Applications of Linear Models: Regression and the Analysis of Variance John Wiley and Sons, New York. Keeler, Emmett B., and John E. Rolph (1988). The demand for episodes of treatment in the Health Insurance Experiment. Journal of Health Economics 7: 337-367. Searle, Shayle R. (1987). Linear Models for Unbalanced Data. John Wiley &amp; Sons, New York. 4.6 Ejercicios 4.1. En este ejercicio, consideramos la relación entre dos estadísticas que resumen qué tan bien se ajusta un modelo de regresión, la razón \\(F\\) y \\(R^2\\), el coeficiente de determinación. (Aquí, la razón \\(F\\) es la estadística utilizada para probar la adecuación del modelo, no una estadística parcial \\(F\\)). Escribe \\(R^2\\) en términos de \\(Error ~SS\\) y \\(Regression ~SS\\). Escribe la razón \\(F\\) en términos de \\(Error ~SS\\), \\(Regression ~SS\\), \\(k\\), y \\(n\\). Establece la relación algebraica \\[ F\\text{-ratio} = \\frac{R^2}{1-R^2} \\frac{n-(k+1)}{k}. \\] Supongamos que \\(n = 40\\), \\(k = 5\\), y \\(R^2 = 0.20\\). Calcula la razón \\(F\\). Realiza la prueba habitual de adecuación del modelo para determinar si las cinco variables explicativas afectan significativamente de manera conjunta a la variable de respuesta. Supongamos que \\(n = 400\\) (no 40), \\(k = 5\\), y \\(R^2 = 0.20\\). Calcula la razón \\(F\\). Realiza la prueba habitual de adecuación del modelo para determinar si las cinco variables explicativas afectan significativamente de manera conjunta a la variable de respuesta. 4.2. Costos Hospitalarios. Este ejercicio considera los datos de gastos hospitalarios proporcionados por la Agencia de Investigación y Calidad de la Atención Médica de EE. UU. (AHRQ) y descritos en el Ejercicio 1.4. Produce un diagrama de dispersión, una correlación, y una regresión lineal de LNTOTCHG en función de AGE. ¿Es AGE un predictor significativo de LNTOTCHG? Te preocupa que los recién nacidos sigan un patrón diferente al de otras edades. Crea una variable binaria que indique si AGE es igual a cero o no. Ejecuta una regresión utilizando esta variable binaria y AGE como variables explicativas. ¿Es la variable binaria estadísticamente significativa? Ahora examina el efecto del género, usando la variable binaria FEMALE que es uno si el paciente es mujer y cero en caso contrario. Ejecuta una regresión utilizando AGE y FEMALE como variables explicativas. Realiza una segunda regresión que incluya estas dos variables con un término de interacción. Comenta si el efecto del género es importante en alguno de los modelos. Ahora considera el tipo de admisión, APRDRG, un acrónimo de “grupo de diagnóstico refinado para todos los pacientes”. Esta es una variable categórica explicativa que proporciona información sobre el tipo de admisión hospitalaria. Hay varios cientos de niveles de esta categoría. Por ejemplo, el nivel 640 representa la admisión de un recién nacido normal, con un peso neonatal mayor o igual a 2.5 kilogramos. Como otro ejemplo, el nivel 225 representa una admisión que resulta en una apendicectomía. d(i). Ejecuta un modelo ANOVA de un factor, utilizando APRDRG para predecir LNTOTCHG. Examina el \\(R^2\\) de este modelo y compáralo con el coeficiente de determinación del modelo de regresión lineal de LNTOTCHG en función de AGE. Basándote en esta comparación, ¿qué modelo crees que es preferido? d(ii). Para el modelo de un factor en la parte d(i), proporciona un intervalo de confianza del 95% para la media de LNTOTCHG para el nivel 225 correspondiente a una apendicectomía. Convierte tu respuesta final de dólares logarítmicos a dólares mediante exponenciación. d(iii). Ejecuta un modelo de regresión de APRDRG, FEMALE, y AGE en LNTOTCHG. Indica si AGE es un predictor estadísticamente significativo de LNTOTCHG. Indica si FEMALE es un predictor estadísticamente significativo de LNTOTCHG. 4.3. Utilización de Hogares de Ancianos. Este ejercicio considera los datos de hogares de ancianos proporcionados por el Departamento de Servicios de Salud y Familia de Wisconsin (DHFS) y descritos en los Ejercicios 1.2, 2.10, y 2.20. Además de las variables de tamaño, también tenemos información sobre varias variables binarias. La variable URBAN se utiliza para indicar la ubicación de la instalación. Es uno si la instalación se encuentra en un entorno urbano y cero en caso contrario. La variable MCERT indica si la instalación está certificada por Medicare. La mayoría, pero no todos, los hogares de ancianos están certificados para proporcionar atención financiada por Medicare. Hay tres estructuras organizativas para los hogares de ancianos: gobierno (estado, condados, municipios), empresas con fines de lucro y organizaciones exentas de impuestos. Periódicamente, las instalaciones pueden cambiar de propietario y, con menos frecuencia, de tipo de propiedad. Creamos dos variables binarias PRO y TAXEXEMPT para denotar empresas con fines de lucro y organizaciones exentas de impuestos, respectivamente. Algunos hogares de ancianos optan por no comprar cobertura de seguro privado para sus empleados. En cambio, estas instalaciones proporcionan directamente seguros y beneficios de pensión a sus empleados; esto se conoce como “auto-financiamiento del seguro”. Usamos la variable binaria SELFFUNDINS para denotarlo. Decides examinar la relación entre LOGTPY (\\(y\\)) y las variables explicativas. Usa los datos del año 2001 del informe de costos y realiza el siguiente análisis. Hay tres niveles de estructuras organizativas, pero solo utilizamos dos variables binarias (PRO y TAXEXEMPT). Explica por qué. Realiza un análisis de varianza de un factor usando TAXEXEMPT como el factor. Decide si exento de impuestos es o no un factor importante para determinar LOGTPY. Establece tu hipótesis nula, hipótesis alternativa y todos los componentes de la regla de toma de decisiones. Utiliza un nivel de significancia del 5%. Realiza un análisis de varianza de un factor usando MCERT como el factor. Decide si MCERT es o no un factor importante para determinar LOGTPY. c(i). Proporciona una estimación puntual de LOGTPY para una instalación de enfermería que no está certificada por Medicare. c(ii). Proporciona un intervalo de confianza del 95% para tu estimación puntual en la parte (i). Ejecuta un modelo de regresión usando las variables binarias, URBAN, PRO, TAXEXEMPT, SELFFUNDINS, y MCERT. Encuentra \\(R^2\\). ¿Qué variables son estadísticamente significativas? Ejecuta un modelo de regresión usando todas las variables explicativas, LOGNUMBED, LOGSQRFOOT, URBAN, PRO, TAXEXEMPT, SELFFUNDINS, y MCERT. Encuentra \\(R^2\\). ¿Qué variables son estadísticamente significativas? e(i). Calcula la correlación parcial entre LOGTPY y LOGSQRFOOT. Compárala con la correlación entre LOGTPY y LOGSQRFOOT. Explica por qué la correlación parcial es pequeña. e(ii). Compara el bajo nivel de los \\(t\\)-ratios (para probar la importancia de los coeficientes de regresión individuales) y el alto nivel de la razón \\(F\\) (para probar la adecuación del modelo). Describe la aparente inconsistencia y proporciona una explicación para esta inconsistencia. 4.4. Reclamaciones de Seguros de Automóviles. Consulta el Ejercicio 1.3. Ejecuta una regresión de LNPAID en AGE. ¿Es AGE una variable estadísticamente significativa? Para responder a esta pregunta, usa una prueba formal de hipótesis. Establece tu hipótesis nula y alternativa, el criterio de toma de decisiones y tu regla de toma de decisiones. También comenta sobre la bondad del ajuste de esta variable. Considera usar la clase como una sola variable explicativa. Usa el factor único para estimar el modelo y responde a las siguientes preguntas. b(i). ¿Cuál es la estimación puntual de las reclamaciones en la clase C7, conductores de 50-69 años, que conducen al trabajo o la escuela, menos de 30 millas por semana con un kilometraje anual inferior a 7500, en unidades logarítmicas naturales? b(ii). Determina el intervalo de confianza del 95% correspondiente de las reclamaciones esperadas, en unidades logarítmicas naturales. b(iii). Convierte el intervalo de confianza del 95% de las reclamaciones esperadas que determinaste en la parte b(ii) a dólares. Ejecuta una regresión de LNPAID en AGE, GENDER, y las variables categóricas STATE CODE y CLASS. c(i). ¿Es GENDER una variable estadísticamente significativa? Para responder a esta pregunta, usa una prueba formal de hipótesis. Establece tu hipótesis nula y alternativa, el criterio de toma de decisiones y tu regla de toma de decisiones. c(ii). ¿Es CLASS una variable estadísticamente significativa? Para responder a esta pregunta, usa una prueba formal de hipótesis. Establece tu hipótesis nula y alternativa, el criterio de toma de decisiones y tu regla de toma de decisiones. c(iii). Usa el modelo para proporcionar una estimación puntual de las reclamaciones en dólares (no en dólares logarítmicos) para un hombre de 60 años en el STATE 2 en la CLASS C7. c(iv). Escribe el coeficiente asociado con la CLASS C7 e interpreta este coeficiente. 4.5. Ventas de la Lotería de Wisconsin. Este ejercicio considera los datos de ventas de lotería del Estado de Wisconsin que se describieron en la Sección 2.1 y se examinaron en el Ejercicio 3.4. Parte 1: Decides examinar la relación entre SALES (\\(y\\)) y las ocho variables explicativas (PERPERHH, MEDSCHYR, MEDHVL, PRCRENT, PRC55P, HHMEDAGE, MEDINC, y POP). Ajusta un modelo de regresión de SALES en las ocho variables explicativas. Encuentra \\(R^2\\). b(i). Utilízalo para calcular el coeficiente de correlación entre los valores observados y los valores ajustados. b(ii). Quieres usar \\(R^2\\) para probar la adecuación del modelo en la parte (a). Usa una prueba formal de hipótesis. Establece tu hipótesis nula y alternativa, el criterio de toma de decisiones y tus reglas de toma de decisiones. Prueba si POP, MEDSCHYR, y MEDHVL son variables explicativas importantes de forma conjunta para comprender SALES. Parte 2: Después del análisis preliminar en la Parte 1, decides examinar la relación entre SALES (\\(y\\)) y POP, MEDSCHYR, y MEDHVL. Ajusta un modelo de regresión de SALES en estas tres variables explicativas. ¿Ha disminuido el coeficiente de determinación del modelo de regresión de ocho variables al modelo de tres variables? ¿Significa esto que el modelo no ha mejorado o proporciona poca información? Explica tu respuesta. Para determinar formalmente si se debe usar el modelo de tres o de ocho variables, utiliza una prueba parcial \\(F\\). Establece tu hipótesis nula y alternativa, el criterio de toma de decisiones y tus reglas de toma de decisiones. 4.6. Gastos de Compañías de Seguros. Este ejercicio considera los datos de compañías de seguros del NAIC y descritos en los Ejercicios 1.6 y 3.5. ¿Son importantes los términos cuadrados? Considera un modelo lineal de LNEXPENSES con doce variables explicativas. Para las variables explicativas, incluye ASSETS, GROUP, ambas versiones de pérdidas y primas brutas, así como las dos variables BLS. También incluye el cuadrado de cada una de las dos pérdidas y las dos primas brutas. Prueba si los cuatro términos cuadrados son conjuntamente estadísticamente significativos, usando una prueba parcial \\(F\\). Establece tus hipótesis nula y alternativa, el criterio de toma de decisiones y tus reglas de toma de decisiones. ¿Son importantes los términos de interacción con GROUP? Omite las dos variables BLS, de manera que ahora hay once variables: ASSETS, GROUP, ambas versiones de pérdidas y primas brutas, así como las interacciones de GROUP con ASSETS y ambas versiones de pérdidas y primas brutas. Prueba si los cinco términos de interacción son conjuntamente estadísticamente significativos, usando una prueba parcial \\(F\\). Establece tus hipótesis nula y alternativa, el criterio de toma de decisiones y tus reglas de toma de decisiones. Estás examinando una compañía que no está en la muestra con valores LONGLOSS = 0.025, SHORTLOSS = 0.040, GPWPERSONAL = 0.050, GPWCOMM = 0.120, ASSETS = 0.400, CASH = 0.350, y GROUP = 1. Usa el modelo de interacción de once variables de la parte (b) para producir un intervalo de predicción del 95% para esta compañía. 4.7. Expectativas de Vida Nacionales. Continuamos el análisis iniciado en los Ejercicios 1.7, 2.22, y 3.6. Considera la regresión utilizando tres variables explicativas, FERTILITY, PUBLICEDUCATION, y ln(HEALTH) que hiciste en el Ejercicio 3.6. Prueba si PUBLICEDUCATION y ln(HEALTH) son conjuntamente estadísticamente significativos, usando una prueba parcial \\(F\\). Establece tus hipótesis nula y alternativa, el criterio de toma de decisiones y tus reglas de toma de decisiones. (Sugerencia: Usa la forma del coeficiente de determinación para calcular el estadístico de la prueba.) Proporciona un valor aproximado de \\(p\\) para la prueba. Ahora introducimos la variable REGION, resumida en la Tabla 4.11. Un diagrama de caja de las expectativas de vida versus REGION se muestra en la Figura 4.8. Describe lo que aprendemos de la Tabla y el diagrama de caja sobre el efecto de REGION en LIFEEXP. Figure 4.8: Diagramas de Caja de LIFEEXP por REGION La Tabla 4.11 resume la esperanza de vida promedio por región. REGIÓN Descripción de la Región Número Media 1 Estados Árabes 13 71.9 2 Asia Oriental y Pacífico 17 69.1 3 América Latina y el Caribe 25 72.8 4 Asia Meridional 7 65.1 5 Europa Meridional 3 67.4 6 África Subsahariana 38 52.2 7 Europa Central y Oriental 24 71.6 8 OECD de Altos Ingresos 23 79.6 Todos 150 67.4 Ajusta un modelo de regresión utilizando solo el factor REGION. ¿Es REGION un determinante estadísticamente significativo de LIFEEXP? Establece tus hipótesis nula y alternativa, el criterio de toma de decisiones y tus reglas de toma de decisiones. Ajusta un modelo de regresión utilizando tres variables explicativas, FERTILITY, PUBLICEDUCATION, y ln(HEALTH), así como la variable categórica REGION. d(i). Estás examinando un país que no está en la muestra con valores FERTILITY = 2.0, PUBLICEDUCATION = 5.0, y ln(HEALTH) = 1.0. Produce dos valores predichos de esperanza de vida asumiendo que el país es de (1) un estado árabe y (2) África Subsahariana. d(ii). Proporciona un intervalo de confianza del 95% para la diferencia en expectativas de vida entre un estado árabe y un país de África Subsahariana. d(iii). Proporciona la estimación puntual (usualmente de mínimos cuadrados ordinarios) para la diferencia en expectativas de vida entre un país de África Subsahariana y un país de altos ingresos de la OECD (Organización para la Cooperación y el Desarrollo Económico). 4.7 Suplemento Técnico - Expresiones Matriciales 4.7.1 Expresión de Modelos con Variables Categóricas en Forma Matricial El Capítulo 3 mostró cómo escribir la ecuación del modelo de regresión en la forma \\(\\mathbf{y = X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\) donde \\(\\mathbf{X}\\) es una matriz de variables explicativas. Esta forma permite el cálculo directo de los coeficientes de regresión, \\(\\mathbf{b} = \\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}\\). Esta sección muestra cómo el modelo y los cálculos se reducen a expresiones más simples cuando las variables explicativas son categóricas. Modelo con una Variable Categórica. Considera el modelo con una variable categórica introducido en la Sección 4.3 con \\(c\\) niveles de la variable categórica. A partir de la ecuación (4.9), este modelo se puede escribir como \\[ \\mathbf{y} = \\begin{bmatrix} y_{1,1} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ y_{n_1,1} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ y_{1,c} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ y_{n_{c},c} \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ 1 &amp; 0 &amp; \\cdots &amp; \\cdots \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} \\mu_1 \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\mu_c \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_{1,1} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\varepsilon_{n_1,1} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\varepsilon_{1,c} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\varepsilon_{n_{c},c} \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\] Para hacer la notación más compacta, escribimos \\(\\mathbf{0}\\) y \\(\\mathbf{1}\\) para una columna de ceros y unos, respectivamente. Con esta convención, otra forma de expresarlo es \\[ \\mathbf{y} = \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdots &amp; \\mathbf{0}_1 \\\\ \\mathbf{0}_2 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0}_c &amp; \\mathbf{0}_c &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix} \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_c \\end{bmatrix} + \\boldsymbol{\\varepsilon} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\tag{4.14} \\] Aquí, \\(\\mathbf{0}_1\\) y \\(\\mathbf{1}_1\\) representan columnas de vectores de longitud \\(n_1\\) de ceros y unos, respectivamente, y de manera similar para \\(\\mathbf{0}_2, \\mathbf{1}_2, \\ldots, \\mathbf{0}_c, \\mathbf{1}_c\\). La ecuación (4.14) nos permite aplicar la maquinaria desarrollada para el modelo de regresión al modelo con una variable categórica. Como cálculo intermedio, tenemos \\[ \\begin{array}{ll} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} &amp;= \\left( \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_2 &amp; \\cdots &amp; \\mathbf{0}_c \\\\ \\mathbf{0}_1 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_c \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0}_1 &amp; \\mathbf{0}_2 &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix}^{\\prime} \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdots &amp; \\mathbf{0}_1 \\\\ \\mathbf{0}_2 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0}_c &amp; \\mathbf{0}_c &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix} \\right)^{-1}\\\\ &amp;= \\begin{bmatrix} n_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; n_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; n_c \\end{bmatrix}^{-1} = \\begin{bmatrix} \\frac{1}{n_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\frac{1}{n_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\frac{1}{n_c} \\end{bmatrix} . \\tag{4.15} \\end{array} \\] Así, las estimaciones de los parámetros son \\[ \\mathbf{b} = \\begin{bmatrix} \\hat{\\mu}_1 \\\\ \\vdots \\\\ \\hat{\\mu}_c \\end{bmatrix} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{X}^{\\prime} \\mathbf{y} = \\begin{bmatrix} \\frac{1}{n_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\frac{1}{n_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\frac{1}{n_c} \\end{bmatrix} \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_2 &amp; \\cdots &amp; \\mathbf{0}_c \\\\ \\mathbf{0}_1 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_c \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0}_1 &amp; \\mathbf{0}_2 &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix}^{\\prime} \\begin{bmatrix} y_{1,1} \\\\ \\vdots \\\\ y_{n_1,1} \\\\ \\vdots \\\\ y_{1,c} \\\\ \\vdots \\\\ y_{n_{c},c} \\end{bmatrix} \\] \\[ = \\begin{bmatrix} \\frac{1}{n_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\frac{1}{n_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\frac{1}{n_c} \\end{bmatrix} \\begin{bmatrix} \\sum_{i=1}^{n_1} y_{i1} \\\\ \\vdots \\\\ \\sum_{i=1}^{n_c} y_{ic} \\end{bmatrix} = \\begin{bmatrix} \\bar{y}_1 \\\\ \\vdots \\\\ \\bar{y}_c \\end{bmatrix} \\tag{4.16} \\] Hemos visto que la estimación por mínimos cuadrados de \\(\\mu_j\\), \\(\\bar{y}_j\\), se puede obtener directamente de la ecuación (4.9). Al reescribir el modelo en notación de regresión matricial, podemos recurrir a los resultados del modelo de regresión y no necesitamos probar las propiedades de los modelos con variables categóricas desde los principios básicos. Es decir, dado que este modelo está en formato de regresión, tenemos inmediatamente todas las propiedades del modelo de regresión. Indicar a tu software que una variable es categórica puede significar cálculos más eficientes, como en los cálculos de las estimaciones de regresión por mínimos cuadrados en la ecuación (4.16). Además, el cálculo de otras cantidades también se puede hacer de manera más directa. Como otro ejemplo, tenemos que el error estándar de \\(\\hat{\\mu}_j\\) es \\[ se(\\hat{\\mu}_j) = s ~ \\sqrt{j \\text{ th } \\textit{ diagonal element of } \\mathbf{(X}^{\\prime}\\mathbf{X)}^{-1}} = s/\\sqrt{n_j}. \\] Modelo con Una Variable Categórica y Una Continua Como otra ilustración, consideramos el modelo de intercepto variable y pendiente constante. Este modelo se resume como \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta + \\boldsymbol \\varepsilon\\) donde \\[ \\mathbf{X} = \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdots &amp; \\mathbf{0}_1 &amp; \\mathbf{x}_1 \\\\ \\mathbf{0}_2 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_2 &amp; \\mathbf{x}_2 \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ \\mathbf{0}_{c} &amp; \\mathbf{0}_{c} &amp; \\cdots &amp; \\mathbf{1}_c &amp; \\mathbf{x}_{c} \\end{bmatrix} \\text{ y } \\boldsymbol \\beta = \\begin{bmatrix} \\beta_{01} \\\\ \\beta_{02} \\\\ \\cdots \\\\ \\cdots \\\\ \\cdots \\\\ \\beta_{0c} \\\\ \\beta_1 \\end{bmatrix}. \\tag{4.17} \\] Aquí, \\(\\mathbf{0}_j\\) y \\(\\mathbf{1}_j\\) representan columnas de vectores de longitud \\(n_j\\) de ceros y unos, respectivamente, y \\(\\mathbf{x}_j = (x_{1j}, x_{2j}, \\ldots, x_{n_j, j})^{\\prime}\\) es la columna de la variable continua en el nivel \\(j\\). Técnicas de álgebra matricial sencillas proporcionan las estimaciones por mínimos cuadrados. 4.7.2 Cálculo Recursivo de Mínimos Cuadrados Cuando se calculan los coeficientes de regresión utilizando mínimos cuadrados, \\(\\mathbf{b} = \\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}\\), para algunas aplicaciones, la dimensión de \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\) puede ser grande, lo que causa dificultades computacionales. Afortunadamente, para algunos problemas, los cálculos se pueden dividir en problemas más pequeños que se pueden resolver de forma recursiva. Cálculo Recursivo de Mínimos Cuadrados. Supongamos que la función de regresión se puede escribir como \\[ \\mathrm{E}~\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta = \\left( \\mathbf{X}_1 : \\mathbf{X}_2 \\right) \\left( \\begin{array}{c} \\boldsymbol \\beta_1 \\\\ \\boldsymbol \\beta_2 \\\\ \\end{array} \\right), \\tag{4.18} \\] donde \\(\\mathbf{X}_1\\) tiene dimensiones \\(n \\times k_1\\), \\(\\mathbf{X}_2\\) tiene dimensiones \\(n \\times k_2\\), \\(k_1 + k_2 = k\\), \\(\\boldsymbol \\beta_1\\) tiene dimensiones \\(k_1 \\times 1\\), y \\(\\boldsymbol \\beta_2\\) tiene dimensiones \\(k_2 \\times 1\\). Definimos \\(\\mathbf{Q}_1 = \\mathbf{I} - \\mathbf{X}_1 \\left(\\mathbf{X}_1^{\\prime} \\mathbf{X}_1\\right)^{-1} \\mathbf{X}_1^{\\prime}\\). Entonces, el estimador por mínimos cuadrados se puede calcular como: \\[ \\mathbf{b} = \\left( \\begin{array}{c} \\mathbf{b}_1 \\\\ \\mathbf{b}_2 \\\\ \\end{array} \\right) = \\left( \\begin{array}{c} (\\mathbf{X}_1^{\\prime} \\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime} \\left( \\mathbf{y} - \\mathbf{X}_2 \\mathbf{b}_2 \\right) \\\\ \\left(\\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2\\right)^{-1} \\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{y} \\\\ \\end{array} \\right). \\tag{4.19} \\] La ecuación (4.19) proporciona el primer paso en la recursión. Puede ser iterada fácilmente para permitir una descomposición más detallada de \\(\\mathbf{X}\\). Caso Especial: Modelo con Una Variable Categórica y Una Continua. Para ilustrar la relevancia de la ecuación (4.19), consideremos el modelo resumido en la ecuación (4.17). Aquí, la dimensión de \\(\\mathbf{X}\\) es \\(n \\times (c+1)\\) y, por lo tanto, la dimensión de \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\) es \\((c+1) \\times (c+1)\\). Tomar la inversa de esta matriz podría ser difícil si \\(c\\) es grande. Para aplicar la ecuación (4.19), definimos \\[ \\mathbf{X}_1 = \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdots &amp; \\mathbf{0}_1 \\\\ \\mathbf{0}_2 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_2 \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ \\mathbf{0}_c &amp; \\mathbf{0}_c &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix} \\text{ y } \\mathbf{X}_2 = \\begin{bmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\cdots \\\\ \\cdots \\\\ \\cdots \\\\ \\mathbf{x}_c \\end{bmatrix}. \\] En este caso, hemos visto en la ecuación (4.15) cómo es sencillo calcular \\(\\left(\\mathbf{X}_1^{\\prime} \\mathbf{X}_1\\right)^{-1}\\) sin necesidad de invertir matrices. Esto hace que el cálculo de \\(\\mathbf{Q}_1\\) sea directo. Con esto, podemos calcular \\(\\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2\\) y, dado que es un escalar, obtener inmediatamente su inversa. Esto proporciona \\(\\mathbf{b}_2\\), que luego se usa para calcular \\(\\mathbf{b}_1\\). Aunque este procedimiento no es tan directo como \\(\\mathbf{b} = \\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}\\), puede ser computacionalmente eficiente. Resultados de Matrices Particionadas Para establecer la ecuación (4.19), utilizamos resultados estándar en álgebra matricial respecto a las inversas de matrices particionadas. Resultados de Matrices Particionadas. Supongamos que podemos particionar la matriz \\((p+q) \\times (p+q)\\) \\(\\mathbf{B}\\) como \\[ \\mathbf{B} = \\begin{bmatrix} \\mathbf{B}_{11} &amp; \\mathbf{B}_{12} \\\\ \\mathbf{B}_{12}^{\\prime} &amp; \\mathbf{B}_{22} \\end{bmatrix}, \\] donde \\(\\mathbf{B}_{11}\\) es una matriz \\(p \\times p\\) invertible, \\(\\mathbf{B}_{22}\\) es una matriz \\(q \\times q\\) invertible, y \\(\\mathbf{B}_{12}\\) es una matriz \\(p \\times q\\). Entonces \\[ \\mathbf{B}^{-1} = \\begin{bmatrix} \\mathbf{C}_{11}^{-1} &amp; - \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{C}_{22}^{-1} \\\\ - \\mathbf{C}_{22}^{-1} \\mathbf{B}_{12}^{\\prime} \\mathbf{B}_{11}^{-1} &amp; \\mathbf{C}_{22}^{-1} \\end{bmatrix}, \\tag{4.20} \\] donde \\(\\mathbf{C}_{11} = \\mathbf{B}_{11} - \\mathbf{B}_{12} \\mathbf{B}_{22}^{-1} \\mathbf{B}_{12}^{\\prime}\\) y \\(\\mathbf{C}_{22} = \\mathbf{B}_{22} - \\mathbf{B}_{12}^{\\prime} \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12}.\\) Para verificar la ecuación (4.20), multiplique \\(\\mathbf{B}^{-1}\\) por \\(\\mathbf{B}\\) para obtener la matriz identidad \\(\\mathbf{I}\\). Además, \\[ \\mathbf{C}_{11}^{-1} = \\mathbf{B}_{11}^{-1} + \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{C}_{22}^{-1} \\mathbf{B}_{12}^{\\prime} \\mathbf{B}_{11}^{-1}. \\tag{4.21} \\] Ahora, primero escribimos el estimador de mínimos cuadrados como \\[ \\begin{array}{ll} \\mathbf{b} &amp;= \\left( \\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y} = \\left( \\left( \\begin{array}{c} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{X}_2^{\\prime} \\\\ \\end{array} \\right) \\left( \\mathbf{X}_1 : \\mathbf{X}_2 \\right)\\right)^{-1} \\left( \\begin{array}{c} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{X}_2^{\\prime} \\\\ \\end{array} \\right) \\mathbf{y} \\\\ &amp; = \\left( \\begin{array}{cc} \\mathbf{X}_1^{\\prime} \\mathbf{X}_1 &amp; \\mathbf{X}_1^{\\prime} \\mathbf{X}_2 \\\\ \\mathbf{X}_2^{\\prime} \\mathbf{X}_1 &amp; \\mathbf{X}_2^{\\prime} \\mathbf{X}_2 \\\\ \\end{array} \\right)^{-1} \\left( \\begin{array}{c} \\mathbf{X}_1^{\\prime} \\mathbf{y} \\\\ \\mathbf{X}_2^{\\prime} \\mathbf{y} \\\\ \\end{array} \\right) = \\left( \\begin{array}{c} \\mathbf{b}_1 \\\\ \\mathbf{b}_2 \\\\ \\end{array} \\right). \\end{array} \\] Para aplicar los resultados de matrices particionadas, definimos \\[ \\mathbf{Q}_j = \\mathbf{I} - \\mathbf{X}_j \\left(\\mathbf{X}_j^{\\prime}\\mathbf{X}_j \\right)^{-1} \\mathbf{X}_j^{\\prime}, \\] donde \\(j=1,2\\), y \\(\\mathbf{B}_{j,k} = \\mathbf{X}_j^{\\prime}\\mathbf{X}_k\\) para \\(j,k=1,2.\\) Esto significa que \\(\\mathbf{C}_{11}=\\mathbf{X}_1^{\\prime}\\mathbf{X}_1 - \\mathbf{X}_1^{\\prime}\\mathbf{X}_2 (\\mathbf{X}_2^{\\prime}\\mathbf{X}_2)^{-1} \\mathbf{X}_2^{\\prime}\\mathbf{X}_1^{\\prime }\\) \\(= \\mathbf{X}_1^{\\prime} \\mathbf{Q}_2 \\mathbf{X}_1\\) y de manera similar \\(\\mathbf{C}_{22} = \\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2\\). A partir de la segunda fila, tenemos \\[ \\begin{array}{ll} \\mathbf{b}_2 &amp;= \\mathbf{C}_{22}^{-1} \\left( -\\mathbf{B}_{12}^{\\prime} \\mathbf{B}_{11}^{-1}\\mathbf{X}_1^{\\prime} \\mathbf{y} + \\mathbf{X}_2^{\\prime} \\mathbf{y} \\right) \\\\ &amp;= \\left(\\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2 \\right)^{-1} \\left( - \\mathbf{X}_2^{\\prime} \\mathbf{X}_1 (\\mathbf{X}_1^{\\prime}\\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y} + \\mathbf{X}_2^{\\prime}\\mathbf{y} \\right) \\\\ &amp;= \\left(\\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2 \\right)^{-1} \\mathbf{X}_2^{\\prime} \\mathbf{Q}_1\\mathbf{y}. \\end{array} \\] A partir de la primera fila, \\[ \\begin{array}{ll} \\mathbf{b}_1 &amp;= \\mathbf{C}_{11}^{-1} \\mathbf{X}_1^{\\prime}\\mathbf{y} - \\mathbf{B}_{11}^{-1}\\mathbf{B}_{12}\\mathbf{C}_{22}^{-1} \\mathbf{X}_2^{\\prime}\\mathbf{y} \\\\ &amp;= \\left( \\mathbf{B}_{11}^{-1} + \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{C}_{22}^{-1} \\mathbf{B}_{21} \\mathbf{B}_{11}^{-1} \\right) \\mathbf{X}_1^{\\prime}\\mathbf{y} - \\mathbf{B}_{11}^{-1}\\mathbf{B}_{12}\\mathbf{C}_{22}^{-1} \\mathbf{X}_2^{\\prime}\\mathbf{y} \\\\ &amp;= \\mathbf{B}_{11}^{-1}\\mathbf{X}_1^{\\prime}\\mathbf{y} - \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{C}_{22}^{-1} \\left( -\\mathbf{B}_{21} \\mathbf{B}_{11}^{-1} \\mathbf{X}_1^{\\prime}\\mathbf{y} + \\mathbf{X}_2^{\\prime}\\mathbf{y} \\right)\\\\ &amp;= \\mathbf{B}_{11}^{-1}\\mathbf{X}_1^{\\prime}\\mathbf{y} - \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{b}_2 \\\\ &amp;= (\\mathbf{X}_1^{\\prime}\\mathbf{X}_1)^{-1}\\mathbf{X}_1^{\\prime}\\mathbf{y} - (\\mathbf{X}_1^{\\prime}\\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime}\\mathbf{X}_2 \\mathbf{b}_2 \\\\ &amp;= (\\mathbf{X}_1^{\\prime}\\mathbf{X}_1)^{-1}\\mathbf{X}_1^{\\prime} \\left( \\mathbf{y} - \\mathbf{X}_2 \\mathbf{b}_2 \\right). \\end{array} \\] Esto establece la ecuación (4.19). Modelo Reparametrizado. Para la función de regresión particionada en la ecuación (4.18), definimos \\(\\mathbf{A}= \\left( \\mathbf{X}_1^{\\prime} \\mathbf{X}_1 \\right)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{X}_2\\) y \\(\\mathbf{E}_2 = \\mathbf{X}_2 - \\mathbf{X}_1 \\mathbf{A}\\). Si se realizara una regresión “multivariada” usando \\(\\mathbf{X}_2\\) como la respuesta y \\(\\mathbf{X}_1\\) como variables explicativas, entonces las estimaciones de parámetros serían \\(\\mathbf{A}\\) y los residuos \\(\\mathbf{E}_2\\). Con estas definiciones, use la ecuación (4.18) para definir el modelo de regresión reparametrizado \\[ \\begin{array}{ll} \\mathbf{y} &amp; = \\mathbf{X}_1 \\boldsymbol \\beta_1 + \\mathbf{X}_2 \\boldsymbol \\beta_2 + \\boldsymbol \\varepsilon = \\mathbf{X}_1 \\boldsymbol \\beta_1 + (\\mathbf{E}_2 + \\mathbf{X}_1 \\mathbf{A})\\boldsymbol \\beta_2 + \\boldsymbol \\varepsilon \\\\ &amp; = \\mathbf{X}_1 \\boldsymbol \\alpha_1 + \\mathbf{E}_2 \\boldsymbol \\beta_2 + \\boldsymbol \\varepsilon, \\end{array} \\tag{4.22} \\] donde \\(\\boldsymbol \\alpha_1 = \\boldsymbol \\beta_1 + \\mathbf{A}\\boldsymbol \\beta_2\\) es un nuevo vector de parámetros. La razón para introducir esta nueva parametrización es que ahora el vector de variables explicativas es ortogonal a las otras variables explicativas, es decir, el álgebra directa muestra que \\(\\mathbf{X}_1^{\\prime }\\mathbf{E}_2=\\mathbf{0}\\). Según la ecuación (4.19), el vector de estimaciones de mínimos cuadrados es \\[ \\begin{array}{ll} \\mathbf{a} = \\begin{bmatrix} \\mathbf{a}_1 \\\\ \\mathbf{b}_2 \\end{bmatrix} = \\left( \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{E}_2^{\\prime} \\end{bmatrix} \\begin{bmatrix} \\mathbf{X}_1 &amp; \\mathbf{E}_2 \\end{bmatrix} \\right) ^{-1} \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{E}_2^{\\prime} \\end{bmatrix} \\mathbf{y} = \\begin{bmatrix} \\left( \\mathbf{X}_1^{\\prime} \\mathbf{X}_1 \\right)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y} \\\\ \\left( \\mathbf{E}_2^{\\prime}\\mathbf{E}_2\\right) ^{-1} \\mathbf{E}_2^{\\prime}\\mathbf{y} \\end{bmatrix}. \\end{array} \\tag{4.23} \\] Suma Extra de Cuadrados. Supongamos que queremos considerar el aumento en la suma de cuadrados del error al pasar de un modelo reducido \\[ \\mathbf{y} = \\mathbf{X}_1 \\boldsymbol \\beta_1 + \\boldsymbol \\varepsilon \\] a un modelo completo \\[ \\mathbf{y} = \\mathbf{X}_1 \\boldsymbol \\beta_1 + \\mathbf{X}_2 \\boldsymbol \\beta_2 + \\boldsymbol \\varepsilon. \\] Para el modelo reducido, la suma de cuadrados del error es \\[ (Error ~ SS)_{reduced} = \\mathbf{y}^{\\prime} \\mathbf{y} - \\mathbf{y}^{\\prime} \\mathbf{X}_1 (\\mathbf{X}_1^{\\prime} \\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y}. \\tag{4.24} \\] Usando la versión reparametrizada del modelo completo, la suma de cuadrados del error es \\[ \\begin{array}{ll} (Error ~ SS)_{full} &amp;= \\mathbf{y}^{\\prime} \\mathbf{y} - \\mathbf{a}^{\\prime} \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{E}_2^{\\prime} \\end{bmatrix} \\mathbf{y} = \\mathbf{y}^{\\prime}\\mathbf{y} - \\begin{bmatrix} \\left( \\mathbf{X}_1^{\\prime} \\mathbf{X}_1 \\right)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y} \\\\ \\left( \\mathbf{E}_2^{\\prime}\\mathbf{E}_2\\right)^{-1} \\mathbf{E}_2^{\\prime} \\mathbf{y} \\end{bmatrix}^{\\prime} \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\mathbf{y} \\\\ \\mathbf{E}_2^{\\prime}\\mathbf{y} \\end{bmatrix} \\\\ &amp;= \\mathbf{y}^{\\prime} \\mathbf{y} - \\mathbf{y}^{\\prime} \\mathbf{X}_1 (\\mathbf{X}_1^{\\prime} \\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y} - \\mathbf{y}^{\\prime} \\mathbf{E}_2 (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1} \\mathbf{E}_2^{\\prime} \\mathbf{y}. \\tag{4.25} \\end{array} \\] Así, la reducción en la suma de cuadrados del error al agregar \\(\\mathbf{X}_2\\) al modelo es \\[ (Error ~SS)_{reduced} - (Error ~SS)_{full} = \\mathbf{y}^{\\prime} \\mathbf{E}_2 (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1} \\mathbf{E}_2^{\\prime} \\mathbf{y}. \\tag{4.26} \\] Como se mencionó en la Sección 4.3, la cantidad \\((Error ~SS)_{reduced} - (Error ~SS)_{full}\\) se llama suma extra de cuadrados, o suma de cuadrados Tipo III. Esto se produce automáticamente por algunos paquetes de software estadístico, evitando así la necesidad de ejecutar regresiones separadas. 4.7.3 Modelo Lineal General Recuerda el modelo lineal general de la Sección 4.4. Es decir, usamos \\[ y_i = \\beta_0 x_{i0} + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik} + \\varepsilon_i, \\] o, en notación de matrices, \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta + \\boldsymbol \\varepsilon\\). Como antes, utilizamos las Suposiciones F1-F4 (o E1-E4) para que los términos de perturbación sean i.i.d. con media cero y varianza común \\(\\sigma^2\\), y las variables explicativas \\(\\{x_{i0},x_{i1},x_{i2},\\ldots,x_{ik}\\}\\) sean no estocásticas. En el modelo lineal general, no requerimos que \\(\\mathbf{X}^{\\prime}\\mathbf{X}\\) sea invertible. Como hemos visto en el Capítulo 4, una razón importante para esta generalización se relaciona con el manejo de variables categóricas. Es decir, para usar variables categóricas, generalmente se recodifican utilizando variables binarias. Para esta recodificación, generalmente se deben hacer algunos tipos de restricciones en el conjunto de parámetros asociados con las variables indicadoras. Sin embargo, no siempre está claro qué tipo de restricciones son las más intuitivas. Al expresar el modelo sin requerir que \\(\\mathbf{X}^{\\prime}\\mathbf{X}\\) sea invertible, las restricciones pueden imponerse después de que se realice la estimación, no antes. Ecuaciones Normales. Incluso cuando \\(\\mathbf{X}^{\\prime}\\mathbf{X}\\) no es invertible, las soluciones a las ecuaciones normales aún proporcionan estimaciones de mínimos cuadrados de \\(\\boldsymbol \\beta\\). Es decir, la suma de cuadrados es \\[ SS(\\mathbf{b}^{\\ast}) = \\mathbf{(y - Xb}^{\\ast}\\mathbf{)}^{\\prime}\\mathbf{(y - Xb}^{\\ast}\\mathbf{)}, \\] donde \\(\\mathbf{b}^{\\ast} = (b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast})^{\\prime}\\) es un vector de estimaciones candidatas. Las soluciones de las ecuaciones normales son aquellos vectores \\(\\mathbf{b}^{\\circ }\\) que satisfacen las ecuaciones normales \\[ \\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\circ } = \\mathbf{X}^{\\prime}\\mathbf{y}. \\tag{4.27} \\] Usamos la notación \\(^{\\circ }\\) para recordarnos que \\(\\mathbf{b}^{\\circ }\\) no tiene que ser único. Sin embargo, es un minimizador de la suma de cuadrados. Para ver esto, considera otro vector candidato \\(\\mathbf{b}^{\\ast}\\) y nota que \\[ SS(\\mathbf{b}^{\\ast}) = \\mathbf{y}^{\\prime}\\mathbf{y} - 2\\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{y} + \\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\ast}. \\] Luego, usando la ecuación (4.27), tenemos \\[ SS(\\mathbf{b}^{\\ast}) - SS(\\mathbf{b}^{\\circ }) = -2\\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{y} + \\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\ast} - (-2\\mathbf{b}^{\\circ \\prime }\\mathbf{Xy} + \\mathbf{b}^{\\circ \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\circ }) \\] \\[ = -2\\mathbf{b}^{\\ast \\prime }\\mathbf{Xb}^{\\circ } + \\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\ast} + \\mathbf{b}^{\\circ \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\circ } \\] \\[ = \\mathbf{(b}^{\\ast} - \\mathbf{b}^{\\circ})^{\\prime} \\mathbf{X}^{\\prime} \\mathbf{X} (\\mathbf{b}^{\\ast} - \\mathbf{b}^{\\circ}) = \\mathbf{z}^{\\prime} \\mathbf{z} \\geq 0, \\] donde \\(\\mathbf{z} = \\mathbf{X} (\\mathbf{b}^{\\ast} - \\mathbf{b}^{\\circ})\\). Así, cualquier otro candidato \\(\\mathbf{b}^{\\ast}\\) produce una suma de cuadrados al menos tan grande como \\(SS(\\mathbf{b}^{\\circ})\\). Valores Ajustados Únicos A pesar de que puede haber (infinitas) soluciones a las ecuaciones normales, los valores ajustados resultantes, \\(\\mathbf{\\hat{y}} = \\mathbf{Xb}^{\\circ}\\), son únicos. Para ver esto, supongamos que \\(\\mathbf{b}_1^{\\circ}\\) y \\(\\mathbf{b}_2^{\\circ}\\) son dos soluciones diferentes de la ecuación (4.27). Sea \\(\\mathbf{\\hat{y}}_1 = \\mathbf{Xb}_1^{\\circ}\\) y \\(\\mathbf{\\hat{y}}_2 = \\mathbf{Xb}_2^{\\circ}\\) los vectores de valores ajustados generados por estas estimaciones. Entonces, \\[ \\mathbf{(\\hat{y}}_1 - \\mathbf{\\hat{y}}_2)^{\\prime} (\\mathbf{\\hat{y}}_1 - \\mathbf{\\hat{y}}_2) = \\mathbf{(b}_1^{\\circ} - \\mathbf{b}_2^{\\circ})^{\\prime} \\mathbf{X}^{\\prime} \\mathbf{X} (\\mathbf{b}_1^{\\circ} - \\mathbf{b}_2^{\\circ}) = 0 \\] porque \\(\\mathbf{X}^{\\prime} \\mathbf{X} (\\mathbf{b}_1^{\\circ} - \\mathbf{b}_2^{\\circ}) = \\mathbf{X}^{\\prime} \\mathbf{y} - \\mathbf{X}^{\\prime} \\mathbf{y} = \\mathbf{0}\\), de la ecuación (4.27). Por lo tanto, tenemos que \\(\\mathbf{\\hat{y}}_1 = \\mathbf{\\hat{y}}_2\\) para cualquier elección de \\(\\mathbf{b}_1^{\\circ}\\) y \\(\\mathbf{b}_2^{\\circ}\\), estableciendo así la unicidad de los valores ajustados. Debido a que los valores ajustados son únicos, los residuos también son únicos. Por lo tanto, la suma de cuadrados de los errores y las estimaciones de la variabilidad (como \\(s^2\\)) también son únicas. Inversas Generalizadas Una inversa generalizada de una matriz \\(\\mathbf{A}\\) es una matriz \\(\\mathbf{B}\\) tal que \\(\\mathbf{ABA = A}\\). Usamos la notación \\(\\mathbf{A}^{\\mathbf{-}}\\) para denotar la inversa generalizada de \\(\\mathbf{A}\\). En el caso de que \\(\\mathbf{A}\\) sea invertible, entonces \\(\\mathbf{A}^{\\mathbf{-}}\\) es única e igual a \\(\\mathbf{A}^{\\mathbf{-1}}\\). Aunque existen varias definiciones de inversas generalizadas, la definición anterior es suficiente para nuestros propósitos. Ver Searle (1987) para una discusión más profunda sobre definiciones alternativas de inversas generalizadas. Con esta definición, se puede mostrar que una solución a la ecuación \\(\\mathbf{Ab = c}\\) puede expresarse como \\(\\mathbf{b = A}^{-} \\mathbf{c}\\). Así, podemos expresar una estimación de mínimos cuadrados de \\(\\boldsymbol \\beta\\) como \\(\\mathbf{b}^{\\circ} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-} \\mathbf{X}^{\\prime} \\mathbf{y}\\). Los paquetes de software estadístico pueden calcular versiones de \\((\\mathbf{X}^{\\prime} \\mathbf{X})^{-}\\) y así generar \\(\\mathbf{b}^{\\circ}\\). Funciones Estimables Anteriormente, vimos que cada valor ajustado \\(\\hat{y}_i\\) es único. Dado que los valores ajustados son simplemente combinaciones lineales de estimaciones de parámetros, parece razonable preguntar qué otras combinaciones lineales de estimaciones de parámetros son únicas. Con este fin, decimos que \\(\\mathbf{C \\boldsymbol \\beta}\\) es una función estimable de los parámetros si \\(\\mathbf{Cb}^{\\circ}\\) no depende (es invariante) de la elección de \\(\\mathbf{b}^{\\circ}\\). Debido a que los valores ajustados son invariantes a la elección de \\(\\mathbf{b}^{\\circ}\\), tenemos que \\(\\mathbf{C = X}\\) produce un tipo de función estimable. Curiosamente, resulta que todas las funciones estimables son de la forma \\(\\mathbf{LXb}^{\\circ}\\), es decir, \\(\\mathbf{C} = \\mathbf{LX}\\). Ver Searle (1987, página 284) para una demostración de esto. Así, todas las funciones estimables son combinaciones lineales de valores ajustados, es decir, \\(\\mathbf{LXb}^{\\circ} = \\mathbf{L\\hat{y}}\\). Las funciones estimables son insesgadas y tienen una varianza que no depende de la elección de la inversa generalizada. Es decir, se puede mostrar que \\(\\text{E } \\mathbf{Cb}^{\\circ} = \\mathbf{C \\boldsymbol \\beta}\\) y \\(\\text{Var } \\mathbf{Cb}^{\\circ} = \\sigma^2 \\mathbf{C(X}^{\\prime} \\mathbf{X})^{-} \\mathbf{C}^{\\prime}\\) no depende de la elección de \\(\\mathbf{(X}^{\\prime} \\mathbf{X})^{-}\\). Hipótesis Comprobables Como se describe en la Sección 4.2, a menudo interesa probar \\(H_0\\): \\(\\mathbf{C \\boldsymbol \\beta} = \\mathbf{d}\\), donde \\(\\mathbf{d}\\) es un vector especificado. Esta hipótesis se dice que es comprobable si \\(\\mathbf{C \\boldsymbol \\beta}\\) es una función estimable, \\(\\mathbf{C}\\) es de rango completo en filas, y el rango de \\(\\mathbf{C}\\) es menor que el rango de \\(\\mathbf{X}\\). Para ser consistentes con la notación de la Sección 4.2, sea \\(p\\) el rango de \\(\\mathbf{C}\\) y \\(k + 1\\) el rango de \\(\\mathbf{X}\\). Recordemos que el rango de una matriz es el menor entre el número de filas linealmente independientes y el número de columnas linealmente independientes. Cuando decimos que \\(\\mathbf{C}\\) tiene rango completo en filas, nos referimos a que hay \\(p\\) filas en \\(\\mathbf{C}\\), de modo que el número de filas es igual al rango. Hipótesis Lineal General Como en la Sección 4.2, la estadística de prueba para examinar \\(H_0\\): \\(\\mathbf{C \\boldsymbol \\beta} = \\mathbf{d}\\) es \\[ F\\text{-ratio} = \\frac{\\mathbf{(Cb}^{\\circ} - \\mathbf{d})^{\\prime} \\mathbf{(C(X}^{\\prime} \\mathbf{X})^{-} \\mathbf{C}^{\\prime})^{-1} \\mathbf{(Cb}^{\\circ} - \\mathbf{d})}{ps_{full}^2}. \\] Note que la estadística \\(F\\)-ratio no depende de la elección de \\(\\mathbf{b}^{\\circ}\\) porque \\(\\mathbf{C b}^{\\circ}\\) es invariante a \\(\\mathbf{b}^{\\circ}\\). Si \\(H_0\\): \\(\\mathbf{C \\boldsymbol \\beta} = \\mathbf{d}\\) es una hipótesis comprobable y los errores \\(\\varepsilon_i\\) son i.i.d. \\(\\text{N}(0, \\sigma^2)\\), entonces el \\(F\\)-ratio tiene una distribución \\(F\\) con \\(df_1 = p\\) y \\(df_2 = n - (k + 1)\\). Modelo de Una Variable Categórica Ahora ilustramos el modelo lineal general considerando una versión sobre-parametr izada del modelo de un factor que aparece en la ecuación (4.10) usando \\[ y_{ij} = \\mu + \\tau_j + e_{ij} = \\mu + \\tau_1 x_{i1} + \\tau_2 x_{i2} + \\ldots + \\tau_c x_{ic} + \\varepsilon_{ij}. \\] En este punto, no imponemos restricciones adicionales en los parámetros. Como en la ecuación (4.13), esto se puede escribir en forma matricial como \\[ \\mathbf{y} = \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdots &amp; \\mathbf{0}_1 \\\\ \\mathbf{1}_2 &amp; \\mathbf{0}_2 &amp; \\mathbf{1}_{\\mathbf{2}} &amp; \\cdots &amp; \\mathbf{0}_2 \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\mathbf{1}_c &amp; \\mathbf{0}_c &amp; \\mathbf{0}_c &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix} \\begin{bmatrix} \\mu \\\\ \\tau_1 \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\tau_c \\end{bmatrix} + \\boldsymbol \\varepsilon = \\mathbf{X \\boldsymbol \\beta + \\boldsymbol \\varepsilon}. \\] Así, la matriz \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\) es \\[ \\mathbf{X}^{\\prime} \\mathbf{X} = \\begin{bmatrix} n &amp; n_1 &amp; n_2 &amp; \\cdots &amp; n_c \\\\ n_1 &amp; n_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ n_2 &amp; 0 &amp; n_2 &amp; \\cdots &amp; 0 \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ n_c &amp; 0 &amp; 0 &amp; \\cdots &amp; n_c \\end{bmatrix}. \\] donde \\(n = n_1 + n_2 + \\ldots + n_{c}\\). Esta matriz no es invertible. Para ver esto, note que al sumar las últimas \\(c\\) filas juntas se obtiene la primera fila. Por lo tanto, las últimas \\(c\\) filas son una combinación lineal exacta de la primera fila, lo que significa que la matriz no tiene rango completo. Las estimaciones (no únicas) de mínimos cuadrados se pueden expresar como \\[ \\mathbf{b}^{\\circ} = \\begin{bmatrix} \\mu^{\\circ} \\\\ \\tau_1^{\\circ} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\tau_c^{\\circ} \\end{bmatrix} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-} \\mathbf{X}^{\\prime} \\mathbf{y}. \\] Las funciones estimables son combinaciones lineales de los valores ajustados. Dado que los valores ajustados son \\(\\hat{y}_{ij} = \\bar{y}_j\\), las funciones estimables pueden expresarse como $L = _{j=1}^{c} a_j {y}_j $ donde \\(a_1, \\ldots, a_{c}\\) son constantes. Esta combinación lineal de los valores ajustados es un estimador insesgado de \\(\\text{E } L = \\sum_{i=1}^{c} a_i (\\mu + \\tau_i)\\). Por lo tanto, por ejemplo, eligiendo \\(a_1 = 1\\) y los otros \\(a_i = 0\\), vemos que \\(\\mu + \\tau_1\\) es estimable. Como otro ejemplo, eligiendo \\(a_1 = 1\\), \\(a_2 = -1\\), y los otros \\(a_i = 0\\), vemos que \\(\\tau_1 - \\tau_2\\) es estimable. Se puede demostrar que \\(\\mu\\) no es un parámetro estimable sin restricciones adicionales sobre \\(\\tau_1, \\ldots, \\tau_c\\). "],["C5VarSelect.html", "Chapter 5 Selección de Variables 5.1 Un Enfoque Iterativo para el Análisis de Datos y Modelado 5.2 Procedimientos Automáticos de Selección de Variables 5.3 Análisis de Residuales 5.4 Puntos Influyentes 5.5 Colinealidad 5.6 Criterios de Selección 5.7 Heterocedasticidad 5.8 Lectura Adicional y Referencias 5.9 Ejercicios 5.10 Suplementos Técnicos para el Capítulo 5", " Chapter 5 Selección de Variables Vista previa del capítulo. Este capítulo describe herramientas y técnicas para ayudar a seleccionar las variables a incluir en un modelo de regresión lineal, comenzando con un proceso iterativo de selección de modelos. En aplicaciones con muchas variables explicativas potenciales, los procedimientos automáticos de selección de variables ayudan a evaluar rápidamente muchos modelos. Sin embargo, los procedimientos automáticos tienen serias limitaciones, incluida la incapacidad de manejar adecuadamente las no linealidades como el impacto de puntos inusuales; este capítulo amplía la discusión del Capítulo 2 sobre puntos inusuales. También se describe la colinealidad, una característica común de los datos de regresión donde las variables explicativas están linealmente relacionadas entre sí. Otros temas que afectan la selección de variables, como la heterocedasticidad y la validación fuera de muestra, también se introducen. 5.1 Un Enfoque Iterativo para el Análisis de Datos y Modelado En nuestra introducción a la regresión lineal básica en el Capítulo 2, examinamos los datos gráficamente, formulamos una hipótesis sobre la estructura del modelo y comparamos los datos con un modelo candidato para formular un modelo mejorado. Box (1980) describe esto como un proceso iterativo, que se muestra en la Figura 5.1. Figure 5.1: El proceso iterativo de especificación del modelo Este proceso iterativo proporciona una receta útil para estructurar la tarea de especificar un modelo que represente un conjunto de datos. El primer paso, la etapa de formulación del modelo, se realiza examinando los datos gráficamente y utilizando el conocimiento previo de las relaciones, como de la teoría económica o de la práctica estándar de la industria. El segundo paso en la iteración se basa en los supuestos del modelo especificado. Estos supuestos deben ser consistentes con los datos para hacer un uso válido del modelo. El tercer paso, verificación diagnóstica, también se conoce como crítica de datos y modelo; los datos y el modelo deben ser consistentes entre sí antes de que se puedan hacer inferencias adicionales. La verificación diagnóstica es una parte importante de la formulación del modelo; puede revelar errores cometidos en pasos anteriores y proporcionar formas de corregir estos errores. 5.2 Procedimientos Automáticos de Selección de Variables Las relaciones en negocios y economía son complicadas; típicamente hay muchas variables que podrían servir como predictores útiles de la variable dependiente. Al buscar una relación adecuada, hay una gran cantidad de modelos potenciales que se basan en combinaciones lineales de variables explicativas y un número infinito de modelos que pueden formarse a partir de combinaciones no lineales. Para buscar entre los modelos basados en combinaciones lineales, existen varios procedimientos automáticos para seleccionar las variables que se incluirán en el modelo. Estos procedimientos automáticos son fáciles de usar y sugerirán uno o más modelos que se pueden explorar con mayor detalle. Para ilustrar cuán grande es el número potencial de modelos lineales, supongamos que solo hay cuatro variables, \\(x_{1}, x_2, x_3\\) y \\(x_4\\), bajo consideración para ajustar un modelo a \\(y\\). Sin considerar la multiplicación u otras combinaciones no lineales de las variables explicativas, ¿cuántos modelos posibles hay? La Tabla 5.1 muestra que la respuesta es 16. Table 5.1: Dieciséis Modelos Posibles Expression Combinations Models E \\(y=\\beta_0\\) 1 modelo sin variables independientes E \\(y=\\beta_0+\\beta_1x_i\\) \\(i\\) = 1,2,3,4 4 modelos con una variable independiente E \\(y = \\beta_0 + \\beta_1 x_i + \\beta_2 x_j\\) (\\(i,j\\)) = (1,2),(1,3),(1,4),(2,3),(2,4),(3,4) 6 modelos con dos variables independientes E \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_j +\\beta_3x_{k}\\) (\\(i,j,k\\)) = (1,2,3),(1,2,4),(1,3,4),(2,3,4) 4 modelos con tres variables independientes E \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +\\beta_3 x_3 + \\beta_4 x_4\\) 1 modelo con todas las variables independientes Si solo hubiera tres variables explicativas, entonces se puede usar la misma lógica para verificar que hay ocho modelos posibles. Extrapolando a partir de estos dos ejemplos, ¿cuántos modelos lineales habrá si hay diez variables explicativas? La respuesta es 1,024, lo cual es bastante. En general, la respuesta es \\(2^k\\), donde \\(k\\) es el número de variables explicativas. Por ejemplo, \\(2^3\\) es 8, \\(2^4\\) es 16, y así sucesivamente. En cualquier caso, para un número moderadamente grande de variables explicativas, hay muchos modelos potenciales que se basan en combinaciones lineales de variables explicativas. Nos gustaría tener un procedimiento para buscar rápidamente entre estos modelos potenciales y darnos más tiempo para pensar en otros aspectos interesantes de la selección de modelos. La regresión por pasos son procedimientos que emplean pruebas \\(t\\) para verificar la “significancia” de las variables explicativas que se incluyen o eliminan del modelo. Para comenzar, en la versión de selección hacia adelante de la regresión por pasos, las variables se agregan una a la vez. En la primera etapa, de todas las variables candidatas, se agrega al modelo la que es más estadísticamente significativa. En la siguiente etapa, con la variable de la primera etapa ya incluida, se agrega la siguiente variable más estadísticamente significativa. Este procedimiento se repite hasta que se hayan agregado todas las variables estadísticamente significativas. Aquí, la significancia estadística generalmente se evalúa utilizando el cociente \\(t\\) de una variable; el umbral para la significancia estadística es típicamente un valor \\(t\\) predefinido (como dos, que corresponde a un nivel de significancia aproximado del 95%). La versión de selección hacia atrás funciona de manera similar, excepto que todas las variables se incluyen en la etapa inicial y luego se eliminan una a la vez (en lugar de agregarse). Más generalmente, un algoritmo que agrega y elimina variables en cada etapa a veces se conoce como el algoritmo de regresión por pasos. Algoritmo de Regresión por Pasos. Suponga que el analista ha identificado una variable como la respuesta, \\(y\\), y \\(k\\) variables explicativas potenciales, \\(x_1, x_2, \\ldots, x_k\\). Considere todas las regresiones posibles usando una variable explicativa. Para cada una de las \\(k\\) regresiones, calcule \\(t(b_1)\\), el cociente \\(t\\) para la pendiente. Elija la variable con el cociente \\(t\\) más grande. Si el cociente \\(t\\) no supera un valor \\(t\\) predefinido (como dos), entonces no elija ninguna variable y detenga el procedimiento. Agregue una variable al modelo del paso anterior. La variable a ingresar es la que hace la contribución más significativa. Para determinar el tamaño de la contribución, use el valor absoluto del cociente \\(t\\) de la variable. Para ingresar, el cociente \\(t\\) debe superar un valor \\(t\\) especificado en valor absoluto. Elimine una variable del modelo del paso anterior. La variable a eliminar es la que hace la menor contribución. Para determinar el tamaño de la contribución, use el valor absoluto del cociente \\(t\\) de la variable. Para ser eliminada, el cociente \\(t\\) debe ser menor que un valor \\(t\\) especificado en valor absoluto. Repita los pasos (ii) y (iii) hasta que se realicen todas las posibles adiciones y eliminaciones. Al implementar esta rutina, algunos paquetes de software estadístico usan una prueba \\(F\\) en lugar de pruebas \\(t\\). Recuerde que, cuando solo se considera una variable, \\((t\\text{-cociente})^2 = F\\)-cociente, y por lo tanto, estos procedimientos son equivalentes. Este algoritmo es útil porque busca rápidamente entre varios modelos candidatos. Sin embargo, presenta varias desventajas: El procedimiento “husmea” entre un gran número de modelos y puede ajustar los datos “demasiado bien.” No hay garantía de que el modelo seleccionado sea el mejor. El algoritmo no considera modelos que se basan en combinaciones no lineales de variables explicativas. También ignora la presencia de valores atípicos y puntos de alta influencia. Además, el algoritmo no busca todos los \\(2^{k}\\) regresiones lineales posibles. El algoritmo utiliza un criterio, un cociente \\(t\\), y no considera otros criterios como \\(s\\), \\(R^2\\), \\(R_a^2\\), y así sucesivamente. Hay una secuencia de pruebas de significancia involucradas. Por lo tanto, el nivel de significancia que determina el valor \\(t\\) no es significativo. Al considerar cada variable por separado, el algoritmo no toma en cuenta el efecto conjunto de las variables explicativas. Los procedimientos puramente automáticos pueden no tener en cuenta el conocimiento especial de un investigador. Muchas de las críticas al algoritmo básico de regresión paso a paso pueden abordarse con software de computación moderno que ahora está ampliamente disponible. Ahora consideraremos cada inconveniente, en orden inverso. Para responder a la desventaja número (7), muchas rutinas de software estadístico tienen opciones para forzar la inclusión de variables en una ecuación de modelo. De esta manera, si otras evidencias indican que una o más variables deben incluirse en el modelo, el investigador puede forzar la inclusión de estas variables. Para la desventaja número (6), en la Sección 5.5.4 sobre variables supresoras, proporcionaremos ejemplos de variables que no tienen efectos individuales importantes pero son importantes cuando se consideran en conjunto. Estas combinaciones de variables pueden no ser detectadas con el algoritmo básico, pero serán detectadas con el algoritmo de selección hacia atrás. Dado que el procedimiento de selección hacia atrás comienza con todas las variables, detectará y conservará las variables que son importantes en conjunto. La desventaja número (5) es realmente una sugerencia sobre la forma de utilizar la regresión paso a paso. Bendel y Afifi (1977) sugirieron usar un valor de corte más pequeño del que normalmente se usaría. Por ejemplo, en lugar de usar un \\(t\\)-valor = 2 que corresponde aproximadamente a un nivel de significancia del 5%, considere usar un \\(t\\)-valor = 1.645 que corresponde aproximadamente a un nivel de significancia del 10%. De esta manera, hay menos posibilidad de excluir variables que pueden ser importantes. Un límite inferior, pero aún una buena opción para trabajo exploratorio, es un corte tan pequeño como \\(t\\)-valor = 1. Esta elección está motivada por un resultado algebraico: cuando una variable entra en un modelo, \\(s\\) disminuirá si el \\(t\\)-ratio excede uno en valor absoluto. Para abordar las desventajas número (3) y (4), ahora introducimos la rutina de mejores regresiones. Las mejores regresiones es un algoritmo útil que ahora está ampliamente disponible en paquetes de software estadístico. El algoritmo de mejor regresión busca en todas las combinaciones posibles de variables explicativas, a diferencia de la regresión paso a paso, que agrega y elimina una variable a la vez. Por ejemplo, suponga que hay cuatro posibles variables explicativas, \\(x_1\\), \\(x_2\\), \\(x_3\\) y \\(x_4\\), y el usuario desea saber cuál es el mejor modelo de dos variables. El algoritmo de mejor regresión busca entre los seis modelos de la forma \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x_i + \\beta_2 x_j\\). Típicamente, una rutina de mejor regresión recomienda uno o dos modelos para cada modelo con coeficiente \\(p\\), donde p es un número especificado por el usuario. Debido a que se ha especificado el número de coeficientes que entrarán en el modelo, no importa qué criterio usemos: \\(R^2\\), \\(R_a^2\\) o \\(s\\). El algoritmo de mejor regresión realiza su búsqueda mediante un uso ingenioso del hecho algebraico de que, cuando se añade una variable al modelo, la suma de cuadrados del error no aumenta. Debido a este hecho, ciertas combinaciones de variables incluidas en el modelo no necesitan ser calculadas. Un inconveniente importante de este algoritmo es que puede tomar mucho tiempo cuando el número de variables consideradas es grande. Los usuarios de la regresión no siempre aprecian la profundidad del inconveniente número (1), data-snooping (exploración de datos). La exploración de datos ocurre cuando el analista ajusta un gran número de modelos a un conjunto de datos. Abordaremos el problema de la exploración de datos en la Sección 5.6.2 sobre validación de modelos. Aquí, ilustraremos el efecto de la exploración de datos en la regresión paso a paso. Ejemplo: Exploración de Datos en Regresión Paso a Paso. La idea de esta ilustración es de Rencher y Pun (1980). Considere \\(n = 100\\) observaciones de \\(y\\) y cincuenta variables explicativas, \\(x_1, x_2, \\ldots, x_{50}\\). Los datos que consideramos aquí se simularon usando variables aleatorias normales estándar independientes. Debido a que las variables se simularon de manera independiente, estamos trabajando bajo la hipótesis nula de que no hay relación entre la respuesta y las variables explicativas, es decir, \\(H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_{50} = 0\\). De hecho, cuando se ajustó el modelo con las cincuenta variables explicativas, resultó que \\(s = 1.142\\), \\(R^2 = 46.2\\%\\), y el \\(F\\)-ratio = \\(\\frac{Regression~MS}{Error~MS} = 0.84\\). Usando una distribución \\(F\\) con \\(df_1 = 50\\) y \\(df_2 = 49\\), el percentil 95 es 1.604. De hecho, 0.84 es el percentil 27 de esta distribución, lo que indica que el valor \\(p\\) es 0.73. Por lo tanto, como era de esperar, los datos están en congruencia con \\(H_0\\). A continuación, se realizó una regresión paso a paso con \\(t\\)-valor = 2. Dos variables fueron retenidas por este procedimiento, lo que resultó en un modelo con \\(s = 1.05\\), \\(R^2 = 9.5\\%\\) y \\(F\\)-ratio = 5.09. Para una distribución \\(F\\) con \\(df_1 = 2\\) y \\(df_2 = 97\\), el percentil 95 es un \\(F\\)-valor = 3.09. Esto indica que las dos variables son predictores estadísticamente significativos de \\(y\\). A primera vista, este resultado es sorprendente. Los datos se generaron de manera que \\(y\\) no estuviera relacionado con las variables explicativas. Sin embargo, debido a que \\(F\\)-ratio \\(&gt;\\) \\(F\\)-valor, la prueba \\(F\\) indica que dos variables explicativas están significativamente relacionadas con \\(y\\). La razón es que la regresión paso a paso ha realizado muchas pruebas de hipótesis en los datos. Por ejemplo, en el Paso 1, se realizaron cincuenta pruebas para encontrar variables significativas. Recuerde que un nivel del 5% significa que esperamos cometer aproximadamente un error en 20. Por lo tanto, con cincuenta pruebas, esperamos encontrar \\(50 \\times 0.05 = 2.5\\) variables “significativas”, incluso bajo la hipótesis nula de que no hay relación entre \\(y\\) y las variables explicativas. Para continuar, se realizó una regresión paso a paso con \\(t\\)-valor = 1.645. Seis variables fueron retenidas por este procedimiento, lo que resultó en un modelo con \\(s = 0.99\\), \\(R^2 = 22.9\\%\\) y \\(F\\)-ratio = 4.61. Como antes, una prueba \\(F\\) indica una relación significativa entre la respuesta y estas seis variables explicativas. Para resumir, utilizando simulación, construimos un conjunto de datos de manera que las variables explicativas no tuvieran relación con la respuesta. Sin embargo, al utilizar la regresión paso a paso para examinar los datos, “encontramos” relaciones aparentemente significativas entre la respuesta y ciertos subconjuntos de las variables explicativas. Este ejemplo ilustra una advertencia general en la selección de modelos: cuando las variables explicativas se seleccionan utilizando los datos, los \\(t\\)-ratios y los \\(F\\)-ratios serán demasiado grandes, exagerando así la importancia de las variables en el modelo. La regresión paso a paso y las mejores regresiones son ejemplos de procedimientos automáticos de selección de variables. En su trabajo de modelado, encontrará que estos procedimientos son útiles porque pueden buscar rápidamente entre varios modelos candidatos. Sin embargo, estos procedimientos ignoran alternativas no lineales, así como el efecto de los valores atípicos y los puntos de alta influencia. El objetivo principal de estos procedimientos es mecanizar ciertas tareas rutinarias. Este enfoque de selección automática se puede extender, y de hecho, hay varios “sistemas expertos” disponibles en el mercado. Por ejemplo, hay algoritmos disponibles que manejan “automáticamente” puntos inusuales como valores atípicos y puntos de alta influencia. Un modelo sugerido por los procedimientos automáticos de selección de variables debe estar sujeto a los mismos procedimientos cuidadosos de verificación diagnóstica que un modelo obtenido por cualquier otro medio. 5.3 Análisis de Residuales Recuerde el papel de un residual en el modelo de regresión lineal introducido en la Sección 2.6. Un residual es una respuesta menos el valor ajustado correspondiente bajo el modelo. Dado que el modelo resume el efecto lineal de varias variables explicativas, podemos pensar en un residual como una respuesta controlada por los valores de las variables explicativas. Si el modelo es una representación adecuada de los datos, entonces los residuales deberían aproximarse a errores aleatorios. Los errores aleatorios se utilizan para representar la variación natural en el modelo; representan el resultado de un mecanismo impredecible. Por lo tanto, en la medida en que los residuales se parezcan a errores aleatorios, no debería haber patrones discernibles en los residuales. Los patrones en los residuales indican la presencia de información adicional que esperamos incorporar en el modelo. La ausencia de patrones en los residuales indica que el modelo parece explicar las relaciones principales en los datos. 5.3.1 Residuales Hay al menos cuatro tipos de patrones que pueden descubrirse a través del análisis de residuales. En esta sección, discutimos los dos primeros: residuales que son inusuales y aquellos que están relacionados con otras variables explicativas. Luego introducimos el tercer tipo, residuales que muestran un patrón heterocedástico, en la Sección 5.7. En nuestro estudio de datos de series temporales que comienza en el Capítulo 7, introduciremos el cuarto tipo, residuales que muestran patrones a lo largo del tiempo. Al examinar los residuales, generalmente es más fácil trabajar con un residual estandarizado, un residual que ha sido reescalado para no tener dimensiones. Generalmente trabajamos con residuales estandarizados porque así logramos transferir cierta experiencia de un conjunto de datos a otro y podemos enfocarnos en relaciones de interés. Al usar residuales estandarizados, podemos entrenarnos para observar una variedad de gráficos de residuales y reconocer inmediatamente un punto inusual al trabajar en unidades estándar. Hay varias formas de definir un residual estandarizado. Usando \\(e_i = y_i - \\hat{y}_i\\) como el \\(i\\)-ésimo residual, aquí hay tres definiciones comúnmente usadas: \\[\\begin{equation} \\text{(a) }\\frac{e_i}{s}, \\quad \\text{(b) }\\frac{e_i}{s\\sqrt{1 - h_{ii}}}, \\quad \\text{(c) }\\frac{e_i}{s_{(i)}\\sqrt{1 - h_{ii}}}. \\tag{5.1} \\end{equation}\\] Aquí, \\(h_{ii}\\) es la influencia del \\(i\\)-ésimo punto. Se calcula en función de los valores de las variables explicativas y se definirá en la Sección 5.4.1. Recuerde que \\(s\\) es la desviación estándar de los residuales (definida en la ecuación 3.8). De manera similar, definimos \\(s_{(i)}\\) como la desviación estándar de los residuales al ejecutar una regresión después de eliminar la \\(i\\)-ésima observación. Ahora, la primera definición en (a) es simple y fácil de explicar. Un cálculo simple muestra que la desviación estándar de la muestra de los residuales es aproximadamente \\(s\\) (una razón por la que \\(s\\) a menudo se denomina desviación estándar de los residuales). Por lo tanto, parece razonable estandarizar los residuales dividiendo por \\(s\\). La segunda opción presentada en (b), aunque más compleja, es más precisa. La varianza del \\(i\\)-ésimo residual es \\[ \\text{Var}(e_i) = \\sigma^2(1 - h_{ii}). \\] Este resultado se establecerá en la ecuación (5.15) de la Sección 5.10. Tenga en cuenta que esta varianza es menor que la varianza del término de error, Var\\((\\varepsilon_i) = \\sigma^2\\). Ahora, podemos reemplazar \\(\\sigma\\) por su estimación, \\(s\\). Entonces, este resultado lleva a usar la cantidad \\(s\\sqrt{1 - h_{ii}}\\) como una desviación estándar estimada, o error estándar, para \\(e_i\\). Por lo tanto, definimos el error estándar de \\(e_i\\) como \\[ \\text{se}(e_i) = s \\sqrt{1 - h_{ii}}. \\] Siguiendo las convenciones introducidas en la Sección 2.6, en este texto usamos \\(e_i / \\text{se}(e_i)\\) como nuestro residual estandarizado. La tercera opción presentada en (c) es una modificación de (b) y se conoce como un residual studentizado. Como se enfatiza en la Sección 5.3.2, un uso importante de los residuales es identificar respuestas inusualmente grandes. Ahora, supongamos que la \\(i\\)-ésima respuesta es inusualmente grande y que esto se mide a través de su residual. Este residual inusualmente grande también hará que el valor de \\(s\\) sea grande. Debido a que el efecto grande aparece tanto en el numerador como en el denominador, el residual estandarizado puede no detectar esta respuesta inusual. Sin embargo, esta respuesta grande no inflará \\(s_{(i)}\\) porque se construye después de eliminar la \\(i\\)-ésima observación. Por lo tanto, al usar residuales studentizados, obtenemos una mejor medida de las observaciones que tienen residuales inusualmente grandes. Al omitir esta observación de la estimación de \\(\\sigma\\), el tamaño de la observación solo afecta al numerador \\(e_i\\) y no al denominador \\(s_{(i)}\\). Como otra ventaja, los residuales studentizados siguen una distribución \\(t\\) con \\(n - (k + 1)\\) grados de libertad, asumiendo que los errores están distribuidos normalmente (suposición E5). Este conocimiento de la distribución precisa nos ayuda a evaluar el grado de ajuste del modelo y es particularmente útil en muestras pequeñas. Es esta relación con la distribución \\(t\\) de “Student” la que sugiere el nombre de “residuales studentizados”. 5.3.2 Uso de los Residuales para Identificar Valores Atípicos Una función importante del análisis de residuales es identificar valores atípicos. Un valor atípico es una observación que no se ajusta bien al modelo; son observaciones donde el residual es inusualmente grande. Una regla general utilizada por muchos paquetes estadísticos es que una observación se marca como un valor atípico si el residual estandarizado excede dos en valor absoluto. En la medida en que la distribución de los residuales estandarizados imite la curva normal estándar, esperamos que solo una de cada 20 observaciones, o el 95%, exceda dos en valor absoluto y muy pocas observaciones excedan tres. Los valores atípicos proporcionan una señal de que una observación debe investigarse para entender las causas especiales asociadas con este punto. Un valor atípico es una observación que parece inusual con respecto al resto del conjunto de datos. A menudo sucede que la razón de este comportamiento atípico puede descubrirse después de una investigación adicional. De hecho, este puede ser el propósito principal del análisis de regresión de un conjunto de datos. Consideremos un ejemplo simple de lo que se llama análisis de desempeño. Supongamos que tenemos disponible una muestra de \\(n\\) vendedores y estamos tratando de entender las ventas de cada persona en el segundo año en función de sus ventas en el primer año. Hasta cierto punto, esperamos que las ventas más altas en el primer año estén asociadas con ventas más altas en el segundo año. Las altas ventas pueden deberse a la habilidad natural del vendedor, ambición, buen territorio, etc. Las ventas del primer año pueden considerarse como una variable proxy que resume estos factores. Esperamos variación en el desempeño de ventas tanto de manera transversal como a lo largo de los años. Es interesante cuando un vendedor tiene un desempeño inusualmente bueno (o malo) en el segundo año en comparación con su desempeño en el primer año. Los residuales proporcionan un mecanismo formal para evaluar las ventas del segundo año después de controlar los efectos de las ventas del primer año. Hay varias opciones disponibles para manejar valores atípicos. Opciones para Manejar Valores Atípicos Incluir la observación en las estadísticas resumen habituales pero comentar sobre sus efectos. Un valor atípico puede ser grande pero no tan grande como para sesgar los resultados de todo el análisis. Si no se pueden determinar causas especiales para esta observación inusual, entonces esta observación puede simplemente reflejar la variabilidad en los datos. Eliminar la observación del conjunto de datos. Puede determinarse que la observación no es representativa de la población de la cual se extrae la muestra. Si este es el caso, entonces puede haber poca información contenida en la observación que pueda usarse para hacer afirmaciones generales sobre la población. Esta opción implica que omitiríamos la observación de las estadísticas resumen de la regresión y la discutiríamos en nuestro informe como un caso separado. Crear una variable binaria para indicar la presencia de un valor atípico. Si se han identificado una o varias causas especiales para explicar un valor atípico, entonces estas causas podrían introducirse formalmente en el procedimiento de modelado mediante la introducción de una variable que indique la presencia (o ausencia) de estas causas. Este enfoque es similar a la eliminación de puntos, pero permite que el valor atípico se incluya formalmente en la formulación del modelo, de modo que, si surgen observaciones adicionales afectadas por las mismas causas, se puedan manejar de forma automática. 5.3.3 Uso de los Residuales para Seleccionar Variables Explicativas Otra función importante del análisis de residuales es ayudar a identificar variables explicativas adicionales que puedan usarse para mejorar la formulación del modelo. Si hemos especificado el modelo correctamente, entonces los residuales deberían parecerse a errores aleatorios y no contener patrones discernibles. Por lo tanto, al comparar residuales con variables explicativas, no esperamos ninguna relación. Si detectamos una relación, esto sugiere la necesidad de controlar esta variable adicional. Esto se puede lograr introduciendo la variable adicional en el modelo de regresión. Las relaciones entre los residuales y las variables explicativas pueden establecerse rápidamente utilizando estadísticas de correlación. Sin embargo, si una variable explicativa ya está incluida en el modelo de regresión, entonces la correlación entre los residuales y una variable explicativa será cero (ver Sección 5.10.1 para la demostración algebraica). Es una buena idea reforzar esta correlación con un diagrama de dispersión. Un gráfico de residuales frente a variables explicativas no solo reforzará gráficamente la estadística de correlación, sino que también servirá para detectar posibles relaciones no lineales. Por ejemplo, una relación cuadrática puede detectarse utilizando un diagrama de dispersión, no una estadística de correlación. Si detecta una relación entre los residuales de un ajuste de modelo preliminar y una variable explicativa adicional, introducir esta variable adicional no siempre mejorará la especificación de su modelo. La razón es que la variable adicional puede estar relacionada linealmente con las variables que ya están en el modelo. Si desea una garantía de que agregar una variable adicional mejorará su modelo, entonces construya un gráfico de variables añadidas (ver Sección 3.4.3). En resumen, después de un ajuste preliminar del modelo, debe: Calcular estadísticas resumen y mostrar la distribución de los residuales (estandarizados) para identificar valores atípicos. Calcular la correlación entre los residuales (estandarizados) y las variables explicativas adicionales para buscar relaciones lineales. Crear gráficos de dispersión entre los residuales (estandarizados) y las variables explicativas adicionales para buscar relaciones no lineales. Ejemplo: Liquidez del Mercado de Valores. La decisión de un inversor de comprar una acción generalmente se toma teniendo en cuenta varios criterios. Primero, los inversores suelen buscar un alto rendimiento esperado. Un segundo criterio es el riesgo de una acción, que puede medirse mediante la variabilidad de los rendimientos. Tercero, muchos inversores están preocupados por el tiempo que están comprometiendo su capital con la compra de un valor. Muchas acciones de ingresos, como las de servicios públicos, devuelven regularmente partes de las inversiones de capital en forma de dividendos. Otras acciones, particularmente las de crecimiento, no devuelven nada hasta la venta del valor. Por lo tanto, la duración promedio de la inversión en un valor es otro criterio. Cuarto, a los inversores les preocupa la capacidad de vender la acción en cualquier momento que sea conveniente para ellos. Nos referimos a este cuarto criterio como la liquidez de la acción. Cuanto más líquida sea la acción, más fácil será venderla. Para medir la liquidez, en este estudio utilizamos el número de acciones negociadas en una bolsa durante un período de tiempo específico (llamado VOLUME). Estamos interesados en estudiar la relación entre el volumen y otras características financieras de una acción. Comenzamos este estudio con 126 empresas cuyas opciones se negociaron el 3 de diciembre de 1984. Los datos de las acciones fueron obtenidos de Francis Emory Fitch, Inc. para el período del 3 de diciembre de 1984 al 28 de febrero de 1985. Para las variables de actividad comercial, examinamos: El volumen total de negociación de tres meses (VOLUME, en millones de acciones), El número total de transacciones de tres meses (NTRAN), y El tiempo promedio entre transacciones (AVGT, medido en minutos). Para las variables de tamaño de la empresa, utilizamos: El precio de apertura de la acción el 2 de enero de 1985 (PRICE), El número de acciones en circulación el 31 de diciembre de 1984 (SHARE, en millones de acciones), y El valor de mercado del capital (VALUE, en miles de millones de dólares) obtenido al tomar el producto de PRICE y SHARE. Finalmente, para el apalancamiento financiero, examinamos la relación deuda-capital (DEB_EQ) obtenida de la Cinta Industrial de Compustat y el manual de Moody’s. Los datos en SHARE se obtienen de la cinta mensual del Centro de Investigación en Precios de Seguridad (CRSP). Después de examinar algunas estadísticas resumen preliminares de los datos, se eliminaron tres empresas porque tenían un volumen inusualmente alto o un precio elevado. Estas son Teledyne y Capital Cities Communication, cuyos precios eran más de cuatro veces el precio promedio de las demás empresas, y American Telephone and Telegraph, cuyo volumen total era más de siete veces el volumen total promedio de las demás empresas. Basado en una investigación adicional, cuyos detalles no se presentan aquí, estas empresas fueron eliminadas porque parecían representar circunstancias especiales que no deseábamos modelar. La Tabla 5.2 resume las estadísticas descriptivas basadas en las \\(n = 123\\) empresas restantes. Por ejemplo, en la Tabla 5.2, vemos que el tiempo promedio entre transacciones es de aproximadamente cinco minutos y este tiempo varía desde un mínimo de menos de 1 minuto hasta un máximo de aproximadamente 20 minutos. Table 5.2: Estadísticas Resumen de las Variables de Liquidez de las Acciones Media Mediana Desviación Estándar Mínimo Máximo VOLUME 13.423 11.556 10.632 0.658 64.572 AVGT 5.441 4.284 3.853 0.590 20.772 NTRAN 6436.000 5071.000 5310.000 999.000 36420.000 PRICE 38.800 34.380 21.370 9.120 122.380 SHARE 94.730 53.830 115.100 6.740 783.050 VALUE 4.116 2.065 8.157 0.115 75.437 DEBEQ 2.697 1.105 6.509 0.185 53.628 Fuente: Francis Emory Fitch, Inc., Standard &amp; Poor’s Compustat, y el Centro de Investigación de Precios de Valores de la Universidad de Chicago. La Tabla 5.3 reporta los coeficientes de correlación y la Figura 5.2 proporciona la matriz de dispersión correspondiente. Si tienes conocimientos en finanzas, te resultará interesante notar que el apalancamiento financiero, medido por DEB_EQ, no parece estar relacionado con las otras variables. A partir del diagrama de dispersión y la matriz de correlación, vemos una fuerte relación entre VOLUME y el tamaño de la empresa, medido por SHARE y VALUE. Además, las tres variables de actividad de negociación, VOLUME, AVGT y NTRAN, están altamente relacionadas entre sí. Table 5.3: Matriz de Correlación de la Liquidez de las Acciones AVGT NTRAN PRICE SHARE VALUE DEB_EQ VOLUME AVGT 1.000 -0.668 -0.128 -0.429 -0.318 0.094 -0.674 NTRAN -0.668 1.000 0.190 0.817 0.760 -0.092 0.913 PRICE -0.128 0.190 1.000 0.177 0.457 -0.038 0.168 SHARE -0.429 0.817 0.177 1.000 0.829 -0.077 0.773 VALUE -0.318 0.760 0.457 0.829 1.000 -0.077 0.702 DEB_EQ 0.094 -0.092 -0.038 -0.077 -0.077 1.000 -0.052 VOLUME -0.674 0.913 0.168 0.773 0.702 -0.052 1.000 Código R para Producir las Tablas 5.2 y 5.3 liquidity &lt;- read.csv(&quot;CSVData/Liquidity.csv&quot;, header=TRUE) varLiquid &lt;- c(&quot;AVGT&quot;, &quot;NTRAN&quot;, &quot;PRICE&quot;, &quot;SHARE&quot;, &quot;VALUE&quot;, &quot;DEBEQ&quot;, &quot;VOLUME&quot;) liquidMat &lt;- data.frame(liquidity[varLiquid]) names(liquidMat)[names(liquidMat) == &quot;DEBEQ&quot;] &lt;- &quot;DEB_EQ&quot; # TABLA 5.2 ESTADÍSTICAS RESUMEN BookSummStats &lt;- function(Xymat){ meanSummary &lt;- sapply(Xymat, mean, na.rm=TRUE) sdSummary &lt;- sapply(Xymat, sd, na.rm=TRUE) minSummary &lt;- sapply(Xymat, min, na.rm=TRUE) maxSummary &lt;- sapply(Xymat, max, na.rm=TRUE) medSummary &lt;- sapply(Xymat, median,na.rm=TRUE) tableMat &lt;- cbind(meanSummary, medSummary, sdSummary, minSummary, maxSummary) return(tableMat) } liquidMat1 &lt;- liquidMat[,c(7,1:6)] tableMat &lt;- BookSummStats(liquidMat1) colnames(tableMat) &lt;- c(&quot;Media&quot;, &quot;Mediana&quot;, &quot;Desviación Estándar&quot;, &quot;Mínimo&quot;, &quot;Máximo&quot;) rownames(tableMat) &lt;- varLiquid[c(7,1:6)] tableMat &lt;- round(tableMat, digits = 3) tableMat[3,] &lt;- round(tableMat[3,], digits = 0) tableMat[4:5,] &lt;- round(tableMat[4:5,], digits = 2) TableGen1(TableData=tableMat, TextTitle=&#39;Estadísticas Resumen de las Variables de Liquidez de las Acciones&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:5, ColWidth = ColWidth5) cor_matrix &lt;- cor(liquidMat) rownames(cor_matrix) &lt;- colnames(cor_matrix) &lt;- c(&quot;AVGT&quot;, &quot;NTRAN&quot;, &quot;PRICE&quot;, &quot;SHARE&quot;, &quot;VALUE&quot;, &quot;DEB_EQ&quot;, &quot;VOLUME&quot;) TableGen1(TableData=cor_matrix, TextTitle=&#39;Matriz de Correlación de la Liquidez de las Acciones&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:6, ColWidth = ColWidth6) La Figura 5.2 muestra que la variable AVGT está inversamente relacionada con VOLUME y NTRAN está inversamente relacionada con AVGT. De hecho, resultó que la correlación entre el tiempo promedio entre transacciones y el recíproco del número de transacciones fue del \\(99.98\\%!\\) Esto no es tan sorprendente cuando se piensa en cómo se podría calcular AVGT. Por ejemplo, en la Bolsa de Valores de Nueva York, el mercado está abierto de 10:00 A.M. a 4:00 P.M. Para cada acción en un día particular, el tiempo promedio entre transacciones multiplicado por el número de transacciones es casi igual a 360 minutos (= 6 horas). Por lo tanto, excepto por errores de redondeo porque las transacciones solo se registran al minuto más cercano, hay una relación lineal perfecta entre AVGT y el recíproco de NTRAN. Figure 5.2: Matriz de dispersión para las variables de liquidez de las acciones. La variable del número de transacciones (NTRAN) parece estar fuertemente relacionada con el VOLUME de acciones negociadas e inversamente relacionada con AVGT. Para comenzar a entender la medida de liquidez VOLUME, primero ajustamos un modelo de regresión utilizando NTRAN como una variable explicativa. El modelo de regresión ajustado es: \\[ \\small{ \\begin{array}{lcc} \\text{VOLUME} &amp;= 1.65 &amp;+ 0.00183 \\text{ NTRAN} \\\\ \\text{errores estándar} &amp; (0.6173) &amp; (0.000074) \\end{array} } \\] con \\(R^2 = 83.4\\%\\) y \\(s = 4.35\\). Note que el cociente \\(t\\) para la pendiente asociada con NTRAN es \\[ t(b_1) = \\frac{b_1}{se(b_1)} = \\frac{0.00183}{0.000074} = 24.7 \\] indicando una fuerte significancia estadística. Los residuos se calcularon utilizando este modelo estimado. Para ver si los residuos están relacionados con otras variables explicativas, la Tabla 5.4 muestra las correlaciones. Table 5.4: Primera Tabla de Correlaciones Variable AVGT PRICE SHARE VALUE DEB_EQ RESID -0.159 -0.014 0.064 0.018 0.078 Nota: Los residuos se crearon a partir de una regresión de VOLUME sobre NTRAN. La correlación entre el residuo y AVGT y el diagrama de dispersión (no mostrado aquí) indica que puede haber alguna información en la variable AVGT en el residuo. Por lo tanto, parece razonable usar AVGT directamente en el modelo de regresión. Recuerde que estamos interpretando el residuo como el valor de VOLUME habiendo controlado el efecto de NTRAN. A continuación, ajustamos un modelo de regresión utilizando NTRAN y AVGT como variables explicativas. El modelo de regresión ajustado es: \\[ \\small{ \\begin{array}{lccc} \\text{VOLUME} &amp;= 4.41 &amp;- 0.322 \\text{ AVGT} &amp;+ 0.00167 \\text{ NTRAN} \\\\ \\text{errores estándar} &amp; (1.30)&amp; (0.135)&amp; (0.000098) \\end{array} } \\] con \\(R^2 = 84.2\\%\\) y \\(s = 4.26\\). Basado en el cociente \\(t\\) para AVGT, \\(t(b_{AVGT}) = \\frac{-0.322}{0.135} = -2.39\\), parece que AVGT es una variable explicativa útil en el modelo. Note también que \\(s\\) ha disminuido, lo que indica que \\(R_a^2\\) ha aumentado. La Tabla 5.5 proporciona correlaciones entre los residuos del modelo y otras posibles variables explicativas e indica que no parece haber mucha información adicional en las variables explicativas. Esto se reafirma por la tabla correspondiente de diagramas de dispersión en la Figura 5.3. Los histogramas en la Figura 5.3 sugieren que, aunque la distribución de los residuos es bastante simétrica, la distribución de cada variable explicativa está sesgada. Debido a esto, se exploraron transformaciones de las variables explicativas. Esta línea de pensamiento no proporcionó mejoras reales y, por lo tanto, no se proporcionan detalles aquí. Figure 5.3: Matriz de dispersión de los residuos de la regresión de VOLUME sobre NTRAN y AVGT en el eje vertical y las variables predictoras restantes en los ejes horizontales. Table 5.5: Segunda Tabla de Correlaciones Variable PRICE SHARE VALUE DEB_EQ RESID -0.015 0.100 0.074 0.089 Nota: Los residuos se crearon a partir de una regresión de VOLUME sobre NTRAN y AVGT. 5.4 Puntos Influyentes No todos los puntos son creados iguales; en esta sección veremos que ciertas observaciones pueden tener un efecto desproporcionado en el ajuste general de la regresión. A estos puntos los llamaremos “influyentes.” Esto no es tan sorprendente; ya hemos visto que las estimaciones de los coeficientes de regresión son sumas ponderadas de respuestas (ver Sección 3.2.4). Algunas observaciones tienen pesos mayores que otras y, por lo tanto, tienen una mayor influencia en las estimaciones de los coeficientes de regresión. Por supuesto, el hecho de que una observación sea influyente no significa que sea incorrecta o que su impacto en el modelo sea engañoso. Como analistas, simplemente nos gustaría saber si nuestro modelo ajustado es sensible a cambios leves, como la eliminación de un solo punto, para sentirnos cómodos al generalizar nuestros resultados de la muestra a una población más grande. Para evaluar la influencia, pensamos en observaciones como respuestas inusuales, dadas un conjunto de variables explicativas, o que tienen un conjunto inusual de valores de variables explicativas. Ya hemos visto en la Sección 5.3 cómo evaluar respuestas inusuales utilizando residuos. Esta sección se centra en conjuntos inusuales de valores de variables explicativas. 5.4.1 Apalancamiento Introdujimos este tema en la Sección 2.6, donde llamamos a una observación con una variable explicativa inusual un “punto de alto apalancamiento.” Con más de una variable explicativa, determinar si una observación es un punto de alto apalancamiento no es tan sencillo. Por ejemplo, es posible que una observación “no sea inusual” para ninguna variable individual y, sin embargo, sea inusual en el espacio de variables explicativas. Considere el conjunto de datos ficticio representado en la Figura 5.4. Visualmente, parece claro que el punto marcado en la esquina superior derecha es inusual. Sin embargo, no es inusual cuando se examina el histograma de \\(x_1\\) o de \\(x_2\\). Es inusual solo cuando se consideran las variables explicativas de manera conjunta. Figure 5.4: El elipsoide representa la mayoría de los datos. La flecha marca un punto inusual. Para dos variables explicativas, esto es evidente al examinar los datos gráficamente. Debido a que es difícil examinar gráficamente los datos con más de dos variables explicativas, necesitamos un procedimiento numérico para evaluar el apalancamiento. Para definir el concepto de apalancamiento en la regresión lineal múltiple, utilizamos algunos conceptos de álgebra matricial. Específicamente, en la Sección 3.1 mostramos que el vector de coeficientes de regresión de mínimos cuadrados se puede calcular usando \\(\\mathbf{b} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}\\). Así, podemos expresar el vector de valores ajustados \\(\\hat{\\mathbf{y}} = (\\hat{y}_1, \\ldots, \\hat{y}_n)^{\\prime}\\) como \\[\\begin{equation} \\mathbf{\\hat{y}} = \\mathbf{Xb} . \\tag{5.2} \\end{equation}\\] De manera similar, el vector de residuos es el vector de respuesta menos el vector de valores ajustados, es decir, \\(\\mathbf{e} = \\mathbf{y - \\hat{y}}\\). A partir de la expresión para los coeficientes de regresión \\(\\mathbf{b}\\) en la ecuación (3.4), tenemos \\[ \\mathbf{\\hat{y}} = \\mathbf{X} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{X}^{\\prime} \\mathbf{y} \\] Esta ecuación sugiere definir \\[ \\mathbf{H} = \\mathbf{X} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{X}^{\\prime} \\] de modo que \\[ \\mathbf{\\hat{y}} = \\mathbf{Hy} \\] A partir de esto, se dice que la matriz \\(\\mathbf{H}\\) proyecta el vector de respuestas \\(\\mathbf{y}\\) en el vector de valores ajustados \\(\\mathbf{\\hat{y}}\\). Alternativamente, puede pensar en \\(\\mathbf{H}\\) como la matriz que pone el “sombrero,” o circunflejo, en \\(\\mathbf{y}\\). A partir de la \\(i\\)-ésima fila de la ecuación vectorial \\(\\mathbf{\\hat{y}} = \\mathbf{Hy}\\), tenemos \\[ \\hat{y}_i = h_{i1} y_1 + h_{i2} y_2 + \\cdots + h_{ii} y_i + \\cdots + h_{in} y_n \\] Aquí, \\(h_{ij}\\) es el número en la \\(i\\)-ésima fila y \\(j\\)-ésima columna de \\(\\mathbf{H}\\). A partir de esta expresión, vemos que cuanto mayor sea \\(h_{ii}\\), mayor será el efecto que la \\(i\\)-ésima respuesta \\((y_i)\\) tiene en el valor ajustado correspondiente \\((\\hat{y}_i)\\). Por lo tanto, llamamos a \\(h_{ii}\\) el apalancamiento para la \\(i\\)-ésima observación. Debido a que \\(h_{ii}\\) es el elemento diagonal \\(i\\)-ésimo de \\(\\mathbf{H}\\), una expresión directa para \\(h_{ii}\\) es \\[\\begin{equation} h_{ii} = \\mathbf{x}_i^{\\prime} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{x}_i \\tag{5.3} \\end{equation}\\] donde \\(\\mathbf{x}_i = (x_{i0}, x_{i1}, \\ldots, x_{ik})^{\\prime}\\). Debido a que los valores de \\(h_{ii}\\) se calculan en base a las variables explicativas, los valores de la variable de respuesta no afectan el cálculo de los apalancamientos. Los valores altos de apalancamiento indican que una observación puede tener un efecto desproporcionado en el ajuste, esencialmente porque está distante de las otras observaciones (al observar el espacio de variables explicativas). ¿Qué tan grande es grande? Existen algunas pautas de álgebra matricial, donde tenemos que \\[ \\frac{1}{n} \\leq h_{ii} \\leq 1 \\] y \\[ \\bar{h} = \\frac{1}{n} \\sum_{i=1}^{n} h_{ii} = \\frac{k+1}{n}. \\] Por lo tanto, cada apalancamiento está limitado por \\(n^{-1}\\) y \\(1\\), y el apalancamiento promedio es igual al número de coeficientes de regresión dividido por el número de observaciones. A partir de estos y argumentos relacionados, utilizamos una convención ampliamente adoptada y declaramos que una observación es un punto de alto apalancamiento si el apalancamiento supera tres veces el promedio, es decir, si \\[ h_{ii} &gt; \\frac{3(k+1)}{n}. \\] Una vez identificados los puntos de alto apalancamiento, al igual que con los valores atípicos, es importante que el analista busque causas especiales que puedan haber producido estos puntos inusuales. Para ilustrar, en la Sección 2.7 identificamos el colapso del mercado de 1987 como la razón detrás del punto de alto apalancamiento. Además, los puntos de alto apalancamiento a menudo se deben a errores administrativos al codificar los datos, que pueden o no ser fáciles de rectificar. En general, las opciones para manejar puntos de alto apalancamiento son similares a las disponibles para tratar con valores atípicos. Opciones para Manejar Puntos de Alto Apalancamiento Incluir la observación en las estadísticas resumidas pero comentar sobre su efecto. Por ejemplo, una observación puede apenas superar un límite y su efecto puede no ser importante en el análisis general. Eliminar la observación del conjunto de datos. Nuevamente, la justificación básica para esta acción es que se considera que la observación no es representativa de una población más grande. Una opción intermedia entre (1) y (2) es presentar el análisis tanto con como sin el punto de alto apalancamiento. De esta manera, se demuestra completamente el impacto del punto y el lector de su análisis puede decidir cuál opción es más adecuada. Elegir otra variable para representar la información. En algunos casos, otra variable explicativa estará disponible para servir como reemplazo. Por ejemplo, en un ejemplo de alquileres de apartamentos, podríamos usar el número de habitaciones para reemplazar una variable de metros cuadrados como medida del tamaño del apartamento. Aunque los metros cuadrados de un apartamento pueden ser inusualmente grandes, lo que lo convierte en un punto de alto apalancamiento, puede tener una, dos o tres habitaciones, dependiendo de la muestra examinada. Usar una transformación no lineal de una variable explicativa. Para ilustrar, con nuestro ejemplo de Liquidez de Acciones en la Sección 5.5.3, podemos transformar la variable continua de razón deuda a capital DEB_EQ en una variable que indique la presencia de “alta” razón deuda a capital. Por ejemplo, podríamos codificar DE_IND = 1 si DEB_EQ &gt; 5 y DE_IND = 0 si DEB_EQ ≤ 5. Con esta recodificación, aún conservamos información sobre el apalancamiento financiero de una empresa sin permitir que los valores grandes de DEB_EQ influyan en el ajuste de la regresión. Algunos analistas usan metodologías de estimación “robustas” como alternativa a la estimación de mínimos cuadrados. La idea básica de estas técnicas es reducir el efecto de cualquier observación en particular. Estas técnicas son útiles para reducir el efecto tanto de valores atípicos como de puntos de alto apalancamiento. Esta táctica puede considerarse intermedia entre un procedimiento extremo, ignorando el efecto de puntos inusuales, y otro extremo, dando plena credibilidad a los puntos inusuales al eliminarlos del conjunto de datos. La palabra robusto sugiere que estas metodologías de estimación son “saludables” incluso cuando son atacadas por una observación ocasionalmente mala (un germen). Hemos visto que esto no es cierto para la estimación de mínimos cuadrados. 5.4.2 Distancia de Cook Para cuantificar la influencia de un punto, una medida que considera tanto las variables de respuesta como las explicativas es la Distancia de Cook. Esta distancia, \\(D_i\\), se define como \\[\\begin{equation} \\begin{array}{ll} D_i &amp;= \\frac{\\sum_{j=1}^{n} (\\hat{y}_j - \\hat{y}_{j(i)})^2}{(k+1) s^2} \\tag{5.4} \\\\ &amp;= \\left( \\frac{e_i}{se(e_i)} \\right)^2 \\frac{h_{ii}}{(k+1)(1 - h_{ii})}. \\end{array} \\end{equation}\\] La primera expresión proporciona una definición. Aquí, \\(\\hat{y}_{j(i)}\\) es la predicción de la \\(j\\)-ésima observación, calculada excluyendo la \\(i\\)-ésima observación del ajuste de regresión. Para medir el impacto de la \\(i\\)-ésima observación, comparamos los valores ajustados con y sin la \\(i\\)-ésima observación. Cada diferencia se eleva al cuadrado y se suma en todas las observaciones para resumir el impacto. La segunda ecuación proporciona otra interpretación de la distancia \\(D_i\\). La primera parte, \\(\\left( \\frac{e_i}{se(e_i)} \\right)^2\\), es el cuadrado del residuo estandarizado \\(i\\)-ésimo. La segunda parte, \\(\\frac{h_{ii}}{(k+1)(1 - h_{ii})}\\), se atribuye únicamente al apalancamiento. Así, la distancia \\(D_i\\) se compone de una medida para valores atípicos multiplicada por una medida de apalancamiento. De esta manera, la distancia de Cook tiene en cuenta tanto las variables de respuesta como las explicativas. La Sección 5.10.3 establece la validez de la ecuación (5.4). Para tener una idea del tamaño esperado de \\(D_i\\) para un punto que no es inusual, recuerde que esperamos que los residuos estandarizados sean aproximadamente uno y que el apalancamiento \\(h_{ii}\\) sea aproximadamente \\(\\frac{k+1}{n}\\). Por lo tanto, anticipamos que \\(D_i\\) debería ser aproximadamente \\(\\frac{1}{n}\\). Otra regla general es comparar \\(D_i\\) con una distribución \\(F\\) con \\(df_1 = k+1\\) y \\(df_2 = n - (k+1)\\) grados de libertad. Los valores de \\(D_i\\) que son grandes en comparación con esta distribución merecen atención. Ejemplo: Valores Atípicos y Puntos de Alto Apalancamiento - Continuación. Para ilustrar, volvemos a nuestro ejemplo de la Sección 2.6. En este ejemplo, consideramos 19 puntos “buenos” o base, más cada uno de los tres tipos de puntos inusuales, etiquetados como A, B y C. La Tabla 5.6 resume los cálculos. Table 5.6: Medidas de Tres Tipos de Puntos Inusuales Observation Standardized Residual \\(e / se(e)\\) Leverage \\(h\\) Cook’s Distance \\(D\\) A 4.00 0.067 0.577 B 0.77 0.550 0.363 C -4.01 0.550 9.832 Como se mencionó en la Sección 2.6, de la columna de residuos estandarizados vemos que tanto los puntos A como C son valores atípicos. Para juzgar el tamaño de los apalancamientos, dado que hay \\(n=20\\) puntos, los apalancamientos están limitados por 0.05 y 1.00, con el apalancamiento promedio siendo \\(\\bar{h} = \\frac{2}{20} = 0.10\\). Usando 0.3 (\\(= 3 \\times \\bar{h}\\)) como un umbral, tanto los puntos B como C son puntos de alto apalancamiento. Nótese que sus valores son los mismos. Esto se debe a que, según la Figura 2.7, los valores de las variables explicativas son los mismos y solo la variable de respuesta ha cambiado. La columna de la distancia de Cook captura ambos tipos de comportamiento inusual. Dado que el valor típico de \\(D_i\\) es \\(\\frac{1}{n}\\) o 0.05, la distancia de Cook proporciona una estadística para alertarnos de que cada punto es inusual en un aspecto u otro. En particular, el punto C tiene un \\(D_i\\) muy grande, lo que refleja el hecho de que es tanto un valor atípico como un punto de alto apalancamiento. El percentil 95 de una distribución \\(F\\) con \\(df_1 = 2\\) y \\(df_2 = 18\\) es 3.555. El hecho de que el punto C tenga un valor de \\(D_i\\) que supera con creces este umbral indica la influencia sustancial de este punto. 5.5 Colinealidad 5.5.1 ¿Qué es la Colinealidad? Colinealidad, o multicolinealidad, ocurre cuando una variable explicativa es, o casi es, una combinación lineal de las otras variables explicativas. Intuitivamente, con datos colineales, es útil pensar en las variables explicativas como altamente correlacionadas entre sí. Si una variable explicativa es colineal, surge la pregunta de si es redundante, es decir, si la variable proporciona poca información adicional sobre la información que ya está en las otras variables explicativas. Las preguntas son: ¿Es importante la colinealidad? Si es así, ¿cómo afecta el ajuste de nuestro modelo y cómo la detectamos? Para abordar la primera pregunta, considere un ejemplo algo patológico. Ejemplo: Variables Explicativas Perfectamente Correlacionadas. Joe Finance fue solicitado para ajustar el modelo \\(\\mathrm{E} ~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) a un conjunto de datos. Su modelo ajustado resultante fue \\(\\hat{y} = -87 + x_1 + 18 x_2.\\) El conjunto de datos considerado es: \\[ \\begin{array}{l|cccc} \\hline i &amp; 1 &amp; 2 &amp; 3 &amp; 4 \\\\ \\hline y_i &amp; 23 &amp; 83 &amp; 63 &amp; 103 \\\\ x_{i1} &amp; 2 &amp; 8 &amp;6 &amp; 10 \\\\ x_{i2} &amp; 6 &amp; 9 &amp; 8 &amp; 10 \\\\ \\hline \\end{array} \\] Joe verificó el ajuste para cada observación. Joe estaba muy contento porque ajustó los datos perfectamente. Por ejemplo, para la tercera observación, el valor ajustado es \\(\\hat{y}_3 = -87 + 6 + 18 \\times 8 = 63\\), que es igual a la tercera respuesta, \\(y_3\\). Debido a que la respuesta es igual al valor ajustado, el residuo es cero. Puede verificar que esto es cierto para cada observación, y así, el \\(R^2\\) resultó ser \\(100\\%\\). Sin embargo, Jane Actuary llegó y ajustó el modelo \\(\\hat{y} = -7 + 9 x_1 + 2 x_2.\\) Jane realizó las mismas comprobaciones cuidadosas que Joe hizo y también obtuvo un ajuste perfecto (\\(R^2 = 1\\)). ¿Quién tiene razón? La respuesta es ambos y ninguno. De hecho, hay un número infinito de ajustes. Esto se debe a la relación perfecta \\(x_2 = 5 + \\frac{x_1}{2}\\) entre las dos variables explicativas. Este ejemplo ilustra algunos hechos importantes sobre la colinealidad. Hechos sobre la Colinealidad La colinealidad no nos impide obtener buenos ajustes ni hacer predicciones de nuevas observaciones. Nótese que en el ejemplo anterior obtuvimos ajustes perfectos. Las estimaciones de las varianzas de error y, por lo tanto, las pruebas de adecuación del modelo, siguen siendo fiables. En casos de colinealidad severa, los errores estándar de los coeficientes de regresión individuales son mayores que en los casos en que, ceteris paribus, no existe colinealidad severa. Con errores estándar grandes, los coeficientes de regresión individuales pueden no ser significativos. Además, debido a que un error estándar grande significa que el correspondiente cociente \\(t\\) es pequeño, es difícil detectar la importancia de una variable. Para detectar la colinealidad, comience con una matriz de coeficientes de correlación de las variables explicativas. Esta matriz es fácil de crear, fácil de interpretar y captura rápidamente las relaciones lineales entre pares de variables. Una matriz de diagramas de dispersión proporciona un refuerzo visual de las estadísticas resumidas en la matriz de correlación. 5.5.2 Factores de Inflación de Varianza Las matrices de correlación y diagramas de dispersión capturan solo las relaciones entre pares de variables. Para capturar relaciones más complejas entre varias variables, introducimos el factor de inflación de varianza (VIF). Para definir un VIF, suponga que el conjunto de variables explicativas está etiquetado como \\(x_1, x_2, \\ldots, x_{k}\\). Ahora, ejecute la regresión utilizando \\(x_j\\) como la “respuesta” y los otros \\(x\\) (\\(x_1, x_2, \\ldots, x_{j-1}, x_{j+1}, \\ldots, x_{k}\\)) como las variables explicativas. Denote el coeficiente de determinación de esta regresión por \\(R_j^2\\). Interpretamos \\(R_j = \\sqrt{R_j^2}\\) como el coeficiente de correlación múltiple entre \\(x_j\\) y las combinaciones lineales de los otros \\(x\\). A partir de este coeficiente de determinación, definimos el factor de inflación de varianza \\[ VIF_j = \\frac{1}{1 - R_j^2}, \\text{ para } j = 1, 2, \\ldots, k. \\] Un mayor \\(R_j^2\\) resulta en un mayor \\(VIF_j\\); esto significa una mayor colinealidad entre \\(x_j\\) y los otros \\(x\\). Ahora, \\(R_j^2\\) por sí solo es suficiente para capturar la relación lineal de interés. Sin embargo, usamos \\(VIF_j\\) en lugar de \\(R_j^2\\) como nuestra medida de colinealidad debido a la relación algebraica \\[\\begin{equation} se(b_j) = s \\frac{\\sqrt{VIF_j}}{s_{x_j} \\sqrt{n - 1}}. \\tag{5.5} \\end{equation}\\] Aquí, \\(se(b_j)\\) y \\(s\\) son errores estándar y la desviación estándar residual de un ajuste completo de regresión de \\(y\\) sobre \\(x_1, \\ldots, x_{k}\\). Además, \\(s_{x_j} = \\sqrt{(n - 1)^{-1} \\sum_{i=1}^{n} (x_{ij} - \\bar{x}_j)^2 }\\) es la desviación estándar muestral de la \\(j\\)-ésima variable \\(x_j\\). La Sección 5.10.3 proporciona una verificación de la ecuación (5.5). Así, un mayor \\(VIF_j\\) resulta en un mayor error estándar asociado con la pendiente \\(j\\)-ésima, \\(b_j\\). Recuerde que \\(se(b_j)\\) es \\(s\\) veces la raíz cuadrada del \\((j+1)\\)-ésimo elemento diagonal de \\((\\mathbf{X}^{\\prime} \\mathbf{X})^{-1}\\). La idea es que cuando ocurre colinealidad, la matriz \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\) tiene propiedades similares al número cero. Cuando intentamos calcular la inversa de \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\), esto es análogo a dividir por cero en números escalares. Como regla general, cuando \\(VIF_j\\) supera 10 (lo cual es equivalente a \\(R_j^2 &gt; 90\\%\\)), decimos que existe colinealidad severa. Esto puede indicar la necesidad de acción. Tolerancia, definida como el recíproco del factor de inflación de varianza, es otra medida de colinealidad utilizada por algunos analistas. Por ejemplo, con \\(k = 2\\) variables explicativas en el modelo, entonces \\(R_1^2\\) es la correlación cuadrada entre las dos variables explicativas, digamos \\(r_{12}^2\\). Entonces, a partir de la ecuación anterior, tenemos que \\[ se(b_j) = s \\left(s_{x_j} \\sqrt{n - 1} \\right)^{-1} \\left(1 - r_{12}^2 \\right)^{-1/2}, \\text{ para } j = 1, 2. \\] A medida que la correlación se acerca a uno en valor absoluto, \\(|r_{12}| \\rightarrow 1\\), entonces el error estándar se vuelve grande, lo que significa que el estadístico \\(t\\) correspondiente se vuelve pequeño. En resumen, un alto \\(VIF\\) puede significar pequeños estadísticos \\(t\\) a pesar de que las variables sean importantes. Además, se puede verificar que la correlación entre \\(b_1\\) y \\(b_2\\) es \\(-r_{12}\\), indicando que las estimaciones de los coeficientes están altamente correlacionadas. Ejemplo: Liquidez del Mercado de Valores - Continuación. Como ejemplo, considere una regresión de VOLUME sobre PRICE, SHARE y VALUE. A diferencia de las variables explicativas consideradas en la Sección 5.5.3, estas tres variables explicativas no son medidas de actividad de trading. A partir de un ajuste de regresión, tenemos \\(R^2 = 61\\%\\) y \\(s = 6.72\\). Las estadísticas asociadas con los coeficientes de regresión están en la Tabla 5.7. Table 5.7: Estadísticas de una regresión de VOLUME sobre PRICE, SHARE y VALUE \\(x_j\\) \\(s_{x_j}\\) \\(b_j\\) \\(se(b_j)\\) \\(t(b_j)\\) \\(VIF_j\\) PRICE 21.370 -0.022 0.035 -0.63 1.5 SHARE 115.100 0.054 0.010 5.19 3.8 VALUE 8.157 0.313 0.162 1.94 4.7 Puede verificar que la relación en la ecuación (5.5) es válida para cada una de las variables explicativas en la Tabla 5.7. Dado que cada estadístico \\(VIF\\) es menor a diez, hay poca razón para sospechar colinealidad severa. Esto es interesante porque puede recordar que existe una relación perfecta entre PRICE, SHARE y VALUE en el sentido de que definimos el valor de mercado como VALUE = PRICE \\(\\times\\) SHARE. Sin embargo, la relación es multiplicativa y, por lo tanto, es no lineal. Debido a que las variables no están relacionadas linealmente, es válido incluir las tres en el modelo de regresión. Desde una perspectiva financiera, la variable VALUE es importante porque mide el valor de una empresa. Desde una perspectiva estadística, la variable VALUE cuantifica la interacción entre PRICE y SHARE (las variables de interacción se introdujeron en la Sección 3.5.3). Para la colinealidad, solo nos interesa detectar tendencias lineales, por lo que las relaciones no lineales entre variables no son un problema aquí. Por ejemplo, hemos visto que a veces es útil mantener tanto una variable explicativa \\(x\\) como su cuadrado \\(x^2\\), a pesar de que existe una relación perfecta (no lineal) entre las dos. Sin embargo, debemos verificar que las relaciones no lineales no sean aproximadamente lineales en la región de muestreo. Aunque la relación es teóricamente no lineal, si es cercana a lineal para nuestra muestra disponible, pueden surgir problemas de colinealidad. La Figura 5.5 ilustra esta situación. Figure 5.5: La relación entre \\(x_1\\) y \\(x_2\\) es no lineal. Sin embargo, en la región muestreada, las variables tienen una relación casi lineal. ¿Qué podemos hacer en presencia de colinealidad? Una opción es centrar cada variable, restando su promedio y dividiendo por su desviación estándar. Por ejemplo, crear una nueva variable \\(x_{ij}^{\\ast} = (x_{ij} - \\bar{x}_j) / s_{x_j}\\). A veces, una variable aparece en millones de unidades y otra en fracciones de unidades. Comparado con la primera variable, la segunda parece ser casi una columna constante de ceros (dado que las computadoras retienen típicamente un número finito de dígitos). Si esto es cierto, entonces la segunda variable se parece mucho a un desplazamiento lineal de la columna constante de unos correspondiente al intercepto. Esto es un problema porque, con las operaciones de mínimos cuadrados, estamos implícitamente elevando al cuadrado números que pueden hacer que estas columnas parezcan aún más similares. Este problema es simplemente computacional y es fácil de corregir. Simplemente recodifique las variables para que las unidades sean de magnitud similar. Algunos analistas de datos centran automáticamente todas las variables para evitar estos problemas. Este es un enfoque legítimo porque las técnicas de regresión buscan relaciones lineales; los desplazamientos en ubicación y escala no afectan las relaciones lineales. Otra opción es simplemente no tener en cuenta explícitamente la colinealidad en el análisis, pero discutir algunas de sus implicaciones al interpretar los resultados del análisis de regresión. Este enfoque es probablemente el más comúnmente adoptado. Es un hecho que, al tratar con datos de negocios y económicos, la colinealidad tiende a existir entre las variables. Dado que los datos tienden a ser observacionales en lugar de experimentales, hay poco que el analista pueda hacer para evitar esta situación. En la mejor de las situaciones, una variable auxiliar que proporcione información similar y que facilite el problema de colinealidad está disponible para reemplazar una variable. Similar a nuestra discusión sobre puntos de alta influencia, una versión transformada de la variable explicativa también puede ser un sustituto útil. En algunas situaciones, un reemplazo ideal no está disponible y nos vemos obligados a eliminar una o más variables. Decidir qué variables eliminar es una elección difícil. Al decidir entre variables, a menudo la elección estará dictada por el juicio del investigador sobre cuál es el conjunto de variables más relevante. 5.5.3 Colinealidad e Influencia Las medidas de colinealidad e influencia comparten características comunes y, sin embargo, están diseñadas para capturar diferentes aspectos de un conjunto de datos. Ambas son útiles para la crítica de datos y del modelo; se aplican después de un ajuste preliminar del modelo con el objetivo de mejorar la especificación del modelo. Además, ambas se calculan utilizando solo las variables explicativas; los valores de las respuestas no entran en ninguno de los cálculos. Nuestra medida de colinealidad, el factor de inflación de la varianza, está diseñada para ayudar con la crítica del modelo. Es una medida calculada para cada variable explicativa, diseñada para explicar la relación con otras variables explicativas. La estadística de influencia está diseñada para ayudarnos con la crítica de datos. Es una medida calculada para cada observación para ayudarnos a explicar cuán inusual es una observación con respecto a otras observaciones. La colinealidad puede estar enmascarada o inducida por puntos de alta influencia, como lo señalaron Mason y Gunst (1985) y Hadi (1988). Las Figuras 5.6 y 5.7 proporcionan ilustraciones de cada caso. Estos ejemplos simples subrayan un punto importante: la crítica de datos y la crítica del modelo no son ejercicios separados. Figure 5.6: Con la excepción del punto marcado, \\(x_1\\) y \\(x_2\\) están altamente relacionados linealmente. Figure 5.7: La relación lineal altamente entre \\(x_1\\) y \\(x_2\\) es principalmente debido al punto marcado. Los ejemplos en las Figuras 5.6 y 5.7 también nos ayudan a ver una forma en que los puntos de alta influencia pueden afectar los errores estándar de los coeficientes de regresión. Recuerde que, en la Sección 5.4.1, vimos que los puntos de alta influencia pueden afectar los valores ajustados del modelo. En las Figuras 5.6 y 5.7, vemos que los puntos de alta influencia afectan la colinealidad. Por lo tanto, a partir de la ecuación (5.5), tenemos que los puntos de alta influencia también pueden afectar nuestros errores estándar de los coeficientes de regresión. 5.5.4 Variables Suprensoras Como hemos visto, la colinealidad severa puede inflar seriamente los errores estándar de los coeficientes de regresión. Dado que dependemos de estos errores estándar para evaluar la utilidad de las variables explicativas, nuestros procedimientos de selección de modelos e inferencias pueden ser deficientes en presencia de colinealidad severa. A pesar de estos inconvenientes, la colinealidad leve en un conjunto de datos no debe considerarse una deficiencia del conjunto de datos; es simplemente una característica de las variables explicativas disponibles. Incluso si una variable explicativa es casi una combinación lineal de las demás, eso no significa necesariamente que la información que proporciona sea redundante. Para ilustrar, ahora consideramos una variable suprensora, una variable explicativa que aumenta la importancia de otras variables explicativas cuando se incluye en el modelo. Ejemplo: Variable Suprensora. La Figura 5.8 muestra una matriz de dispersión de un conjunto de datos hipotético con cincuenta observaciones. Este conjunto de datos contiene una variable dependiente y dos variables explicativas. La Tabla 5.8 proporciona la matriz de coeficientes de correlación correspondiente. Aquí, vemos que las dos variables explicativas están altamente correlacionadas. Ahora recuerde que, para una regresión con una variable explicativa, el coeficiente de correlación al cuadrado es el coeficiente de determinación. Así, usando la Tabla 5.8, para una regresión de \\(y\\) sobre \\(x_1\\), el coeficiente de determinación es \\((0.188)^2 = 3.5\\%\\). De manera similar, para una regresión de \\(y\\) sobre \\(x_2\\), el coeficiente de determinación es \\((-0.022)^2 = 0.04\\%\\). Sin embargo, para una regresión de \\(y\\) sobre \\(x_1\\) y \\(x_2\\), el coeficiente de determinación resulta ser sorprendentemente alto, \\(80.7\\%\\). La interpretación es que, individualmente, tanto \\(x_1\\) como \\(x_2\\) tienen poco impacto en \\(y\\). Sin embargo, cuando se toman conjuntamente, las dos variables explicativas tienen un efecto significativo en \\(y\\). Aunque la Tabla 5.8 muestra que \\(x_1\\) y \\(x_2\\) están fuertemente relacionados linealmente, esta relación no significa que \\(x_1\\) y \\(x_2\\) proporcionen la misma información. De hecho, en este ejemplo, las dos variables se complementan entre sí. Figure 5.8: Matriz de dispersión de una variable dependiente y dos variables explicativas para el ejemplo de variable suprensora Table 5.8: Matriz de Correlación para el Ejemplo de Suprensor \\(x_1\\) \\(x_2\\) \\(x_2\\) 0.972 \\(y\\) 0.188 -0.022 5.5.5 Variables Ortogonales Otra forma de entender el impacto de la colinealidad es estudiar el caso en el que no hay relaciones entre conjuntos de variables explicativas. Matemáticamente, se dice que dos matrices \\(\\mathbf{X}_1\\) y \\(\\mathbf{X}_2\\) son ortogonales si \\(\\mathbf{X}_1^{\\prime} \\mathbf{X}_2 = \\mathbf{0}\\). Intuitivamente, dado que generalmente trabajamos con variables centradas (con medias cero), esto significa que cada columna de \\(\\mathbf{X}_1\\) no está correlacionada con cada columna de \\(\\mathbf{X}_2\\). Aunque es poco probable que ocurra con datos observacionales en las ciencias sociales, al diseñar tratamientos experimentales o construir polinomios de alto grado, las aplicaciones de variables ortogonales se utilizan regularmente (véase, por ejemplo, Hocking, 2003). Para nuestros propósitos, trabajaremos con variables ortogonales simplemente para entender las consecuencias lógicas de una ausencia total de colinealidad. Supongamos que \\(\\mathbf{x}_2\\) es una variable explicativa que es ortogonal a \\(\\mathbf{X}_1\\), donde \\(\\mathbf{X}_1\\) es una matriz de variables explicativas que incluye la intersección. Entonces, es sencillo comprobar que la adición de \\(\\mathbf{x}_2\\) a la ecuación de regresión no cambia el ajuste para los coeficientes correspondientes a \\(\\mathbf{X}_1\\). Es decir, sin \\(\\mathbf{x}_2\\), los coeficientes correspondientes a \\(\\mathbf{X}_1\\) se calcularían como \\(\\mathbf{b}_1 = \\left(\\mathbf{X}_1^{\\prime} \\mathbf{X}_1 \\right)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y}\\). Usar el ortogonal \\(\\mathbf{x}_2\\) como parte del cálculo de mínimos cuadrados no cambiaría el resultado para \\(\\mathbf{b}_1\\) (véase el cálculo recursivo de mínimos cuadrados en la Sección 4.7.2). Además, el factor de inflación de la varianza para \\(\\mathbf{x}_2\\) es 1, lo que indica que el error estándar no se ve afectado por las otras variables explicativas. De manera similar, la reducción en la suma de errores al agregar la variable ortogonal \\(\\mathbf{x}_2\\) se debe únicamente a esa variable, y no a su interacción con otras variables en \\(\\mathbf{X}_1\\). Las variables ortogonales pueden ser creadas para datos observacionales en ciencias sociales (así como otros datos colineales) utilizando el método de componentes principales. Con este método, se utiliza una transformación lineal de la matriz de variables explicativas de la forma, \\(\\mathbf{X}^{\\ast} = \\mathbf{X} \\mathbf{P}\\), de manera que la matriz resultante \\(\\mathbf{X}^{\\ast}\\) esté compuesta por columnas ortogonales. La función de regresión transformada es \\(\\mathrm{E~}\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta = \\mathbf{X} \\mathbf{P} \\mathbf{P}^{-1} \\boldsymbol \\beta = \\mathbf{X}^{\\ast} \\boldsymbol \\beta^{\\ast}\\), donde \\(\\boldsymbol \\beta^{\\ast} = \\mathbf{P}^{-1} \\boldsymbol \\beta\\) es el conjunto de nuevos coeficientes de regresión. La estimación procede como antes, con el conjunto ortogonal de variables explicativas. Al elegir la matriz \\(\\mathbf{P}\\) apropiadamente, cada columna de \\(\\mathbf{X}^{\\ast}\\) tiene una contribución identificable. Así, podemos usar técnicas de selección de variables para identificar las porciones de “componentes principales” de \\(\\mathbf{X}^{\\ast}\\) para usar en la ecuación de regresión. La regresión por componentes principales es un método ampliamente utilizado en algunas áreas de aplicación, como la psicología. Puede abordar fácilmente datos altamente colineales de manera disciplinada. La principal desventaja de esta técnica es que las estimaciones de parámetros resultantes son difíciles de interpretar. 5.6 Criterios de Selección 5.6.1 Bondad de Ajuste ¿Qué tan bien se ajusta el modelo a los datos? Los criterios que miden la proximidad entre el modelo ajustado y los datos reales se conocen como estadísticas de bondad de ajuste. Específicamente, interpretamos el valor ajustado \\(\\hat{y}_i\\) como la mejor aproximación del modelo para la \\(i\\)-ésima observación y lo comparamos con el valor real \\(y_i\\). En la regresión lineal, examinamos la diferencia a través del residuo \\(e_i = y_i - \\hat{y}_i\\); residuos pequeños implican un buen ajuste del modelo. Hemos cuantificado esto a través del tamaño del error típico \\((s)\\), incluyendo el coeficiente de determinación \\((R^2)\\) y una versión ajustada \\((R_{a}^2)\\). Para modelos no lineales, necesitaremos medidas adicionales, y es útil introducir estas medidas en este caso lineal más simple. Una de estas medidas es el Criterio de Información de Akaike que se definirá en términos de ajustes de verosimilitud en la Sección 11.9.4. Para la regresión lineal, se reduce a \\[\\begin{equation} AIC = n \\ln (s^2) + n \\ln (2 \\pi) + n + 3 + k. \\tag{5.6} \\end{equation}\\] Para la comparación de modelos, cuanto menor sea el \\(AIC\\), mejor es el ajuste. Comparar modelos con el mismo número de variables (\\(k\\)) significa que seleccionar un modelo con valores bajos de \\(AIC\\) lleva a la misma elección que seleccionar un modelo con valores bajos de la desviación estándar de los residuos \\(s\\). Además, un pequeño número de parámetros implica un valor bajo de \\(AIC\\), manteniéndose todo lo demás constante. La idea es que esta medida equilibra el ajuste (\\(n \\ln (s^2)\\)) con una penalización por complejidad (el número de parámetros, \\(k+2\\)). Los paquetes estadísticos a menudo omiten constantes como \\(n \\ln (2 \\pi)\\) y \\(n+3\\) al reportar \\(AIC\\) porque no importan al comparar modelos. La Sección 11.9.4 presentará otra medida, el Criterio de Información de Bayes (\\(BIC\\)), que da un peso menor a la penalización por complejidad. Una tercera medida de bondad de ajuste que se usa en modelos de regresión lineal es la estadística \\(C_p\\). Para definir esta estadística, supongamos que tenemos disponibles \\(k\\) variables explicativas \\(x_1, ..., x_{k}\\) y realizamos una regresión para obtener \\(s_{full}^2\\) como el error cuadrático medio. Ahora, supongamos que consideramos usar solo \\(p-1\\) variables explicativas de modo que haya \\(p\\) coeficientes de regresión. Con estas \\(p-1\\) variables explicativas, realizamos una regresión para obtener la suma de cuadrados del error \\((Error~SS)_p\\). Así, estamos en posición de definir \\[ C_{p} = \\frac{(Error~SS)_p}{s_{full}^2} - n + 2p. \\] Como criterio de selección, elegimos el modelo con un coeficiente \\(C_{p}\\) “pequeño”, donde pequeño se entiende en relación con \\(p\\). En general, los modelos con valores más pequeños de \\(C_{p}\\) son más deseables. Al igual que las estadísticas \\(AIC\\) y \\(BIC\\), la estadística \\(C_{p}\\) busca un equilibrio entre el ajuste del modelo y la complejidad. Es decir, cada estadística resume el compromiso entre el ajuste del modelo y la complejidad, aunque con diferentes pesos. Para la mayoría de los conjuntos de datos, recomiendan el mismo modelo, por lo que un analista puede reportar cualquiera o todas las tres estadísticas. Sin embargo, para algunas aplicaciones, llevan a diferentes modelos recomendados. En este caso, el analista necesita confiar más en criterios no basados en datos para la selección del modelo (los cuales siempre son importantes en cualquier aplicación de regresión). 5.6.2 Validación del Modelo La validación del modelo es el proceso de confirmar que nuestro modelo propuesto es apropiado, especialmente a la luz de los propósitos de la investigación. Recuerda el proceso iterativo de formulación y selección de modelos descrito en la Sección 5.1. Una crítica importante a este proceso iterativo es que es culpable de búsqueda de datos, es decir, ajustar un gran número de modelos a un solo conjunto de datos. Como vimos en la Sección 5.2 sobre la búsqueda de datos en la regresión paso a paso, al mirar una gran cantidad de modelos podemos sobreajustar los datos y subestimar la variación natural en nuestra representación. Podemos responder a esta crítica utilizando una técnica llamada validación fuera de muestra. La situación ideal es tener disponibles dos conjuntos de datos, uno para el desarrollo del modelo y otro para la validación del modelo. Inicialmente desarrollamos uno o varios modelos en el primer conjunto de datos. Los modelos desarrollados a partir del primer conjunto de datos se llaman nuestros modelos candidatos. Luego, el rendimiento relativo de los modelos candidatos podría medirse en un segundo conjunto de datos. De esta manera, los datos utilizados para validar el modelo no se ven afectados por los procedimientos utilizados para formular el modelo. Desafortunadamente, rara vez estarán disponibles dos conjuntos de datos para el investigador. Sin embargo, podemos implementar el proceso de validación dividiendo el conjunto de datos en dos submuestras. A estas las llamamos las submuestras de desarrollo del modelo y submuestras de validación, respectivamente. También se conocen como muestras de entrenamiento y prueba, respectivamente. Para ver cómo funciona el proceso en el contexto de la regresión lineal, considera el siguiente procedimiento. Procedimiento de Validación Fuera de Muestra Comienza con un tamaño de muestra de \\(n\\) y divídelo en dos submuestras, llamadas la submuestra de desarrollo del modelo y la submuestra de validación. Sea \\(n_1\\) y \\(n_2\\) el tamaño de cada submuestra. En regresión transversal, realiza esta división usando un mecanismo de muestreo aleatorio. Usa la notación \\(i=1,...,n_1\\) para representar las observaciones de la submuestra de desarrollo del modelo y \\(i=n_1+1,...,n_1+n_2=n\\) para las observaciones de la submuestra de validación. La Figura 5.9 ilustra este procedimiento. Usando la submuestra de desarrollo del modelo, ajusta un modelo candidato al conjunto de datos \\(i=1,...,n_1\\). Usando el modelo creado en el Paso (ii) y las variables explicativas de la submuestra de validación, “predice” las variables dependientes en la submuestra de validación, \\(\\hat{y}_i\\), donde \\(i=n_1+1,...,n_1+n_2\\). (Para obtener estas predicciones, puede que necesites transformar las variables dependientes de nuevo a la escala original.) Evalúa la proximidad de las predicciones a los datos retenidos. Una medida es la suma de errores cuadráticos de predicción \\[\\begin{equation} SSPE = \\sum_{i=n_1+1}^{n_1+n_2} (y_i - \\hat{y}_i)^2 . \\tag{5.7} \\end{equation}\\] Repite los Pasos (ii) a (iv) para cada modelo candidato. Elige el modelo con el menor SSPE. Figure 5.9: Para la validación del modelo, un conjunto de datos de tamaño \\(n\\) se divide aleatoriamente en dos submuestras Existen varias críticas a la SSPE. Primero, es evidente que calcular esta estadística para cada uno de varios modelos candidatos lleva una cantidad considerable de tiempo y esfuerzo. Sin embargo, como ocurre con muchas técnicas estadísticas, esto es simplemente una cuestión de tener disponible software estadístico especializado para realizar los pasos descritos anteriormente. Segundo, dado que la estadística en sí se basa en un subconjunto aleatorio de la muestra, su valor variará de un analista a otro. Esta objeción podría superarse utilizando las primeras \\(n_1\\) observaciones de la muestra. En la mayoría de las aplicaciones, esto no se hace por si hay una relación oculta en el orden de las observaciones. Tercero, y quizás lo más importante, es el hecho de que la elección de los tamaños relativos de los subconjuntos, \\(n_1\\) y \\(n_2\\), no está clara. Varios investigadores recomiendan diferentes proporciones para la asignación. Snee (1977) sugiere que la división de datos no se realice a menos que el tamaño de la muestra sea moderadamente grande, específicamente, \\(n \\geq 2(k+1) + 20\\). Las directrices de Picard y Berk (1990) muestran que cuanto mayor es el número de parámetros a estimar, mayor es la proporción de observaciones necesarias para la submuestra de desarrollo del modelo. Como regla general, para conjuntos de datos con 100 observaciones o menos, usa alrededor del 25-35% de la muestra para validación fuera de muestra. Para conjuntos de datos con 500 o más observaciones, usa el 50% de la muestra para validación fuera de muestra. Hastie, Tibshirani y Friedman (2001) señalan que una división típica es 50% para desarrollo/entrenamiento, 25% para validación, y el 25% restante para una tercera etapa de validación adicional que ellos llaman prueba. Debido a estas críticas, los analistas utilizan varias variantes del proceso básico de validación fuera de muestra. Aunque no existe un procedimiento teóricamente mejor, se acuerda ampliamente que la validación del modelo es una parte importante para confirmar la utilidad de un modelo. 5.6.3 Validación Cruzada La validación cruzada es una técnica de validación de modelos que divide los datos en dos conjuntos disjuntos. La Sección 5.6.2 discutió la validación fuera de muestra, donde los datos se dividieron aleatoriamente en dos subconjuntos, ambos conteniendo un porcentaje considerable de los datos. Otro método popular es la validación cruzada de dejar uno fuera, donde la muestra de validación consiste en una sola observación y la muestra de desarrollo se basa en el resto del conjunto de datos. Especialmente para tamaños de muestra pequeños, una estadística atractiva de validación cruzada de dejar uno fuera es PRESS, la Suma de Cuadrados de Residuos Predichos. Para definir la estadística, considera el siguiente procedimiento donde suponemos que hay un modelo candidato disponible. Procedimiento de Validación PRESS Desde la muestra completa, omite el \\(i\\)-ésimo punto y usa las \\(n-1\\) observaciones restantes para calcular los coeficientes de regresión. Usa los coeficientes de regresión calculados en el primer paso y las variables explicativas para la \\(i\\)-ésima observación para calcular la respuesta predicha, \\(\\hat{y}_{(i)}\\). Esta parte del procedimiento es similar al cálculo de la estadística SSPE con \\(n_1=n-1\\) y \\(n_2=1\\). Ahora, repite (i) y (ii) para \\(i=1,...,n\\). Resumiendo, define \\[\\begin{equation} PRESS = \\sum_{i=1}^{n} (y_i - \\hat{y}_{(i)})^2 . \\tag{5.8} \\end{equation}\\] Al igual que con SSPE, esta estadística se calcula para cada uno de varios modelos competidores. Bajo este criterio, elegimos el modelo con el PRESS más pequeño. Basado en esta definición, la estadística parece ser muy intensiva en cálculos ya que requiere \\(n\\) ajustes de regresión para evaluarla. Para abordar esto, los lectores interesados encontrarán que la Sección 5.10.2 establece \\[\\begin{equation} y_i - \\hat{y}_{(i)} = \\frac{e_i}{1 - h_{ii}} . \\tag{5.9} \\end{equation}\\] Aquí, \\(e_i\\) y \\(h_{ii}\\) representan el \\(i\\)-ésimo residuo y la influencia del ajuste de regresión utilizando el conjunto de datos completo. Esto da lugar a \\[\\begin{equation} PRESS = \\sum_{i=1}^{n} \\left( \\frac{e_i}{1 - h_{ii}} \\right)^2 , \\tag{5.10} \\end{equation}\\] lo cual es una fórmula computacionalmente mucho más fácil. Así, la estadística PRESS es menos intensiva en cálculos que SSPE. Otra ventaja importante de esta estadística, en comparación con SSPE, es que no necesitamos hacer una elección arbitraria sobre los tamaños relativos de los subconjuntos. De hecho, dado que estamos realizando una validación “fuera de muestra” para cada observación, se puede argumentar que este procedimiento es más eficiente, una consideración especialmente importante cuando el tamaño de la muestra es pequeño (por ejemplo, menos de 50 observaciones). Una desventaja es que, dado que el modelo se vuelve a ajustar para cada punto eliminado, PRESS no goza de la apariencia de independencia entre los aspectos de estimación y predicción, a diferencia de SSPE. 5.7 Heterocedasticidad En la mayoría de las aplicaciones de regresión, el objetivo es entender los determinantes de la función de regresión \\(\\mathrm{E~}y_i = \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta = \\mu_i\\). Nuestra capacidad para entender la media está fuertemente influenciada por la cantidad de dispersión respecto a la media, que cuantificamos usando la varianza \\(\\mathrm{E}\\left(y_i - \\mu_i\\right)^2\\). En algunas aplicaciones, como cuando me peso en una balanza, hay relativamente poca variabilidad; las mediciones repetidas dan casi el mismo resultado. En otras aplicaciones, como el tiempo que me toma volar a Nueva York, las mediciones repetidas muestran una variabilidad sustancial y están llenas de incertidumbre inherente. La cantidad de incertidumbre también puede variar de un caso a otro. Denotamos el caso de “variabilidad variable” con la notación \\(\\sigma_i^2 = \\mathrm{E}\\left(y_i - \\mu_i\\right)^2\\). Cuando la variabilidad varía según la observación, esto se conoce como heterocedasticidad, que significa “dispersión diferente”. En contraste, la suposición habitual de variabilidad común (suposición E3/F3 en la Sección 3.2) se llama homocedasticidad, lo que significa “misma dispersión”. Nuestras estrategias de estimación dependen del grado de heterocedasticidad. Para conjuntos de datos con solo una ligera heterocedasticidad, se puede usar mínimos cuadrados para estimar los coeficientes de regresión, tal vez combinado con un ajuste para los errores estándar (descritos en la Sección 5.7.2). Esto se debe a que los estimadores de mínimos cuadrados son insesgados incluso en presencia de heterocedasticidad (ver Propiedad 1 en la Sección 3.2). Sin embargo, con variables dependientes heterocedásticas, el teorema de Gauss-Markov ya no se aplica, por lo que los estimadores de mínimos cuadrados no están garantizados como óptimos. En casos de heterocedasticidad severa, se utilizan estimadores alternativos, siendo los más comunes aquellos basados en transformaciones de la variable dependiente, como se describirá en la Sección 5.7.4. 5.7.1 Detección de Heterocedasticidad Para decidir una estrategia para manejar la posible heterocedasticidad, primero debemos evaluar o detectar su presencia. Para detectar heterocedasticidad de manera gráfica, una buena idea es realizar un ajuste preliminar de regresión de los datos y trazar los residuos frente a los valores ajustados. Para ilustrar, la Figura 5.10 muestra un gráfico de un conjunto de datos ficticio con una variable explicativa donde la dispersión aumenta a medida que aumenta la variable explicativa. Se realizó una regresión por mínimos cuadrados: se calcularon los residuos y los valores ajustados. La Figura 5.11 es un ejemplo de un gráfico de residuos frente a valores ajustados. El ajuste preliminar de regresión elimina muchos de los patrones principales en los datos y deja al ojo libre para concentrarse en otros patrones que pueden influir en el ajuste. Trazamos los residuos frente a los valores ajustados porque los valores ajustados son una aproximación del valor esperado de la respuesta y, en muchas situaciones, la variabilidad crece con la respuesta esperada. Figure 5.10: El área sombreada representa los datos. Figure 5.11: Residuos trazados frente a los valores ajustados para los datos en la Figura 5.10. Más pruebas formales de heterocedasticidad también están disponibles en la literatura de regresión. Por ejemplo, consideremos una prueba de Breusch y Pagan (1980). Específicamente, esta prueba examina la hipótesis alternativa \\(H_a\\): $ y_i = ^2 + _i^{} $, donde \\(\\mathbf{z}_i\\) es un vector conocido de variables y \\(\\boldsymbol \\gamma\\) es un vector de parámetros de dimensión \\(p\\). Así, la hipótesis nula es \\(H_0:~ \\boldsymbol \\gamma = \\mathbf{0}\\), que es equivalente a homocedasticidad, \\(\\mathrm{Var~} y_i = \\sigma^2.\\) Procedimiento para la Prueba de Heterocedasticidad Ajuste un modelo de regresión y calcule los residuos del modelo, \\(e_i\\). Calcule los residuos estandarizados al cuadrado, \\(e_i^{\\ast 2} = e_i^2 / s^2\\). Ajuste un modelo de regresión de \\(e_i^{\\ast 2}\\) sobre \\(\\mathbf{z}_i\\). La estadística de la prueba es \\(LM = \\frac{\\text{Regress~SS}_z}{2}\\), donde \\(Regress~SS_z\\) es la suma de cuadrados de la regresión del ajuste del modelo en el paso (iii). Rechace la hipótesis nula si \\(LM\\) excede un percentil de una distribución chi-cuadrado con \\(p\\) grados de libertad. El percentil es uno menos el nivel de significancia de la prueba. Aquí usamos \\(LM\\) para denotar la estadística de la prueba porque Breusch y Pagan la derivaron como una estadística de multiplicador de Lagrange; consulte Breusch y Pagan (1980) para más detalles. 5.7.2 Errores Estándar Consistentes con Heterocedasticidad Para conjuntos de datos con solo una leve heterocedasticidad, una estrategia sensata es emplear estimadores de mínimos cuadrados de los coeficientes de regresión y ajustar el cálculo de errores estándar para tener en cuenta la heterocedasticidad. En la Sección 3.2 sobre propiedades, vimos que los coeficientes de regresión por mínimos cuadrados pueden escribirse como \\(\\mathbf{b} = \\sum_{i=1}^n \\mathbf{w}_i y_i,\\) donde \\(\\mathbf{w}_i = \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1} \\mathbf{x}_i\\). Así, con \\(\\sigma_i^2 = \\mathrm{Var~} y_i\\), tenemos \\[\\begin{equation} \\mathrm{Var~}\\mathbf{b} = \\sum_{i=1}^n \\mathbf{w}_i \\mathbf{w}_i^{\\prime} \\sigma_i^2 = \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1} \\left( \\sum_{i=1}^n \\sigma_i^2 \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\right) \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1}. \\tag{5.11} \\end{equation}\\] Esta cantidad es conocida excepto por \\(\\sigma_i^2\\). Podemos calcular los residuos usando los coeficientes de regresión por mínimos cuadrados como \\(e_i = y_i - \\mathbf{x}_i^{\\prime} \\mathbf{b}\\). Con estos, podemos definir la estimación empírica, o robusta, de la matriz varianza-covarianza como \\[ \\widehat{\\mathrm{Var~}\\mathbf{b}} = \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1} \\left( \\sum_{i=1}^n e_i^2 \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\right) \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1}. \\] Los correspondientes errores estándar “consistentes con heterocedasticidad” son \\[\\begin{equation} se_r(b_j) = \\sqrt{(j+1)^{\\text{er}}~ \\text{elemento diagonal de }\\widehat{\\mathrm{Var~}\\mathbf{b}}}. \\tag{5.12} \\end{equation}\\] La lógica detrás de este estimador es que cada residual al cuadrado, \\(e_i^2\\), puede ser una mala estimación de \\(\\sigma_i^2\\). Sin embargo, nuestro interés es estimar una (ponderada) suma de varianzas en la ecuación (5.11); estimar la suma es una tarea mucho más fácil que estimar cualquier estimación de varianza individual. Los errores estándar robustos, o consistentes con heterocedasticidad, están ampliamente disponibles en paquetes de software estadístico. Aquí, también verá definiciones alternativas de residuos empleados, como en la Sección 5.3.1. Si su paquete estadístico ofrece opciones, el estimador robusto que utiliza residuos studentizados es generalmente preferido. 5.7.3 Mínimos Cuadrados Ponderados Los estimadores de mínimos cuadrados son menos útiles para conjuntos de datos con heterocedasticidad severa. Una estrategia es usar una variación de la estimación por mínimos cuadrados mediante el ponderado de las observaciones. La idea es que, al minimizar la suma de errores cuadrados usando datos heterocedásticos, la variabilidad esperada de algunas observaciones es menor que la de otras. Intuitivamente, parece razonable que, cuanto menor es la variabilidad de la respuesta, más confiable es esa respuesta y mayor peso debería recibir en el procedimiento de minimización. Los mínimos cuadrados ponderados son una técnica que tiene en cuenta esta “variabilidad variable”. Específicamente, usamos los supuestos E1, E2 y E4 de la Sección 3.2.3, con E3 reemplazado por E \\(\\varepsilon_i = 0\\) y \\(\\text{Var} \\varepsilon_i = \\sigma^2 / w_i\\), de modo que la variabilidad es proporcional a un peso conocido \\(w_i\\). Por ejemplo, si la unidad de análisis \\(i\\) representa una entidad geográfica como un estado, podrías usar el número de personas en el estado como peso. O, si \\(i\\) representa una empresa, podrías usar los activos de la empresa para la variable de ponderación. Valores mayores de \\(w_i\\) indican una variable de respuesta más precisa a través de una menor variabilidad. En aplicaciones actuariales, se usan pesos para tener en cuenta una exposición, como el monto de la prima de seguro, el número de empleados, el tamaño de la nómina, el número de vehículos asegurados, etc. (una discusión más detallada está en el Capítulo 18). Este modelo puede convertirse fácilmente en el problema de “mínimos cuadrados ordinarios” multiplicando todas las variables de regresión por \\(\\sqrt{w_i}\\). Es decir, si definimos \\(y_i^{\\ast} = y_i \\times \\sqrt{w_i}\\) y \\(x_{ij}^{\\ast} = x_{ij} \\times \\sqrt{w_i}\\), entonces, a partir del supuesto E1, tenemos \\[ \\begin{array}{ll} y_i^{\\ast} &amp; = y_i \\times \\sqrt{w_i} = \\left( \\beta_0 x_{i0} + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik} + \\varepsilon_i \\right) \\sqrt{w_i} \\\\ &amp;= \\beta_0 x_{i0}^{\\ast} + \\beta_1 x_{i1}^{\\ast} + \\ldots + \\beta_k x_{ik}^{\\ast} + \\varepsilon_i^{\\ast} \\end{array} \\] donde \\(\\varepsilon_i^{\\ast} = \\varepsilon_i \\times \\sqrt{w_i}\\) tiene una varianza homocedástica \\(\\sigma^2\\). Así, con las variables reescaladas, toda la inferencia puede proceder como antes. Este trabajo ha sido automatizado en paquetes estadísticos donde el usuario simplemente especifica los pesos \\(w_i\\) y el paquete hace el resto. En términos de álgebra de matrices, este procedimiento se puede llevar a cabo definiendo una matriz de pesos \\(n \\times n\\) \\(\\mathbf{W} = \\text{diag}(w_i)\\), de modo que el elemento diagonal \\(i\\)-ésimo de \\(\\mathbf{W}\\) sea \\(w_i\\). Ampliando la ecuación (3.14), por ejemplo, las estimaciones de mínimos cuadrados ponderados se pueden expresar como \\[\\begin{equation} \\mathbf{b}_{WLS} = \\left( \\mathbf{X}^{\\prime} \\mathbf{W} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{W} \\mathbf{y}. \\tag{5.13} \\end{equation}\\] Discuciones adicionales sobre la estimación de mínimos cuadrados ponderados se presentarán en la Sección 15.1.1. 5.7.4 Transformaciones Otro enfoque que maneja la heterocedasticidad severa, introducido en la Sección 1.3, es transformar la variable dependiente, típicamente con una transformación logarítmica de la forma \\(y^{\\ast} = \\ln y\\). Como vimos en la Sección 1.3, las transformaciones pueden servir para “reducir” la dispersión de los datos y simetrizar una distribución. A través de un cambio de escala, una transformación también cambia la variabilidad, potencialmente alterando un conjunto de datos heterocedástico en uno homocedástico. Esta es tanto una fortaleza como una limitación del enfoque de transformación: una transformación afecta simultáneamente tanto la distribución como la heterocedasticidad. Las transformaciones de potencia, como la transformación logarítmica, son más útiles cuando la variabilidad de los datos crece con la media. En este caso, la transformación servirá para “reducir” los datos a una escala que parece ser homocedástica. Por el contrario, dado que las transformaciones son funciones monótonas, no ayudarán con patrones de variabilidad que son no monótonos. Además, si tus datos son razonablemente simétricos pero heterocedásticos, una transformación no será útil porque cualquier elección que mitigue la heterocedasticidad sesgará la distribución. Cuando los datos no son positivos, es común agregar una constante a cada observación para que todas las observaciones sean positivas antes de la transformación. Por ejemplo, la transformación \\(\\ln(1+y)\\) acomoda la presencia de ceros. También se puede multiplicar por una constante para que se mantengan las unidades originales aproximadas. Por ejemplo, la transformación \\(100 \\ln(1 + y/100)\\) puede aplicarse a datos porcentuales donde a veces aparecen porcentajes negativos. Nuestras discusiones sobre transformaciones se han centrado en transformar variables dependientes. Como se señaló en la Sección 3.5, también es posible transformar variables explicativas. Esto se debe a que los supuestos de regresión condicionan las variables explicativas (Sección 3.2.3). Algunos analistas prefieren transformar variables para aproximarse a la normalidad, considerando las distribuciones normales multivariadas como una base para el análisis de regresión. Otros son reacios a transformar variables explicativas debido a las dificultades para interpretar los modelos resultantes. El enfoque aquí es usar transformaciones que sean fácilmente interpretables, como las introducidas en la Sección 3.5. Otras transformaciones son ciertamente candidatas para incluir en un modelo seleccionado, pero deben proporcionar dividendos sustanciales en términos de ajuste o poder predictivo si son difíciles de comunicar. 5.8 Lectura Adicional y Referencias Long y Ervin (2000) reúnen pruebas convincentes sobre el uso de estimadores alternativos consistentes con la heterocedasticidad para los errores estándar que tienen un mejor rendimiento en muestras finitas que las versiones clásicas. Las propiedades de gran muestra de los estimadores empíricos han sido establecidas por Eicker (1967), Huber (1967) y White (1980) en el caso de la regresión lineal. Para el caso de la regresión lineal, MacKinnon y White (1985) sugieren alternativas que proporcionan mejores propiedades en muestras pequeñas. Para muestras pequeñas, la evidencia se basa en (1) el sesgo de los estimadores, (2) su motivación como estimadores jackknife y (3) su rendimiento en estudios de simulación. Otros métodos para medir la colinealidad basados en conceptos de álgebra de matrices que involucran valores propios, como los números de condición y los índices de condición, son utilizados por algunos analistas. Consulta a Belsey, Kuh y Welsch (1980) para un tratamiento sólido de la colinealidad y los diagnósticos de regresión. Hocking (2003) proporciona lecturas adicionales sobre colinealidad y componentes principales. Consulta a Carroll y Ruppert (1988) para más discusiones sobre transformaciones en la regresión. Hastie, Tibshirani y Friedman (2001) ofrecen una discusión avanzada sobre problemas de selección de modelos, centrándose en los aspectos predictivos de los modelos en el lenguaje del aprendizaje automático. Referencias del Capítulo Belseley, David A., Edwin Kuh and Roy E. Welsch (1980). Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. Wiley, New York. Bendel, R. B. and Afifi, A. A. (1977). Comparison of stopping rules in forward “stepwise” regression. Journal of the American Statistical Association 72, 46-53. Box, George E. P. (1980). Sampling and Bayes inference in scientific modeling and robustness (with discussion). Journal of the Royal Statistical Society, Series A, 143, 383-430. Breusch, T. S. and A. R. Pagan (1980). The Lagrange multiplier test and its applications to model specification in econometrics. Review of Economic Studies, 47, 239-53. Carroll, Raymond J. and David Ruppert (1988). Transformation and Weighting in Regression, Chapman-Hall. Eicker, F. (1967). Limit theorems for regressions with unequal and dependent errors. Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability 1, LeCam, L. M. and J. Neyman, editors, University of California Press, pp, 59-82. Hadi, A. S. (1988). Diagnosing collinearity-influential observations. Computational Statistics and Data Analysis 7, 143-159. Hastie, Trevor, Robert Tibshirani and Jerome Friedman (2001). The Elements of Statistical Learning: Data Mining, Inference and Prediction. Springer-Verlag, New York. Hocking, Ronald R. (2003). Methods and Applications of Linear Models: Regression and the Analysis of Variance. Wiley, New York. Huber, P. J. (1967). The behaviour of maximum likelihood estimators under non-standard conditions. Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability 1, LeCam, L. M. and Neyman, J. editors, University of California Press, pp, 221-33. Long, J.S. and L.H. Ervin (2000). Using heteroscedasticity consistent standard errors in the linear regression model. American Statistician 54, 217-224. MacKinnon, J.G. and H. White (1985). Some heteroskedasticity consistent covariance matrix estimators with improved finite sample properties. Journal of Econometrics 29, 53-57. Mason, R. L. and Gunst, R. F. (1985). Outlier-induced collinearities. Technometrics 27, 401-407. Picard, R. R. and Berk, K. N. (1990). Data splitting. The American Statistician 44, 140-147. Rencher, A. C. and Pun, F. C. (1980). Inflation of \\(R^2\\) in best subset regression. Technometrics 22, 49-53. Snee, R. D. (1977). Validation of regression models. Methods and examples. Technometrics 19, 415-428. 5.9 Ejercicios 5.1. Estás realizando una regresión con una variable explicativa, por lo que considera el modelo de regresión lineal básico \\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\). Muestra que el apalancamiento \\(i\\)-ésimo se puede simplificar a \\[ h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\overline{x})^2}{(n-1) s_x^2}. \\] Muestra que \\(\\overline{h} = 2 / n\\). Supón que \\(h_{ii} = 6/n\\). ¿Cuántas desviaciones estándar está \\(x_i\\) alejado (ya sea por encima o por debajo) de la media? 5.2. Considera los resultados de una regresión usando una variable explicativa con \\(n=3\\) observaciones. Los residuos y los apalancamientos son: \\[ \\small{ \\begin{array}{l|ccc} \\hline i &amp; 1 &amp; 2 &amp; 3 \\\\ \\hline \\text{Residuos } e_i &amp; 3.181 &amp; -6.362 &amp; 3.181 \\\\ \\text{Apalancamientos } h_{ii} &amp; 0.8333 &amp; 0.3333 &amp; 0.8333 \\\\ \\hline \\end{array} } \\] Calcula el estadístico \\(PRESS\\). 5.3. Expectativas de Vida Nacionales. Continuamos con el análisis iniciado en los Ejercicios 1.7, 2.22, 3.6 y 4.7. El enfoque de este ejercicio es la selección de variables. Comienza con los datos de \\(n=185\\) países de todo el mundo que tienen expectativas de vida válidas (no faltantes). Grafica la expectativa de vida frente al producto interno bruto y los gastos privados en salud. A partir de estos gráficos, describe por qué es deseable utilizar transformaciones logarítmicas, lnGDP y lnHEALTH, respectivamente. También grafica la expectativa de vida frente a lnGDP y lnHEALTH para confirmar tu intuición. Utiliza un algoritmo de regresión paso a paso para ayudarte a seleccionar un modelo. No consideres las variables RESEARCHERS, SMOKING y FEMALEBOSS ya que tienen muchos valores faltantes. Para las variables restantes, utiliza solo las observaciones sin valores faltantes. Hazlo dos veces, con y sin la variable categórica REGION. Regresa al conjunto de datos completo de \\(n=185\\) países y ejecuta un modelo de regresión usando FERTILITY, PUBLICEDUCATION y lnHEALTH como variables explicativas. c(i). Proporciona histogramas de residuos estandarizados y apalancamientos. c(ii). Identifica el residuo estandarizado y el apalancamiento asociados con Lesoto, anteriormente Basutolandia, un reino rodeado por Sudáfrica. ¿Es esta observación un valor atípico, un punto de alto apalancamiento, o ambos? c(iii). Repite la regresión sin Lesoto. Cita cualquier diferencia en los coeficientes estadísticos entre este modelo y el del apartado c(i). 5.4. Seguro de Vida a Término. Continuamos con nuestro estudio de la Demanda de Seguro de Vida a Término de los Capítulos 3 y 4. Específicamente, examinamos la Encuesta de Finanzas del Consumidor (SCF) de 2004, una muestra representativa a nivel nacional que contiene información extensa sobre activos, pasivos, ingresos y características demográficas de los encuestados (potenciales clientes de EE.UU.). Estudiamos una muestra aleatoria de 500 familias con ingresos positivos. De la muestra de 500, inicialmente consideramos una submuestra de \\(n=275\\) familias que compraron seguro de vida a término. Considera una regresión lineal de LNINCOME, EDUCATION, NUMHH, MARSTAT, AGE y GENDER sobre LNFACE. Colinealidad. No todas las variables resultaron ser estadísticamente significativas. Para investigar una posible explicación, calcula los factores de inflación de la varianza. a(i). Explica brevemente la idea de colinealidad y un factor de inflación de la varianza. a(ii). ¿Qué constituye un gran factor de inflación de la varianza? a(iii). Si se detecta un gran factor de inflación de la varianza, ¿qué posibles acciones podemos tomar para abordar este aspecto de los datos? a(iv). Complementa las estadísticas de los factores de inflación de la varianza con una tabla de correlaciones de las variables explicativas. Basado en estas estadísticas, ¿es la colinealidad un problema con este modelo ajustado? ¿Por qué o por qué no? Puntos Inusuales. A veces, un ajuste deficiente del modelo puede deberse a puntos inusuales. b(i). Define la idea de apalancamiento para una observación. b(ii). Para este modelo ajustado, da reglas generales para identificar puntos con apalancamiento inusual. Identifica cualquier punto inusual. b(iii). Un analista está preocupado por los valores de apalancamiento de este modelo ajustado y sugiere usar FACE como la variable dependiente en lugar de LNFACE. Describe cómo cambiarían los valores de apalancamiento usando esta variable dependiente alternativa. Análisis de Residuos. Podemos aprender cómo mejorar los ajustes del modelo a partir de los análisis de residuos. c(i). Proporciona un gráfico de residuos frente a valores ajustados. ¿Qué esperamos aprender de este tipo de gráfico? ¿Este gráfico muestra alguna inadecuación del modelo? c(ii). Proporciona un gráfico \\(qq\\) de residuos. ¿Qué esperamos aprender de este tipo de gráfico? ¿Este gráfico muestra alguna inadecuación del modelo? c(iii). Proporciona un gráfico de residuos frente a apalancamientos. ¿Qué esperamos aprender de este tipo de gráfico? ¿Este gráfico muestra alguna inadecuación del modelo? Regresión Paso a Paso. Ejecuta un algoritmo de regresión paso a paso. Supón que este algoritmo sugiere un modelo utilizando LNINCOME, EDUCATION, NUMHH y GENDER como variables explicativas para predecir la variable dependiente LNFACE. d(i). ¿Cuál es el propósito de la regresión paso a paso? d(ii). Describe dos desventajas importantes de los algoritmos de regresión paso a paso. 5.10 Suplementos Técnicos para el Capítulo 5 5.10.1 Matriz de Proyección Matriz de Sombrero. Definimos la matriz de sombrero como \\(\\mathbf{H} = \\mathbf{X(X}^{\\prime}\\mathbf{X)}^{-1} \\mathbf{X}^{\\prime}\\), de manera que \\(\\mathbf{\\hat{y}} = \\mathbf{X b} = \\mathbf{Hy}\\). De esto, se dice que la matriz \\(\\mathbf{H}\\) proyecta el vector de respuestas \\(\\mathbf{y}\\) sobre el vector de valores ajustados \\(\\mathbf{\\hat{y}}\\). Dado que \\(\\mathbf{H}^{\\prime} = \\mathbf{H}\\), la matriz de sombrero es simétrica. Además, también es una matriz idempotente debido a la propiedad de que \\(\\mathbf{HH} = \\mathbf{H}\\). Para ver esto, tenemos que \\[ \\begin{array}{ll} \\mathbf{HH} &amp;= \\mathbf{(X(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathbf{)(X(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathbf{)} \\\\ &amp;= \\mathbf{X(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{(\\mathbf{X}^{\\prime}X)(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime} = \\mathbf{X(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime} = \\mathbf{H}. \\end{array} \\] De manera similar, es fácil verificar que \\(\\mathbf{I-H}\\) es idempotente. Dado que \\(\\mathbf{H}\\) es idempotente, a partir de algunos resultados en álgebra de matrices, es sencillo mostrar que \\[ \\sum_{i=1}^{n} h_{ii} = k + 1. \\] Como se discutió en la Sección 5.4.1, usamos nuestros límites y el apalancamiento promedio, \\(\\bar{h} = (k + 1)/n\\), para ayudar a identificar observaciones con apalancamiento inusualmente alto. Varianza de los Residuos. Usando la ecuación del modelo \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\), podemos expresar el vector de residuos como \\[\\begin{equation} \\mathbf{e} = \\mathbf{y} - \\mathbf{\\hat{y}} = \\mathbf{y - Hy} = \\mathbf{(I-H)(X \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon})} = \\mathbf{(I-H) \\boldsymbol{\\varepsilon}}. \\tag{5.14} \\end{equation}\\] La última igualdad se debe al hecho de que \\(\\mathbf{(I-H)X} = \\mathbf{X - HX} = \\mathbf{X - X} = \\mathbf{0}\\). Usando \\(\\text{Var~} \\boldsymbol{\\varepsilon} = \\sigma^2 \\mathbf{I}\\), tenemos \\[ \\begin{array}{ll} \\text{Var } \\mathbf{e} &amp;= \\text{Var }\\left[ \\mathbf{(I-H)\\boldsymbol{\\varepsilon}} \\right] = \\mathbf{(I-H)} \\text{Var } \\boldsymbol{\\varepsilon} \\mathbf{(I-H)} \\\\ &amp;= \\sigma^2 \\mathbf{(I-H)} \\mathbf{I} \\mathbf{(I-H)} = \\sigma^2 \\mathbf{(I-H)}. \\end{array} \\] La última igualdad proviene del hecho de que \\(\\mathbf{I-H}\\) es idempotente. Así, tenemos que \\[\\begin{equation} \\text{Var } e_i = \\sigma^2 (1 - h_{ii}) \\text{ y Cov } (e_i, e_j) = -\\sigma^2 h_{ij}. \\tag{5.15} \\end{equation}\\] Así, aunque los errores verdaderos \\(\\boldsymbol{\\varepsilon}\\) son no correlacionados, hay una pequeña correlación negativa entre los residuos \\(\\mathbf{e}\\). Dominio del Error en el Residuo. Examinando la fila \\(i\\)-ésima de la ecuación (5.14), tenemos que el residuo \\(i\\)-ésimo \\[\\begin{equation} e_i = \\varepsilon_i - \\sum_{j=1}^{n} h_{ij} \\varepsilon_j \\tag{5.16} \\end{equation}\\] se puede expresar como una combinación lineal de errores independientes. La relación \\(\\mathbf{H} = \\mathbf{HH}\\) da lugar a \\[\\begin{equation} h_{ii} = \\sum_{j=1}^{n} h_{ij}^2. \\tag{5.17} \\end{equation}\\] Dado que \\(h_{ii}\\) es, en promedio, \\((k + 1)/n\\), esto indica que cada \\(h_{ij}\\) es pequeño en relación con 1. Así, al interpretar la ecuación (5.16), decimos que la mayor parte de la información en \\(e_i\\) se debe a \\(\\varepsilon_i\\). Correlaciones con los Residuos. Primero define \\(\\mathbf{x}^j = (x_{1j}, x_{2j}, \\dots, x_{nj})^{\\prime}\\) como la columna que representa la \\(j\\)-ésima variable. Con esta notación, podemos particionar la matriz de variables explicativas como \\(\\mathbf{X} = \\left( \\mathbf{x}^{0}, \\mathbf{x}^{1}, \\dots, \\mathbf{x}^{k} \\right)\\). Ahora, examinando la columna \\(j\\)-ésima de la relación \\(\\mathbf{(I-H)X} = \\mathbf{0}\\), tenemos \\(\\mathbf{(I-H)x}^{j} = \\mathbf{0}\\). Con \\(\\mathbf{e} = \\mathbf{(I-H) \\boldsymbol{\\varepsilon}}\\), esto da \\[ \\mathbf{e}^{\\prime} \\mathbf{x}^{j} = \\boldsymbol{\\varepsilon}^{\\prime} \\mathbf{(I-H)x}^{j} = 0, \\] para \\(j = 0, 1, \\ldots, k.\\) Este resultado tiene varias implicaciones. Si el intercepto está en el modelo, entonces \\(\\mathbf{x}^{0} = (1, 1, \\ldots, 1)^{\\prime}\\) es un vector de unos. Aquí, \\(\\mathbf{e}^{\\prime} \\mathbf{x}^{0} = 0\\) significa que \\(\\sum_{i=1}^{n} e_i = 0\\) o, el residuo promedio es cero. Además, dado que \\(\\mathbf{e}^{\\prime} \\mathbf{x}^{j} = 0\\), es fácil verificar que la correlación muestral entre \\(\\mathbf{e}\\) y \\(\\mathbf{x}^{j}\\) es cero. En la misma línea, también tenemos que \\(\\mathbf{e}^{\\prime} \\mathbf{\\hat{y}} = \\mathbf{e}^{\\prime} \\mathbf{(I-H)Xb} = \\mathbf{0}\\). Así, usando el mismo argumento que antes, la correlación muestral entre \\(\\mathbf{e}\\) y \\(\\mathbf{\\hat{y}}\\) es cero. Coeficiente de Correlación Múltiple. Para un ejemplo de una correlación diferente de cero, considera \\(r(\\mathbf{y, \\hat{y}})\\), la correlación muestral entre \\(\\mathbf{y}\\) y \\(\\mathbf{\\hat{y}}\\). Dado que \\(\\mathbf{(I-H)x}^{0} = \\mathbf{0}\\), tenemos \\(\\mathbf{x}^{0} = \\mathbf{Hx}^{0}\\) y, por lo tanto, \\(\\mathbf{\\hat{y}}^{\\prime} \\mathbf{x}^{0} = \\mathbf{y}^{\\prime} \\mathbf{Hx}^{0} = \\mathbf{y^{\\prime} x}^{0}\\). Asumiendo que \\(\\mathbf{x}^{0} = (1, 1, \\ldots, 1)^{\\prime}\\), esto significa que \\(\\sum_{i=1}^{n} \\hat{y}_i = \\sum_{i=1}^{n} y_i\\), por lo que el valor promedio ajustado es \\(\\bar{y}\\). \\[ r(\\mathbf{y, \\hat{y}}) = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(\\hat{y}_i - \\bar{y})}{(n-1) s_y s_{\\hat{y}}}. \\] Recuerda que \\((n-1) s_y^2 = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = Total ~SS\\) y \\((n-1) s_{\\hat{y}}^2 = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2 = Regress ~SS\\). Además, con \\(\\mathbf{x}^0 = (1, 1, \\ldots, 1)^{\\prime}\\), \\[ \\begin{array}{ll} \\sum_{i=1}^{n} (y_i - \\bar{y})(\\hat{y}_i - \\bar{y}) &amp;= (\\mathbf{y} - \\bar{y} \\mathbf{x}^0)^{\\prime} (\\mathbf{\\hat{y}} - \\bar{y} \\mathbf{x}^0) = \\mathbf{y}^{\\prime} \\mathbf{\\hat{y}} - \\bar{y}^2 \\mathbf{x}^{0 \\prime} \\mathbf{x}^0 \\\\ &amp;= \\mathbf{y}^{\\prime} \\mathbf{Xb} - n \\bar{y}^2 = Regress ~SS. \\end{array} \\] Esto da \\[\\begin{equation} r(\\mathbf{y, \\hat{y}}) = \\frac{Regress ~SS}{\\sqrt{\\left( Total ~SS \\right) \\left( Regress ~SS \\right)}} = \\sqrt{\\frac{Regress ~SS}{Total ~SS}} = \\sqrt{R^2}. \\tag{5.18} \\end{equation}\\] Es decir, el coeficiente de determinación se puede interpretar como la raíz cuadrada de la correlación entre las respuestas observadas y las ajustadas. 5.10.2 Estadísticas Leave-One-Out Notación. Para probar la sensibilidad de las cantidades de regresión, hay varias estadísticas de interés que se basan en la noción de “dejar fuera” u omitir una observación. Con este fin, la notación de subíndice \\((i)\\) significa dejar fuera la \\(i\\)-ésima observación. Por ejemplo, omitir la fila de variables explicativas \\(\\mathbf{x}_i^{\\prime} = (x_{i0}, x_{i1}, \\dots, x_{ik})\\) de \\(\\mathbf{X}\\) da lugar a \\(\\mathbf{X}_{(i)}\\), una matriz de \\((n-1) \\times (k+1)\\) de variables explicativas. De manera similar, \\(\\mathbf{y}_{(i)}\\) es un vector de \\((n-1) \\times 1\\), basado en eliminar la \\(i\\)-ésima fila de \\(\\mathbf{y}\\). Resultado Básico de Matrices. Supongamos que \\(\\mathbf{A}\\) es una matriz invertible de \\(p \\times p\\) y \\(\\mathbf{z}\\) es un vector de \\(p \\times 1\\). El siguiente resultado de álgebra de matrices proporciona una herramienta importante para entender las estadísticas leave-one-out en el análisis de regresión lineal. \\[\\begin{equation} \\left( \\mathbf{A - zz}^{\\prime} \\right)^{-1} = \\mathbf{A}^{-1} + \\frac{\\mathbf{A}^{-1} \\mathbf{zz}^{\\prime} \\mathbf{A}^{-1}}{1 - \\mathbf{z}^{\\prime} \\mathbf{A}^{-1} \\mathbf{z}}. \\tag{5.19} \\end{equation}\\] Para verificar este resultado, simplemente multiplica \\(\\mathbf{A - zz}^{\\prime}\\) por el lado derecho de la ecuación para obtener \\(\\mathbf{I}\\), la matriz identidad. Vector de Coeficientes de Regresión. Al omitir la \\(i\\)-ésima observación, nuestro nuevo vector de coeficientes de regresión es \\(\\mathbf{b}_{(i)} = \\left( \\mathbf{X}_{(i)}^{\\prime} \\mathbf{X}_{(i)} \\right)^{-1} \\mathbf{X}_{(i)}^{\\prime} \\mathbf{y}_{(i)}.\\) Una expresión alternativa para \\(\\mathbf{b}_{(i)}\\) que resulta ser más simple de calcular es \\[\\begin{equation} \\mathbf{b}_{(i)} = \\mathbf{b} - \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}}. \\tag{5.20} \\end{equation}\\] Para verificar esto, primero usa el resultado de inversión de matrices con \\(\\mathbf{A} = \\mathbf{X}^{\\prime} \\mathbf{X}\\) y \\(\\mathbf{z} = \\mathbf{x}_i\\) para obtener \\[ \\left( \\mathbf{X}_{(i)}^{\\prime} \\mathbf{X}_{(i)} \\right)^{-1} = (\\mathbf{X}^{\\prime} \\mathbf{X} - \\mathbf{x}_i \\mathbf{x}_i^{\\prime})^{-1} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} + \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1}}{1 - h_{ii}}, \\] donde, a partir del resultado de apalancamiento, tenemos \\(h_{ii} = \\mathbf{x}_i^{\\prime} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{x}_i\\). Multiplicando cada lado por \\[ \\mathbf{X}_{(i)}^{\\prime} \\mathbf{y}_{(i)} = \\mathbf{X}^{\\prime} \\mathbf{y} - \\mathbf{x}_i y_i \\] da $$ \\[\\begin{array}{ll} \\mathbf{b}_{(i)} &amp;= \\left( \\mathbf{X}_{(i)}^{\\prime} \\mathbf{X}_{(i)} \\right)^{-1} \\mathbf{X}_{(i)}^{\\prime} \\mathbf{y}_{(i)} \\\\ &amp;= \\left( (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} + \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1}}{1 - h_{ii}} \\right) \\left( \\mathbf{X}^{\\prime} \\mathbf{y} - \\mathbf{x}_i y_i \\right) \\\\ &amp;= \\mathbf{b} - \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i y_i + \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\mathbf{b} - \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i y_i}{1 - h_{ii}} \\\\ &amp;= \\mathbf{b} - \\frac{\\left( 1 - h_{ii} \\right) \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i y_i - \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\mathbf{b} - \\left( \\mathbf{ X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i h_{ii} y_i}{1 - h_{ii}} \\\\ &amp;= \\mathbf{b} - \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i y_i - \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\mathbf{b}}{1 - h_{ii}} \\\\ &amp;= \\mathbf{b} - \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}}. \\end{array}\\] $$ Esto establece el resultado. Distancia de Cook. Para medir el efecto, o influencia, de omitir la \\(i\\)-ésima observación, Cook examinó la diferencia entre los valores ajustados con y sin la observación. Definimos la Distancia de Cook como \\[ D_i = \\frac{\\left( \\mathbf{\\hat{y} - \\hat{y}}_{(i)} \\right)^{\\prime} \\left( \\mathbf{\\hat{y} - \\hat{y}}_{(i)} \\right)}{(k+1) s^2} \\] donde \\(\\mathbf{\\hat{y}}_{(i)} = \\mathbf{Xb}_{(i)}\\) es el vector de valores ajustados calculado omitiendo el punto \\(i\\)-ésimo. Usando la ecuación (5.20) y \\(\\mathbf{\\hat{y}} = \\mathbf{Xb}\\), una expresión alternativa para la Distancia de Cook es \\[ \\begin{array}{ll} D_i &amp;= \\frac{\\left( \\mathbf{b - b}_{(i)} \\right)^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right) \\left( \\mathbf{b - b}_{(i)} \\right)}{(k+1) s^2} \\\\ &amp;= \\frac{e_i^2}{(1 - h_{ii})^2} \\frac{\\mathbf{x}_i^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right) \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i}{(k+1) s^2} \\\\ &amp; = \\frac{e_i^2}{(1 - h_{ii})^2} \\frac{h_{ii}}{(k+1) s^2} \\\\ &amp;= \\left( \\frac{e_i}{s \\sqrt{1 - h_{ii}}} \\right)^2 \\frac{h_{ii}}{(k+1) (1 - h_{ii})}. \\end{array} \\] Este resultado no solo es útil computacionalmente, sino que también sirve para descomponer la estadística en la parte debida al residuo estandarizado, \\((e_i/(s \\sqrt{1 - h_{ii}}))^2\\), y en la parte debida al apalancamiento, \\(\\frac{h_{ii}}{(k+1) (1 - h_{ii})}\\). Residuo Leave-One-Out. El residuo leave-one-out se define como \\(e_{(i)} = y_i - \\mathbf{x}_i^{\\prime} \\mathbf{b}_{(i)}\\). Se usa en el cálculo de la estadística PRESS, descrita en la Sección 5.6.3. Una expresión computacional simple es \\(e_{(i)} = \\frac{e_i}{1 - h_{ii}}\\). Para verificar esto, usa la ecuación (5.20) para obtener \\[ e_{(i)} = y_i - \\mathbf{x}_i^{\\prime} \\mathbf{b}_{(i)} = y_i - \\mathbf{x}_i^{\\prime} \\left( \\mathbf{b} - \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}} \\right) \\] \\[ = e_i + \\frac{\\mathbf{x}_i \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}} = e_i + \\frac{h_{ii} e_i}{1 - h_{ii}} = \\frac{e_i}{1 - h_{ii}}. \\] Estimación de Varianza Leave-One-Out. La estimación leave-one-out de la varianza se define como \\[ s_{(i)}^2 = \\frac{((n - 1) - (k + 1))^{-1} \\sum_{j \\ne i} \\left( y_j - \\mathbf{x}_j^{\\prime} \\mathbf{b}_{(i)} \\right)^2}{(n - 1) - (k + 1)}. \\] Se usa en la definición del residuo estandarizado, definido en la Sección 5.3.1. Una expresión computacional simple está dada por \\[\\begin{equation} s_{(i)}^2 = \\frac{(n - (k + 1)) s^2 - \\frac{e_i^2}{1 - h_{ii}}}{(n - 1) - (k + 1)}. \\tag{5.21} \\end{equation}\\] Para ver esto, primero nota que, a partir de la ecuación (5.14), tenemos \\(\\mathbf{He} = \\mathbf{H(I - H) \\boldsymbol{\\varepsilon}} = \\mathbf{0}\\), porque \\(\\mathbf{H} = \\mathbf{HH}\\). En particular, desde la fila \\(i\\)-ésima de \\(\\mathbf{He} = \\mathbf{0}\\), tenemos \\(\\sum_{j=1}^{n} h_{ij} e_j = 0\\). Ahora, usando las ecuaciones (5.17) y (5.20), tenemos \\[ \\begin{array}{ll} \\sum_{j \\ne i} \\left( y_j - \\mathbf{x}_j^{\\prime} \\mathbf{b}_{(i)} \\right)^2 &amp;= \\sum_{j=1}^{n} \\left( y_j - \\mathbf{x}_j^{\\prime} \\mathbf{b}_{(i)} \\right)^2 - \\left( y_i - \\mathbf{x}_i^{\\prime} \\mathbf{b}_{(i)} \\right)^2 \\\\ &amp;= \\sum_{j=1}^{n} \\left( y_j - \\mathbf{x}_j^{\\prime} \\mathbf{b} + \\frac{\\mathbf{x}_j^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}} \\right) - e_{(i)}^2 \\\\ &amp;= \\sum_{j=1}^{n} \\left( e_j + \\frac{h_{ij} e_i}{1 - h_{ii}} \\right)^2 - \\frac{e_i^2}{(1 - h_{ii})^2} \\\\ &amp;= \\sum_{j=1}^{n} e_j^2 + 0 + \\frac{e_i^2}{(1 - h_{ii})^2} h_{ii} - \\frac{e_i^2}{(1 - h_{ii})^2} \\\\ &amp;= \\sum_{j=1}^{n} e_j^2 - \\frac{e_i^2}{1 - h_{ii}} = (n - (k + 1)) s^2 - \\frac{e_i^2}{1 - h_{ii}}. \\end{array} \\] Esto establece la ecuación (5.21). 5.10.3 Omisión de Variables Notación. Para medir el efecto en las cantidades de regresión, hay una serie de estadísticas de interés basadas en la noción de omitir una variable explicativa. A tal fin, la notación de superíndice \\((j)\\) significa omitir la \\(j\\)-ésima variable, donde \\(j=0,1,\\ldots,k\\). Primero, recuerda que \\(\\mathbf{x}^{j} = (x_{1j}, x_{2j}, \\ldots, x_{nj})^{\\prime}\\) es la columna que representa la \\(j\\)-ésima variable. Además, define \\(\\mathbf{X}^{(j)}\\) como la matriz \\(n \\times k\\) de variables explicativas definida al eliminar \\(\\mathbf{x}^{j}\\) de \\(\\mathbf{X}\\). Por ejemplo, tomando \\(j=k\\), a menudo particionamos \\(\\mathbf{X}\\) como \\(\\mathbf{X} = \\left( \\mathbf{X}^{(k)}: \\mathbf{x}^k \\right)\\). Usando los resultados de la Sección 4.7.2, utilizaremos \\(\\mathbf{X}^{(k)} = \\mathbf{X}_1\\) y \\(\\mathbf{x}^k = \\mathbf{X}_2\\). Factor de Inflación de la Varianza. Primero, nos gustaría establecer la relación entre la definición del error estándar de \\(b_j\\) dada por \\[ se(b_j) = s \\sqrt{(j+1)\\text{ésimo elemento diagonal de }(\\mathbf{X}^{\\prime}\\mathbf{X})^{-1}} \\] y la relación que involucra el factor de inflación de la varianza, \\[ se(b_j) = s \\frac{\\sqrt{VIF_j}}{s_{x_j}\\sqrt{n-1}}. \\] Por simetría de las variables independientes, solo necesitamos considerar el caso donde \\(j=k\\). Así, nos gustaría establecer \\[\\begin{equation} (k+1)\\text{ésimo elemento diagonal de }(\\mathbf{X}^{\\prime}\\mathbf{X})^{-1} = \\frac{VIF_{k}}{(n-1) s_{x_{k}}^2}. \\tag{5.22} \\end{equation}\\] Primero considera el modelo reparametrizado en la ecuación (4.22). A partir de la ecuación (4.23), podemos expresar la estimación del coeficiente de regresión \\[ b_{k} = \\frac{\\mathbf{e}_1^{\\prime}\\mathbf{y}}{\\mathbf{e}_1^{\\prime}\\mathbf{e}_1}. \\] De la ecuación (4.23), tenemos que \\(\\text{Var} \\, b_{k} = \\sigma^2 (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1}\\) y así \\[\\begin{equation} se(b_{k}) = s (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1/2}. \\tag{5.23} \\end{equation}\\] Así, \\((\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1}\\) es el \\((k+1)\\)-ésimo elemento diagonal de \\[ \\left( \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{E}_2^{\\prime} \\end{bmatrix} \\begin{bmatrix} \\mathbf{X}_1 &amp; \\mathbf{E}_2 \\end{bmatrix} \\right)^{-1} \\] y también es el \\((k+1)\\)-ésimo elemento diagonal de \\((\\mathbf{X}^{\\prime} \\mathbf{X})^{-1}\\). Alternativamente, esto se puede verificar directamente utilizando la inversa de la matriz particionada en la ecuación (4.19). Ahora, supongamos que realizamos una regresión usando \\(\\mathbf{x}^{k} = \\mathbf{X}_2\\) como el vector de respuesta y \\(\\mathbf{X}^{(k)} = \\mathbf{X}_1\\) como la matriz de variables explicativas. Como se anotó arriba en la ecuación (4.22), \\(\\mathbf{E}_2\\) representa los “residuos” de esta regresión y así \\(\\mathbf{E}_2^{\\prime} \\mathbf{E}_2\\) representa la suma de cuadrados del error. Para esta regresión, la suma total de cuadrados es \\[ \\sum_{i=1}^{n} (x_{ik} - \\bar{x}_{k})^2 = (n-1) s_{x_{k}}^2 \\] y el coeficiente de determinación es \\(R_{k}^2\\). Así, \\[ \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 = Error ~SS = Total ~SS (1 - R_{k}^2) = \\frac{(n-1) s_{x_{k}}^2}{VIF_{k}}. \\] Esto establece el resultado. Estableciendo \\(t^2 = F\\). Para probar la hipótesis nula \\(H_0\\): \\(\\beta_{k} = 0\\), el material en la Sección 3.4.1 proporciona una descripción de una prueba basada en el estadístico \\(t\\), \\(t(b_{k}) = \\frac{b_{k}}{se(b_{k})}\\). Un procedimiento alternativo de prueba, descrito en las Secciones 4.2.2, utiliza el estadístico de prueba \\[ F-\\text{ratio} = \\frac{(Error ~SS)_{reducido} - (Error ~SS)_{completo}}{p \\times (Error~MS)_{completo}} = \\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right)^2}{s^2 \\mathbf{E}_2^{\\prime} \\mathbf{E}_2} \\] de la ecuación (4.26). Alternativamente, a partir de las ecuaciones (4.23) y (5.23), tenemos \\[\\begin{equation} t(b_{k}) = \\frac{b_{k}}{se(b_{k})} = \\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right) / \\left( \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 \\right)}{s / \\sqrt{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2}} = \\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right)}{s \\sqrt{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2}}. \\tag{5.24} \\end{equation}\\] Así, \\(t(b_{k})^2 = F\\)-ratio. Coeficientes de Correlación Parcial. A partir del modelo de regresión completo \\[ \\mathbf{y} = \\mathbf{X}^{(k)} \\boldsymbol{\\beta}^{(k)} + \\mathbf{x}_{k} \\beta_{k} + \\boldsymbol{\\varepsilon}, \\] considera dos regresiones separadas. Una regresión usando \\(\\mathbf{x}^{k}\\) como el vector de respuesta y \\(\\mathbf{X}^{(k)}\\) como la matriz de variables explicativas produce los residuos \\(\\mathbf{E}_2\\). De manera similar, una regresión con \\(\\mathbf{y}\\) como el vector de respuesta y \\(\\mathbf{X}^{(k)}\\) como la matriz de variables explicativas produce los residuos \\[ \\mathbf{E}_1 = \\mathbf{y} - \\mathbf{X}^{(k)} \\left( \\mathbf{X}^{(k)\\prime} \\mathbf{X}^{(k)} \\right)^{-1} \\mathbf{X}^{(k)} \\mathbf{y}. \\] Si \\(x^{0} = (1,1,\\ldots,1)^{\\prime}\\), entonces el promedio de \\(\\mathbf{E}_1\\) y \\(\\mathbf{E}_2\\) es cero. En este caso, la correlación muestral entre \\(\\mathbf{E}_1\\) y \\(\\mathbf{E}_2\\) es \\[ r(\\mathbf{E}_1, \\mathbf{E}_2) = \\frac{\\sum_{i=1}^{n} E_{1i} E_{2i}}{\\sqrt{\\left( \\sum_{i=1}^{n} E_{1i}^2 \\right) \\left( \\sum_{i=1}^{n} E_{2i}^2 \\right)}} = \\frac{\\mathbf{E}_1^{\\prime} \\mathbf{E}_2}{\\sqrt{\\left( \\mathbf{E}_1^{\\prime} \\mathbf{E}_1 \\right) \\left( \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 \\right)}}. \\] Como \\(\\mathbf{E}_2\\) es un vector de residuos usando \\(\\mathbf{X}^{(k)}\\) como la matriz de variables explicativas, tenemos que \\(\\mathbf{E}_2^{\\prime} \\mathbf{X}^{(k)} = 0\\). Así, para el numerador, tenemos \\[ \\mathbf{E}_2^{\\prime} \\mathbf{E}_1 = \\mathbf{E}_2^{\\prime} \\left( \\mathbf{y} - \\mathbf{X}^{(k)} \\left( \\mathbf{X}^{(k)\\prime} \\mathbf{X}^{(k)} \\right)^{-1} \\mathbf{X}^{(k)} \\mathbf{y} \\right) = \\mathbf{E}_2^{\\prime} \\mathbf{y}. \\] A partir de las ecuaciones (4.24) y (4.25), tenemos que \\[ (n - (k+1)) s^2 = (Error ~SS)_{completo} = \\mathbf{E}_1^{\\prime} \\mathbf{E}_1 - \\frac{\\left( \\mathbf{E}_1^{\\prime} \\mathbf{y} \\right)^2}{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2} = \\mathbf{E}_1^{\\prime} \\mathbf{E}_1 - \\frac{\\left( \\mathbf{E}_1^{\\prime} \\mathbf{E}_2 \\right)^2}{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2}. \\] Así, a partir de la ecuación (5.24) \\[ \\begin{array}{ll} \\frac{t(b_{k})}{\\sqrt{t(b_{k})^2 + n - (k+1)}} &amp;= \\frac{\\mathbf{E}_2^{\\prime} \\mathbf{y} / \\left(s \\sqrt{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2}\\right)}{\\sqrt{\\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right)^2}{s^2 \\mathbf{E}_2^{\\prime} \\mathbf{E}_2} + n - (k+1)}} \\\\ &amp; = \\frac{\\mathbf{E}_2^{\\prime} \\mathbf{y}}{\\sqrt{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right)^2 + \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 s^2 \\left(n - (k+1) \\right)}} \\\\ &amp; = \\frac{\\mathbf{E}_2^{\\prime} \\mathbf{E}_1}{\\sqrt{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{E}_1 \\right)^2 + \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 \\left( \\mathbf{E}_1^{\\prime} \\mathbf{E}_1 - \\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{E}_1 \\right)^2}{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2} \\right)}} \\\\ &amp;= \\frac{\\mathbf{E}_1^{\\prime} \\mathbf{E}_2}{\\sqrt{(\\mathbf{E}_1^{\\prime} \\mathbf{E}_1) (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)}} = r(\\mathbf{E}_1, \\mathbf{E}_2). \\end{array} \\] Esto establece la relación entre el coeficiente de correlación parcial y el estadístico \\(t\\)-ratio. "],["bibliography.html", "Bibliography", " Bibliography "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
