[["index.html", "Modelado de Regresión con Aplicaciones Actuariales y Financieras Prefacio Prólogo Dedicación", " Modelado de Regresión con Aplicaciones Actuariales y Financieras Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University Prefacio Prólogo Los actuarios y otros analistas financieros cuantifican situaciones usando datos; somos personas de ‘números’. Muchos de nuestros enfoques y modelos son estilizados, basados en años de experiencia e investigaciones realizadas por legiones de analistas. Sin embargo, el mundo financiero y de gestión de riesgos evoluciona rápidamente. Muchos analistas se enfrentan a nuevas situaciones en las que los métodos probados simplemente no funcionan. Aquí es donde entra un conjunto de herramientas como el análisis de regresión. La regresión es el estudio de las relaciones entre variables. Es una disciplina estadística genérica que no se limita al mundo financiero; tiene aplicaciones en campos de ciencias sociales, biológicas y físicas. Puedes usar técnicas de regresión para investigar conjuntos de datos grandes y complejos. Para familiarizarte con la regresión, este libro explora muchos ejemplos y conjuntos de datos basados en aplicaciones actuariales y financieras. Esto no quiere decir que no encontrarás aplicaciones fuera del mundo financiero (por ejemplo, un actuario puede necesitar entender la evidencia científica más reciente sobre pruebas genéticas para fines de suscripción). Sin embargo, al familiarizarte con este conjunto de herramientas, verás cómo la regresión puede aplicarse en muchas (y a veces nuevas) situaciones. ¿Para Quién Es Este Libro? Este libro está escrito para analistas financieros que enfrentan eventos inciertos y desean cuantificar estos eventos utilizando información empírica. No se asume conocimiento previo del sector, aunque los lectores encontrarán la lectura mucho más fácil si tienen interés en las aplicaciones discutidas aquí. Este libro está diseñado para estudiantes que están siendo introducidos al campo, así como para analistas de la industria que deseen repasar técnicas antiguas y (para los capítulos posteriores) obtener una introducción a nuevos desarrollos. Para leer este libro, asumo un conocimiento comparable a una introducción de un semestre a la probabilidad y estadística; el Apéndice A1 proporciona una breve revisión para refrescar conceptos si estás oxidado. Los estudiantes actuariales en Norteamérica tendrán una introducción de un año a la probabilidad y estadística; este tipo de introducción ayudará a los lectores a captar conceptos más rápidamente que una base de un semestre. Finalmente, los lectores encontrarán útil el álgebra matricial, o lineal, aunque no es un requisito previo para leer este texto. Los diferentes lectores están interesados en entender la estadística a diferentes niveles. Este libro está escrito para acomodar al ‘lector de sillón’, es decir, aquel que lee pasivamente y no se involucra intentando realizar los ejercicios del texto. Considera una analogía con el fútbol, o cualquier otro juego. Al igual que el mariscal de campo de sillón en el fútbol, hay mucho que puedes aprender sobre el juego solo con observar. Sin embargo, si quieres agudizar tus habilidades, tienes que salir y jugar el juego. Si realizas los ejercicios o reproduces los análisis estadísticos en el texto, te convertirás en un mejor jugador. Aún así, este texto está escrito entrelazando ejemplos con los principios básicos. Así, incluso el lector de sillón puede obtener una sólida comprensión de las técnicas de regresión a través de este texto. ¿De Qué Trata Este Libro? La Tabla de Contenidos proporciona una visión general de los temas tratados, organizados en cuatro partes. La primera parte introduce la regresión lineal. Este es el material central del libro, con refrescadores sobre estadísticas matemáticas, distribuciones y álgebra matricial entrelazados según sea necesario. La segunda parte se dedica a temas en series temporales. ¿Por qué integrar temas de series temporales en un libro de regresión? Las razones son simples, pero convincentes; la mayoría de los datos contables, financieros y económicos se vuelven disponibles a lo largo del tiempo. Aunque las inferencias transversales son útiles, las decisiones empresariales deben tomarse en tiempo real con los datos actualmente disponibles. Los Capítulos 7-10 introducen técnicas de series temporales que se pueden realizar fácilmente usando herramientas de regresión (y hay muchas). La regresión no lineal es el tema de la tercera parte. Muchas de las herramientas modernas de ‘modelado predictivo’ están basadas en regresión no lineal; estas son las herramientas de trabajo en las oficinas estadísticas de la industria financiera y de gestión de riesgos. La cuarta parte se refiere a ‘aplicaciones actuariales,’ temas que he encontrado relevantes en mi investigación y trabajo de consultoría en gestión de riesgos financieros. Los primeros cuatro capítulos de esta parte consisten en variaciones de modelos de regresión que son particularmente útiles en la gestión de riesgos. Los últimos dos capítulos se centran en las comunicaciones, específicamente, en la redacción de informes y el diseño de gráficos. Comunicar información es un aspecto importante de toda disciplina técnica y la estadística ciertamente no es una excepción. Aquí está la traducción al español del texto proporcionado, manteniendo la estructura en Markdown y utilizando un lenguaje simple cuando sea posible: ¿Cómo Transmite Este Libro Su Mensaje? Desarrollo de Capítulos. Cada capítulo tiene varios ejemplos entretejidos con teoría. En los capítulos donde se introduce un modelo, comienzo con un ejemplo y discuto el análisis de datos sin hacer referencia a la teoría. Este análisis se presenta a un nivel intuitivo, sin referencia a un modelo específico. Esto es sencillo, porque se trata de poco más que ajustar curvas. El objetivo es que los estudiantes resuman los datos de manera sensata sin que la noción de un modelo obscurezca un buen análisis de datos. Luego, se proporciona una introducción a la teoría en el contexto del ejemplo introductorio. Se siguen uno o más ejemplos adicionales que refuerzan la teoría ya introducida y proporcionan un contexto para explicar la teoría adicional. En los Capítulos 5 y 6, que no introducen modelos sino técnicas para el análisis, comienzo con una introducción de la técnica. Esta introducción es seguida por un ejemplo que refuerza la explicación. De esta manera, el análisis de datos se puede omitir fácilmente sin pérdida de continuidad, si el tiempo es una preocupación. Datos Reales. Muchos de los ejercicios piden al lector que trabaje con datos reales. La necesidad de trabajar con datos reales está bien documentada; por ejemplo, véase Hogg (1972) o Singer y Willett (1990). Algunos criterios de Singer y Willett para juzgar un buen conjunto de datos incluyen: (1) autenticidad, (2) disponibilidad de información de fondo, (3) interés y relevancia para el aprendizaje sustantivo, y (4) disponibilidad de elementos con los que los lectores puedan identificarse. Por supuesto, hay algunas desventajas importantes al trabajar con datos reales. Los conjuntos de datos pueden volverse obsoletos rápidamente. Además, el conjunto de datos ideal para ilustrar un problema estadístico específico es difícil de encontrar. Esto se debe a que, con datos reales, casi por definición, varios problemas ocurren simultáneamente. Esto hace que sea difícil aislar un aspecto específico. Particularmente disfruto trabajar con grandes conjuntos de datos. Cuanto mayor es el conjunto de datos, mayor es la necesidad de estadísticas para resumir el contenido informativo. Software Estadístico y Datos. Mi objetivo al escribir este texto es llegar a un amplio grupo de estudiantes y analistas de la industria. Por lo tanto, para evitar excluir grandes segmentos, elegí no integrar ningún paquete de software estadístico específico en el texto. Sin embargo, debido a la orientación hacia las aplicaciones, es crucial que la metodología presentada pueda lograrse fácilmente utilizando paquetes disponibles. Para el curso que enseño en la Universidad de Wisconsin, uso los paquetes estadísticos SAS y R. En el sitio web del libro, http://research.bus.wisc.edu/RegActuaries los usuarios encontrarán scripts escritos en SAS y R para el análisis presentado en el texto. Los datos están disponibles en formato de texto, permitiendo a los lectores usar cualquier paquete estadístico que deseen. Cuando veas una nota como esta en el margen, también podrás encontrar este conjunto de datos () en el sitio web del libro. Suplementos Técnicos. Los suplementos técnicos refuerzan y amplían los resultados en el cuerpo principal del texto al proporcionar un tratamiento más formal y matemático del material. Este tratamiento es en realidad un suplemento porque las aplicaciones y ejemplos están descritos en el cuerpo principal del texto. Para los lectores con suficiente fondo matemático, los suplementos proporcionan material adicional que es útil para comunicarse con audiencias técnicas. Los suplementos técnicos ofrecen una cobertura más profunda y amplia de la regresión aplicada. Creo que los analistas deberían tener una idea de ‘lo que ocurre bajo el capó,’ o ‘cómo funciona el motor.’ La mayoría de estos temas se omitirán en la primera lectura del material. Sin embargo, a medida que trabajes con regresión, te enfrentarás a preguntas sobre ‘¿Por qué?’ y necesitarás profundizar en los detalles para ver exactamente cómo funciona una técnica en particular. Además, los suplementos técnicos proporcionan un menú de elementos opcionales que un instructor puede desear cubrir. Cursos Sugeridos. Hay una amplia variedad de temas que pueden incluirse en un curso de regresión. Aquí están algunos cursos sugeridos. El curso que enseño en la Universidad de Wisconsin es el primero en la lista de la siguiente tabla. \\[ {\\small \\begin{array}{ll} \\begin{array}{lll} \\hline \\textbf{Audiencia} &amp; \\textbf{Naturaleza del Curso}&amp; \\textbf{Capítulos Sugeridos} \\\\ \\hline \\text{Fondo de un año en} &amp; \\text{Introducción a la regresión y} &amp; \\text{Capítulos } 1-8 ,11-13, 20-21,\\\\ ~~~\\text{probabilidad y estadística} &amp; ~~~\\text{modelos de series temporales} &amp; ~~~\\text{solo cuerpo principal del texto} \\\\ \\text{Fondo de un año en} &amp; \\text{Regresión y modelos de} &amp; \\text{ Capítulos } 1-8, 20-21, \\text{seleccionados} \\\\ ~~~\\text{probabilidad y estadística} &amp; ~~~\\text{series temporales} &amp; ~~~\\text{porciones de los suplementos técnicos} \\\\ \\text{Fondo de un año en} &amp; \\text{Modelado de regresión} &amp; \\text{Capítulos } 1-6, 11-13, 20-21, \\text{seleccionados} \\\\ ~~~\\text{probabilidad y estadística} &amp; &amp; ~~~ \\text{porciones de los suplementos técnicos} \\\\ \\text{Fondo en estadísticas} &amp; \\text{Regresión actuarial} &amp; \\text{ Capítulos } 10-21, \\text{seleccionados} \\\\ ~~~\\text{y regresión lineal} &amp; ~~~\\text{modelos} &amp; ~~~ \\text{porciones de los suplementos técnicos} \\\\ \\hline \\end{array} \\end{array} } \\] Además de estos cursos sugeridos, este libro está diseñado para lectura complementaria para un curso de series temporales, así como un libro de referencia para analistas de la industria. Mi esperanza es que los estudiantes universitarios que utilicen las primeras partes del libro en su curso universitario encuentren los capítulos posteriores útiles en sus posiciones en la industria. De esta manera, espero promover el aprendizaje continuo a lo largo de la vida. Agradecimientos Es apropiado comenzar la sección de agradecimientos agradeciendo a los estudiantes del programa actuarial aquí en la Universidad de Wisconsin; los estudiantes son socios importantes en el negocio de la creación y difusión del conocimiento en las universidades. A través de sus preguntas y comentarios, he aprendido una cantidad tremenda a lo largo de los años. También he beneficiado de la asistencia de quienes me han ayudado a reunir todas las piezas para este libro, específicamente, Missy Pinney, Peng Shi, Yunjie (Winnie) Sun y Ziyan Xie. He disfrutado trabajando con varios antiguos estudiantes y colegas en problemas de regresión en los últimos años, incluyendo a Katrien Antonio, Jie Gao, Paul Johnson, Margie Rosenberg, Jiafeng Sun, Emil Valdez y Ping Wang. Sus contribuciones están reflejadas indirectamente a lo largo del texto. Debido a mi larga asociación con la Universidad de Wisconsin-Madison, soy reacio a retroceder más en el tiempo y proporcionar una lista más extensa por temor a olvidar personas importantes. También he tenido la suerte de tener una asociación más reciente con el Insurance Services Office (ISO). Los colegas en ISO me han proporcionado importantes perspectivas sobre aplicaciones. A través de este texto que presenta aplicaciones de regresión en problemas actuariales y de la industria financiera, espero fomentar asociaciones adicionales entre la academia y la industria. Estoy encantado de reconocer las revisiones detalladas que he recibido de los colegas Tim Welnetz y Margie Rosenberg. También deseo agradecer a Bob Miller por permitirme incluir nuestro trabajo conjunto sobre diseño de gráficos efectivos en el Capítulo 21. Bob me ha enseñado mucho sobre regresión a lo largo de los años. Además, me alegra reconocer el apoyo financiero a través del Assurant Health Professorship en Ciencias Actuariales en la Universidad de Wisconsin-Madison. Guardando lo más importante para el final, agradezco a mi familia por su apoyo. Mil gracias a mi madre Mary, hermanos Randy, Guy y Joe, mi esposa Deirdre y nuestros hijos Nathan y Adam. Dedicación Hay un viejo dicho, atribuido a Sir Isaac Newton y que se puede encontrar en Google Scholar, Si he visto más lejos, es porque me he subido a los hombros de gigantes. Dedico este libro a la memoria de dos gigantes que me ayudaron, y a todos los que los conocieron, a ver más lejos y vivir mejor: James C. Hickman y Joseph P. Sullivan. "],["translation.html", "Translation", " Translation Under Construction!!! What? The purpose of this project is to develop a Spanish translation for the text Regression Modeling with Actuarial and Financial Application. This translation will be open and freely available - a resource for our community. Why? Regression provides the foundations for data science techniques such as machine learning. Although this book is a bit dated (written in 2009), it provides an easy introduction to statistical learning tools geared for applications of interest to actuaries and other financial analysts. Who? Jed Frees jfrees@bus.wisc.edu is the author of the text and has secured permission from Cambridge University Press to publish an online Spanish version. Jed is learning Spanish and is familiar with online publishing. He will take responsibility for uploading the translated version to the web. Armando Zarruk Rivera azarrukr@unal.edu.co is the chair of the Society of Actuaries Latin America Committee. He is coordinating translation volunteers. How? We are following a procedure similar to our successful Spanish version of Loss Data Analytics, available at https://openacttexts.github.io/LDASpanish/. Check out our Google Usage Data to see who is using this book. In short, Jed is converting his text, written in 2009 using latex, to R markdown, and then using ChatGPT for the initial translation into Spanish. Translation volunteers will review this translation. Translation volunteers will supply Jed with the corrections and he will correct the online version. If the number of suggested changes are small, probably these easiest thing is to send them directly to Jed. (For example, give an approximate location, indicate the old and revised versions.) If major changes are required, please go to the Github page, download the .Rmd file, make the changes in the file, and send on to Jed. An “.Rmd” file is based on R’s version of a markdown file. You can open it with any text editor (e.g. Notepad - not Word), make changes, and save as text file. Date: 05 August 2024 "],["regresión-y-la-distribución-normal.html", "Chapter 1 Regresión y la Distribución Normal 1.1 ¿Qué es el Análisis de Regresión? 1.2 Ajuste de Datos a una Distribución Normal 1.3 Transformaciones de Potencia 1.4 Muestreo y el Papel de la Normalidad 1.5 Regresión y Diseños de Muestreo 1.6 Aplicaciones Actuariales de la Regresión 1.7 Lecturas Adicionales y Referencias 1.8 Ejercicios 1.9 Suplemento Técnico - Teorema del Límite Central", " Chapter 1 Regresión y la Distribución Normal Vista Previa del Capítulo. El análisis de regresión es un método estadístico que se utiliza ampliamente en muchos campos de estudio, y la ciencia actuarial no es una excepción. Este capítulo proporciona una introducción al papel de la distribución normal en la regresión, el uso de transformaciones logarítmicas para especificar relaciones de regresión y la base de muestreo que es crítica para inferir resultados de regresión a poblaciones amplias de interés. 1.1 ¿Qué es el Análisis de Regresión? La estadística trata sobre datos. Como disciplina, se ocupa de la recolección, resumen y análisis de datos para hacer afirmaciones sobre el mundo real. Cuando los analistas recolectan datos, realmente están recolectando información que se cuantifica, es decir, se transforma a una escala numérica. Existen reglas fáciles y bien entendidas para reducir los datos, utilizando medidas de resumen numéricas o gráficas. Estas medidas de resumen pueden luego vincularse a una representación teórica, o modelo, de los datos. Con un modelo que se calibra con datos, se pueden hacer afirmaciones sobre el mundo. Los métodos estadísticos han tenido un gran impacto en varios campos de estudio. En el área de recolección de datos, el diseño cuidadoso de encuestas por muestreo es crucial para los grupos de investigación de mercado y para los procedimientos de auditoría de las firmas de contabilidad. El diseño experimental es una segunda subdisciplina dedicada a la recolección de datos. El enfoque del diseño experimental es construir métodos de recolección de datos que extraigan información de la manera más eficiente posible. Esto es especialmente importante en campos como la agricultura y la ingeniería, donde cada observación es costosa, posiblemente costando millones de dólares. Otros métodos estadísticos aplicados se centran en la gestión y predicción de datos. El control de procesos se ocupa de monitorear un proceso a lo largo del tiempo y decidir cuándo la intervención es más fructífera. El control de procesos ayuda a gestionar la calidad de los bienes producidos por los fabricantes. La previsión se trata de extrapolar un proceso hacia el futuro, ya sea las ventas de un producto o los movimientos de una tasa de interés. El análisis de regresión es un método estadístico utilizado para analizar datos. Como veremos, la característica distintiva de este método es la capacidad de hacer afirmaciones sobre variables después de haber controlado los valores de variables explicativas conocidas. Aunque otros métodos son importantes, el análisis de regresión ha sido el más influyente. Para ilustrar, un índice de revistas de negocios, ABI/INFORM, enumera más de veinticuatro mil artículos que utilizan técnicas de regresión en el período de treinta años de 1978-2007. ¡Y estas son solo las aplicaciones que se consideraron lo suficientemente innovadoras como para ser publicadas en revistas académicas! El análisis de regresión de datos es tan omnipresente en los negocios modernos que es fácil pasar por alto el hecho de que la metodología tiene poco más de 120 años. Los estudiosos atribuyen el nacimiento de la regresión al discurso presidencial de 1885 de Sir Francis Galton en la sección antropológica de la Asociación Británica para el Avance de las Ciencias. En ese discurso, descrito en Stigler (1986), Galton proporcionó una descripción de la regresión y la vinculó a la teoría de la curva normal. Su descubrimiento surgió de sus estudios sobre las propiedades de la selección natural y la herencia. Para ilustrar un conjunto de datos que se puede analizar utilizando métodos de regresión, la Tabla 1.1 muestra algunos datos incluidos en el artículo de Galton de 1885. Esta tabla muestra las alturas de 928 hijos adultos, clasificados por un índice de la altura de sus padres. Aquí, todas las alturas femeninas se multiplicaron por 1.08, y el índice se creó tomando el promedio de la altura del padre y la altura reescalada de la madre. Galton era consciente de que tanto la altura de los padres como la del hijo adulto podían ser adecuadamente aproximadas por una curva normal. Al desarrollar el análisis de regresión, proporcionó un único modelo para la distribución conjunta de alturas. Table 1.1: Galtons 1885 Regression Data &lt;64.0 64.5 65.5 66.5 67.5 68.5 69.5 70.5 71.5 72.5 &gt;73.0 Total &gt;73.7 0 0 0 0 0 0 5 3 2 4 0 14 73.2 0 0 0 0 0 3 4 3 2 2 3 17 72.2 0 0 1 0 4 4 11 4 9 7 1 41 71.2 0 0 2 0 11 18 20 7 4 2 0 64 70.2 0 0 5 4 19 21 25 14 10 1 0 99 69.2 1 2 7 13 38 48 33 18 5 2 0 167 68.2 1 0 7 14 28 34 20 12 3 1 0 120 67.2 2 5 11 17 38 31 27 3 4 0 0 138 66.2 2 5 11 17 36 25 17 1 3 0 0 117 65.2 1 1 7 2 15 16 4 1 1 0 0 48 64.2 4 4 5 5 14 11 16 0 0 0 0 59 63.2 2 4 9 3 5 7 1 1 0 0 0 32 62.2 0 1 0 3 3 0 0 0 0 0 0 7 &lt;61.2 1 1 1 0 0 1 0 1 0 0 0 5 Total 14 23 66 78 211 219 183 68 43 19 4 928 Fuente: Stigler (1986) La Tabla 1.1 muestra que gran parte de la información sobre la altura de un hijo adulto puede atribuirse o ‘explicarse’ en términos de la altura de los padres. Por lo tanto, utilizamos el término variable explicativa para las mediciones que proporcionan información sobre una variable de interés. El análisis de regresión es un método para cuantificar la relación entre una variable de interés y las variables explicativas. La metodología utilizada para estudiar los datos en la Tabla 1.1 también se puede utilizar para estudiar problemas actuariales y de gestión de riesgos, la tesis de este libro. 1.2 Ajuste de Datos a una Distribución Normal Históricamente, la distribución normal tuvo un papel fundamental en el desarrollo del análisis de regresión. Continúa desempeñando un papel importante, aunque estaremos interesados en extender las ideas de regresión a datos altamente ‘no normales’. Formalmente, la curva normal se define por la función \\[\\begin{equation} \\mathrm{f}(y)=\\frac{1}{\\sigma \\sqrt{2\\pi }}\\exp \\left( -\\frac{1}{2\\sigma ^{2}% }\\left( y-\\mu \\right) ^{2}\\right) . \\tag{1.1} \\end{equation}\\] Esta curva es una función de densidad de probabilidad con toda la línea real como su dominio. De la ecuación (1.1), vemos que la curva es simétrica respecto a \\(\\mu\\) (la media y la mediana). El grado de agudeza está controlado por el parámetro \\(\\sigma ^{2}\\). Estos dos parámetros, \\(\\mu\\) y \\(\\sigma ^{2}\\), son conocidos como los parámetros de ubicación y escala, respectivamente. El Apéndice A3.1 proporciona detalles adicionales sobre esta curva, incluyendo un gráfico y tablas de su distribución acumulada que utilizaremos a lo largo del texto. La curva normal también se muestra en la Figura 1.1, una imagen de un billete de moneda alemana ahora fuera de circulación, el diez Deutsche Mark. Este billete contiene la imagen del alemán Carl Gauss, un eminente matemático cuyo nombre a menudo se asocia con la curva normal (a veces se refiere a ella como la curva Gaussiana). Gauss desarrolló la curva normal en relación con la teoría de los mínimos cuadrados para ajustar curvas a los datos en 1809, aproximadamente al mismo tiempo que el trabajo relacionado del científico francés Pierre LaPlace. Según Stigler (1986), ¡hubo bastante acritud entre estos dos científicos sobre la prioridad del descubrimiento! La curva normal se utilizó por primera vez como una aproximación a los histogramas de datos alrededor de 1835 por Adolph Quetelet, un matemático y científico social belga. Como muchas cosas buenas, la curva normal ha existido durante algún tiempo, desde aproximadamente 1720 cuando Abraham de Moivre la derivó para su trabajo sobre modelado de juegos de azar. La curva normal es popular porque es fácil de usar y ha demostrado ser exitosa en muchas aplicaciones. Figure 1.1: Diez Deutsche Mark. Moneda alemana con el científico Gauss y la curva normal. Ejemplo: Reclamaciones por Lesiones Corporales en Massachusetts. Para nuestra primera mirada al ajuste de la curva normal a un conjunto de datos, consideramos los datos de Rempala y Derrig (2005). Ellos consideraron las reclamaciones derivadas de coberturas de seguros de lesiones corporales por accidentes de automóvil. Estos son montos incurridos por tratamientos médicos ambulatorios que surgen de accidentes de automóvil, típicamente esguinces, fracturas de clavícula y similares. Los datos consisten en una muestra de 272 reclamaciones de Massachusetts que se cerraron en 2001 (por ‘cerradas,’ queremos decir que la reclamación está resuelta y no pueden surgir responsabilidades adicionales del mismo accidente). Rempala y Derrig estaban interesados en desarrollar procedimientos para manejar mezclas de reclamaciones ‘típicas’ y otras de proveedores que informaron reclamaciones fraudulentas. Para esta muestra, consideramos solo esas reclamaciones típicas, ignorando las potencialmente fraudulentas. La Tabla 1.2 proporciona varias estadísticas que resumen diferentes aspectos de la distribución. Los montos de las reclamaciones están en unidades de logaritmos de miles de dólares. La reclamación logarítmica promedio es 0.481, lo que corresponde a $1,617.77 (=1000 \\(\\exp(0.481)\\)). Las reclamaciones más pequeñas y más grandes son -3.101 (45 dólares) y 3.912 (50,000 dólares), respectivamente. Table 1.2: Summary Statistics of Massachusetts Automobile Bodily Injury Claims Number Mean Median Standard Deviation Minimum Maximum 25th Percentile 75th Percentile Claims 272 0.481 0.793 1.101 -3.101 3.912 -0.114 1.168 Para completar, aquí hay algunas definiciones. La muestra es el conjunto de datos disponibles para el análisis, denotado por \\(y_1,\\ldots,y_n\\). Aquí, \\(n\\) es el número de observaciones, \\(y_1\\) representa la primera observación, \\(y_2\\) la segunda, y así sucesivamente hasta \\(y_n\\) para la \\(n\\)-ésima observación. Aquí hay algunas estadísticas de resumen importantes. Estadísticas Básicas de Resumen La media es el promedio de las observaciones, es decir, la suma de las observaciones dividida por el número de unidades. Usando notación algebraica, la media es \\[ \\overline{y}=\\frac{1}{n}\\left( y_1 + \\cdots + y_n \\right) = \\frac{1}{n} \\sum_{i=1}^{n} y_i. \\] La mediana es la observación central cuando las observaciones están ordenadas por tamaño. Es decir, es la observación en la que el 50% está por debajo de ella (y el 50% está por encima de ella). La desviación estándar es una medida de la dispersión, o escala, de la distribución. Se calcula como \\[ s_y = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^{2}} . \\] Un percentil es un número en el que una fracción específica de las observaciones está por debajo de él, cuando las observaciones están ordenadas por tamaño. Por ejemplo, el percentil 25 es aquel número en el que el 25% de las observaciones están por debajo de él. Para ayudar a visualizar la distribución, la Figura 1.2 muestra un histograma de los datos. Aquí, la altura de cada rectángulo muestra la frecuencia relativa de observaciones que caen dentro del rango dado por su base. El histograma proporciona una impresión visual rápida de la distribución; muestra que el rango de los datos es aproximadamente (-4,4), la tendencia central es ligeramente mayor que cero y que la distribución es aproximadamente simétrica. Aproximación de la Curva Normal La Figura 1.2 también muestra una curva normal superpuesta, utilizando \\(\\overline{y}\\) para \\(\\mu\\) y \\(s_y^{2}\\) para \\(\\sigma ^{2}\\). Con la curva normal, solo se requieren dos cantidades (\\(\\mu\\) y \\(\\sigma ^{2}\\)) para resumir toda la distribución. Por ejemplo, la Tabla 1.2 muestra que 1.168 es el percentil 75, que es aproximadamente la observación número 204 (\\(=0.75\\times 272\\)) más grande de toda la muestra. De la ecuación (1.1) de la distribución normal, tenemos que \\(z=(y-\\mu )/\\sigma\\) es una normal estándar, de la cual 0.675 es el percentil 75. Así, \\(\\overline{y}+0.675s_y=\\) \\(0.481+0.675\\times 1.101=1.224\\) es el percentil 75 usando la aproximación de la curva normal. Figure 1.2: Frecuencia Relativa de Lesiones Corporales con Curva Normal Superpuesta. Código R para Producir la Figura 1.2 injury &lt;- read.csv(&quot;CSVData/MassBodilyInjury.csv&quot;, header=TRUE) injury2&lt;-subset(injury, providerA != 0 ) LOGCLAIMS&lt;-log(injury2$claims) # FIGURA 1.2 x &lt;- seq(-4, 4, 0.01) y &lt;- dnorm(x, mean=mean(LOGCLAIMS), sd=sqrt(var(LOGCLAIMS))) hist(LOGCLAIMS, freq=FALSE, main=&quot;&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Densidad&quot;, side=2, at=.35,las=1, adj=.7,cex=1.4) lines(x,y) Diagrama de Caja Una inspección visual rápida de la distribución de una variable puede revelar algunas características sorprendentes que están ocultas por estadísticas, medidas de resumen numéricas. El diagrama de caja, también conocido como diagrama de ‘caja y bigotes’, es uno de estos dispositivos gráficos. La Figura 1.3 ilustra un diagrama de caja para las reclamaciones por lesiones corporales. Aquí, la caja captura el 50% central de los datos, con las tres líneas horizontales que corresponden a los percentiles 75, 50 y 25, leyendo de arriba a abajo. Las líneas horizontales por encima y por debajo de la caja son los ‘bigotes’. El bigote superior es 1.5 veces el rango intercuartílico (la diferencia entre los percentiles 75 y 25) por encima del percentil 75. De manera similar, el bigote inferior es 1.5 veces el rango intercuartílico por debajo del percentil 25. Las observaciones individuales fuera de los bigotes se denotan con pequeños símbolos de trazado circulares, y se denominan ‘valores atípicos’. Figure 1.3: Diagrama de Caja de Reclamaciones por Lesiones Corporales. Código R para Producir la Figura 1.3 boxplot(LOGCLAIMS, boxwex=.7, las=1) text(1, .57, &quot;mediana&quot;, cex=1.2) text(1.36, -0.2, &quot;percentil 25&quot;, cex=1.2) text(1.36, 1.1, &quot;percentil 75&quot;, cex=1.2) arrows(1.05, -2, 1.05, -3.3, code=3, angle=20, length=0.1) text(1.15, -2.5, &quot;valores atípicos&quot;, cex=1.2) text(1.13, 3.9, &quot;valor atípico&quot;, cex=1.2) Los gráficos son herramientas poderosas; permiten a los analistas visualizar fácilmente relaciones no lineales que son difíciles de comprender cuando se expresan verbalmente o mediante fórmulas matemáticas. Sin embargo, debido a su gran flexibilidad, los gráficos también pueden engañar fácilmente al analista. El Capítulo 21 subrayará este punto. Por ejemplo, la Figura 1.4 es un re-dibujo de la Figura 1.2; la diferencia es que la Figura 1.4 usa más y más finos rectángulos. Este análisis más detallado revela la naturaleza asimétrica de la distribución de la muestra que no era evidente en la Figura 1.2. Figure 1.4: Re-dibujo de la Figura 1.2 con un número aumentado de rectángulos. Código R para Producir la Figura 1.4 hist(LOGCLAIMS, freq=FALSE, nclass=32, main=&quot;&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Densidad&quot;, side=2, at=.75,las=1, adj=.7,cex=1.1) lines(x,y) Gráficos Cuantiles-Cuantiles Aumentar el número de rectángulos puede descubrir características que no eran evidentes antes; sin embargo, en general hay menos observaciones por rectángulo, lo que significa que la incertidumbre de la estimación de la frecuencia relativa aumenta. Esto representa un compromiso. En lugar de forzar al analista a tomar una decisión arbitraria sobre el número de rectángulos, una alternativa es usar un dispositivo gráfico para comparar una distribución con otra conocida, llamado gráfico de cuantiles-cuantiles, o qq. La Figura 1.5 ilustra un gráfico \\(qq\\) para los datos de lesiones corporales utilizando la curva normal como distribución de referencia. Para cada punto, el eje vertical da el cuantil usando la distribución de la muestra. El eje horizontal da la cantidad correspondiente usando la curva normal. Por ejemplo, anteriormente consideramos el punto del percentil 75. Este punto aparece como (1.168, 0.675) en el gráfico. Para interpretar un gráfico \\(qq\\), si los puntos de los cuantiles se alinean a lo largo de la línea superpuesta, entonces la muestra y la distribución de referencia normal tienen la misma forma. (Esta línea se define conectando los percentiles 75 y 25). En la Figura 1.5, los percentiles pequeños de la muestra son consistentemente más pequeños que los valores correspondientes de la normal estándar, lo que indica que la distribución está sesgada a la izquierda. La diferencia en los valores en los extremos de la distribución se debe a los valores atípicos mencionados anteriormente que también podrían interpretarse como que la distribución de la muestra tiene colas más grandes que la distribución de referencia normal. Figure 1.5: Un gráfico \\(qq\\) de Reclamaciones por Lesiones Corporales, usando una distribución de referencia normal. Código R para Producir la Figura 1.5 qqnorm(LOGCLAIMS, main=&quot;&quot;, las=1, ylab=&quot;&quot;) mtext(&quot;Cuantiles de la Muestra&quot;, side=2, at=4.5, las=1,cex=1.1,adj=.4) qqline(LOGCLAIMS) 1.3 Transformaciones de Potencia En el ejemplo de la Sección 1.2, consideramos las reclamaciones sin justificar el uso de la escala logarítmica. Al analizar variables como los activos de las empresas, los salarios de los individuos y los precios de las viviendas en aplicaciones empresariales y económicas, es común considerar logaritmos en lugar de las unidades originales. Una transformación logarítmica mantiene el orden original (por ejemplo, los salarios altos siguen siendo altos en la escala de logaritmos de los salarios) pero sirve para ‘acercar’ los valores extremos de la distribución. Para ilustrar, la Figura 1.6 muestra la distribución de las reclamaciones por lesiones corporales en (miles de) dólares. Para graficar los datos de manera significativa, se eliminó la observación más grande ($50,000) antes de hacer este gráfico. Incluso con esta observación eliminada, la Figura 1.6 muestra que la distribución está muy inclinada hacia la derecha, con varios valores grandes de reclamaciones apareciendo. Las distribuciones que están inclinadas en una dirección u otra se conocen como sesgadas. La Figura 1.6 es un ejemplo de una distribución sesgada a la derecha, o sesgada positivamente. Aquí, la cola de la distribución a la derecha es más larga y hay una mayor concentración de masa a la izquierda. En contraste, una distribución sesgada a la izquierda, o sesgada negativamente, tiene una cola más larga a la izquierda y una mayor concentración de masa a la derecha. Muchas distribuciones de reclamaciones de seguros están sesgadas a la derecha (ver el texto de Klugman, Panjer y Willmot, 2008, para discusiones extensas). Como vimos en las Figuras 1.4 y 1.5, una transformación logarítmica produce una distribución que está solo ligeramente sesgada a la izquierda. Figure 1.6: Distribución de Reclamaciones por Lesiones Corporales. Las observaciones están en (miles de) dólares con la observación más grande omitida. Código R para Producir la Figura 1.6 injury3 = subset(injury, claims &lt; 25 ) CLAIMS25 &lt;- injury3$claims par(mar=c(4.2,4,1.2,.2),cex=1.1) hist(CLAIMS25, freq=FALSE, main=&quot;&quot;, las=1, ylab=&quot;&quot;, xlab=&quot;CLAIMS&quot;) mtext(&quot;Densidad&quot;, side=2, at=.28, las=1,cex=1.1) Las transformaciones logarítmicas se usan extensamente en el trabajo de estadística aplicada. Una ventaja es que sirven para simetrizar distribuciones que están sesgadas. Más generalmente, consideramos transformaciones de potencia, también conocidas como la familia de transformaciones de Box-Cox. Dentro de esta familia de transformaciones, en lugar de usar la respuesta \\(y\\), usamos una versión transformada o reescalada, \\(y^{\\lambda}\\). Aquí, la potencia \\(\\lambda\\) (lambda, una ‘l’ griega) es un número que puede ser especificado por el usuario. Los valores típicos de \\(\\lambda\\) que se usan en la práctica son \\(\\lambda\\)=1, 1/2, 0 o -1. Cuando usamos \\(\\lambda =0\\), queremos decir \\(\\ln (y)\\), es decir, la transformación logarítmica natural. Más formalmente, la familia de Box-Cox puede expresarse como \\[ y^{(\\lambda )}=\\left\\{ \\begin{array}{ll} \\frac{y^{\\lambda }-1}{\\lambda } &amp; \\lambda \\neq 0 \\\\ \\ln (y) &amp; \\lambda =0 \\end{array} \\right. . \\] Como veremos, porque las estimaciones de regresión no se ven afectadas por desplazamientos de ubicación y escala, en la práctica no necesitamos restar uno ni dividir por \\(\\lambda\\) al reescalar la respuesta. La ventaja de la expresión anterior es que, si dejamos que \\(\\lambda\\) se acerque a 0, entonces \\(y^{(\\lambda )}\\) se acerca a \\(\\ln (y)\\), a partir de algunos argumentos de cálculo sencillos. Para ilustrar la utilidad de las transformaciones, simulamos 500 observaciones de una distribución chi-cuadrado con dos grados de libertad. El Apéndice A3.2 introduce esta distribución (que encontraremos nuevamente más adelante al estudiar el comportamiento de los estadísticos de prueba). El panel superior izquierdo de la Figura 1.7 muestra que la distribución original está muy sesgada hacia la derecha. Los otros paneles en la Figura 1.7 muestran los datos reescalados utilizando las transformaciones de raíz cuadrada, logarítmica y recíproca negativa. La transformación logarítmica, en el panel inferior izquierdo, proporciona la mejor aproximación a la simetría para este ejemplo. La transformación recíproca negativa se basa en \\(\\lambda =-1\\), y luego multiplicando las observaciones reescaladas por menos uno, de modo que las observaciones grandes sigan siendo grandes. Figure 1.7: 500 observaciones simuladas de una distribución chi-cuadrado. El panel superior izquierdo se basa en la distribución original. El superior derecho corresponde a la transformación de raíz cuadrada, el inferior izquierdo a la transformación logarítmica y el inferior derecho a la transformación recíproca negativa. Código R para Producir la Figura 1.7 set.seed(1237) X1 &lt;- 10000*rchisq(500*1, df=2) X2 &lt;- sqrt(X1) X3 &lt;- log(X1) X4 &lt;- -1/X1 par(mfrow=c(2, 2), cex=.75, mar=c(3,5,1.5,0)) hist(X1, freq=FALSE, nclass=16, main=&quot;&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, las=1, yaxt=&quot;n&quot;,xlim=c(0,200000),ylim=c(0,.00005)) axis(2, at=seq(0,.00005,.00001),las=1, cex=.3, labels=c(&quot;0&quot;, &quot;0.00001&quot;, &quot;0.00002&quot;,&quot;0.00003&quot;, &quot;0.00004&quot;, &quot;0.00005&quot;)) mtext(&quot;Densidad&quot;, side=2, at=.000055, las=1, cex=.75) mtext(&quot;y&quot;, side=1, cex=.75, line=2) par(mar=c(3,4,1.5,0.2)) hist(X2, freq=FALSE, nclass=16, main=&quot;&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, las=1,xlim=c(0,400), ylim=c(0,.008)) mtext(&quot;Densidad&quot;, side=2, at=.0088, las=1, cex=.75) mtext(&quot;Raíz cuadrada de y&quot;, side=1, cex=.75, line=2) par(mar=c(3.2,5,1,0)) hist(X3, freq=FALSE, nclass=16, main=&quot;&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, las=1, ylim=c(0,.4)) mtext(&quot;Densidad&quot;, side=2, at=.44, las=1, cex=.75) mtext(&quot;Logaritmo de y&quot;, side=1, cex=.75, line=2) par(mar=c(3.2,4,1,0.2)) hist(X4, freq=FALSE, nclass=16, main=&quot;&quot;,xlab=&quot;&quot;, ylab=&quot;&quot;, las=1, ylim=c(0,100)) mtext(&quot;Densidad&quot;, side=2, at=110, las=1, cex=.75) mtext(&quot;Recíproco negativo de y&quot;, side=1, cex=.75, line=2) 1.4 Muestreo y el Papel de la Normalidad Una estadística es una medida resumen de los datos, como una media, mediana o percentil. Las colecciones de estadísticas son muy útiles para analistas, tomadores de decisiones y consumidores cotidianos para comprender grandes cantidades de datos que representan situaciones complejas. Hasta este punto, nuestro enfoque ha sido introducir técnicas sensatas para resumir variables; técnicas que se usarán repetidamente a lo largo de este texto. Sin embargo, la verdadera utilidad de la disciplina de la estadística es su capacidad para decir algo sobre lo desconocido, no solo para resumir la información ya disponible. Con este fin, necesitamos hacer algunas suposiciones bastante formales sobre la manera en que se observan los datos. Como ciencia, una característica destacada de la estadística (como disciplina) es la capacidad de criticar estas suposiciones y ofrecer alternativas mejoradas en situaciones específicas. Es costumbre suponer que los datos se extraen de una población más grande que estamos interesados en describir. El proceso de extracción de los datos se conoce como muestreo, o proceso generador de datos. Denotamos esta muestra como \\(\\{y_1,\\ldots,y_n\\}\\). Para que podamos criticar y modificar estas suposiciones de muestreo, las enumeramos a continuación en detalle: \\[ \\begin{array}{l} \\hline \\textbf{Suposiciones Básicas de Muestreo} \\\\ \\hline 1. ~\\mathrm{E~}y_i=\\mu \\\\ 2. ~\\mathrm{Var~}y_i=\\sigma ^{2} \\\\ 3. ~\\{y_i\\} \\text{ son independientes} \\\\ 4. ~\\{y_i\\} \\text{ están distribuidos normalmente}. \\\\ \\hline \\end{array} \\] En esta configuración básica, \\(\\mu\\) y \\(\\sigma ^{2}\\) sirven como parámetros que describen la ubicación y escala de la población de origen. El objetivo es inferir algo sensato sobre ellos basándose en estadísticas como \\(\\overline{y}\\) y \\(s_y^{2}\\). Para la tercera suposición, asumimos independencia entre las extracciones. En un esquema de muestreo, esto puede ser garantizado tomando una muestra aleatoria simple de una población. La cuarta suposición no es necesaria para muchos procedimientos de inferencia estadística porque los teoremas del límite central proporcionan una normalidad aproximada para muchas estadísticas de interés. Sin embargo, una justificación formal de algunas estadísticas, como las t-estadísticas, requiere esta suposición adicional. La Sección 1.9 proporciona una declaración explícita de una versión del teorema del límite central, dando condiciones bajo las cuales \\(\\overline{y}\\) está aproximadamente distribuido normalmente. Esta sección también discute un resultado relacionado, conocido como aproximación de Edgeworth, que muestra que la calidad de la aproximación normal es mejor para poblaciones de origen simétricas en comparación con distribuciones sesgadas. ¿Cómo se aplica esta discusión al estudio del análisis de regresión? Después de todo, hasta ahora nos hemos centrado solo en el promedio aritmético simple, \\(\\overline{y}\\). En capítulos posteriores, enfatizaremos que la regresión lineal es el estudio de promedios ponderados; específicamente, muchos coeficientes de regresión pueden expresarse como promedios ponderados con pesos apropiadamente elegidos. Los teoremas de límite central y aproximación de Edgeworth están disponibles para promedios ponderados; estos resultados asegurarán la normalidad aproximada de los coeficientes de regresión. Para usar aproximaciones de la curva normal en un contexto de regresión, a menudo transformaremos variables para lograr una simetría aproximada. 1.5 Regresión y Diseños de Muestreo La aproximación a la normalidad será un tema importante en las aplicaciones prácticas de la regresión lineal. Las Partes I y II de este libro se centran en la regresión lineal, donde aprenderemos conceptos básicos de regresión y diseño de muestreo. La Parte III se centrará en la regresión no lineal, que involucra respuestas binarias, de conteo y de colas pesadas, donde la normalidad no es la distribución de referencia más útil. Las ideas sobre conceptos básicos y diseño también se usarán en el contexto no lineal. En el análisis de regresión, nos enfocamos en una medición de interés y la llamamos variable dependiente. Otras mediciones se usan como variables explicativas. Un objetivo es comparar las diferencias en la variable dependiente en términos de diferencias en las variables explicativas. Como se mencionó en la Sección 1.1, la regresión se usa extensamente en muchos campos científicos. Tabla 1.3 enumera términos alternativos que puedes encontrar al leer aplicaciones de regresión. Tabla 1.3. Terminología para Variables de Regresión \\[ {\\small \\begin{array}{ll}\\hline\\hline y-\\text{Variable} &amp; x-\\text{Variable} \\\\\\hline \\text{Resultado de interés} &amp; \\text{Variable explicativa} \\\\ \\text{Variable dependiente} &amp; \\text{Variable independiente} \\\\ \\text{Variable endógena} &amp; \\text{Variable exógena} \\\\ \\text{Respuesta} &amp; \\text{Tratamiento} \\\\ \\text{Regresando} &amp; \\text{Regresor} \\\\ \\text{Variable del lado izquierdo} &amp; \\text{Variable del lado derecho} \\\\ \\text{Variable explicada} &amp; \\text{Variable predictora} \\\\ \\text{Resultado} &amp; \\text{Entrada} \\\\ \\hline \\end{array} } \\] En la última parte del siglo XIX y principios del siglo XX, la estadística comenzó a tener un impacto importante en el desarrollo de la ciencia experimental. Las ciencias experimentales a menudo utilizan estudios diseñados, donde los datos están bajo el control de un analista. Los estudios diseñados se realizan en entornos de laboratorio, donde hay restricciones físicas estrictas en cada variable que un investigador considera importante. Los estudios diseñados también ocurren en experimentos de campo más grandes, donde los mecanismos de control son diferentes a los de los entornos de laboratorio. La agricultura y la medicina utilizan estudios diseñados. Los datos de un estudio diseñado se dicen que son datos experimentales. Para ilustrar, un ejemplo clásico es considerar el rendimiento de un cultivo como el maíz, donde cada uno de varios parcelas de tierra (las observaciones) se asigna a varios niveles de fertilizante. El objetivo es determinar el efecto del fertilizante (la variable explicativa) en el rendimiento del maíz (la variable de respuesta). Aunque los investigadores intentan hacer que las parcelas de tierra sean lo más similares posible, inevitablemente surgen diferencias. Los investigadores agrícolas utilizan técnicas de aleatorización para asignar diferentes niveles de fertilizante a cada parcela de tierra. De esta manera, los analistas pueden explicar la variación en los rendimientos de maíz en términos de la variación de los niveles de fertilizante. A través del uso de técnicas de aleatorización, los investigadores que utilizan estudios diseñados pueden inferir que el tratamiento tiene un efecto causal sobre la respuesta. El Capítulo 6 discute la causalidad más a fondo. Ejemplo: Experimento de Seguro de Salud Rand. ¿Cómo están relacionados los gastos en atención médica con la demanda de seguro? Muchos estudios han establecido una relación positiva entre la cantidad gastada en atención médica y la demanda de seguro de salud. Aquellos en mala salud anticipan usar más servicios médicos que las personas en buena o regular salud y buscarán niveles más altos de seguro de salud para compensar estos gastos anticipados. Obtienen este seguro adicional al (i) seleccionar un plan de seguro de salud más generoso de un empleador, (ii) elegir un empleador con un plan de seguro de salud más generoso o (iii) pagar más por un seguro de salud individual. Así, es difícil desenredar la relación causa-efecto de los gastos en atención médica y la disponibilidad de seguro de salud. Un estudio reportado por Manning et al. (1987) buscó responder a esta pregunta utilizando un experimento cuidadosamente diseñado. En este estudio, los hogares inscritos de seis ciudades, entre noviembre de 1974 y febrero de 1977, fueron asignados aleatoriamente a uno de 14 planes de seguro diferentes. Estos planes variaban según los elementos de participación en los costos, la tasa de coaseguro (el porcentaje pagado de los gastos de bolsillo que variaba entre 0, 25, 50 y 95%) así como el deducible (5, 10 o 15 por ciento del ingreso familiar, hasta un máximo de $1,000). Así, hubo una asignación aleatoria a los niveles del tratamiento, la cantidad de seguro de salud. El estudio encontró que los planes más favorables resultaron en mayores gastos totales, incluso después de controlar el estado de salud de los participantes. Para la ciencia actuarial y otras ciencias sociales, los estudios diseñados son la excepción más que la regla. Por ejemplo, si queremos estudiar los efectos del tabaquismo en la mortalidad, es muy poco probable que podamos conseguir que los participantes en el estudio acepten ser asignados aleatoriamente a grupos de fumadores/no fumadores durante varios años solo para observar sus patrones de mortalidad. Al igual que en el estudio de Galton de la Sección 1.1, los investigadores en ciencias sociales generalmente trabajan con datos observacionales. Los datos observacionales no están bajo el control del analista. Con datos observacionales, no podemos inferir relaciones causales, pero podemos introducir medidas de asociación. Para ilustrar, en los datos de Galton, es evidente que los padres ‘altos’ tienden a tener hijos ‘altos’ y, a la inversa, los padres ‘bajos’ tienden a tener hijos ‘bajos’. El Capítulo 2 introducirá la correlación y otras medidas de asociación. Sin embargo, no podemos inferir causalidad a partir de los datos. Por ejemplo, puede haber otra variable, como la dieta familiar, que esté relacionada con ambas variables. Una buena dieta en la familia podría estar asociada con alturas altas de los padres y de los hijos adultos, mientras que una dieta pobre limita el crecimiento. Si ese fuera el caso, llamaríamos a la dieta familiar una variable confusora. En experimentos diseñados como el Experimento de Seguro de Salud Rand, podemos controlar los efectos de variables como el estado de salud mediante métodos de asignación aleatoria. En estudios observacionales, usamos control estadístico, en lugar de control experimental. Para ilustrar, en los datos de Galton, podríamos dividir nuestras observaciones en dos grupos, uno para ‘buena dieta familiar’ y otro para ‘mala dieta familiar’, y examinar la relación entre la altura de los padres y la de los hijos para cada subgrupo. Esta es la esencia del método de regresión, comparar un \\(y\\) y un \\(x\\), ‘controlando’ los efectos de otras variables explicativas. Por supuesto, para usar control estadístico y métodos de regresión, uno debe registrar la dieta familiar y cualquier otra medida de altura que pueda confundir los efectos de la altura de los padres en la altura de su hijo adulto. La dificultad en diseñar estudios es tratar de imaginar todas las variables que podrían afectar una variable de respuesta, una tarea imposible en la mayoría de los problemas de interés en ciencias sociales. Para dar algunas orientaciones sobre cuándo ‘es suficiente’, el Capítulo 6 discutirá medidas de la importancia de una variable explicativa y su impacto en la selección del modelo. 1.6 Aplicaciones Actuariales de la Regresión Este libro introduce un método estadístico, el análisis de regresión. La introducción está organizada en torno a la tríada tradicional de la inferencia estadística: prueba de hipótesis, estimación y predicción. Además, este libro muestra cómo esta metodología puede ser utilizada en aplicaciones que probablemente serán de interés para los actuarios y otros analistas de riesgos. Como tal, es útil comenzar con las tres áreas tradicionales de aplicaciones actuariales: tarificación, reservas y pruebas de solvencia. Tarificación y selección adversa El análisis de regresión puede utilizarse para determinar los precios de seguros para muchas líneas de negocio. Por ejemplo, en el seguro de automóviles de pasajeros privados, las reclamaciones esperadas varían según el género del asegurado, la edad, la ubicación (ciudad versus rural), el propósito del vehículo (trabajo o placer) y una serie de otras variables explicativas. La regresión puede utilizarse para identificar las variables que son determinantes importantes de las reclamaciones esperadas. En mercados competitivos, las compañías de seguros no usan el mismo precio para todos los asegurados. Si lo hicieran, los ‘buenos riesgos’, aquellos con reclamaciones esperadas inferiores a la media, pagarían de más y abandonarían la compañía. En contraste, los ‘malos riesgos’, aquellos con reclamaciones esperadas superiores a la media, permanecerían con la compañía. Si la compañía continuara con esta política de precios plana, las primas aumentarían (para compensar las reclamaciones de la creciente proporción de malos riesgos) y la participación en el mercado disminuiría a medida que la compañía pierde buenos riesgos. Este problema se conoce como ‘selección adversa’. Usando un conjunto apropiado de variables explicativas, se pueden desarrollar sistemas de clasificación para que cada asegurado pague su parte justa. Reservas y pruebas de solvencia Tanto la constitución de reservas como las pruebas de solvencia se preocupan por predecir si las obligaciones asociadas con un grupo de pólizas excederán el capital destinado a cumplir con las obligaciones derivadas de las pólizas. La constitución de reservas implica determinar la cantidad apropiada de capital para cumplir con estas obligaciones. Las pruebas de solvencia se centran en evaluar la adecuación del capital para financiar las obligaciones de un bloque de negocio. En algunas áreas de práctica, la regresión puede usarse para pronosticar futuras obligaciones y ayudar a determinar las reservas (ver, por ejemplo, el Capítulo 19). La regresión también puede utilizarse para comparar las características de empresas saludables y financieramente angustiadas para pruebas de solvencia (ver, por ejemplo, el Capítulo 14). Otras aplicaciones en gestión de riesgos El análisis de regresión es una herramienta cuantitativa que puede aplicarse en una amplia variedad de problemas de negocio, no solo en las áreas tradicionales de tarificación, reservas y pruebas de solvencia. Al familiarizarse con el análisis de regresión, los actuarios tendrán otra habilidad cuantitativa que puede aplicarse a problemas generales relacionados con la seguridad financiera de personas, empresas y organizaciones gubernamentales. Para ayudarte a desarrollar ideas, este libro proporciona muchos ejemplos de aplicaciones potenciales ‘no actuariales’ a través de viñetas destacadas etiquetadas como ‘ejemplos’ y conjuntos de datos ilustrativos. Para entender las posibles aplicaciones de la regresión, comienza revisando los varios conjuntos de datos presentados en los Ejercicios del Capítulo 1. Incluso si no completas los ejercicios para fortalecer tus habilidades de resumen de datos (que requieren el uso de una computadora), una revisión de las descripciones de los problemas te ayudará a familiarizarte con los tipos de aplicaciones en las que un actuario podría usar técnicas de regresión. 1.7 Lecturas Adicionales y Referencias Este libro introduce herramientas de regresión y series temporales que son más relevantes para los actuarios y otros analistas de riesgos financieros. Afortunadamente, existen otras fuentes que proporcionan excelentes introducciones a estos temas estadísticos (aunque no desde un punto de vista de gestión de riesgos). En particular, para los analistas que desean especializarse en estadística, es útil obtener otra perspectiva. Para regresión, recomiendo Weisburg (2005) y Faraway (2005). Para series temporales, Diebold (2004) es una buena fuente. Además, Klugman, Panjer y Willmot (2008) proporciona una buena introducción a las aplicaciones actuariales de la estadística; este libro está destinado a complementar el libro de Klugman et al. al centrarse en métodos de regresión y series temporales. Referencias del Capítulo Beard, Robert E., Teivo Pentik\"{a}inen and Erkki Pesonen (1984). Risk Theory: The Stochastic Basis of Insurance (Third Edition). Chapman &amp; Hall, New York. Diebold, Francis. X. (2004). Elements of Forecasting, Third Edition. Thomson, South-Western, Mason, Ohio. Faraway, Julian J. (2005). Linear Models in R. Chapman &amp; Hall/CRC, New York. Hogg, Robert V. (1972). On statistical education. The American Statistician 26, 8-11. Klugman, Stuart A, Harry H. Panjer and Gordon E. Willmot (2008). Loss Models: From Data to Decisions. John Wiley &amp; Sons, Hoboken, New Jersey. Manning, Willard G., Joseph P. Newhouse, Naihua Duan, Emmett B. Keeler, Arleen Leibowitz and M. Susan Marquis (1987). Health insurance and the demand for medical care: Evidence from a randomized experiment. American Economic Review 77, No. 3, 251-277. Rempala, Grzegorz A. and Richard A. Derrig (2005). Modeling hidden exposures in claim severity via the EM algorithm. North American Actuarial Journal 9, No. 2, 108-128. Singer, Judith D. and Willett, J. B. (1990). Improving the teaching of applied statistics: Putting the data back into data analysis. The American Statistician 44, 223-230. Stigler, Steven M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. The Belknap Press of Harvard University Press, Cambridge, MA. Weisberg, Sanford (2005). Applied Linear Regression, Third Edition. John Wiley &amp; Sons, New York. 1.8 Ejercicios 1.1 Gastos de Salud de MEPS. Este ejercicio considera datos de la Encuesta de Gastos Médicos (MEPS), realizada por la Agencia de Investigación y Calidad en Salud de EE.UU. (AHRQ). MEPS es una encuesta probabilística que proporciona estimaciones nacionalmente representativas del uso de atención médica, gastos, fuentes de pago y cobertura de seguros para la población civil de EE.UU. Esta encuesta recopila información detallada sobre episodios de atención médica de cada tipo de servicio, incluyendo visitas a consultorios médicos, visitas a salas de emergencia hospitalarias, visitas a hospitales ambulatorios, estancias hospitalarias, otras visitas a proveedores médicos y uso de medicamentos prescritos. Esta información detallada permite desarrollar modelos de utilización de atención médica para predecir futuros gastos. Puedes obtener más información sobre MEPS en http://www.meps.ahrq.gov/mepsweb/. Consideramos los datos de MEPS de los paneles 7 y 8 de 2003, que consisten en 18,735 individuos entre las edades de 18 y 65 años. De esta muestra, tomamos una muestra aleatoria de 2,000 individuos que aparecen en el archivo ‘HealthExpend’. De esta muestra, hay 157 individuos que tuvieron gastos hospitalarios positivos. También hay 1,352 que tuvieron gastos ambulatorios positivos. Analizaremos estas dos muestras por separado. Nuestras variables dependientes consisten en las cantidades de gastos para visitas hospitalarias (EXPENDIP) y ambulatorias (EXPENDOP). Para MEPS, los eventos ambulatorios incluyen visitas al departamento ambulatorio del hospital, visitas a proveedores en consultorios y visitas a salas de emergencia, excluyendo servicios dentales. (Los servicios dentales, en comparación con otros tipos de servicios de atención médica, son más predecibles y ocurren de manera más regular). Las estancias hospitalarias con la misma fecha de admisión y alta, conocidas como ‘estancias de cero noches’, se incluyeron en los recuentos y gastos ambulatorios. (Los pagos asociados con visitas a salas de emergencia que precedieron inmediatamente a una estancia hospitalaria se incluyeron en los gastos hospitalarios. Los medicamentos prescritos que se pueden vincular a las admisiones hospitalarias se incluyeron en los gastos hospitalarios, no en la utilización ambulatoria). Parte 1: Usa solo los 157 individuos que tuvieron gastos hospitalarios positivos y realiza el siguiente análisis. Calcula estadísticas descriptivas para los gastos hospitalarios (EXPENDIP). a(i). ¿Cuál es el gasto típico (media y mediana)? a(ii). ¿Cómo se compara la desviación estándar con la media? ¿Los datos parecen estar sesgados? Calcula un diagrama de caja, un histograma y un gráfico \\(qq\\) (normal) para EXPENDIP. Comenta sobre la forma de la distribución. Transformaciones. c(i). Realiza una transformación de raíz cuadrada de los gastos hospitalarios. Resume la distribución resultante usando un histograma y un gráfico \\(qq\\). ¿Parece estar aproximadamente distribuida normalmente? c(ii). Realiza una transformación logarítmica (natural) de los gastos hospitalarios. Resume la distribución resultante usando un histograma y un gráfico \\(qq\\). ¿Parece estar aproximadamente distribuida normalmente? Parte 2: Usa solo los 1,352 individuos que tuvieron gastos ambulatorios positivos. Repite la parte (a) y calcula histogramas para los gastos y los gastos logarítmicos. Comenta sobre la normalidad aproximada de cada histograma. 1.2 Utilización de Hogares de Cuidado de Ancianos. Este ejercicio considera datos de hogares de cuidado de ancianos proporcionados por el Departamento de Salud y Servicios Familiares de Wisconsin (DHFS). El programa Medicaid del Estado de Wisconsin financia el cuidado en hogares de cuidado de ancianos para individuos que califican en base a necesidades y estado financiero. Como parte de las condiciones de participación, los hogares de cuidado de ancianos certificados por Medicaid deben presentar un informe de costos anual al DHFS, resumiendo el volumen y costo del cuidado proporcionado a todos sus residentes, financiados por Medicaid y de otro tipo. Estos informes de costos son auditados por el personal del DHFS y forman la base para las tasas diarias de pago de Medicaid específicas para cada instalación para los períodos subsiguientes. Los datos están disponibles públicamente; consulta para más información. El DHFS está interesado en técnicas predictivas que proporcionen pronósticos de utilización confiables para actualizar su programa de tasas de financiamiento de Medicaid para instalaciones de cuidado de ancianos. En esta tarea, consideramos los datos en el archivo ‘WiscNursingHome’ en los años de informe de costos 2000 y 2001. Hay 362 instalaciones en 2000 y 355 instalaciones en 2001. Típicamente, la utilización del cuidado en hogares de cuidado de ancianos se mide en días de paciente (‘días de paciente’ es el número de días que cada paciente estuvo en la instalación, sumado sobre todos los pacientes). Para este ejercicio, definimos la variable de resultado como años totales de pacientes (TPY), el número total de días de pacientes en el período de informe de costos dividido por el número de días operativos de la instalación en el período de informe de costos (ver Rosenberg et al., 2007, Apéndice 1, para una discusión adicional sobre esta elección). El número de camas (NUMBED) y el metraje cuadrado (SQRFOOT) del hogar de cuidado de ancianos miden el tamaño de la instalación. No sorprende que estas variables continuas sean importantes predictores de TPY. Parte 1: Usa los datos del año de informe de costos 2000 y realiza el siguiente análisis. Calcula estadísticas descriptivas para TPY, NUMBED y SQRFOOT. Resume la distribución de TPY usando un histograma y un gráfico \\(qq\\). ¿Parece estar aproximadamente distribuida normalmente? Transformaciones. Realiza una transformación logarítmica (natural) de TPY (LOGTPY). Resume la distribución resultante usando un histograma y un gráfico \\(qq\\). ¿Parece estar aproximadamente distribuida normalmente? Parte 2: Usa los datos del año de informe de costos 2001 y repite las partes (a) y (c). 1.3 Reclamaciones de Seguros de Automóviles. Como analista actuarial, estás trabajando con una gran compañía de seguros para ayudarles a entender la distribución de sus reclamaciones para sus pólizas de automóviles particulares. Tienes disponibles datos de reclamaciones para un año reciente, que consisten en: STATE CODE: códigos del 01 al 17 utilizados, con cada código asignado aleatoriamente a un estado real. CLASS: clase de calificación del operador, basada en edad, género, estado civil y uso del vehículo. GENDER: género del operador. AGE: edad del operador. PAID: monto pagado para resolver y cerrar una reclamación. Te estás enfocando en conductores mayores de 50 años, para los cuales hay \\(n = 6,773\\) reclamaciones disponibles. Examina el histograma del monto PAID y comenta sobre la simetría. Crea una nueva variable, las reclamaciones pagadas en logaritmo natural, LNPAID. Crea un histograma y un gráfico \\(qq\\) de LNPAID. Comenta sobre la simetría de esta variable. 1.4 Costos Hospitalarios. Supongamos que eres un actuario de beneficios para empleados trabajando con una empresa de tamaño mediano en Wisconsin. Esta empresa está considerando ofrecer, por primera vez en su industria, cobertura de seguro hospitalario para los hijos dependientes de sus empleados. Tienes acceso a los registros de la empresa y, por lo tanto, tienes disponibles el número, edad y género de los hijos dependientes, pero no tienes otra información sobre los costos hospitalarios de la empresa. En particular, ninguna empresa en esta industria ha ofrecido esta cobertura, por lo que tienes poca experiencia histórica en la industria sobre la cual puedas prever los reclamos esperados. Recopilas datos del Nationwide Inpatient Sample del Healthcare Cost and Utilization Project (NIS-HCUP), una encuesta nacional de costos hospitalarios realizada por la Agencia de Investigación y Calidad en Salud de EE.UU. (AHRQ). Restringes la consideración a hospitales de Wisconsin y analizas una muestra aleatoria de \\(n=500\\) reclamaciones de datos de 2003. Aunque los datos provienen de registros hospitalarios, están organizados por alta individual y por lo tanto tienes información sobre la edad y el género del paciente dado de alta. Específicamente, consideras pacientes de 0 a 17 años. En un proyecto separado, considerarás la frecuencia de hospitalización. Para este proyecto, el objetivo es modelar la gravedad de los cargos hospitalarios, por edad y género. Examina la distribución de la variable dependiente, TOTCHG. Haz esto creando un histograma y luego un gráfico \\(qq\\), comparando el empírico con una distribución normal. Realiza una transformación logarítmica natural y llama a la nueva variable LNTOTCHG. Examina la distribución de esta variable transformada. Para visualizar la relación logarítmica, traza LNTOTCHG frente a TOTCHG. 1.5 Reclamaciones de Seguros de Lesiones Automovilísticas. Consideramos datos de reclamaciones por lesiones en automóviles utilizando datos del Insurance Research Council (IRC), una división del American Institute for Chartered Property Casualty Underwriters y del Insurance Institute of America. Los datos, recopilados en 2002, contienen información sobre la demografía del reclamante, la participación del abogado y la pérdida económica (LOSS, en miles), entre otras variables. Aquí analizamos una muestra de \\(n=1,340\\) pérdidas de un solo estado. El estudio completo de 2002 contiene más de 70,000 reclamaciones cerradas basadas en datos de treinta y dos aseguradoras. El IRC realizó estudios similares en 1977, 1987, 1992 y 1997. Calcula las estadísticas descriptivas para la pérdida económica total (LOSS). ¿Cuál es la pérdida típica? Calcula un histograma y un gráfico \\(qq\\) (normal) para LOSS. Comenta sobre la forma de la distribución. Divide el conjunto de datos en dos submuestras, una correspondiente a las reclamaciones que involucraron a un ATTORNEY (=1) y otra en la que no se involucró un ATTORNEY (=2). c(i). Para cada submuestra, calcula la pérdida típica. ¿Parece haber una diferencia en las pérdidas típicas según la participación del abogado? c(ii) Para comparar las distribuciones, calcula un diagrama de caja (boxplot) por nivel de participación del abogado. c(iii). Para cada submuestra, calcula un histograma y un gráfico \\(qq\\). Compara las dos distribuciones entre sí. 1.6 Gastos de la Compañía de Seguros. Al igual que otros negocios, las compañías de seguros buscan minimizar los gastos asociados con la realización de negocios para mejorar la rentabilidad. Para estudiar los gastos, este ejercicio examina una muestra aleatoria de 500 compañías de seguros de la base de datos de la National Association of Insurance Commissioners (NAIC) de más de 3,000 compañías. La NAIC mantiene una de las bases de datos regulatorias de seguros más grandes del mundo; consideramos aquí datos basados en los informes anuales de 2005 para todas las compañías de seguros de propiedad y casualty en los Estados Unidos. Los informes anuales son estados financieros que utilizan principios contables estatutarios. Específicamente, nuestra variable dependiente es EXPENSES, los gastos no relacionados con reclamaciones para una compañía. Aunque no es necesario para este ejercicio, los gastos no relacionados con reclamaciones se basan en tres componentes: ajuste de pérdidas no asignadas, gastos de suscripción y gastos de inversión. El gasto por ajuste de pérdidas no asignadas es el gasto no directamente atribuible a una reclamación pero que está asociado indirectamente con la resolución de reclamaciones; incluye elementos como los salarios de los ajustadores de reclamaciones, honorarios legales, costos judiciales, testigos expertos y costos de investigación. Los gastos de suscripción consisten en costos de adquisición de pólizas, como comisiones, así como la parte de los gastos administrativos, generales y otros atribuibles a las operaciones de suscripción. Los gastos de inversión son aquellos gastos relacionados con las actividades de inversión del asegurador. Examina la distribución de la variable dependiente, EXPENSES. Haz esto creando un histograma y luego un gráfico \\(qq\\), comparando el empírico con una distribución normal. Realiza una transformación logarítmica natural y examina la distribución de esta variable transformada. ¿Ha ayudado la transformación a simetrizar la distribución? 1.7 Esperanzas de Vida Nacionales. ¿Quién está haciendo bien el cuidado de la salud? Las decisiones sobre la atención médica se toman a nivel individual, corporativo y gubernamental. Prácticamente todas las personas, corporaciones y gobiernos tienen su propia perspectiva sobre la atención médica; estas diferentes perspectivas dan lugar a una amplia variedad de sistemas para gestionar la atención médica. Comparar diferentes sistemas de atención médica nos ayuda a aprender sobre enfoques distintos al nuestro, lo que a su vez nos ayuda a tomar mejores decisiones al diseñar sistemas mejorados. Aquí, consideramos los sistemas de atención médica de \\(n=185\\) países en todo el mundo. Como medida de la calidad de la atención, utilizamos LIFEEXP, la esperanza de vida al nacer. Esta variable dependiente, con varias variables explicativas, se enumeran en [Tabla 1.4]. A partir de esta tabla, notarás que aunque hay 185 países considerados en este estudio, no todos los países proporcionaron información para cada variable. Los datos no disponibles se indican en la columna Num Miss. Los datos provienen del Informe sobre el Desarrollo Humano de las Naciones Unidas (UN). Examina la distribución de la variable dependiente, LIFEEXP. Haz esto creando un histograma y luego un gráfico \\(qq\\), comparando el empírico con una distribución normal. Realiza una transformación logarítmica natural y examina la distribución de esta variable transformada. ¿Ha ayudado la transformación a simetrizar la distribución? \\[ \\small{ \\begin{array}{ll|crrrrr} \\hline &amp; &amp; Num &amp; &amp; &amp; Std &amp; Mini- &amp; Maxi- \\\\ Variable &amp; Description &amp; Miss &amp; Mean &amp; Median &amp; Dev &amp; mum &amp; mum \\\\\\hline BIRTH &amp; \\text{ Births attended by skilled} &amp; 7 &amp; 78.25 &amp; 92.00 &amp; 26.42 &amp; 6.00 &amp; 100.00 \\\\ ~~ATTEND&amp; ~~ \\text{ health personnel} (\\%)\\\\ FEMALE &amp; \\text{ Legislators, senior officials} &amp; 87 &amp; 29.07 &amp; 30.00 &amp; 11.71 &amp; 2.00 &amp; 58.00 \\\\ ~~BOSS&amp; ~~ \\text{ and managers, % female }\\\\ FERTILITY &amp; \\text{ Total fertility rate,}&amp; 4 &amp; 3.19 &amp; 2.70 &amp; 1.71 &amp; 0.90 &amp; 7.50 \\\\ &amp; ~~ \\text{ births per woman }&amp; \\\\ GDP &amp; \\text{ Gross domestic product,} &amp; 7 &amp; 247.55 &amp; 14.20 &amp; 1,055.69 &amp; 0.10 &amp; 12,416.50 \\\\ &amp; ~~\\text{ in billions of USD} \\\\ HEALTH&amp; \\text{ 2004 Health expenditure} &amp; 5 &amp; 718.01 &amp; 297.50 &amp; 1,037.01 &amp; 15.00 &amp; 6,096.00 \\\\ ~~ EXPEND &amp; ~~ \\text{ per capita, PPP in USD} \\\\ ILLITERATE &amp; \\text{ Adult illiteracy rate,} &amp; 14 &amp; 17.69 &amp; 10.10 &amp; 19.86 &amp; 0.20 &amp; 76.40 \\\\ &amp; ~~ \\% \\text{ aged 15 and older} &amp; \\\\ PHYSICIAN &amp; \\text{ Physicians,}&amp; 3 &amp; 146.08 &amp; 107.50 &amp; 138.55 &amp; 2.00 &amp; 591.00 \\\\ &amp; ~~ \\text{ per 100,000 people} \\\\ POP &amp; \\text{ 2005 population,} &amp; 1 &amp; 35.36 &amp; 7.80 &amp; 131.70 &amp; 0.10 &amp; 1,313.00 \\\\ &amp; ~~\\text{ in millions }\\\\ PRIVATE &amp; \\text{ 2004 Private expenditure} &amp; 1 &amp; 2.52 &amp; 2.40 &amp; 1.33 &amp; 0.30 &amp; 8.50 \\\\ ~~HEALTH&amp; ~~\\text{on health, % of GDP} \\\\ PUBLIC &amp; \\text{ Public expenditure} &amp; 28 &amp; 4.69 &amp; 4.60 &amp; 2.05 &amp; 0.60 &amp; 13.40 \\\\ ~~EDUCATION&amp; ~~ \\text{ on education, % of GDP} \\\\ RESEAR &amp; \\text{ Researchers in R &amp; D,} &amp; 95 &amp; 2,034.66 &amp; 848.00 &amp; 4,942.93 &amp; 15.00 &amp; 45,454.00 \\\\ ~~CHERS&amp;~~ \\text{ per million people} &amp; \\\\ SMOKING &amp; \\text{ Prevalence of smoking,} &amp; 88 &amp; 35.09 &amp; 32.00 &amp; 14.40 &amp; 6.00 &amp; 68.00 \\\\ &amp; ~~\\text{ (male) % of adults } \\\\ \\hline LIFEEXP &amp; \\text{ Life expectancy at birth,}&amp; &amp; 67.05 &amp; 71.00 &amp; 11.08 &amp; 40.50 &amp; 82.30 \\\\ &amp; ~~ \\text{ in years } \\\\ \\hline \\end{array} } \\] 1.9 Suplemento Técnico - Teorema del Límite Central Los teoremas del límite central forman la base para gran parte de la inferencia estadística utilizada en el análisis de regresión. Por lo tanto, es útil proporcionar una declaración explícita de una versión del teorema del límite central. Teorema del Límite Central. Supongamos que \\(y_1, \\ldots, y_n\\) están distribuidos de manera independiente con media \\(\\mu\\), varianza finita \\(\\sigma^2\\) y \\(\\mathrm{E}|y|^3\\) es finito. Entonces, \\[ \\lim_{n \\rightarrow \\infty} \\Pr \\left( \\frac{\\sqrt{n}}{\\sigma }(\\overline{y} - \\mu ) \\leq x \\right) = \\Phi \\left( x \\right), \\] para cada \\(x\\), donde \\(\\Phi \\left( \\cdot \\right)\\) es la función de distribución normal estándar. Bajo las suposiciones de este teorema, la distribución reescalada de \\(\\overline{y}\\) se aproxima a una distribución normal estándar a medida que aumenta el tamaño de la muestra, \\(n\\). Interpretamos esto como que, para tamaños de muestra ‘grandes’, la distribución de \\(\\overline{y}\\) puede aproximarse mediante una distribución normal. Investigaciones empíricas han mostrado que tamaños de muestra de \\(n = 25\\) a 50 proporcionan aproximaciones adecuadas para la mayoría de los propósitos. ¿Cuándo no funciona bien el teorema del límite central? Algunos conocimientos se proporcionan mediante otro resultado de la estadística matemática. Aproximación de Edgeworth. Supongamos que \\(y_1, \\ldots, y_n\\) están distribuidos idéntica e independientemente con media \\(\\mu\\), varianza finita \\(\\sigma^2\\) y \\(\\mathrm{E}|y|^3\\) es finito. Entonces, \\[ \\Pr \\left( \\frac{\\sqrt{n}}{\\sigma }(\\overline{y} - \\mu ) \\leq x \\right) = \\Phi \\left( x \\right) + \\frac{1}{6}\\frac{1}{\\sqrt{2\\pi }} e^{-x^2/2} \\frac{\\mathrm{E}(y - \\mu)^3}{\\sigma^3 \\sqrt{n}} + \\frac{h_n}{\\sqrt{n}}, \\] para cada \\(x\\), donde \\(h_n \\rightarrow 0\\) a medida que \\(n \\rightarrow \\infty\\). Este resultado sugiere que la distribución de \\(\\bar{y}\\) se acerca más a una distribución normal a medida que la asimetría, \\(\\mathrm{E}(\\overline{y} - \\mu)^3\\), se acerca a cero. Esto es importante en aplicaciones de seguros porque muchas distribuciones tienden a ser asimétricas. Históricamente, los analistas utilizaban el segundo término en el lado derecho del resultado para proporcionar una ‘corrección’ para la aproximación de la curva normal. Véase, por ejemplo, Beard, Pentikäinen y Pesonen (1984) para una discusión adicional sobre las aproximaciones de Edgeworth en la ciencia actuarial. Una alternativa (utilizada en este libro) que vimos en la Sección 1.3 es transformar los datos, logrando así una aproximada simetría. Como sugiere el teorema de aproximación de Edgeworth, si nuestra población parental es cercana a simétrica, entonces la distribución de \\(\\overline{y}\\) será aproximadamente normal. "],["C2BasicLR.html", "Chapter 2 Regresión Lineal Básica 2.1 Correlaciones y Mínimos Cuadrados 2.2 Modelo Básico de Regresión Lineal 2.3 ¿Es Útil el Modelo? Algunas Medidas de Resumen Básicas 2.4 Propiedades de los Estimadores del Coeficiente de Regresión 2.5 Inferencia Estadística 2.6 Construyendo un Mejor Modelo: Análisis de Residuos 2.7 Aplicación: Modelo de Valoración de Activos Financieros 2.8 Salida Computacional Ilustrativa de Regresión 2.9 Lecturas Adicionales y Referencias 2.10 Ejercicios 2.11 Suplemento Técnico - Elementos del Álgebra de Matrices", " Chapter 2 Regresión Lineal Básica Vista previa del capítulo. Este capítulo considera la regresión en el caso de tener solo una variable explicativa. A pesar de esta aparente simplicidad, la mayoría de las ideas profundas de la regresión pueden desarrollarse en este marco. Al limitarnos al caso de una variable, podemos expresar muchos cálculos usando álgebra simple. Esto nos permitirá desarrollar nuestra intuición sobre las técnicas de regresión al reforzarla con demostraciones simples. Además, podemos ilustrar las relaciones entre dos variables gráficamente porque estamos trabajando en solo dos dimensiones. Las herramientas gráficas resultan ser importantes para desarrollar un vínculo entre los datos y un modelo. 2.1 Correlaciones y Mínimos Cuadrados La regresión trata sobre relaciones. Específicamente, estudiaremos cómo dos variables, una \\(x\\) y una \\(y\\), están relacionadas. Queremos poder responder preguntas como, si cambiamos el nivel de \\(x\\), ¿qué pasará con el nivel de \\(y\\)? Si comparamos dos “sujetos” que parecen similares excepto por la medición de \\(x\\), ¿cómo diferirán sus mediciones de \\(y\\)? Entender las relaciones entre variables es fundamental para la gestión cuantitativa, particularmente en ciencias actuariales donde la incertidumbre es tan prevalente. Es útil trabajar con un ejemplo específico para familiarizarnos con conceptos clave. El análisis de ventas de lotería no ha sido parte de la práctica actuarial tradicional, pero es un área de crecimiento en la que los actuarios podrían contribuir. Ejemplo: Ventas de la Lotería de Wisconsin. Los administradores de la lotería del estado de Wisconsin están interesados en evaluar los factores que afectan las ventas de lotería. Las ventas consisten en boletos de lotería en línea que se venden en establecimientos minoristas seleccionados en Wisconsin. Estos boletos generalmente tienen un precio de $1.00, por lo que el número de boletos vendidos equivale a los ingresos de la lotería. Analizamos las ventas promedio de lotería (SALES) durante un período de cuarenta semanas, de abril de 1998 a enero de 1999, en cincuenta áreas seleccionadas al azar identificadas por código postal (ZIP) dentro del estado de Wisconsin. Aunque muchas variables económicas y demográficas podrían influir en las ventas, nuestro primer análisis se centra en la población (POP) como un determinante clave. El Capítulo 3 mostrará cómo considerar variables explicativas adicionales. Intuitivamente, parece claro que las áreas geográficas con más personas tendrán mayores ventas. Entonces, otras cosas siendo iguales, un \\(x=POP\\) más grande significa un \\(y=SALES\\) más grande. Sin embargo, la lotería es una fuente importante de ingresos para el estado y queremos ser lo más precisos posible. Una notación adicional será útil posteriormente. En esta muestra, hay cincuenta áreas geográficas y usamos subíndices para identificar cada área. Por ejemplo, \\(y_1\\) = 1,285.4 representa las ventas para la primera área en la muestra que tiene una población de \\(x_1\\) = 435. Llamamos al par ordenado (\\(x_1\\), \\(y_1\\)) = (435, 1285.4) la primera observación. Extendiendo esta notación, la muestra completa que contiene cincuenta observaciones puede representarse por (\\(x_1\\), \\(y_1\\)), …, (\\(x_{50}\\), \\(y_{50}\\)). Los puntos suspensivos ( … ) significan que el patrón continúa hasta que se encuentra el último objeto. A menudo hablaremos de un miembro genérico de la muestra, refiriéndonos a (\\(x_i\\), \\(y_i\\)) como la \\(i\\)-ésima observación. Los conjuntos de datos pueden complicarse, por lo que será útil si comienza trabajando con cada variable por separado. Los dos paneles en la Figura 2.1 muestran histogramas que dan una impresión visual rápida de la distribución de cada variable de forma aislada. La Tabla 2.1 proporciona resúmenes numéricos correspondientes. Para ilustrar, para la variable población (POP), vemos que el área con el menor número contenía 280 personas, mientras que la más grande contenía 39,098. El promedio, sobre 50 códigos postales, fue de 9,311.04. Para nuestra segunda variable, las ventas fueron tan bajas como 189 y tan altas como 33,181. Figure 2.1: Histogramas de Población y Ventas. Cada distribución está sesgada a la derecha, lo que indica que hay muchas áreas pequeñas en comparación con unas pocas áreas con mayores ventas y poblaciones. Table 2.1: Estadísticas Resumen de Cada Variable Promedio Mediana Desviación Estándar Mínimo Máximo POP 9,311 4,406 11,098 280 39,098 SALES 6,495 2,426 8,103 189 33,181 Fuente: Frees y Miller (2003) Código R para producir la Figura 2.1 y la Tabla 2.1 Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURA 2.1 par(mfrow=c(1, 2), cex=1.3, mar=c(4.1,3.1,1.2,1)) hist(Lot$POP, main=&quot;&quot;, ylab=&quot;&quot;, las=1, xlab = &quot;POP&quot;) mtext(&quot;Frecuencia&quot;, side=2, at=30, las=1, cex=1.3, adj=.6) hist(Lot$SALES, main=&quot;&quot;, ylab=&quot;&quot;, las=1, xlab = &quot;SALES&quot;) mtext(&quot;Frecuencia&quot;, side=2, at=34, las=1, cex=1.3, adj=.6) # TABLA 2.1 ESTADÍSTICAS RESUMEN BookSummStats &lt;- function(Xymat){ meanSummary &lt;- sapply(Xymat, mean, na.rm=TRUE) sdSummary &lt;- sapply(Xymat, sd, na.rm=TRUE) minSummary &lt;- sapply(Xymat, min, na.rm=TRUE) maxSummary &lt;- sapply(Xymat, max, na.rm=TRUE) medSummary &lt;- sapply(Xymat, median,na.rm=TRUE) tableMat &lt;- cbind(meanSummary, medSummary, sdSummary, minSummary, maxSummary) return(tableMat) } Xymat &lt;- data.frame(cbind(Lot$POP,Lot$SALES)) tableMat &lt;- BookSummStats(Xymat) colnames(tableMat) &lt;- c(&quot;Promedio&quot; , &quot;Mediana&quot; , &quot;Desviación Estándar&quot; , &quot;Mínimo&quot; , &quot;Máximo&quot;) rownames(tableMat) &lt;- c(&quot;POP&quot;, &quot;SALES&quot;) tableMat1 &lt;- format(round(tableMat, digits=0), big.mark = &#39;,&#39;) TableGen1(TableData=tableMat1, TextTitle=&#39;Estadísticas Resumen de Cada Variable&#39;, Align=&#39;r&#39;, Digits=0, ColumnSpec=1:5, ColWidth = ColWidth5) %&gt;% footnote(general = &quot;Frees y Miller (2003)&quot;, general_title = &quot;Fuente:&quot;, footnote_as_chunk = TRUE) Como muestra la Tabla 2.1, las estadísticas resumen básicas dan ideas útiles de la estructura de las características clave de los datos. Después de entender la información en cada variable de forma aislada, podemos comenzar a explorar la relación entre las dos variables. Gráfico de Dispersión y Coeficientes de Correlación - Herramientas Básicas de Resumen La herramienta gráfica básica utilizada para investigar la relación entre dos variables es un gráfico de dispersión, como se muestra en la Figura 2.2. Aunque podemos perder los valores exactos de las observaciones al graficar los datos, ganamos una impresión visual de la relación entre la población y las ventas. En la Figura 2.2 vemos que las áreas con poblaciones más grandes tienden a comprar más boletos de lotería. ¿Qué tan fuerte es esta relación? ¿Puede el conocimiento de la población del área ayudarnos a anticipar los ingresos por ventas de lotería? Exploramos estas dos preguntas a continuación. Figure 2.2: Un gráfico de dispersión de los datos de la lotería. Cada uno de los 50 símbolos de la gráfica corresponde a un código postal en el estudio. Esta figura sugiere que las áreas postales con poblaciones más grandes tienen mayores ingresos de lotería. Código R para Producir la Figura 2.2 #Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURA 2.2, CON CORRELACIONES par(mar=c(4.1,3.8,2,1),cex=1.1) plot(Lot$POP, Lot$SALES, ylab=&quot;&quot;, las=1, xlab = &quot;POP&quot;) mtext(&quot;SALES&quot;,side=2, at=36000, las=1, cex=1.1) Una forma de resumir la fuerza de la relación entre dos variables es a través de una estadística de correlación. Definición. El coeficiente de correlación ordinario, o de Pearson se define como \\[\\begin{equation*} r=\\frac{1}{(n-1)s_xs_y}\\sum_{i=1}^{n}\\left( x_{i}-\\overline{x}\\right) \\left( y_{i}-\\overline{y}\\right) . \\end{equation*}\\] Aquí, usamos la desviación estándar de la muestra \\(s_y = \\sqrt{(n-1)^{-1} \\sum_{i=1}^{n}\\left( y_i - \\overline{y}\\right)^{2}}\\) definida en la Sección 1.2, con una notación similar para \\(s_x\\). Aunque existen otras estadísticas de correlación, el coeficiente de correlación ideado por Pearson (1895) tiene varias propiedades deseables. Una propiedad importante es que, para cualquier conjunto de datos, \\(r\\) está acotado entre -1 y 1, es decir, \\(-1\\leq r\\leq 1\\). (El Ejercicio 2.3 proporciona pasos para comprobar esta propiedad.) Si \\(r\\) es mayor que cero, se dice que las variables están correlacionadas positivamente. Si \\(r\\) es menor que cero, se dice que las variables están correlacionadas negativamente. Cuanto mayor sea el coeficiente en valor absoluto, más fuerte será la relación. De hecho, si \\(r=1\\), entonces las variables están perfectamente correlacionadas. En este caso, todos los datos se encuentran en una línea recta que pasa por los cuadrantes inferior izquierdo y superior derecho. Si \\(r=-1\\), entonces todos los datos se encuentran en una línea que pasa por los cuadrantes superior izquierdo e inferior derecho. El coeficiente \\(r\\) es una medida de una relación lineal entre dos variables. Se dice que el coeficiente de correlación es invariante a la ubicación y la escala. Así, el centro de ubicación de cada variable no importa en el cálculo de \\(r\\). Por ejemplo, si agregamos $100 a las ventas de cada código postal, cada \\(y_i\\) aumentará en 100. Sin embargo, \\(\\overline{y}\\), el precio de compra promedio, también aumentará en 100 de modo que la desviación \\(y_i - \\overline{y}\\) permanece sin cambios, o invariante. Además, la escala de cada variable no importa en el cálculo de \\(r\\). Por ejemplo, supongamos que dividimos cada población entre 1000, de modo que \\(x_i\\) ahora representa la población en miles. Así, \\(\\overline{x}\\) también se divide entre 1000 y usted debería verificar que \\(s_x\\) también se divide entre 1000. Así, la versión estandarizada de \\(x_i\\), \\(\\left( x_i-\\overline{x}\\right) /s_x\\), permanece sin cambios, o invariante. Muchos paquetes estadísticos calculan una versión estandarizada de una variable restando el promedio y dividiendo por la desviación estándar. Ahora, usemos \\(y_{i,std}=\\left( y_i- \\overline{y}\\right) /s_y\\) y \\(x_{i,std}=\\left( x_i-\\overline{x} \\right) /s_x\\) para que sean las versiones estandarizadas de \\(y_i\\) y \\(x_i\\), respectivamente. Con esta notación, podemos expresar el coeficiente de correlación como \\(r=(n-1)^{-1}\\sum_{i=1}^{n}x_{i,std}\\times y_{i,std}.\\) Se dice que el coeficiente de correlación es una medida adimensional. Esto se debe a que hemos eliminado dólares, y todas las demás unidades de medida, considerando las variables estandarizadas \\(x_{i,std}\\) y \\(y_{i,std}\\). Debido a que el coeficiente de correlación no depende de las unidades de medida, es una estadística que puede compararse fácilmente entre diferentes conjuntos de datos. En el mundo de los negocios, el término “correlación” se usa a menudo como sinónimo del término “relación.” Para los propósitos de este texto, utilizamos el término correlación cuando nos referimos únicamente a relaciones lineales. La relación no lineal clásica es \\(y=x^{2}\\), una relación cuadrática. Considere esta relación y el conjunto de datos ficticios para \\(x\\), \\(\\{-2,1,0,1,2\\}\\). Ahora, como ejercicio (2.2), produzca un gráfico aproximado del conjunto de datos: \\[ \\begin{array}{l|rrrrr} \\hline i &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\ \\hline x_i &amp; -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 \\\\ y_i &amp; 4 &amp; 1 &amp; 0 &amp; 1 &amp; 4 \\\\ \\hline \\end{array} \\] El coeficiente de correlación para este conjunto de datos resulta ser \\(r=0\\) (verifíquelo). Por lo tanto, a pesar de que hay una relación perfecta entre \\(x\\) y \\(y\\) (\\(=x^{2}\\)), hay una correlación cero. Recuerde que los cambios de ubicación y escala no son relevantes en las discusiones sobre correlación, por lo que podríamos cambiar fácilmente los valores de \\(x\\) y \\(y\\) para que sean más representativos de un conjunto de datos de negocios. ¿Qué tan fuerte es la relación entre \\(y\\) y \\(x\\) para los datos de la lotería? Gráficamente, la respuesta es un gráfico de dispersión, como en la Figura 2.2. Numéricamente, la respuesta principal es el coeficiente de correlación, que resulta ser \\(r\\) = 0.886 para este conjunto de datos. Interpretamos esta estadística diciendo que SALES y POP están correlacionados (positivamente). La fuerza de la relación es fuerte porque \\(r\\) = 0.886 está cerca de uno. En resumen, podemos describir esta relación diciendo que hay una fuerte correlación entre SALES y POP. Método de Mínimos Cuadrados Ahora comenzamos a explorar la pregunta: “¿Puede el conocimiento de la población ayudarnos a entender las ventas?” Para responder a esta pregunta, identificamos las ventas como la variable de respuesta, o dependiente. La variable de población, que se usa para ayudar a entender las ventas, se llama la variable explicativa, o independiente. Supongamos que tenemos disponibles los datos de muestra de cincuenta ventas \\(\\{y_1, \\ldots, y_{50} \\}\\) y tu trabajo es predecir las ventas de un código postal seleccionado al azar. Sin conocimiento de la variable de población, un predictor sensato es simplemente \\(\\overline{y}=6,495\\), el promedio de la muestra disponible. Naturalmente, anticipas que las áreas con mayores poblaciones tendrán mayores ventas. Es decir, si también tienes conocimiento de la población, ¿puede mejorarse esta estimación? Si es así, ¿cuánto? Para responder a estas preguntas, el primer paso asume una relación lineal aproximada entre \\(x\\) y \\(y\\). Para ajustar una línea a nuestro conjunto de datos, usamos el método de mínimos cuadrados. Necesitamos una técnica general para que, si diferentes analistas están de acuerdo en los datos y en la técnica de ajuste, entonces estarán de acuerdo en la línea. Si diferentes analistas ajustan un conjunto de datos usando aproximaciones a ojo, en general llegarán a diferentes líneas, incluso usando el mismo conjunto de datos. El método comienza con la línea \\(y=b_0^{\\ast}+b_1^{\\ast}x\\), donde la intersección y la pendiente, \\(b_0^{\\ast}\\) y \\(b_1^{\\ast}\\), son meramente valores genéricos. Para la \\(i\\)-ésima observación, \\(y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right)\\) representa la desviación del valor observado \\(y_i\\) de la línea en \\(x_i\\). La cantidad \\[\\begin{equation*} SS(b_0^{\\ast},b_1^{\\ast})=\\sum_{i=1}^{n}\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) ^{2} \\end{equation*}\\] representa la suma de desviaciones cuadradas para esta línea candidata. El método de mínimos cuadrados consiste en determinar los valores de \\(b_0^{\\ast}\\) y \\(b_1^{\\ast}\\) que minimizan \\(SS(b_0^{\\ast},b_1^{\\ast})\\). Este es un problema fácil que puede resolverse mediante cálculo, de la siguiente manera. Tomando derivadas parciales con respecto a cada argumento obtenemos \\[\\begin{equation*} \\frac{\\partial }{\\partial b_0^{\\ast}}SS(b_0^{\\ast},b_1^{\\ast})=\\sum_{i=1}^{n}(-2)\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) \\end{equation*}\\] y \\[\\begin{equation*} \\frac{\\partial }{\\partial b_1^{\\ast}}SS(b_0^{\\ast},b_1^{\\ast})=\\sum_{i=1}^{n}(-2x_i)\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) . \\end{equation*}\\] Se invita al lector a tomar las segundas derivadas parciales para asegurarse de que estamos minimizando, no maximizando, esta función. Igualando estas cantidades a cero y cancelando términos constantes obtenemos \\[\\begin{equation*} \\sum_{i=1}^{n}\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) =0 \\end{equation*}\\] y \\[\\begin{equation*} \\sum_{i=1}^{n}x_i\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) =0, \\end{equation*}\\] que son conocidas como las ecuaciones normales. Resolver estas ecuaciones proporciona los valores de \\(b_0^{\\ast}\\) y \\(b_1^{\\ast}\\) que minimizan la suma de cuadrados. Definición. Las estimaciones de intersección y pendiente de mínimos cuadrados son \\[\\begin{equation*} b_1=r\\frac{s_y}{s_x}~~~~~\\mathrm{y}~~~~~b_0=\\overline{y}-b_1 \\overline{x}. \\end{equation*}\\] La línea que determinan, \\(\\widehat{y}=b_0+b_1x\\), se llama la línea de regresión ajustada. Hemos eliminado la notación de asterisco, o estrella, porque \\(b_0\\) y \\(b_1\\) ya no son valores “candidatos”. ¿Proporciona este procedimiento una línea sensata para nuestras ventas de lotería de Wisconsin? Anteriormente, calculamos \\(r=0.886\\). A partir de esto y de las estadísticas básicas resumidas en la Tabla 2.1, tenemos \\(b_1 = 0.886 \\left( 8,103\\right) /11,098=0.647\\) y \\(b_0 = 6,495-(0.647)9,311 = 469.7\\). Esto produce la línea de regresión ajustada \\[\\begin{equation*} \\widehat{y} = 469.7 + (0.647)x. \\end{equation*}\\] El sombrero, o “gorro”, encima de la \\(y\\) nos recuerda que esta \\(\\widehat{y}\\), o \\(\\widehat{SALES}\\), es un valor ajustado. Una aplicación de la línea de regresión es estimar ventas para una población específica, digamos, \\(x=10,000\\). La estimación es la altura de la línea de regresión, que es \\(469.7 + (0.647)(10,000) = 6,939.7\\). Ejemplo: Resumiendo Simulaciones. El análisis de regresión es una herramienta para resumir datos complejos. En el trabajo práctico, los actuarios a menudo simulan escenarios financieros complicados; a menudo se pasa por alto que la regresión puede usarse para resumir relaciones de interés. Para ilustrar, Manistre y Hancock (2005) simularon muchas realizaciones de una opción put europea a 10 años y demostraron la relación entre dos medidas de riesgo actuarial, el valor en riesgo (VaR) y la expectativa de cola condicional (CTE). Para un ejemplo, estos autores examinaron rendimientos de acciones distribuidos logarítmicamente con un precio inicial de $100, de modo que en 10 años el precio de la acción estaría distribuido como \\[\\begin{equation*} S(Z)=100 \\exp \\left( (.08) 10 + .15 \\sqrt{10} Z \\right), \\end{equation*}\\] basado en un retorno medio anual del 10%, desviación estándar del 15% y el resultado de una variable aleatoria normal estándar \\(Z\\). La opción put paga la diferencia entre el precio de ejercicio, que se tomará como 110 para este ejemplo, y \\(S(Z)\\). El valor presente de esta opción es \\[\\begin{equation*} C(Z)= \\mathrm{e}^{-0.06(10)} \\mathrm{max} \\left(0, 110-S(Z) \\right), \\end{equation*}\\] basado en una tasa de descuento del 6%. Para estimar el VaR y el CTE, para cada \\(i\\), se simularon 1000 variables aleatorias normales estándar i.i.d. y se usaron para calcular 1000 valores presentes, \\(C_{i1}, \\ldots, C_{i,1000}.\\) El percentil 95 de estos valores presentes es la estimación del valor en riesgo, denotado como \\(VaR_i.\\) El promedio de los 50 valores presentes más altos (\\(= (1-.05) \\times 1000\\)) es la estimación de la expectativa de cola condicional, denotada como \\(CTE_i\\). Manistre y Hancock (2005) realizaron este cálculo \\(i=1, \\ldots, 1000\\) veces; el resultado se presenta en la Figura 2.3. El diagrama de dispersión muestra una relación fuerte pero no perfecta entre el \\(VaR\\) y el \\(CTE\\), el coeficiente de correlación resulta ser \\(r=0.782\\). Figure 2.3: Gráfico de la Expectativa de Cola Condicional (CTE) frente al Valor en Riesgo (VaR). Basado en \\(n=1,000\\) simulaciones de un bono put europeo a 10 años. Fuente: Manistre y Hancock (2005). Código R para producir la Figura 2.3 # FIGURA 2.3 # simulación S &lt;- vector(mode = &quot;numeric&quot;, length = 1000) C &lt;- vector(mode = &quot;numeric&quot;, length = 1000) Var &lt;- vector(mode = &quot;numeric&quot;, length = 1000) CTE &lt;- vector(mode = &quot;numeric&quot;, length = 1000) for (i in 1:1000){ for (j in 1:1000){ S[j] &lt;- 100 * exp(.08 * 10 + .15 * (10 ^ .5) * rnorm(1)) C[j] &lt;- exp(-.06 * 10) * max(0, 110 - S[j]) } C &lt;- sort(C) Var[i] &lt;- C[950] CTE[i] &lt;- mean(C[950:1000]) } model &lt;- lm(CTE ~ Var) b0 &lt;- round(model$coef[1], digits = 3) b1 &lt;- round(model$coef[2], digits = 3) R2 &lt;- round(summary(model)$r.squared, digits = 4) plot(Var, CTE, xlab = expression(paste(&quot;Estimaciones de VaR&quot;)), ylab = expression(paste(&quot;Estimaciones de CTE&quot;)), xlim = c(0, 12), ylim = c(8, 20), xaxs = &quot;i&quot;, yaxs = &quot;i&quot;, pch = 20, cex = 0.4) lines(Var, model$fitted, lwd = .5) abline(h = c(10, 12, 14, 16, 18, 20), col = &quot;grey&quot;) 2.2 Modelo Básico de Regresión Lineal El diagrama de dispersión, el coeficiente de correlación y la línea de regresión ajustada son herramientas útiles para resumir la relación entre dos variables para un conjunto de datos específico. Para inferir relaciones generales, necesitamos modelos para representar los resultados de poblaciones amplias. Este capítulo se centra en un modelo de “regresión lineal básica”. La parte de “regresión lineal” proviene del hecho de que ajustamos una línea a los datos. La parte de “básica” es porque usamos solo una variable explicativa, \\(x\\). Este modelo también se conoce como una “regresión lineal simple”. Este texto evita este lenguaje porque da la falsa impresión de que las ideas e interpretaciones de regresión con una variable explicativa son siempre sencillas. Ahora introducimos dos conjuntos de supuestos del modelo básico, las representaciones “observables” y de “error”. Son equivalentes, pero cada una nos ayudará a medida que extendamos los modelos de regresión más allá de lo básico. \\[ {\\small \\begin{array}{l} \\hline \\hline &amp;\\textbf{Modelo Básico de Regresión Lineal} \\\\ &amp;\\textbf{Supuestos de Muestreo de la Representación Observable} \\\\ \\hline \\text{F1}. &amp; \\mathrm{E}~y_i=\\beta_0 + \\beta_1 x_i . \\\\ \\text{F2}. &amp; \\{x_1,\\ldots ,x_n\\} \\text{son variables no estocásticas}. \\\\ \\text{F3}. &amp; \\mathrm{Var}~y_i=\\sigma ^{2}. \\\\ \\text{F4}. &amp; \\{ y_i\\} \\text{son variables aleatorias independientes}. \\\\ \\hline\\ \\end{array} } \\] La “representación observable” se enfoca en variables que podemos ver (u observar), \\((x_i,y_i)\\). La inferencia sobre la distribución de \\(y\\) es condicional a las variables explicativas observadas, de modo que podemos tratar \\(\\{x_1,\\ldots ,x_n\\}\\) como variables no estocásticas (supuesto F2). Al considerar tipos de mecanismos de muestreo para \\((x_i,y_i)\\), es conveniente pensar en un esquema de muestreo aleatorio estratificado, donde los valores de \\(\\{x_1,\\ldots ,x_n\\}\\) se tratan como los estratos, o grupos. Bajo el muestreo estratificado, para cada valor único de \\(x_i\\), tomamos una muestra aleatoria de una población. Para ilustrar, supongamos que se está extrayendo de una base de datos de empresas para comprender el rendimiento de las acciones (\\(y\\)) y desea estratificar según el tamaño de la empresa. Si la cantidad de activos es una variable continua, entonces podemos imaginar tomar una muestra de tamaño 1 para cada empresa. De esta manera, hipotetizamos una distribución de rendimientos de acciones condicional al tamaño de los activos de la empresa. Digresión: A menudo verá informes que resumen resultados para los “50 mejores gerentes” o las “100 mejores universidades”, medidos por alguna variable de resultado. En aplicaciones de regresión, asegúrese de no seleccionar observaciones basadas en una variable dependiente, como el rendimiento más alto de las acciones, porque esto es estratificar basado en el \\(y\\), no en el \\(x\\). El Capítulo 6 discutirá los procedimientos de muestreo con mayor detalle. El muestreo estratificado también proporciona motivación para el supuesto F4, la independencia entre respuestas. Se puede motivar el supuesto F1 pensando en \\((x_i,y_i)\\) como una extracción de una población, donde la media de la distribución condicional de \\(y_i\\) dado {\\(x_i\\)} es lineal en la variable explicativa. El supuesto F3 se conoce como homocedasticidad, que discutiremos ampliamente en la Sección 5.7. Ver Goldberger (1991) para más información sobre esta representación. Un quinto supuesto que a menudo se usa implícitamente es: \\[ \\text{F5}. \\{y_i\\} \\text{ están distribuidos normalmente}. \\] Este supuesto no es necesario para muchos procedimientos de inferencia estadística porque los teoremas del límite central proporcionan normalidad aproximada para muchas estadísticas de interés. Sin embargo, la justificación formal para algunas, como las estadísticas \\(t\\), requieren este supuesto adicional. En contraste con la representación observable, un conjunto alternativo de supuestos se enfoca en las desviaciones, o “errores”, en la regresión, definidos como \\(\\varepsilon_i=y_i-\\left( \\beta_0 + \\beta_1 x_i \\right)\\). \\[ {\\small \\begin{array}{l} \\hline \\hline &amp;\\textbf{Modelo Básico de Regresión Lineal} \\\\ &amp;\\textbf{Supuestos de Muestreo de la Representación de Error} \\\\ \\hline \\text{E1}. &amp; y_i=\\beta_0 + \\beta_1 x_i + \\varepsilon_i . \\\\ \\text{E2}. &amp; \\{x_1,\\ldots ,x_n\\} \\text{ son variables no estocásticas}. \\\\ \\text{E3}. &amp; \\mathrm{E}~\\varepsilon _i=0 \\text{ y } \\mathrm{Var}~\\varepsilon _i=\\sigma ^{2}. \\\\ \\text{E4}. &amp; \\{ \\varepsilon_i\\} \\text{ son variables aleatorias independientes}. \\\\ \\hline\\ \\end{array} } \\] La “representación de error” se basa en la teoría gaussiana de errores (ver Stigler, 1986, para un contexto histórico). El supuesto E1 asume que \\(y\\) es en parte debido a una función lineal de la variable explicativa observada, \\(x\\). Otras variables no observadas que influyen en la medición de \\(y\\) se interpretan como incluidas en el término de “error” \\(\\varepsilon _i\\), que también se conoce como el término de “perturbación”. La independencia de errores, E4, puede motivarse asumiendo que {\\(\\varepsilon _i\\)} se realizan a través de una muestra aleatoria simple de una población desconocida de errores. Los supuestos E1-E4 son equivalentes a F1-F4. La representación de error proporciona una base útil para motivar las medidas de ajuste (Sección 2.3). Sin embargo, una desventaja de la representación de error es que desvía la atención de las cantidades observables \\((x_i,y_i)\\) a una cantidad no observable, {\\(\\varepsilon _i\\)}. Para ilustrar, la base de muestreo, ver {\\(\\varepsilon _i\\)} como una muestra aleatoria simple, no es directamente verificable porque no se puede observar directamente la muestra {\\(\\varepsilon _i\\)}. Además, el supuesto de errores aditivos en E1 será problemático cuando consideremos modelos de regresión no lineales. La Figura 2.4 ilustra algunos de los supuestos del modelo básico de regresión lineal. Los datos (\\(x_1,y_1\\)), (\\(x_2,y_2\\)) y (\\(x_3,y_3\\)) son observados y se representan con los símbolos de trazado circulares opacos. Según el modelo, estas observaciones deben estar cerca de la línea de regresión \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x\\). Cada desviación de la línea es aleatoria. A menudo asumimos que la distribución de desviaciones puede representarse por una curva normal, como en la Figura 2.4. Figure 2.4: La distribución de la respuesta varía según el nivel de la variable explicativa. Los supuestos del modelo básico de regresión lineal describen la población subyacente. Tabla 2.2 destaca la idea de que las características de esta población pueden resumirse mediante los parámetros \\(\\beta_0\\), \\(\\beta_1\\) y \\(\\sigma ^{2}\\). En la Sección 2.1, resumimos datos de una muestra, introduciendo las estadísticas \\(b_0\\) y \\(b_1\\). La Sección 2.3 introducirá \\(s^{2}\\), la estadística correspondiente al parámetro \\(\\sigma ^{2}\\). Table 2.2. Medidas Resumen de la Población y la Muestra \\[ {\\small \\begin{array}{llccc}\\hline\\hline &amp; \\text{Resumen} \\\\ \\text{Datos} &amp; \\text{Medidas} &amp; \\text{Intercepto} &amp; \\text{Pendiente} &amp; \\text{Varianza} \\\\\\hline \\text{Población} &amp; \\text{Parámetros} &amp; \\beta_0 &amp; \\beta_1 &amp; \\sigma^2 \\\\ \\text{Muestra} &amp; \\text{Estadísticas} &amp; b_0 &amp; b_1 &amp; s^2 \\\\ \\hline \\end{array} } \\] 2.3 ¿Es Útil el Modelo? Algunas Medidas de Resumen Básicas Aunque la estadística es la ciencia de resumir datos, también es el arte de argumentar con datos. Esta sección desarrolla algunas de las herramientas básicas usadas para justificar el modelo de regresión lineal básica. Un diagrama de dispersión puede proporcionar una fuerte evidencia visual de que \\(x\\) influye en \\(y\\); desarrollar evidencia numérica nos permitirá cuantificar la fuerza de la relación. Además, la evidencia numérica será útil cuando consideremos otros conjuntos de datos donde la evidencia gráfica no sea convincente. 2.3.1 Particionando la Variabilidad Las desviaciones cuadradas, \\(\\left( y_i-\\overline{y}\\right) ^2\\), proporcionan una base para medir la dispersión de los datos. Si deseamos estimar la \\(i\\)-ésima variable dependiente sin conocimiento de \\(x\\), entonces \\(\\overline{y}\\) es una estimación adecuada y \\(y_i- \\overline{y}\\) representa la desviación de la estimación. Usamos \\(Total~SS=\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^2\\), la suma total de cuadrados, para representar la variación en todas las respuestas. Supongamos ahora que también tenemos conocimiento de \\(x\\), una variable explicativa. Usando la línea de regresión ajustada, para cada observación podemos calcular el valor ajustado correspondiente, \\(\\widehat{y}_i = b_0 + b_1x_i\\). El valor ajustado es nuestra estimación con conocimiento de la variable explicativa. Como antes, la diferencia entre la respuesta y el valor ajustado, \\(y_i- \\widehat{y}_i\\), representa la desviación de esta estimación. Ahora tenemos dos “estimaciones” de \\(y_i\\), que son \\(\\widehat{y}_i\\) y \\(\\overline{y}\\). Presumiblemente, si la línea de regresión es útil, entonces \\(\\widehat{y}_i\\) es una medida más precisa que \\(\\overline{y}\\). Para juzgar esta utilidad, descomponemos algebraicamente la desviación total como: \\[\\begin{equation} {\\small \\begin{array}{ccccc} \\underbrace{y_i-\\overline{y}} &amp; = &amp; \\underbrace{y_i-\\widehat{y}_i} &amp; + &amp; \\underbrace{\\widehat{y}_i-\\overline{y}} \\\\ \\text{desviación} &amp; = &amp; \\text{desviación} &amp; + &amp; \\text{desviación} \\\\ \\text{total} &amp; &amp; \\text{no explicada} &amp; &amp; \\text{explicada} \\\\ \\end{array} \\tag{2.1} } \\end{equation}\\] Interpreta esta ecuación como “la desviación sin conocimiento de \\(x\\) es igual a la desviación con conocimiento de \\(x\\) más la desviación explicada por \\(x\\).” La Figura 2.5 es una representación geométrica de esta descomposición. En la figura, se eligió una observación por encima de la línea, lo que da una desviación positiva de la línea de regresión ajustada, para hacer que el gráfico sea más fácil de leer. Un buen ejercicio es hacer un boceto aproximado correspondiente a la Figura 2.5 con una observación por debajo de la línea de regresión ajustada. Figure 2.5: Representación geométrica de la descomposición de la desviación. Ahora, a partir de la descomposición algebraica en la ecuación (2.1), eleva al cuadrado cada lado de la ecuación y suma sobre todas las observaciones. Después de un poco de manipulación algebraica, esto da como resultado \\[\\begin{equation} \\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^2=\\sum_{i=1}^{n}\\left( y_i-\\widehat{y}_i\\right) ^2+\\sum_{i=1}^{n}\\left( \\widehat{y}_i- \\overline{y}\\right) ^2. \\tag{2.2} \\end{equation}\\] Reescribimos esto como \\(Total~SS=Error~SS+Regression~SS\\) donde \\(SS\\) significa suma de cuadrados. Interpretamos: \\(Total~SS\\) como la variación total sin conocimiento de \\(x\\), \\(Error~SS\\) como la variación total que queda después de introducir \\(x\\), y \\(Regression~SS\\) como la diferencia entre el \\(Total~SS\\) y el \\(Error~SS\\), o la variación total “explicada” mediante el conocimiento de \\(x\\). Al elevar al cuadrado el lado derecho de la ecuación (2.1), tenemos el término de producto cruzado \\(2\\left(y_i-\\widehat{y}_i\\right) \\left( \\widehat{y}_i-\\overline{y}\\right)\\). Con la “manipulación algebraica”, se puede comprobar que la suma de los productos cruzados sobre todas las observaciones es cero. Este resultado no es cierto para todas las líneas ajustadas, pero es una propiedad especial de la línea ajustada por mínimos cuadrados. En muchos casos, la descomposición de la variabilidad se reporta a través de un solo estadístico. Definición. El coeficiente de determinación se denota por el símbolo \\(R^2\\), llamado “\\(R\\)-cuadrado”, y se define como \\[\\begin{equation*} R^2=\\frac{Regression~SS}{Total~SS}. \\end{equation*}\\] Interpretamos \\(R^2\\) como la proporción de variabilidad explicada por la línea de regresión. En un caso extremo donde la línea de regresión se ajusta perfectamente a los datos, tenemos \\(Error~SS=0\\) y \\(R^2=1\\). En el otro caso extremo donde la línea de regresión no proporciona ninguna información sobre la respuesta, tenemos \\(Regression~SS=0\\) y \\(R^2=0\\). El coeficiente de determinación está limitado por las desigualdades \\(0 \\leq R^2 \\leq 1\\) con valores mayores que implican un mejor ajuste. 2.3.2 El Tamaño de una Desviación Típica: s En el modelo de regresión lineal básica, la desviación de la respuesta de la línea de regresión, $y_i-( _0+_1x_i) $, no es una cantidad observable porque los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) no son observados. Sin embargo, usando los estimadores \\(b_0\\) y \\(b_1\\), podemos aproximar esta desviación usando \\[\\begin{equation*} e_i=y_i-\\widehat{y}_i=y_i-\\left( b_0+b_1x_i\\right) , \\end{equation*}\\] conocido como el residuo. Los residuos serán cruciales para desarrollar estrategias para mejorar la especificación del modelo en la Sección 2.6. Ahora mostramos cómo usar los residuos para estimar \\(\\sigma ^2\\). De un primer curso en estadística, sabemos que si se pudieran observar las desviaciones \\(\\varepsilon _i\\), entonces una estimación deseable de \\(\\sigma ^2\\) sería \\((n-1)^{-1}\\sum_{i=1}^{n}\\left( \\varepsilon _i-\\overline{\\varepsilon }\\right) ^2\\). Como \\(\\{\\varepsilon _i\\}\\) no se observan, usamos lo siguiente. Definición. Un estimador de \\(\\sigma ^2\\), el error cuadrático medio (MSE), se define como \\[\\begin{equation} s^2=\\frac{1}{n-2}\\sum_{i=1}^{n}e_i{}^2. \\tag{2.3} \\end{equation}\\] La raíz cuadrada positiva, \\(s=\\sqrt{s^2},\\) se llama la desviación estándar residual. Comparando las definiciones de \\(s^2\\) y \\((n-1)^{-1}\\sum_{i=1}^{n}\\left( \\varepsilon _i-\\overline{\\varepsilon }\\right) ^2\\), verá dos diferencias importantes. Primero, al definir \\(s^2\\) no hemos restado el residuo promedio de cada residuo antes de elevar al cuadrado. Esto se debe a que el residuo promedio es cero, una propiedad especial de la estimación de mínimos cuadrados (ver Ejercicio 2.14). Este resultado se puede mostrar usando álgebra y está garantizado para todos los conjuntos de datos. En segundo lugar, al definir \\(s^2\\) hemos dividido por \\(n-2\\) en lugar de \\(n-1\\). Intuitivamente, dividir por \\(n\\) o \\(n-1\\) tiende a subestimar \\(\\sigma ^2\\). La razón es que, al ajustar líneas a los datos, necesitamos al menos dos observaciones para determinar una línea. Por ejemplo, debemos tener al menos tres observaciones para que haya alguna variabilidad alrededor de una línea. ¿Cuánta “libertad” hay para la variabilidad alrededor de una línea? Diremos que los grados de libertad del error son el número de observaciones disponibles, \\(n\\), menos el número de observaciones necesarias para determinar una línea, 2 (con símbolos, \\(df=n-2\\)). Sin embargo, como vimos en la subsección de estimación de mínimos cuadrados, no necesitamos identificar dos observaciones reales para determinar una línea. La idea es que si un analista conoce la línea y \\(n-2\\) observaciones, entonces las dos observaciones restantes se pueden determinar, sin variabilidad. Al dividir por \\(n-2\\), se puede mostrar que \\(s^2\\) es un estimador insesgado de \\(\\sigma ^2\\). También podemos expresar \\(s^2\\) en términos de las sumas de cuadrados. Es decir, \\[\\begin{equation*} s^2=\\frac{1}{n-2}\\sum_{i=1}^{n}\\left( y_i-\\widehat{y}_i\\right) ^2= \\frac{Error~SS}{n-2}=MSE. \\end{equation*}\\] Esto nos lleva a la tabla de análisis de varianza o ANOVA: \\[ {\\small \\begin{array}{llcl} \\hline \\hline \\text{Tabla ANOVA} \\\\ \\hline \\text{Fuente} &amp; \\text{Suma de Cuadrados} &amp; df &amp; \\text{Cuadrado Medio} \\\\ \\hline \\text{Regresión} &amp; Regression~SS &amp; 1 &amp; Regression~MS \\\\ \\text{Error} &amp; Error~SS &amp; n-2 &amp; MSE \\\\ \\text{Total} &amp; Total~SS &amp; n-1 &amp; \\\\ \\hline \\hline \\end{array} } \\] La tabla ANOVA es simplemente un dispositivo de contabilidad utilizado para hacer un seguimiento de las fuentes de variabilidad; aparece rutinariamente en paquetes de software estadístico como parte de los resultados de la regresión. Las figuras de la columna de cuadrados medios se definen como las sumas de cuadrados (\\(SS\\)) divididas por sus respectivos grados de libertad (\\(df\\)). En particular, el cuadrado medio de los errores (\\(MSE\\)) es igual a \\(s^2\\) y la suma de cuadrados de la regresión es igual al cuadrado medio de la regresión. Esta última propiedad es específica para la regresión con una variable; no es cierta cuando consideramos más de una variable explicativa. Los grados de libertad del error en la tabla ANOVA son \\(n-2\\). Los grados de libertad totales son \\(n-1\\), lo que refleja el hecho de que la suma total de cuadrados se centra en la media (se requieren al menos dos observaciones para una variabilidad positiva). El grado de libertad único asociado con la parte de regresión significa que la pendiente, más una observación, es suficiente información para determinar la línea. Esto se debe a que se necesitan dos observaciones para determinar una línea y al menos tres observaciones para que haya alguna variabilidad alrededor de la línea. La tabla de análisis de varianza para los datos de la lotería es: Suma de Cuadrados \\(df\\) Cuadrado Medio Regresión 2,527,165,015 1 2,527,165,015 Error 690,116,755 48 14,377,432 Total 3,217,281,770 49 Código R para Producir la Tabla ANOVA de Lotería #Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) model.basiclinearreg&lt;-lm(Lot$SALES ~ Lot$POP) #summary(model.basiclinearreg) ANOVA &lt;- anova(model.basiclinearreg) row1 &lt;- c(ANOVA$`Sum Sq`[1], ANOVA$`Df`[1], ANOVA$`Mean Sq`[1]) row2 &lt;- c(ANOVA$`Sum Sq`[2], ANOVA$`Df`[2], ANOVA$`Mean Sq`[2]) row3 &lt;- c(ANOVA$`Sum Sq`[1]+ANOVA$`Sum Sq`[2], ANOVA$`Df`[1] +ANOVA$`Df`[2], NaN) ANOVATable &lt;- rbind(row1, row2, row3) ANOVATable1 &lt;- format(round(ANOVATable, digits = 0), big.mark = &#39;,&#39;) ANOVATable1[3,3] &lt;- &quot;&quot; rownames(ANOVATable1) &lt;- c(&quot;Regresión&quot;, &quot;Error&quot;, &quot;Total&quot;) colnames(ANOVATable1) &lt;- c(&quot;Suma de Cuadrados&quot;, &quot;$df$&quot;, &quot;Cuadrado Medio&quot;) kable(ANOVATable1, align = &#39;r&#39;) %&gt;% kable_styling(position = &quot;center&quot;, full_width = FALSE) %&gt;% kableExtra::kable_classic(font = 12, html_font = &quot;Cambria&quot;) De esta tabla, puede verificar que \\(R^2=78.5\\%\\) y \\(s=3,792.\\) 2.4 Propiedades de los Estimadores del Coeficiente de Regresión Las estimaciones de mínimos cuadrados se pueden expresar como una suma ponderada de las respuestas. Para ver esto, define los pesos \\[\\begin{equation*} w_i=\\frac{x_i-\\overline{x}}{s_x^2(n-1)}. \\end{equation*}\\] Como la suma de las desviaciones de \\(x\\) (\\(x_i-\\overline{x}\\)) es cero, vemos que \\(\\sum_{i=1}^{n}w_i=0\\). Así, podemos expresar la estimación de la pendiente \\[\\begin{equation} b_1=r\\frac{s_y}{s_x}=\\frac{1}{(n-1)s_x^2}\\sum_{i=1}^{n}\\left( x_i-\\overline{x}\\right) \\left( y_i-\\overline{y}\\right) =\\sum_{i=1}^{n}w_i\\left( y_i-\\overline{y}\\right) =\\sum_{i=1}^{n}w_iy_i. \\tag{2.4} \\end{equation}\\] Los ejercicios piden al lector verificar que \\(b_0\\) también puede expresarse como una suma ponderada de las respuestas, por lo que nuestra discusión se refiere a ambos coeficientes de regresión. Dado que los coeficientes de regresión son sumas ponderadas de respuestas, pueden verse afectados drásticamente por observaciones inusuales (ver Sección 2.6). Como \\(b_1\\) es una suma ponderada, es sencillo derivar la esperanza y la varianza de esta estadística. Por la linealidad de las esperanzas y la Suposición F1, tenemos \\[\\begin{equation*} \\mathrm{E}~b_1=\\sum_{i=1}^{n}w_i~\\mathrm{E}~y_i=\\beta_0\\sum_{i=1}^{n}w_i+\\beta_1\\sum_{i=1}^{n}w_ix_i=\\beta_1. \\end{equation*}\\] Es decir, \\(b_1\\) es un estimador imparcial de \\(\\beta_1\\). Aquí, la suma $ _{i=1}^{n}w_ix_i$ \\(=\\) \\(\\left[ s_x^2(n-1)\\right] ^{-1}\\sum_{i=1}^{n}\\left( x_i-\\overline{x}\\right) x_i\\) \\(=\\left[s_x^2(n-1)\\right] ^{-1}\\sum_{i=1}^{n}\\left( x_i-\\overline{x}\\right) ^2=1.\\) A partir de la definición de los pesos, una sencilla algebra también muestra que \\(\\sum_{i=1}^{n}w_i^2=1/\\left( s_x^2(n-1)\\right)\\). Además, la independencia de las respuestas implica que la varianza de la suma es la suma de las varianzas, y así tenemos \\[\\begin{equation*} \\mathrm{Var}~b_1 =\\sum_{i=1}^{n}w_i^2\\mathrm{Var}~y_i=\\frac{\\sigma^2}{s_x^2(n-1)}. \\end{equation*}\\] Sustituyendo \\(\\sigma ^2\\) por su estimador \\(s^2\\) y tomando raíces cuadradas se obtiene lo siguiente. Definición. El error estándar de \\(b_1\\), la desviación estándar estimada de \\(b_1\\), se define como \\[\\begin{equation} se(b_1)=\\frac{s}{s_x\\sqrt{n-1}}. \\tag{2.5} \\end{equation}\\] Esta es nuestra medida de la fiabilidad, o precisión, del estimador de la pendiente. Usando la ecuación (2.5), vemos que \\(se(b_1)\\) está determinado por tres cantidades: \\(n\\), \\(s\\) y \\(s_x\\), de la siguiente manera: Si tenemos más observaciones, de manera que \\(n\\) sea mayor, entonces \\(se(b_1)\\) será menor, manteniendo todo lo demás constante. Si las observaciones tienen una mayor tendencia a estar más cerca de la línea, de manera que \\(s\\) sea menor, entonces \\(se(b_1)\\) será menor, manteniendo todo lo demás constante. Si los valores de la variable explicativa están más dispersos, de manera que \\(s_x\\) aumenta, entonces \\(se(b_1)\\) será menor, manteniendo todo lo demás constante. Valores menores de \\(se(b_1)\\) ofrecen una mejor oportunidad para detectar relaciones entre \\(y\\) y \\(x\\). La Figura 2.6 ilustra estas relaciones. Aquí, el diagrama de dispersión en el medio tiene el valor más pequeño de \\(se(b_1)\\). Comparado con el gráfico del medio, el gráfico de la izquierda tiene un valor mayor de \\(s\\) y por lo tanto \\(se(b_1)\\). Comparado con el gráfico de la derecha, el gráfico del medio tiene un valor mayor de \\(s_x\\), y por lo tanto un valor menor de \\(se(b_1)\\). Figure 2.6: Estos tres diagramas de dispersión muestran la misma relación lineal entre \\(y\\) y \\(x\\). El gráfico a la izquierda muestra una mayor variabilidad alrededor de la línea que el gráfico del medio. El gráfico a la derecha muestra una desviación estándar menor en \\(x\\) que el gráfico del medio. Código R para producir la Figura 2.6 # FIGURA 2.6 AQUÍ par(mfrow=c(1, 3),mar=c(3.8,2.8,1,1), cex=1.3) x &lt;- c(1,2,2.3,2.5,1.5,1.7,2.6,2.8,.9,.88,.8,1.2,1.3,1.45,1.8,2.2,2.1) y &lt;- c(.5,2.2,2.6,2.5,.8,1.5,2.3,2.4,.75,.7,1.3,1.5,1.7,2.3,2.3,2.7,1.25) plot(x,y, xlim=c(0.5,3), ylim=c(0,3.5), bty=&quot;l&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab=&quot;&quot;) mtext(&quot;y&quot;, side=2, at=3.5, line=2, las=1, cex=1.3) mtext(&quot;x&quot;, side=1, line=2, cex=1.3) a &lt;- seq(.75,2.75, by = .001) b = a lines(a,b) x &lt;- c(1,2,2.3,2.5,1.5,1.7,2.6,2.8,.9,.88,.8,1.2,1.3,1.45,1.8,2.2,2.45) y &lt;- c(1,2,2.3,2.5,1.2,1.6,2.4,2.6,1.1,1.11,1.2,1.3,1.45,1.6,1.95,2.3,2.7) plot(x,y, xlim=c(0.5,3), ylim=c(0,3.5), bty=&quot;l&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab=&quot;&quot;) mtext(&quot;y&quot;, side=2, at=3.5, line=2, las=1, cex=1.3) mtext(&quot;x&quot;, side=1, line=2, cex=1.3) a &lt;- seq(.75,2.75, by = .001) b = a lines(a,b) x &lt;- c(1,2,2.3,2.5,1.5,1.7,2.6,2.8,2.6,1.5,2,1.2,1.3,1.45,1.8,2.2,2.45) y &lt;- c(1,2,2.3,2.5,1.2,1.6,2.4,2.6,2,2,2.4,1.3,1.55,1.6,1.95,1.6,2.7) plot(x,y, xlim=c(-1.5,5), ylim=c(-2,5.5), bty=&quot;l&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab=&quot;&quot;) mtext(&quot;y&quot;, side=2, at=5.1, line=2, las=1, cex=1.3) mtext(&quot;x&quot;, side=1, line=2, cex=1.3) a &lt;- seq(-.5,4.5, by = .001) b = a lines(a,b) La ecuación (2.4) también implica que el coeficiente de regresión \\(b_1\\) sigue una distribución normal. Es decir, recordemos de la estadística matemática que las combinaciones lineales de variables aleatorias normales también son normales. Así, si se cumple la Suposición F5, entonces \\(b_1\\) sigue una distribución normal. Además, existen varias versiones de los teoremas del límite central para sumas ponderadas (ver, por ejemplo, Serfling, 1980). Así, como se discute en la Sección 1.4, si las respuestas \\(y_i\\) están siquiera aproximadamente distribuidas normalmente, entonces será razonable usar una aproximación normal para la distribución muestral de \\(b_1\\). Usando \\(se(b_1)\\) como la desviación estándar estimada de \\(b_1\\), para valores grandes de \\(n\\) tenemos que \\(\\left( b_1-\\beta_1\\right) /se(b_1)\\) tiene una distribución normal estándar aproximada. Aunque no lo probaremos aquí, bajo la Suposición F5 \\(\\left( b_1-\\beta_1\\right) /se(b_1)\\) sigue una distribución \\(t\\) con grados de libertad \\(df=n-2\\). 2.5 Inferencia Estadística Una vez que hemos ajustado un modelo con un conjunto de datos, podemos hacer una serie de afirmaciones importantes. Generalmente, es útil pensar en estas afirmaciones en tres categorías: (i) pruebas de ideas hipotetizadas, (ii) estimaciones de parámetros del modelo y (iii) predicciones de nuevos resultados. 2.5.1 ¿Es Importante la Variable Explicativa?: La Prueba t Respondemos a la pregunta de si la variable explicativa es importante investigando si \\(\\beta_1=0\\). La lógica es que si \\(\\beta_1=0\\), entonces el modelo de regresión lineal básico ya no incluye una variable explicativa \\(x\\). Por lo tanto, traducimos nuestra pregunta sobre la importancia de la variable explicativa en una pregunta más específica que puede ser respondida utilizando el marco de pruebas de hipótesis. Esta pregunta más específica es: ¿es válida la hipótesis nula \\(H_0:\\beta_1=0\\)? Respondemos a esta pregunta observando la estadística de prueba: \\[ {\\small t-\\mathrm{ratio}=\\frac{\\mathrm{valor~estimado~del~parámetro~-~valor~hipotetizado}} {\\mathrm{error~estándar~del~estimador}}. } \\] En el caso de \\(H_0:\\beta_1=0\\), examinamos la razón t \\(t(b_1)=b_1/se(b_1)\\) porque el valor hipotetizado de \\(\\beta_1\\) es 0. Esta es la estandarización apropiada porque, bajo la hipótesis nula y las suposiciones del modelo descritas en la Sección 2.4, la distribución muestral de \\(t(b_1)\\) se puede demostrar que sigue una distribución t con \\(df=n-2\\) grados de libertad. Así, para probar la hipótesis nula \\(H_0\\) contra la alternativa \\(H_{a}:\\beta_1\\neq 0\\), rechazamos \\(H_0\\) a favor de \\(H_{a}\\) si \\(|t(b_1)|\\) excede un valor t. Aquí, este valor t es un percentil de la distribución t usando \\(df=n-2\\) grados de libertad. Denotamos el nivel de significancia como \\(\\alpha\\) y este valor t como \\(t_{n-2,1-\\alpha /2}\\). Ejemplo: Ventas de Lotería - Continuación. Para el ejemplo de ventas de lotería, la desviación estándar residual es \\(s=3,792\\). En la Tabla 2.1, tenemos \\(s_x = 11,098\\). Por lo tanto, el error estándar de la pendiente es \\(se(b_1) = 3792/(11098\\sqrt{50-1})=0.0488\\). Según la Sección 2.1, la estimación de la pendiente es \\(b_1=0.647\\). Por lo tanto, la estadística t es \\(t(b_1) = 0.647/0.0488 = 13.4\\). Interpretamos esto diciendo que la pendiente está 13.4 errores estándar por encima de cero. Para el nivel de significancia, usamos el valor habitual de \\(\\alpha\\) = 5%. El percentil 97.5 de una distribución t con \\(df=50-2=48\\) grados de libertad es \\(t_{48,0.975}=2.011\\). Dado que \\(|13.4|&gt;2.011\\), rechazamos la hipótesis nula de que la pendiente \\(\\beta_1 = 0\\) a favor de la alternativa de que \\(\\beta_1 \\neq 0\\). Tomar decisiones comparando una razón t con un valor t se llama una prueba t. Probar \\(H_0:\\beta_1=0\\) frente a \\(H_{a}:\\beta_1\\neq 0\\) es solo una de las muchas pruebas de hipótesis que se pueden realizar, aunque es la más común. Tabla 2.3 describe procedimientos alternativos para la toma de decisiones. Estos procedimientos son para probar \\(H_0:\\beta_1 = d\\) donde \\(d\\) es un valor prescrito por el usuario que puede ser igual a cero o cualquier otro valor conocido. Por ejemplo, en nuestro ejemplo de la Sección 2.7, usaremos \\(d=1\\) para probar teorías financieras sobre el mercado de valores. Tabla 2.3 Procedimientos de Toma de Decisiones para Probar \\(H_0:\\beta_1 = d\\) \\[ {\\small \\begin{array}{c|c} \\hline \\text{Hipótesis Alternativa} (H_{a}) &amp; \\text{Procedimiento: Rechazar } H_0 \\text{ a favor de } H_{a} \\text{ si} \\\\ \\hline \\beta_1&gt;d &amp; t-\\mathrm{ratio}&gt;t_{n-2,1-\\alpha }. \\\\ \\beta_1&lt;d &amp; t-\\mathrm{ratio}&lt;-t_{n-2,1-\\alpha }. \\\\ \\beta_1\\neq d &amp; |t-\\mathrm{ratio}\\mathit{|}&gt;t_{n-2,1-\\alpha /2}. \\\\ \\end{array} }\\\\ {\\small \\begin{array}{l} \\hline \\text{Notas: El nivel de significancia es } \\alpha . \\text{Aquí, }t_{n-2,1-\\alpha} \\text{ es el percentil } (1-\\alpha )\\\\ ~~\\text{de la distribución *t* con } df=n-2 \\text{ grados de libertad.}\\\\ ~~\\text{La estadística de prueba es }t-\\mathrm{ratio} = (b_1 -d)/se(b_1) . \\\\ \\hline \\end{array} } \\] Alternativamente, se pueden construir valores de probabilidad (\\(p\\)-) y compararlos con los niveles de significancia dados. El valor \\(p\\)- es una estadística resumen útil para el analista de datos ya que permite al lector del informe entender la fuerza de la desviación de la hipótesis nula. Tabla 2.4 resume el procedimiento para calcular los valores \\(p\\)-. Tabla 2.4 Valores de Probabilidad para Probar \\(H_0:\\beta_1 = d\\) \\[ {\\small \\begin{array}{c|ccc} \\hline \\text{Hipótesis} &amp; &amp; &amp; \\\\ \\text{Alternativa} (H_a) &amp; \\beta_1&gt;d &amp; \\beta_1&lt;d &amp; \\beta_1\\neq d \\\\ \\hline p-value &amp; \\Pr(t_{n-2}&gt;t-\\mathrm{ratio}) &amp; \\Pr(t_{n-2}&lt;t-\\mathrm{ratio}) &amp; \\Pr (|t_{n-2}|&gt;|t-\\mathrm{ratio}\\mathit{|}) \\\\\\hline \\end{array} }\\\\ {\\small \\begin{array}{l} \\hline \\text{Notas: Aquí, }t_{n-2} \\text{ es una variable aleatoria distribuida como *t* con } df=n-2 \\text{ grados de libertad.}\\\\ ~~\\text{La estadística de prueba es }t-\\mathrm{ratio} = (b_1 -d)/se(b_1) . \\\\ \\hline \\end{array} } \\] Otra forma interesante de abordar la cuestión de la importancia de una variable explicativa es a través del coeficiente de correlación. Recuerda que el coeficiente de correlación es una medida de la relación lineal entre \\(x\\) e \\(y\\). Denotemos esta estadística por \\(r(y,x)\\). Esta cantidad no se ve afectada por cambios de escala en ninguna de las variables. Por ejemplo, si multiplicamos la variable \\(x\\) por el número \\(b_1\\), entonces el coeficiente de correlación permanece sin cambios. Además, las correlaciones no cambian con los desplazamientos aditivos. Así, si agregamos un número, digamos \\(b_0\\), a cada variable \\(x\\), entonces el coeficiente de correlación permanece sin cambios. Usar un cambio de escala y un desplazamiento aditivo en la variable \\(x\\) puede utilizarse para producir el valor ajustado \\(\\widehat{y}=b_0+b_1x\\). Por lo tanto, usando la notación, tenemos \\(r(y,x)=r(y,\\widehat{y})\\). Así, podemos interpretar que la correlación entre las respuestas y la variable explicativa es igual a la correlación entre las respuestas y los valores ajustados. Esto lleva al siguiente hecho algebraico interesante: \\(R^2=r^2.\\) Es decir, el coeficiente de determinación es igual al cuadrado del coeficiente de correlación. Esto es mucho más fácil de interpretar si uno piensa en \\(r\\) como la correlación entre los valores observados y los ajustados. Consulta el Ejercicio 2.13 para los pasos útiles para confirmar este resultado. 2.5.2 Intervalos de Confianza Los investigadores a menudo citan el mecanismo formal de pruebas de hipótesis para responder a la pregunta: “¿Tiene la variable explicativa una influencia real en la respuesta?” Una pregunta de seguimiento natural es: “¿En qué medida afecta \\(x\\) a \\(y\\)?” Hasta cierto punto, se puede responder utilizando el tamaño del \\(t\\)-ratio o el valor de \\(p\\). Sin embargo, en muchos casos, un intervalo de confianza para la pendiente es más útil. Para introducir los intervalos de confianza para la pendiente, recordemos que \\(b_1\\) es nuestro estimador puntual de la verdadera pendiente desconocida \\(\\beta_1\\). La Sección 2.4 argumentó que este estimador tiene un error estándar \\(se(b_1)\\) y que \\(\\left( b_1-\\beta_1\\right) /se(b_1)\\) sigue una distribución \\(t\\) con \\(n-2\\) grados de libertad. Las declaraciones de probabilidad se pueden invertir para obtener intervalos de confianza. Usando esta lógica, tenemos el siguiente intervalo de confianza para la pendiente \\(\\beta_1\\). Definición. Un intervalo de confianza del \\(100(1-\\alpha)\\)% para la pendiente \\(\\beta_1\\) es \\[\\begin{equation} b_1\\pm t_{n-2,1-\\alpha /2} ~se(b_1). \\tag{2.6} \\end{equation}\\] Al igual que con las pruebas de hipótesis, \\(t_{n-2,1-\\alpha /2}\\) es el percentil (1-\\(\\alpha\\) /2) de la distribución \\(t\\) con \\(df=n-2\\) grados de libertad. Debido a la naturaleza bilateral de los intervalos de confianza, el percentil es 1 - (1 - nivel de confianza) / 2. En este texto, por simplicidad, generalmente usamos un intervalo de confianza del 95%, por lo que el percentil es 1-(1-0.95)/2 = 0.975. El intervalo de confianza proporciona un rango de confiabilidad que mide la utilidad de la estimación. En la Sección 2.1, establecimos que la estimación de la pendiente por mínimos cuadrados para el ejemplo de ventas de lotería es \\(b_1=0.647\\). La interpretación es que si la población de un código postal difiere en 1,000, entonces esperamos que las ventas promedio de lotería difieran en $647. ¿Qué tan confiable es esta estimación? Resulta que \\(se(b_1)=0.0488\\) y, por lo tanto, un intervalo de confianza aproximado del 95% para la pendiente es \\[\\begin{equation*} 0.647\\pm (2.011)(.0488), \\end{equation*}\\] o (0.549, 0.745). De manera similar, si la población difiere en 1,000, un intervalo de confianza del 95% para el cambio esperado en las ventas es (549, 745). Aquí, usamos el valor \\(t\\) \\(t_{48,0.975}=2.011\\) porque hay 48 (= \\(n\\)-2) grados de libertad y, para un intervalo de confianza del 95%, necesitamos el percentil 97.5. 2.5.3 Intervalos de Predicción En la Sección 2.1, mostramos cómo usar los estimadores de mínimos cuadrados para predecir las ventas de lotería para un código postal, fuera de nuestra muestra, con una población de 10,000. Dado que la predicción es una tarea tan importante para los actuarios, formalizamos el procedimiento para que pueda ser utilizado regularmente. Para predecir una observación adicional, asumimos que el nivel de la variable explicativa es conocido y se denota por \\(x_{\\ast}\\). Por ejemplo, en nuestro ejemplo anterior de ventas de lotería usamos \\(x_{\\ast} = 10,000\\). También asumimos que la observación adicional sigue el mismo modelo de regresión lineal que las observaciones en la muestra. Usando nuestros estimadores de mínimos cuadrados, nuestra predicción puntual es \\(\\widehat{y}_{\\ast} = b_0 + b_1 x_{\\ast}\\), la altura de la línea de regresión ajustada en \\(x_{\\ast}\\). Podemos descomponer el error de predicción en dos partes: \\[ \\begin{array}{ccccc} \\underbrace{y_{\\ast} - \\widehat{y}_{\\ast}} &amp; = &amp; \\underbrace{\\beta_0 - b_0 + \\left( \\beta_1 - b_1 \\right) x_{\\ast}} &amp; + &amp; \\underbrace{\\varepsilon_{\\ast}} \\\\ {\\small \\text{error de predicción}} &amp; {\\small =} &amp; {\\small \\text{error en la estimación de la }} &amp; {\\small +} &amp; {\\small \\text{desviación de la observación adicional}} \\\\ &amp; &amp; {\\small \\text{línea de regresión en } x}_{\\ast} &amp; &amp; {\\small \\text{respuesta de su media}} \\end{array} \\] Se puede demostrar que el error estándar de la predicción es \\[\\begin{equation*} se(pred) = s \\sqrt{1+\\frac{1}{n}+\\frac{\\left( x_{\\ast}-\\overline{x}\\right) ^2}{(n-1)s_x^2}}. \\end{equation*}\\] Al igual que con \\(se(b_1)\\), los términos \\(n^{-1}\\) y $( x_{}- ) ^2/$ se acercan a cero a medida que el tamaño de la muestra \\(n\\) se vuelve grande. Por lo tanto, para grandes \\(n\\), tenemos que \\(se(pred)\\approx s\\), lo que refleja que el error en la estimación de la línea de regresión en un punto se vuelve insignificante y la desviación de la respuesta adicional de su media se convierte en la única fuente de incertidumbre. Definición. Un intervalo de predicción del \\(100(1-\\alpha)\\)% en \\(x_{\\ast}\\) es \\[\\begin{equation} \\widehat{y}_{\\ast} \\pm t_{n-2,1-\\alpha /2} ~se(pred) \\tag{2.7} \\end{equation}\\] donde el valor \\(t\\) \\(t_{n-2,1-\\alpha /2}\\) es el mismo que se usa para la prueba de hipótesis y el intervalo de confianza. Por ejemplo, la predicción puntual en \\(x_{\\ast} = 10,000\\) es \\(\\widehat{y}_{\\ast}\\)= 469.7 + 0.647 (10000) = 6,939.7. El error estándar de esta predicción es \\[\\begin{equation*} se(pred) = 3,792 \\sqrt{1+\\frac{1}{50} + \\frac{\\left( 10,000-9,311\\right)^2}{(50-1)(11,098)^2}} = 3,829.6. \\end{equation*}\\] Con un valor \\(t\\) igual a 2.011, esto da lugar a un intervalo de predicción aproximado del 95% \\[\\begin{equation*} 6,939.7 \\pm (2.011)(3,829.6) = 6,939.7 \\pm 7,701.3 = (-761.6, ~14,641.0). \\end{equation*}\\] Interpretamos estos resultados señalando primero que nuestra mejor estimación de ventas de lotería para un código postal con una población de 10,000 es 6,939.70. Nuestro intervalo de predicción del 95% representa un rango de confiabilidad para esta predicción. Si pudiéramos observar muchos códigos postales, cada uno con una población de 10,000, en promedio esperaríamos que aproximadamente 19 de cada 20, o el 95%, tendrían ventas de lotería entre 0 y 14,641. Es habitual truncar el límite inferior del intervalo de predicción a cero si se considera que los valores negativos de la respuesta son inapropiados. Código R para producir los análisis de la Sección 2.5 # RESULTADOS DE LA SECCIÓN 2.1 model.basiclinearreg&lt;-lm(SALES ~ POP, data = Lot) summary(model.basiclinearreg) # SECCIÓN 2.5.2 INTERVALOS DE CONFIANZA confint(model.basiclinearreg) confint(model.basiclinearreg, level=.90) # SECCIÓN 2.5.3 INTERVALOS DE PREDICCIÓN newdata &lt;- data.frame(POP &lt;- 10000) predict(model.basiclinearreg, newdata, interval=&quot;prediction&quot;) predict(model.basiclinearreg, newdata, interval=&quot;prediction&quot;, level=.90) # PROPORCIONA EL PERCENTIL 97.5 DE UNA DISTRIBUCIÓN T, SOLO PARA COMPROBAR qt(.975, 48) 2.6 Construyendo un Mejor Modelo: Análisis de Residuos Las disciplinas cuantitativas calibran modelos con datos. La estadística lleva esto un paso más allá, utilizando las discrepancias entre las suposiciones y los datos para mejorar la especificación del modelo. Examinaremos las suposiciones del modelo de la Sección 2.2 a la luz de los datos y utilizaremos cualquier desajuste para especificar un mejor modelo; este proceso se conoce como verificación diagnóstica (como cuando vas al médico y él o ella realiza pruebas diagnósticas para revisar tu salud). Comenzaremos con la representación del error de la Sección 2.2. Bajo este conjunto de suposiciones, las desviaciones {\\(\\varepsilon _i\\)} son idénticamente e independientemente distribuidas (i.i.d), y bajo la suposición F5, distribuidas normalmente. Para evaluar la validez de estas suposiciones, se usan los residuos (observados) {\\(e_i\\)} como aproximaciones para las desviaciones (no observadas) {\\(\\varepsilon _i\\)}. El tema básico es que si los residuos están relacionados con una variable o muestran algún otro patrón reconocible, entonces deberíamos poder aprovechar esta información y mejorar la especificación de nuestro modelo. Los residuos deberían contener poca o ninguna información y representar solo la variación natural de la muestra que no se puede atribuir a ninguna fuente específica. Análisis de residuos es el ejercicio de verificar los residuos en busca de patrones. Existen cinco tipos de discrepancias en el modelo que los analistas comúnmente buscan. Si se detectan, las discrepancias pueden corregirse con los ajustes apropiados en la especificación del modelo. Problemas de Especificación del Modelo Falta de Independencia. Puede haber relaciones entre las desviaciones {\\(\\varepsilon _i\\)} de modo que no sean independientes. Heterocedasticidad. La suposición E3 indica que todas las observaciones tienen una variabilidad común (aunque desconocida), conocida como homocedasticidad. Heterocedasticidad es el término usado cuando la variabilidad varía según la observación. Relaciones entre Desviaciones del Modelo y Variables Explicativas. Si una variable explicativa tiene la capacidad de ayudar a explicar la desviación \\(\\varepsilon\\), entonces deberíamos poder usar esta información para predecir mejor \\(y\\). Distribuciones No Normales. Si la distribución de la desviación representa una desviación seria de la normalidad, entonces los procedimientos de inferencia usuales ya no son válidos. Puntos Inusuales. Las observaciones individuales pueden tener un gran efecto en el ajuste del modelo de regresión, lo que significa que los resultados pueden ser sensibles al impacto de una sola observación. Esta lista servirá al lector durante el estudio del análisis de regresión. Por supuesto, con solo una introducción a los modelos básicos aún no hemos visto modelos alternativos que podrían usarse cuando encontramos estas discrepancias en el modelo. En la Parte II de este libro sobre modelos de series temporales, estudiaremos la falta de independencia entre datos ordenados en el tiempo. El Capítulo 5 considerará la heterocedasticidad con más detalle. La introducción a la regresión lineal múltiple en el Capítulo 3 será nuestra primera vista sobre cómo manejar las relaciones entre {\\(\\varepsilon _i\\)} y variables explicativas adicionales. Sin embargo, ya hemos tenido una introducción al efecto de las distribuciones normales, viendo que los gráficos \\(qq\\) pueden detectar la no normalidad y que las transformaciones pueden ayudar a inducir la normalidad aproximada. En esta sección, discutimos los efectos de los puntos inusuales. Gran parte del análisis de residuos se realiza examinando un residuo estandarizado, que es un residuo dividido por su error estándar. Un error estándar aproximado del residuo es \\(s\\); en el Capítulo 3 daremos una definición matemática precisa. Hay dos razones por las que a menudo examinamos residuos estandarizados en lugar de residuos básicos. Primero, si las respuestas están distribuidas normalmente, entonces los residuos estandarizados son aproximadamente realizaciones de una distribución normal estándar. Esto proporciona una distribución de referencia para comparar los valores de los residuos estandarizados. Por ejemplo, si un residuo estandarizado supera dos en valor absoluto, esto se considera inusualmente grande y la observación se llama outlier (punto atípico). Segundo, dado que los residuos estandarizados son adimensionales, podemos transferir la experiencia de un conjunto de datos a otro. Esto es cierto independientemente de si la distribución de referencia normal es aplicable o no. Puntos Atípicos y Puntos de Alta Influencia. Otra parte importante del análisis de residuos es la identificación de observaciones inusuales en un conjunto de datos. Debido a que las estimaciones de regresión son promedios ponderados con pesos que varían según la observación, algunas observaciones son más importantes que otras. Esta ponderación es más importante de lo que muchos usuarios del análisis de regresión se dan cuenta. De hecho, el ejemplo a continuación demuestra que una sola observación puede tener un efecto dramático en un gran conjunto de datos. Hay dos direcciones en las que un punto de datos puede ser inusual: la dirección horizontal y la dirección vertical. Por “inusual”, nos referimos a que una observación bajo consideración parece estar lejos de la mayoría del conjunto de datos. Una observación que es inusual en la dirección vertical se llama punto atípico. Una observación que es inusual en la dirección horizontal se llama punto de alta influencia. Una observación puede ser tanto un punto atípico como un punto de alta influencia. Ejemplo: Puntos Atípicos y Puntos de Alta Influencia. Considera el conjunto de datos ficticio de 19 puntos más tres puntos, etiquetados como A, B y C, que se muestra en la Figura 2.7 y Tabla 2.5. Piensa en los primeros 19 puntos como observaciones “buenas” que representan algún tipo de fenómeno. Queremos investigar el efecto de agregar un solo punto aberrante. Tabla 2.5. 19 Puntos Base Más Tres Tipos de Observaciones Inusuales \\[ \\begin{array}{c|cccccccccc|ccc} %\\hline Variables &amp; \\multicolumn{10}{|c|}{19 Puntos Base} &amp; A &amp; B &amp; C \\hline Variables &amp; &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp; &amp; A &amp; B &amp; C \\\\ \\hline x &amp; 1.5 &amp; 1.7 &amp; 2.0 &amp; 2.2 &amp; 2.5 &amp; 2.5 &amp; 2.7 &amp; 2.9 &amp; 3.0 &amp; 3.5 &amp; 3.4 &amp; 9.5 &amp; 9.5 \\\\ y &amp; 3.0 &amp; 2.5 &amp; 3.5 &amp; 3.0 &amp; 3.1 &amp; 3.6 &amp; 3.2 &amp; 3.9 &amp; 4.0 &amp; 4.0 &amp; 8.0 &amp; 8.0 &amp; 2.5 \\\\ \\hline x &amp; 3.8 &amp; 4.2 &amp; 4.3 &amp; 4.6 &amp; 4.0 &amp; 5.1 &amp; 5.1 &amp; 5.2 &amp; 5.5 &amp; &amp; &amp; &amp; \\\\ y &amp; 4.2 &amp; 4.1 &amp; 4.8 &amp; 4.2 &amp; 5.1 &amp; 5.1 &amp; 5.1 &amp; 4.8 &amp; 5.3 &amp; &amp; &amp; &amp; \\\\ \\hline \\end{array} \\] Figure 2.7: Gráfico de dispersión de 19 puntos base más tres puntos inusuales, etiquetados A, B, y C. Código R para Producir la Figura 2.7 # EJEMPLO 2.6 PUNTO ATÍPICO OUTLR &lt;- read.csv(&quot;CSVData/OutlierExample.csv&quot;, header=TRUE) # FIGURA 2.7 par(mar=c(4.1,3.1,1.1,.1), cex=1.3) plot(OUTLR$X, OUTLR$Y, xlab=&quot;x&quot;, ylab=&quot;&quot;, xlim=c(0, 10), ylim=c(2, 9), las=1) mtext(&quot;y&quot;, at=5.5,side=2,las=1,cex=1.3, line=2.3) points(4.3, 8.0) text(4.7, 8.0, &quot;A&quot;, cex=1.3) points(9.5, 8.0) text(9.9, 8.0, &quot;B&quot;, cex=1.3) points(9.5, 2.5) text(9.9, 2.5, &quot;C&quot;, cex=1.3) Para investigar el efecto de cada tipo de punto aberrante, Tabla 2.6 resume los resultados de cuatro regresiones separadas. La primera regresión es para los diecinueve puntos base. Las otras tres regresiones utilizan los diecinueve puntos base más cada tipo de observación inusual. Tabla 2.6. Resultados de Cuatro Regresiones \\[ {\\small \\begin{array}{l|rrrrr} \\hline Datos &amp; b_0 &amp; b_1 &amp; s &amp; R^2(\\%) &amp; t(b_1) \\\\ \\hline 19 \\text{ Puntos Base} &amp; 1.869 &amp; 0.611 &amp; 0.288 &amp; 89.0 &amp; 11.71 \\\\ 19 \\text{ Puntos Base} ~+~ A &amp; 1.750 &amp; 0.693 &amp; 0.846 &amp; 53.7 &amp; 4.57 \\\\ 19 \\text{ Puntos Base} ~+~ B &amp; 1.775 &amp; 0.640 &amp; 0.285 &amp; 94.7 &amp; 18.01 \\\\ 19 \\text{ Puntos Base} ~+~ C &amp; 3.356 &amp; 0.155 &amp; 0.865 &amp; 10.3 &amp; 1.44 \\\\ \\hline \\end{array} } \\] Tabla 2.6 muestra que una línea de regresión proporciona un buen ajuste para los diecinueve puntos base. El coeficiente de determinación, \\(R^2\\), indica que alrededor del 89% de la variabilidad ha sido explicada por la línea. El tamaño del error típico, \\(s\\), es de aproximadamente 0.29, pequeño en comparación con la dispersión en los valores de \\(y\\). Además, el cociente \\(t\\) para el coeficiente de la pendiente es grande. Cuando se agrega el punto atípico A a los diecinueve puntos base, la situación empeora dramáticamente. El \\(R^2\\) baja del 89% al 53.7% y \\(s\\) aumenta de aproximadamente 0.29 a alrededor de 0.85. La línea de regresión ajustada en sí no cambia mucho, aunque nuestra confianza en las estimaciones ha disminuido. Un punto atípico es inusual en el valor de \\(y\\), pero “inusual en el valor de \\(y\\)” depende del valor de \\(x\\). Para ver esto, mantén el valor de \\(y\\) del Punto A igual, pero aumenta el valor de \\(x\\) y llama al punto B. Cuando se agrega el punto B a los diecinueve puntos base, la línea de regresión proporciona un ajuste mejor. El punto B está cerca de estar en la línea de ajuste de regresión generada por los diecinueve puntos base. Así, la línea de regresión ajustada y el tamaño del error típico, \\(s\\), no cambian mucho. Sin embargo, \\(R^2\\) aumenta del 89% a casi el 95%. Si pensamos en \\(R^2\\) como \\(1-(Error~SS)/(Total~SS)\\), al agregar el punto B hemos aumentado \\(Total~SS\\), la desviación total cuadrada en los \\(y\\), aunque el \\(Error~SS\\) se mantiene relativamente sin cambios. El punto B no es un punto atípico, pero es un punto de alta influencia. Para mostrar cuán influyente es este punto, reduce considerablemente el valor de \\(y\\) y llama a este el nuevo punto C. Cuando se agrega este punto a los diecinueve puntos base, la situación empeora dramáticamente. El coeficiente \\(R^2\\) baja del 89% al 10%, y el \\(s\\) más que se triplica, de 0.29 a 0.87. Además, los coeficientes de la línea de regresión cambian drásticamente. La mayoría de los usuarios de la regresión al principio no creen que un punto de veinte pueda tener un efecto tan dramático en el ajuste de la regresión. El ajuste de una línea de regresión siempre puede mejorarse eliminando un punto atípico. Si el punto es un punto de alta influencia y no un punto atípico, no está claro si el ajuste mejorará cuando el punto sea eliminado. ¡Simplemente porque puedes mejorar dramáticamente un ajuste de regresión omitiendo una observación no significa que siempre debas hacerlo! El objetivo del análisis de datos es comprender la información en los datos. A lo largo del texto, encontraremos muchos conjuntos de datos donde los puntos inusuales proporcionan alguna de la información más interesante sobre los datos. El objetivo de esta subsección es reconocer los efectos de los puntos inusuales; el Capítulo 5 proporcionará opciones para manejar puntos inusuales en tu análisis. Todas las disciplinas cuantitativas, como contabilidad, economía, programación lineal, etc., practican el arte del análisis de sensibilidad. El análisis de sensibilidad es una descripción de los cambios globales en un sistema debido a un pequeño cambio local en un elemento del sistema. Examinar los efectos de observaciones individuales en el ajuste de regresión es un tipo de análisis de sensibilidad. Ejemplo: Ventas de Lotería – Continuación. La Figura 2.8 muestra un valor atípico; el punto en la parte superior izquierda del gráfico representa un código postal que incluye a Kenosha, Wisconsin. Las ventas para este código postal son inusualmente altas dada su población. Kenosha está cerca de la frontera con Illinois; los residentes de Illinois probablemente participen en la lotería de Wisconsin, lo que aumenta efectivamente el potencial de ventas en Kenosha. Tabla 2.7 resume el ajuste de la regresión tanto con como sin este código postal. Tabla 2.7. Resultados de la Regresión con y sin Kenosha \\[ {\\small \\begin{array}{l|rrrrr} \\hline \\text{Datos} &amp; b_0 &amp; b_1 &amp; s &amp; R^2(\\%) &amp; t(b_1) \\\\ \\hline \\text{Con Kenosha} &amp; 469.7 &amp; 0.647 &amp; 3,792 &amp; 78.5 &amp; 13.26 \\\\ \\text{Sin Kenosha} &amp; -43.5 &amp; 0.662 &amp; 2,728 &amp; 88.3 &amp; 18.82 \\\\ \\hline \\end{array} } \\] Figure 2.8: Gráfico de dispersión de SALES versus POP, con el valor atípico correspondiente a Kenosha marcado. Código R para producir la Figura 2.8 y la Tabla 2.7 Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURA 2.8 par(mar=c(4.1,3.9,2,1),cex=1.1) plot(Lot$POP, Lot$SALES, ylab=&quot;&quot;, las=1, xlab = &quot;POP&quot;) mtext(&quot;SALES&quot;,side=2, at=36000, las=1, cex=1.1) text(5000, 24000, &quot;Kenosha&quot;) # TABLA 2.7 model.basiclinearreg&lt;-lm(SALES ~ POP, Lot) summary(model.basiclinearreg) model.Kenosha&lt;-lm(SALES ~ POP, Lot, subset=-c(9)) summary(model.Kenosha) Para los propósitos de inferencia sobre la pendiente, la presencia de Kenosha no altera los resultados de manera dramática. Ambas estimaciones de la pendiente son cualitativamente similares y los correspondientes valores \\(t\\) son muy altos, muy por encima de los umbrales para la significancia estadística. Sin embargo, hay diferencias notables al evaluar la calidad del ajuste. El coeficiente de determinación, \\(R^2\\), aumentó del 78.5% al 88.3% al eliminar Kenosha. Además, nuestro “desviación típica” \\(s\\) disminuyó en más de $1,000. Esto es particularmente importante si queremos ajustar nuestros intervalos de predicción. Para verificar la exactitud de nuestras suposiciones, también es común revisar la suposición de normalidad. Una forma de hacerlo es mediante el gráfico \\(qq\\), introducido en la Sección 1.2. Los dos paneles en las Figuras 2.9 son gráficos \\(qq\\) con y sin el código postal de Kenosha. Recuerda que los puntos “cercanos” a una línea indican normalidad aproximada. En el panel derecho de la Figura 2.9, la secuencia parece ser lineal, por lo que los residuos están aproximadamente distribuidos de manera normal. Este no es el caso en el panel izquierdo, donde la secuencia de puntos parece aumentar dramáticamente para grandes cuantiles. Lo interesante es que la no-normalidad de la distribución se debe a un solo valor atípico, no a un patrón de sesgo común a todas las observaciones. Figure 2.9: Gráficos \\(qq\\) de los residuos de la Lotería de Wisconsin. El panel izquierdo se basa en los 50 puntos. El panel derecho se basa en 49 puntos, residuos de una regresión después de eliminar Kenosha. Código R para producir la Figura 2.9 #Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURA 2.9 # TABLA 2.7 model.basiclinearreg&lt;-lm(SALES ~ POP, Lot) #summary(model.basiclinearreg) model.Kenosha&lt;-lm(SALES ~ POP, Lot, subset=-c(9)) #summary(model.Kenosha) par(mfrow=c(1, 2), mar=c(4.1,3.9,1.7,1),cex=1.1) qqnorm(residuals(model.basiclinearreg), main=&quot;&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Cuantiles Muestrales&quot;, side=2,at=20500,las=1,cex=1.1, adj=.5) qqnorm(residuals(model.Kenosha), main=&quot;&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Cuantiles Muestrales&quot;, side=2,at=9050,las=1,cex=1.1, adj=.5) 2.7 Aplicación: Modelo de Valoración de Activos Financieros En esta sección, estudiamos una aplicación financiera, el Modelo de Valoración de Activos Financieros, a menudo conocido por el acrónimo CAPM. El nombre es algo engañoso, ya que el modelo realmente trata sobre rendimientos basados en activos de capital, no sobre los precios en sí mismos. Los tipos de activos que examinamos son valores de acciones que se negocian en un mercado activo, como la Bolsa de Valores de Nueva York (NYSE). Para una acción en la bolsa, podemos relacionar los rendimientos con los precios mediante la siguiente expresión: \\[ {\\small \\mathrm{rendimiento =}\\frac{\\mathrm{precio~al~final~de~un~período+dividendos-precio~al~inicio~de~un~período}}{\\mathrm{precio~al~inicio~de~un~período}}. } \\] Si podemos estimar los rendimientos que genera una acción, entonces el conocimiento del precio al inicio de un período financiero genérico nos permite estimar el valor al final del período (precio final más dividendos). Por lo tanto, seguimos la práctica estándar y modelamos los rendimientos de una acción. Una idea intuitivamente atractiva, y una de las características básicas del CAPM, es que debería haber una relación entre el rendimiento de una acción y el mercado. Una justificación es simplemente que si las fuerzas económicas hacen que el mercado mejore, entonces esas mismas fuerzas deberían actuar sobre una acción individual, sugiriendo que también debería mejorar. Como se mencionó anteriormente, medimos el rendimiento de una acción a través del rendimiento. Para medir el rendimiento del mercado, existen varios índices de mercado que resumen el rendimiento de cada bolsa. Usaremos el índice “ponderado por igual” del Standard &amp; Poor’s 500. El Standard &amp; Poor’s 500 es la colección de las 500 empresas más grandes que se negocian en la NYSE, donde “grande” es identificado por Standard &amp; Poor’s, una organización de calificación de servicios financieros. El índice ponderado por igual se define asumiendo que se crea una cartera invirtiendo un dólar en cada una de las 500 empresas. Otra justificación para una relación entre los rendimientos de las acciones y el mercado proviene de la teoría de la economía financiera. Esta es la teoría CAPM, atribuida a Sharpe (1964) y Lintner (1965) y basada en las ideas de diversificación de cartera de Harry Markowitz (1959). Otros factores iguales, los inversionistas desearían seleccionar un rendimiento con un alto valor esperado y una baja desviación estándar, esta última siendo una medida de riesgo. Una de las propiedades deseables de usar desviaciones estándar como medida de riesgo es que es sencillo calcular la desviación estándar de una cartera. Solo es necesario conocer la desviación estándar de cada acción y las correlaciones entre acciones. Una acción notable es una libre de riesgo, es decir, una acción que teóricamente tiene una desviación estándar cero. Los inversionistas a menudo utilizan un bono del Tesoro de EE. UU. a 30 días como una aproximación de una acción libre de riesgo, argumentando que la probabilidad de default del gobierno de EE. UU. dentro de 30 días es insignificante. Positando la existencia de un activo libre de riesgo y algunas otras condiciones suaves, bajo la teoría CAPM existe una frontera eficiente llamada la línea de mercado de valores. Esta frontera especifica el rendimiento mínimo esperado que los inversionistas deberían exigir para un nivel específico de riesgo. Para estimar esta línea, podemos usar la ecuación: \\[\\begin{equation*} \\mathrm{E}~r = \\beta_0 + \\beta_1 r_m \\end{equation*}\\] donde \\(r\\) es el rendimiento de la acción y \\(r_m\\) es el rendimiento del mercado. Interpretamos \\(\\beta_1 r_m\\) como una medida de la cantidad de rendimiento de la acción que se atribuye al comportamiento del mercado. Probar la teoría económica, o modelos que surgen de cualquier disciplina, implica recolectar datos. La teoría CAPM trata sobre rendimientos ex-ante (antes del hecho), aunque solo podemos probar con rendimientos ex-post (después del hecho). Antes del hecho, los rendimientos son desconocidos y hay toda una distribución de rendimientos. Después del hecho, solo hay una realización única del rendimiento de la acción y del mercado. Debido a que se requieren al menos dos observaciones para determinar una línea, los modelos CAPM se estiman usando datos de acciones y del mercado recopilados a lo largo del tiempo. De esta manera, se pueden realizar varias observaciones. Para los propósitos de nuestras discusiones, seguimos la práctica estándar en la industria de valores y examinamos precios mensuales. Datos Para ilustrar, considere los rendimientos mensuales durante el período de cinco años desde enero de 1986 hasta diciembre de 1990, inclusive. Específicamente, usamos los rendimientos de la acción de Lincoln National Insurance Corporation como la variable dependiente (\\(y\\)) y los rendimientos del mercado del índice Standard &amp; Poor’s 500 como la variable explicativa (\\(x\\)). En ese momento, Lincoln era una gran compañía de seguros multirama, con sede en el medio oeste de EE. UU., específicamente en Fort Wayne, Indiana. Debido a que era bien conocida por su gestión prudente y estabilidad, es una buena compañía para comenzar nuestro análisis de la relación entre el mercado y una acción individual. Comenzamos interpretando algunas estadísticas básicas, en la Tabla 2.8, en términos de teoría financiera. Primero, un inversionista en Lincoln estará preocupado de que el rendimiento promedio de cinco años, \\(\\overline{y}=0.00510\\), esté por debajo del rendimiento del mercado, \\(\\overline{x}=0.00741\\). Los estudiantes de teoría de intereses reconocen que los rendimientos mensuales se pueden convertir a una base anual usando la capitalización geométrica. Por ejemplo, el rendimiento anual de Lincoln es \\((1.0051)^{12}-1=0.062946\\), o aproximadamente 6.29 por ciento. Esto se compara con un rendimiento anual de 9.26% (= (1\\(00((1.00741)^{12}-1\\))) para el mercado. Una medida de riesgo, o volatilidad, que se usa en finanzas es la desviación estándar. Así, interprete \\(s_y\\) = 0.0859 \\(&gt;\\) 0.05254 = \\(s_x\\) para significar que una inversión en Lincoln es más riesgosa que la del mercado. Otro aspecto interesante de la Tabla 2.8 es que el rendimiento más bajo del mercado, -0.22052, está 4.338 desviaciones estándar por debajo de su promedio ((-0.22052-0.00741)/0.05254 = -4.338). Esto es muy inusual con respecto a una distribución normal. knitr::kable(2, caption = &quot;Silly. Crear una tabla solo para actualizar el contador...&quot;) Table 2.2: Silly. Crear una tabla solo para actualizar el contador… x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Table 2.3: Silly. x 2 knitr::kable(2, caption = &quot;Silly. &quot;) Table 2.4: Silly. x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Table 2.5: Silly. x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Table 2.6: Silly. x 2 Table 2.7: Silly. x 2 Table 2.8: Estadísticas Resumen de 60 Observaciones Mensuales Promedio Mediana Desviación Estándar Mínimo Máximo LINCOLN 0.0051 0.0075 0.0859 -0.2803 0.3147 MARKET 0.0074 0.0142 0.0525 -0.2205 0.1275 Fuente: Center for Research on Security Prices, University of Chicago A continuación, examinamos los datos a lo largo del tiempo, como se muestra gráficamente en la Figura 2.10. Estos son gráficos de dispersión de los rendimientos versus el tiempo, llamados gráficos de series temporales. En la Figura 2.10, se puede ver claramente el rendimiento más bajo del mercado y un vistazo rápido al eje horizontal revela que este punto inusual está en octubre de 1987, el momento del conocido colapso del mercado. Figure 2.10: Gráfico de series temporales de los rendimientos de la Lincoln National Corporation y del mercado. Hay 60 rendimientos mensuales durante el período de enero de 1986 a diciembre de 1990. El gráfico de dispersión en la Figura 2.11 resume gráficamente la relación entre el rendimiento de Lincoln y el rendimiento del mercado. El colapso del mercado es claramente evidente en la Figura 2.11 y representa un punto de alta influencia. Con la línea de regresión (descrita a continuación) superpuesta, los dos puntos atípicos que se pueden ver en la Figura 2.10 también son evidentes. A pesar de estas anomalías, el gráfico en la Figura 2.11 sugiere que hay una relación lineal entre los rendimientos de Lincoln y del mercado. Figure 2.11: Gráfico de dispersión del rendimiento de Lincoln versus el rendimiento del índice S&amp;P 500. La línea de regresión está superpuesta, lo que nos permite identificar el colapso del mercado y dos puntos atípicos. Código R para producir la Tabla 2.8 y las Figuras 2.10 y 2.11 CAPM &lt;- read.csv(&quot;CSVData/CAPM.csv&quot;, header=TRUE) # TABLA 2.8 ESTADÍSTICAS RESUMEN Xymat &lt;- data.frame(cbind(CAPM$LINCOLN,CAPM$MARKET)) tableMat &lt;- BookSummStats(Xymat) colnames(tableMat) &lt;- c(&quot;Promedio&quot; , &quot;Mediana&quot; , &quot;Desviación Estándar&quot; , &quot;Mínimo&quot; , &quot;Máximo&quot;) rownames(tableMat) &lt;- c(&quot;LINCOLN&quot;, &quot;MARKET&quot;) #tableMat1 &lt;- format(round(tableMat, digits=0), big.mark = &#39;,&#39;) TableGen1(TableData=tableMat, TextTitle=&#39;Estadísticas Resumen de 60 Observaciones Mensuales&#39;, Align=&#39;r&#39;, Digits=4, ColumnSpec=1:5, ColWidth = ColWidth5) %&gt;% footnote(general = &quot;Center for Research on Security Prices, University of Chicago&quot;, general_title = &quot;Fuente:&quot;, footnote_as_chunk = TRUE) # FIGURA 2.10 par(mar=c(4.1,3.1,2,1),cex=1.1, las=1) foo &lt;- ts(CAPM, freq = 12, start = c(1986, 1)) ts.plot(foo[,2], foo[,3], xlab=&quot;Año&quot;, ylab=&quot;&quot;, type=&quot;o&quot;, lty=c(1, 2)) mtext(&quot;Rendimiento Mensual&quot;, side=2, at=.38,las=1,cex=1.1, adj=.5) legend(1986, 0.3, c(&quot;LINCOLN&quot;, &quot;MARKET&quot;), lty=1:2, cex=0.5) # FIGURA 2.11 par(mar=c(4.1,3.1,1.4,0.2),cex=1.1, las=1) plot(CAPM$MARKET, CAPM$LINCOLN, xlab=&quot;MARKET&quot;, ylab=&quot;&quot;, xlim=c(-0.3, 0.2), ylim=c(-0.3, 0.4),las=1) mtext(&quot;LINCOLN&quot;, side=2,at=0.46,las=1, cex=1.1, adj=.5) reg &lt;- lm(LINCOLN ~ MARKET, data = CAPM) abline(reg) arrows(-0.22, -0.1, -0.22, -0.22,length=0.1, angle = 10) text(-0.22, -0.08, &quot;COLAPSO DE OCTUBRE, 1987&quot;, cex=0.8) arrows(0.1, 0.02, 0, -0.27,length=0.1, angle = 10) arrows(0.1, 0.02, 0.06, 0.3,length=0.1, angle = 10) text(0.16, 0.02, &quot;PUNTOS ATÍPICOS DE 1990&quot;, cex=0.8) Puntos Inusuales Para resumir la relación entre el mercado y el rendimiento de Lincoln, se ajustó un modelo de regresión. La regresión ajustada es \\[\\begin{equation*} \\widehat{LINCOLN}=-0.00214+0.973 MARKET. \\end{equation*}\\] El error estándar estimado resultante, \\(s = 0.0696\\), es menor que la desviación estándar de los rendimientos de Lincoln, \\(s_y=0.0859\\). Por lo tanto, el modelo de regresión explica parte de la variabilidad de los rendimientos de Lincoln. Además, el estadístico \\(t\\) asociado con la pendiente \\(b_1\\) resulta ser \\(t(b_1)=5.64\\), lo cual es significativamente alto. Un aspecto decepcionante es que el estadístico \\(R^2=35.4\\%\\) se puede interpretar como que el mercado explica solo un poco más de un tercio de la variabilidad. Por lo tanto, aunque el mercado es claramente un determinante importante, como lo evidencian el alto estadístico \\(t\\), solo proporciona una explicación parcial del rendimiento de los rendimientos de Lincoln. En el contexto del modelo de mercado, podemos interpretar la desviación estándar del mercado, \\(s_x\\), como riesgo no diversificable. Por lo tanto, el riesgo de un valor puede descomponerse en dos componentes: el componente diversificable y el componente del mercado, que es no diversificable. La idea es que, al combinar varios valores, podemos crear una cartera de valores que, en la mayoría de los casos, reducirá el riesgo de nuestras inversiones en comparación con un solo valor. Nuevamente, la razón para tener un valor es que estamos compensados con rendimientos esperados más altos al tener un valor con mayor riesgo. Para cuantificar el riesgo relativo, no es difícil demostrar que \\[\\begin{equation} s_y^2 = b_1^2 s_x^2 + s^2 \\frac{n-2}{n-1}. \\tag{2.8} \\end{equation}\\] El riesgo de un valor se debe al riesgo del mercado más el riesgo de un componente diversificable. Tenga en cuenta que el riesgo del componente del mercado, \\(s_x^2\\), es mayor para los valores con pendientes más grandes. Por esta razón, los inversores consideran que los valores con pendientes \\(b_1\\) mayores que uno son “agresivos” y las pendientes menores que uno como “defensivos”. Análisis de Sensibilidad El resumen anterior plantea inmediatamente dos cuestiones adicionales. Primero, ¿cuál es el efecto del colapso de octubre de 1987 en la ecuación de regresión ajustada? Sabemos que las observaciones inusuales, como el colapso, pueden influir mucho en el ajuste. Con este fin, se volvió a ejecutar la regresión sin la observación correspondiente al colapso. La motivación para esto es que el colapso de octubre de 1987 representa una combinación de eventos altamente inusuales (la interacción de varios programas de comercio automatizado operados por grandes casas de corretaje de valores) que no deseamos representar con el mismo modelo que nuestras otras observaciones. Eliminando esta observación, la regresión ajustada es \\[\\begin{equation*} \\widehat{LINCOLN} = -0.00181 + 0.956 MARKET, \\end{equation*}\\] con \\(R^2=26.4\\%\\), \\(t(b_1)=4.52\\), \\(s=0.0702\\) y \\(s_y=0.0811\\). Interpretamos estas estadísticas de la misma manera que el modelo ajustado que incluye el colapso de octubre de 1987. Sin embargo, es interesante notar que la proporción de variabilidad explicada ha disminuido al excluir el punto influyente. Esto sirve para ilustrar un punto importante. Los puntos de alta influencia a menudo son temidos por los analistas de datos porque, por definición, son diferentes de otras observaciones en el conjunto de datos y requieren una atención especial. Sin embargo, al ajustar las relaciones entre variables, también representan una oportunidad porque permiten al analista de datos observar la relación entre variables en rangos más amplios que de otro modo serían posibles. La desventaja es que estas relaciones pueden ser no lineales o seguir un patrón completamente diferente en comparación con las relaciones observadas en la parte principal de los datos. La segunda pregunta planteada por el análisis de regresión es qué se puede decir sobre las circunstancias inusuales que dieron lugar al comportamiento inusual de los rendimientos de Lincoln en octubre y noviembre de 1990. Una característica útil del análisis de regresión es identificar y plantear la pregunta; no la resuelve. Debido a que el análisis señala claramente dos puntos altamente inusuales, sugiere al analista de datos que vuelva y haga algunas preguntas específicas sobre las fuentes de los datos. En este caso, la respuesta es directa. En octubre de 1990, la compañía Travelers’ Insurance, una competidora, anunció que tomaría una gran amortización en su cartera de bienes raíces debido a un número sin precedentes de incumplimientos hipotecarios. El mercado reaccionó rápidamente a esta noticia, y los inversores asumieron que otras grandes compañías de seguros de vida también anunciarían pronto grandes amortizaciones. Anticipando esta noticia, los inversores trataron de vender sus carteras de, por ejemplo, las acciones de Lincoln, lo que provocó una caída en el precio. Sin embargo, resultó que los inversores reaccionaron en exceso a esta noticia y que la cartera de bienes raíces de Lincoln estaba en realidad en buen estado. Así, los precios rápidamente volvieron a sus niveles históricos. 2.8 Salida Computacional Ilustrativa de Regresión Las computadoras y los paquetes de software estadístico que realizan cálculos especializados juegan un papel vital en los análisis estadísticos modernos. Las capacidades informáticas económicas han permitido a los analistas de datos centrarse en las relaciones de interés. Es mucho menos importante especificar modelos que sean atractivos únicamente por su simplicidad computacional en comparación con épocas anteriores a la disponibilidad generalizada de computación económica. Un tema importante de este texto es centrarse en las relaciones de interés y confiar en el software estadístico ampliamente disponible para estimar los modelos que especificamos. Con cualquier paquete de computadora, generalmente las partes más difíciles de operar el paquete son (i) la entrada, (ii) el uso de los comandos y (iii) la interpretación de la salida. Encontrarás que la mayoría de los paquetes estadísticos modernos aceptan archivos en formato de hoja de cálculo o texto, lo que facilita la entrada de datos. Los paquetes de software estadístico para computadoras personales tienen lenguajes de comando basados en menús con facilidades de ayuda en línea fácilmente accesibles. Una vez que decides qué hacer, encontrar los comandos correctos es relativamente fácil. Esta sección proporciona orientación para interpretar la salida de los paquetes estadísticos. La mayoría de los paquetes estadísticos generan salidas similares. A continuación, se presentan tres ejemplos de paquetes estadísticos estándar: EXCEL, SAS y R. El símbolo de anotación “[.]” marca una cantidad estadística que se describe en la leyenda. Así, esta sección proporciona un enlace entre la notación utilizada en el texto y la salida de algunos de los paquetes estadísticos estándar. Salida en EXCEL Regression Statistics Multiple R 0.886283[F] R Square 0.785497[k] Adjusted R Square 0.781028[l] Standard Error 3791.758[j] Observations 50[a] ANOVA df SS MS F Significance F Regression 1[m] 2527165015 [p] 2527165015 [s] 175.773[u] 1.15757E-17[v] Residual 48[n] 690116754.8[q] 14377432.39[t] Total 49[o] 3217281770 [r] Coefficients Standard Error t Stat P-value Intercept 469.7036[b] 702.9061896[d] 0.668230846[f] 0.507187[h] X Variable 1 0.647095[c] 0.048808085[e] 13.25794257[g] 1.16E-17[i] El Sistema SAS The REG Procedure Dependent Variable: SALES Analysis of Variance Sum of Mean Source DF Squares Square F Value Pr &gt; F Model 1[m] 2527165015[p] 2527165015[s] 175.77[u] &lt;.0001[v] Error 48[n] 690116755[q] 14377432[t] Corrected Total 49[o] 3217281770[r] Root MSE 3791.75848[j] R-Square 0.7855[k] Dependent Mean 6494.82900[H] Adj R-Sq 0.7810[l] Coeff Var 58.38119[I] Parameter Estimates Parameter Standard Variable Label DF Estimate Error t Value Pr &gt; |t| Intercept Intercept 1 469.70360[b] 702.90619[d] 0.67[f] 0.5072[h] POP POP 1 0.64709[c] 0.04881[e] 13.26[g] &lt;.0001[i] Salida en R Analysis of Variance Table Response: SALES Df Sum Sq Mean Sq F value Pr(&gt;F) POP 1[m] 2527165015[p] 2527165015[s] 175.77304[u] &lt;2.22e-16[v]*** Residuals 48[n] 690116755[q] 14377432[t] --- Call: lm(formula = SALES ~ POP) Residuals: Min 1Q Median 3Q Max -6047 -1461 -670 486 18229 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 469.7036[b] 702.9062[c] 0.67[f] 0.51 [h] POP 0.6471[c] 0.0488[e] 13.26[g] &lt;2e-16 ***[i] --- Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 Residual standard error: 3790[j] on 48[n] degrees of freedom Multiple R-Squared: 0.785[k], Adjusted R-squared: 0.781[l] F-statistic: 176[u] on 1[m] and 48[n] DF, p-value: &lt;2e-16[v] Definición de Anotación de Leyenda, Símbolo [a] Número de observaciones \\(n\\). [b] La intersección estimada \\(b_0\\). [c] La pendiente estimada \\(b_1\\). [d] El error estándar de la intersección, \\(se(b_0)\\). [e] El error estándar de la pendiente, \\(se(b_1)\\). [f] El valor del \\(t\\) asociado con la intersección, \\(t(b_0) = b_0/se(b_0)\\). [g] El valor del \\(t\\) asociado con la pendiente, \\(t(b_1) = b_1/se(b_1)\\). [h] El valor \\(p\\) asociado con la intersección; aquí, \\(p-value=Pr(|t_{n-2}|&gt;|t(b_0)|)\\), donde \\(t(b_0)\\) es el valor realizado (0.67 aquí) y \\(t_{n-2}\\) tiene una distribución \\(t\\) con \\(df=n-2\\). [i] El valor \\(p\\) asociado con la pendiente; aquí, \\(p-value=Pr(|t_{n-2}|&gt;|t(b_1)|)\\), donde \\(t(b_1)\\) es el valor realizado (13.26 aquí) y \\(t_{n-2}\\) tiene una distribución \\(t\\) con \\(df=n-2\\). [j] La desviación estándar residual, \\(s\\). [k] El coeficiente de determinación, \\(R^2\\). [l] El coeficiente de determinación ajustado por grados de libertad, \\(R_{a}^2\\). (Este término se definirá en el Capítulo 3.) [m] Grados de libertad para el componente de regresión. Esto es 1 para una variable explicativa. [n] Grados de libertad para el componente de error, \\(n-2\\), para la regresión con una variable explicativa. [o] Grados de libertad totales, \\(n-1\\). [p] La suma de cuadrados de la regresión, \\(Regression~SS\\). [q] La suma de cuadrados del error, \\(Error~SS\\). [r] La suma total de cuadrados, \\(Total~SS\\). [s] El cuadrado medio de la regresión, \\(Regression~MS = Regression~SS/1\\), para una variable explicativa. [t] El cuadrado medio del error, \\(s^2=Error~MS = Error~SS/(n-2)\\), para una variable explicativa. [u] El \\(F-ratio=(Regression~MS)/(Error~MS)\\). (Este término se definirá en el Capítulo 3.) [v] El valor \\(p\\) asociado con el \\(F-ratio\\). (Este término se definirá en el Capítulo 3.) [w] El número de observación, \\(i\\). [x] El valor de la variable explicativa para la \\(i\\)-ésima observación, \\(x_i\\). [y] La respuesta para la \\(i\\)-ésima observación, \\(y_i\\). [z] El valor ajustado para la \\(i\\)-ésima observación, \\(\\widehat{y}_i\\). [A] El error estándar del ajuste, \\(se(\\widehat{y}_i)\\). [B] El residual para la \\(i\\)-ésima observación, \\(e_i\\). [C] El residual estandarizado para la \\(i\\)-ésima observación, \\(e_i/se(e_i)\\). El error estándar \\(se(e_i)\\) se definirá en la Sección 5.3.1. [F] El coeficiente de correlación múltiple es la raíz cuadrada del coeficiente de determinación, \\(R=\\sqrt{R^2}\\). Esto se definirá en el Capítulo 3. [G] El coeficiente estandarizado es \\(b_1s_x/s_y\\). Para regresión con una variable explicativa, esto es equivalente a \\(r\\), el coeficiente de correlación. [H] La respuesta promedio, \\(\\overline{y}\\). [I] El coeficiente de variación de la respuesta es \\(s_y/\\overline{y}\\). SAS imprime \\(100s_y/\\overline{y}\\). 2.9 Lecturas Adicionales y Referencias Relativamente pocas aplicaciones de la regresión son básicas en el sentido de que usan solo una variable explicativa; el propósito del análisis de regresión es reducir las relaciones complejas entre muchas variables. La Sección 2.7 describe una excepción importante a esta regla general, el modelo financiero CAPM; consulta a Panjer et al. (1998) para descripciones actuariales adicionales de este modelo. Campbell et al. (1997) ofrece una perspectiva desde la econometría financiera. Referencias del Capítulo Anscombe, Frank (1973). Graphs in statistical analysis. The American Statistician 27, 17-21. Campbell, John Y., Andrew W. Lo and A. Craig MacKinlay (1997). The Econometrics of Financial Markets. Princeton University Press, Princeton, New Jersey. Frees, Edward W. and Tom W. Miller (2003). Sales forecasting using longitudinal data models. International Journal of Forecasting 20, 97-111. Goldberger, Arthur (1991). A Course in Econometrics. Harvard University Press, Cambridge. Koch, Gary J. (1985). A basic demonstration of the [-1, 1] range for the correlation coefficient. American Statistician 39, 201-202. Linter, J. (1965). The valuation of risky assets and the selection of risky investments in stock portfolios and capital budgets. Review of Economics and Statistics, 13-37. Manistre, B. John and Geoffrey H. Hancock (2005). Variance of the CTE estimator. North American Actuarial Journal 9(2), 129-156. Markowitz, Harry (1959). Portfolio Selection: Efficient Diversification of Investments. John Wiley, New York. Panjer, Harry H., Phelim P. Boyle, Samuel H. Cox, Daniel Dufresne, Hans U. Gerber, Heinz H. Mueller, Hal W. Pedersen, Stanley R. Pliska, Michael Sherris, Elias S. Shiu and Ken S. Tan (1998). Financial Economics: With Applications to Investment, Insurance and Pensions. Society of Actuaries, Schaumburg, Illinois. Pearson, Karl (1895). Royal Society Proceedings 58, 241. Serfling, Robert J. (1980). Approximation Theorems of Mathematical Statistics. John Wiley and Sons, New York. Sharpe, William F. (1964). Capital asset prices: A theory of market equilibrium under risk. Journal of Finance, 425-442. Stigler, Steven M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. Harvard University Press, Cambridge, MA. 2.10 Ejercicios Secciones 2.1-2.2 2.1 Considera el siguiente conjunto de datos \\[ \\begin{array}{l|ccc} \\hline i &amp; 1 &amp; 2 &amp; 3 \\\\ \\hline x_i &amp; 2 &amp; -6 &amp; 7 \\\\ y_i &amp; 3 &amp; 4 &amp; 6\\\\ \\hline \\end{array} \\] Ajusta una línea de regresión utilizando el método de mínimos cuadrados. Determina \\(r\\), \\(b_1\\) y \\(b_0\\). 2.2 Una relación perfecta, pero sin correlación. Considera la relación cuadrática \\(y=x^2\\), con datos \\[ \\begin{array}{l|ccccc} \\hline i &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5\\\\ \\hline x_i &amp; -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 \\\\ y_i &amp; 4 &amp; 1 &amp; 0 &amp; 1 &amp; 4\\\\ \\hline \\end{array} \\] Produce un gráfico aproximado para este conjunto de datos. Verifica que el coeficiente de correlación es \\(r=0\\). 2.3 Acotación del coeficiente de correlación. Utiliza los siguientes pasos para demostrar que \\(r\\) está acotado entre -1 y 1 (Estos pasos son de Koch, 1990). Deja que \\(a\\) y \\(c\\) sean constantes genéricas. Verifica \\[\\begin{eqnarray*} 0 &amp; \\leq &amp; \\frac{1}{n-1}\\sum_{i=1}^{n}\\left( a\\frac{x_i-\\overline{x}}{s_x}-c \\frac{y_i-\\overline{y}}{s_y}\\right) ^2 \\\\ &amp;=&amp; a^2+c^2-2acr. \\end{eqnarray*}\\] Utiliza los resultados del apartado (a) para demostrar que \\(2ac(r-1)\\leq (a-c)^2.\\) Al tomar \\(a=c\\), utiliza el resultado del apartado (b) para demostrar que \\(r\\leq 1\\). Al tomar \\(a=-c\\), utiliza los resultados del apartado (b) para demostrar que \\(r\\geq -1\\). ¿En qué condiciones es \\(r=-1\\)? ¿En qué condiciones es \\(r=1\\)? 2.4 Los coeficientes de regresión son sumas ponderadas. Demuestra que el término de intercepto, \\(b_0\\), puede expresarse como una suma ponderada de las variables dependientes. Es decir, demuestra que \\(b_0=\\sum_{i=1}^{n}w_{i,0}y_i.\\) Además, expresa los pesos en términos de los pesos de la pendiente, \\(w_i\\). 2.5 Otra expresión para la pendiente como una suma ponderada Utilizando álgebra, establece una expresión alternativa \\[\\begin{equation*} b_1=\\frac{\\sum_{i=1}^{n}weight_i~slope_i}{ \\sum_{i=1}^{n}weight_i}. \\end{equation*}\\] Aquí, \\(slope_i\\) es la pendiente entre \\((x_i,y_i)\\) y \\((\\bar{x},\\bar{y})\\). Da una forma precisa para el peso \\(weight_i\\) como una función de la variable explicativa \\(x\\). Supón que \\(\\bar{x} = 4, \\bar{y} = 3, x_1 = 2 \\text{ y } y_1= 6\\). Determina la pendiente y el peso para la primera observación, es decir, \\(slope_1\\) y \\(weight_1\\). 2.6 Considera dos variables, \\(y\\) y \\(x\\). Realiza una regresión de \\(y\\) sobre \\(x\\) para obtener un coeficiente de pendiente que llamaremos \\(b_{1,x,y}\\). Realiza otra regresión de \\(x\\) sobre \\(y\\) para obtener un coeficiente de pendiente que llamaremos \\(b_{1,y,x}\\). Demuestra que el coeficiente de correlación entre \\(x\\) y \\(y\\) es la media geométrica de los dos coeficientes de pendiente según el signo, es decir, demuestra que \\(|r|=\\sqrt{ b_{1,x,y}b_{1,y,x}}.\\) 2.7 Regresión a través del origen. Considera el modelo \\(y_i=\\beta_1 x_i + \\varepsilon _i\\), es decir, regresión con una variable explicativa sin el término de intercepto. Este modelo se llama regresión a través del origen porque la verdadera línea de regresión \\(\\mathrm{E}y = \\beta_1 x\\) pasa por el origen (el punto (0, 0)). Para este modelo, la estimación de mínimos cuadrados de \\(\\beta_1\\) es ese número \\(b_1\\) que minimiza la suma de cuadrados \\(\\mathrm{SS}(b_1^{\\ast} )=\\sum_{i=1}^{n}\\left( y_i - b_1^{\\ast}x_i\\right) ^2.\\) Verifica que \\[\\begin{equation*} b_1 = \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sum_{i=1}^{n}x_i^2}. \\end{equation*}\\] Considera el modelo \\(y_i=\\beta_1 z_i^2 + \\varepsilon _i\\), un modelo cuadrático que pasa por el origen. Utiliza el resultado del apartado (a) para determinar la estimación de mínimos cuadrados de \\(\\beta_1\\). 2.8 a. Demuestra que \\[\\begin{equation*} s_y^2=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^2= \\frac{1}{n-1}\\left( \\sum_{i=1}^{n}y_i^2-n\\overline{y}^2\\right) . \\end{equation*}\\] Sigue los mismos pasos para demostrar que \\(\\sum_{i=1}^{n}\\left( y_i - \\overline{y} \\right) \\left( x_i-\\overline{x}\\right) =\\sum_{i=1}^{n} x_i y_i - n \\overline{x}~\\overline{y}.\\) Demuestra que \\[ b_{1}=\\frac{\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) \\left( x_i- \\overline{x}\\right) }{\\sum_{i=1}^{n}\\left( x_i - \\overline{x} \\right) ^2} \\] Establece la fórmula comúnmente utilizada \\[ b_{1}= \\frac{\\sum_{i=1}^{n}x_iy_i-n\\overline{x}~\\overline{y}} {\\sum_{i=1}^{n}x_i^2 - n\\overline{x}^2}. \\] 2.9 Interpretación de los coeficientes asociados con una variable explicativa binaria. Supón que \\(x_i\\) solo toma los valores 0 y 1. De las \\(n\\) observaciones, \\(n_1\\) toman el valor \\(x=0\\). Estas \\(n_1\\) observaciones tienen un valor promedio \\(y\\) de \\(\\overline{y}_1\\). Las restantes \\(n-n_1\\) observaciones tienen el valor \\(x=1\\) y un valor promedio \\(y\\) de \\(\\overline{y}_2\\). Utiliza el Ejercicio 2.8 para demostrar que \\(b_1 = \\overline{y}_2 - \\overline{y}_1.\\) Secciones 2.3-2.4 2.10 Utilización de Hogares de Cuidado. Este ejercicio considera los datos de hogares de cuidado proporcionados por el Departamento de Salud y Servicios Familiares de Wisconsin (DHFS) y descritos en el Ejercicio 1.2. Parte 1: Utiliza los datos del año 2000 y realiza el siguiente análisis. Correlaciones a(i). Calcula la correlación entre TPY y LOGTPY. Comenta tu resultado. a(ii). Calcula la correlación entre TPY, NUMBED y SQRFOOT. ¿Parecen estas variables altamente correlacionadas? a(iii). Calcula la correlación entre TPY y NUMBED/10. Comenta tu resultado. Diagramas de dispersión. Grafica TPY versus NUMBED y TPY versus SQRFOOT. Comenta los gráficos. Regresión lineal básica. c(i). Ajusta un modelo de regresión lineal básico usando TPY como variable de resultado y NUMBED como variable explicativa. Resume el ajuste citando el coeficiente de determinación, \\(R^2\\), y el estadístico \\(t\\) para NUMBED. c(ii). Repite c(i), usando SQRFOOT en lugar de NUMBED. En términos de \\(R^2\\), ¿cuál modelo se ajusta mejor? c(iii). Repite c(i), usando LOGTPY como variable de resultado y LOG(NUMBED) como variable explicativa. c(iv). Repite c(iii), usando LOGTPY como variable de resultado y LOG(SQRFOOT) como variable explicativa. Parte 2: Ajusta el modelo en la Parte 1.c(i) usando datos de 2001. ¿Son los patrones estables a lo largo del tiempo? 2.11 Supón que, para un tamaño de muestra de \\(n\\) = 3, tienes \\(e_2\\) = 24 y \\(e_{3}\\) = -1. Determina \\(e_{1}\\). 2.12 Supón que \\(r=0\\), \\(n=15\\) y \\(s_y = 10\\). Determina \\(s\\). 2.13 El coeficiente de correlación y el coeficiente de determinación. Usa los siguientes pasos para establecer una relación entre el coeficiente de determinación y el coeficiente de correlación. Muestra que \\(\\widehat{y}_i-\\overline{y}=b_1(x_i-\\overline{x}).\\) Usa el apartado (a) para mostrar que \\(Regress~SS=\\sum_{i=1}^{n}\\left(\\widehat{y}_i - \\overline{y} \\right)^2 = b_1^2s_x^2(n-1).\\) Usa el apartado (b) para establecer que \\(R^2=r^2.\\) 2.14 Muestra que el residuo promedio es cero, es decir, muestra que \\(n^{-1}\\sum_{i=1}^{n} e_i=0.\\) 2.15 Correlación entre residuos y variables explicativas. Considera una secuencia genérica de pares de números \\((x_1,y_1)\\), …, \\((x_n,y_n)\\) con el coeficiente de correlación calculado como \\(r(y,x)=\\left[ (n-1)s_ys_x\\right] ^{-1}\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) \\left( x_i-\\overline{x}\\right) .\\) Supón que \\(\\overline{y}=0\\), \\(\\overline{x}=0\\) o ambos \\(\\overline{x}\\) y \\(\\overline{y}=0\\). Luego, verifica que \\(r(y,x)=0\\) implica \\(\\sum_{i=1}^{n}y_i x_i=0\\) y viceversa. Muestra que la correlación entre los residuos y las variables explicativas es cero. Haz esto usando la parte (a) del Ejercicio 2.13 para mostrar que \\(\\sum_{i=1}^{n} x_i e_i = 0\\) y luego aplica la parte (a). Muestra que la correlación entre los residuos y los valores ajustados es cero. Haz esto mostrando que \\(\\sum_{i=1}^n \\widehat{y}_i e_i = 0\\) y luego aplica la parte (a). 2.16 Correlación y estadísticas \\(t\\). Usa los siguientes pasos para establecer una relación entre el coeficiente de correlación y el estadístico \\(t\\) para la pendiente. Usa álgebra para verificar que \\[\\begin{equation*} R^2=1-\\frac{n-2}{n-1}\\frac{s^2}{s_y^2}. \\end{equation*}\\] Usa la parte (a) para establecer la siguiente fórmula rápida para \\(s\\), \\[\\begin{equation*}s = s_y \\sqrt{(1-r^2)\\frac{n-1}{n-2}}.\\end{equation*}\\] Usa la parte (b) para mostrar que \\[\\begin{equation*} t(b_1) = \\sqrt{n-2}\\frac{r}{\\sqrt{1-r^2}}. \\end{equation*}\\] 2.17 Efectos de un punto inusual. Estás analizando un conjunto de datos de tamaño \\(n=100\\). Has realizado un análisis de regresión usando una variable predictora y notas que el residuo para la décima observación es inusualmente grande. Supón que, de hecho, resulta que \\(e_{10}=8s\\). ¿Qué porcentaje de la suma de cuadrados de los errores, \\(Error~SS\\), se debe a la décima observación? Supón que \\(e_{10}=4s\\). ¿Qué porcentaje de la suma de cuadrados de errores, \\(Error~SS\\), se debe a la décima observación? Supón que reduces el conjunto de datos a tamaño \\(n=20\\). Después de realizar la regresión, resulta que todavía tenemos \\(e_{10}=4s\\). ¿Qué porcentaje de la suma de cuadrados de errores, \\(Error~SS\\), se debe a la décima observación? 2.18 Considera un conjunto de datos de 20 observaciones con las siguientes estadísticas resumen: \\(\\overline{x}=0\\), \\(\\overline{y}=9\\), \\(s_x=1\\) y \\(s_y=10\\). Realizas una regresión usando una variable y determinas que \\(s=7\\). Determina el error estándar de una predicción en \\(x_{\\ast}=1.\\) 2.19 Las estadísticas resumen pueden ocultar relaciones importantes. Los datos en Tabla 2.9 son de Anscombe (1973). El propósito de este ejercicio es demostrar cómo graficar los datos puede revelar información importante que no es evidente en las estadísticas numéricas resumen. Tabla 2.9. Datos de Anscombe (1973) \\[ {\\small \\begin{array}{c|rrrrrr} \\hline obs &amp; &amp; &amp; &amp; &amp; &amp; \\\\ num &amp; x_1 &amp; y_1 &amp; y_2 &amp; y_3 &amp; x_2 &amp; y_4 \\\\ \\hline 1 &amp; 10 &amp; 8.04 &amp; 9.14 &amp; 7.46 &amp; 8 &amp; 6.58 \\\\ 2 &amp; 8 &amp; 6.95 &amp; 8.14 &amp; 6.77 &amp; 8 &amp; 5.76 \\\\ 3 &amp; 13 &amp; 7.58 &amp; 8.74 &amp; 12.74 &amp; 8 &amp; 7.71 \\\\ 4 &amp; 9 &amp; 8.81 &amp; 8.77 &amp; 7.11 &amp; 8 &amp; 8.84 \\\\ 5 &amp; 11 &amp; 8.33 &amp; 9.26 &amp; 7.81 &amp; 8 &amp; 8.47 \\\\ 6 &amp; 14 &amp; 9.96 &amp; 8.10 &amp; 8.84 &amp; 8 &amp; 7.04 \\\\ 7 &amp; 6 &amp; 7.24 &amp; 6.13 &amp; 6.08 &amp; 8 &amp; 5.25 \\\\ 8 &amp; 4 &amp; 4.26 &amp; 3.10 &amp; 5.39 &amp; 8 &amp; 5.56 \\\\ 9 &amp; 12 &amp; 10.84 &amp; 9.13 &amp; 8.15 &amp; 8 &amp; 7.91 \\\\ 10 &amp; 7 &amp; 4.82 &amp; 7.26 &amp; 6.42 &amp; 8 &amp; 6.89 \\\\ 11 &amp; 5 &amp; 5.68 &amp; 4.74 &amp; 5.73 &amp; 19 &amp; 12.50 \\\\ \\hline \\end{array} } \\] Calcula los promedios y desviaciones estándar de cada columna de datos. Verifica que los promedios y desviaciones estándar de cada una de las columnas \\(x\\) son iguales, dentro de dos decimales, y de manera similar para cada una de las columnas \\(y\\). Realiza cuatro regresiones, (1) \\(y_{1}\\) sobre \\(x_{1}\\), (2) \\(y_2\\) sobre \\(x_{1}\\), (3) \\(y_{3}\\) sobre \\(x_{1}\\) y (4) \\(y_{4}\\) sobre \\(x_2\\). Verifica, para cada uno de los cuatro ajustes de regresión, que \\(b_0\\approx 3.0\\), \\(b_{1}\\approx 0.5\\), \\(s\\approx 1.237\\) y \\(R^2\\approx 0.677\\), dentro de dos decimales. Produce diagramas de dispersión para cada uno de los cuatro modelos de regresión que ajustaste en el apartado (b). Discute el hecho de que los modelos de regresión ajustados en el apartado (b) implican que los cuatro conjuntos de datos son similares, aunque los cuatro diagramas de dispersión producidos en el apartado (c) muestran una historia dramáticamente diferente. 2.20 Utilización de Hogares de Cuidado. Este ejercicio considera los datos de hogares de cuidado proporcionados por el Departamento de Salud y Servicios Familiares de Wisconsin (DHFS) y descritos en el Ejercicio 1.2 y 2.10. Decides examinar la relación entre los años totales de pacientes (LOGTPY) y el número de camas (LOGNUMBED), ambos en unidades logarítmicas, usando datos del año 2001. Estadísticas descriptivas. Crea estadísticas descriptivas básicas para cada variable. Resume la relación mediante un estadístico de correlación y un diagrama de dispersión. Ajusta el modelo lineal básico. Cita las estadísticas descriptivas básicas, incluye el coeficiente de determinación, el coeficiente de regresión para LOGNUMBED y el estadístico \\(t\\) correspondiente. Pruebas de hipótesis. Prueba las siguientes hipótesis al nivel de significancia del 5% usando un estadístico \\(t\\). También calcula el valor \\(p\\) correspondiente. c(i). Prueba \\(H_0: \\beta_1 = 0\\) frente a \\(H_a: \\beta_1 \\neq 0\\). c(ii). Prueba \\(H_0: \\beta_1 = 1\\) frente a \\(H_a: \\beta_1 \\neq 1\\). c(iii). Prueba \\(H_0: \\beta_1 = 1\\) frente a \\(H_a: \\beta_1 &gt; 1\\). c(iv). Prueba \\(H_0: \\beta_1 = 1\\) frente a \\(H_a: \\beta_1 &lt; 1\\). Estás interesado en el efecto que un cambio marginal en LOGNUMBED tiene sobre el valor esperado de LOGTPY. d(i). Supón que hay un cambio marginal en LOGNUMBED de 2. Proporciona una estimación puntual del cambio esperado en LOGTPY. d(ii). Proporciona un intervalo de confianza del 95% correspondiente a la estimación puntual en la parte d(i). d(iii). Proporciona un intervalo de confianza del 99% correspondiente a la estimación puntual en la parte d(i). En un número especificado de camas estimado en \\(x_{*} = 100\\), haz lo siguiente: e(i). Encuentra el valor predicho de LOGTPY. e(ii). Obtén el error estándar de la predicción. e(iii). Obtén un intervalo de predicción del 95% para tu predicción. e(iv). Convierte la predicción puntual en la parte e(i) y el intervalo de predicción obtenido en la parte e(iii) en años totales de personas (mediante exponenciación). e(v). Obtén un intervalo de predicción como en la parte e(iv), correspondiente a un nivel del 90% (en lugar del 95%). 2.21 Ofertas Públicas Iniciales. Como analista financiero, deseas convencer a un cliente de las ventajas de invertir en empresas que acaban de ingresar a una bolsa de valores, en una OPI (oferta pública inicial). Por lo tanto, reúnes datos de 116 empresas que fijaron precios durante el período de seis meses del 1 de enero de 1998 al 1 de junio de 1998. Al mirar estos datos históricos recientes, puedes calcular RETURN, el retorno de la empresa en un año (en porcentaje). También estás interesado en observar características financieras de la empresa que puedan ayudarte a entender (y predecir) el retorno. Inicialmente examinas REVENUE, los ingresos de la empresa en 1997 en millones de dólares. Desafortunadamente, esta variable no estaba disponible para seis empresas. Por lo tanto, las estadísticas a continuación son para las 110 empresas que tienen tanto REVENUE como RETURN. Además, la Tabla 2.9 proporciona información sobre los ingresos logarítmicos (naturales), denominados como LnREV, y el precio inicial de la acción, denominado PRICEIPO. Table 2.9: Estadísticas Resumen de Cada Variable Media Mediana Desviación Estándar Mínimo Máximo RETURN 0.106 -0.130 0.824 -0.938 4.333 REV 134.487 39.971 261.881 0.099 1455.761 LnREV 3.686 3.688 1.698 -2.316 7.283 PRICEIPO 13.195 13.000 4.694 4.000 29.000 Hipotetizas que las empresas más grandes, medida por ingresos, son más estables y, por lo tanto, deberían tener mayores retornos. Has determinado que la correlación entre RETURN y REVENUE es -0.0175. a(i). Calcula el ajuste de mínimos cuadrados usando REVENUE para predecir RETURN. Determina \\(b_0\\) y \\(b_1\\). a(ii). Para Hyperion Telecommunications, los ingresos son 95.55 (millones de dólares). Calcula el RETURN ajustado usando el ajuste de regresión en la parte a(i). Tabla 2.11. Resultados de la Regresión con Ingresos Logarítmicos \\[ {\\small \\begin{array}{l|rrr} \\hline &amp; &amp; \\text{Error} &amp; \\\\ \\text{Variable} &amp; \\text{Coeficiente} &amp; \\text{Estándar} &amp; t-\\text{estadístico} \\\\ \\hline \\text{INTERCEPTO} &amp; 0.438 &amp; 0.186 &amp; 2.35\\\\ \\text{LnREV} &amp; -0.090 &amp; 0.046 &amp; -1.97 \\\\ \\hline s = 0.8136, &amp; R^2 = 0.03452 \\\\ \\hline \\end{array} } \\] Ingresos logarítmicos y retornos. b(i). Supón que usas LnREV para predecir RETURN. Calcula el RETURN ajustado bajo este modelo de regresión. ¿Es igual a tu respuesta en la parte a(ii)? b(ii) ¿Afectan significativamente los ingresos logarítmicos a los retornos? Para ello, proporciona una prueba formal de hipótesis. Expón tus hipótesis nula y alternativa, el criterio de toma de decisiones y la regla de toma de decisiones. Usa un nivel de significancia del 10%. b(iii). Hipotetizas que, manteniendo todo constante, las empresas con mayores ingresos serán más estables y, por lo tanto, tendrán un mayor retorno inicial. Por lo tanto, deseas considerar la hipótesis nula de ninguna relación entre LnREV y RETURN frente a la hipótesis alternativa de que hay una relación positiva entre LnREV y RETURN. Para ello, proporciona una prueba formal de hipótesis. Expón tus hipótesis nula y alternativa, el criterio de toma de decisiones y la regla de toma de decisiones. Usa un nivel de significancia del 10%. Determina la correlación entre LnREV y RETURN. Asegúrate de indicar si esta correlación es positiva, negativa o cero. Estás considerando invertir en una empresa que tiene LnREV = 2 (por lo que los ingresos son \\(e^2\\) = 7.389 millones de dólares). d(i). Usando el modelo de regresión ajustado, determina la predicción puntual de mínimos cuadrados. d(ii). Determina el intervalo de predicción del 95% correspondiente a tu predicción en la parte d(i). El \\(R^2\\) del modelo de regresión ajustado es un decepcionante 3.5%. Parte de la dificultad se debe a la observación número 59, la Corporación Inktomi. Las ventas de Inktomi están en el 12º lugar más bajo del conjunto de datos, con LnREV = 1.76 (por lo que los ingresos son \\(e^{1.76} = 5.79\\) millones de dólares), pero tiene el mayor retorno en el primer año, con RETURN = 433.33. e(i). Calcula el residuo para esta observación. e(ii). ¿Qué proporción de la variabilidad no explicada (suma de cuadrados de errores) representa esta observación? e(iii). Define la idea de una observación de alto apalancamiento. e(iv). ¿Se consideraría esta observ ación como una observación de alto apalancamiento? Justifica tu respuesta. 2.22 Esperanzas de Vida Nacionales. Continuamos el análisis iniciado en el Ejercicio 1.7 examinando la relación entre \\(y= LIFEEXP\\) y \\(x=FERTILITY\\), mostrado en la Figura 2.12. Ajusta un modelo de regresión lineal de \\(LIFEEXP\\) usando la variable explicativa \\(x=FERTILITY\\). Figure 2.12: Gráfico de FERTILITY versus LIFEEXP. EE.UU. tiene una tasa de FERTILITY de 2.0. Determina la esperanza de vida ajustada. La nación insular Dominica no reportó una tasa de FERTILITY y, por lo tanto, no se incluyó en la regresión. Supón que su tasa de FERTILITY es 2.0. Proporciona un intervalo de predicción del 95% para la esperanza de vida en Dominica. China tiene una tasa de FERTILITY de 1.7 y una esperanza de vida de 72.5. Determina el residuo bajo el modelo. ¿Cuántos múltiplos de \\(s\\) está este residuo alejado de cero? Supón que tu hipótesis previa es que la pendiente de FERTILITY es -6.0 y deseas probar la hipótesis nula de que la pendiente ha aumentado (es decir, la pendiente es mayor que -6.0). Prueba esta hipótesis al nivel de significancia del 5%. También calcula un valor \\(p\\) aproximado. 2.11 Suplemento Técnico - Elementos del Álgebra de Matrices Los ejemplos son una herramienta excelente para introducir temas técnicos como la regresión. Sin embargo, este capítulo también ha utilizado álgebra, así como probabilidad y estadística básica, para darte una comprensión más profunda del análisis de regresión. A partir de ahora, estudiaremos relaciones multivariantes. Con muchas cosas ocurriendo simultáneamente en varias dimensiones, el álgebra ya no es útil para proporcionar información. En cambio, necesitaremos el álgebra de matrices. Este suplemento ofrece una breve introducción al álgebra de matrices para que puedas estudiar los capítulos de regresión lineal de este texto. El Apéndice A3 define conceptos adicionales de matrices. 2.11.1 Definiciones Básicas Una matriz es una tabla rectangular de números organizados en filas y columnas (el plural de matriz es matrices). Por ejemplo, considera los ingresos y la edad de 3 personas. \\[\\begin{equation*} \\mathbf{A}= \\begin{array}{c} Fila~1 \\\\ Fila~2 \\\\ Fila~3 \\end{array} \\overset{ \\begin{array}{cc} ~~~Col~1~ &amp; Col~2 \\end{array} }{\\left( \\begin{array}{cc} 6,000 &amp; 23 \\\\ 13,000 &amp; 47 \\\\ 11,000 &amp; 35 \\end{array} \\right) } \\end{equation*}\\] Aquí, la columna 1 representa el ingreso y la columna 2 representa la edad. Cada fila corresponde a un individuo. Por ejemplo, el primer individuo tiene 23 años y un ingreso de $6,000. El número de filas y columnas se llama la dimensión de la matriz. Por ejemplo, la dimensión de la matriz \\(\\mathbf{A}\\) anterior es \\(3\\times 2\\) (se lee 3 “por” 2). Esto significa 3 filas y 2 columnas. Si quisiéramos representar los ingresos y la edad de 100 personas, entonces la dimensión de la matriz sería \\(100\\times 2\\). Es conveniente representar una matriz usando la notación \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\\\ a_{31} &amp; a_{31} \\end{array} \\right) . \\end{equation*}\\] Aquí, \\(a_{ij}\\) es el símbolo para el número en la \\(i\\)-ésima fila y \\(j\\)-ésima columna de \\(\\mathbf{A}\\). En general, trabajamos con matrices de la forma \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nk} \\end{array} \\right) . \\end{equation*}\\] En este caso, la matriz \\(\\mathbf{A}\\) tiene dimensión \\(n\\times k\\). Un vector es una matriz especial. Un vector fila es una matriz que contiene solo 1 fila (\\(k=1\\)). Un vector columna es una matriz que contiene solo 1 columna (\\(n=1\\)). Por ejemplo, \\[\\begin{equation*} \\text{vector columna}\\rightarrow \\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\end{array} \\right) ~~~~\\text{vector fila}\\rightarrow \\left( \\begin{array}{ccccc} 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\end{array} \\right) . \\end{equation*}\\] Observa que el vector fila ocupa mucho menos espacio en una página impresa que el vector columna correspondiente. Una operación básica que relaciona estas dos cantidades es la transposición. La transposición de una matriz \\(\\mathbf{A}\\) se define intercambiando las filas y columnas y se denota por \\(\\mathbf{A }^{\\prime }\\) (o \\(\\mathbf{A}^{T}\\)). Por ejemplo, \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} 6,000 &amp; 23 \\\\ 13,000 &amp; 47 \\\\ 11,000 &amp; 35 \\end{array} \\right) ~~~\\mathbf{A}^{\\prime }=\\left( \\begin{array}{ccc} 6,000 &amp; 13,000 &amp; 11,000 \\\\ 23 &amp; 47 &amp; 35 \\end{array} \\right) . \\end{equation*}\\] Así, si \\(\\mathbf{A}\\) tiene dimensión \\(n\\times k\\), entonces \\(\\mathbf{A}^{\\prime }\\) tiene dimensiones \\(k\\times n\\). 2.11.2 Algunas Matrices Especiales Una matriz cuadrada es una matriz donde el número de filas es igual al número de columnas, es decir, \\(n=k\\). Los números diagonales de una matriz cuadrada son los números en una matriz donde el número de fila es igual al número de columna, por ejemplo, \\(a_{11}\\), \\(a_{22}\\), y así sucesivamente. Una matriz diagonal es una matriz cuadrada en la que todos los números no diagonales son iguales a 0. Por ejemplo, \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{ccc} -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{array} \\right) . \\end{equation*}\\] Una matriz identidad es una matriz diagonal donde todos los números diagonales son iguales a 1. Esta matriz especial se denota a menudo por \\(\\mathbf{I}\\). Una matriz simétrica es una matriz cuadrada \\(\\mathbf{A}\\) tal que la matriz permanece sin cambios si intercambiamos las filas y las columnas. Más formalmente, una matriz \\(\\mathbf{A}\\) es simétrica si \\(\\mathbf{A=A} ^{\\prime }\\). Por ejemplo, \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{ccc} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 10 \\end{array} \\right) \\mathbf{=A}^{\\prime }. \\end{equation*}\\] Observa que una matriz diagonal es una matriz simétrica. 2.11.3 Operaciones Básicas Multiplicación por un Escalar Sea \\(\\mathbf{A}\\) una matriz de \\(n\\times k\\) y sea \\(c\\) un número real. Es decir, un número real es una matriz de \\(1\\times 1\\) y también se llama escalar. Multiplicar un escalar \\(c\\) por una matriz \\(\\mathbf{A}\\) se denota por \\(c\\mathbf{A}\\) y se define por \\[\\begin{equation*} c\\mathbf{A}=\\left( \\begin{array}{cccc} ca_{11} &amp; ca_{12} &amp; \\cdots &amp; ca_{1k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ ca_{n1} &amp; ca_{n2} &amp; \\cdots &amp; ca_{nk} \\end{array} \\right) . \\end{equation*}\\] Por ejemplo, supongamos que \\(c=10\\) y \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} 1 &amp; 2 \\\\ 6 &amp; 8 \\end{array} \\right) ~~~~~\\text{entonces} ~~~~\\mathbf{B}=c\\mathbf{A}=\\left( \\begin{array}{cc} 10 &amp; 20 \\\\ 60 &amp; 80 \\end{array} \\right) . \\end{equation*}\\] Observa que \\(c\\mathbf{A}=\\mathbf{A}c\\). Suma y Resta de Matrices Sean \\(\\mathbf{A}\\) y \\(\\mathbf{B}\\) matrices con dimensiones \\(n\\times k\\). Utiliza \\(a_{ij}\\) y \\(b_{ij}\\) para denotar los números en la \\(i\\)-ésima fila y \\(j\\)-ésima columna de \\(\\mathbf{A}\\) y \\(\\mathbf{B}\\), respectivamente. Entonces, la matriz \\(\\mathbf{C}=\\mathbf{A}+\\mathbf{B}\\) se define como la matriz con \\((a_{ij}+b_{ij})\\) en la \\(i\\)-ésima fila y \\(j\\)-ésima columna. De manera similar, la matriz \\(\\mathbf{C}=\\mathbf{A}-\\mathbf{B}\\) se define como la matriz con \\((a_{ij}-b_{ij})\\) en la \\(i\\)-ésima fila y \\(j\\)-ésima columna. Simbólicamente, escribimos esto como sigue. \\[\\begin{equation*} \\text{Si }\\mathbf{A=}\\left( a_{ij}\\right) _{ij}\\text{ y } \\mathbf{B=}\\left( b_{ij}\\right) _{ij}\\text{, entonces} \\end{equation*}\\] \\[\\begin{equation*} \\mathbf{C}=\\mathbf{A}+\\mathbf{B=}\\left( a_{ij}+b_{ij}\\right) _{ij}\\text{ y }\\mathbf{C}=\\mathbf{A}-\\mathbf{B=}\\left( a_{ij}-b_{ij}\\right) _{ij}. \\end{equation*}\\] Por ejemplo, considera \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} 2 &amp; 5 \\\\ 4 &amp; 1 \\end{array} \\right) ~~~\\mathbf{B}=\\left( \\begin{array}{cc} 4 &amp; 6 \\\\ 8 &amp; 1 \\end{array} \\right). \\end{equation*}\\] Entonces \\[\\begin{equation*} \\mathbf{A}+\\mathbf{B}=\\left( \\begin{array}{cc} 6 &amp; 11 \\\\ 12 &amp; 2 \\end{array} \\right) ~~~\\mathbf{A}-\\mathbf{B}=\\left( \\begin{array}{cc} -2 &amp; -1 \\\\ -4 &amp; 0 \\end{array} \\right) . \\end{equation*}\\] Ejemplo Básico de Regresión Lineal de Suma y Resta. Ahora, recuerda que el modelo básico de regresión lineal puede escribirse como \\(n\\) ecuaciones: \\[\\begin{equation*} \\begin{array}{c} y_1=\\beta_0+\\beta_1x_1+\\varepsilon _1 \\\\ \\vdots \\\\ y_n=\\beta_0+\\beta_1x_n+\\varepsilon _n. \\end{array} \\end{equation*}\\] Podemos definir \\[\\begin{equation*} \\mathbf{y}=\\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) ~~~\\boldsymbol \\varepsilon = \\left( \\begin{array}{c} \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_n \\end{array} \\right) ~~~\\text{y}~~~ \\mathrm{E~}\\mathbf{y} =\\left( \\begin{array}{c} \\beta_0+\\beta_1 x_1 \\\\ \\vdots \\\\ \\beta_0 + \\beta_1 x_n \\end{array} \\right) . \\end{equation*}\\] Con esta notación, podemos expresar las \\(n\\) ecuaciones de manera más compacta como \\(\\mathbf{y} = \\mathrm{E~}\\mathbf{y}+\\boldsymbol \\varepsilon\\). Multiplicación de Matrices En general, si \\(\\mathbf{A}\\) es una matriz de dimensión \\(n\\times c\\) y \\(\\mathbf{B}\\) es una matriz de dimensión \\(c\\times k\\), entonces \\(\\mathbf{C}=\\mathbf{AB}\\) es una matriz de dimensión \\(n\\times k\\) y se define por \\[\\begin{equation*} \\mathbf{C}=\\mathbf{AB}=\\left( \\sum_{s=1}^{c}a_{is}b_{sj}\\right)_{ij}. \\end{equation*}\\] Por ejemplo, considera las matrices \\(2\\times 2\\) \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} 2 &amp; 5 \\\\ 4 &amp; 1 \\end{array} \\right) ~~~\\mathbf{B}=\\left( \\begin{array}{cc} 4 &amp; 6 \\\\ 8 &amp; 1 \\end{array} \\right) . \\end{equation*}\\] La matriz \\(\\mathbf{AB}\\) tiene dimensión \\(2\\times 2\\). Para ilustrar el cálculo, considera el número en la primera fila y segunda columna de \\(\\mathbf{AB}\\). Según la regla presentada arriba, con \\(i=1\\) y \\(j=2\\), el elemento correspondiente de \\(\\mathbf{AB}\\) es \\(\\sum_{s=1}^2a_{1s}b_{s2}=a_{11}b_{12}+a_{12}b_{22}=2(6)+5(1)=17\\). Los otros cálculos se resumen como \\[\\begin{equation*} \\mathbf{AB}=\\left( \\begin{array}{cc} 2(4)+5(8) &amp; 2(6)+5(1) \\\\ 4(4)+1(8) &amp; 4(6)+1(1) \\end{array} \\right) =\\left( \\begin{array}{cc} 48 &amp; 17 \\\\ 24 &amp; 25 \\end{array} \\right) . \\end{equation*}\\] Como otro ejemplo, supongamos \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{ccc} 1 &amp; 2 &amp; 4 \\\\ 0 &amp; 5 &amp; 8 \\end{array} \\right) ~~~\\mathbf{B}=\\left( \\begin{array}{c} 3 \\\\ 5 \\\\ 2 \\end{array} \\right) . \\end{equation*}\\] Como \\(\\mathbf{A}\\) tiene dimensión \\(2\\times 3\\) y \\(\\mathbf{B}\\) tiene dimensión \\(3\\times 1\\), esto significa que el producto \\(\\mathbf{AB}\\) tiene dimensión \\(2\\times 1\\). Los cálculos se resumen como \\[\\begin{equation*} \\mathbf{AB}=\\left( \\begin{array}{c} 1(3)+2(5)+4(2) \\\\ 0(3)+5(5)+2 \\end{array} \\right) =\\left( \\begin{array}{c} 21 \\\\ 41 \\end{array} \\right) . \\end{equation*}\\] Para algunos ejemplos adicionales, tenemos \\[\\begin{equation*} \\left( \\begin{array}{cc} 4 &amp; 2 \\\\ 5 &amp; 8 \\end{array} \\right) \\left( \\begin{array}{c} a_1 \\\\ a_2 \\end{array} \\right) =\\left( \\begin{array}{c} 4a_1+2a_2 \\\\ 5a_1+8a_2 \\end{array} \\right) . \\end{equation*}\\] \\[\\begin{equation*} \\left( \\begin{array}{ccc} 2 &amp; 3 &amp; 5 \\end{array} \\right) \\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 5 \\end{array} \\right) =2^2+3^2+5^2=38~~~\\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 5 \\end{array} \\right) \\left( \\begin{array}{ccc} 2 &amp; 3 &amp; 5 \\end{array} \\right) =\\left( \\begin{array}{ccc} 4 &amp; 6 &amp; 10 \\\\ 6 &amp; 9 &amp; 15 \\\\ 10 &amp; 15 &amp; 25 \\end{array} \\right) . \\end{equation*}\\] En general, observa que \\(\\mathbf{AB}\\neq \\mathbf{BA}\\) en la multiplicación de matrices, a diferencia de la multiplicación de escalares (números reales). Además, observamos que la matriz identidad cumple el papel de “uno” en la multiplicación de matrices, ya que \\(\\mathbf{AI=A}\\) y \\(\\mathbf{IA=A}\\) para cualquier matriz \\(\\mathbf{A}\\), siempre que las dimensiones sean compatibles para permitir la multiplicación de matrices. Ejemplo Básico de Regresión Lineal de Multiplicación de Matrices. Define \\[\\begin{equation*} \\mathbf{X}=\\left( \\begin{array}{cc} 1 &amp; x_1 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{array} \\right) \\text{ y } \\boldsymbol \\beta =\\left( \\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array} \\right) \\text{, para obtener } \\mathbf{X} \\boldsymbol{\\beta} =\\left( \\begin{array}{c} \\beta_0+\\beta_1x_1 \\\\ \\vdots \\\\ \\beta_0+\\beta_1x_n \\end{array} \\right) =\\mathbf{\\mathrm{E~}\\mathbf{y}}. \\end{equation*}\\] Así, se obtiene la expresión matricial familiar del modelo de regresión, \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\). Otras cantidades útiles incluyen \\[\\begin{equation*} \\mathbf{y}^{\\prime }\\mathbf{y}=\\left( \\begin{array}{ccc} y_1 &amp; \\cdots &amp; y_n \\end{array} \\right) \\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) =y_1^2+\\cdots +y_n^2=\\sum_{i=1}^{n}y_i^2, \\end{equation*}\\] \\[\\begin{equation*} \\mathbf{X}^{\\prime }\\mathbf{y}=\\left( \\begin{array}{ccc} 1 &amp; \\cdots &amp; 1 \\\\ x_1 &amp; \\cdots &amp; x_n \\end{array} \\right) \\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) =\\left( \\begin{array}{c} \\sum_{i=1}^{n}y_i \\\\ \\sum_{i=1}^{n}x_iy_i \\end{array} \\right) \\end{equation*}\\] y \\[\\begin{equation*} \\mathbf{X}^{\\prime }\\mathbf{X}=\\left( \\begin{array}{ccc} 1 &amp; \\cdots &amp; 1 \\\\ x_1 &amp; \\cdots &amp; x_n \\end{array} \\right) \\left( \\begin{array}{cc} 1 &amp; x_1 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{array} \\right) =\\left( \\begin{array}{cc} n &amp; \\sum_{i=1}^{n}x_i \\\\ \\sum_{i=1}^{n}x_i &amp; \\sum_{i=1}^{n} x_i^2 \\end{array} \\right) . \\end{equation*}\\] Observa que \\(\\mathbf{X}^{\\prime }\\mathbf{X}\\) es una matriz simétrica. Inversas de Matrices En álgebra de matrices, no existe el concepto de “división.” En su lugar, extendemos el concepto de “recíprocos” de los números reales. Para comenzar, supongamos que \\(\\mathbf{A}\\) es una matriz cuadrada de dimensión \\(k \\times k\\) y que \\(\\mathbf{I}\\) es la matriz identidad de dimensión \\(k \\times k\\). Si existe una matriz \\(k \\times k\\) llamada \\(\\mathbf{B}\\) tal que \\(\\mathbf{AB}=\\mathbf{I}=\\mathbf{BA}\\), entonces \\(\\mathbf{B}\\) se llama inversa de \\(\\mathbf{A}\\) y se escribe como \\[\\begin{equation*} \\mathbf{B}=\\mathbf{A}^{-1}. \\end{equation*}\\] No todas las matrices cuadradas tienen inversas. Además, incluso cuando existe una inversa, puede no ser fácil de calcular manualmente. Una excepción a esta regla son las matrices diagonales. Supongamos que \\(\\mathbf{A}\\) es una matriz diagonal de la forma \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{ccc} a_{11} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; a_{kk} \\end{array} \\right). \\text{ Entonces } \\mathbf{A}^{-1}=\\left( \\begin{array}{ccc} \\frac{1}{a_{11}} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; \\frac{1}{a_{kk}} \\end{array} \\right). \\end{equation*}\\] Por ejemplo, \\[\\begin{equation*} \\begin{array}{cccc} \\left( \\begin{array}{cc} 2 &amp; 0 \\\\ 0 &amp; -19 \\end{array} \\right) &amp; \\left( \\begin{array}{cc} \\frac{1}{2} &amp; 0 \\\\ 0 &amp; -\\frac{1}{19} \\end{array} \\right) &amp; = &amp; \\left( \\begin{array}{cc} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right) \\\\ \\mathbf{A} &amp; \\mathbf{A}^{-1} &amp; = &amp; \\mathbf{I} \\end{array} . \\end{equation*}\\] En el caso de una matriz de dimensión \\(2\\times 2\\), el procedimiento de inversión se puede realizar manualmente incluso cuando la matriz no es diagonal. En el caso de \\(2\\times 2\\), supongamos que si \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} a &amp; b \\\\ c &amp; d \\end{array} \\right), \\text{ entonces } \\mathbf{A}^{-1}=\\frac{1}{ad-bc}\\left( \\begin{array}{cc} d &amp; -b \\\\ -c &amp; a \\end{array} \\right) \\text{.} \\end{equation*}\\] Así, por ejemplo, si \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} 2 &amp; 2 \\\\ 3 &amp; 4 \\end{array} \\right) \\text{ entonces } \\mathbf{A}^{-1}=\\frac{1}{2(4)-2(3)} \\left( \\begin{array}{cc} 4 &amp; -2 \\\\ -3 &amp; 2 \\end{array} \\right) =\\left( \\begin{array}{cc} 2 &amp; -1 \\\\ -3/2 &amp; 1 \\end{array} \\right) \\text{.} \\end{equation*}\\] Como verificación, tenemos \\[\\begin{equation*} \\mathbf{A}\\mathbf{A}^{-1}=\\left( \\begin{array}{cc} 2 &amp; 2 \\\\ 3 &amp; 4 \\end{array} \\right) \\left( \\begin{array}{cc} 2 &amp; -1 \\\\ -3/2 &amp; 1 \\end{array} \\right) =\\left( \\begin{array}{cc} 2(2)-2(3/2) &amp; 2(-1)+2(1) \\\\ 3(2)-4(3/2) &amp; 3(-1)+4(1) \\end{array} \\right) =\\left( \\begin{array}{cc} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right) =\\mathbf{I}\\text{.} \\end{equation*}\\] Ejemplo Básico de Regresión Lineal de Inversas de Matrices. Con \\[\\begin{equation*} \\mathbf{X}^{\\prime }\\mathbf{X}=\\left( \\begin{array}{cc} n &amp; \\sum\\limits_{i=1}^{n}x_i \\\\ \\sum\\limits_{i=1}^{n}x_i &amp; \\sum\\limits_{i=1}^{n}x_i^2 \\end{array} \\right), \\end{equation*}\\] tenemos \\[\\begin{equation*} \\left( \\mathbf{X}^{\\prime }\\mathbf{X}\\right)^{-1}=\\frac{1}{n\\sum_{i=1}^{n}x_i^2-\\left( \\sum_{i=1}^{n}x_i\\right) ^2}\\left( \\begin{array}{cc} \\sum\\limits_{i=1}^{n}x_i^2 &amp; -\\sum\\limits_{i=1}^{n}x_i \\\\ -\\sum\\limits_{i=1}^{n}x_i &amp; n \\end{array} \\right). \\end{equation*}\\] Para simplificar esta expresión, recuerda que \\(\\overline{x}=n^{-1} \\sum_{i=1}^{n}x_i\\). Así, \\[\\begin{equation} \\left( \\mathbf{X}^{\\prime }\\mathbf{X}\\right)^{-1}=\\frac{1}{ \\sum_{i=1}^{n}x_i^2-n\\overline{x}^2}\\left( \\begin{array}{cc} n^{-1}\\sum\\limits_{i=1}^{n}x_i^2 &amp; -\\overline{x} \\\\ -\\overline{x} &amp; 1 \\end{array} \\right) . \\tag{2.9} \\end{equation}\\] La Sección 3.1 discutirá la relación \\(\\mathbf{b}=\\left( \\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{\\prime}\\mathbf{y}\\). Para ilustrar el cálculo, tenemos \\[\\begin{eqnarray*} \\mathbf{b} &amp;=&amp;\\left( \\mathbf{X}^{\\prime }\\mathbf{X}\\right)^{-1}\\mathbf{X} ^{\\prime }\\mathbf{y}=\\frac{1}{\\sum_{i=1}^{n}x_i^2-n\\overline{x}^2} \\left( \\begin{array}{cc} n^{-1}\\sum\\limits_{i=1}^{n}x_i^2 &amp; -\\overline{x} \\\\ -\\overline{x} &amp; 1 \\end{array} \\right) \\left( \\begin{array}{c} \\sum\\limits_{i=1}^{n}y_i \\\\ \\sum\\limits_{i=1}^{n}x_iy_i \\end{array} \\right) \\\\ &amp;=&amp;\\frac{1}{\\sum_{i=1}^{n}x_i^2-n\\overline{x}^2}\\left( \\begin{array}{c} \\sum\\limits_{i=1}^{n}\\left( \\overline{y}x_i^2-\\overline{x} x_iy_i\\right) \\\\ \\sum\\limits_{i=1}^{n}x_iy_i-n\\overline{x}\\overline{y} \\end{array} \\right) =\\left( \\begin{array}{c} b_0 \\\\ b_1 \\end{array} \\right) . \\end{eqnarray*}\\] De esta expresión, podemos ver \\[\\begin{equation*} b_1=\\frac{\\sum\\limits_{i=1}^{n}x_iy_i-n\\overline{x}\\overline{y}}{\\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x}^2} \\end{equation*}\\] y \\[\\begin{equation*} b_0=\\frac{\\overline{y}\\sum\\limits_{i=1}^{n}x_i^2-\\overline{x} \\sum\\limits_{i=1}^{n}x_iy_i}{\\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x}^2}=\\frac{\\overline{y}\\left( \\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x} ^2\\right) -\\overline{x}\\left( \\sum\\limits_{i=1}^{n} x_i y_i - n\\overline{x} \\overline{y}\\right) }{\\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x}^2}=\\overline{y}-b_1\\overline{x}. \\end{equation*}\\] Estas son las expresiones usuales para la pendiente \\(b_1\\) (Ejercicio 2A.8) y el intercepto \\(b_0\\). 2.11.4 Matrices Aleatorias Esperanzas. Consideremos una matriz de variables aleatorias \\[\\begin{equation*} \\mathbf{U=}\\left( \\begin{array}{cccc} u_{11} &amp; u_{12} &amp; \\cdots &amp; u_{1c} \\\\ u_{21} &amp; u_{22} &amp; \\cdots &amp; u_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ u_{n1} &amp; u_{n2} &amp; \\cdots &amp; u_{nc} \\end{array} \\right). \\end{equation*}\\] Cuando escribimos la esperanza de una matriz, esto es una forma abreviada para la matriz de esperanzas. Específicamente, supongamos que la función de probabilidad conjunta de \\({u_{11}, u_{12}, ..., u_{1c}, ..., u_{n1}, ..., u_{nc}}\\) está disponible para definir el operador de esperanza. Entonces definimos \\[\\begin{equation*} \\mathrm{E} ~ \\mathbf{U} = \\left( \\begin{array}{cccc} \\mathrm{E }u_{11} &amp; \\mathrm{E }u_{12} &amp; \\cdots &amp; \\mathrm{E }u_{1c} \\\\ \\mathrm{E }u_{21} &amp; \\mathrm{E }u_{22} &amp; \\cdots &amp; \\mathrm{E }u_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathrm{E }u_{n1} &amp; \\mathrm{E }u_{n2} &amp; \\cdots &amp; \\mathrm{E }u_{nc} \\end{array} \\right). \\end{equation*}\\] Como un caso especial importante, consideremos la función de probabilidad conjunta para las variables aleatorias \\(y_1, \\ldots, y_n\\) y el operador de expectativas correspondiente. Entonces \\[\\begin{equation*} \\mathrm{E}~ \\mathbf{y=} \\mathrm{E } \\left( \\begin{array}{cccc} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) = \\left( \\begin{array}{cccc} \\mathrm{E }y_1 \\\\ \\vdots \\\\ \\mathrm{E }y_n \\end{array} \\right). \\end{equation*}\\] Por la linealidad de las esperanzas, para una matriz no aleatoria A y un vector , tenemos \\(\\mathrm{E} (\\textbf{A y} + \\textbf{B}) = \\textbf{A} \\mathrm{E} \\textbf{y + B}\\). Varianzas. También podemos trabajar con los segundos momentos de vectores aleatorios. La varianza de un vector de variables aleatorias se llama matriz de varianza-covarianza. Se define como \\[\\begin{equation} \\mathrm{Var} ~ \\mathbf{y} = \\mathrm{E} ( (\\mathbf{y} - \\mathrm{E} \\mathbf{y})(\\mathbf{y} - \\mathrm{E} \\mathbf{y})^{\\prime} ). \\tag{2.10} \\end{equation}\\] Es decir, podemos expresar \\[\\begin{equation*} \\mathrm{Var}~\\mathbf{y=} \\mathrm{E } \\left( \\left( \\begin{array}{c} y_1 -\\mathrm{E } y_1 \\\\ \\vdots \\\\ y_n -\\mathrm{E } y_n \\end{array}\\right) \\left(\\begin{array}{ccc} y_1 - \\mathrm{E } y_1 &amp; \\cdots &amp; y_n - \\mathrm{E } y_n \\end{array}\\right) \\right) \\end{equation*}\\] \\[\\begin{equation*} = \\left( \\begin{array}{cccc} \\mathrm{Var}~y_1 &amp; \\mathrm{Cov}(y_1, y_2) &amp; \\cdots &amp;\\mathrm{Cov}(y_1, y_n) \\\\ \\mathrm{Cov}(y_2, y_1) &amp; \\mathrm{Var}~y_2 &amp; \\cdots &amp; \\mathrm{Cov}(y_2, y_n) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\mathrm{Cov}(y_n, y_1) &amp; \\mathrm{Cov}(y_n, y_2) &amp; \\cdots &amp; \\mathrm{Var}~y_n \\\\ \\end{array}\\right), \\end{equation*}\\] porque \\(\\mathrm{E} ( (y_i - \\mathrm{E} y_i)(y_j - \\mathrm{E} y_j) ) = \\mathrm{Cov}(y_i, y_j)\\) para \\(i \\neq j\\) y \\(\\mathrm{Cov}(y_i, y_i) = \\mathrm{Var}~y_i\\). En el caso de que \\(y_1, \\ldots, y_n\\) sean mutuamente no correlacionados, tenemos que \\(\\mathrm{Cov}(y_i, y_j)=0\\) para \\(i \\neq j\\) y así \\[\\begin{equation*} \\mathrm{Var}~\\mathbf{y=} \\left( \\begin{array}{cccc} \\mathrm{Var}~y_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\mathrm{Var}~y_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0 &amp; \\cdots &amp; \\mathrm{Var}~y_n \\\\ \\end{array}\\right). \\end{equation*}\\] Además, si las varianzas son idénticas, de modo que \\(\\mathrm{Var}~y_i=\\sigma ^2\\), entonces podemos escribir \\(\\mathrm{Var} ~\\mathbf{y} = \\sigma ^2 \\mathbf{I}\\), donde I es la matriz identidad \\(n \\times n\\). Por ejemplo, si \\(y_1, \\ldots, y_n\\) son i.i.d., entonces \\(\\mathrm{Var} ~\\mathbf{y} = \\sigma ^2 \\mathbf{I}\\). A partir de la ecuación (2.10), se puede demostrar que \\[\\begin{equation} \\mathrm{Var}\\left( \\mathbf{Ay +B} \\right) = \\mathrm{Var}\\left( \\mathbf{Ay} \\right) = \\mathbf{A} \\left( \\mathrm{Var}~\\mathbf{y} \\right) \\mathbf{A}^{\\prime}. \\tag{2.11} \\end{equation}\\] Por ejemplo, si \\(\\mathbf{A} = (a_1, a_2, \\ldots,a_n)= \\mathbf{a}^{\\prime}\\) y B = 0, entonces la ecuación (2.11) se reduce a \\[\\begin{equation*} \\mathrm{Var}\\left( \\sum_{i=1}^n a_i y_i \\right) = \\mathrm{Var} \\left( \\mathbf{a^{\\prime} y} \\right) = \\mathbf{a^{\\prime}} \\left( \\mathrm{Var} ~\\mathbf{y} \\right) \\mathbf{a} = (a_1, a_2, \\ldots,a_n) \\left( \\mathrm{Var} ~\\mathbf{y} \\right) \\left(\\begin{array}{c} a_1 \\\\ \\vdots \\\\ a_n \\end{array}\\right) \\end{equation*}\\] \\[\\begin{equation*} = \\sum_{i=1}^n a_i^2 \\mathrm{Var} ~y_i ~+~2 \\sum_{i=2}^n \\sum_{j=1}^{i-1} a_i a_j \\mathrm{Cov}(y_i, y_j). \\end{equation*}\\] Definición - Distribución Normal Multivariante. Un vector de variables aleatorias \\(\\mathbf{y} = \\left(y_1, \\ldots, y_n \\right)^{\\prime}\\) se dice que es normal multivariante si todas las combinaciones lineales de la forma \\(\\sum_{i=1}^n a_i y_i\\) están distribuidas normalmente. En este caso, escribimos \\(\\mathbf{y} \\sim N (\\mathbf{\\boldsymbol \\mu}, \\mathbf{\\Sigma} )\\), donde \\(\\mathbf{\\boldsymbol \\mu} = \\mathrm{E}~ \\mathbf{y}\\) es el valor esperado de y y \\(\\mathbf{\\Sigma}= \\mathrm{Var}~\\mathbf{y}\\) es la matriz de varianza-covarianza de y. Según la definición, tenemos que \\(\\mathbf{y}\\sim N (\\mathbf{\\boldsymbol \\mu}, \\mathbf{\\Sigma} )\\) implica que \\(\\mathbf{a^{\\prime}y}\\sim N (\\mathbf{a^{\\prime} \\boldsymbol \\mu}, \\mathbf{a^{\\prime}\\Sigma a})\\). Así, si \\(y_i\\) son i.i.d., entonces \\(\\sum_{i=1}^n a_i y_i\\) está distribuido normalmente con media \\(\\mu \\sum_{i=1}^n a_i\\) y varianza \\(\\sigma ^2 \\sum_{i=1}^n a_i ^2\\). "],["bibliography.html", "Bibliography", " Bibliography "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
