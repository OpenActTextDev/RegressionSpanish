[["index.html", "Modelado de Regresión con Aplicaciones Actuariales y Financieras Prefacio Prólogo Dedicación", " Modelado de Regresión con Aplicaciones Actuariales y Financieras Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University Prefacio Prólogo Los actuarios y otros analistas financieros cuantifican situaciones usando datos; somos personas de ‘números’. Muchos de nuestros enfoques y modelos son estilizados, basados en años de experiencia e investigaciones realizadas por legiones de analistas. Sin embargo, el mundo financiero y de gestión de riesgos evoluciona rápidamente. Muchos analistas se enfrentan a nuevas situaciones en las que los métodos probados simplemente no funcionan. Aquí es donde entra un conjunto de herramientas como el análisis de regresión. La regresión es el estudio de las relaciones entre variables. Es una disciplina estadística genérica que no se limita al mundo financiero; tiene aplicaciones en campos de ciencias sociales, biológicas y físicas. Puedes usar técnicas de regresión para investigar conjuntos de datos grandes y complejos. Para familiarizarte con la regresión, este libro explora muchos ejemplos y conjuntos de datos basados en aplicaciones actuariales y financieras. Esto no quiere decir que no encontrarás aplicaciones fuera del mundo financiero (por ejemplo, un actuario puede necesitar entender la evidencia científica más reciente sobre pruebas genéticas para fines de suscripción). Sin embargo, al familiarizarte con este conjunto de herramientas, verás cómo la regresión puede aplicarse en muchas (y a veces nuevas) situaciones. ¿Para Quién Es Este Libro? Este libro está escrito para analistas financieros que enfrentan eventos inciertos y desean cuantificar estos eventos utilizando información empírica. No se asume conocimiento previo del sector, aunque los lectores encontrarán la lectura mucho más fácil si tienen interés en las aplicaciones discutidas aquí. Este libro está diseñado para estudiantes que están siendo introducidos al campo, así como para analistas de la industria que deseen repasar técnicas antiguas y (para los capítulos posteriores) obtener una introducción a nuevos desarrollos. Para leer este libro, asumo un conocimiento comparable a una introducción de un semestre a la probabilidad y estadística; el Apéndice A1 proporciona una breve revisión para refrescar conceptos si estás oxidado. Los estudiantes actuariales en Norteamérica tendrán una introducción de un año a la probabilidad y estadística; este tipo de introducción ayudará a los lectores a captar conceptos más rápidamente que una base de un semestre. Finalmente, los lectores encontrarán útil el álgebra matricial, o lineal, aunque no es un requisito previo para leer este texto. Los diferentes lectores están interesados en entender la estadística a diferentes niveles. Este libro está escrito para acomodar al ‘lector de sillón’, es decir, aquel que lee pasivamente y no se involucra intentando realizar los ejercicios del texto. Considera una analogía con el fútbol, o cualquier otro juego. Al igual que el mariscal de campo de sillón en el fútbol, hay mucho que puedes aprender sobre el juego solo con observar. Sin embargo, si quieres agudizar tus habilidades, tienes que salir y jugar el juego. Si realizas los ejercicios o reproduces los análisis estadísticos en el texto, te convertirás en un mejor jugador. Aún así, este texto está escrito entrelazando ejemplos con los principios básicos. Así, incluso el lector de sillón puede obtener una sólida comprensión de las técnicas de regresión a través de este texto. ¿De Qué Trata Este Libro? La Tabla de Contenidos proporciona una visión general de los temas tratados, organizados en cuatro partes. La primera parte introduce la regresión lineal. Este es el material central del libro, con refrescadores sobre estadísticas matemáticas, distribuciones y álgebra matricial entrelazados según sea necesario. La segunda parte se dedica a temas en series temporales. ¿Por qué integrar temas de series temporales en un libro de regresión? Las razones son simples, pero convincentes; la mayoría de los datos contables, financieros y económicos se vuelven disponibles a lo largo del tiempo. Aunque las inferencias transversales son útiles, las decisiones empresariales deben tomarse en tiempo real con los datos actualmente disponibles. Los Capítulos 7-10 introducen técnicas de series temporales que se pueden realizar fácilmente usando herramientas de regresión (y hay muchas). La regresión no lineal es el tema de la tercera parte. Muchas de las herramientas modernas de ‘modelado predictivo’ están basadas en regresión no lineal; estas son las herramientas de trabajo en las oficinas estadísticas de la industria financiera y de gestión de riesgos. La cuarta parte se refiere a ‘aplicaciones actuariales,’ temas que he encontrado relevantes en mi investigación y trabajo de consultoría en gestión de riesgos financieros. Los primeros cuatro capítulos de esta parte consisten en variaciones de modelos de regresión que son particularmente útiles en la gestión de riesgos. Los últimos dos capítulos se centran en las comunicaciones, específicamente, en la redacción de informes y el diseño de gráficos. Comunicar información es un aspecto importante de toda disciplina técnica y la estadística ciertamente no es una excepción. ¿Cómo Transmite Este Libro Su Mensaje? Desarrollo de Capítulos. Cada capítulo tiene varios ejemplos entretejidos con teoría. En los capítulos donde se introduce un modelo, comienzo con un ejemplo y discuto el análisis de datos sin hacer referencia a la teoría. Este análisis se presenta a un nivel intuitivo, sin referencia a un modelo específico. Esto es sencillo, porque se trata de poco más que ajustar curvas. El objetivo es que los estudiantes resuman los datos de manera sensata sin que la noción de un modelo obscurezca un buen análisis de datos. Luego, se proporciona una introducción a la teoría en el contexto del ejemplo introductorio. Se siguen uno o más ejemplos adicionales que refuerzan la teoría ya introducida y proporcionan un contexto para explicar la teoría adicional. En los Capítulos 5 y 6, que no introducen modelos sino técnicas para el análisis, comienzo con una introducción de la técnica. Esta introducción es seguida por un ejemplo que refuerza la explicación. De esta manera, el análisis de datos se puede omitir fácilmente sin pérdida de continuidad, si el tiempo es una preocupación. Datos Reales. Muchos de los ejercicios piden al lector que trabaje con datos reales. La necesidad de trabajar con datos reales está bien documentada; por ejemplo, véase Hogg (1972) o Singer y Willett (1990). Algunos criterios de Singer y Willett para juzgar un buen conjunto de datos incluyen: (1) autenticidad, (2) disponibilidad de información de fondo, (3) interés y relevancia para el aprendizaje sustantivo, y (4) disponibilidad de elementos con los que los lectores puedan identificarse. Por supuesto, hay algunas desventajas importantes al trabajar con datos reales. Los conjuntos de datos pueden volverse obsoletos rápidamente. Además, el conjunto de datos ideal para ilustrar un problema estadístico específico es difícil de encontrar. Esto se debe a que, con datos reales, casi por definición, varios problemas ocurren simultáneamente. Esto hace que sea difícil aislar un aspecto específico. Particularmente disfruto trabajar con grandes conjuntos de datos. Cuanto mayor es el conjunto de datos, mayor es la necesidad de estadísticas para resumir el contenido informativo. Software Estadístico y Datos. Mi objetivo al escribir este texto es llegar a un amplio grupo de estudiantes y analistas de la industria. Por lo tanto, para evitar excluir grandes segmentos, elegí no integrar ningún paquete de software estadístico específico en el texto. Sin embargo, debido a la orientación hacia las aplicaciones, es crucial que la metodología presentada pueda lograrse fácilmente utilizando paquetes disponibles. Para el curso que enseño en la Universidad de Wisconsin, uso los paquetes estadísticos SAS y R. En el Sitio Web del Libro los usuarios encontrarán scripts escritos en SAS y R para el análisis presentado en el texto. Los datos están disponibles en formato de texto, permitiendo a los lectores usar cualquier paquete estadístico que deseen. Cuando veas una nota como esta en el margen, también podrás encontrar este conjunto de datos (TermLife) en el sitio web del libro. Código en la Versión en Línea. La mayoría de los cálculos ilustrativos para el libro, escritos alrededor de 2008, se realizaron en SAS. Siguiendo el enfoque moderno de análisis, el código ilustrativo proporcionado en la versión en línea está en el software estadístico R. Por lo tanto, los usuarios deben esperar ver algunas discrepancias entre los resultados en el libro y los que se calculan utilizando el software R. Suplementos Técnicos. Los suplementos técnicos refuerzan y amplían los resultados en el cuerpo principal del texto al proporcionar un tratamiento más formal y matemático del material. Este tratamiento es en realidad un suplemento porque las aplicaciones y ejemplos están descritos en el cuerpo principal del texto. Para los lectores con suficiente fondo matemático, los suplementos proporcionan material adicional que es útil para comunicarse con audiencias técnicas. Los suplementos técnicos ofrecen una cobertura más profunda y amplia de la regresión aplicada. Creo que los analistas deberían tener una idea de ‘lo que ocurre bajo el capó,’ o ‘cómo funciona el motor.’ La mayoría de estos temas se omitirán en la primera lectura del material. Sin embargo, a medida que trabajes con regresión, te enfrentarás a preguntas sobre ‘¿Por qué?’ y necesitarás profundizar en los detalles para ver exactamente cómo funciona una técnica en particular. Además, los suplementos técnicos proporcionan un menú de elementos opcionales que un instructor puede desear cubrir. Cursos Sugeridos. Hay una amplia variedad de temas que pueden incluirse en un curso de regresión. Aquí están algunos cursos sugeridos. El curso que enseño en la Universidad de Wisconsin es el primero en la lista de la siguiente tabla. \\[ {\\small \\begin{array}{ll} \\begin{array}{lll} \\hline \\textbf{Audiencia} &amp; \\textbf{Naturaleza del Curso}&amp; \\textbf{Capítulos Sugeridos} \\\\ \\hline \\text{Fondo de un año en} &amp; \\text{Introducción a la regresión y} &amp; \\text{Capítulos } 1-8 ,11-13, 20-21,\\\\ ~~~\\text{probabilidad y estadística} &amp; ~~~\\text{modelos de series temporales} &amp; ~~~\\text{solo cuerpo principal del texto} \\\\ \\text{Fondo de un año en} &amp; \\text{Regresión y modelos de} &amp; \\text{ Capítulos } 1-8, 20-21, \\text{seleccionados} \\\\ ~~~\\text{probabilidad y estadística} &amp; ~~~\\text{series temporales} &amp; ~~~\\text{porciones de los suplementos técnicos} \\\\ \\text{Fondo de un año en} &amp; \\text{Modelado de regresión} &amp; \\text{Capítulos } 1-6, 11-13, 20-21, \\text{seleccionados} \\\\ ~~~\\text{probabilidad y estadística} &amp; &amp; ~~~ \\text{porciones de los suplementos técnicos} \\\\ \\text{Fondo en estadísticas} &amp; \\text{Regresión actuarial} &amp; \\text{ Capítulos } 10-21, \\text{seleccionados} \\\\ ~~~\\text{y regresión lineal} &amp; ~~~\\text{modelos} &amp; ~~~ \\text{porciones de los suplementos técnicos} \\\\ \\hline \\end{array} \\end{array} } \\] Además de estos cursos sugeridos, este libro está diseñado para lectura complementaria para un curso de series temporales, así como un libro de referencia para analistas de la industria. Mi esperanza es que los estudiantes universitarios que utilicen las primeras partes del libro en su curso universitario encuentren los capítulos posteriores útiles en sus posiciones en la industria. De esta manera, espero promover el aprendizaje continuo a lo largo de la vida. Agradecimientos Es apropiado comenzar la sección de agradecimientos agradeciendo a los estudiantes del programa actuarial aquí en la Universidad de Wisconsin; los estudiantes son socios importantes en el negocio de la creación y difusión del conocimiento en las universidades. A través de sus preguntas y comentarios, he aprendido una cantidad tremenda a lo largo de los años. También he beneficiado de la excelente asistencia de quienes me han ayudado a reunir todas las piezas para este libro, específicamente, Missy Pinney, Peng Shi, Yunjie (Winnie) Sun y Ziyan Xie. He disfrutado trabajando con varios antiguos estudiantes y colegas en problemas de regresión en los últimos años, incluyendo a Katrien Antonio, Jie Gao, Paul Johnson, Margie Rosenberg, Jiafeng Sun, Emil Valdez y Ping Wang. Sus contribuciones están reflejadas indirectamente a lo largo del texto. Debido a mi larga asociación con la Universidad de Wisconsin-Madison, soy reacio a retroceder más en el tiempo y proporcionar una lista más extensa por temor a olvidar personas importantes. También he tenido la suerte de tener una asociación más reciente con el Insurance Services Office (ISO). Los colegas en ISO me han proporcionado importantes perspectivas sobre aplicaciones. A través de este texto que presenta aplicaciones de regresión en problemas actuariales y de la industria financiera, espero fomentar asociaciones adicionales entre la academia y la industria. Estoy encantado de reconocer las revisiones detalladas que he recibido de los colegas Tim Welnetz y Margie Rosenberg. También deseo agradecer a Bob Miller por permitirme incluir nuestro trabajo conjunto sobre diseño de gráficos efectivos en el Capítulo 21. Bob me ha enseñado mucho sobre regresión a lo largo de los años. Además, me alegra reconocer el apoyo financiero a través del Assurant Health Professorship en Ciencias Actuariales en la Universidad de Wisconsin-Madison. Guardando lo más importante para el final, agradezco a mi familia por su apoyo. Mil gracias a mi madre Mary, hermanos Randy, Guy y Joe, mi esposa Deirdre y nuestros hijos Nathan y Adam. Dedicación Hay un viejo dicho, atribuido a Sir Isaac Newton y que se puede encontrar en Google Scholar, Si he visto más lejos, es porque me he subido a los hombros de gigantes. Dedico este libro a la memoria de dos gigantes que me ayudaron, y a todos los que los conocieron, a ver más lejos y vivir mejor: James C. Hickman y Joseph P. Sullivan. "],["traducción.html", "Traducción", " Traducción Under Construction!!! What? The purpose of this project is to develop a Spanish translation for the text Regression Modeling with Actuarial and Financial Application. This translation will be open and freely available - a resource for our community. Why? Regression provides the foundations for data science techniques such as machine learning. Although this book is a bit dated (written in 2009), it provides an easy introduction to statistical learning tools geared for applications of interest to actuaries and other financial analysts. Who? Jed Frees jfrees@bus.wisc.edu is the author of the text and has secured permission from Cambridge University Press to publish an online Spanish version. Jed is learning Spanish and is familiar with online publishing. He will take responsibility for uploading the translated version to the web. Carla Parodi and Armando Zarruk (Chair) of the Society of Actuaries´s Latin America Committee will be coordinating translation volunteers. It has been already confirmed the collaboration from the different Associations of Actuaries of Latam countries, including those from Argentina, Colombia, Ecuador, Mexico, Panama, and Peru. If you are interested in participating, please contact carla.parodi@carlaparodi.com and/or azarrukr@unal.edu.co . How? We are following a procedure similar to our successful Spanish version of Loss Data Analytics, available at https://openacttexts.github.io/LDASpanish/. Check out our Google Usage Data to see who is using this book. In short, Jed has converted his text, written in 2009 using latex, to R markdown. He then used ChatGPT for the initial translation into Spanish. Volunteers will review this translation. If the number of suggested changes are small, probably these easiest thing is to send them Carla and Armando. They will review for consistency among terminology, then forward on to Jed who will upload the changes to Github. If major changes are required, another possibility is to go to the Github site, download the .Rmd file, make the changes in the file, and send on to Jed. An “.Rmd” file is based on R’s version of a markdown file. You can open it with any text editor (e.g. Notepad - not Word), make changes, and save as text file. Date: 10 July 2025 "],["regresión-y-la-distribución-normal.html", "Capítulo 1 Regresión y la Distribución Normal 1.1 ¿Qué es el Análisis de Regresión? 1.2 Ajuste de Datos a una Distribución Normal 1.3 Transformaciones de Potencia 1.4 Muestreo y el Papel de la Normalidad 1.5 Regresión y Diseños de Muestreo 1.6 Aplicaciones Actuariales de la Regresión 1.7 Lecturas Adicionales y Referencias 1.8 Ejercicios 1.9 Suplemento Técnico - Teorema del Límite Central", " Capítulo 1 Regresión y la Distribución Normal Vista Previa del Capítulo. El análisis de regresión es un método estadístico que se utiliza ampliamente en muchos campos de estudio, y la ciencia actuarial no es una excepción. Este capítulo proporciona una introducción al papel de la distribución normal en la regresión, el uso de transformaciones logarítmicas para especificar relaciones de regresión y la base de muestreo que es crítica para inferir resultados de regresión a poblaciones amplias de interés. 1.1 ¿Qué es el Análisis de Regresión? La estadística trata sobre datos. Como disciplina, se ocupa de la recolección, resumen y análisis de datos para hacer afirmaciones sobre el mundo real. Cuando los analistas recolectan datos, realmente están recolectando información que se cuantifica, es decir, se transforma a una escala numérica. Existen reglas fáciles y bien entendidas para reducir los datos, utilizando medidas de resumen numéricas o gráficas. Estas medidas de resumen pueden luego vincularse a una representación teórica, o modelo, de los datos. Con un modelo que se calibra con datos, se pueden hacer afirmaciones sobre el mundo. Los métodos estadísticos han tenido un gran impacto en varios campos de estudio. En el área de recolección de datos, el diseño cuidadoso de encuestas por muestreo es crucial para los grupos de investigación de mercado y para los procedimientos de auditoría de las firmas de contabilidad. El diseño experimental es una segunda subdisciplina dedicada a la recolección de datos. El enfoque del diseño experimental es construir métodos de recolección de datos que extraigan información de la manera más eficiente posible. Esto es especialmente importante en campos como la agricultura y la ingeniería, donde cada observación es costosa, posiblemente costando millones de dólares. Otros métodos estadísticos aplicados se centran en la gestión y predicción de datos. El control de procesos se ocupa de monitorear un proceso a lo largo del tiempo y decidir cuándo la intervención es más fructífera. El control de procesos ayuda a gestionar la calidad de los bienes producidos por los fabricantes. La previsión se trata de extrapolar un proceso hacia el futuro, ya sea las ventas de un producto o los movimientos de una tasa de interés. El análisis de regresión es un método estadístico utilizado para analizar datos. Como veremos, la característica distintiva de este método es la capacidad de hacer afirmaciones sobre variables después de haber controlado los valores de variables explicativas conocidas. Aunque otros métodos son importantes, el análisis de regresión ha sido el más influyente. Para ilustrar, un índice de revistas de negocios, ABI/INFORM, enumera más de veinticuatro mil artículos que utilizan técnicas de regresión en el período de treinta años de 1978-2007. ¡Y estas son solo las aplicaciones que se consideraron lo suficientemente innovadoras como para ser publicadas en revistas académicas! El análisis de regresión de datos es tan omnipresente en los negocios modernos que es fácil pasar por alto el hecho de que la metodología tiene poco más de 120 años. Los estudiosos atribuyen el nacimiento de la regresión al discurso presidencial de 1885 de Sir Francis Galton en la sección antropológica de la Asociación Británica para el Avance de las Ciencias. En ese discurso, descrito en Stigler (1986), Galton proporcionó una descripción de la regresión y la vinculó a la teoría de la curva normal. Su descubrimiento surgió de sus estudios sobre las propiedades de la selección natural y la herencia. Para ilustrar un conjunto de datos que se puede analizar utilizando métodos de regresión, la Tabla 1.1 muestra algunos datos incluidos en el artículo de Galton de 1885. Esta tabla muestra las alturas de 928 hijos adultos, clasificados por un índice de la altura de sus padres. Aquí, todas las alturas femeninas se multiplicaron por 1.08, y el índice se creó tomando el promedio de la altura del padre y la altura reescalada de la madre. Galton era consciente de que tanto la altura de los padres como la del hijo adulto podían ser adecuadamente aproximadas por una curva normal. Al desarrollar el análisis de regresión, proporcionó un único modelo para la distribución conjunta de alturas. Tabla 1.1: Galtons 1885 Datos de Regresión &lt;64.0 64.5 65.5 66.5 67.5 68.5 69.5 70.5 71.5 72.5 &gt;73.0 Total &gt;73.7 0 0 0 0 0 0 5 3 2 4 0 14 73.2 0 0 0 0 0 3 4 3 2 2 3 17 72.2 0 0 1 0 4 4 11 4 9 7 1 41 71.2 0 0 2 0 11 18 20 7 4 2 0 64 70.2 0 0 5 4 19 21 25 14 10 1 0 99 69.2 1 2 7 13 38 48 33 18 5 2 0 167 68.2 1 0 7 14 28 34 20 12 3 1 0 120 67.2 2 5 11 17 38 31 27 3 4 0 0 138 66.2 2 5 11 17 36 25 17 1 3 0 0 117 65.2 1 1 7 2 15 16 4 1 1 0 0 48 64.2 4 4 5 5 14 11 16 0 0 0 0 59 63.2 2 4 9 3 5 7 1 1 0 0 0 32 62.2 0 1 0 3 3 0 0 0 0 0 0 7 &lt;61.2 1 1 1 0 0 1 0 1 0 0 0 5 Total 14 23 66 78 211 219 183 68 43 19 4 928 Fuente: Stigler (1986) La Tabla 1.1 muestra que gran parte de la información sobre la altura de un hijo adulto puede atribuirse o ‘explicarse’ en términos de la altura de los padres. Por lo tanto, utilizamos el término variable explicativa para las mediciones que proporcionan información sobre una variable de interés. El análisis de regresión es un método para cuantificar la relación entre una variable de interés y las variables explicativas. La metodología utilizada para estudiar los datos en la Tabla 1.1 también se puede utilizar para estudiar problemas actuariales y de gestión de riesgos, la tesis de este libro. 1.2 Ajuste de Datos a una Distribución Normal Históricamente, la distribución normal tuvo un papel fundamental en el desarrollo del análisis de regresión. Continúa desempeñando un papel importante, aunque estaremos interesados en extender las ideas de regresión a datos altamente ‘no normales’. Formalmente, la curva normal se define por la función \\[\\begin{equation} \\mathrm{f}(y)=\\frac{1}{\\sigma \\sqrt{2\\pi }}\\exp \\left( -\\frac{1}{2\\sigma ^{2} }\\left( y-\\mu \\right) ^{2}\\right) . \\tag{1.1} \\end{equation}\\] Esta curva es una función de densidad de probabilidad con toda la línea real como su dominio. De la ecuación (1.1), vemos que la curva es simétrica respecto a \\(\\mu\\) (la media y la mediana). El grado de agudeza está controlado por el parámetro \\(\\sigma ^{2}\\). Estos dos parámetros, \\(\\mu\\) y \\(\\sigma ^{2}\\), son conocidos como los parámetros de ubicación y escala, respectivamente. El Apéndice A3.1 proporciona detalles adicionales sobre esta curva, incluyendo un gráfico y tablas de su distribución acumulada que utilizaremos a lo largo del texto. La curva normal también se muestra en la Figura 1.1, una imagen de un billete de moneda alemana ahora fuera de circulación, el diez Deutsche Mark. Este billete contiene la imagen del alemán Carl Gauss, un eminente matemático cuyo nombre a menudo se asocia con la curva normal (a veces se refiere a ella como la curva Gaussiana). Gauss desarrolló la curva normal en relación con la teoría de los mínimos cuadrados para ajustar curvas a los datos en 1809, aproximadamente al mismo tiempo que el trabajo relacionado del científico francés Pierre LaPlace. Según Stigler (1986), ¡hubo bastante acritud entre estos dos científicos sobre la prioridad del descubrimiento! La curva normal se utilizó por primera vez como una aproximación a los histogramas de datos alrededor de 1835 por Adolph Quetelet, un matemático y científico social belga. Como muchas cosas buenas, la curva normal ha existido durante algún tiempo, desde aproximadamente 1720 cuando Abraham de Moivre la derivó para su trabajo sobre modelado de juegos de azar. La curva normal es popular porque es fácil de usar y ha demostrado ser exitosa en muchas aplicaciones. Figura 1.1: Diez Deutsche Mark. Moneda alemana con el científico Gauss y la curva normal. Ejemplo: Reclamaciones por Lesiones Corporales en Massachusetts. Para nuestra primera mirada al ajuste de la curva normal a un conjunto de datos, consideramos los datos de Rempala y Derrig (2005). Ellos consideraron las reclamaciones derivadas de coberturas de seguros de lesiones corporales por accidentes de automóvil. Estos son montos incurridos por tratamientos médicos ambulatorios que surgen de accidentes de automóvil, típicamente esguinces, fracturas de clavícula y similares. Los datos consisten en una muestra de 272 reclamaciones de Massachusetts que se cerraron en 2001 (por ‘cerradas,’ queremos decir que la reclamación está resuelta y no pueden surgir responsabilidades adicionales del mismo accidente). Rempala y Derrig estaban interesados en desarrollar procedimientos para manejar mezclas de reclamaciones ‘típicas’ y otras de proveedores que informaron reclamaciones fraudulentas. Para esta muestra, consideramos solo esas reclamaciones típicas, ignorando las potencialmente fraudulentas. La Tabla 1.2 proporciona varias estadísticas que resumen diferentes aspectos de la distribución. Los montos de las reclamaciones están en unidades de logaritmos de miles de dólares. La reclamación logarítmica promedio es 0.481, lo que corresponde a $1,617.77 (=1000 \\(\\exp(0.481)\\)). Las reclamaciones más pequeñas y más grandes son -3.101 (45 dólares) y 3.912 (50,000 dólares), respectivamente. Tabla 1.2: Estadísticas Resumen de Reclamaciones por Lesiones Corporales en Automóviles de Massachusetts Número Media Mediana Desviación Estándar Mínimo Máximo Percentil 25 Percentil 75 Reclamaciones 272 0.481 0.793 1.101 -3.101 3.912 -0.114 1.168 Para completar, aquí hay algunas definiciones. La muestra es el conjunto de datos disponibles para el análisis, denotado por \\(y_1,\\ldots,y_n\\). Aquí, \\(n\\) es el número de observaciones, \\(y_1\\) representa la primera observación, \\(y_2\\) la segunda, y así sucesivamente hasta \\(y_n\\) para la \\(n\\)-ésima observación. Aquí hay algunas estadísticas de resumen importantes. Estadísticas Básicas de Resumen La media es el promedio de las observaciones, es decir, la suma de las observaciones dividida por el número de unidades. Usando notación algebraica, la media es \\[ \\overline{y}=\\frac{1}{n}\\left( y_1 + \\cdots + y_n \\right) = \\frac{1}{n} \\sum_{i=1}^{n} y_i. \\] La mediana es la observación central cuando las observaciones están ordenadas por tamaño. Es decir, es la observación en la que el 50% está por debajo de ella (y el 50% está por encima de ella). La desviación estándar es una medida de la dispersión, o escala, de la distribución. Se calcula como \\[ s_y = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^{2}} . \\] Un percentil es un número en el que una fracción específica de las observaciones está por debajo de él, cuando las observaciones están ordenadas por tamaño. Por ejemplo, el percentil 25 es aquel número en el que el 25% de las observaciones están por debajo de él. Para ayudar a visualizar la distribución, la Figura 1.2 muestra un histograma de los datos. Aquí, la altura de cada rectángulo muestra la frecuencia relativa de observaciones que caen dentro del rango dado por su base. El histograma proporciona una impresión visual rápida de la distribución; muestra que el rango de los datos es aproximadamente (-4,4), la tendencia central es ligeramente mayor que cero y que la distribución es aproximadamente simétrica. Aproximación de la Curva Normal. La Figura 1.2 también muestra una curva normal superpuesta, utilizando \\(\\overline{y}\\) para \\(\\mu\\) y \\(s_y^{2}\\) para \\(\\sigma ^{2}\\). Con la curva normal, solo se requieren dos cantidades (\\(\\mu\\) y \\(\\sigma ^{2}\\)) para resumir toda la distribución. Por ejemplo, la Tabla 1.2 muestra que 1.168 es el percentil 75, que es aproximadamente la observación número 204 (\\(=0.75\\times 272\\)) más grande de toda la muestra. De la ecuación (1.1) de la distribución normal, tenemos que \\(z=(y-\\mu )/\\sigma\\) es una normal estándar, de la cual 0.675 es el percentil 75. Así, \\(\\overline{y}+0.675s_y=\\) \\(0.481+0.675\\times 1.101=1.224\\) es el percentil 75 usando la aproximación de la curva normal. Figura 1.2: Frecuencia Relativa de Lesiones Corporales con Curva Normal Superpuesta. Código R para Producir la Figura 1.2 injury &lt;- read.csv(&quot;CSVData/MassBodilyInjury.csv&quot;, header=TRUE) injury2&lt;-subset(injury, providerA != 0 ) LOGCLAIMS&lt;-log(injury2$claims) # FIGURA 1.2 x &lt;- seq(-4, 4, 0.01) y &lt;- dnorm(x, mean=mean(LOGCLAIMS), sd=sqrt(var(LOGCLAIMS))) hist(LOGCLAIMS, freq=FALSE, main=&quot;&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Densidad&quot;, side=2, at=.35,las=1, adj=.7,cex=1.4) lines(x,y) Diagrama de Caja. Una inspección visual rápida de la distribución de una variable puede revelar algunas características sorprendentes que están ocultas por estadísticas, medidas de resumen numéricas. El diagrama de caja, también conocido como diagrama de ‘caja y bigotes’, es uno de estos dispositivos gráficos. La Figura 1.3 ilustra un diagrama de caja para las reclamaciones por lesiones corporales. Aquí, la caja captura el 50% central de los datos, con las tres líneas horizontales que corresponden a los percentiles 75, 50 y 25, leyendo de arriba a abajo. Las líneas horizontales por encima y por debajo de la caja son los ‘bigotes’. El bigote superior es 1.5 veces el rango intercuartílico (la diferencia entre los percentiles 75 y 25) por encima del percentil 75. De manera similar, el bigote inferior es 1.5 veces el rango intercuartílico por debajo del percentil 25. Las observaciones individuales fuera de los bigotes se denotan con pequeños símbolos de trazado circulares, y se denominan ‘valores atípicos’. Figura 1.3: Diagrama de Caja de Reclamaciones por Lesiones Corporales. Código R para Producir la Figura 1.3 boxplot(LOGCLAIMS, boxwex=.7, las=1) text(1, .57, &quot;mediana&quot;, cex=1.2) text(1.36, -0.2, &quot;percentil 25&quot;, cex=1.2) text(1.36, 1.1, &quot;percentil 75&quot;, cex=1.2) arrows(1.05, -2, 1.05, -3.3, code=3, angle=20, length=0.1) text(1.35, -2.5, &quot;valores atípicos&quot;, cex=1.2) text(1.13, 3.9, &quot;valor atípico&quot;, cex=1.2) Los gráficos son herramientas poderosas; permiten a los analistas visualizar fácilmente relaciones no lineales que son difíciles de comprender cuando se expresan verbalmente o mediante fórmulas matemáticas. Sin embargo, debido a su gran flexibilidad, los gráficos también pueden engañar fácilmente al analista. El Capítulo 21 subrayará este punto. Por ejemplo, la Figura 1.4 es un re-dibujo de la Figura 1.2; la diferencia es que la Figura 1.4 usa más y más finos rectángulos. Este análisis más detallado revela la naturaleza asimétrica de la distribución de la muestra que no era evidente en la Figura 1.2. Figura 1.4: Re-dibujo de la Figura 1.2 con un número aumentado de rectángulos. Código R para Producir la Figura 1.4 hist(LOGCLAIMS, freq=FALSE, nclass=32, main=&quot;&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Densidad&quot;, side=2, at=.75,las=1, adj=.7,cex=1.1) lines(x,y) Gráficos Cuantiles-Cuantiles. Aumentar el número de rectángulos puede descubrir características que no eran evidentes antes; sin embargo, en general hay menos observaciones por rectángulo, lo que significa que la incertidumbre de la estimación de la frecuencia relativa aumenta. Esto representa un compromiso. En lugar de forzar al analista a tomar una decisión arbitraria sobre el número de rectángulos, una alternativa es usar un dispositivo gráfico para comparar una distribución con otra conocida, llamado gráfico de cuantiles-cuantiles, o qq. La Figura 1.5 ilustra un gráfico \\(qq\\) para los datos de lesiones corporales utilizando la curva normal como distribución de referencia. Para cada punto, el eje vertical da el cuantil usando la distribución de la muestra. El eje horizontal da la cantidad correspondiente usando la curva normal. Por ejemplo, anteriormente consideramos el punto del percentil 75. Este punto aparece como (1.168, 0.675) en el gráfico. Para interpretar un gráfico \\(qq\\), si los puntos de los cuantiles se alinean a lo largo de la línea superpuesta, entonces la muestra y la distribución de referencia normal tienen la misma forma. (Esta línea se define conectando los percentiles 75 y 25). En la Figura 1.5, los percentiles pequeños de la muestra son consistentemente más pequeños que los valores correspondientes de la normal estándar, lo que indica que la distribución está sesgada a la izquierda. La diferencia en los valores en los extremos de la distribución se debe a los valores atípicos mencionados anteriormente que también podrían interpretarse como que la distribución de la muestra tiene colas más grandes que la distribución de referencia normal. Figura 1.5: Un gráfico \\(qq\\) de Reclamaciones por Lesiones Corporales, usando una distribución de referencia normal. Código R para Producir la Figura 1.5 qqnorm(LOGCLAIMS, main=&quot;&quot;, las=1, ylab=&quot;&quot;, xlab = &quot;Cuantiles Teóricos&quot;) mtext(&quot;Cuantiles de la Muestra&quot;, side=2, at=4.8, las=1,cex=1.1,adj=.4) qqline(LOGCLAIMS) 1.3 Transformaciones de Potencia En el ejemplo de la Sección 1.2, consideramos las reclamaciones sin justificar el uso de la escala logarítmica. Al analizar variables como los activos de las empresas, los salarios de los individuos y los precios de las viviendas en aplicaciones empresariales y económicas, es común considerar logaritmos en lugar de las unidades originales. Una transformación logarítmica mantiene el orden original (por ejemplo, los salarios altos siguen siendo altos en la escala de logaritmos de los salarios) pero sirve para ‘acercar’ los valores extremos de la distribución. Para ilustrar, la Figura 1.6 muestra la distribución de las reclamaciones por lesiones corporales en (miles de) dólares. Para graficar los datos de manera significativa, se eliminó la observación más grande ($50,000) antes de hacer este gráfico. Incluso con esta observación eliminada, la Figura 1.6 muestra que la distribución está muy inclinada hacia la derecha, con varios valores grandes de reclamaciones apareciendo. Las distribuciones que están inclinadas en una dirección u otra se conocen como sesgadas. La Figura 1.6 es un ejemplo de una distribución sesgada a la derecha, o sesgada positivamente. Aquí, la cola de la distribución a la derecha es más larga y hay una mayor concentración de masa a la izquierda. En contraste, una distribución sesgada a la izquierda, o sesgada negativamente, tiene una cola más larga a la izquierda y una mayor concentración de masa a la derecha. Muchas distribuciones de reclamaciones de seguros están sesgadas a la derecha (ver el texto de Klugman, Panjer y Willmot, 2008, para discusiones extensas). Como vimos en las Figuras 1.4 y 1.5, una transformación logarítmica produce una distribución que está solo ligeramente sesgada a la izquierda. Figura 1.6: Distribución de Reclamaciones por Lesiones Corporales. Las observaciones están en (miles de) dólares con la observación más grande omitida. Código R para Producir la Figura 1.6 injury3 = subset(injury, claims &lt; 25 ) CLAIMS25 &lt;- injury3$claims par(mar=c(4.2,4,1.2,.2),cex=1.1) hist(CLAIMS25, freq=FALSE, main=&quot;&quot;, las=1, ylab=&quot;&quot;, xlab=&quot;CLAIMS&quot;) mtext(&quot;Densidad&quot;, side=2, at=.28, las=1,cex=1.1) Las transformaciones logarítmicas se usan extensamente en el trabajo de estadística aplicada. Una ventaja es que sirven para simetrizar distribuciones que están sesgadas. Más generalmente, consideramos transformaciones de potencia, también conocidas como la familia de transformaciones de Box-Cox. Dentro de esta familia de transformaciones, en lugar de usar la respuesta \\(y\\), usamos una versión transformada o reescalada, \\(y^{\\lambda}\\). Aquí, la potencia \\(\\lambda\\) (lambda, una ‘l’ griega) es un número que puede ser especificado por el usuario. Los valores típicos de \\(\\lambda\\) que se usan en la práctica son \\(\\lambda\\)=1, 1/2, 0 o -1. Cuando usamos \\(\\lambda =0\\), queremos decir \\(\\ln (y)\\), es decir, la transformación logarítmica natural. Más formalmente, la familia de Box-Cox puede expresarse como \\[ y^{(\\lambda )}=\\left\\{ \\begin{array}{ll} \\frac{y^{\\lambda }-1}{\\lambda } &amp; \\lambda \\neq 0 \\\\ \\ln (y) &amp; \\lambda =0 \\end{array} \\right. . \\] Como veremos, porque las estimaciones de regresión no se ven afectadas por desplazamientos de ubicación y escala, en la práctica no necesitamos restar uno ni dividir por \\(\\lambda\\) al reescalar la respuesta. La ventaja de la expresión anterior es que, si dejamos que \\(\\lambda\\) se acerque a 0, entonces \\(y^{(\\lambda )}\\) se acerca a \\(\\ln (y)\\), a partir de algunos argumentos de cálculo sencillos. Para ilustrar la utilidad de las transformaciones, simulamos 500 observaciones de una distribución chi-cuadrado con dos grados de libertad. El Apéndice A3.2 introduce esta distribución (que encontraremos nuevamente más adelante al estudiar el comportamiento de los estadísticos de prueba). El panel superior izquierdo de la Figura 1.7 muestra que la distribución original está muy sesgada hacia la derecha. Los otros paneles en la Figura 1.7 muestran los datos reescalados utilizando las transformaciones de raíz cuadrada, logarítmica y recíproca negativa. La transformación logarítmica, en el panel inferior izquierdo, proporciona la mejor aproximación a la simetría para este ejemplo. La transformación recíproca negativa se basa en \\(\\lambda =-1\\), y luego multiplicando las observaciones reescaladas por menos uno, de modo que las observaciones grandes sigan siendo grandes. Figura 1.7: 500 observaciones simuladas de una distribución chi-cuadrado. El panel superior izquierdo se basa en la distribución original. El superior derecho corresponde a la transformación de raíz cuadrada, el inferior izquierdo a la transformación logarítmica y el inferior derecho a la transformación recíproca negativa. Código R para Producir la Figura 1.7 set.seed(1237) X1 &lt;- 10000*rchisq(500*1, df=2) X2 &lt;- sqrt(X1) X3 &lt;- log(X1) X4 &lt;- -1/X1 par(mfrow=c(2, 2), cex=.75, mar=c(3,5,1.5,0)) hist(X1, freq=FALSE, nclass=16, main=&quot;&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, las=1, yaxt=&quot;n&quot;,xlim=c(0,200000),ylim=c(0,.00005)) axis(2, at=seq(0,.00005,.00001),las=1, cex=.3, labels=c(&quot;0&quot;, &quot;0.00001&quot;, &quot;0.00002&quot;,&quot;0.00003&quot;, &quot;0.00004&quot;, &quot;0.00005&quot;)) mtext(&quot;Densidad&quot;, side=2, at=.000055, las=1, cex=.75) mtext(&quot;y&quot;, side=1, cex=.75, line=2) par(mar=c(3,4,1.5,0.2)) hist(X2, freq=FALSE, nclass=16, main=&quot;&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, las=1,xlim=c(0,400), ylim=c(0,.008)) mtext(&quot;Densidad&quot;, side=2, at=.0088, las=1, cex=.75) mtext(&quot;Raíz cuadrada de y&quot;, side=1, cex=.75, line=2) par(mar=c(3.2,5,1,0)) hist(X3, freq=FALSE, nclass=16, main=&quot;&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, las=1, ylim=c(0,.4)) mtext(&quot;Densidad&quot;, side=2, at=.44, las=1, cex=.75) mtext(&quot;Logaritmo de y&quot;, side=1, cex=.75, line=2) par(mar=c(3.2,4,1,0.2)) hist(X4, freq=FALSE, nclass=16, main=&quot;&quot;,xlab=&quot;&quot;, ylab=&quot;&quot;, las=1, ylim=c(0,100)) mtext(&quot;Densidad&quot;, side=2, at=110, las=1, cex=.75) mtext(&quot;Recíproco negativo de y&quot;, side=1, cex=.75, line=2) 1.4 Muestreo y el Papel de la Normalidad Una estadística es una medida resumen de los datos, como una media, mediana o percentil. Las colecciones de estadísticas son muy útiles para analistas, tomadores de decisiones y consumidores cotidianos para comprender grandes cantidades de datos que representan situaciones complejas. Hasta este punto, nuestro enfoque ha sido introducir técnicas sensatas para resumir variables; técnicas que se usarán repetidamente a lo largo de este texto. Sin embargo, la verdadera utilidad de la disciplina de la estadística es su capacidad para decir algo sobre lo desconocido, no solo para resumir la información ya disponible. Con este fin, necesitamos hacer algunas suposiciones bastante formales sobre la manera en que se observan los datos. Como ciencia, una característica destacada de la estadística (como disciplina) es la capacidad de criticar estas suposiciones y ofrecer alternativas mejoradas en situaciones específicas. Es costumbre suponer que los datos se extraen de una población más grande que estamos interesados en describir. El proceso de extracción de los datos se conoce como muestreo, o proceso generador de datos. Denotamos esta muestra como \\(\\{y_1,\\ldots,y_n\\}\\). Para que podamos criticar y modificar estas suposiciones de muestreo, las enumeramos a continuación en detalle: \\[ \\begin{array}{l} \\hline \\textbf{Suposiciones Básicas de Muestreo} \\\\ \\hline 1. ~\\mathrm{E~}y_i=\\mu \\\\ 2. ~\\mathrm{Var~}y_i=\\sigma ^{2} \\\\ 3. ~\\{y_i\\} \\text{ son independientes} \\\\ 4. ~\\{y_i\\} \\text{ están distribuidos normalmente}. \\\\ \\hline \\end{array} \\] En esta configuración básica, \\(\\mu\\) y \\(\\sigma ^{2}\\) sirven como parámetros que describen la ubicación y escala de la población de origen. El objetivo es inferir algo sensato sobre ellos basándose en estadísticas como \\(\\overline{y}\\) y \\(s_y^{2}\\). Para la tercera suposición, asumimos independencia entre las extracciones. En un esquema de muestreo, esto puede ser garantizado tomando una muestra aleatoria simple de una población. La cuarta suposición no es necesaria para muchos procedimientos de inferencia estadística porque los teoremas del límite central proporcionan una normalidad aproximada para muchas estadísticas de interés. Sin embargo, una justificación formal de algunas estadísticas, como las t-estadísticas, requiere esta suposición adicional. La Sección 1.9 proporciona una declaración explícita de una versión del teorema del límite central, dando condiciones bajo las cuales \\(\\overline{y}\\) está aproximadamente distribuido normalmente. Esta sección también discute un resultado relacionado, conocido como aproximación de Edgeworth, que muestra que la calidad de la aproximación normal es mejor para poblaciones de origen simétricas en comparación con distribuciones sesgadas. ¿Cómo se aplica esta discusión al estudio del análisis de regresión? Después de todo, hasta ahora nos hemos centrado solo en el promedio aritmético simple, \\(\\overline{y}\\). En capítulos posteriores, enfatizaremos que la regresión lineal es el estudio de promedios ponderados; específicamente, muchos coeficientes de regresión pueden expresarse como promedios ponderados con pesos apropiadamente elegidos. Los teoremas de límite central y aproximación de Edgeworth están disponibles para promedios ponderados; estos resultados asegurarán la normalidad aproximada de los coeficientes de regresión. Para usar aproximaciones de la curva normal en un contexto de regresión, a menudo transformaremos variables para lograr una simetría aproximada. 1.5 Regresión y Diseños de Muestreo La aproximación a la normalidad será un tema importante en las aplicaciones prácticas de la regresión lineal. Las Partes I y II de este libro se centran en la regresión lineal, donde aprenderemos conceptos básicos de regresión y diseño de muestreo. La Parte III se centrará en la regresión no lineal, que involucra respuestas binarias, de conteo y de colas pesadas, donde la normalidad no es la distribución de referencia más útil. Las ideas sobre conceptos básicos y diseño también se usarán en el contexto no lineal. En el análisis de regresión, nos enfocamos en una medición de interés y la llamamos variable dependiente. Otras mediciones se usan como variables explicativas. Un objetivo es comparar las diferencias en la variable dependiente en términos de diferencias en las variables explicativas. Como se mencionó en la Sección 1.1, la regresión se usa extensamente en muchos campos científicos. Tabla 1.3 enumera términos alternativos que puedes encontrar al leer aplicaciones de regresión. Tabla 1.3. Terminología para Variables de Regresión \\[ {\\small \\begin{array}{ll}\\hline\\hline y-\\text{Variable} &amp; x-\\text{Variable} \\\\\\hline \\text{Resultado de interés} &amp; \\text{Variable explicativa} \\\\ \\text{Variable dependiente} &amp; \\text{Variable independiente} \\\\ \\text{Variable endógena} &amp; \\text{Variable exógena} \\\\ \\text{Respuesta} &amp; \\text{Tratamiento} \\\\ \\text{Regresando} &amp; \\text{Regresor} \\\\ \\text{Variable del lado izquierdo} &amp; \\text{Variable del lado derecho} \\\\ \\text{Variable explicada} &amp; \\text{Variable predictora} \\\\ \\text{Resultado} &amp; \\text{Entrada} \\\\ \\hline \\end{array} } \\] En la última parte del siglo XIX y principios del siglo XX, la estadística comenzó a tener un impacto importante en el desarrollo de la ciencia experimental. Las ciencias experimentales a menudo utilizan estudios diseñados, donde los datos están bajo el control de un analista. Los estudios diseñados se realizan en entornos de laboratorio, donde hay restricciones físicas estrictas en cada variable que un investigador considera importante. Los estudios diseñados también ocurren en experimentos de campo más grandes, donde los mecanismos de control son diferentes a los de los entornos de laboratorio. La agricultura y la medicina utilizan estudios diseñados. Los datos de un estudio diseñado se dicen que son datos experimentales. Para ilustrar, un ejemplo clásico es considerar el rendimiento de un cultivo como el maíz, donde cada uno de varios parcelas de tierra (las observaciones) se asigna a varios niveles de fertilizante. El objetivo es determinar el efecto del fertilizante (la variable explicativa) en el rendimiento del maíz (la variable de respuesta). Aunque los investigadores intentan hacer que las parcelas de tierra sean lo más similares posible, inevitablemente surgen diferencias. Los investigadores agrícolas utilizan técnicas de aleatorización para asignar diferentes niveles de fertilizante a cada parcela de tierra. De esta manera, los analistas pueden explicar la variación en los rendimientos de maíz en términos de la variación de los niveles de fertilizante. A través del uso de técnicas de aleatorización, los investigadores que utilizan estudios diseñados pueden inferir que el tratamiento tiene un efecto causal sobre la respuesta. El Capítulo 6 discute la causalidad más a fondo. Ejemplo: Experimento de Seguro de Salud Rand. ¿Cómo están relacionados los gastos en atención médica con la demanda de seguro? Muchos estudios han establecido una relación positiva entre la cantidad gastada en atención médica y la demanda de seguro de salud. Aquellos en mala salud anticipan usar más servicios médicos que las personas en buena o regular salud y buscarán niveles más altos de seguro de salud para compensar estos gastos anticipados. Obtienen este seguro adicional al (i) seleccionar un plan de seguro de salud más generoso de un empleador, (ii) elegir un empleador con un plan de seguro de salud más generoso o (iii) pagar más por un seguro de salud individual. Así, es difícil desenredar la relación causa-efecto de los gastos en atención médica y la disponibilidad de seguro de salud. Un estudio reportado por Manning et al. (1987) buscó responder a esta pregunta utilizando un experimento cuidadosamente diseñado. En este estudio, los hogares inscritos de seis ciudades, entre noviembre de 1974 y febrero de 1977, fueron asignados aleatoriamente a uno de 14 planes de seguro diferentes. Estos planes variaban según los elementos de participación en los costos, la tasa de coaseguro (el porcentaje pagado de los gastos de bolsillo que variaba entre 0, 25, 50 y 95%) así como el deducible (5, 10 o 15 por ciento del ingreso familiar, hasta un máximo de $1,000). Así, hubo una asignación aleatoria a los niveles del tratamiento, la cantidad de seguro de salud. El estudio encontró que los planes más favorables resultaron en mayores gastos totales, incluso después de controlar el estado de salud de los participantes. Para la ciencia actuarial y otras ciencias sociales, los estudios diseñados son la excepción más que la regla. Por ejemplo, si queremos estudiar los efectos del tabaquismo en la mortalidad, es muy poco probable que podamos conseguir que los participantes en el estudio acepten ser asignados aleatoriamente a grupos de fumadores/no fumadores durante varios años solo para observar sus patrones de mortalidad. Al igual que en el estudio de Galton de la Sección 1.1, los investigadores en ciencias sociales generalmente trabajan con datos observacionales. Los datos observacionales no están bajo el control del analista. Con datos observacionales, no podemos inferir relaciones causales, pero podemos introducir medidas de asociación. Para ilustrar, en los datos de Galton, es evidente que los padres ‘altos’ tienden a tener hijos ‘altos’ y, a la inversa, los padres ‘bajos’ tienden a tener hijos ‘bajos’. El Capítulo 2 introducirá la correlación y otras medidas de asociación. Sin embargo, no podemos inferir causalidad a partir de los datos. Por ejemplo, puede haber otra variable, como la dieta familiar, que esté relacionada con ambas variables. Una buena dieta en la familia podría estar asociada con alturas altas de los padres y de los hijos adultos, mientras que una dieta pobre limita el crecimiento. Si ese fuera el caso, llamaríamos a la dieta familiar una variable confusora. En experimentos diseñados como el Experimento de Seguro de Salud Rand, podemos controlar los efectos de variables como el estado de salud mediante métodos de asignación aleatoria. En estudios observacionales, usamos control estadístico, en lugar de control experimental. Para ilustrar, en los datos de Galton, podríamos dividir nuestras observaciones en dos grupos, uno para ‘buena dieta familiar’ y otro para ‘mala dieta familiar’, y examinar la relación entre la altura de los padres y la de los hijos para cada subgrupo. Esta es la esencia del método de regresión, comparar un \\(y\\) y un \\(x\\), ‘controlando’ los efectos de otras variables explicativas. Por supuesto, para usar control estadístico y métodos de regresión, uno debe registrar la dieta familiar y cualquier otra medida de altura que pueda confundir los efectos de la altura de los padres en la altura de su hijo adulto. La dificultad en diseñar estudios es tratar de imaginar todas las variables que podrían afectar una variable de respuesta, una tarea imposible en la mayoría de los problemas de interés en ciencias sociales. Para dar algunas orientaciones sobre cuándo ‘es suficiente’, el Capítulo 6 discutirá medidas de la importancia de una variable explicativa y su impacto en la selección del modelo. 1.6 Aplicaciones Actuariales de la Regresión Este libro introduce un método estadístico, el análisis de regresión. La introducción está organizada en torno a la tríada tradicional de la inferencia estadística: prueba de hipótesis, estimación y predicción. Además, este libro muestra cómo esta metodología puede ser utilizada en aplicaciones que probablemente serán de interés para los actuarios y otros analistas de riesgos. Como tal, es útil comenzar con las tres áreas tradicionales de aplicaciones actuariales: tarificación, reservas y pruebas de solvencia. Tarificación y selección adversa. El análisis de regresión puede utilizarse para determinar los precios de seguros para muchas líneas de negocio. Por ejemplo, en el seguro de automóviles de pasajeros privados, las reclamaciones esperadas varían según el género del asegurado, la edad, la ubicación (ciudad versus rural), el propósito del vehículo (trabajo o placer) y una serie de otras variables explicativas. La regresión puede utilizarse para identificar las variables que son determinantes importantes de las reclamaciones esperadas. En mercados competitivos, las compañías de seguros no usan el mismo precio para todos los asegurados. Si lo hicieran, los ‘buenos riesgos’, aquellos con reclamaciones esperadas inferiores a la media, pagarían de más y abandonarían la compañía. En contraste, los ‘malos riesgos’, aquellos con reclamaciones esperadas superiores a la media, permanecerían con la compañía. Si la compañía continuara con esta política de precios plana, las primas aumentarían (para compensar las reclamaciones de la creciente proporción de malos riesgos) y la participación en el mercado disminuiría a medida que la compañía pierde buenos riesgos. Este problema se conoce como ‘selección adversa’. Usando un conjunto apropiado de variables explicativas, se pueden desarrollar sistemas de clasificación para que cada asegurado pague su parte justa. Reservas y pruebas de solvencia. Tanto la constitución de reservas como las pruebas de solvencia se preocupan por predecir si las obligaciones asociadas con un grupo de pólizas excederán el capital destinado a cumplir con las obligaciones derivadas de las pólizas. La constitución de reservas implica determinar la cantidad apropiada de capital para cumplir con estas obligaciones. Las pruebas de solvencia se centran en evaluar la adecuación del capital para financiar las obligaciones de un bloque de negocio. En algunas áreas de práctica, la regresión puede usarse para pronosticar futuras obligaciones y ayudar a determinar las reservas (ver, por ejemplo, el Capítulo 19). La regresión también puede utilizarse para comparar las características de empresas saludables y financieramente angustiadas para pruebas de solvencia (ver, por ejemplo, el Capítulo 14). Otras aplicaciones en gestión de riesgos. El análisis de regresión es una herramienta cuantitativa que puede aplicarse en una amplia variedad de problemas de negocio, no solo en las áreas tradicionales de tarificación, reservas y pruebas de solvencia. Al familiarizarse con el análisis de regresión, los actuarios tendrán otra habilidad cuantitativa que puede aplicarse a problemas generales relacionados con la seguridad financiera de personas, empresas y organizaciones gubernamentales. Para ayudarte a desarrollar ideas, este libro proporciona muchos ejemplos de aplicaciones potenciales ‘no actuariales’ a través de viñetas destacadas etiquetadas como ‘ejemplos’ y conjuntos de datos ilustrativos. Para entender las posibles aplicaciones de la regresión, comienza revisando los varios conjuntos de datos presentados en los Ejercicios del Capítulo 1. Incluso si no completas los ejercicios para fortalecer tus habilidades de resumen de datos (que requieren el uso de una computadora), una revisión de las descripciones de los problemas te ayudará a familiarizarte con los tipos de aplicaciones en las que un actuario podría usar técnicas de regresión. 1.7 Lecturas Adicionales y Referencias Este libro introduce herramientas de regresión y series temporales que son más relevantes para los actuarios y otros analistas de riesgos financieros. Afortunadamente, existen otras fuentes que proporcionan excelentes introducciones a estos temas estadísticos (aunque no desde un punto de vista de gestión de riesgos). En particular, para los analistas que desean especializarse en estadística, es útil obtener otra perspectiva. Para regresión, recomiendo Weisburg (2005) y Faraway (2005). Para series temporales, Diebold (2004) es una buena fuente. Además, Klugman, Panjer y Willmot (2008) proporciona una buena introducción a las aplicaciones actuariales de la estadística; este libro está destinado a complementar el libro de Klugman et al. al centrarse en métodos de regresión y series temporales. Referencias del Capítulo Beard, Robert E., Teivo Pentikäinen and Erkki Pesonen (1984). Risk Theory: The Stochastic Basis of Insurance (Third Edition). Chapman &amp; Hall, New York. Diebold, Francis. X. (2004). Elements of Forecasting, Third Edition. Thomson, South-Western, Mason, Ohio. Faraway, Julian J. (2005). Linear Models in R. Chapman &amp; Hall/CRC, New York. Hogg, Robert V. (1972). On statistical education. The American Statistician 26, 8-11. Klugman, Stuart A, Harry H. Panjer and Gordon E. Willmot (2008). Loss Models: From Data to Decisions. John Wiley &amp; Sons, Hoboken, New Jersey. Manning, Willard G., Joseph P. Newhouse, Naihua Duan, Emmett B. Keeler, Arleen Leibowitz and M. Susan Marquis (1987). Health insurance and the demand for medical care: Evidence from a randomized experiment. American Economic Review 77, No. 3, 251-277. Rempala, Grzegorz A. and Richard A. Derrig (2005). Modeling hidden exposures in claim severity via the EM algorithm. North American Actuarial Journal 9, No. 2, 108-128. Singer, Judith D. and Willett, J. B. (1990). Improving the teaching of applied statistics: Putting the data back into data analysis. The American Statistician 44, 223-230. Stigler, Steven M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. The Belknap Press of Harvard University Press, Cambridge, MA. Weisberg, Sanford (2005). Applied Linear Regression, Third Edition. John Wiley &amp; Sons, New York. 1.8 Ejercicios 1.1 Gastos de Salud de MEPS. Este ejercicio considera datos de la Encuesta de Gastos Médicos (MEPS), realizada por la Agencia de Investigación y Calidad en Salud de EE.UU. (AHRQ). MEPS es una encuesta probabilística que proporciona estimaciones nacionalmente representativas del uso de atención médica, gastos, fuentes de pago y cobertura de seguros para la población civil de EE.UU. Esta encuesta recopila información detallada sobre episodios de atención médica de cada tipo de servicio, incluyendo visitas a consultorios médicos, visitas a salas de emergencia hospitalarias, visitas a hospitales ambulatorios, estancias hospitalarias, otras visitas a proveedores médicos y uso de medicamentos prescritos. Esta información detallada permite desarrollar modelos de utilización de atención médica para predecir futuros gastos. Puedes obtener más información sobre MEPS en http://www.meps.ahrq.gov/mepsweb/. Consideramos los datos de MEPS de los paneles 7 y 8 de 2003, que consisten en 18,735 individuos entre las edades de 18 y 65 años. De esta muestra, tomamos una muestra aleatoria de 2,000 individuos que aparecen en el archivo ‘HealthExpend’. De esta muestra, hay 157 individuos que tuvieron gastos hospitalarios positivos. También hay 1,352 que tuvieron gastos ambulatorios positivos. Analizaremos estas dos muestras por separado. Nuestras variables dependientes consisten en las cantidades de gastos para visitas hospitalarias (EXPENDIP) y ambulatorias (EXPENDOP). Para MEPS, los eventos ambulatorios incluyen visitas al departamento ambulatorio del hospital, visitas a proveedores en consultorios y visitas a salas de emergencia, excluyendo servicios dentales. (Los servicios dentales, en comparación con otros tipos de servicios de atención médica, son más predecibles y ocurren de manera más regular). Las estancias hospitalarias con la misma fecha de admisión y alta, conocidas como ‘estancias de cero noches’, se incluyeron en los recuentos y gastos ambulatorios. (Los pagos asociados con visitas a salas de emergencia que precedieron inmediatamente a una estancia hospitalaria se incluyeron en los gastos hospitalarios. Los medicamentos prescritos que se pueden vincular a las admisiones hospitalarias se incluyeron en los gastos hospitalarios, no en la utilización ambulatoria). Parte 1: Usa solo los 157 individuos que tuvieron gastos hospitalarios positivos y realiza el siguiente análisis. Calcula estadísticas descriptivas para los gastos hospitalarios (EXPENDIP). a(i). ¿Cuál es el gasto típico (media y mediana)? a(ii). ¿Cómo se compara la desviación estándar con la media? ¿Los datos parecen estar sesgados? Calcula un diagrama de caja, un histograma y un gráfico \\(qq\\) (normal) para EXPENDIP. Comenta sobre la forma de la distribución. Transformaciones. c(i). Realiza una transformación de raíz cuadrada de los gastos hospitalarios. Resume la distribución resultante usando un histograma y un gráfico \\(qq\\). ¿Parece estar aproximadamente distribuida normalmente? c(ii). Realiza una transformación logarítmica (natural) de los gastos hospitalarios. Resume la distribución resultante usando un histograma y un gráfico \\(qq\\). ¿Parece estar aproximadamente distribuida normalmente? Parte 2: Usa solo los 1,352 individuos que tuvieron gastos ambulatorios positivos. Repite la parte (a) y calcula histogramas para los gastos y los gastos logarítmicos. Comenta sobre la normalidad aproximada de cada histograma. 1.2 Utilización de Hogares de Cuidado de Ancianos. Este ejercicio considera datos de hogares de cuidado de ancianos proporcionados por el Departamento de Salud y Servicios Familiares de Wisconsin (DHFS). El programa Medicaid del Estado de Wisconsin financia el cuidado en hogares de cuidado de ancianos para individuos que califican en base a necesidades y estado financiero. Como parte de las condiciones de participación, los hogares de cuidado de ancianos certificados por Medicaid deben presentar un informe de costos anual al DHFS, resumiendo el volumen y costo del cuidado proporcionado a todos sus residentes, financiados por Medicaid y de otro tipo. Estos informes de costos son auditados por el personal del DHFS y forman la base para las tasas diarias de pago de Medicaid específicas para cada instalación para los períodos subsiguientes. Los datos están disponibles públicamente; consulta [http://dhs.wisconsin.gov] para más información. El DHFS está interesado en técnicas predictivas que proporcionen pronósticos de utilización confiables para actualizar su programa de tasas de financiamiento de Medicaid para instalaciones de cuidado de ancianos. En esta tarea, consideramos los datos en el archivo ‘WiscNursingHome’ en los años de informe de costos 2000 y 2001. Hay 362 instalaciones en 2000 y 355 instalaciones en 2001. Típicamente, la utilización del cuidado en hogares de cuidado de ancianos se mide en días de paciente (‘días de paciente’ es el número de días que cada paciente estuvo en la instalación, sumado sobre todos los pacientes). Para este ejercicio, definimos la variable de resultado como años totales de pacientes (TPY), el número total de días de pacientes en el período de informe de costos dividido por el número de días operativos de la instalación en el período de informe de costos (ver Rosenberg et al., 2007, Apéndice 1, para una discusión adicional sobre esta elección). El número de camas (NUMBED) y el metraje cuadrado (SQRFOOT) del hogar de cuidado de ancianos miden el tamaño de la instalación. No sorprende que estas variables sean importantes predictores de TPY. Parte 1: Usa los datos del año de informe de costos 2000 y realiza el siguiente análisis. Calcula estadísticas descriptivas para TPY, NUMBED y SQRFOOT. Resume la distribución de TPY usando un histograma y un gráfico \\(qq\\). ¿Parece estar aproximadamente distribuida normalmente? Transformaciones. Realiza una transformación logarítmica (natural) de TPY (LOGTPY). Resume la distribución resultante usando un histograma y un gráfico \\(qq\\). ¿Parece estar aproximadamente distribuida normalmente? Parte 2: Usa los datos del año de informe de costos 2001 y repite las partes (a) y (c). 1.3 Reclamaciones de Seguros de Automóviles. Como analista actuarial, estás trabajando con una gran compañía de seguros para ayudarles a entender la distribución de sus reclamaciones para sus pólizas de automóviles particulares. Tienes disponibles datos de reclamaciones para un año reciente, que consisten en: STATE CODE: códigos del 01 al 17 utilizados, con cada código asignado aleatoriamente a un estado real. CLASS: clase de calificación del operador, basada en edad, género, estado civil y uso del vehículo. GENDER: género del operador. AGE: edad del operador. PAID: monto pagado para resolver y cerrar una reclamación. Te estás enfocando en conductores mayores de 50 años, para los cuales hay \\(n = 6,773\\) reclamaciones disponibles. Examina el histograma del monto PAID y comenta sobre la simetría. Crea una nueva variable, las reclamaciones pagadas en logaritmo natural, LNPAID. Crea un histograma y un gráfico \\(qq\\) de LNPAID. Comenta sobre la simetría de esta variable. 1.4 Costos Hospitalarios. Supongamos que eres un actuario de beneficios para empleados trabajando con una empresa de tamaño mediano en Wisconsin. Esta empresa está considerando ofrecer, por primera vez en su industria, cobertura de seguro hospitalario para los hijos dependientes de sus empleados. Tienes acceso a los registros de la empresa y, por lo tanto, tienes disponibles el número, edad y género de los hijos dependientes, pero no tienes otra información sobre los costos hospitalarios de la empresa. En particular, ninguna empresa en esta industria ha ofrecido esta cobertura, por lo que tienes poca experiencia histórica en la industria sobre la cual puedas prever los reclamos esperados. Recopilas datos del Nationwide Inpatient Sample del Healthcare Cost and Utilization Project (NIS-HCUP), una encuesta nacional de costos hospitalarios realizada por la Agencia de Investigación y Calidad en Salud de EE.UU. (AHRQ). Restringes la consideración a hospitales de Wisconsin y analizas una muestra aleatoria de \\(n=500\\) reclamaciones de datos de 2003. Aunque los datos provienen de registros hospitalarios, están organizados por alta individual y por lo tanto tienes información sobre la edad y el género del paciente dado de alta. Específicamente, consideras pacientes de 0 a 17 años. En un proyecto separado, considerarás la frecuencia de hospitalización. Para este proyecto, el objetivo es modelar la gravedad de los cargos hospitalarios, por edad y género. Examina la distribución de la variable dependiente, TOTCHG. Haz esto creando un histograma y luego un gráfico \\(qq\\), comparando el empírico con una distribución normal. Realiza una transformación logarítmica natural y llama a la nueva variable LNTOTCHG. Examina la distribución de esta variable transformada. Para visualizar la relación logarítmica, traza LNTOTCHG frente a TOTCHG. 1.5 Reclamaciones de Seguros de Lesiones Automovilísticas. Consideramos datos de reclamaciones por lesiones en automóviles utilizando datos del Insurance Research Council (IRC), una división del American Institute for Chartered Property Casualty Underwriters y del Insurance Institute of America. Los datos, recopilados en 2002, contienen información sobre la demografía del reclamante, la participación del abogado y la pérdida económica (LOSS, en miles), entre otras variables. Aquí analizamos una muestra de \\(n=1,340\\) pérdidas de un solo estado. El estudio completo de 2002 contiene más de 70,000 reclamaciones cerradas basadas en datos de treinta y dos aseguradoras. El IRC realizó estudios similares en 1977, 1987, 1992 y 1997. Calcula las estadísticas descriptivas para la pérdida económica total (LOSS). ¿Cuál es la pérdida típica? Calcula un histograma y un gráfico \\(qq\\) (normal) para LOSS. Comenta sobre la forma de la distribución. Divide el conjunto de datos en dos submuestras, una correspondiente a las reclamaciones que involucraron a un ATTORNEY (=1) y otra en la que no se involucró un ATTORNEY (=2). c(i). Para cada submuestra, calcula la pérdida típica. ¿Parece haber una diferencia en las pérdidas típicas según la participación del abogado? c(ii) Para comparar las distribuciones, calcula un diagrama de caja (boxplot) por nivel de participación del abogado. c(iii). Para cada submuestra, calcula un histograma y un gráfico \\(qq\\). Compara las dos distribuciones entre sí. 1.6 Gastos de la Compañía de Seguros. Al igual que otros negocios, las compañías de seguros buscan minimizar los gastos asociados con la realización de negocios para mejorar la rentabilidad. Para estudiar los gastos, este ejercicio examina una muestra aleatoria de 500 compañías de seguros de la base de datos de la National Association of Insurance Commissioners (NAIC) de más de 3,000 compañías. La NAIC mantiene una de las bases de datos regulatorias de seguros más grandes del mundo; consideramos aquí datos basados en los informes anuales de 2005 para todas las compañías de seguros de propiedad y casualty en los Estados Unidos. Los informes anuales son estados financieros que utilizan principios contables estatutarios. Específicamente, nuestra variable dependiente es EXPENSES, los gastos no relacionados con reclamaciones para una compañía. Aunque no es necesario para este ejercicio, los gastos no relacionados con reclamaciones se basan en tres componentes: ajuste de pérdidas no asignadas, gastos de suscripción y gastos de inversión. El gasto por ajuste de pérdidas no asignadas es el gasto no directamente atribuible a una reclamación pero que está asociado indirectamente con la resolución de reclamaciones; incluye elementos como los salarios de los ajustadores de reclamaciones, honorarios legales, costos judiciales, testigos expertos y costos de investigación. Los gastos de suscripción consisten en costos de adquisición de pólizas, como comisiones, así como la parte de los gastos administrativos, generales y otros atribuibles a las operaciones de suscripción. Los gastos de inversión son aquellos gastos relacionados con las actividades de inversión del asegurador. Examina la distribución de la variable dependiente, EXPENSES. Haz esto creando un histograma y luego un gráfico \\(qq\\), comparando el empírico con una distribución normal. Realiza una transformación logarítmica natural y examina la distribución de esta variable transformada. ¿Ha ayudado la transformación a simetrizar la distribución? 1.7 Esperanzas de Vida Nacionales. ¿Quién está haciendo bien el cuidado de la salud? Las decisiones sobre la atención médica se toman a nivel individual, corporativo y gubernamental. Prácticamente todas las personas, corporaciones y gobiernos tienen su propia perspectiva sobre la atención médica; estas diferentes perspectivas dan lugar a una amplia variedad de sistemas para gestionar la atención médica. Comparar diferentes sistemas de atención médica nos ayuda a aprender sobre enfoques distintos al nuestro, lo que a su vez nos ayuda a tomar mejores decisiones al diseñar sistemas mejorados. Aquí, consideramos los sistemas de atención médica de \\(n=185\\) países en todo el mundo. Como medida de la calidad de la atención, utilizamos LIFEEXP, la esperanza de vida al nacer. Esta variable dependiente, con varias variables explicativas, se enumeran en [Tabla 1.4]. A partir de esta tabla, notarás que aunque hay 185 países considerados en este estudio, no todos los países proporcionaron información para cada variable. Los datos no disponibles se indican en la columna Num Miss. Los datos provienen del Informe sobre el Desarrollo Humano de las Naciones Unidas (UN). Examina la distribución de la variable dependiente, LIFEEXP. Haz esto creando un histograma y luego un gráfico \\(qq\\), comparando el empírico con una distribución normal. Realiza una transformación logarítmica natural y examina la distribución de esta variable transformada. ¿Ha ayudado la transformación a simetrizar la distribución? \\[ {\\scriptsize \\begin{array}{ll|crrrrr} \\hline &amp; &amp; Num &amp; &amp; &amp; Desv &amp; Mín- &amp; Máx- \\\\ Variable &amp; Descripción &amp; Faltantes &amp; Media &amp; Mediana &amp; Est. &amp; imo &amp; imo \\\\\\hline BIRTH &amp; \\text{ Nacimientos atendidos por personal} &amp; 7 &amp; 78.25 &amp; 92.00 &amp; 26.42 &amp; 6.00 &amp; 100.00 \\\\ ~~ATTEND&amp; ~~ \\text{ sanitario capacitado (%)}\\\\ FEMALE &amp; \\text{ Legisladoras, altas funcionarias} &amp; 87 &amp; 29.07 &amp; 30.00 &amp; 11.71 &amp; 2.00 &amp; 58.00 \\\\ ~~BOSS&amp; ~~ \\text{ y gerentes, % mujeres }\\\\ FERTILITY &amp; \\text{ Tasa de fertilidad total,}&amp; 4 &amp; 3.19 &amp; 2.70 &amp; 1.71 &amp; 0.90 &amp; 7.50 \\\\ &amp; ~~ \\text{ nacimientos por mujer }&amp; \\\\ GDP &amp; \\text{ Producto interno bruto,} &amp; 7 &amp; 247.55 &amp; 14.20 &amp; 1,055.69 &amp; 0.10 &amp; 12,416.50 \\\\ &amp; ~~\\text{ en miles de millones de USD} \\\\ HEALTH&amp; \\text{ Gasto en salud 2004} &amp; 5 &amp; 718.01 &amp; 297.50 &amp; 1,037.01 &amp; 15.00 &amp; 6,096.00 \\\\ ~~ EXPEND &amp; ~~ \\text{ per cápita, PPA en USD} \\\\ ILLITERATE &amp; \\text{ Tasa de analfabetismo adulto,} &amp; 14 &amp; 17.69 &amp; 10.10 &amp; 19.86 &amp; 0.20 &amp; 76.40 \\\\ &amp; ~~ \\% \\text{ personas de 15 años y más} &amp; \\\\ PHYSICIAN &amp; \\text{ Médicos,}&amp; 3 &amp; 146.08 &amp; 107.50 &amp; 138.55 &amp; 2.00 &amp; 591.00 \\\\ &amp; ~~ \\text{ por cada 100,000 habitantes} \\\\ POP &amp; \\text{ Población en 2005,} &amp; 1 &amp; 35.36 &amp; 7.80 &amp; 131.70 &amp; 0.10 &amp; 1,313.00 \\\\ &amp; ~~\\text{ en millones }\\\\ PRIVATE &amp; \\text{ Gasto privado en salud 2004} &amp; 1 &amp; 2.52 &amp; 2.40 &amp; 1.33 &amp; 0.30 &amp; 8.50 \\\\ ~~HEALTH&amp; ~~\\text{% del PIB} \\\\ PUBLIC &amp; \\text{ Gasto público} &amp; 28 &amp; 4.69 &amp; 4.60 &amp; 2.05 &amp; 0.60 &amp; 13.40 \\\\ ~~EDUCATION&amp; ~~ \\text{ en educación, % del PIB} \\\\ RESEAR &amp; \\text{ Investigadores en I + D,} &amp; 95 &amp; 2,034.66 &amp; 848.00 &amp; 4,942.93 &amp; 15.00 &amp; 45,454.00 \\\\ ~~CHERS&amp;~~ \\text{ por cada millón de habitantes} &amp; \\\\ SMOKING &amp; \\text{ Prevalencia de tabaquismo,} &amp; 88 &amp; 35.09 &amp; 32.00 &amp; 14.40 &amp; 6.00 &amp; 68.00 \\\\ &amp; ~~\\text{ (hombres) % de adultos } \\\\ \\hline LIFEEXP &amp; \\text{ Esperanza de vida al nacer,}&amp; &amp; 67.05 &amp; 71.00 &amp; 11.08 &amp; 40.50 &amp; 82.30 \\\\ &amp; ~~ \\text{ en años } \\\\ \\hline \\end{array} } \\] 1.9 Suplemento Técnico - Teorema del Límite Central Los teoremas del límite central forman la base para gran parte de la inferencia estadística utilizada en el análisis de regresión. Por lo tanto, es útil proporcionar una declaración explícita de una versión del teorema del límite central. Teorema del Límite Central. Supongamos que \\(y_1, \\ldots, y_n\\) están distribuidos de manera independiente con media \\(\\mu\\), varianza finita \\(\\sigma^2\\) y \\(\\mathrm{E}|y|^3\\) es finito. Entonces, \\[ \\lim_{n \\rightarrow \\infty} \\Pr \\left( \\frac{\\sqrt{n}}{\\sigma }(\\overline{y} - \\mu ) \\leq x \\right) = \\Phi \\left( x \\right), \\] para cada \\(x\\), donde \\(\\Phi \\left( \\cdot \\right)\\) es la función de distribución normal estándar. Bajo las suposiciones de este teorema, la distribución reescalada de \\(\\overline{y}\\) se aproxima a una distribución normal estándar a medida que aumenta el tamaño de la muestra, \\(n\\). Interpretamos esto como que, para tamaños de muestra ‘grandes’, la distribución de \\(\\overline{y}\\) puede aproximarse mediante una distribución normal. Investigaciones empíricas han mostrado que tamaños de muestra de \\(n = 25\\) a 50 proporcionan aproximaciones adecuadas para la mayoría de los propósitos. ¿Cuándo no funciona bien el teorema del límite central? Algunos conocimientos se proporcionan mediante otro resultado de la estadística matemática. Aproximación de Edgeworth. Supongamos que \\(y_1, \\ldots, y_n\\) están distribuidos idéntica e independientemente con media \\(\\mu\\), varianza finita \\(\\sigma^2\\) y \\(\\mathrm{E}|y|^3\\) es finito. Entonces, \\[ \\Pr \\left( \\frac{\\sqrt{n}}{\\sigma }(\\overline{y} - \\mu ) \\leq x \\right) = \\Phi \\left( x \\right) + \\frac{1}{6}\\frac{1}{\\sqrt{2\\pi }} e^{-x^2/2} \\frac{\\mathrm{E}(y - \\mu)^3}{\\sigma^3 \\sqrt{n}} + \\frac{h_n}{\\sqrt{n}}, \\] para cada \\(x\\), donde \\(h_n \\rightarrow 0\\) a medida que \\(n \\rightarrow \\infty\\). Este resultado sugiere que la distribución de \\(\\bar{y}\\) se acerca más a una distribución normal a medida que la asimetría, \\(\\mathrm{E}(\\overline{y} - \\mu)^3\\), se acerca a cero. Esto es importante en aplicaciones de seguros porque muchas distribuciones tienden a ser asimétricas. Históricamente, los analistas utilizaban el segundo término en el lado derecho del resultado para proporcionar una ‘corrección’ para la aproximación de la curva normal. Véase, por ejemplo, Beard, Pentikäinen y Pesonen (1984) para una discusión adicional sobre las aproximaciones de Edgeworth en la ciencia actuarial. Una alternativa (utilizada en este libro) que vimos en la Sección 1.3 es transformar los datos, logrando así una aproximada simetría. Como sugiere el teorema de aproximación de Edgeworth, si nuestra población parental es cercana a simétrica, entonces la distribución de \\(\\overline{y}\\) será aproximadamente normal. "],["C2BasicLR.html", "Capítulo 2 Regresión Lineal Básica 2.1 Correlaciones y Mínimos Cuadrados 2.2 Modelo Básico de Regresión Lineal 2.3 ¿Es Útil el Modelo? Algunas Medidas de Resumen Básicas 2.4 Propiedades de los Estimadores del Coeficiente de Regresión 2.5 Inferencia Estadística 2.6 Construyendo un Mejor Modelo: Análisis de Residuos 2.7 Aplicación: Modelo de Valoración de Activos Financieros 2.8 Salida Computacional Ilustrativa de Regresión 2.9 Lecturas Adicionales y Referencias 2.10 Ejercicios 2.11 Suplemento Técnico - Elementos del Álgebra de Matrices", " Capítulo 2 Regresión Lineal Básica Vista previa del capítulo. Este capítulo considera la regresión en el caso de tener solo una variable explicativa. A pesar de esta aparente simplicidad, la mayoría de las ideas profundas de la regresión pueden desarrollarse en este marco. Al limitarnos al caso de una variable, podemos expresar muchos cálculos usando álgebra simple. Esto nos permitirá desarrollar nuestra intuición sobre las técnicas de regresión al reforzarla con demostraciones simples. Además, podemos ilustrar las relaciones entre dos variables gráficamente porque estamos trabajando en solo dos dimensiones. Las herramientas gráficas resultan ser importantes para desarrollar un vínculo entre los datos y un modelo. 2.1 Correlaciones y Mínimos Cuadrados La regresión trata sobre relaciones. Específicamente, estudiaremos cómo dos variables, una \\(x\\) y una \\(y\\), están relacionadas. Queremos poder responder preguntas como, si cambiamos el nivel de \\(x\\), ¿qué pasará con el nivel de \\(y\\)? Si comparamos dos “sujetos” que parecen similares excepto por la medición de \\(x\\), ¿cómo diferirán sus mediciones de \\(y\\)? Entender las relaciones entre variables es fundamental para la gestión cuantitativa, particularmente en ciencias actuariales donde la incertidumbre es tan prevalente. Es útil trabajar con un ejemplo específico para familiarizarnos con conceptos clave. El análisis de ventas de lotería no ha sido parte de la práctica actuarial tradicional, pero es un área de crecimiento en la que los actuarios podrían contribuir. Ejemplo: Ventas de la Lotería de Wisconsin. Los administradores de la lotería del estado de Wisconsin están interesados en evaluar los factores que afectan las ventas de lotería. Las ventas consisten en boletos de lotería en línea que se venden en establecimientos minoristas seleccionados en Wisconsin. Estos boletos generalmente tienen un precio de $1.00, por lo que el número de boletos vendidos equivale a los ingresos de la lotería. Analizamos las ventas promedio de lotería (SALES) durante un período de cuarenta semanas, de abril de 1998 a enero de 1999, en cincuenta áreas seleccionadas al azar identificadas por código postal (ZIP) dentro del estado de Wisconsin. Aunque muchas variables económicas y demográficas podrían influir en las ventas, nuestro primer análisis se centra en la población (POP) como un determinante clave. El Capítulo 3 mostrará cómo considerar variables explicativas adicionales. Intuitivamente, parece claro que las áreas geográficas con más personas tendrán mayores ventas. Entonces, otras cosas siendo iguales, un \\(x=POP\\) más grande significa un \\(y=SALES\\) más grande. Sin embargo, la lotería es una fuente importante de ingresos para el estado y queremos ser lo más precisos posible. Una notación adicional será útil posteriormente. En esta muestra, hay cincuenta áreas geográficas y usamos subíndices para identificar cada área. Por ejemplo, \\(y_1\\) = 1,285.4 representa las ventas para la primera área en la muestra que tiene una población de \\(x_1\\) = 435. Llamamos al par ordenado (\\(x_1\\), \\(y_1\\)) = (435, 1285.4) la primera observación. Extendiendo esta notación, la muestra completa que contiene cincuenta observaciones puede representarse por (\\(x_1\\), \\(y_1\\)), …, (\\(x_{50}\\), \\(y_{50}\\)). Los puntos suspensivos ( … ) significan que el patrón continúa hasta que se encuentra el último objeto. A menudo hablaremos de un miembro genérico de la muestra, refiriéndonos a (\\(x_i\\), \\(y_i\\)) como la \\(i\\)-ésima observación. Los conjuntos de datos pueden complicarse, por lo que será útil si comienza trabajando con cada variable por separado. Los dos paneles en la Figura 2.1 muestran histogramas que dan una impresión visual rápida de la distribución de cada variable de forma aislada. La Tabla 2.1 proporciona resúmenes numéricos correspondientes. Para ilustrar, para la variable población (POP), vemos que el área con el menor número contenía 280 personas, mientras que la más grande contenía 39,098. El promedio, sobre 50 códigos postales, fue de 9,311.04. Para nuestra segunda variable, las ventas fueron tan bajas como 189 y tan altas como 33,181. Figura 2.1: Histogramas de Población y Ventas. Cada distribución está sesgada a la derecha, lo que indica que hay muchas áreas pequeñas en comparación con unas pocas áreas con mayores ventas y poblaciones. Tabla 2.1: Estadísticas Resumen de Cada Variable Promedio Mediana Desviación Estándar Mínimo Máximo POP 9,311 4,406 11,098 280 39,098 SALES 6,495 2,426 8,103 189 33,181 Fuente: Frees y Miller (2003) Código R para producir la Figura 2.1 y la Tabla 2.1 Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURA 2.1 par(mfrow=c(1, 2), cex=1.3, mar=c(4.1,3.1,1.2,1)) hist(Lot$POP, main=&quot;&quot;, ylab=&quot;&quot;, las=1, xlab = &quot;POP&quot;) mtext(&quot;Frecuencia&quot;, side=2, at=30, las=1, cex=1.3, adj=.6) hist(Lot$SALES, main=&quot;&quot;, ylab=&quot;&quot;, las=1, xlab = &quot;SALES&quot;) mtext(&quot;Frecuencia&quot;, side=2, at=34, las=1, cex=1.3, adj=.6) # TABLA 2.1 ESTADÍSTICAS RESUMEN BookSummStats &lt;- function(Xymat){ meanSummary &lt;- sapply(Xymat, mean, na.rm=TRUE) sdSummary &lt;- sapply(Xymat, sd, na.rm=TRUE) minSummary &lt;- sapply(Xymat, min, na.rm=TRUE) maxSummary &lt;- sapply(Xymat, max, na.rm=TRUE) medSummary &lt;- sapply(Xymat, median,na.rm=TRUE) tableMat &lt;- cbind(meanSummary, medSummary, sdSummary, minSummary, maxSummary) return(tableMat) } Xymat &lt;- data.frame(cbind(Lot$POP,Lot$SALES)) tableMat &lt;- BookSummStats(Xymat) colnames(tableMat) &lt;- c(&quot;Promedio&quot; , &quot;Mediana&quot; , &quot;Desviación Estándar&quot; , &quot;Mínimo&quot; , &quot;Máximo&quot;) rownames(tableMat) &lt;- c(&quot;POP&quot;, &quot;SALES&quot;) tableMat1 &lt;- format(round(tableMat, digits=0), big.mark = &#39;,&#39;) TableGen1(TableData=tableMat1, TextTitle=&#39;Estadísticas Resumen de Cada Variable&#39;, Align=&#39;r&#39;, Digits=0, ColumnSpec=1:5, ColWidth = ColWidth5) %&gt;% footnote(general = &quot;Frees y Miller (2003)&quot;, general_title = &quot;Fuente:&quot;, footnote_as_chunk = TRUE) Como muestra la Tabla 2.1, las estadísticas resumen básicas dan ideas útiles de la estructura de las características clave de los datos. Después de entender la información en cada variable de forma aislada, podemos comenzar a explorar la relación entre las dos variables. Gráfico de Dispersión y Coeficientes de Correlación - Herramientas Básicas de Resumen La herramienta gráfica básica utilizada para investigar la relación entre dos variables es un gráfico de dispersión, como se muestra en la Figura 2.2. Aunque podemos perder los valores exactos de las observaciones al graficar los datos, ganamos una impresión visual de la relación entre la población y las ventas. En la Figura 2.2 vemos que las áreas con poblaciones más grandes tienden a comprar más boletos de lotería. ¿Qué tan fuerte es esta relación? ¿Puede el conocimiento de la población del área ayudarnos a anticipar los ingresos por ventas de lotería? Exploramos estas dos preguntas a continuación. Figura 2.2: Un gráfico de dispersión de los datos de la lotería. Cada uno de los 50 símbolos de la gráfica corresponde a un código postal en el estudio. Esta figura sugiere que las áreas postales con poblaciones más grandes tienen mayores ingresos de lotería. Código R para Producir la Figura 2.2 #Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURA 2.2, CON CORRELACIONES par(mar=c(4.1,3.8,2,1),cex=1.1) plot(Lot$POP, Lot$SALES, ylab=&quot;&quot;, las=1, xlab = &quot;POP&quot;) mtext(&quot;SALES&quot;,side=2, at=36000, las=1, cex=1.1) Una forma de resumir la fuerza de la relación entre dos variables es a través de una estadística de correlación. Definición. El coeficiente de correlación ordinario, o de Pearson se define como \\[ r = \\frac{1}{(n-1)s_xs_y}\\sum_{i=1}^{n}\\left( x_{i}-\\overline{x}\\right) \\left( y_{i}-\\overline{y}\\right) . \\] Aquí, usamos la desviación estándar de la muestra \\(s_y = \\sqrt{(n-1)^{-1} \\sum_{i=1}^{n}\\left( y_i - \\overline{y}\\right)^{2}}\\) definida en la Sección 1.2, con una notación similar para \\(s_x\\). Aunque existen otras estadísticas de correlación, el coeficiente de correlación ideado por Pearson (1895) tiene varias propiedades deseables. Una propiedad importante es que, para cualquier conjunto de datos, \\(r\\) está acotado entre -1 y 1, es decir, \\(-1\\leq r\\leq 1\\). (El Ejercicio 2.3 proporciona pasos para comprobar esta propiedad.) Si \\(r\\) es mayor que cero, se dice que las variables están correlacionadas positivamente. Si \\(r\\) es menor que cero, se dice que las variables están correlacionadas negativamente. Cuanto mayor sea el coeficiente en valor absoluto, más fuerte será la relación. De hecho, si \\(r=1\\), entonces las variables están perfectamente correlacionadas. En este caso, todos los datos se encuentran en una línea recta que pasa por los cuadrantes inferior izquierdo y superior derecho. Si \\(r=-1\\), entonces todos los datos se encuentran en una línea que pasa por los cuadrantes superior izquierdo e inferior derecho. El coeficiente \\(r\\) es una medida de una relación lineal entre dos variables. Se dice que el coeficiente de correlación es invariante a la ubicación y la escala. Así, el centro de ubicación de cada variable no importa en el cálculo de \\(r\\). Por ejemplo, si agregamos $100 a las ventas de cada código postal, cada \\(y_i\\) aumentará en 100. Sin embargo, \\(\\overline{y}\\), el precio de compra promedio, también aumentará en 100 de modo que la desviación \\(y_i - \\overline{y}\\) permanece sin cambios, o invariante. Además, la escala de cada variable no importa en el cálculo de \\(r\\). Por ejemplo, supongamos que dividimos cada población entre 1000, de modo que \\(x_i\\) ahora representa la población en miles. Así, \\(\\overline{x}\\) también se divide entre 1000 y usted debería verificar que \\(s_x\\) también se divide entre 1000. Así, la versión estandarizada de \\(x_i\\), \\(\\left( x_i-\\overline{x}\\right) /s_x\\), permanece sin cambios, o invariante. Muchos paquetes estadísticos calculan una versión estandarizada de una variable restando el promedio y dividiendo por la desviación estándar. Ahora, usemos \\(y_{i,std}=\\left( y_i- \\overline{y}\\right) /s_y\\) y \\(x_{i,std}=\\left( x_i-\\overline{x} \\right) /s_x\\) para que sean las versiones estandarizadas de \\(y_i\\) y \\(x_i\\), respectivamente. Con esta notación, podemos expresar el coeficiente de correlación como \\(r=(n-1)^{-1}\\sum_{i=1}^{n}x_{i,std}\\times y_{i,std}.\\) Se dice que el coeficiente de correlación es una medida adimensional. Esto se debe a que hemos eliminado dólares, y todas las demás unidades de medida, considerando las variables estandarizadas \\(x_{i,std}\\) y \\(y_{i,std}\\). Debido a que el coeficiente de correlación no depende de las unidades de medida, es una estadística que puede compararse fácilmente entre diferentes conjuntos de datos. En el mundo de los negocios, el término “correlación” se usa a menudo como sinónimo del término “relación.” Para los propósitos de este texto, utilizamos el término correlación cuando nos referimos únicamente a relaciones lineales. La relación no lineal clásica es \\(y=x^{2}\\), una relación cuadrática. Considere esta relación y el conjunto de datos ficticios para \\(x\\), \\(\\{-2,1,0,1,2\\}\\). Ahora, como ejercicio (2.2), produzca un gráfico aproximado del conjunto de datos: \\[ \\begin{array}{l|rrrrr} \\hline i &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\ \\hline x_i &amp; -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 \\\\ y_i &amp; 4 &amp; 1 &amp; 0 &amp; 1 &amp; 4 \\\\ \\hline \\end{array} \\] El coeficiente de correlación para este conjunto de datos resulta ser \\(r=0\\) (verifíquelo). Por lo tanto, a pesar de que hay una relación perfecta entre \\(x\\) y \\(y\\) (\\(=x^{2}\\)), hay una correlación cero. Recuerde que los cambios de ubicación y escala no son relevantes en las discusiones sobre correlación, por lo que podríamos cambiar fácilmente los valores de \\(x\\) y \\(y\\) para que sean más representativos de un conjunto de datos de negocios. ¿Qué tan fuerte es la relación entre \\(y\\) y \\(x\\) para los datos de la lotería? Gráficamente, la respuesta es un gráfico de dispersión, como en la Figura 2.2. Numéricamente, la respuesta principal es el coeficiente de correlación, que resulta ser \\(r\\) = 0.886 para este conjunto de datos. Interpretamos esta estadística diciendo que SALES y POP están correlacionados (positivamente). La fuerza de la relación es fuerte porque \\(r\\) = 0.886 está cerca de uno. En resumen, podemos describir esta relación diciendo que hay una fuerte correlación entre SALES y POP. Método de Mínimos Cuadrados Ahora comenzamos a explorar la pregunta: “¿Puede el conocimiento de la población ayudarnos a entender las ventas?” Para responder a esta pregunta, identificamos las ventas como la variable de respuesta, o dependiente. La variable de población, que se usa para ayudar a entender las ventas, se llama la variable explicativa, o independiente. Supongamos que tenemos disponibles los datos de muestra de cincuenta ventas \\(\\{y_1, \\ldots, y_{50} \\}\\) y tu trabajo es predecir las ventas de un código postal seleccionado al azar. Sin conocimiento de la variable de población, un predictor sensato es simplemente \\(\\overline{y}=6,495\\), el promedio de la muestra disponible. Naturalmente, anticipas que las áreas con mayores poblaciones tendrán mayores ventas. Es decir, si también tienes conocimiento de la población, ¿puede mejorarse esta estimación? Si es así, ¿cuánto? Para responder a estas preguntas, el primer paso asume una relación lineal aproximada entre \\(x\\) y \\(y\\). Para ajustar una línea a nuestro conjunto de datos, usamos el método de mínimos cuadrados. Necesitamos una técnica general para que, si diferentes analistas están de acuerdo en los datos y en la técnica de ajuste, entonces estarán de acuerdo en la línea. Si diferentes analistas ajustan un conjunto de datos usando aproximaciones a ojo, en general llegarán a diferentes líneas, incluso usando el mismo conjunto de datos. El método comienza con la línea \\(y=b_0^{\\ast}+b_1^{\\ast}x\\), donde la intersección y la pendiente, \\(b_0^{\\ast}\\) y \\(b_1^{\\ast}\\), son meramente valores genéricos. Para la \\(i\\)-ésima observación, \\(y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right)\\) representa la desviación del valor observado \\(y_i\\) de la línea en \\(x_i\\). La cantidad \\[ SS(b_0^{\\ast},b_1^{\\ast})=\\sum_{i=1}^{n}\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) ^{2} \\] representa la suma de desviaciones cuadradas para esta línea candidata. El método de mínimos cuadrados consiste en determinar los valores de \\(b_0^{\\ast}\\) y \\(b_1^{\\ast}\\) que minimizan \\(SS(b_0^{\\ast},b_1^{\\ast})\\). Este es un problema fácil que puede resolverse mediante cálculo, de la siguiente manera. Tomando derivadas parciales con respecto a cada argumento obtenemos \\[ \\frac{\\partial }{\\partial b_0^{\\ast}}SS(b_0^{\\ast},b_1^{\\ast})=\\sum_{i=1}^{n}(-2)\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) \\] y \\[ \\frac{\\partial }{\\partial b_1^{\\ast}}SS(b_0^{\\ast},b_1^{\\ast})=\\sum_{i=1}^{n}(-2x_i)\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) . \\] Se invita al lector a tomar las segundas derivadas parciales para asegurarse de que estamos minimizando, no maximizando, esta función. Igualando estas cantidades a cero y cancelando términos constantes obtenemos \\[ \\sum_{i=1}^{n}\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) =0 \\] y \\[ \\sum_{i=1}^{n}x_i\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) =0, \\] que son conocidas como las ecuaciones normales. Resolver estas ecuaciones proporciona los valores de \\(b_0^{\\ast}\\) y \\(b_1^{\\ast}\\) que minimizan la suma de cuadrados. Definición. Las estimaciones de intersección y pendiente de mínimos cuadrados son \\[ b_1=r\\frac{s_y}{s_x}~~~~~\\mathrm{y}~~~~~b_0=\\overline{y}-b_1 \\overline{x}. \\] La línea que determinan, \\(\\widehat{y}=b_0+b_1x\\), se llama la línea de regresión ajustada. Hemos eliminado la notación de asterisco, o estrella, porque \\(b_0\\) y \\(b_1\\) ya no son valores “candidatos”. ¿Proporciona este procedimiento una línea sensata para nuestras ventas de lotería de Wisconsin? Anteriormente, calculamos \\(r=0.886\\). A partir de esto y de las estadísticas básicas resumidas en la Tabla 2.1, tenemos \\(b_1 = 0.886 \\left( 8,103\\right) /11,098=0.647\\) y \\(b_0 = 6,495-(0.647)9,311 = 469.7\\). Esto produce la línea de regresión ajustada \\[ \\widehat{y} = 469.7 + (0.647)x. \\] El sombrero, o “gorro”, encima de la \\(y\\) nos recuerda que esta \\(\\widehat{y}\\), o \\(\\widehat{SALES}\\), es un valor ajustado. Una aplicación de la línea de regresión es estimar ventas para una población específica, digamos, \\(x=10,000\\). La estimación es la altura de la línea de regresión, que es \\(469.7 + (0.647)(10,000)\\) \\(= 6,939.7\\). Ejemplo: Resumiendo Simulaciones. El análisis de regresión es una herramienta para resumir datos complejos. En el trabajo práctico, los actuarios a menudo simulan escenarios financieros complicados; a menudo se pasa por alto que la regresión puede usarse para resumir relaciones de interés. Para ilustrar, Manistre y Hancock (2005) simularon muchas realizaciones de una opción put europea a 10 años y demostraron la relación entre dos medidas de riesgo actuarial, el valor en riesgo (VaR) y la expectativa de cola condicional (CTE). Para un ejemplo, estos autores examinaron rendimientos de acciones distribuidos logarítmicamente con un precio inicial de $100, de modo que en 10 años el precio de la acción estaría distribuido como \\[ S(Z)=100 \\exp \\left( (.08) 10 + .15 \\sqrt{10} Z \\right), \\] basado en un retorno medio anual del 8%, desviación estándar del 15% y el resultado de una variable aleatoria normal estándar \\(Z\\). La opción put paga la diferencia entre el precio de ejercicio, que se tomará como 110 para este ejemplo, y \\(S(Z)\\). El valor presente de esta opción es \\[ C(Z)= \\mathrm{e}^{-0.06(10)} \\mathrm{max} \\left(0, 110-S(Z) \\right), \\] basado en una tasa de descuento del 6%. Para estimar el VaR y el CTE, para cada \\(i\\), se simularon 1000 variables aleatorias normales estándar i.i.d. y se usaron para calcular 1000 valores presentes, \\(C_{i1}, \\ldots, C_{i,1000}.\\) El percentil 95 de estos valores presentes es la estimación del valor en riesgo, denotado como \\(VaR_i.\\) El promedio de los 50 valores presentes más altos (\\(= (1-.05) \\times 1000\\)) es la estimación de la expectativa de cola condicional, denotada como \\(CTE_i\\). Manistre y Hancock (2005) realizaron este cálculo \\(i=1, \\ldots, 1000\\) veces; el resultado se presenta en la Figura 2.3. El diagrama de dispersión muestra una relación fuerte pero no perfecta entre el \\(VaR\\) y el \\(CTE\\), el coeficiente de correlación resulta ser \\(r=0.782\\). Figura 2.3: Gráfico de la Expectativa de Cola Condicional (CTE) frente al Valor en Riesgo (VaR). Basado en \\(n=1,000\\) simulaciones de un bono put europeo a 10 años. Fuente: Manistre y Hancock (2005). Código R para producir la Figura 2.3 # FIGURA 2.3 # simulación S &lt;- vector(mode = &quot;numeric&quot;, length = 1000) C &lt;- vector(mode = &quot;numeric&quot;, length = 1000) Var &lt;- vector(mode = &quot;numeric&quot;, length = 1000) CTE &lt;- vector(mode = &quot;numeric&quot;, length = 1000) for (i in 1:1000){ for (j in 1:1000){ S[j] &lt;- 100 * exp(.08 * 10 + .15 * (10 ^ .5) * rnorm(1)) C[j] &lt;- exp(-.06 * 10) * max(0, 110 - S[j]) } C &lt;- sort(C) Var[i] &lt;- C[950] CTE[i] &lt;- mean(C[950:1000]) } model &lt;- lm(CTE ~ Var) b0 &lt;- round(model$coef[1], digits = 3) b1 &lt;- round(model$coef[2], digits = 3) R2 &lt;- round(summary(model)$r.squared, digits = 4) plot(Var, CTE, xlab = expression(paste(&quot;Estimaciones de VaR&quot;)), ylab = expression(paste(&quot;Estimaciones de CTE&quot;)), xlim = c(0, 12), ylim = c(8, 20), xaxs = &quot;i&quot;, yaxs = &quot;i&quot;, pch = 20, cex = 0.4) lines(Var, model$fitted, lwd = .5) abline(h = c(10, 12, 14, 16, 18, 20), col = &quot;grey&quot;) 2.2 Modelo Básico de Regresión Lineal El diagrama de dispersión, el coeficiente de correlación y la línea de regresión ajustada son herramientas útiles para resumir la relación entre dos variables para un conjunto de datos específico. Para inferir relaciones generales, necesitamos modelos para representar los resultados de poblaciones amplias. Este capítulo se centra en un modelo de “regresión lineal básica”. La parte de “regresión lineal” proviene del hecho de que ajustamos una línea a los datos. La parte de “básica” es porque usamos solo una variable explicativa, \\(x\\). Este modelo también se conoce como una “regresión lineal simple”. Este texto evita este lenguaje porque da la falsa impresión de que las ideas e interpretaciones de regresión con una variable explicativa son siempre sencillas. Ahora introducimos dos conjuntos de supuestos del modelo básico, las representaciones “observables” y de “error”. Son equivalentes, pero cada una nos ayudará a medida que extendamos los modelos de regresión más allá de lo básico. \\[ {\\small \\begin{array}{l} \\hline \\hline &amp;\\textbf{Modelo Básico de Regresión Lineal} \\\\ &amp;\\textbf{Supuestos de Muestreo de la Representación Observable} \\\\ \\hline \\text{F1}. &amp; \\mathrm{E}~y_i=\\beta_0 + \\beta_1 x_i . \\\\ \\text{F2}. &amp; \\{x_1,\\ldots ,x_n\\} \\text{ son variables no estocásticas}. \\\\ \\text{F3}. &amp; \\mathrm{Var}~y_i=\\sigma ^{2}. \\\\ \\text{F4}. &amp; \\{ y_i\\} \\text{ son variables aleatorias independientes}. \\\\ \\hline\\ \\end{array} } \\] La “representación observable” se enfoca en variables que podemos ver (u observar), \\((x_i,y_i)\\). La inferencia sobre la distribución de \\(y\\) es condicional a las variables explicativas observadas, de modo que podemos tratar \\(\\{x_1,\\ldots ,x_n\\}\\) como variables no estocásticas (supuesto F2). Al considerar tipos de mecanismos de muestreo para \\((x_i,y_i)\\), es conveniente pensar en un esquema de muestreo aleatorio estratificado, donde los valores de \\(\\{x_1,\\ldots ,x_n\\}\\) se tratan como los estratos, o grupos. Bajo el muestreo estratificado, para cada valor único de \\(x_i\\), tomamos una muestra aleatoria de una población. Para ilustrar, supongamos que se está extrayendo de una base de datos de empresas para comprender el rendimiento de las acciones (\\(y\\)) y desea estratificar según el tamaño de la empresa. Si la cantidad de activos es una variable continua, entonces podemos imaginar tomar una muestra de tamaño 1 para cada empresa. De esta manera, hipotetizamos una distribución de rendimientos de acciones condicional al tamaño de los activos de la empresa. Digresión: A menudo verá informes que resumen resultados para los “50 mejores gerentes” o las “100 mejores universidades”, medidos por alguna variable de resultado. En aplicaciones de regresión, asegúrese de no seleccionar observaciones basadas en una variable dependiente, como el rendimiento más alto de las acciones, porque esto es estratificar basado en el \\(y\\), no en el \\(x\\). El Capítulo 6 discutirá los procedimientos de muestreo con mayor detalle. El muestreo estratificado también proporciona motivación para el supuesto F4, la independencia entre respuestas. Se puede motivar el supuesto F1 pensando en \\((x_i,y_i)\\) como una extracción de una población, donde la media de la distribución condicional de \\(y_i\\) dado {\\(x_i\\)} es lineal en la variable explicativa. El supuesto F3 se conoce como homocedasticidad, que discutiremos ampliamente en la Sección 5.7. Ver Goldberger (1991) para más información sobre esta representación. Un quinto supuesto que a menudo se usa implícitamente es: \\[ \\text{F5}. \\{y_i\\} \\text{ están distribuidos normalmente}. \\] Este supuesto no es necesario para muchos procedimientos de inferencia estadística porque los teoremas del límite central proporcionan normalidad aproximada para muchas estadísticas de interés. Sin embargo, la justificación formal para algunas, como las estadísticas \\(t\\), requieren este supuesto adicional. En contraste con la representación observable, un conjunto alternativo de supuestos se enfoca en las desviaciones, o “errores”, en la regresión, definidos como \\(\\varepsilon_i=y_i-\\left( \\beta_0 + \\beta_1 x_i \\right)\\). \\[ {\\small \\begin{array}{l} \\hline \\hline &amp;\\textbf{Modelo Básico de Regresión Lineal} \\\\ &amp;\\textbf{Supuestos de Muestreo de la Representación de Error} \\\\ \\hline \\text{E1}. &amp; y_i=\\beta_0 + \\beta_1 x_i + \\varepsilon_i . \\\\ \\text{E2}. &amp; \\{x_1,\\ldots ,x_n\\} \\text{ son variables no estocásticas}. \\\\ \\text{E3}. &amp; \\mathrm{E}~\\varepsilon_i=0 \\text{ y } \\mathrm{Var}~\\varepsilon_i=\\sigma ^{2}. \\\\ \\text{E4}. &amp; \\{ \\varepsilon_i\\} \\text{ son variables aleatorias independientes}. \\\\ \\hline\\ \\end{array} } \\] La “representación de error” se basa en la teoría gaussiana de errores (ver Stigler, 1986, para un contexto histórico). El supuesto E1 asume que \\(y\\) es en parte debido a una función lineal de la variable explicativa observada, \\(x\\). Otras variables no observadas que influyen en la medición de \\(y\\) se interpretan como incluidas en el término de “error” \\(\\varepsilon_i\\), que también se conoce como el término de “perturbación”. La independencia de errores, E4, puede motivarse asumiendo que {\\(\\varepsilon_i\\)} se realizan a través de una muestra aleatoria simple de una población desconocida de errores. Los supuestos E1-E4 son equivalentes a F1-F4. La representación de error proporciona una base útil para motivar las medidas de ajuste (Sección 2.3). Sin embargo, una desventaja de la representación de error es que desvía la atención de las cantidades observables \\((x_i,y_i)\\) a una cantidad no observable, {\\(\\varepsilon_i\\)}. Para ilustrar, la base de muestreo, ver {\\(\\varepsilon_i\\)} como una muestra aleatoria simple, no es directamente verificable porque no se puede observar directamente la muestra {\\(\\varepsilon_i\\)}. Además, el supuesto de errores aditivos en E1 será problemático cuando consideremos modelos de regresión no lineales. La Figura 2.4 ilustra algunos de los supuestos del modelo básico de regresión lineal. Los datos (\\(x_1,y_1\\)), (\\(x_2,y_2\\)) y (\\(x_3,y_3\\)) son observados y se representan con los símbolos de trazado circulares opacos. Según el modelo, estas observaciones deben estar cerca de la línea de regresión \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x\\). Cada desviación de la línea es aleatoria. A menudo asumimos que la distribución de desviaciones puede representarse por una curva normal, como en la Figura 2.4. Figura 2.4: La distribución de la respuesta varía según el nivel de la variable explicativa. Los supuestos del modelo básico de regresión lineal describen la población subyacente. Tabla 2.2 destaca la idea de que las características de esta población pueden resumirse mediante los parámetros \\(\\beta_0\\), \\(\\beta_1\\) y \\(\\sigma ^{2}\\). En la Sección 2.1, resumimos datos de una muestra, introduciendo las estadísticas \\(b_0\\) y \\(b_1\\). La Sección 2.3 introducirá \\(s^{2}\\), la estadística correspondiente al parámetro \\(\\sigma ^{2}\\). Tabla 2.2. Medidas Resumen de la Población y la Muestra \\[ {\\small \\begin{array}{llccc}\\hline\\hline &amp; \\text{Resumen} \\\\ \\text{Datos} &amp; \\text{Medidas} &amp; \\text{Intercepto} &amp; \\text{Pendiente} &amp; \\text{Varianza} \\\\\\hline \\text{Población} &amp; \\text{Parámetros} &amp; \\beta_0 &amp; \\beta_1 &amp; \\sigma^2 \\\\ \\text{Muestra} &amp; \\text{Estadísticas} &amp; b_0 &amp; b_1 &amp; s^2 \\\\ \\hline \\end{array} } \\] 2.3 ¿Es Útil el Modelo? Algunas Medidas de Resumen Básicas Aunque la estadística es la ciencia de resumir datos, también es el arte de argumentar con datos. Esta sección desarrolla algunas de las herramientas básicas usadas para justificar el modelo de regresión lineal básica. Un diagrama de dispersión puede proporcionar una fuerte evidencia visual de que \\(x\\) influye en \\(y\\); desarrollar evidencia numérica nos permitirá cuantificar la fuerza de la relación. Además, la evidencia numérica será útil cuando consideremos otros conjuntos de datos donde la evidencia gráfica no sea convincente. 2.3.1 Particionando la Variabilidad Las desviaciones cuadradas, \\(\\left( y_i-\\overline{y}\\right) ^2\\), proporcionan una base para medir la dispersión de los datos. Si deseamos estimar la \\(i\\)-ésima variable dependiente sin conocimiento de \\(x\\), entonces \\(\\overline{y}\\) es una estimación adecuada y \\(y_i- \\overline{y}\\) representa la desviación de la estimación. Usamos \\(Total~SS=\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^2\\), la suma total de cuadrados, para representar la variación en todas las respuestas. Supongamos ahora que también tenemos conocimiento de \\(x\\), una variable explicativa. Usando la línea de regresión ajustada, para cada observación podemos calcular el valor ajustado correspondiente, \\(\\widehat{y}_i = b_0 + b_1x_i\\). El valor ajustado es nuestra estimación con conocimiento de la variable explicativa. Como antes, la diferencia entre la respuesta y el valor ajustado, \\(y_i- \\widehat{y}_i\\), representa la desviación de esta estimación. Ahora tenemos dos “estimaciones” de \\(y_i\\), que son \\(\\widehat{y}_i\\) y \\(\\overline{y}\\). Presumiblemente, si la línea de regresión es útil, entonces \\(\\widehat{y}_i\\) es una medida más precisa que \\(\\overline{y}\\). Para juzgar esta utilidad, descomponemos algebraicamente la desviación total como: \\[\\begin{equation} {\\small \\begin{array}{ccccc} \\underbrace{y_i-\\overline{y}} &amp; = &amp; \\underbrace{y_i-\\widehat{y}_i} &amp; + &amp; \\underbrace{\\widehat{y}_i-\\overline{y}} \\\\ \\text{desviación} &amp; = &amp; \\text{desviación} &amp; + &amp; \\text{desviación} \\\\ \\text{total} &amp; &amp; \\text{no explicada} &amp; &amp; \\text{explicada} \\\\ \\end{array} \\tag{2.1} } \\end{equation}\\] Interpreta esta ecuación como “la desviación sin conocimiento de \\(x\\) es igual a la desviación con conocimiento de \\(x\\) más la desviación explicada por \\(x\\).” La Figura 2.5 es una representación geométrica de esta descomposición. En la figura, se eligió una observación por encima de la línea, lo que da una desviación positiva de la línea de regresión ajustada, para hacer que el gráfico sea más fácil de leer. Un buen ejercicio es hacer un boceto aproximado correspondiente a la Figura 2.5 con una observación por debajo de la línea de regresión ajustada. Figura 2.5: Representación geométrica de la descomposición de la desviación. Ahora, a partir de la descomposición algebraica en la ecuación (2.1), eleva al cuadrado cada lado de la ecuación y suma sobre todas las observaciones. Después de un poco de manipulación algebraica, esto da como resultado \\[\\begin{equation} \\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^2=\\sum_{i=1}^{n}\\left( y_i-\\widehat{y}_i\\right) ^2+\\sum_{i=1}^{n}\\left( \\widehat{y}_i- \\overline{y}\\right) ^2. \\tag{2.2} \\end{equation}\\] Reescribimos esto como \\(Total~SS=Error~SS+Regression~SS\\) donde \\(SS\\) significa suma de cuadrados. Interpretamos: \\(Total~SS\\) como la variación total sin conocimiento de \\(x\\), \\(Error~SS\\) como la variación total que queda después de introducir \\(x\\), y \\(Regression~SS\\) como la diferencia entre el \\(Total~SS\\) y el \\(Error~SS\\), o la variación total “explicada” mediante el conocimiento de \\(x\\). Al elevar al cuadrado el lado derecho de la ecuación (2.1), tenemos el término de producto cruzado \\(2\\left(y_i-\\widehat{y}_i\\right) \\left( \\widehat{y}_i-\\overline{y}\\right)\\). Con la “manipulación algebraica”, se puede comprobar que la suma de los productos cruzados sobre todas las observaciones es cero. Este resultado no es cierto para todas las líneas ajustadas, pero es una propiedad especial de la línea ajustada por mínimos cuadrados. En muchos casos, la descomposición de la variabilidad se reporta a través de un solo estadístico. Definición. El coeficiente de determinación se denota por el símbolo \\(R^2\\), llamado “\\(R\\)-cuadrado”, y se define como \\[ R^2=\\frac{Regression~SS}{Total~SS}. \\] Interpretamos \\(R^2\\) como la proporción de variabilidad explicada por la línea de regresión. En un caso extremo donde la línea de regresión se ajusta perfectamente a los datos, tenemos \\(Error~SS=0\\) y \\(R^2=1\\). En el otro caso extremo donde la línea de regresión no proporciona ninguna información sobre la respuesta, tenemos \\(Regression~SS=0\\) y \\(R^2=0\\). El coeficiente de determinación está limitado por las desigualdades \\(0 \\leq R^2 \\leq 1\\) con valores mayores que implican un mejor ajuste. 2.3.2 El Tamaño de una Desviación Típica: s En el modelo de regresión lineal básica, la desviación de la respuesta de la línea de regresión, \\(y_i-\\left( \\beta_0+\\beta_1x_i\\right)\\), no es una cantidad observable porque los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) no son observados. Sin embargo, usando los estimadores \\(b_0\\) y \\(b_1\\), podemos aproximar esta desviación usando \\[ e_i=y_i-\\widehat{y}_i=y_i-\\left( b_0+b_1x_i\\right) , \\] conocido como el residuo. Los residuos serán cruciales para desarrollar estrategias para mejorar la especificación del modelo en la Sección 2.6. Ahora mostramos cómo usar los residuos para estimar \\(\\sigma ^2\\). De un primer curso en estadística, sabemos que si se pudieran observar las desviaciones \\(\\varepsilon_i\\), entonces una estimación deseable de \\(\\sigma ^2\\) sería \\((n-1)^{-1}\\sum_{i=1}^{n}\\left( \\varepsilon _i-\\overline{\\varepsilon }\\right) ^2\\). Como \\(\\{\\varepsilon_i\\}\\) no se observan, usamos lo siguiente. Definición. Un estimador de \\(\\sigma ^2\\), el error cuadrático medio (MSE), se define como \\[\\begin{equation} s^2=\\frac{1}{n-2}\\sum_{i=1}^{n}e_i{}^2. \\tag{2.3} \\end{equation}\\] La raíz cuadrada positiva, \\(s=\\sqrt{s^2},\\) se llama la desviación estándar residual. Comparando las definiciones de \\(s^2\\) y \\((n-1)^{-1}\\sum_{i=1}^{n}\\left( \\varepsilon_i-\\overline{\\varepsilon }\\right) ^2\\), verá dos diferencias importantes. Primero, al definir \\(s^2\\) no hemos restado el residuo promedio de cada residuo antes de elevar al cuadrado. Esto se debe a que el residuo promedio es cero, una propiedad especial de la estimación de mínimos cuadrados (ver Ejercicio 2.14). Este resultado se puede mostrar usando álgebra y está garantizado para todos los conjuntos de datos. En segundo lugar, al definir \\(s^2\\) hemos dividido por \\(n-2\\) en lugar de \\(n-1\\). Intuitivamente, dividir por \\(n\\) o \\(n-1\\) tiende a subestimar \\(\\sigma ^2\\). La razón es que, al ajustar líneas a los datos, necesitamos al menos dos observaciones para determinar una línea. Por ejemplo, debemos tener al menos tres observaciones para que haya alguna variabilidad alrededor de una línea. ¿Cuánta “libertad” hay para la variabilidad alrededor de una línea? Diremos que los grados de libertad del error son el número de observaciones disponibles, \\(n\\), menos el número de observaciones necesarias para determinar una línea, 2 (con símbolos, \\(df=n-2\\)). Sin embargo, como vimos en la subsección de estimación de mínimos cuadrados, no necesitamos identificar dos observaciones reales para determinar una línea. La idea es que si un analista conoce la línea y \\(n-2\\) observaciones, entonces las dos observaciones restantes se pueden determinar, sin variabilidad. Al dividir por \\(n-2\\), se puede mostrar que \\(s^2\\) es un estimador insesgado de \\(\\sigma ^2\\). También podemos expresar \\(s^2\\) en términos de las sumas de cuadrados. Es decir, \\[ s^2=\\frac{1}{n-2}\\sum_{i=1}^{n}\\left( y_i-\\widehat{y}_i\\right) ^2= \\frac{Error~SS}{n-2}=MSE. \\] Esto nos lleva a la tabla de análisis de varianza o ANOVA: \\[ {\\small \\begin{array}{llcl} \\hline \\hline \\text{Tabla ANOVA} \\\\ \\hline \\text{Fuente} &amp; \\text{Suma de Cuadrados} &amp; df &amp; \\text{Cuadrado Medio} \\\\ \\hline \\text{Regresión} &amp; Regression~SS &amp; 1 &amp; Regression~MS \\\\ \\text{Error} &amp; Error~SS &amp; n-2 &amp; MSE \\\\ \\text{Total} &amp; Total~SS &amp; n-1 &amp; \\\\ \\hline \\hline \\end{array} } \\] La tabla ANOVA es simplemente un dispositivo de contabilidad utilizado para hacer un seguimiento de las fuentes de variabilidad; aparece rutinariamente en paquetes de software estadístico como parte de los resultados de la regresión. Las figuras de la columna de cuadrados medios se definen como las sumas de cuadrados (\\(SS\\)) divididas por sus respectivos grados de libertad (\\(df\\)). En particular, el cuadrado medio de los errores (\\(MSE\\)) es igual a \\(s^2\\) y la suma de cuadrados de la regresión es igual al cuadrado medio de la regresión. Esta última propiedad es específica para la regresión con una variable; no es cierta cuando consideramos más de una variable explicativa. Los grados de libertad del error en la tabla ANOVA son \\(n-2\\). Los grados de libertad totales son \\(n-1\\), lo que refleja el hecho de que la suma total de cuadrados se centra en la media (se requieren al menos dos observaciones para una variabilidad positiva). El grado de libertad único asociado con la parte de regresión significa que la pendiente, más una observación, es suficiente información para determinar la línea. Esto se debe a que se necesitan dos observaciones para determinar una línea y al menos tres observaciones para que haya alguna variabilidad alrededor de la línea. La tabla de análisis de varianza para los datos de la lotería es: Suma de Cuadrados \\(df\\) Cuadrado Medio Regresión 2,527,165,015 1 2,527,165,015 Error 690,116,755 48 14,377,432 Total 3,217,281,770 49 Código R para Producir la Tabla ANOVA de Lotería #Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) model.basiclinearreg&lt;-lm(Lot$SALES ~ Lot$POP) #summary(model.basiclinearreg) ANOVA &lt;- anova(model.basiclinearreg) row1 &lt;- c(ANOVA$`Sum Sq`[1], ANOVA$`Df`[1], ANOVA$`Mean Sq`[1]) row2 &lt;- c(ANOVA$`Sum Sq`[2], ANOVA$`Df`[2], ANOVA$`Mean Sq`[2]) row3 &lt;- c(ANOVA$`Sum Sq`[1]+ANOVA$`Sum Sq`[2], ANOVA$`Df`[1] +ANOVA$`Df`[2], NaN) ANOVATable &lt;- rbind(row1, row2, row3) ANOVATable1 &lt;- format(round(ANOVATable, digits = 0), big.mark = &#39;,&#39;) ANOVATable1[3,3] &lt;- &quot;&quot; rownames(ANOVATable1) &lt;- c(&quot;Regresión&quot;, &quot;Error&quot;, &quot;Total&quot;) colnames(ANOVATable1) &lt;- c(&quot;Suma de Cuadrados&quot;, &quot;$df$&quot;, &quot;Cuadrado Medio&quot;) kable(ANOVATable1, align = &#39;r&#39;) %&gt;% kable_styling(position = &quot;center&quot;, full_width = FALSE) %&gt;% kableExtra::kable_classic(font = 12, html_font = &quot;Cambria&quot;) De esta tabla, puede verificar que \\(R^2=78.5\\%\\) y \\(s=3,792.\\) 2.4 Propiedades de los Estimadores del Coeficiente de Regresión Las estimaciones de mínimos cuadrados se pueden expresar como una suma ponderada de las respuestas. Para ver esto, define los pesos \\[ w_i=\\frac{x_i-\\overline{x}}{s_x^2(n-1)}. \\] Como la suma de las desviaciones de \\(x\\) (\\(x_i-\\overline{x}\\)) es cero, vemos que \\(\\sum_{i=1}^{n}w_i=0\\). Así, podemos expresar la estimación de la pendiente \\[\\begin{equation} b_1=r\\frac{s_y}{s_x}=\\frac{1}{(n-1)s_x^2}\\sum_{i=1}^{n}\\left( x_i-\\overline{x}\\right) \\left( y_i-\\overline{y}\\right) =\\sum_{i=1}^{n}w_i\\left( y_i-\\overline{y}\\right) =\\sum_{i=1}^{n}w_iy_i. \\tag{2.4} \\end{equation}\\] Los ejercicios piden al lector verificar que \\(b_0\\) también puede expresarse como una suma ponderada de las respuestas, por lo que nuestra discusión se refiere a ambos coeficientes de regresión. Dado que los coeficientes de regresión son sumas ponderadas de respuestas, pueden verse afectados drásticamente por observaciones inusuales (ver Sección 2.6). Como \\(b_1\\) es una suma ponderada, es sencillo derivar la esperanza y la varianza de esta estadística. Por la linealidad de las esperanzas y la Suposición F1, tenemos \\[ \\mathrm{E}~b_1=\\sum_{i=1}^{n}w_i~\\mathrm{E}~y_i=\\beta_0\\sum_{i=1}^{n}w_i+\\beta_1\\sum_{i=1}^{n}w_ix_i=\\beta_1. \\] Es decir, \\(b_1\\) es un estimador imparcial de \\(\\beta_1\\). Aquí, la suma \\(\\sum_{i=1}^{n}w_ix_i\\) \\(=\\) \\(\\left[ s_x^2(n-1)\\right] ^{-1}\\sum_{i=1}^{n}\\left( x_i-\\overline{x}\\right) x_i\\) \\(=\\left[s_x^2(n-1)\\right] ^{-1}\\sum_{i=1}^{n}\\left( x_i-\\overline{x}\\right) ^2=1.\\) A partir de la definición de los pesos, una sencilla algebra también muestra que \\(\\sum_{i=1}^{n}w_i^2=1/\\left( s_x^2(n-1)\\right)\\). Además, la independencia de las respuestas implica que la varianza de la suma es la suma de las varianzas, y así tenemos \\[ \\mathrm{Var}~b_1 =\\sum_{i=1}^{n}w_i^2\\mathrm{Var}~y_i=\\frac{\\sigma^2}{s_x^2(n-1)}. \\] Sustituyendo \\(\\sigma ^2\\) por su estimador \\(s^2\\) y tomando raíces cuadradas se obtiene lo siguiente. Definición. El error estándar de \\(b_1\\), la desviación estándar estimada de \\(b_1\\), se define como \\[\\begin{equation} se(b_1)=\\frac{s}{s_x\\sqrt{n-1}}. \\tag{2.5} \\end{equation}\\] Esta es nuestra medida de la fiabilidad, o precisión, del estimador de la pendiente. Usando la ecuación (2.5), vemos que \\(se(b_1)\\) está determinado por tres cantidades: \\(n\\), \\(s\\) y \\(s_x\\), de la siguiente manera: Si tenemos más observaciones, de manera que \\(n\\) sea mayor, entonces \\(se(b_1)\\) será menor, manteniendo todo lo demás constante. Si las observaciones tienen una mayor tendencia a estar más cerca de la línea, de manera que \\(s\\) sea menor, entonces \\(se(b_1)\\) será menor, manteniendo todo lo demás constante. Si los valores de la variable explicativa están más dispersos, de manera que \\(s_x\\) aumenta, entonces \\(se(b_1)\\) será menor, manteniendo todo lo demás constante. Valores menores de \\(se(b_1)\\) ofrecen una mejor oportunidad para detectar relaciones entre \\(y\\) y \\(x\\). La Figura 2.6 ilustra estas relaciones. Aquí, el diagrama de dispersión en el medio tiene el valor más pequeño de \\(se(b_1)\\). Comparado con el gráfico del medio, el gráfico de la izquierda tiene un valor mayor de \\(s\\) y por lo tanto \\(se(b_1)\\). Comparado con el gráfico de la derecha, el gráfico del medio tiene un valor mayor de \\(s_x\\), y por lo tanto un valor menor de \\(se(b_1)\\). Figura 2.6: Estos tres diagramas de dispersión muestran la misma relación lineal entre \\(y\\) y \\(x\\). El gráfico a la izquierda muestra una mayor variabilidad alrededor de la línea que el gráfico del medio. El gráfico a la derecha muestra una desviación estándar menor en \\(x\\) que el gráfico del medio. Código R para producir la Figura 2.6 # FIGURA 2.6 AQUÍ par(mfrow=c(1, 3),mar=c(3.8,2.8,1,1), cex=1.3) x &lt;- c(1,2,2.3,2.5,1.5,1.7,2.6,2.8,.9,.88,.8,1.2,1.3,1.45,1.8,2.2,2.1) y &lt;- c(.5,2.2,2.6,2.5,.8,1.5,2.3,2.4,.75,.7,1.3,1.5,1.7,2.3,2.3,2.7,1.25) plot(x,y, xlim=c(0.5,3), ylim=c(0,3.5), bty=&quot;l&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab=&quot;&quot;) mtext(&quot;y&quot;, side=2, at=3.5, line=2, las=1, cex=1.3) mtext(&quot;x&quot;, side=1, line=2, cex=1.3) a &lt;- seq(.75,2.75, by = .001) b = a lines(a,b) x &lt;- c(1,2,2.3,2.5,1.5,1.7,2.6,2.8,.9,.88,.8,1.2,1.3,1.45,1.8,2.2,2.45) y &lt;- c(1,2,2.3,2.5,1.2,1.6,2.4,2.6,1.1,1.11,1.2,1.3,1.45,1.6,1.95,2.3,2.7) plot(x,y, xlim=c(0.5,3), ylim=c(0,3.5), bty=&quot;l&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab=&quot;&quot;) mtext(&quot;y&quot;, side=2, at=3.5, line=2, las=1, cex=1.3) mtext(&quot;x&quot;, side=1, line=2, cex=1.3) a &lt;- seq(.75,2.75, by = .001) b = a lines(a,b) x &lt;- c(1,2,2.3,2.5,1.5,1.7,2.6,2.8,2.6,1.5,2,1.2,1.3,1.45,1.8,2.2,2.45) y &lt;- c(1,2,2.3,2.5,1.2,1.6,2.4,2.6,2,2,2.4,1.3,1.55,1.6,1.95,1.6,2.7) plot(x,y, xlim=c(-1.5,5), ylim=c(-2,5.5), bty=&quot;l&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab=&quot;&quot;) mtext(&quot;y&quot;, side=2, at=5.1, line=2, las=1, cex=1.3) mtext(&quot;x&quot;, side=1, line=2, cex=1.3) a &lt;- seq(-.5,4.5, by = .001) b = a lines(a,b) La ecuación (2.4) también implica que el coeficiente de regresión \\(b_1\\) sigue una distribución normal. Es decir, recordemos de la estadística matemática que las combinaciones lineales de variables aleatorias normales también son normales. Así, si se cumple la Suposición F5, entonces \\(b_1\\) sigue una distribución normal. Además, existen varias versiones de los teoremas del límite central para sumas ponderadas (ver, por ejemplo, Serfling, 1980). Así, como se discute en la Sección 1.4, si las respuestas \\(y_i\\) están siquiera aproximadamente distribuidas normalmente, entonces será razonable usar una aproximación normal para la distribución muestral de \\(b_1\\). Usando \\(se(b_1)\\) como la desviación estándar estimada de \\(b_1\\), para valores grandes de \\(n\\) tenemos que \\(\\left( b_1-\\beta_1\\right) /se(b_1)\\) tiene una distribución normal estándar aproximada. Aunque no lo probaremos aquí, bajo la Suposición F5 \\(\\left( b_1-\\beta_1\\right) /se(b_1)\\) sigue una distribución \\(t\\) con grados de libertad \\(df=n-2\\). 2.5 Inferencia Estadística Una vez que hemos ajustado un modelo con un conjunto de datos, podemos hacer una serie de afirmaciones importantes. Generalmente, es útil pensar en estas afirmaciones en tres categorías: (i) pruebas de ideas hipotetizadas, (ii) estimaciones de parámetros del modelo y (iii) predicciones de nuevos resultados. 2.5.1 ¿Es Importante la Variable Explicativa?: La Prueba t Respondemos a la pregunta de si la variable explicativa es importante investigando si \\(\\beta_1=0\\). La lógica es que si \\(\\beta_1=0\\), entonces el modelo de regresión lineal básico ya no incluye una variable explicativa \\(x\\). Por lo tanto, traducimos nuestra pregunta sobre la importancia de la variable explicativa en una pregunta más específica que puede ser respondida utilizando el marco de pruebas de hipótesis. Esta pregunta más específica es: ¿es válida la hipótesis nula \\(H_0:\\beta_1=0\\)? Respondemos a esta pregunta observando la estadística de prueba: \\[ {\\small t-\\mathrm{ratio}=\\frac{\\mathrm{valor~estimado~del~parámetro~-~valor~hipotetizado}} {\\mathrm{error~estándar~del~estimador}}. } \\] En el caso de \\(H_0:\\beta_1=0\\), examinamos la razón t \\(t(b_1)=b_1/se(b_1)\\) porque el valor hipotetizado de \\(\\beta_1\\) es 0. Esta es la estandarización apropiada porque, bajo la hipótesis nula y las suposiciones del modelo descritas en la Sección 2.4, la distribución muestral de \\(t(b_1)\\) se puede demostrar que sigue una distribución t con \\(df=n-2\\) grados de libertad. Así, para probar la hipótesis nula \\(H_0\\) contra la alternativa \\(H_{a}:\\beta_1\\neq 0\\), rechazamos \\(H_0\\) a favor de \\(H_{a}\\) si \\(|t(b_1)|\\) excede un valor t. Aquí, este valor t es un percentil de la distribución t usando \\(df=n-2\\) grados de libertad. Denotamos el nivel de significancia como \\(\\alpha\\) y este valor t como \\(t_{n-2,1-\\alpha /2}\\). Ejemplo: Ventas de Lotería - Continuación. Para el ejemplo de ventas de lotería, la desviación estándar residual es \\(s=3,792\\). En la Tabla 2.1, tenemos \\(s_x = 11,098\\). Por lo tanto, el error estándar de la pendiente es \\(se(b_1) = 3792/(11098\\sqrt{50-1})=0.0488\\). Según la Sección 2.1, la estimación de la pendiente es \\(b_1=0.647\\). Por lo tanto, la estadística t es \\(t(b_1) = 0.647/0.0488 = 13.4\\). Interpretamos esto diciendo que la pendiente está 13.4 errores estándar por encima de cero. Para el nivel de significancia, usamos el valor habitual de \\(\\alpha\\) = 5%. El percentil 97.5 de una distribución t con \\(df=50-2=48\\) grados de libertad es \\(t_{48,0.975}=2.011\\). Dado que \\(|13.4|&gt;2.011\\), rechazamos la hipótesis nula de que la pendiente \\(\\beta_1 = 0\\) a favor de la alternativa de que \\(\\beta_1 \\neq 0\\). Tomar decisiones comparando una razón t con un valor t se llama una prueba t. Probar \\(H_0:\\beta_1=0\\) frente a \\(H_{a}:\\beta_1\\neq 0\\) es solo una de las muchas pruebas de hipótesis que se pueden realizar, aunque es la más común. Tabla 2.3 describe procedimientos alternativos para la toma de decisiones. Estos procedimientos son para probar \\(H_0:\\beta_1 = d\\) donde \\(d\\) es un valor prescrito por el usuario que puede ser igual a cero o cualquier otro valor conocido. Por ejemplo, en nuestro ejemplo de la Sección 2.7, usaremos \\(d=1\\) para probar teorías financieras sobre el mercado de valores. Tabla 2.3 Procedimientos de Toma de Decisiones para Probar \\(H_0:\\beta_1 = d\\) \\[ {\\small \\begin{array}{c|c} \\hline \\text{Hipótesis Alternativa } (H_{a}) &amp; \\text{Procedimiento: Rechazar } H_0 \\text{ a favor de } H_{a} \\text{ si} \\\\ \\hline \\beta_1&gt;d &amp; t-\\mathrm{ratio}&gt;t_{n-2,1-\\alpha }. \\\\ \\beta_1&lt;d &amp; t-\\mathrm{ratio}&lt;-t_{n-2,1-\\alpha }. \\\\ \\beta_1\\neq d &amp; |t-\\mathrm{ratio}\\mathit{|}&gt;t_{n-2,1-\\alpha /2}. \\\\ \\end{array} }\\\\ {\\small \\begin{array}{l} \\hline \\text{Notas: El nivel de significancia es } \\alpha . \\text{Aquí, }t_{n-2,1-\\alpha} \\text{ es el percentil } (1-\\alpha )\\\\ ~~\\text{de la distribución }t \\text{ con } df=n-2 \\text{ grados de libertad.}\\\\ ~~\\text{La estadística de prueba es }t-\\mathrm{ratio} = (b_1 -d)/se(b_1) . \\\\ \\hline \\end{array} } \\] Alternativamente, se pueden construir valores de probabilidad (\\(p\\)-) y compararlos con los niveles de significancia dados. El valor \\(p\\)- es una estadística resumen útil para el analista de datos ya que permite al lector del informe entender la fuerza de la desviación de la hipótesis nula. Tabla 2.4 resume el procedimiento para calcular los valores \\(p\\)-. Tabla 2.4 Valores de Probabilidad para Probar \\(H_0:\\beta_1 = d\\) \\[ {\\small \\begin{array}{c|ccc} \\hline \\text{Hipótesis} &amp; &amp; &amp; \\\\ \\text{Alternativa } (H_a) &amp; \\beta_1&gt;d &amp; \\beta_1&lt;d &amp; \\beta_1\\neq d \\\\ \\hline p-value &amp; \\Pr(t_{n-2}&gt;t-\\mathrm{ratio}) &amp; \\Pr(t_{n-2}&lt;t-\\mathrm{ratio}) &amp; \\Pr (|t_{n-2}|&gt;|t-\\mathrm{ratio}\\mathit{|}) \\\\\\hline \\end{array} }\\\\ {\\small \\begin{array}{l} \\hline \\text{Notas: Aquí, }t_{n-2} \\text{ es una variable aleatoria distribuida como }t \\text{ con } df=n-2 \\text{ grados de libertad.}\\\\ ~~\\text{La estadística de prueba es }t-\\mathrm{ratio} = (b_1 -d)/se(b_1) . \\\\ \\hline \\end{array} } \\] Otra forma interesante de abordar la cuestión de la importancia de una variable explicativa es a través del coeficiente de correlación. Recuerda que el coeficiente de correlación es una medida de la relación lineal entre \\(x\\) e \\(y\\). Denotemos esta estadística por \\(r(y,x)\\). Esta cantidad no se ve afectada por cambios de escala en ninguna de las variables. Por ejemplo, si multiplicamos la variable \\(x\\) por el número \\(b_1\\), entonces el coeficiente de correlación permanece sin cambios. Además, las correlaciones no cambian con los desplazamientos aditivos. Así, si agregamos un número, digamos \\(b_0\\), a cada variable \\(x\\), entonces el coeficiente de correlación permanece sin cambios. Usar un cambio de escala y un desplazamiento aditivo en la variable \\(x\\) puede utilizarse para producir el valor ajustado \\(\\widehat{y}=b_0+b_1x\\). Por lo tanto, usando la notación, tenemos \\(|r(y,x)|=r(y,\\widehat{y})\\). Así, podemos interpretar que la correlación entre las respuestas y la variable explicativa es igual a la correlación entre las respuestas y los valores ajustados. Esto lleva al siguiente hecho algebraico interesante: \\(R^2=r^2.\\) Es decir, el coeficiente de determinación es igual al cuadrado del coeficiente de correlación. Esto es mucho más fácil de interpretar si uno piensa en \\(r\\) como la correlación entre los valores observados y los ajustados. Consulta el Ejercicio 2.13 para los pasos útiles para confirmar este resultado. 2.5.2 Intervalos de Confianza Los investigadores a menudo citan el mecanismo formal de pruebas de hipótesis para responder a la pregunta: “¿Tiene la variable explicativa una influencia real en la respuesta?” Una pregunta de seguimiento natural es: “¿En qué medida afecta \\(x\\) a \\(y\\)?” Hasta cierto punto, se puede responder utilizando el tamaño del \\(t\\)-ratio o el valor de \\(p\\). Sin embargo, en muchos casos, un intervalo de confianza para la pendiente es más útil. Para introducir los intervalos de confianza para la pendiente, recordemos que \\(b_1\\) es nuestro estimador puntual de la verdadera pendiente desconocida \\(\\beta_1\\). La Sección 2.4 argumentó que este estimador tiene un error estándar \\(se(b_1)\\) y que \\(\\left( b_1-\\beta_1\\right) /se(b_1)\\) sigue una distribución \\(t\\) con \\(n-2\\) grados de libertad. Las declaraciones de probabilidad se pueden invertir para obtener intervalos de confianza. Usando esta lógica, tenemos el siguiente intervalo de confianza para la pendiente \\(\\beta_1\\). Definición. Un intervalo de confianza del \\(100(1-\\alpha)\\)% para la pendiente \\(\\beta_1\\) es \\[\\begin{equation} b_1\\pm t_{n-2,1-\\alpha /2} ~se(b_1). \\tag{2.6} \\end{equation}\\] Al igual que con las pruebas de hipótesis, \\(t_{n-2,1-\\alpha /2}\\) es el percentil (1-\\(\\alpha\\) /2) de la distribución \\(t\\) con \\(df=n-2\\) grados de libertad. Debido a la naturaleza bilateral de los intervalos de confianza, el percentil es 1 - (1 - nivel de confianza) / 2. En este texto, por simplicidad, generalmente usamos un intervalo de confianza del 95%, por lo que el percentil es 1-(1-0.95)/2 = 0.975. El intervalo de confianza proporciona un rango de confiabilidad que mide la utilidad de la estimación. En la Sección 2.1, establecimos que la estimación de la pendiente por mínimos cuadrados para el ejemplo de ventas de lotería es \\(b_1=0.647\\). La interpretación es que si la población de un código postal difiere en 1,000, entonces esperamos que las ventas promedio de lotería difieran en $647. ¿Qué tan confiable es esta estimación? Resulta que \\(se(b_1)=0.0488\\) y, por lo tanto, un intervalo de confianza aproximado del 95% para la pendiente es \\[ 0.647\\pm (2.011)(.0488), \\] o (0.549, 0.745). De manera similar, si la población difiere en 1,000, un intervalo de confianza del 95% para el cambio esperado en las ventas es (549, 745). Aquí, usamos el valor \\(t\\) \\(t_{48,0.975}=2.011\\) porque hay 48 (= \\(n\\)-2) grados de libertad y, para un intervalo de confianza del 95%, necesitamos el percentil 97.5. 2.5.3 Intervalos de Predicción En la Sección 2.1, mostramos cómo usar los estimadores de mínimos cuadrados para predecir las ventas de lotería para un código postal, fuera de nuestra muestra, con una población de 10,000. Dado que la predicción es una tarea tan importante para los actuarios, formalizamos el procedimiento para que pueda ser utilizado regularmente. Para predecir una observación adicional, asumimos que el nivel de la variable explicativa es conocido y se denota por \\(x_{\\ast}\\). Por ejemplo, en nuestro ejemplo anterior de ventas de lotería usamos \\(x_{\\ast} = 10,000\\). También asumimos que la observación adicional sigue el mismo modelo de regresión lineal que las observaciones en la muestra. Usando nuestros estimadores de mínimos cuadrados, nuestra predicción puntual es \\(\\widehat{y}_{\\ast} = b_0 + b_1 x_{\\ast}\\), la altura de la línea de regresión ajustada en \\(x_{\\ast}\\). Podemos descomponer el error de predicción en dos partes: \\[ \\begin{array}{ccccc} \\underbrace{y_{\\ast} - \\widehat{y}_{\\ast}} &amp; = &amp; \\underbrace{\\beta_0 - b_0 + \\left( \\beta_1 - b_1 \\right) x_{\\ast}} &amp; + &amp; \\underbrace{\\varepsilon_{\\ast}} \\\\ {\\small \\text{error de predicción}} &amp; {\\small =} &amp; {\\small \\text{error en la estimación de la }} &amp; {\\small +} &amp; {\\small \\text{desviación de la observación adicional}} \\\\ &amp; &amp; {\\small \\text{línea de regresión en } x}_{\\ast} &amp; &amp; {\\small \\text{respuesta de su media}} \\end{array} \\] Se puede demostrar que el error estándar de la predicción es \\[ se(pred) = s \\sqrt{1+\\frac{1}{n}+\\frac{\\left( x_{\\ast}-\\overline{x}\\right) ^2}{(n-1)s_x^2}}. \\] Al igual que con \\(se(b_1)\\), los términos \\(n^{-1}\\) y \\(\\left(x_{\\ast}-\\overline{x} \\right) ^2/\\left[ (n-1)s_x^2\\right]\\) se acercan a cero a medida que el tamaño de la muestra \\(n\\) se vuelve grande. Por lo tanto, para grandes \\(n\\), tenemos que \\(se(pred)\\approx s\\), lo que refleja que el error en la estimación de la línea de regresión en un punto se vuelve insignificante y la desviación de la respuesta adicional de su media se convierte en la única fuente de incertidumbre. Definición. Un intervalo de predicción del \\(100(1-\\alpha)\\)% en \\(x_{\\ast}\\) es \\[\\begin{equation} \\widehat{y}_{\\ast} \\pm t_{n-2,1-\\alpha /2} ~se(pred) \\tag{2.7} \\end{equation}\\] donde el valor \\(t\\) \\(t_{n-2,1-\\alpha /2}\\) es el mismo que se usa para la prueba de hipótesis y el intervalo de confianza. Por ejemplo, la predicción puntual en \\(x_{\\ast} = 10,000\\) es \\(\\widehat{y}_{\\ast}\\)= 469.7 + 0.647 (10000) = 6,939.7. El error estándar de esta predicción es \\[ se(pred) = 3,792 \\sqrt{1+\\frac{1}{50} + \\frac{\\left( 10,000-9,311\\right)^2}{(50-1)(11,098)^2}} = 3,829.6. \\] Con un valor \\(t\\) igual a 2.011, esto da lugar a un intervalo de predicción aproximado del 95% \\[ 6,939.7 \\pm (2.011)(3,829.6) = 6,939.7 \\pm 7,701.3 = (-761.6, ~14,641.0). \\] Interpretamos estos resultados señalando primero que nuestra mejor estimación de ventas de lotería para un código postal con una población de 10,000 es 6,939.70. Nuestro intervalo de predicción del 95% representa un rango de confiabilidad para esta predicción. Si pudiéramos observar muchos códigos postales, cada uno con una población de 10,000, en promedio esperaríamos que aproximadamente 19 de cada 20, o el 95%, tendrían ventas de lotería entre 0 y 14,641. Es habitual truncar el límite inferior del intervalo de predicción a cero si se considera que los valores negativos de la respuesta son inapropiados. Código R para producir los análisis de la Sección 2.5 # RESULTADOS DE LA SECCIÓN 2.1 model.basiclinearreg&lt;-lm(SALES ~ POP, data = Lot) summary(model.basiclinearreg) # SECCIÓN 2.5.2 INTERVALOS DE CONFIANZA confint(model.basiclinearreg) confint(model.basiclinearreg, level=.90) # SECCIÓN 2.5.3 INTERVALOS DE PREDICCIÓN newdata &lt;- data.frame(POP &lt;- 10000) predict(model.basiclinearreg, newdata, interval=&quot;prediction&quot;) predict(model.basiclinearreg, newdata, interval=&quot;prediction&quot;, level=.90) # PROPORCIONA EL PERCENTIL 97.5 DE UNA DISTRIBUCIÓN T, SOLO PARA COMPROBAR qt(.975, 48) 2.6 Construyendo un Mejor Modelo: Análisis de Residuos Las disciplinas cuantitativas calibran modelos con datos. La estadística lleva esto un paso más allá, utilizando las discrepancias entre las suposiciones y los datos para mejorar la especificación del modelo. Examinaremos las suposiciones del modelo de la Sección 2.2 a la luz de los datos y utilizaremos cualquier desajuste para especificar un mejor modelo; este proceso se conoce como verificación diagnóstica (como cuando vas al médico y él o ella realiza pruebas diagnósticas para revisar tu salud). Comenzaremos con la representación del error de la Sección 2.2. Bajo este conjunto de suposiciones, las desviaciones {\\(\\varepsilon_i\\)} son idénticamente e independientemente distribuidas (i.i.d), y bajo la suposición F5, distribuidas normalmente. Para evaluar la validez de estas suposiciones, se usan los residuos (observados) {\\(e_i\\)} como aproximaciones para las desviaciones (no observadas) {\\(\\varepsilon_i\\)}. El tema básico es que si los residuos están relacionados con una variable o muestran algún otro patrón reconocible, entonces deberíamos poder aprovechar esta información y mejorar la especificación de nuestro modelo. Los residuos deberían contener poca o ninguna información y representar solo la variación natural de la muestra que no se puede atribuir a ninguna fuente específica. Análisis de residuos es el ejercicio de verificar los residuos en busca de patrones. Existen cinco tipos de discrepancias en el modelo que los analistas comúnmente buscan. Si se detectan, las discrepancias pueden corregirse con los ajustes apropiados en la especificación del modelo. Problemas de Especificación del Modelo Falta de Independencia. Puede haber relaciones entre las desviaciones {\\(\\varepsilon_i\\)} de modo que no sean independientes. Heterocedasticidad. La suposición E3 indica que todas las observaciones tienen una variabilidad común (aunque desconocida), conocida como homocedasticidad. Heterocedasticidad es el término usado cuando la variabilidad varía según la observación. Relaciones entre Desviaciones del Modelo y Variables Explicativas. Si una variable explicativa tiene la capacidad de ayudar a explicar la desviación \\(\\varepsilon\\), entonces deberíamos poder usar esta información para predecir mejor \\(y\\). Distribuciones No Normales. Si la distribución de la desviación representa una desviación seria de la normalidad, entonces los procedimientos de inferencia usuales ya no son válidos. Puntos Inusuales. Las observaciones individuales pueden tener un gran efecto en el ajuste del modelo de regresión, lo que significa que los resultados pueden ser sensibles al impacto de una sola observación. Esta lista servirá al lector durante el estudio del análisis de regresión. Por supuesto, con solo una introducción a los modelos básicos aún no hemos visto modelos alternativos que podrían usarse cuando encontramos estas discrepancias en el modelo. En la Parte II de este libro sobre modelos de series temporales, estudiaremos la falta de independencia entre datos ordenados en el tiempo. El Capítulo 5 considerará la heterocedasticidad con más detalle. La introducción a la regresión lineal múltiple en el Capítulo 3 será nuestra primera vista sobre cómo manejar las relaciones entre {\\(\\varepsilon_i\\)} y variables explicativas adicionales. Sin embargo, ya hemos tenido una introducción al efecto de las distribuciones normales, viendo que los gráficos \\(qq\\) pueden detectar la no normalidad y que las transformaciones pueden ayudar a inducir la normalidad aproximada. En esta sección, discutimos los efectos de los puntos inusuales. Gran parte del análisis de residuos se realiza examinando un residuo estandarizado, que es un residuo dividido por su error estándar. Un error estándar aproximado del residuo es \\(s\\); en el Capítulo 3 daremos una definición matemática precisa. Hay dos razones por las que a menudo examinamos residuos estandarizados en lugar de residuos básicos. Primero, si las respuestas están distribuidas normalmente, entonces los residuos estandarizados son aproximadamente realizaciones de una distribución normal estándar. Esto proporciona una distribución de referencia para comparar los valores de los residuos estandarizados. Por ejemplo, si un residuo estandarizado supera dos en valor absoluto, esto se considera inusualmente grande y la observación se llama outlier (punto atípico). Segundo, dado que los residuos estandarizados son adimensionales, podemos transferir la experiencia de un conjunto de datos a otro. Esto es cierto independientemente de si la distribución de referencia normal es aplicable o no. Puntos Atípicos y Puntos de Alta Influencia Otra parte importante del análisis de residuos es la identificación de observaciones inusuales en un conjunto de datos. Debido a que las estimaciones de regresión son promedios ponderados con pesos que varían según la observación, algunas observaciones son más importantes que otras. Esta ponderación es más importante de lo que muchos usuarios del análisis de regresión se dan cuenta. De hecho, el ejemplo a continuación demuestra que una sola observación puede tener un efecto dramático en un gran conjunto de datos. Hay dos direcciones en las que un punto de datos puede ser inusual: la dirección horizontal y la dirección vertical. Por “inusual”, nos referimos a que una observación bajo consideración parece estar lejos de la mayoría del conjunto de datos. Una observación que es inusual en la dirección vertical se llama punto atípico. Una observación que es inusual en la dirección horizontal se llama punto de alta influencia. Una observación puede ser tanto un punto atípico como un punto de alta influencia. Ejemplo: Puntos Atípicos y Puntos de Alta Influencia. Considera el conjunto de datos ficticio de 19 puntos más tres puntos, etiquetados como A, B y C, que se muestra en la Figura 2.7 y Tabla 2.5. Piensa en los primeros 19 puntos como observaciones “buenas” que representan algún tipo de fenómeno. Queremos investigar el efecto de agregar un solo punto aberrante. Tabla 2.5. 19 Puntos Base Más Tres Tipos de Observaciones Inusuales \\[ \\small{ \\begin{array}{c|cccccccccc|ccc} \\hline Variables &amp; &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp; &amp; A &amp; B &amp; C \\\\ \\hline x &amp; 1.5 &amp; 1.7 &amp; 2.0 &amp; 2.2 &amp; 2.5 &amp; 2.5 &amp; 2.7 &amp; 2.9 &amp; 3.0 &amp; 3.5 &amp; 3.4 &amp; 9.5 &amp; 9.5 \\\\ y &amp; 3.0 &amp; 2.5 &amp; 3.5 &amp; 3.0 &amp; 3.1 &amp; 3.6 &amp; 3.2 &amp; 3.9 &amp; 4.0 &amp; 4.0 &amp; 8.0 &amp; 8.0 &amp; 2.5 \\\\ \\hline x &amp; 3.8 &amp; 4.2 &amp; 4.3 &amp; 4.6 &amp; 4.0 &amp; 5.1 &amp; 5.1 &amp; 5.2 &amp; 5.5 &amp; &amp; &amp; &amp; \\\\ y &amp; 4.2 &amp; 4.1 &amp; 4.8 &amp; 4.2 &amp; 5.1 &amp; 5.1 &amp; 5.1 &amp; 4.8 &amp; 5.3 &amp; &amp; &amp; &amp; \\\\ \\hline \\end{array} } \\] Figura 2.7: Gráfico de dispersión de 19 puntos base más tres puntos inusuales, etiquetados A, B, y C. Código R para Producir la Figura 2.7 # EJEMPLO 2.6 PUNTO ATÍPICO OUTLR &lt;- read.csv(&quot;CSVData/OutlierExample.csv&quot;, header=TRUE) # FIGURA 2.7 par(mar=c(4.1,3.1,1.1,.1), cex=1.3) plot(OUTLR$X, OUTLR$Y, xlab=&quot;x&quot;, ylab=&quot;&quot;, xlim=c(0, 10), ylim=c(2, 9), las=1) mtext(&quot;y&quot;, at=5.5,side=2,las=1,cex=1.3, line=2.3) points(4.3, 8.0) text(4.7, 8.0, &quot;A&quot;, cex=1.3) points(9.5, 8.0) text(9.9, 8.0, &quot;B&quot;, cex=1.3) points(9.5, 2.5) text(9.9, 2.5, &quot;C&quot;, cex=1.3) Para investigar el efecto de cada tipo de punto aberrante, Tabla 2.6 resume los resultados de cuatro regresiones separadas. La primera regresión es para los diecinueve puntos base. Las otras tres regresiones utilizan los diecinueve puntos base más cada tipo de observación inusual. Tabla 2.6. Resultados de Cuatro Regresiones \\[ {\\small \\begin{array}{l|rrrrr} \\hline Datos &amp; b_0 &amp; b_1 &amp; s &amp; R^2(\\%) &amp; t(b_1) \\\\ \\hline 19 \\text{ Puntos Base} &amp; 1.869 &amp; 0.611 &amp; 0.288 &amp; 89.0 &amp; 11.71 \\\\ 19 \\text{ Puntos Base} ~+~ A &amp; 1.750 &amp; 0.693 &amp; 0.846 &amp; 53.7 &amp; 4.57 \\\\ 19 \\text{ Puntos Base} ~+~ B &amp; 1.775 &amp; 0.640 &amp; 0.285 &amp; 94.7 &amp; 18.01 \\\\ 19 \\text{ Puntos Base} ~+~ C &amp; 3.356 &amp; 0.155 &amp; 0.865 &amp; 10.3 &amp; 1.44 \\\\ \\hline \\end{array} } \\] Tabla 2.6 muestra que una línea de regresión proporciona un buen ajuste para los diecinueve puntos base. El coeficiente de determinación, \\(R^2\\), indica que alrededor del 89% de la variabilidad ha sido explicada por la línea. El tamaño del error típico, \\(s\\), es de aproximadamente 0.29, pequeño en comparación con la dispersión en los valores de \\(y\\). Además, el cociente \\(t\\) para el coeficiente de la pendiente es grande. Cuando se agrega el punto atípico A a los diecinueve puntos base, la situación empeora dramáticamente. El \\(R^2\\) baja del 89% al 53.7% y \\(s\\) aumenta de aproximadamente 0.29 a alrededor de 0.85. La línea de regresión ajustada en sí no cambia mucho, aunque nuestra confianza en las estimaciones ha disminuido. Un punto atípico es inusual en el valor de \\(y\\), pero “inusual en el valor de \\(y\\)” depende del valor de \\(x\\). Para ver esto, mantén el valor de \\(y\\) del Punto A igual, pero aumenta el valor de \\(x\\) y llama al punto B. Cuando se agrega el punto B a los diecinueve puntos base, la línea de regresión proporciona un ajuste mejor. El punto B está cerca de estar en la línea de ajuste de regresión generada por los diecinueve puntos base. Así, la línea de regresión ajustada y el tamaño del error típico, \\(s\\), no cambian mucho. Sin embargo, \\(R^2\\) aumenta del 89% a casi el 95%. Si pensamos en \\(R^2\\) como \\(1-(Error~SS)/(Total~SS)\\), al agregar el punto B hemos aumentado \\(Total~SS\\), la desviación total cuadrada en los \\(y\\), aunque el \\(Error~SS\\) se mantiene relativamente sin cambios. El punto B no es un punto atípico, pero es un punto de alta influencia. Para mostrar cuán influyente es este punto, reduce considerablemente el valor de \\(y\\) y llama a este el nuevo punto C. Cuando se agrega este punto a los diecinueve puntos base, la situación empeora dramáticamente. El coeficiente \\(R^2\\) baja del 89% al 10%, y el \\(s\\) más que se triplica, de 0.29 a 0.87. Además, los coeficientes de la línea de regresión cambian drásticamente. La mayoría de los usuarios de la regresión al principio no creen que un punto de veinte pueda tener un efecto tan dramático en el ajuste de la regresión. El ajuste de una línea de regresión siempre puede mejorarse eliminando un punto atípico. Si el punto es un punto de alta influencia y no un punto atípico, no está claro si el ajuste mejorará cuando el punto sea eliminado. ¡Simplemente porque puedes mejorar dramáticamente un ajuste de regresión omitiendo una observación no significa que siempre debas hacerlo! El objetivo del análisis de datos es comprender la información en los datos. A lo largo del texto, encontraremos muchos conjuntos de datos donde los puntos inusuales proporcionan alguna de la información más interesante sobre los datos. El objetivo de esta subsección es reconocer los efectos de los puntos inusuales; el Capítulo 5 proporcionará opciones para manejar puntos inusuales en tu análisis. Todas las disciplinas cuantitativas, como contabilidad, economía, programación lineal, etc., practican el arte del análisis de sensibilidad. El análisis de sensibilidad es una descripción de los cambios globales en un sistema debido a un pequeño cambio local en un elemento del sistema. Examinar los efectos de observaciones individuales en el ajuste de regresión es un tipo de análisis de sensibilidad. Ejemplo: Ventas de Lotería – Continuación. La Figura 2.8 muestra un valor atípico; el punto en la parte superior izquierda del gráfico representa un código postal que incluye a Kenosha, Wisconsin. Las ventas para este código postal son inusualmente altas dada su población. Kenosha está cerca de la frontera con Illinois; los residentes de Illinois probablemente participen en la lotería de Wisconsin, lo que aumenta efectivamente el potencial de ventas en Kenosha. Tabla 2.7 resume el ajuste de la regresión tanto con como sin este código postal. Tabla 2.7. Resultados de la Regresión con y sin Kenosha \\[ {\\small \\begin{array}{l|rrrrr} \\hline \\text{Datos} &amp; b_0 &amp; b_1 &amp; s &amp; R^2(\\%) &amp; t(b_1) \\\\ \\hline \\text{Con Kenosha} &amp; 469.7 &amp; 0.647 &amp; 3,792 &amp; 78.5 &amp; 13.26 \\\\ \\text{Sin Kenosha} &amp; -43.5 &amp; 0.662 &amp; 2,728 &amp; 88.3 &amp; 18.82 \\\\ \\hline \\end{array} } \\] Figura 2.8: Gráfico de dispersión de SALES versus POP, con el valor atípico correspondiente a Kenosha marcado. Código R para producir la Figura 2.8 y la Tabla 2.7 Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURA 2.8 par(mar=c(4.1,3.9,2,1),cex=1.1) plot(Lot$POP, Lot$SALES, ylab=&quot;&quot;, las=1, xlab = &quot;POP&quot;) mtext(&quot;SALES&quot;,side=2, at=36000, las=1, cex=1.1) text(5000, 24000, &quot;Kenosha&quot;) # TABLA 2.7 model.basiclinearreg&lt;-lm(SALES ~ POP, Lot) summary(model.basiclinearreg) model.Kenosha&lt;-lm(SALES ~ POP, Lot, subset=-c(9)) summary(model.Kenosha) Para los propósitos de inferencia sobre la pendiente, la presencia de Kenosha no altera los resultados de manera dramática. Ambas estimaciones de la pendiente son cualitativamente similares y los correspondientes valores \\(t\\) son muy altos, muy por encima de los umbrales para la significancia estadística. Sin embargo, hay diferencias notables al evaluar la calidad del ajuste. El coeficiente de determinación, \\(R^2\\), aumentó del 78.5% al 88.3% al eliminar Kenosha. Además, nuestro “desviación típica” \\(s\\) disminuyó en más de $1,000. Esto es particularmente importante si queremos ajustar nuestros intervalos de predicción. Para verificar la exactitud de nuestras suposiciones, también es común revisar la suposición de normalidad. Una forma de hacerlo es mediante el gráfico \\(qq\\), introducido en la Sección 1.2. Los dos paneles en las Figuras 2.9 son gráficos \\(qq\\) con y sin el código postal de Kenosha. Recuerda que los puntos “cercanos” a una línea indican normalidad aproximada. En el panel derecho de la Figura 2.9, la secuencia parece ser lineal, por lo que los residuos están aproximadamente distribuidos de manera normal. Este no es el caso en el panel izquierdo, donde la secuencia de puntos parece aumentar dramáticamente para grandes cuantiles. Lo interesante es que la no-normalidad de la distribución se debe a un solo valor atípico, no a un patrón de sesgo común a todas las observaciones. Figura 2.9: Gráficos \\(qq\\) de los residuos de la Lotería de Wisconsin. El panel izquierdo se basa en los 50 puntos. El panel derecho se basa en 49 puntos, residuos de una regresión después de eliminar Kenosha. Código R para producir la Figura 2.9 #Lot &lt;- read.csv(&quot;../../CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURA 2.9 # TABLA 2.7 model.basiclinearreg&lt;-lm(SALES ~ POP, Lot) #summary(model.basiclinearreg) model.Kenosha&lt;-lm(SALES ~ POP, Lot, subset=-c(9)) #summary(model.Kenosha) par(mfrow=c(1, 2), mar=c(4.1,3.9,1.7,1),cex=1.1) qqnorm(residuals(model.basiclinearreg), main=&quot;&quot;, ylab=&quot;&quot;, las=1, xlab = &quot;Cuantiles Teóricos&quot;) mtext(&quot;Cuantiles Muestrales&quot;, side=2,at=20500,las=1,cex=1.1, adj=.48) qqnorm(residuals(model.Kenosha), main=&quot;&quot;, ylab=&quot;&quot;, las=1, xlab = &quot;Cuantiles Teóricos&quot;) mtext(&quot;Cuantiles Muestrales&quot;, side=2,at=9050,las=1,cex=1.1, adj=.48) 2.7 Aplicación: Modelo de Valoración de Activos Financieros En esta sección, estudiamos una aplicación financiera, el Modelo de Valoración de Activos Financieros, a menudo conocido por el acrónimo CAPM. El nombre es algo engañoso, ya que el modelo realmente trata sobre rendimientos basados en activos de capital, no sobre los precios en sí mismos. Los tipos de activos que examinamos son valores de acciones que se negocian en un mercado activo, como la Bolsa de Valores de Nueva York (NYSE). Para una acción en la bolsa, podemos relacionar los rendimientos con los precios mediante la siguiente expresión: \\[ {\\small \\mathrm{rendimiento =}\\frac{\\mathrm{precio~al~final~de~un~período+dividendos-precio~al~inicio~de~un~período}}{\\mathrm{precio~al~inicio~de~un~período}}. } \\] Si podemos estimar los rendimientos que genera una acción, entonces el conocimiento del precio al inicio de un período financiero genérico nos permite estimar el valor al final del período (precio final más dividendos). Por lo tanto, seguimos la práctica estándar y modelamos los rendimientos de una acción. Una idea intuitivamente atractiva, y una de las características básicas del CAPM, es que debería haber una relación entre el rendimiento de una acción y el mercado. Una justificación es simplemente que si las fuerzas económicas hacen que el mercado mejore, entonces esas mismas fuerzas deberían actuar sobre una acción individual, sugiriendo que también debería mejorar. Como se mencionó anteriormente, medimos el rendimiento de una acción a través del rendimiento. Para medir el rendimiento del mercado, existen varios índices de mercado que resumen el rendimiento de cada bolsa. Usaremos el índice “ponderado por igual” del Standard &amp; Poor’s 500. El Standard &amp; Poor’s 500 es la colección de las 500 empresas más grandes que se negocian en la NYSE, donde “grande” es identificado por Standard &amp; Poor’s, una organización de calificación de servicios financieros. El índice ponderado por igual se define asumiendo que se crea una cartera invirtiendo un dólar en cada una de las 500 empresas. Otra justificación para una relación entre los rendimientos de las acciones y el mercado proviene de la teoría de la economía financiera. Esta es la teoría CAPM, atribuida a Sharpe (1964) y Lintner (1965) y basada en las ideas de diversificación de cartera de Harry Markowitz (1959). Otros factores iguales, los inversionistas desearían seleccionar un rendimiento con un alto valor esperado y una baja desviación estándar, esta última siendo una medida de riesgo. Una de las propiedades deseables de usar desviaciones estándar como medida de riesgo es que es sencillo calcular la desviación estándar de una cartera. Solo es necesario conocer la desviación estándar de cada acción y las correlaciones entre acciones. Una acción notable es una libre de riesgo, es decir, una acción que teóricamente tiene una desviación estándar cero. Los inversionistas a menudo utilizan un bono del Tesoro de EE. UU. a 30 días como una aproximación de una acción libre de riesgo, argumentando que la probabilidad de default del gobierno de EE. UU. dentro de 30 días es insignificante. Positando la existencia de un activo libre de riesgo y algunas otras condiciones suaves, bajo la teoría CAPM existe una frontera eficiente llamada la línea de mercado de valores. Esta frontera especifica el rendimiento mínimo esperado que los inversionistas deberían exigir para un nivel específico de riesgo. Para estimar esta línea, podemos usar la ecuación: \\[ \\mathrm{E}~r = \\beta_0 + \\beta_1 r_m \\] donde \\(r\\) es el rendimiento de la acción y \\(r_m\\) es el rendimiento del mercado. Interpretamos \\(\\beta_1 r_m\\) como una medida de la cantidad de rendimiento de la acción que se atribuye al comportamiento del mercado. Probar la teoría económica, o modelos que surgen de cualquier disciplina, implica recolectar datos. La teoría CAPM trata sobre rendimientos ex-ante (antes del hecho), aunque solo podemos probar con rendimientos ex-post (después del hecho). Antes del hecho, los rendimientos son desconocidos y hay toda una distribución de rendimientos. Después del hecho, solo hay una realización única del rendimiento de la acción y del mercado. Debido a que se requieren al menos dos observaciones para determinar una línea, los modelos CAPM se estiman usando datos de acciones y del mercado recopilados a lo largo del tiempo. De esta manera, se pueden realizar varias observaciones. Para los propósitos de nuestras discusiones, seguimos la práctica estándar en la industria de valores y examinamos precios mensuales. Datos Para ilustrar, considere los rendimientos mensuales durante el período de cinco años desde enero de 1986 hasta diciembre de 1990, inclusive. Específicamente, usamos los rendimientos de la acción de Lincoln National Insurance Corporation como la variable dependiente (\\(y\\)) y los rendimientos del mercado del índice Standard &amp; Poor’s 500 como la variable explicativa (\\(x\\)). En ese momento, Lincoln era una gran compañía de seguros multirama, con sede en el medio oeste de EE. UU., específicamente en Fort Wayne, Indiana. Debido a que era bien conocida por su gestión prudente y estabilidad, es una buena compañía para comenzar nuestro análisis de la relación entre el mercado y una acción individual. Comenzamos interpretando algunas estadísticas básicas, en la Tabla 2.8, en términos de teoría financiera. Primero, un inversionista en Lincoln estará preocupado de que el rendimiento promedio de cinco años, \\(\\overline{y}=0.00510\\), esté por debajo del rendimiento del mercado, \\(\\overline{x}=0.00741\\). Los estudiantes de teoría de intereses reconocen que los rendimientos mensuales se pueden convertir a una base anual usando la capitalización geométrica. Por ejemplo, el rendimiento anual de Lincoln es \\((1.0051)^{12}-1=0.062946\\), o aproximadamente 6.29 por ciento. Esto se compara con un rendimiento anual de 9.26% (= (1\\(00((1.00741)^{12}-1\\))) para el mercado. Una medida de riesgo, o volatilidad, que se usa en finanzas es la desviación estándar. Así, interprete \\(s_y\\) = 0.0859 \\(&gt;\\) 0.05254 = \\(s_x\\) para significar que una inversión en Lincoln es más riesgosa que la del mercado. Otro aspecto interesante de la Tabla 2.8 es que el rendimiento más bajo del mercado, -0.22052, está 4.338 desviaciones estándar por debajo de su promedio ((-0.22052-0.00741)/0.05254 = -4.338). Esto es muy inusual con respecto a una distribución normal. knitr::kable(2, caption = &quot;Silly. Crear una tabla solo para actualizar el contador...&quot;) Tabla 2.2: Silly. Crear una tabla solo para actualizar el contador… x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Tabla 2.3: Silly. x 2 knitr::kable(2, caption = &quot;Silly. &quot;) Tabla 2.4: Silly. x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Tabla 2.5: Silly. x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Tabla 2.6: Silly. x 2 Tabla 2.7: Silly. x 2 Tabla 2.8: Estadísticas Resumen de 60 Observaciones Mensuales Promedio Mediana Desviación Estándar Mínimo Máximo LINCOLN 0.0051 0.0075 0.0859 -0.2803 0.3147 MARKET 0.0074 0.0142 0.0525 -0.2205 0.1275 Fuente: Center for Research on Security Prices, University of Chicago A continuación, examinamos los datos a lo largo del tiempo, como se muestra gráficamente en la Figura 2.10. Estos son gráficos de dispersión de los rendimientos versus el tiempo, llamados gráficos de series temporales. En la Figura 2.10, se puede ver claramente el rendimiento más bajo del mercado y un vistazo rápido al eje horizontal revela que este punto inusual está en octubre de 1987, el momento del conocido colapso del mercado. Figura 2.10: Gráfico de series temporales de los rendimientos de la Lincoln National Corporation y del mercado. Hay 60 rendimientos mensuales durante el período de enero de 1986 a diciembre de 1990. El gráfico de dispersión en la Figura 2.11 resume gráficamente la relación entre el rendimiento de Lincoln y el rendimiento del mercado. El colapso del mercado es claramente evidente en la Figura 2.11 y representa un punto de alta influencia. Con la línea de regresión (descrita a continuación) superpuesta, los dos puntos atípicos que se pueden ver en la Figura 2.10 también son evidentes. A pesar de estas anomalías, el gráfico en la Figura 2.11 sugiere que hay una relación lineal entre los rendimientos de Lincoln y del mercado. Figura 2.11: Gráfico de dispersión del rendimiento de Lincoln versus el rendimiento del índice S&amp;P 500. La línea de regresión está superpuesta, lo que nos permite identificar el colapso del mercado y dos puntos atípicos. Código R para producir la Tabla 2.8 y las Figuras 2.10 y 2.11 CAPM &lt;- read.csv(&quot;CSVData/CAPM.csv&quot;, header=TRUE) # TABLA 2.8 ESTADÍSTICAS RESUMEN Xymat &lt;- data.frame(cbind(CAPM$LINCOLN,CAPM$MARKET)) tableMat &lt;- BookSummStats(Xymat) colnames(tableMat) &lt;- c(&quot;Promedio&quot; , &quot;Mediana&quot; , &quot;Desviación Estándar&quot; , &quot;Mínimo&quot; , &quot;Máximo&quot;) rownames(tableMat) &lt;- c(&quot;LINCOLN&quot;, &quot;MARKET&quot;) #tableMat1 &lt;- format(round(tableMat, digits=0), big.mark = &#39;,&#39;) TableGen1(TableData=tableMat, TextTitle=&#39;Estadísticas Resumen de 60 Observaciones Mensuales&#39;, Align=&#39;r&#39;, Digits=4, ColumnSpec=1:5, ColWidth = ColWidth5) %&gt;% footnote(general = &quot;Center for Research on Security Prices, University of Chicago&quot;, general_title = &quot;Fuente:&quot;, footnote_as_chunk = TRUE) # FIGURA 2.10 par(mar=c(4.1,3.1,2,1),cex=1.1, las=1) foo &lt;- ts(CAPM, freq = 12, start = c(1986, 1)) ts.plot(foo[,2], foo[,3], xlab=&quot;Año&quot;, ylab=&quot;&quot;, type=&quot;o&quot;, lty=c(1, 2)) mtext(&quot;Rendimiento Mensual&quot;, side=2, at=.38,las=1,cex=1.1, adj=.25) legend(1986, 0.3, c(&quot;LINCOLN&quot;, &quot;MARKET&quot;), lty=1:2, cex=0.5) # FIGURA 2.11 par(mar=c(4.1,3.1,1.4,0.2),cex=1.1, las=1) plot(CAPM$MARKET, CAPM$LINCOLN, xlab=&quot;MARKET&quot;, ylab=&quot;&quot;, xlim=c(-0.3, 0.2), ylim=c(-0.3, 0.4),las=1) mtext(&quot;LINCOLN&quot;, side=2,at=0.46,las=1, cex=1.1, adj=.5) reg &lt;- lm(LINCOLN ~ MARKET, data = CAPM) abline(reg) arrows(-0.22, -0.1, -0.22, -0.22,length=0.1, angle = 10) text(-0.22, -0.08, &quot;COLAPSO DE OCTUBRE, 1987&quot;, cex=0.8) arrows(0.1, 0.02, 0, -0.27,length=0.1, angle = 10) arrows(0.1, 0.02, 0.06, 0.3,length=0.1, angle = 10) text(0.16, 0.02, &quot;PUNTOS ATÍPICOS \\n DE 1990&quot;, cex=0.8) Puntos Inusuales Para resumir la relación entre el mercado y el rendimiento de Lincoln, se ajustó un modelo de regresión. La regresión ajustada es \\[ \\widehat{LINCOLN}=-0.00214+0.973 MARKET. \\] El error estándar estimado resultante, \\(s = 0.0696\\), es menor que la desviación estándar de los rendimientos de Lincoln, \\(s_y=0.0859\\). Por lo tanto, el modelo de regresión explica parte de la variabilidad de los rendimientos de Lincoln. Además, el estadístico \\(t\\) asociado con la pendiente \\(b_1\\) resulta ser \\(t(b_1)=5.64\\), lo cual es significativamente alto. Un aspecto decepcionante es que el estadístico \\(R^2=35.4\\%\\) se puede interpretar como que el mercado explica solo un poco más de un tercio de la variabilidad. Por lo tanto, aunque el mercado es claramente un determinante importante, como lo evidencian el alto estadístico \\(t\\), solo proporciona una explicación parcial del rendimiento de los rendimientos de Lincoln. En el contexto del modelo de mercado, podemos interpretar la desviación estándar del mercado, \\(s_x\\), como riesgo no diversificable. Por lo tanto, el riesgo de un valor puede descomponerse en dos componentes: el componente diversificable y el componente del mercado, que es no diversificable. La idea es que, al combinar varios valores, podemos crear una cartera de valores que, en la mayoría de los casos, reducirá el riesgo de nuestras inversiones en comparación con un solo valor. Nuevamente, la razón para tener un valor es que estamos compensados con rendimientos esperados más altos al tener un valor con mayor riesgo. Para cuantificar el riesgo relativo, no es difícil demostrar que \\[\\begin{equation} s_y^2 = b_1^2 s_x^2 + s^2 \\frac{n-2}{n-1}. \\tag{2.8} \\end{equation}\\] El riesgo de un valor se debe al riesgo del mercado más el riesgo de un componente diversificable. Tenga en cuenta que el riesgo del componente del mercado, \\(s_x^2\\), es mayor para los valores con pendientes más grandes. Por esta razón, los inversores consideran que los valores con pendientes \\(b_1\\) mayores que uno son “agresivos” y las pendientes menores que uno como “defensivos”. Análisis de Sensibilidad El resumen anterior plantea inmediatamente dos cuestiones adicionales. Primero, ¿cuál es el efecto del colapso de octubre de 1987 en la ecuación de regresión ajustada? Sabemos que las observaciones inusuales, como el colapso, pueden influir mucho en el ajuste. Con este fin, se volvió a ejecutar la regresión sin la observación correspondiente al colapso. La motivación para esto es que el colapso de octubre de 1987 representa una combinación de eventos altamente inusuales (la interacción de varios programas de comercio automatizado operados por grandes casas de corretaje de valores) que no deseamos representar con el mismo modelo que nuestras otras observaciones. Eliminando esta observación, la regresión ajustada es \\[ \\widehat{LINCOLN} = -0.00181 + 0.956 MARKET, \\] con \\(R^2=26.4\\%\\), \\(t(b_1)=4.52\\), \\(s=0.0702\\) y \\(s_y=0.0811\\). Interpretamos estas estadísticas de la misma manera que el modelo ajustado que incluye el colapso de octubre de 1987. Sin embargo, es interesante notar que la proporción de variabilidad explicada ha disminuido al excluir el punto influyente. Esto sirve para ilustrar un punto importante. Los puntos de alta influencia a menudo son temidos por los analistas de datos porque, por definición, son diferentes de otras observaciones en el conjunto de datos y requieren una atención especial. Sin embargo, al ajustar las relaciones entre variables, también representan una oportunidad porque permiten al analista de datos observar la relación entre variables en rangos más amplios que de otro modo serían posibles. La desventaja es que estas relaciones pueden ser no lineales o seguir un patrón completamente diferente en comparación con las relaciones observadas en la parte principal de los datos. La segunda pregunta planteada por el análisis de regresión es qué se puede decir sobre las circunstancias inusuales que dieron lugar al comportamiento inusual de los rendimientos de Lincoln en octubre y noviembre de 1990. Una característica útil del análisis de regresión es identificar y plantear la pregunta; no la resuelve. Debido a que el análisis señala claramente dos puntos altamente inusuales, sugiere al analista de datos que vuelva y haga algunas preguntas específicas sobre las fuentes de los datos. En este caso, la respuesta es directa. En octubre de 1990, la compañía Travelers’ Insurance, una competidora, anunció que tomaría una gran amortización en su cartera de bienes raíces debido a un número sin precedentes de incumplimientos hipotecarios. El mercado reaccionó rápidamente a esta noticia, y los inversores asumieron que otras grandes compañías de seguros de vida también anunciarían pronto grandes amortizaciones. Anticipando esta noticia, los inversores trataron de vender sus carteras de, por ejemplo, las acciones de Lincoln, lo que provocó una caída en el precio. Sin embargo, resultó que los inversores reaccionaron en exceso a esta noticia y que la cartera de bienes raíces de Lincoln estaba en realidad en buen estado. Así, los precios rápidamente volvieron a sus niveles históricos. 2.8 Salida Computacional Ilustrativa de Regresión Las computadoras y los paquetes de software estadístico que realizan cálculos especializados juegan un papel vital en los análisis estadísticos modernos. Las capacidades informáticas económicas han permitido a los analistas de datos centrarse en las relaciones de interés. Es mucho menos importante especificar modelos que sean atractivos únicamente por su simplicidad computacional en comparación con épocas anteriores a la disponibilidad generalizada de computación económica. Un tema importante de este texto es centrarse en las relaciones de interés y confiar en el software estadístico ampliamente disponible para estimar los modelos que especificamos. Con cualquier paquete de computadora, generalmente las partes más difíciles de operar el paquete son (i) la entrada, (ii) el uso de los comandos y (iii) la interpretación de la salida. Encontrarás que la mayoría de los paquetes estadísticos modernos aceptan archivos en formato de hoja de cálculo o texto, lo que facilita la entrada de datos. Los paquetes de software estadístico para computadoras personales tienen lenguajes de comando basados en menús con facilidades de ayuda en línea fácilmente accesibles. Una vez que decides qué hacer, encontrar los comandos correctos es relativamente fácil. Esta sección proporciona orientación para interpretar la salida de los paquetes estadísticos. La mayoría de los paquetes estadísticos generan salidas similares. A continuación, se presentan tres ejemplos de paquetes estadísticos estándar: EXCEL, SAS y R. El símbolo de anotación “[.]” marca una cantidad estadística que se describe en la leyenda. Así, esta sección proporciona un enlace entre la notación utilizada en el texto y la salida de algunos de los paquetes estadísticos estándar. Salida en EXCEL Regression Statistics Multiple R 0.886283[F] R Square 0.785497[k] Adjusted R Square 0.781028[l] Standard Error 3791.758[j] Observations 50[a] ANOVA df SS MS F Significance F Regression 1[m] 2527165015 [p] 2527165015 [s] 175.773[u] 1.15757E-17[v] Residual 48[n] 690116754.8[q] 14377432.39[t] Total 49[o] 3217281770 [r] Coefficients Standard Error t Stat P-value Intercept 469.7036[b] 702.9061896[d] 0.668230846[f] 0.507187[h] X Variable 1 0.647095[c] 0.048808085[e] 13.25794257[g] 1.16E-17[i] El Sistema SAS The REG Procedure Dependent Variable: SALES Analysis of Variance Sum of Mean Source DF Squares Square F Value Pr &gt; F Model 1[m] 2527165015[p] 2527165015[s] 175.77[u] &lt;.0001[v] Error 48[n] 690116755[q] 14377432[t] Corrected Total 49[o] 3217281770[r] Root MSE 3791.75848[j] R-Square 0.7855[k] Dependent Mean 6494.82900[H] Adj R-Sq 0.7810[l] Coeff Var 58.38119[I] Parameter Estimates Parameter Standard Variable Label DF Estimate Error t Value Pr &gt; |t| Intercept Intercept 1 469.70360[b] 702.90619[d] 0.67[f] 0.5072[h] POP POP 1 0.64709[c] 0.04881[e] 13.26[g] &lt;.0001[i] Salida en R Analysis of Variance Table Response: SALES Df Sum Sq Mean Sq F value Pr(&gt;F) POP 1[m] 2527165015[p] 2527165015[s] 175.77304[u] &lt;2.22e-16[v]*** Residuals 48[n] 690116755[q] 14377432[t] --- Call: lm(formula = SALES ~ POP) Residuals: Min 1Q Median 3Q Max -6047 -1461 -670 486 18229 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 469.7036[b] 702.9062[d] 0.67[f] 0.51 [h] POP 0.6471[c] 0.0488[e] 13.26[g] &lt;2e-16 ***[i] --- Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 Residual standard error: 3792[j] on 48[n] degrees of freedom Multiple R-Squared: 0.785[k], Adjusted R-squared: 0.781[l] F-statistic: 176[u] on 1[m] and 48[n] DF, p-value: &lt;2e-16[v] Definición de Anotación de Leyenda, Símbolo [a] Número de observaciones \\(n\\). [b] La intersección estimada \\(b_0\\). [c] La pendiente estimada \\(b_1\\). [d] El error estándar de la intersección, \\(se(b_0)\\). [e] El error estándar de la pendiente, \\(se(b_1)\\). [f] El valor del \\(t\\) asociado con la intersección, \\(t(b_0) = b_0/se(b_0)\\). [g] El valor del \\(t\\) asociado con la pendiente, \\(t(b_1) = b_1/se(b_1)\\). [h] El valor \\(p\\) asociado con la intersección; aquí, \\(p-value=Pr(|t_{n-2}|&gt;|t(b_0)|)\\), donde \\(t(b_0)\\) es el valor realizado (0.67 aquí) y \\(t_{n-2}\\) tiene una distribución \\(t\\) con \\(df=n-2\\). [i] El valor \\(p\\) asociado con la pendiente; aquí, \\(p-value=Pr(|t_{n-2}|&gt;|t(b_1)|)\\), donde \\(t(b_1)\\) es el valor realizado (13.26 aquí) y \\(t_{n-2}\\) tiene una distribución \\(t\\) con \\(df=n-2\\). [j] La desviación estándar residual, \\(s\\). [k] El coeficiente de determinación, \\(R^2\\). [l] El coeficiente de determinación ajustado por grados de libertad, \\(R_{a}^2\\). (Este término se definirá en el Capítulo 3.) [m] Grados de libertad para el componente de regresión. Esto es 1 para una variable explicativa. [n] Grados de libertad para el componente de error, \\(n-2\\), para la regresión con una variable explicativa. [o] Grados de libertad totales, \\(n-1\\). [p] La suma de cuadrados de la regresión, \\(Regression~SS\\). [q] La suma de cuadrados del error, \\(Error~SS\\). [r] La suma total de cuadrados, \\(Total~SS\\). [s] El cuadrado medio de la regresión, \\(Regression~MS = Regression~SS/1\\), para una variable explicativa. [t] El cuadrado medio del error, \\(s^2=Error~MS = Error~SS/(n-2)\\), para una variable explicativa. [u] El \\(F-ratio=(Regression~MS)/(Error~MS)\\). (Este término se definirá en el Capítulo 3.) [v] El valor \\(p\\) asociado con el \\(F-ratio\\). (Este término se definirá en el Capítulo 3.) [w] El número de observación, \\(i\\). [x] El valor de la variable explicativa para la \\(i\\)-ésima observación, \\(x_i\\). [y] La respuesta para la \\(i\\)-ésima observación, \\(y_i\\). [z] El valor ajustado para la \\(i\\)-ésima observación, \\(\\widehat{y}_i\\). [A] El error estándar del ajuste, \\(se(\\widehat{y}_i)\\). [B] El residual para la \\(i\\)-ésima observación, \\(e_i\\). [C] El residual estandarizado para la \\(i\\)-ésima observación, \\(e_i/se(e_i)\\). El error estándar \\(se(e_i)\\) se definirá en la Sección 5.3.1. [F] El coeficiente de correlación múltiple es la raíz cuadrada del coeficiente de determinación, \\(R=\\sqrt{R^2}\\). Esto se definirá en el Capítulo 3. [G] El coeficiente estandarizado es \\(b_1s_x/s_y\\). Para regresión con una variable explicativa, esto es equivalente a \\(r\\), el coeficiente de correlación. [H] La respuesta promedio, \\(\\overline{y}\\). [I] El coeficiente de variación de la respuesta es \\(s_y/\\overline{y}\\). SAS imprime \\(100s_y/\\overline{y}\\). 2.9 Lecturas Adicionales y Referencias Relativamente pocas aplicaciones de la regresión son básicas en el sentido de que usan solo una variable explicativa; el propósito del análisis de regresión es reducir las relaciones complejas entre muchas variables. La Sección 2.7 describe una excepción importante a esta regla general, el modelo financiero CAPM; consulta a Panjer et al. (1998) para descripciones actuariales adicionales de este modelo. Campbell et al. (1997) ofrece una perspectiva desde la econometría financiera. Referencias del Capítulo Anscombe, Frank (1973). Graphs in statistical analysis. The American Statistician 27, 17-21. Campbell, John Y., Andrew W. Lo and A. Craig MacKinlay (1997). The Econometrics of Financial Markets. Princeton University Press, Princeton, New Jersey. Frees, Edward W. and Tom W. Miller (2003). Sales forecasting using longitudinal data models. International Journal of Forecasting 20, 97-111. Goldberger, Arthur (1991). A Course in Econometrics. Harvard University Press, Cambridge. Koch, Gary J. (1985). A basic demonstration of the [-1, 1] range for the correlation coefficient. American Statistician 39, 201-202. Linter, J. (1965). The valuation of risky assets and the selection of risky investments in stock portfolios and capital budgets. Review of Economics and Statistics, 13-37. Manistre, B. John and Geoffrey H. Hancock (2005). Variance of the CTE estimator. North American Actuarial Journal 9(2), 129-156. Markowitz, Harry (1959). Portfolio Selection: Efficient Diversification of Investments. John Wiley, New York. Panjer, Harry H., Phelim P. Boyle, Samuel H. Cox, Daniel Dufresne, Hans U. Gerber, Heinz H. Mueller, Hal W. Pedersen, Stanley R. Pliska, Michael Sherris, Elias S. Shiu and Ken S. Tan (1998). Financial Economics: With Applications to Investment, Insurance and Pensions. Society of Actuaries, Schaumburg, Illinois. Pearson, Karl (1895). Royal Society Proceedings 58, 241. Serfling, Robert J. (1980). Approximation Theorems of Mathematical Statistics. John Wiley and Sons, New York. Sharpe, William F. (1964). Capital asset prices: A theory of market equilibrium under risk. Journal of Finance, 425-442. Stigler, Steven M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. Harvard University Press, Cambridge, MA. 2.10 Ejercicios Secciones 2.1-2.2 2.1 Considera el siguiente conjunto de datos \\[ \\begin{array}{l|ccc} \\hline i &amp; 1 &amp; 2 &amp; 3 \\\\ \\hline x_i &amp; 2 &amp; -6 &amp; 7 \\\\ y_i &amp; 3 &amp; 4 &amp; 6\\\\ \\hline \\end{array} \\] Ajusta una línea de regresión utilizando el método de mínimos cuadrados. Determina \\(r\\), \\(b_1\\) y \\(b_0\\). 2.2 Una relación perfecta, pero sin correlación. Considera la relación cuadrática \\(y=x^2\\), con datos \\[ \\begin{array}{l|ccccc} \\hline i &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5\\\\ \\hline x_i &amp; -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 \\\\ y_i &amp; 4 &amp; 1 &amp; 0 &amp; 1 &amp; 4\\\\ \\hline \\end{array} \\] Produce un gráfico aproximado para este conjunto de datos. Verifica que el coeficiente de correlación es \\(r=0\\). 2.3 Acotación del coeficiente de correlación. Utiliza los siguientes pasos para demostrar que \\(r\\) está acotado entre -1 y 1 (Estos pasos son de Koch, 1990). Deja que \\(a\\) y \\(c\\) sean constantes genéricas. Verifica \\[\\begin{eqnarray*} 0 &amp; \\leq &amp; \\frac{1}{n-1}\\sum_{i=1}^{n}\\left( a\\frac{x_i-\\overline{x}}{s_x}-c \\frac{y_i-\\overline{y}}{s_y}\\right) ^2 \\\\ &amp;=&amp; a^2+c^2-2acr. \\end{eqnarray*}\\] Utiliza los resultados del apartado (a) para demostrar que \\(2ac(r-1)\\leq (a-c)^2.\\) Al tomar \\(a=c\\), utiliza el resultado del apartado (b) para demostrar que \\(r\\leq 1\\). Al tomar \\(a=-c\\), utiliza los resultados del apartado (b) para demostrar que \\(r\\geq -1\\). ¿En qué condiciones es \\(r=-1\\)? ¿En qué condiciones es \\(r=1\\)? 2.4 Los coeficientes de regresión son sumas ponderadas. Demuestra que el término de intercepto, \\(b_0\\), puede expresarse como una suma ponderada de las variables dependientes. Es decir, demuestra que \\(b_0=\\sum_{i=1}^{n}w_{i,0}y_i.\\) Además, expresa los pesos en términos de los pesos de la pendiente, \\(w_i\\). 2.5 Otra expresión para la pendiente como una suma ponderada Utilizando álgebra, establece una expresión alternativa \\[ b_1=\\frac{\\sum_{i=1}^{n}weight_i~slope_i}{ \\sum_{i=1}^{n}weight_i}. \\] Aquí, \\(slope_i\\) es la pendiente entre \\((x_i,y_i)\\) y \\((\\bar{x},\\bar{y})\\). Da una forma precisa para el peso \\(weight_i\\) como una función de la variable explicativa \\(x\\). Supón que \\(\\bar{x} = 4, \\bar{y} = 3, x_1 = 2 \\text{ y } y_1= 6\\). Determina la pendiente y el peso para la primera observación, es decir, \\(slope_1\\) y \\(weight_1\\). 2.6 Considera dos variables, \\(y\\) y \\(x\\). Realiza una regresión de \\(y\\) sobre \\(x\\) para obtener un coeficiente de pendiente que llamaremos \\(b_{1,x,y}\\). Realiza otra regresión de \\(x\\) sobre \\(y\\) para obtener un coeficiente de pendiente que llamaremos \\(b_{1,y,x}\\). Demuestra que el coeficiente de correlación entre \\(x\\) y \\(y\\) es la media geométrica de los dos coeficientes de pendiente según el signo, es decir, demuestra que \\(|r|=\\sqrt{ b_{1,x,y}b_{1,y,x}}.\\) 2.7 Regresión a través del origen. Considera el modelo \\(y_i=\\beta_1 x_i + \\varepsilon_i\\), es decir, regresión con una variable explicativa sin el término de intercepto. Este modelo se llama regresión a través del origen porque la verdadera línea de regresión \\(\\mathrm{E}y = \\beta_1 x\\) pasa por el origen (el punto (0, 0)). Para este modelo, la estimación de mínimos cuadrados de \\(\\beta_1\\) es ese número \\(b_1\\) que minimiza la suma de cuadrados \\(\\mathrm{SS}(b_1^{\\ast} )=\\sum_{i=1}^{n}\\left( y_i - b_1^{\\ast}x_i\\right) ^2.\\) Verifica que \\[ b_1 = \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sum_{i=1}^{n}x_i^2}. \\] Considera el modelo \\(y_i=\\beta_1 z_i^2 + \\varepsilon_i\\), un modelo cuadrático que pasa por el origen. Utiliza el resultado del apartado (a) para determinar la estimación de mínimos cuadrados de \\(\\beta_1\\). 2.8 a. Demuestra que \\[ s_y^2=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^2= \\frac{1}{n-1}\\left( \\sum_{i=1}^{n}y_i^2-n\\overline{y}^2\\right) . \\] Sigue los mismos pasos para demostrar que \\(\\sum_{i=1}^{n}\\left( y_i - \\overline{y} \\right) \\left( x_i-\\overline{x}\\right) =\\sum_{i=1}^{n} x_i y_i - n \\overline{x}~\\overline{y}.\\) Demuestra que \\[ b_{1}=\\frac{\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) \\left( x_i- \\overline{x}\\right) }{\\sum_{i=1}^{n}\\left( x_i - \\overline{x} \\right) ^2} \\] Establece la fórmula comúnmente utilizada \\[ b_{1}= \\frac{\\sum_{i=1}^{n}x_iy_i-n\\overline{x}~\\overline{y}} {\\sum_{i=1}^{n}x_i^2 - n\\overline{x}^2}. \\] 2.9 Interpretación de los coeficientes asociados con una variable explicativa binaria. Supón que \\(x_i\\) solo toma los valores 0 y 1. De las \\(n\\) observaciones, \\(n_1\\) toman el valor \\(x=0\\). Estas \\(n_1\\) observaciones tienen un valor promedio \\(y\\) de \\(\\overline{y}_1\\). Las restantes \\(n-n_1\\) observaciones tienen el valor \\(x=1\\) y un valor promedio \\(y\\) de \\(\\overline{y}_2\\). Utiliza el Ejercicio 2.8 para demostrar que \\(b_1 = \\overline{y}_2 - \\overline{y}_1.\\) 2.10 Utilización de Hogares de Cuidado. Este ejercicio considera los datos de hogares de cuidado proporcionados por el Departamento de Salud y Servicios Familiares de Wisconsin (DHFS) y descritos en el Ejercicio 1.2. Parte 1: Utiliza los datos del año 2000 y realiza el siguiente análisis. Correlaciones a(i). Calcula la correlación entre TPY y LOGTPY. Comenta tu resultado. a(ii). Calcula la correlación entre TPY, NUMBED y SQRFOOT. ¿Parecen estas variables altamente correlacionadas? a(iii). Calcula la correlación entre TPY y NUMBED/10. Comenta tu resultado. Diagramas de dispersión. Grafica TPY versus NUMBED y TPY versus SQRFOOT. Comenta los gráficos. Regresión lineal básica. c(i). Ajusta un modelo de regresión lineal básico usando TPY como variable de resultado y NUMBED como variable explicativa. Resume el ajuste citando el coeficiente de determinación, \\(R^2\\), y el estadístico \\(t\\) para NUMBED. c(ii). Repite c(i), usando SQRFOOT en lugar de NUMBED. En términos de \\(R^2\\), ¿cuál modelo se ajusta mejor? c(iii). Repite c(i), usando LOGTPY como variable de resultado y LOG(NUMBED) como variable explicativa. c(iv). Repite c(iii), usando LOGTPY como variable de resultado y LOG(SQRFOOT) como variable explicativa. Parte 2: Ajusta el modelo en la Parte 1.c(i) usando datos de 2001. ¿Son los patrones estables a lo largo del tiempo? Secciones 2.3-2.4 2.11 Supón que, para un tamaño de muestra de \\(n\\) = 3, tienes \\(e_2\\) = 24 y \\(e_{3}\\) = -1. Determina \\(e_{1}\\). 2.12 Supón que \\(r=0\\), \\(n=15\\) y \\(s_y = 10\\). Determina \\(s\\). 2.13 El coeficiente de correlación y el coeficiente de determinación. Usa los siguientes pasos para establecer una relación entre el coeficiente de determinación y el coeficiente de correlación. Muestra que \\(\\widehat{y}_i-\\overline{y}=b_1(x_i-\\overline{x}).\\) Usa el apartado (a) para mostrar que \\(Regress~SS=\\sum_{i=1}^{n}\\left(\\widehat{y}_i - \\overline{y} \\right)^2 = b_1^2s_x^2(n-1).\\) Usa el apartado (b) para establecer que \\(R^2=r^2.\\) 2.14 Muestra que el residuo promedio es cero, es decir, muestra que \\(n^{-1}\\sum_{i=1}^{n} e_i=0.\\) 2.15 Correlación entre residuos y variables explicativas. Considera una secuencia genérica de pares de números \\((x_1,y_1)\\), …, \\((x_n,y_n)\\) con el coeficiente de correlación calculado como \\(r(y,x)=\\left[ (n-1)s_ys_x\\right] ^{-1}\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) \\left( x_i-\\overline{x}\\right) .\\) Supón que \\(\\overline{y}=0\\), \\(\\overline{x}=0\\) o ambos \\(\\overline{x}\\) y \\(\\overline{y}=0\\). Luego, verifica que \\(r(y,x)=0\\) implica \\(\\sum_{i=1}^{n}y_i x_i=0\\) y viceversa. Muestra que la correlación entre los residuos y las variables explicativas es cero. Haz esto usando la parte (a) del Ejercicio 2.13 para mostrar que \\(\\sum_{i=1}^{n} x_i e_i = 0\\) y luego aplica la parte (a). Muestra que la correlación entre los residuos y los valores ajustados es cero. Haz esto mostrando que \\(\\sum_{i=1}^n \\widehat{y}_i e_i = 0\\) y luego aplica la parte (a). 2.16 Correlación y estadísticas \\(t\\). Usa los siguientes pasos para establecer una relación entre el coeficiente de correlación y el estadístico \\(t\\) para la pendiente. Usa álgebra para verificar que \\[ R^2=1-\\frac{n-2}{n-1}\\frac{s^2}{s_y^2}. \\] Usa la parte (a) para establecer la siguiente fórmula rápida para \\(s\\), \\[s = s_y \\sqrt{(1-r^2)\\frac{n-1}{n-2}}.\\] Usa la parte (b) para mostrar que \\[ t(b_1) = \\sqrt{n-2}\\frac{r}{\\sqrt{1-r^2}}. \\] Secciones 2.6-2.7 2.17 Efectos de un punto inusual. Estás analizando un conjunto de datos de tamaño \\(n=100\\). Has realizado un análisis de regresión usando una variable predictora y notas que el residuo para la décima observación es inusualmente grande. Supón que, de hecho, resulta que \\(e_{10}=8s\\). ¿Qué porcentaje de la suma de cuadrados de los errores, \\(Error~SS\\), se debe a la décima observación? Supón que \\(e_{10}=4s\\). ¿Qué porcentaje de la suma de cuadrados de errores, \\(Error~SS\\), se debe a la décima observación? Supón que reduces el conjunto de datos a tamaño \\(n=20\\). Después de realizar la regresión, resulta que todavía tenemos \\(e_{10}=4s\\). ¿Qué porcentaje de la suma de cuadrados de errores, \\(Error~SS\\), se debe a la décima observación? 2.18 Considera un conjunto de datos de 20 observaciones con las siguientes estadísticas resumen: \\(\\overline{x}=0\\), \\(\\overline{y}=9\\), \\(s_x=1\\) y \\(s_y=10\\). Realizas una regresión usando una variable y determinas que \\(s=7\\). Determina el error estándar de una predicción en \\(x_{\\ast}=1.\\) 2.19 Las estadísticas resumen pueden ocultar relaciones importantes. Los datos en Tabla 2.9 son de Anscombe (1973). El propósito de este ejercicio es demostrar cómo graficar los datos puede revelar información importante que no es evidente en las estadísticas numéricas resumen. Tabla 2.9. Datos de Anscombe (1973) \\[ {\\small \\begin{array}{c|rrrrrr} \\hline obs &amp; &amp; &amp; &amp; &amp; &amp; \\\\ num &amp; x_1 &amp; y_1 &amp; y_2 &amp; y_3 &amp; x_2 &amp; y_4 \\\\ \\hline 1 &amp; 10 &amp; 8.04 &amp; 9.14 &amp; 7.46 &amp; 8 &amp; 6.58 \\\\ 2 &amp; 8 &amp; 6.95 &amp; 8.14 &amp; 6.77 &amp; 8 &amp; 5.76 \\\\ 3 &amp; 13 &amp; 7.58 &amp; 8.74 &amp; 12.74 &amp; 8 &amp; 7.71 \\\\ 4 &amp; 9 &amp; 8.81 &amp; 8.77 &amp; 7.11 &amp; 8 &amp; 8.84 \\\\ 5 &amp; 11 &amp; 8.33 &amp; 9.26 &amp; 7.81 &amp; 8 &amp; 8.47 \\\\ 6 &amp; 14 &amp; 9.96 &amp; 8.10 &amp; 8.84 &amp; 8 &amp; 7.04 \\\\ 7 &amp; 6 &amp; 7.24 &amp; 6.13 &amp; 6.08 &amp; 8 &amp; 5.25 \\\\ 8 &amp; 4 &amp; 4.26 &amp; 3.10 &amp; 5.39 &amp; 8 &amp; 5.56 \\\\ 9 &amp; 12 &amp; 10.84 &amp; 9.13 &amp; 8.15 &amp; 8 &amp; 7.91 \\\\ 10 &amp; 7 &amp; 4.82 &amp; 7.26 &amp; 6.42 &amp; 8 &amp; 6.89 \\\\ 11 &amp; 5 &amp; 5.68 &amp; 4.74 &amp; 5.73 &amp; 19 &amp; 12.50 \\\\ \\hline \\end{array} } \\] Calcula los promedios y desviaciones estándar de cada columna de datos. Verifica que los promedios y desviaciones estándar de cada una de las columnas \\(x\\) son iguales, dentro de dos decimales, y de manera similar para cada una de las columnas \\(y\\). Realiza cuatro regresiones, (1) \\(y_{1}\\) sobre \\(x_{1}\\), (2) \\(y_2\\) sobre \\(x_{1}\\), (3) \\(y_{3}\\) sobre \\(x_{1}\\) y (4) \\(y_{4}\\) sobre \\(x_2\\). Verifica, para cada uno de los cuatro ajustes de regresión, que \\(b_0\\approx 3.0\\), \\(b_{1}\\approx 0.5\\), \\(s\\approx 1.237\\) y \\(R^2\\approx 0.677\\), dentro de dos decimales. Produce diagramas de dispersión para cada uno de los cuatro modelos de regresión que ajustaste en el apartado (b). Discute el hecho de que los modelos de regresión ajustados en el apartado (b) implican que los cuatro conjuntos de datos son similares, aunque los cuatro diagramas de dispersión producidos en el apartado (c) muestran una historia dramáticamente diferente. 2.20 Utilización de Hogares de Cuidado. Este ejercicio considera los datos de hogares de cuidado proporcionados por el Departamento de Salud y Servicios Familiares de Wisconsin (DHFS) y descritos en el Ejercicio 1.2 y 2.10. Decides examinar la relación entre los años totales de pacientes (LOGTPY) y el número de camas (LOGNUMBED), ambos en unidades logarítmicas, usando datos del año 2001. Estadísticas descriptivas. Crea estadísticas descriptivas básicas para cada variable. Resume la relación mediante un estadístico de correlación y un diagrama de dispersión. Ajusta el modelo lineal básico. Cita las estadísticas descriptivas básicas, incluye el coeficiente de determinación, el coeficiente de regresión para LOGNUMBED y el estadístico \\(t\\) correspondiente. Pruebas de hipótesis. Prueba las siguientes hipótesis al nivel de significancia del 5% usando un estadístico \\(t\\). También calcula el valor \\(p\\) correspondiente. c(i). Prueba \\(H_0: \\beta_1 = 0\\) frente a \\(H_a: \\beta_1 \\neq 0\\). c(ii). Prueba \\(H_0: \\beta_1 = 1\\) frente a \\(H_a: \\beta_1 \\neq 1\\). c(iii). Prueba \\(H_0: \\beta_1 = 1\\) frente a \\(H_a: \\beta_1 &gt; 1\\). c(iv). Prueba \\(H_0: \\beta_1 = 1\\) frente a \\(H_a: \\beta_1 &lt; 1\\). Estás interesado en el efecto que un cambio marginal en LOGNUMBED tiene sobre el valor esperado de LOGTPY. d(i). Supón que hay un cambio marginal en LOGNUMBED de 2. Proporciona una estimación puntual del cambio esperado en LOGTPY. d(ii). Proporciona un intervalo de confianza del 95% correspondiente a la estimación puntual en la parte d(i). d(iii). Proporciona un intervalo de confianza del 99% correspondiente a la estimación puntual en la parte d(i). En un número especificado de camas estimado en \\(x_{*} = 100\\), haz lo siguiente: e(i). Encuentra el valor predicho de LOGTPY. e(ii). Obtén el error estándar de la predicción. e(iii). Obtén un intervalo de predicción del 95% para tu predicción. e(iv). Convierte la predicción puntual en la parte e(i) y el intervalo de predicción obtenido en la parte e(iii) en años totales de personas (mediante exponenciación). e(v). Obtén un intervalo de predicción como en la parte e(iv), correspondiente a un nivel del 90% (en lugar del 95%). 2.21 Ofertas Públicas Iniciales. Como analista financiero, deseas convencer a un cliente de las ventajas de invertir en empresas que acaban de ingresar a una bolsa de valores, en una OPI (oferta pública inicial). Por lo tanto, reúnes datos de 116 empresas que fijaron precios durante el período de seis meses del 1 de enero de 1998 al 1 de junio de 1998. Al mirar estos datos históricos recientes, puedes calcular RETURN, el retorno de la empresa en un año (en porcentaje). También estás interesado en observar características financieras de la empresa que puedan ayudarte a entender (y predecir) el retorno. Inicialmente examinas REVENUE, los ingresos de la empresa en 1997 en millones de dólares. Desafortunadamente, esta variable no estaba disponible para seis empresas. Por lo tanto, las estadísticas a continuación son para las 110 empresas que tienen tanto REVENUE como RETURN. Además, la Tabla 2.9 proporciona información sobre los ingresos logarítmicos (naturales), denominados como LnREV, y el precio inicial de la acción, denominado PRICEIPO. Tabla 2.9: Estadísticas Resumen de Cada Variable Media Mediana Desviación Estándar Mínimo Máximo RETURN 0.106 -0.130 0.824 -0.938 4.333 REV 134.487 39.971 261.881 0.099 1455.761 LnREV 3.686 3.688 1.698 -2.316 7.283 PRICEIPO 13.195 13.000 4.694 4.000 29.000 Hipotetizas que las empresas más grandes, medida por ingresos, son más estables y, por lo tanto, deberían tener mayores retornos. Has determinado que la correlación entre RETURN y REVENUE es -0.0175. a(i). Calcula el ajuste de mínimos cuadrados usando REVENUE para predecir RETURN. Determina \\(b_0\\) y \\(b_1\\). a(ii). Para Hyperion Telecommunications, los ingresos son 95.55 (millones de dólares). Calcula el RETURN ajustado usando el ajuste de regresión en la parte a(i). Tabla 2.11. Resultados de la Regresión con Ingresos Logarítmicos \\[ {\\small \\begin{array}{l|rrr} \\hline &amp; &amp; \\text{Error} &amp; \\\\ \\text{Variable} &amp; \\text{Coeficiente} &amp; \\text{Estándar} &amp; t-\\text{estadístico} \\\\ \\hline \\text{INTERCEPTO} &amp; 0.438 &amp; 0.186 &amp; 2.35\\\\ \\text{LnREV} &amp; -0.090 &amp; 0.046 &amp; -1.97 \\\\ \\hline s = 0.8136, &amp; R^2 = 0.03452 \\\\ \\hline \\end{array} } \\] Tabla 2.11 resume una regresión utilizando ingresos y rendimientos logarítmicos. b(i). Supón que usas LnREV para predecir RETURN. Calcula el RETURN ajustado bajo este modelo de regresión. ¿Es igual a tu respuesta en la parte a(ii)? b(ii) ¿Afectan significativamente los ingresos logarítmicos a los retornos? Para ello, proporciona una prueba formal de hipótesis. Expón tus hipótesis nula y alternativa, el criterio de toma de decisiones y la regla de toma de decisiones. Usa un nivel de significancia del 10%. b(iii). Hipotetizas que, manteniendo todo constante, las empresas con mayores ingresos serán más estables y, por lo tanto, tendrán un mayor retorno inicial. Por lo tanto, deseas considerar la hipótesis nula de ninguna relación entre LnREV y RETURN frente a la hipótesis alternativa de que hay una relación positiva entre LnREV y RETURN. Para ello, proporciona una prueba formal de hipótesis. Expón tus hipótesis nula y alternativa, el criterio de toma de decisiones y la regla de toma de decisiones. Usa un nivel de significancia del 10%. Determina la correlación entre LnREV y RETURN. Asegúrate de indicar si esta correlación es positiva, negativa o cero. Estás considerando invertir en una empresa que tiene LnREV = 2 (por lo que los ingresos son \\(e^2\\) = 7.389 millones de dólares). d(i). Usando el modelo de regresión ajustado, determina la predicción puntual de mínimos cuadrados. d(ii). Determina el intervalo de predicción del 95% correspondiente a tu predicción en la parte d(i). El \\(R^2\\) del modelo de regresión ajustado es un decepcionante 3.5%. Parte de la dificultad se debe a la observación número 59, la Corporación Inktomi. Las ventas de Inktomi están en el 12º lugar más bajo del conjunto de datos, con LnREV = 1.76 (por lo que los ingresos son \\(e^{1.76} = 5.79\\) millones de dólares), pero tiene el mayor retorno en el primer año, con RETURN = 433.33. e(i). Calcula el residuo para esta observación. e(ii). ¿Qué proporción de la variabilidad no explicada (suma de cuadrados de errores) representa esta observación? e(iii). Define la idea de una observación de alto apalancamiento. e(iv). ¿Se consideraría esta observ ación como una observación de alto apalancamiento? Justifica tu respuesta. 2.22 Esperanzas de Vida Nacionales. Continuamos el análisis iniciado en el Ejercicio 1.7 examinando la relación entre \\(y= LIFEEXP\\) y \\(x=FERTILITY\\), mostrado en la Figura 2.12. Ajusta un modelo de regresión lineal de \\(LIFEEXP\\) usando la variable explicativa \\(x=FERTILITY\\). Figura 2.12: Gráfico de FERTILITY versus LIFEEXP. EE.UU. tiene una tasa de FERTILITY de 2.0. Determina la esperanza de vida ajustada. La nación insular Dominica no reportó una tasa de FERTILITY y, por lo tanto, no se incluyó en la regresión. Supón que su tasa de FERTILITY es 2.0. Proporciona un intervalo de predicción del 95% para la esperanza de vida en Dominica. China tiene una tasa de FERTILITY de 1.7 y una esperanza de vida de 72.5. Determina el residuo bajo el modelo. ¿Cuántos múltiplos de \\(s\\) está este residuo alejado de cero? Supón que tu hipótesis previa es que la pendiente de FERTILITY es -6.0 y deseas probar la hipótesis nula de que la pendiente ha aumentado (es decir, la pendiente es mayor que -6.0). Prueba esta hipótesis al nivel de significancia del 5%. También calcula un valor \\(p\\) aproximado. 2.11 Suplemento Técnico - Elementos del Álgebra de Matrices Los ejemplos son una herramienta excelente para introducir temas técnicos como la regresión. Sin embargo, este capítulo también ha utilizado álgebra, así como probabilidad y estadística básica, para darte una comprensión más profunda del análisis de regresión. A partir de ahora, estudiaremos relaciones multivariantes. Con muchas cosas ocurriendo simultáneamente en varias dimensiones, el álgebra ya no es útil para proporcionar información. En cambio, necesitaremos el álgebra de matrices. Este suplemento ofrece una breve introducción al álgebra de matrices para que puedas estudiar los capítulos de regresión lineal de este texto. El Apéndice A3 define conceptos adicionales de matrices. 2.11.1 Definiciones Básicas Una matriz es una tabla rectangular de números organizados en filas y columnas (el plural de matriz es matrices). Por ejemplo, considera los ingresos y la edad de 3 personas. \\[ \\mathbf{A}= \\begin{array}{c} Fila~1 \\\\ Fila~2 \\\\ Fila~3 \\end{array} \\overset{ \\begin{array}{cc} ~~~Col~1~ &amp; Col~2 \\end{array} }{\\left( \\begin{array}{cc} 6,000 &amp; 23 \\\\ 13,000 &amp; 47 \\\\ 11,000 &amp; 35 \\end{array} \\right) } \\] Aquí, la columna 1 representa el ingreso y la columna 2 representa la edad. Cada fila corresponde a un individuo. Por ejemplo, el primer individuo tiene 23 años y un ingreso de $6,000. El número de filas y columnas se llama la dimensión de la matriz. Por ejemplo, la dimensión de la matriz \\(\\mathbf{A}\\) anterior es \\(3\\times 2\\) (se lee 3 “por” 2). Esto significa 3 filas y 2 columnas. Si quisiéramos representar los ingresos y la edad de 100 personas, entonces la dimensión de la matriz sería \\(100\\times 2\\). Es conveniente representar una matriz usando la notación \\[ \\mathbf{A}=\\left( \\begin{array}{cc} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\\\ a_{31} &amp; a_{32} \\end{array} \\right) . \\] Aquí, \\(a_{ij}\\) es el símbolo para el número en la \\(i\\)-ésima fila y \\(j\\)-ésima columna de \\(\\mathbf{A}\\). En general, trabajamos con matrices de la forma \\[ \\mathbf{A}=\\left( \\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nk} \\end{array} \\right) . \\] En este caso, la matriz \\(\\mathbf{A}\\) tiene dimensión \\(n\\times k\\). Un vector es una matriz especial. Un vector fila es una matriz que contiene solo 1 fila (\\(k=1\\)). Un vector columna es una matriz que contiene solo 1 columna (\\(n=1\\)). Por ejemplo, \\[ \\text{vector columna}\\rightarrow \\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\end{array} \\right) ~~~~\\text{vector fila}\\rightarrow \\left( \\begin{array}{ccccc} 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\end{array} \\right) . \\] Observa que el vector fila ocupa mucho menos espacio en una página impresa que el vector columna correspondiente. Una operación básica que relaciona estas dos cantidades es la transposición. La transposición de una matriz \\(\\mathbf{A}\\) se define intercambiando las filas y columnas y se denota por \\(\\mathbf{A }^{\\prime }\\) (o \\(\\mathbf{A}^{T}\\)). Por ejemplo, \\[ \\mathbf{A}=\\left( \\begin{array}{cc} 6,000 &amp; 23 \\\\ 13,000 &amp; 47 \\\\ 11,000 &amp; 35 \\end{array} \\right) ~~~\\mathbf{A}^{\\prime }=\\left( \\begin{array}{ccc} 6,000 &amp; 13,000 &amp; 11,000 \\\\ 23 &amp; 47 &amp; 35 \\end{array} \\right) . \\] Así, si \\(\\mathbf{A}\\) tiene dimensión \\(n\\times k\\), entonces \\(\\mathbf{A}^{\\prime }\\) tiene dimensiones \\(k\\times n\\). 2.11.2 Algunas Matrices Especiales Una matriz cuadrada es una matriz donde el número de filas es igual al número de columnas, es decir, \\(n=k\\). Los números diagonales de una matriz cuadrada son los números en una matriz donde el número de fila es igual al número de columna, por ejemplo, \\(a_{11}\\), \\(a_{22}\\), y así sucesivamente. Una matriz diagonal es una matriz cuadrada en la que todos los números no diagonales son iguales a 0. Por ejemplo, \\[ \\mathbf{A}=\\left( \\begin{array}{ccc} -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{array} \\right) . \\] Una matriz identidad es una matriz diagonal donde todos los números diagonales son iguales a 1. Esta matriz especial se denota a menudo por \\(\\mathbf{I}\\). Una matriz simétrica es una matriz cuadrada \\(\\mathbf{A}\\) tal que la matriz permanece sin cambios si intercambiamos las filas y las columnas. Más formalmente, una matriz \\(\\mathbf{A}\\) es simétrica si \\(\\mathbf{A=A} ^{\\prime }\\). Por ejemplo, \\[ \\mathbf{A}=\\left( \\begin{array}{ccc} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 10 \\end{array} \\right) \\mathbf{=A}^{\\prime }. \\] Observa que una matriz diagonal es una matriz simétrica. 2.11.3 Operaciones Básicas Multiplicación por un Escalar Sea \\(\\mathbf{A}\\) una matriz de \\(n\\times k\\) y sea \\(c\\) un número real. Es decir, un número real es una matriz de \\(1\\times 1\\) y también se llama escalar. Multiplicar un escalar \\(c\\) por una matriz \\(\\mathbf{A}\\) se denota por \\(c\\mathbf{A}\\) y se define por \\[ c\\mathbf{A}=\\left( \\begin{array}{cccc} ca_{11} &amp; ca_{12} &amp; \\cdots &amp; ca_{1k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ ca_{n1} &amp; ca_{n2} &amp; \\cdots &amp; ca_{nk} \\end{array} \\right) . \\] Por ejemplo, supongamos que \\(c=10\\) y \\[ \\mathbf{A}=\\left( \\begin{array}{cc} 1 &amp; 2 \\\\ 6 &amp; 8 \\end{array} \\right) ~~~~~\\text{entonces} ~~~~\\mathbf{B}=c\\mathbf{A}=\\left( \\begin{array}{cc} 10 &amp; 20 \\\\ 60 &amp; 80 \\end{array} \\right) . \\] Observa que \\(c\\mathbf{A}=\\mathbf{A}c\\). Suma y Resta de Matrices Sean \\(\\mathbf{A}\\) y \\(\\mathbf{B}\\) matrices con dimensiones \\(n\\times k\\). Utiliza \\(a_{ij}\\) y \\(b_{ij}\\) para denotar los números en la \\(i\\)-ésima fila y \\(j\\)-ésima columna de \\(\\mathbf{A}\\) y \\(\\mathbf{B}\\), respectivamente. Entonces, la matriz \\(\\mathbf{C}=\\mathbf{A}+\\mathbf{B}\\) se define como la matriz con \\((a_{ij}+b_{ij})\\) en la \\(i\\)-ésima fila y \\(j\\)-ésima columna. De manera similar, la matriz \\(\\mathbf{C}=\\mathbf{A}-\\mathbf{B}\\) se define como la matriz con \\((a_{ij}-b_{ij})\\) en la \\(i\\)-ésima fila y \\(j\\)-ésima columna. Simbólicamente, escribimos esto como sigue. \\[ \\text{Si }\\mathbf{A=}\\left( a_{ij}\\right) _{ij}\\text{ y } \\mathbf{B=}\\left( b_{ij}\\right) _{ij}\\text{, entonces} \\] \\[ \\mathbf{C}=\\mathbf{A}+\\mathbf{B=}\\left( a_{ij}+b_{ij}\\right) _{ij}\\text{ y }\\mathbf{C}=\\mathbf{A}-\\mathbf{B=}\\left( a_{ij}-b_{ij}\\right) _{ij}. \\] Por ejemplo, considera \\[ \\mathbf{A}=\\left( \\begin{array}{cc} 2 &amp; 5 \\\\ 4 &amp; 1 \\end{array} \\right) ~~~\\mathbf{B}=\\left( \\begin{array}{cc} 4 &amp; 6 \\\\ 8 &amp; 1 \\end{array} \\right). \\] Entonces \\[ \\mathbf{A}+\\mathbf{B}=\\left( \\begin{array}{cc} 6 &amp; 11 \\\\ 12 &amp; 2 \\end{array} \\right) ~~~\\mathbf{A}-\\mathbf{B}=\\left( \\begin{array}{cc} -2 &amp; -1 \\\\ -4 &amp; 0 \\end{array} \\right) . \\] Ejemplo Básico de Regresión Lineal de Suma y Resta. Ahora, recuerda que el modelo básico de regresión lineal puede escribirse como \\(n\\) ecuaciones: \\[ \\begin{array}{c} y_1=\\beta_0+\\beta_1x_1+\\varepsilon_1 \\\\ \\vdots \\\\ y_n=\\beta_0+\\beta_1x_n+\\varepsilon_n. \\end{array} \\] Podemos definir \\[ \\mathbf{y}=\\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) ~~~\\boldsymbol \\varepsilon = \\left( \\begin{array}{c} \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_n \\end{array} \\right) ~~~\\text{y}~~~ \\mathrm{E~}\\mathbf{y} =\\left( \\begin{array}{c} \\beta_0+\\beta_1 x_1 \\\\ \\vdots \\\\ \\beta_0 + \\beta_1 x_n \\end{array} \\right) . \\] Con esta notación, podemos expresar las \\(n\\) ecuaciones de manera más compacta como \\(\\mathbf{y} = \\mathrm{E~}\\mathbf{y}+\\boldsymbol \\varepsilon\\). Multiplicación de Matrices En general, si \\(\\mathbf{A}\\) es una matriz de dimensión \\(n\\times c\\) y \\(\\mathbf{B}\\) es una matriz de dimensión \\(c\\times k\\), entonces \\(\\mathbf{C}=\\mathbf{AB}\\) es una matriz de dimensión \\(n\\times k\\) y se define por \\[ \\mathbf{C}=\\mathbf{AB}=\\left( \\sum_{s=1}^{c}a_{is}b_{sj}\\right)_{ij}. \\] Por ejemplo, considera las matrices \\(2\\times 2\\) \\[ \\mathbf{A}=\\left( \\begin{array}{cc} 2 &amp; 5 \\\\ 4 &amp; 1 \\end{array} \\right) ~~~\\mathbf{B}=\\left( \\begin{array}{cc} 4 &amp; 6 \\\\ 8 &amp; 1 \\end{array} \\right) . \\] La matriz \\(\\mathbf{AB}\\) tiene dimensión \\(2\\times 2\\). Para ilustrar el cálculo, considera el número en la primera fila y segunda columna de \\(\\mathbf{AB}\\). Según la regla presentada arriba, con \\(i=1\\) y \\(j=2\\), el elemento correspondiente de \\(\\mathbf{AB}\\) es \\(\\sum_{s=1}^2a_{1s}b_{s2}=a_{11}b_{12}+a_{12}b_{22}=2(6)+5(1)=17\\). Los otros cálculos se resumen como \\[ \\mathbf{AB}=\\left( \\begin{array}{cc} 2(4)+5(8) &amp; 2(6)+5(1) \\\\ 4(4)+1(8) &amp; 4(6)+1(1) \\end{array} \\right) =\\left( \\begin{array}{cc} 48 &amp; 17 \\\\ 24 &amp; 25 \\end{array} \\right) . \\] Como otro ejemplo, supongamos \\[ \\mathbf{A}=\\left( \\begin{array}{ccc} 1 &amp; 2 &amp; 4 \\\\ 0 &amp; 5 &amp; 8 \\end{array} \\right) ~~~\\mathbf{B}=\\left( \\begin{array}{c} 3 \\\\ 5 \\\\ 2 \\end{array} \\right) . \\] Como \\(\\mathbf{A}\\) tiene dimensión \\(2\\times 3\\) y \\(\\mathbf{B}\\) tiene dimensión \\(3\\times 1\\), esto significa que el producto \\(\\mathbf{AB}\\) tiene dimensión \\(2\\times 1\\). Los cálculos se resumen como \\[ \\mathbf{AB}=\\left( \\begin{array}{c} 1(3)+2(5)+4(2) \\\\ 0(3)+5(5)+8(2) \\end{array} \\right) =\\left( \\begin{array}{c} 21 \\\\ 41 \\end{array} \\right) . \\] Para algunos ejemplos adicionales, tenemos \\[ \\left( \\begin{array}{cc} 4 &amp; 2 \\\\ 5 &amp; 8 \\end{array} \\right) \\left( \\begin{array}{c} a_1 \\\\ a_2 \\end{array} \\right) =\\left( \\begin{array}{c} 4a_1+2a_2 \\\\ 5a_1+8a_2 \\end{array} \\right) . \\] \\[ \\left( \\begin{array}{ccc} 2 &amp; 3 &amp; 5 \\end{array} \\right) \\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 5 \\end{array} \\right) =2^2+3^2+5^2=38~~~\\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 5 \\end{array} \\right) \\left( \\begin{array}{ccc} 2 &amp; 3 &amp; 5 \\end{array} \\right) =\\left( \\begin{array}{ccc} 4 &amp; 6 &amp; 10 \\\\ 6 &amp; 9 &amp; 15 \\\\ 10 &amp; 15 &amp; 25 \\end{array} \\right) . \\] En general, observa que \\(\\mathbf{AB}\\neq \\mathbf{BA}\\) en la multiplicación de matrices, a diferencia de la multiplicación de escalares (números reales). Además, observamos que la matriz identidad cumple el papel de “uno” en la multiplicación de matrices, ya que \\(\\mathbf{AI=A}\\) y \\(\\mathbf{IA=A}\\) para cualquier matriz \\(\\mathbf{A}\\), siempre que las dimensiones sean compatibles para permitir la multiplicación de matrices. Ejemplo Básico de Regresión Lineal de Multiplicación de Matrices. Define \\[ \\mathbf{X}=\\left( \\begin{array}{cc} 1 &amp; x_1 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{array} \\right) \\text{ y } \\boldsymbol \\beta =\\left( \\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array} \\right) \\text{, para obtener } \\mathbf{X} \\boldsymbol{\\beta} =\\left( \\begin{array}{c} \\beta_0+\\beta_1x_1 \\\\ \\vdots \\\\ \\beta_0+\\beta_1x_n \\end{array} \\right) =\\mathbf{\\mathrm{E~}\\mathbf{y}}. \\] Así, se obtiene la expresión matricial familiar del modelo de regresión, \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\). Otras cantidades útiles incluyen \\[ \\mathbf{y}^{\\prime }\\mathbf{y}=\\left( \\begin{array}{ccc} y_1 &amp; \\cdots &amp; y_n \\end{array} \\right) \\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) =y_1^2+\\cdots +y_n^2=\\sum_{i=1}^{n}y_i^2, \\] \\[ \\mathbf{X}^{\\prime }\\mathbf{y}=\\left( \\begin{array}{ccc} 1 &amp; \\cdots &amp; 1 \\\\ x_1 &amp; \\cdots &amp; x_n \\end{array} \\right) \\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) =\\left( \\begin{array}{c} \\sum_{i=1}^{n}y_i \\\\ \\sum_{i=1}^{n}x_iy_i \\end{array} \\right) \\] y \\[ \\mathbf{X}^{\\prime }\\mathbf{X}=\\left( \\begin{array}{ccc} 1 &amp; \\cdots &amp; 1 \\\\ x_1 &amp; \\cdots &amp; x_n \\end{array} \\right) \\left( \\begin{array}{cc} 1 &amp; x_1 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{array} \\right) =\\left( \\begin{array}{cc} n &amp; \\sum_{i=1}^{n}x_i \\\\ \\sum_{i=1}^{n}x_i &amp; \\sum_{i=1}^{n} x_i^2 \\end{array} \\right) . \\] Observa que \\(\\mathbf{X}^{\\prime }\\mathbf{X}\\) es una matriz simétrica. Inversas de Matrices En álgebra de matrices, no existe el concepto de “división.” En su lugar, extendemos el concepto de “recíprocos” de los números reales. Para comenzar, supongamos que \\(\\mathbf{A}\\) es una matriz cuadrada de dimensión \\(k \\times k\\) y que \\(\\mathbf{I}\\) es la matriz identidad de dimensión \\(k \\times k\\). Si existe una matriz \\(k \\times k\\) llamada \\(\\mathbf{B}\\) tal que \\(\\mathbf{AB}=\\mathbf{I}=\\mathbf{BA}\\), entonces \\(\\mathbf{B}\\) se llama inversa de \\(\\mathbf{A}\\) y se escribe como \\[ \\mathbf{B}=\\mathbf{A}^{-1}. \\] No todas las matrices cuadradas tienen inversas. Además, incluso cuando existe una inversa, puede no ser fácil de calcular manualmente. Una excepción a esta regla son las matrices diagonales. Supongamos que \\(\\mathbf{A}\\) es una matriz diagonal de la forma \\[ \\mathbf{A}=\\left( \\begin{array}{ccc} a_{11} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; a_{kk} \\end{array} \\right). \\text{ Entonces } \\mathbf{A}^{-1}=\\left( \\begin{array}{ccc} \\frac{1}{a_{11}} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; \\frac{1}{a_{kk}} \\end{array} \\right). \\] Por ejemplo, \\[ \\begin{array}{cccc} \\left( \\begin{array}{cc} 2 &amp; 0 \\\\ 0 &amp; -19 \\end{array} \\right) &amp; \\left( \\begin{array}{cc} \\frac{1}{2} &amp; 0 \\\\ 0 &amp; -\\frac{1}{19} \\end{array} \\right) &amp; = &amp; \\left( \\begin{array}{cc} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right) \\\\ \\mathbf{A} &amp; \\mathbf{A}^{-1} &amp; = &amp; \\mathbf{I} \\end{array} . \\] En el caso de una matriz de dimensión \\(2\\times 2\\), el procedimiento de inversión se puede realizar manualmente incluso cuando la matriz no es diagonal. En el caso de \\(2\\times 2\\), supongamos que si \\[ \\mathbf{A}=\\left( \\begin{array}{cc} a &amp; b \\\\ c &amp; d \\end{array} \\right), \\text{ entonces } \\mathbf{A}^{-1}=\\frac{1}{ad-bc}\\left( \\begin{array}{cc} d &amp; -b \\\\ -c &amp; a \\end{array} \\right) \\text{.} \\] Así, por ejemplo, si \\[ \\mathbf{A}=\\left( \\begin{array}{cc} 2 &amp; 2 \\\\ 3 &amp; 4 \\end{array} \\right) \\text{ entonces } \\mathbf{A}^{-1}=\\frac{1}{2(4)-2(3)} \\left( \\begin{array}{cc} 4 &amp; -2 \\\\ -3 &amp; 2 \\end{array} \\right) =\\left( \\begin{array}{cc} 2 &amp; -1 \\\\ -3/2 &amp; 1 \\end{array} \\right) \\text{.} \\] Como verificación, tenemos \\[ \\mathbf{A}\\mathbf{A}^{-1}=\\left( \\begin{array}{cc} 2 &amp; 2 \\\\ 3 &amp; 4 \\end{array} \\right) \\left( \\begin{array}{cc} 2 &amp; -1 \\\\ -3/2 &amp; 1 \\end{array} \\right) =\\left( \\begin{array}{cc} 2(2)-2(3/2) &amp; 2(-1)+2(1) \\\\ 3(2)-4(3/2) &amp; 3(-1)+4(1) \\end{array} \\right) =\\left( \\begin{array}{cc} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right) =\\mathbf{I}\\text{.} \\] Ejemplo Básico de Regresión Lineal de Inversas de Matrices. Con \\[ \\mathbf{X}^{\\prime }\\mathbf{X}=\\left( \\begin{array}{cc} n &amp; \\sum\\limits_{i=1}^{n}x_i \\\\ \\sum\\limits_{i=1}^{n}x_i &amp; \\sum\\limits_{i=1}^{n}x_i^2 \\end{array} \\right), \\] tenemos \\[ \\left( \\mathbf{X}^{\\prime }\\mathbf{X}\\right)^{-1}=\\frac{1}{n\\sum_{i=1}^{n}x_i^2-\\left( \\sum_{i=1}^{n}x_i\\right) ^2}\\left( \\begin{array}{cc} \\sum\\limits_{i=1}^{n}x_i^2 &amp; -\\sum\\limits_{i=1}^{n}x_i \\\\ -\\sum\\limits_{i=1}^{n}x_i &amp; n \\end{array} \\right). \\] Para simplificar esta expresión, recuerda que \\(\\overline{x}=n^{-1} \\sum_{i=1}^{n}x_i\\). Así, \\[\\begin{equation} \\left( \\mathbf{X}^{\\prime }\\mathbf{X}\\right)^{-1}=\\frac{1}{ \\sum_{i=1}^{n}x_i^2-n\\overline{x}^2}\\left( \\begin{array}{cc} n^{-1}\\sum\\limits_{i=1}^{n}x_i^2 &amp; -\\overline{x} \\\\ -\\overline{x} &amp; 1 \\end{array} \\right) . \\tag{2.9} \\end{equation}\\] La Sección 3.1 discutirá la relación \\(\\mathbf{b}=\\left( \\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{\\prime}\\mathbf{y}\\). Para ilustrar el cálculo, tenemos \\[\\begin{eqnarray*} \\mathbf{b} &amp;=&amp;\\left( \\mathbf{X}^{\\prime }\\mathbf{X}\\right)^{-1}\\mathbf{X} ^{\\prime }\\mathbf{y}=\\frac{1}{\\sum_{i=1}^{n}x_i^2-n\\overline{x}^2} \\left( \\begin{array}{cc} n^{-1}\\sum\\limits_{i=1}^{n}x_i^2 &amp; -\\overline{x} \\\\ -\\overline{x} &amp; 1 \\end{array} \\right) \\left( \\begin{array}{c} \\sum\\limits_{i=1}^{n}y_i \\\\ \\sum\\limits_{i=1}^{n}x_iy_i \\end{array} \\right) \\\\ &amp;=&amp;\\frac{1}{\\sum_{i=1}^{n}x_i^2-n\\overline{x}^2}\\left( \\begin{array}{c} \\sum\\limits_{i=1}^{n}\\left( \\overline{y}x_i^2-\\overline{x} x_iy_i\\right) \\\\ \\sum\\limits_{i=1}^{n}x_iy_i-n\\overline{x}\\overline{y} \\end{array} \\right) =\\left( \\begin{array}{c} b_0 \\\\ b_1 \\end{array} \\right) . \\end{eqnarray*}\\] De esta expresión, podemos ver \\[ b_1=\\frac{\\sum\\limits_{i=1}^{n}x_iy_i-n\\overline{x}\\overline{y}}{\\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x}^2} \\] y \\[ b_0=\\frac{\\overline{y}\\sum\\limits_{i=1}^{n}x_i^2-\\overline{x} \\sum\\limits_{i=1}^{n}x_iy_i}{\\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x}^2}=\\frac{\\overline{y}\\left( \\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x} ^2\\right) -\\overline{x}\\left( \\sum\\limits_{i=1}^{n} x_i y_i - n\\overline{x} \\overline{y}\\right) }{\\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x}^2}=\\overline{y}-b_1\\overline{x}. \\] Estas son las expresiones usuales para la pendiente \\(b_1\\) (Ejercicio 2.8) y el intercepto \\(b_0\\). 2.11.4 Matrices Aleatorias Esperanzas. Consideremos una matriz de variables aleatorias \\[ \\mathbf{U=}\\left( \\begin{array}{cccc} u_{11} &amp; u_{12} &amp; \\cdots &amp; u_{1c} \\\\ u_{21} &amp; u_{22} &amp; \\cdots &amp; u_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ u_{n1} &amp; u_{n2} &amp; \\cdots &amp; u_{nc} \\end{array} \\right). \\] Cuando escribimos la esperanza de una matriz, esto es una forma abreviada para la matriz de esperanzas. Específicamente, supongamos que la función de probabilidad conjunta de \\({u_{11}, u_{12}, ..., u_{1c}, ..., u_{n1}, ..., u_{nc}}\\) está disponible para definir el operador de esperanza. Entonces definimos \\[ \\mathrm{E} ~ \\mathbf{U} = \\left( \\begin{array}{cccc} \\mathrm{E }u_{11} &amp; \\mathrm{E }u_{12} &amp; \\cdots &amp; \\mathrm{E }u_{1c} \\\\ \\mathrm{E }u_{21} &amp; \\mathrm{E }u_{22} &amp; \\cdots &amp; \\mathrm{E }u_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathrm{E }u_{n1} &amp; \\mathrm{E }u_{n2} &amp; \\cdots &amp; \\mathrm{E }u_{nc} \\end{array} \\right). \\] Como un caso especial importante, consideremos la función de probabilidad conjunta para las variables aleatorias \\(y_1, \\ldots, y_n\\) y el operador de expectativas correspondiente. Entonces \\[ \\mathrm{E}~ \\mathbf{y=} \\mathrm{E } \\left( \\begin{array}{cccc} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) = \\left( \\begin{array}{cccc} \\mathrm{E }y_1 \\\\ \\vdots \\\\ \\mathrm{E }y_n \\end{array} \\right). \\] Por la linealidad de las esperanzas, para una matriz no aleatoria A y un vector , tenemos \\(\\mathrm{E} (\\textbf{A y} + \\textbf{B}) = \\textbf{A} \\mathrm{E} \\textbf{y + B}\\). Varianzas. También podemos trabajar con los segundos momentos de vectores aleatorios. La varianza de un vector de variables aleatorias se llama matriz de varianza-covarianza. Se define como \\[\\begin{equation} \\mathrm{Var} ~ \\mathbf{y} = \\mathrm{E} ( (\\mathbf{y} - \\mathrm{E} \\mathbf{y})(\\mathbf{y} - \\mathrm{E} \\mathbf{y})^{\\prime} ). \\tag{2.10} \\end{equation}\\] Es decir, podemos expresar \\[ \\mathrm{Var}~\\mathbf{y=} \\mathrm{E } \\left( \\left( \\begin{array}{c} y_1 -\\mathrm{E } y_1 \\\\ \\vdots \\\\ y_n -\\mathrm{E } y_n \\end{array}\\right) \\left(\\begin{array}{ccc} y_1 - \\mathrm{E } y_1 &amp; \\cdots &amp; y_n - \\mathrm{E } y_n \\end{array}\\right) \\right) \\] \\[ = \\left( \\begin{array}{cccc} \\mathrm{Var}~y_1 &amp; \\mathrm{Cov}(y_1, y_2) &amp; \\cdots &amp;\\mathrm{Cov}(y_1, y_n) \\\\ \\mathrm{Cov}(y_2, y_1) &amp; \\mathrm{Var}~y_2 &amp; \\cdots &amp; \\mathrm{Cov}(y_2, y_n) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\mathrm{Cov}(y_n, y_1) &amp; \\mathrm{Cov}(y_n, y_2) &amp; \\cdots &amp; \\mathrm{Var}~y_n \\\\ \\end{array}\\right), \\] porque \\(\\mathrm{E} ( (y_i - \\mathrm{E} y_i)(y_j - \\mathrm{E} y_j) ) = \\mathrm{Cov}(y_i, y_j)\\) para \\(i \\neq j\\) y \\(\\mathrm{Cov}(y_i, y_i) = \\mathrm{Var}~y_i\\). En el caso de que \\(y_1, \\ldots, y_n\\) sean mutuamente no correlacionados, tenemos que \\(\\mathrm{Cov}(y_i, y_j)=0\\) para \\(i \\neq j\\) y así \\[ \\mathrm{Var}~\\mathbf{y=} \\left( \\begin{array}{cccc} \\mathrm{Var}~y_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\mathrm{Var}~y_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0 &amp; \\cdots &amp; \\mathrm{Var}~y_n \\\\ \\end{array}\\right). \\] Además, si las varianzas son idénticas, de modo que \\(\\mathrm{Var}~y_i=\\sigma ^2\\), entonces podemos escribir \\(\\mathrm{Var} ~\\mathbf{y} = \\sigma ^2 \\mathbf{I}\\), donde I es la matriz identidad \\(n \\times n\\). Por ejemplo, si \\(y_1, \\ldots, y_n\\) son i.i.d., entonces \\(\\mathrm{Var} ~\\mathbf{y} = \\sigma ^2 \\mathbf{I}\\). A partir de la ecuación (2.10), se puede demostrar que \\[\\begin{equation} \\mathrm{Var}\\left( \\mathbf{Ay +B} \\right) = \\mathrm{Var}\\left( \\mathbf{Ay} \\right) = \\mathbf{A} \\left( \\mathrm{Var}~\\mathbf{y} \\right) \\mathbf{A}^{\\prime}. \\tag{2.11} \\end{equation}\\] Por ejemplo, si \\(\\mathbf{A} = (a_1, a_2, \\ldots,a_n)= \\mathbf{a}^{\\prime}\\) y B = 0, entonces la ecuación (2.11) se reduce a \\[ \\mathrm{Var}\\left( \\sum_{i=1}^n a_i y_i \\right) = \\mathrm{Var} \\left( \\mathbf{a^{\\prime} y} \\right) = \\mathbf{a^{\\prime}} \\left( \\mathrm{Var} ~\\mathbf{y} \\right) \\mathbf{a} = (a_1, a_2, \\ldots,a_n) \\left( \\mathrm{Var} ~\\mathbf{y} \\right) \\left(\\begin{array}{c} a_1 \\\\ \\vdots \\\\ a_n \\end{array}\\right) \\] \\[ = \\sum_{i=1}^n a_i^2 \\mathrm{Var} ~y_i ~+~2 \\sum_{i=2}^n \\sum_{j=1}^{i-1} a_i a_j \\mathrm{Cov}(y_i, y_j). \\] Definición - Distribución Normal Multivariante. Un vector de variables aleatorias \\(\\mathbf{y} = \\left(y_1, \\ldots, y_n \\right)^{\\prime}\\) se dice que es normal multivariante si todas las combinaciones lineales de la forma \\(\\sum_{i=1}^n a_i y_i\\) están distribuidas normalmente. En este caso, escribimos \\(\\mathbf{y} \\sim N (\\mathbf{\\boldsymbol \\mu}, \\mathbf{\\Sigma} )\\), donde \\(\\mathbf{\\boldsymbol \\mu} = \\mathrm{E}~ \\mathbf{y}\\) es el valor esperado de y y \\(\\mathbf{\\Sigma}= \\mathrm{Var}~\\mathbf{y}\\) es la matriz de varianza-covarianza de y. Según la definición, tenemos que \\(\\mathbf{y}\\sim N (\\mathbf{\\boldsymbol \\mu}, \\mathbf{\\Sigma} )\\) implica que \\(\\mathbf{a^{\\prime}y}\\sim N (\\mathbf{a^{\\prime} \\boldsymbol \\mu}, \\mathbf{a^{\\prime}\\Sigma a})\\). Así, si \\(y_i\\) son i.i.d., entonces \\(\\sum_{i=1}^n a_i y_i\\) está distribuido normalmente con media \\(\\mu \\sum_{i=1}^n a_i\\) y varianza \\(\\sigma ^2 \\sum_{i=1}^n a_i ^2\\). "],["C3BasicMLR.html", "Capítulo 3 Regresión Lineal Múltiple - I 3.1 Método de Mínimos Cuadrados 3.2 Modelo de Regresión Lineal y Propiedades de los Estimadores 3.3 Estimación y Bondad de Ajuste 3.4 Inferencia Estadística para un Coeficiente Único 3.5 Algunas Variables Explicativas Especiales 3.6 Lectura Adicional y Referencias 3.7 Ejercicios", " Capítulo 3 Regresión Lineal Múltiple - I Vista previa del capítulo. Este capítulo introduce la regresión lineal en el caso de varios variables explicativas, conocida como regresión lineal múltiple. Muchos conceptos básicos de la regresión lineal se extienden directamente, incluyendo medidas de bondad de ajuste como \\(R^2\\) y la inferencia usando estadísticas \\(t\\). Los modelos de regresión lineal múltiple proporcionan un marco para resumir datos altamente complejos y multivariados. Debido a que este marco solo requiere linealidad en los parámetros, podemos ajustar modelos que son funciones no lineales de las variables explicativas, proporcionando así un amplio alcance de aplicaciones potenciales. 3.1 Método de Mínimos Cuadrados El Capítulo 2 trató sobre el problema de una respuesta que depende de una sola variable explicativa. Ahora extendemos el enfoque de ese capítulo y estudiamos cómo una respuesta puede depender de varias variables explicativas. Ejemplo: Seguro de Vida Temporal. Como todas las empresas, las compañías de seguros de vida buscan continuamente nuevas formas de llevar productos al mercado. Aquellos involucrados en el desarrollo de productos desean saber “¿quién compra seguro y cuánto compran?” En economía, esto se conoce como el lado de la demanda de un mercado de productos. Los analistas pueden obtener fácilmente información sobre las características de los clientes actuales a través de las bases de datos de la empresa. Los clientes potenciales, aquellos que no tienen seguro con la compañía, son a menudo el principal objetivo para expandir la cuota de mercado. En este ejemplo, examinamos la Encuesta de Finanzas del Consumidor (SCF), una muestra representativa a nivel nacional que contiene información extensa sobre activos, pasivos, ingresos y características demográficas de los encuestados (potenciales clientes en EE. UU.). Estudiamos una muestra aleatoria de 500 hogares con ingresos positivos que fueron entrevistados en la encuesta de 2004. Inicialmente, consideramos el subconjunto de \\(n=275\\) familias que compraron seguro de vida temporal. Deseamos abordar la segunda parte de la pregunta de la demanda y determinar las características de la familia que influyen en la cantidad de seguro comprado. El Capítulo 11 considerará la primera parte, es decir, si un hogar compra o no un seguro, a través de modelos donde la respuesta es una variable aleatoria binaria. Para el seguro de vida temporal, la cantidad de seguro se mide por el valor nominal de la póliza, FACE, la cantidad que la compañía pagará en caso de la muerte del asegurado. Las características que resultarán importantes incluyen los ingresos anuales, INCOME, el número de años de EDUCATION del encuestado y el número de miembros del hogar, NUMHH. En general, consideraremos conjuntos de datos donde hay \\(k\\) variables explicativas y una variable de respuesta en una muestra de tamaño \\(n\\). Es decir, los datos consisten en: \\[ \\left\\{ \\begin{aligned} x_{11},x_{12},\\ldots,x_{1k},y_1 \\\\ x_{21},x_{22},\\ldots,x_{2k},y_2 \\\\ \\vdots \\\\ x_{n1},x_{n2},\\ldots,x_{nk},y_n \\end{aligned} \\right\\}. \\] La \\(i\\)-ésima observación corresponde a la \\(i\\)-ésima fila, que consiste en \\((x_{i1},x_{i2},\\ldots,x_{ik},y_i)\\). Para este caso general, tomamos \\(k+1\\) mediciones en cada entidad. Para el ejemplo de demanda de seguros, \\(k=3\\) y los datos consisten en \\((x_{11},x_{12},x_{13}, y_1), \\ldots , (x_{275,1},x_{275,2},x_{275,3},y_{275})\\). Es decir, usamos cuatro mediciones de cada uno de los \\(n=275\\) hogares. Resumiendo los Datos Comenzamos el análisis de los datos examinando cada variable por separado. La Tabla 3.1 proporciona estadísticas descriptivas básicas de las cuatro variables. Para FACE e INCOME, vemos que la media es mucho mayor que la mediana, lo que sugiere que la distribución está sesgada hacia la derecha. Los histogramas (no reportados aquí) muestran que este es el caso. Será útil considerar también sus transformaciones logarítmicas, LNFACE y LNINCOME, respectivamente, que también se informan en la Tabla 3.1. Tabla 3.1: Estadísticas Descriptivas del Seguro de Vida Temporal Media Mediana Desviación Estándar Mínimo Máximo FACE 747,581 150,000 1,674,362 800 14,000,000 INCOME 208,975 65,000 824,010 260 10,000,000 EDUCATION 14.524 16 2.549 2 17 NUMHH 2.96 3 1.493 1 9 LNFACE 11.99 11.918 1.871 6.685 16.455 LNINCOME 11.149 11.082 1.295 5.561 16.118 El siguiente paso es medir la relación entre cada \\(x\\) sobre \\(y\\), comenzando con los diagramas de dispersión en la Figura 3.1. El panel de la izquierda es un gráfico de FACE versus INCOME; en este panel, vemos una gran concentración en la esquina inferior izquierda que corresponde a hogares con ingresos y cantidades de seguro pequeños. Ambas variables tienen distribuciones sesgadas y su efecto conjunto es altamente no lineal. El panel derecho presenta las mismas variables, pero utilizando transformaciones logarítmicas. Aquí, vemos una relación que se puede aproximar más fácilmente con una línea. Figura 3.1: Ingresos versus Monto Nominal del Seguro de Vida Temporal. El panel de la izquierda es un gráfico de monto nominal versus ingresos, mostrando un patrón altamente no lineal. En el panel derecho, el monto nominal versus ingresos está en unidades logarítmicas naturales, sugiriendo un patrón lineal (aunque variable). Código R para producir la Tabla 3.1 y la Figura 3.1 Term &lt;- read.csv(&quot;CSVData/TermLife.csv&quot;, header=TRUE) # SELECCIONAR EL SUBCONJUNTO DE DATOS CORRESPONDIENTE A LA COMPRA DE SEGURO Term2 &lt;-subset(Term, FACE &gt; 0) # TABLA 3.1 ESTADÍSTICAS DESCRIPTIVAS BookSummStats &lt;- function(Xymat){ meanSummary &lt;- sapply(Xymat, mean, na.rm=TRUE) sdSummary &lt;- sapply(Xymat, sd, na.rm=TRUE) minSummary &lt;- sapply(Xymat, min, na.rm=TRUE) maxSummary &lt;- sapply(Xymat, max, na.rm=TRUE) medSummary &lt;- sapply(Xymat, median,na.rm=TRUE) tableMat &lt;- cbind(meanSummary, medSummary, sdSummary, minSummary, maxSummary) return(tableMat) } LNFACE &lt;- log(Term2$FACE) LNINCOME &lt;- log(Term2$INCOME) Xymat &lt;- data.frame(cbind(Term2$FACE, Term2$INCOME,Term2$EDUCATION, Term2$NUMHH,LNFACE, LNINCOME) ) tableMat &lt;- BookSummStats(Xymat) colnames(tableMat) &lt;- c(&quot;Media&quot; , &quot;Mediana&quot; , &quot;Desviación Estándar&quot; , &quot;Mínimo&quot; , &quot;Máximo&quot;) rownames(tableMat) &lt;- c(&quot;FACE&quot;, &quot;INCOME&quot;, &quot;EDUCATION&quot;, &quot;NUMHH&quot;, &quot;LNFACE&quot;, &quot;LNINCOME&quot;) tableMat1 &lt;- tableMat tableMat1[3:6,] &lt;- round(tableMat1[3:6,], digits = 3) tableMat1[1:2,] &lt;- format(round(tableMat[1:2,], digits=0), big.mark = &#39;,&#39;) TableGen1(TableData=tableMat1, TextTitle=&#39;Estadísticas Descriptivas del Seguro de Vida Temporal&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:5, ColWidth = ColWidth5) Term &lt;- read.csv(&quot;CSVData/TermLife.csv&quot;, header=TRUE) # SELECCIONAR EL SUBCONJUNTO DE DATOS CORRESPONDIENTE A LA COMPRA DE SEGURO Term2 &lt;-subset(Term, FACE &gt; 0) Term2$LNFACE &lt;- log(Term2$FACE) Term2$LNINCOME &lt;- log(Term2$INCOME) # FIGURA 3.1 par(mfrow=c(1, 2), cex=1.1, mar=c(4.1,4,1.5,1)) plot(Term2$INCOME, Term2$FACE, ylab=&quot;&quot;, las=1, yaxt=&quot;n&quot;, xaxt=&quot;n&quot;, xlab=&quot;INCOME (en Millones)&quot;) mtext(&quot;FACE (en Millones)&quot;, side=2, at=15200000, las=1, cex=1.1, adj=.4) axis(2,at=seq(0,14000000,2000000), labels=c(&quot;0&quot;, &quot;2&quot;, &quot;4&quot;, &quot;6&quot;, &quot;8&quot;,&quot;10&quot;,&quot;12&quot;,&quot;14&quot;), las=1) axis(1,at=seq(0,11000000,1000000), labels=c(&quot;0&quot;,&quot;1&quot;, &quot;2&quot;,&quot;3 &quot;, &quot;4&quot;,&quot;5&quot;, &quot;6&quot;,&quot;7&quot;,&quot;8&quot;,&quot;9&quot;,&quot;10&quot;,&quot;11&quot;)) plot(Term2$LNINCOME, Term2$LNFACE, ylab=&quot;&quot;, xlab = &quot;LNINCOME&quot;, las=1) mtext(&quot;LNFACE&quot;, side=2, at=16.8, las=1, cex=1.1, adj=1.1) Los datos de Seguro de Vida Temporal son multivariados en el sentido de que se toman varias mediciones en cada hogar. Es difícil producir un gráfico de observaciones en tres o más dimensiones en una plataforma bidimensional, como una hoja de papel, que no sea confuso, engañoso o ambos. Para resumir gráficamente datos multivariados en aplicaciones de regresión, considere usar una matriz de diagramas de dispersión como en la Figura 3.2. Cada cuadrado de esta figura representa un gráfico simple de una variable contra otra. Para cada cuadrado, la variable de la fila da las unidades del eje vertical y la variable de la columna da las unidades del eje horizontal. La matriz a veces se llama una matriz de dispersión parcial porque solo se presentan los elementos en la parte inferior izquierda. Figura 3.2: Matriz de diagramas de dispersión de cuatro variables. Cada cuadrado es un diagrama de dispersión. La matriz de diagramas de dispersión se puede resumir numéricamente usando una matriz de correlación. Cada correlación en la Tabla 3.2 corresponde a un cuadrado de la matriz de diagramas de dispersión en la Figura 3.2. Los analistas a menudo presentan tablas de correlaciones porque son fáciles de interpretar. Sin embargo, recuerde que un coeficiente de correlación solo mide la magnitud de las relaciones lineales. Por lo tanto, una tabla de correlaciones proporciona una idea de las relaciones lineales, pero puede pasar por alto una relación no lineal que se puede revelar en una matriz de diagramas de dispersión. Tabla 3.2: Correlaciones del Seguro de Vida Temporal NUMHH EDUCATION LNINCOME EDUCATION -0.064 LNINCOME 0.179 0.343 LNFACE 0.288 0.383 0.482 La matriz de diagramas de dispersión y la correspondiente matriz de correlación son herramientas útiles para resumir datos multivariados. Son fáciles de producir e interpretar. Sin embargo, cada una captura solo relaciones entre pares de variables y no puede cuantificar relaciones entre varias variables. Código R para producir la Tabla 3.2 y la Figura 3.2 tableCor &lt;- cor(Term1) tableCor &lt;- round(tableCor, digits = 3) tableCor[upper.tri(tableCor, diag = TRUE)] &lt;- &quot;&quot; tablePrint &lt;- tableCor[-1,] tablePrint &lt;- tablePrint[,-4] TableGen1(TableData=tablePrint, TextTitle=&#39;Correlaciones del Seguro de Vida Temporal&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:3, ColWidth = ColWidth5) # FIGURA 3.2 par(mar=c(4.1,2.1,2.1,2.1), cex=1.1) varTerm &lt;- c(&quot;NUMHH&quot;,&quot;EDUCATION&quot;, &quot;LNINCOME&quot;, &quot;LNFACE&quot;) Term1 &lt;- Term2[varTerm] pairs(Term1,upper.panel=NULL, gap=0,cex.labels=1.25, las=1) Método de Mínimos Cuadrados Consideremos la pregunta: “¿Puede el conocimiento de la educación, el tamaño del hogar y el ingreso ayudarnos a entender la demanda de seguros?” Las correlaciones en la Tabla 3.2 y los gráficos en las Figuras 3.1 y 3.2 sugieren que cada variable, EDUCATION, NUMHH y LNINCOME, puede ser una variable explicativa útil de LNFACE cuando se consideran individualmente. Parece razonable investigar el efecto conjunto de estas variables en una respuesta. El concepto geométrico de un plano se utiliza para explorar la relación lineal entre una respuesta y varias variables explicativas. Recuerde que un plano extiende el concepto de una línea a más de dos dimensiones. Un plano puede definirse mediante una ecuación algebraica como \\[ y = b_0 + b_1 x_1 + \\ldots + b_k x_k. \\] Esta ecuación define un plano en \\(k+1\\) dimensiones. La Figura 3.3 muestra un plano en tres dimensiones. En esta figura, hay una variable de respuesta, LNFACE, y dos variables explicativas, EDUCATION y LNINCOME (NUMHH se mantiene fijo). Es difícil graficar más de tres dimensiones de una manera significativa. Figura 3.3: Un ejemplo de un plano tridimensional Necesitamos una manera de determinar un plano basado en los datos. La dificultad es que en la mayoría de las aplicaciones de análisis de regresión, el número de observaciones, \\(n\\), excede con creces el número de observaciones requeridas para ajustar un plano, \\(k+1\\). Por lo tanto, generalmente no es posible encontrar un solo plano que pase por todas las \\(n\\) observaciones. Como en el Capítulo 2, utilizamos el método de mínimos cuadrados para determinar un plano a partir de los datos. El método de mínimos cuadrados se basa en determinar los valores de \\(b_0^{\\ast},b_1^{\\ast},\\ldots,b_k^{\\ast}\\) que minimizan la cantidad \\[\\begin{equation} SS(b_0^{\\ast},b_1^{\\ast},\\ldots,b_k^{\\ast})=\\sum_{i=1}^{n}\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_{i1}+\\ldots+b_k^{\\ast}x_{ik}\\right) \\right) ^2. \\tag{3.1} \\end{equation}\\] Dejamos de usar la notación con asterisco, o estrella, y usamos \\(b_0, b_1, \\ldots, b_k\\) para denotar los mejores valores, conocidos como las estimaciones de mínimos cuadrados. Con las estimaciones de mínimos cuadrados, definimos el plano de regresión de mínimos cuadrados, o ajustado, como \\[ \\widehat{y} = b_0 + b_1 x_1 + \\ldots + b_k x_k. \\] Las estimaciones de mínimos cuadrados se determinan minimizando \\(SS(b_0^{\\ast},b_1^{\\ast},\\ldots,b_k^{\\ast})\\). Es difícil escribir los estimadores de mínimos cuadrados resultantes utilizando una fórmula simple a menos que se recurra a la notación matricial. Debido a su importancia en los modelos estadísticos aplicados, se proporciona una fórmula explícita para los estimadores a continuación. Sin embargo, estas fórmulas han sido programadas en una gran variedad de paquetes de software estadístico y de hojas de cálculo. La disponibilidad de estos paquetes permite a los analistas de datos concentrarse en las ideas del procedimiento de estimación en lugar de enfocarse en los detalles de los procedimientos de cálculo. Como ejemplo, se ajustó un plano de regresión a los datos de Seguro de Vida Temporal donde se utilizaron tres variables explicativas, \\(x_1\\) para EDUCATION, \\(x_2\\) para NUMHH y \\(x_3\\) para LNINCOME. El plano de regresión ajustado resultante es \\[\\begin{equation} \\widehat{y} = 2.584 + 0.206 x_1 + 0.306 x_2 + 0.494 x_3. \\tag{3.2} \\end{equation}\\] Notación Matricial Supongamos que los datos son de la forma \\((x_{i0}, x_{i1}, \\ldots, x_{ik}, y_i)\\), donde \\(i = 1, \\ldots, n\\). Aquí, la variable \\(x_{i0}\\) está asociada con el término “intercepto”. En la mayoría de las aplicaciones, suponemos que \\(x_{i0}\\) es idénticamente igual a 1 y, por lo tanto, no es necesario representarlo explícitamente. Sin embargo, hay aplicaciones importantes donde este no es el caso, y por lo tanto, para expresar el modelo en notación general, se incluye aquí. Los datos se representan en notación matricial usando: \\[ \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} ~~~~ \\mathbf{X} = \\begin{pmatrix} x_{10} &amp; x_{11} &amp; \\cdots &amp; x_{1k} \\\\ x_{20} &amp; x_{21} &amp; \\cdots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n0} &amp; x_{n1} &amp; \\cdots &amp; x_{nk} \\end{pmatrix}. \\] Aquí, \\(\\mathbf{y}\\) es el vector de respuestas de \\(n \\times 1\\) y \\(\\mathbf{X}\\) es la matriz de variables explicativas de \\(n \\times (k+1)\\). Usamos la convención de álgebra matricial de que las letras minúsculas y mayúsculas en negrita representan vectores y matrices, respectivamente. (Si necesita repasar sobre matrices, revise la Sección 2.11). Ejemplo: Seguro de Vida Temporal - Continuación. Recuerde que \\(y\\) representa el valor logarítmico del seguro, \\(x_1\\) para los años de educación, \\(x_2\\) para el número de miembros del hogar, y \\(x_3\\) para el ingreso logarítmico. Por lo tanto, hay \\(k = 3\\) variables explicativas y \\(n = 275\\) hogares. El vector de respuestas y la matriz de variables explicativas son: \\[ \\small{ \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{275} \\end{pmatrix} = \\begin{pmatrix} 9.904 \\\\ 11.775 \\\\ \\vdots \\\\ 9.210 \\end{pmatrix} \\\\ \\mathbf{X} = \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{275,1} &amp; x_{275,2} &amp; x_{275,3} \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 16 &amp; 3 &amp; 10.669 \\\\ 1 &amp; 9 &amp; 3 &amp; 9.393 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; 12 &amp; 1 &amp; 10.545 \\end{pmatrix}. } \\] Por ejemplo, para la primera observación en el conjunto de datos, la variable dependiente es \\(y_1 = 9.904\\) (correspondiente a \\(\\exp(9.904) = \\$20,000\\)), para un encuestado con 16 años de educación que vive en un hogar con 3 personas y un ingreso logarítmico de 10.669 (\\(\\exp(10.669) = \\$43,000\\)). Bajo el principio de estimación de mínimos cuadrados, nuestro objetivo es elegir los coeficientes \\(b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast}\\) para minimizar la función de suma de cuadrados \\(SS(b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast})\\). Usando cálculo, regresamos a la ecuación (3.1), tomamos derivadas parciales con respecto a cada coeficiente y establecemos estas cantidades igual a cero: \\[ \\begin{array}{ll} \\frac{\\partial }{\\partial b_j^{\\ast}}SS(b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast}) &amp;= \\sum_{i=1}^{n}\\left( -2x_{ij}\\right) \\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_{i1}+\\ldots+b_k^{\\ast}x_{ik}\\right) \\right) \\\\ &amp;= 0, ~~~ \\text{para} ~ j=0,1,\\ldots,k. \\end{array} \\] Este es un sistema de \\(k+1\\) ecuaciones y \\(k+1\\) incógnitas que se puede resolver fácilmente usando notación matricial, como sigue. Podemos expresar el vector de parámetros a minimizar como \\(\\mathbf{b}^{\\ast}=(b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast})^{\\prime}\\). Usando esto, la suma de cuadrados se puede escribir como \\(SS(\\mathbf{b}^{\\ast}) = (\\mathbf{y-Xb}^{\\ast})^{\\prime}(\\mathbf{y-Xb}^{\\ast})\\). Así, en forma matricial, la solución al problema de minimización se puede expresar como \\(\\frac{\\partial}{\\partial \\mathbf{b}^{\\ast}} SS(\\mathbf{b}^{\\ast}) = \\mathbf{0}\\). Esta solución satisface las ecuaciones normales: \\[\\begin{equation} \\mathbf{X^{\\prime}Xb} = \\mathbf{X}^{\\prime}\\mathbf{y}. \\tag{3.3} \\end{equation}\\] Aquí, se ha eliminado la notación de asterisco (*) para denotar el hecho de que \\(\\mathbf{b} = (b_0, b_1, \\ldots, b_k)^{\\prime}\\) representa el mejor vector de valores en el sentido de minimizar \\(SS(\\mathbf{b}^{\\ast})\\) sobre todas las opciones de \\(\\mathbf{b}^{\\ast}\\). El estimador de mínimos cuadrados \\(\\mathbf{b}\\) no necesita ser único. Sin embargo, suponiendo que las variables explicativas no son combinaciones lineales entre sí, tenemos que \\(\\mathbf{X^{\\prime}X}\\) es invertible. En este caso, podemos escribir la solución única como: \\[\\begin{equation} \\mathbf{b} = \\left( \\mathbf{X^{\\prime}X} \\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}. \\tag{3.4} \\end{equation}\\] Para ilustrar, para el ejemplo de Seguro de Vida Temporal, la ecuación (3.4) produce: \\[ \\small{ \\mathbf{b} = \\begin{pmatrix} b_0 \\\\ b_1 \\\\ b_2 \\\\ b_3 \\\\ \\end{pmatrix} = \\begin{pmatrix} 2.584 \\\\ 0.206 \\\\ 0.306 \\\\ 0.494 \\\\ \\end{pmatrix}. } \\] 3.2 Modelo de Regresión Lineal y Propiedades de los Estimadores En la sección anterior, aprendimos cómo utilizar el método de mínimos cuadrados para ajustar un plano de regresión con un conjunto de datos. Esta sección describe los supuestos que sustentan el modelo de regresión y algunas de las propiedades resultantes de los estimadores de los coeficientes de regresión. Con el modelo y los datos ajustados, podremos hacer inferencias sobre el conjunto de datos de la muestra a una población más grande. Además, más adelante utilizaremos estos supuestos del modelo de regresión para ayudarnos a mejorar la especificación del modelo en el Capítulo 5. 3.2.1 Función de Regresión La mayoría de los supuestos del modelo de regresión lineal múltiple se trasladarán directamente de los supuestos del modelo de regresión lineal básico introducidos en la Sección 2.2. La principal diferencia es que ahora resumimos la relación entre la variable respuesta y las variables explicativas a través de la función de regresión: \\[\\begin{equation} \\mathrm{E~}y = \\beta_0 x_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k, \\tag{3.5} \\end{equation}\\] que es lineal en los parámetros \\(\\beta_0,\\ldots,\\beta_k\\). De aquí en adelante, usaremos \\(x_0 = 1\\) para la variable asociada con el parámetro \\(\\beta_0\\); esto es lo predeterminado en la mayoría de los paquetes estadísticos, y la mayoría de las aplicaciones de regresión incluyen el término de intercepto \\(\\beta_0\\). El intercepto es el valor esperado de \\(y\\) cuando todas las variables explicativas son iguales a cero. Aunque rara vez es de interés, el término \\(\\beta_0\\) sirve para establecer la altura del plano de regresión ajustado. En cambio, los otros betas son típicamente parámetros importantes en un estudio de regresión. Para ayudar a interpretarlos, inicialmente asumimos que \\(x_j\\) varía de manera continua y no está relacionado con las otras variables explicativas. Entonces, podemos interpretar \\(\\beta_j\\) como el cambio esperado en \\(y\\) por unidad de cambio en \\(x_j\\) asumiendo que todas las demás variables explicativas se mantienen fijas. Es decir, desde el cálculo, reconocerás que \\(\\beta_j\\) puede interpretarse como una derivada parcial. Específicamente, usando la ecuación anterior, tenemos que \\[ \\beta_j = \\frac{\\partial }{\\partial x_j}\\mathrm{E}~y. \\] 3.2.2 Interpretación del Coeficiente de Regresión Examinemos las estimaciones de los coeficientes de regresión del ejemplo de Seguro de Vida Temporal y enfoquémonos inicialmente en el signo de los coeficientes. Por ejemplo, en la ecuación (3.4), el coeficiente asociado con NUMHH es \\(b_2 = 0.306 &gt; 0\\). Si consideramos dos hogares que tienen el mismo ingreso y el mismo nivel de educación, entonces se espera que el hogar más grande (en términos de NUMHH) demande más seguro de vida temporal bajo el modelo de regresión. Esta es una interpretación sensata; los hogares más grandes tienen más dependientes para los cuales el seguro de vida temporal puede proporcionar activos financieros necesarios en caso de la muerte prematura de un sostén de la familia. El coeficiente positivo asociado con el ingreso (\\(b_3 = 0.494\\)) también es plausible; los hogares con mayores ingresos tienen más dinero disponible para comprar seguros. El signo positivo asociado con EDUCATION (\\(b_1 = 0.206)\\) también es razonable; más educación sugiere que los encuestados son más conscientes de sus necesidades de seguro, otras cosas iguales. También necesitas interpretar la cantidad del coeficiente de regresión. Consideremos primero el coeficiente de EDUCATION. Usando la ecuación (3.4), se calcularon los valores ajustados de \\(\\widehat{\\mathrm{LNFACE}}\\) permitiendo que EDUCATION variara y manteniendo NUMHH y LNINCOME fijos en los promedios de la muestra. Los resultados son: Efectos de Pequeños Cambios en la Educación EDUCATION 14 14.1 14.2 14.3 \\(\\widehat{\\mathit{LNFACE}}\\) 11.883 11.904 11.924 11.945 \\(\\widehat{\\mathit{FACE}}\\) 144,803 147,817 150,893 154,034 \\(\\widehat{\\mathit{FACE}}\\) % Cambio 2.081 2.081 2.081 A medida que EDUCATION aumenta, \\(\\widehat{\\mathrm{LNFACE}}\\) también aumenta. Además, la cantidad de incremento en \\(\\widehat{\\mathrm{LNFACE}}\\) es un 0.0206 constante. Esto viene directamente de la ecuación (3.4); a medida que EDUCATION aumenta en 0.1 años, se espera que la demanda de seguros aumente en 0.0206 dólares logarítmicos, manteniendo NUMHH y LNINCOME fijos. Esta interpretación es correcta, pero la mayoría de los directores de desarrollo de productos no son muy partidarios de los dólares logarítmicos. Para volver a dólares, los valores ajustados pueden calcularse a través de la exponenciación como \\(\\widehat{\\mathrm{FACE}} = \\exp(\\widehat{\\mathrm{LNFACE}})\\). Además, se puede calcular el cambio porcentual; por ejemplo, \\(100 \\times (147,817/144,803 - 1) \\approx 2.08\\%\\). Esto proporciona otra interpretación del coeficiente de regresión; a medida que EDUCATION aumenta en 0.1 años, se espera que la demanda de seguros aumente en un 2.08%. Esta es una simple consecuencia del cálculo usando \\(\\partial \\ln y / \\partial x = \\left(\\partial y / \\partial x \\right) / y\\); es decir, un pequeño cambio en el valor logarítmico de \\(y\\) equivale a un pequeño cambio en \\(y\\) como una proporción de \\(y\\). Es debido a este resultado del cálculo que utilizamos logaritmos naturales en lugar de logaritmos comunes en el análisis de regresión. Dado que esta tabla utiliza un cambio discreto en EDUCATION, el 2.08% difiere ligeramente del resultado continuo \\(0.206 \\times (\\mathrm{cambio~en~EDUCATION}) = 2.06\\%\\). Sin embargo, esta proximidad generalmente se considera adecuada para fines de interpretación. Continuando con esta lógica, consideremos pequeños cambios en el ingreso logarítmico. Efectos de Pequeños Cambios en el Ingreso Logarítmico LNINCOME 11 11.1 11.2 11.3 INCOME 59,874 66,171 73,130 80,822 INCOME % Cambio 10.52 10.52 10.52 \\(\\widehat{\\mathit{LNFACE}}\\) 11.957 12.006 12.055 12.105 \\(\\widehat{\\mathit{FACE}}\\) 155,831 163,722 172,013 180,724 \\(\\widehat{\\mathit{FACE}}\\) % Cambio 5.06 5.06 5.06 \\(\\widehat{\\mathit{FACE}}\\) % Cambio / INCOME % Cambio 0.482 0.482 0.482 Podemos usar la misma lógica para interpretar el coeficiente LNINCOME en la ecuación (3.4). A medida que el ingreso logarítmico aumenta en 0.1 unidades, se espera que la demanda de seguros aumente en un 5.06%. Esto se refiere a las unidades logarítmicas en \\(y\\) pero no en \\(x\\). Podemos usar la misma lógica para decir que a medida que el ingreso logarítmico aumenta en 0.1 unidades, el INCOME aumenta en un 10.52%. Por lo tanto, un cambio del 10.52% en el INCOME corresponde a un cambio del 5.06% en el FACE. Resumiendo, decimos que, manteniendo NUMHH y EDUCATION fijos, esperamos que un aumento del 1% en el INCOME esté asociado con un aumento del 0.482% en \\(\\widehat{\\mathrm{FACE}}\\) (como antes, esto es cercano a la estimación del parámetro \\(b_3 = 0.494\\)). El coeficiente asociado con el ingreso se conoce como elasticidad en economía. En economía, la elasticidad es la relación entre el cambio porcentual en una variable y el cambio porcentual en otra variable. Matemáticamente, resumimos esto como: \\[ \\frac{\\partial \\ln y}{\\partial \\ln x} = \\left(\\frac{\\partial y}{y}\\right)/\\left(\\frac{\\partial x}{x}\\right). \\] 3.2.3 Suposiciones del Modelo Como en la Sección 2.2 para una sola variable explicativa, existen dos conjuntos de suposiciones que se pueden utilizar para la regresión lineal múltiple. Son conjuntos equivalentes, cada uno con ventajas comparativas a medida que avanzamos en nuestro estudio de la regresión. La representación de “observables” se centra en las variables de interés \\((x_{i1}, \\ldots, x_{ik}, y_i)\\). La “representación del error” proporciona una base para motivar nuestras medidas de bondad de ajuste y el estudio del análisis de residuales. Sin embargo, el segundo conjunto de suposiciones se centra en el caso de errores aditivos y oscurece la base de muestreo del modelo. \\[ \\textbf{Suposiciones de Muestreo del Modelo de Regresión Lineal Múltiple} \\\\ \\small{ \\begin{array}{ll} \\text{Representación de Observables} &amp; \\text{Representación de Error} \\\\ \\hline F1.~ \\mathrm{E}~y_i=\\beta_0+\\beta_1 x_{i1}+\\ldots+\\beta_k x_{ik}. &amp; E1.~ y_i=\\beta_0+\\beta_1 x_{i1}+\\ldots+\\beta_k x_{ik}+\\varepsilon_i. \\\\ F2.~ \\{x_{i1},\\ldots ,x_{ik}\\} &amp; E2.~ \\{x_{i1},\\ldots ,x_{ik}\\} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{son variables no estocásticas.} &amp; \\ \\ \\ \\ \\ \\ \\ \\ \\text{son variables no estocásticas.} \\\\ F3.~ \\mathrm{Var}~y_i=\\sigma^2. &amp; E3.~ \\mathrm{E}~\\varepsilon_i=0 \\text{ y } \\mathrm{Var}~\\varepsilon_i=\\sigma^2. \\\\ F4.~ \\{y_i\\} \\text{ son variables aleatorias independientes.} &amp; E4.~ \\{\\varepsilon_i\\} \\text{ son variables aleatorias independientes.} \\\\ F5.~ \\{y_i\\} \\text{ están distribuidos normalmente.} &amp; E5.~ \\{\\varepsilon_i\\} \\text{ están distribuidos normalmente.} \\\\ \\hline \\end{array} } \\] Para motivar aún más las Suposiciones F2 y F4, generalmente asumimos que nuestros datos han sido obtenidos como resultado de un esquema de muestreo estratificado, donde cada valor único de \\(\\{x_{i1}, \\ldots, x_{ik}\\}\\) se trata como un estrato. Es decir, para cada valor de \\(\\{x_{i1}, \\ldots, x_{ik}\\}\\), tomamos una muestra aleatoria de respuestas de una población. Así, las respuestas dentro de cada estrato son independientes entre sí, al igual que las respuestas de diferentes estratos. El Capítulo 6 discutirá esta base de muestreo en mayor detalle. 3.2.4 Propiedades de los Estimadores de los Coeficientes de Regresión La Sección 3.1 describió el método de mínimos cuadrados para estimar los coeficientes de regresión. Con las suposiciones del modelo de regresión, podemos establecer algunas propiedades básicas de estos estimadores. Para hacerlo, de la Sección 2.11.4, tenemos que la esperanza de un vector es el vector de esperanzas, de modo que \\[ \\small{ \\mathrm{E}~\\mathbf{y} = \\left( \\begin{array}{l} \\mathrm{E}~y_1 \\\\ \\mathrm{E}~y_2 \\\\ \\vdots \\\\ \\mathrm{E}~y_n \\end{array} \\right) . } \\] Además, la multiplicación básica de matrices muestra que \\[ \\small{ \\mathbf{X} \\boldsymbol \\beta = \\left( \\begin{array}{cccc} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; \\cdots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{nk} \\end{array} \\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) = \\left( \\begin{array}{c} \\beta_0 + \\beta_1 x_{11} + \\cdots + \\beta_k x_{1k} \\\\ \\beta_0 + \\beta_1 x_{21} + \\cdots + \\beta_k x_{2k} \\\\ \\vdots \\\\ \\beta_0 + \\beta_1 x_{n1} + \\cdots + \\beta_k x_{nk} \\end{array} \\right) . } \\] Dado que la \\(i\\)-ésima fila de la suposición F1 es \\(\\mathrm{E}~y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik}\\), podemos reescribir esta suposición en forma matricial como \\(\\mathrm{E}~\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta\\). Ahora estamos en condiciones de enunciar la primera propiedad importante de los estimadores de regresión por mínimos cuadrados. Propiedad 1. Consideremos un modelo de regresión y que se cumplan las Suposiciones F1-F4. Entonces, el estimador \\(\\mathbf{b}\\) definido en la ecuación (3.4) es un estimador insesgado del vector de parámetros \\(\\boldsymbol \\beta\\). Para establecer la Propiedad 1, tenemos que \\[ \\begin{array}{ll} \\mathrm{E}~\\mathbf{b} &amp; = \\mathrm{E}~\\left((\\mathbf{X^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathbf{y}\\right) = (\\mathbf{X^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathrm{E}~\\mathbf{y} \\\\ &amp;= (\\mathbf{X^{\\prime}X)}^{-1} \\mathbf{X}^{\\prime} \\left( \\mathbf{X} \\boldsymbol \\beta \\right) = \\boldsymbol \\beta, \\end{array} \\] utilizando reglas de multiplicación de matrices. Este capítulo asume que \\(\\mathbf{X^{\\prime}X}\\) es invertible. También se puede mostrar que el estimador de mínimos cuadrados solo necesita ser una solución de las ecuaciones normales para ser insesgado (sin requerir que \\(\\mathbf{X^{\\prime}X}\\) sea invertible, ver Sección 4.7.3). Así, se dice que \\(\\mathbf{b}\\) es un estimador insesgado de \\(\\boldsymbol \\beta\\). En particular, \\(\\mathrm{E}~b_j = \\beta_j\\) para \\(j = 0,1,\\ldots,k\\). Dado que la independencia implica covarianza cero, de la Suposición F4 tenemos que \\(\\mathrm{Cov}(y_i,y_j) = 0\\) para \\(i \\neq j\\). A partir de esto, de la Suposición F3 y de la definición de la varianza de un vector, tenemos que \\[ \\small{ \\begin{array}{ll} \\mathrm{Var~}\\mathbf{y} &amp;= \\left( \\begin{array}{cccc} \\mathrm{Var~}y_1 &amp; \\mathrm{Cov}(y_1,y_2) &amp; \\cdots &amp; \\mathrm{Cov}(y_1,y_n) \\\\ \\mathrm{Cov}(y_2,y_1) &amp; \\mathrm{Var~}y_2 &amp; \\cdots &amp; \\mathrm{Cov}(y_2,y_n) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathrm{Cov}(y_n,y_1) &amp; \\mathrm{Cov}(y_n,y_2) &amp; \\cdots &amp; \\mathrm{Var~}y_n \\end{array} \\right) \\\\ &amp;= \\left( \\begin{array}{cccc} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{array} \\right) = \\sigma^2 \\mathbf{I}, \\end{array} } \\] donde \\(\\mathbf{I}\\) es una matriz identidad de \\(n \\times n\\). Ahora estamos en condiciones de enunciar la segunda propiedad importante de los estimadores de regresión por mínimos cuadrados. Propiedad 2. Consideremos un modelo de regresión y que se cumplan las Suposiciones F1-F4. Entonces, el estimador \\(\\mathbf{b}\\) definido en la ecuación (3.4) tiene una varianza de \\(\\mathrm{Var~}\\mathbf{b} = \\sigma^2(\\mathbf{X^{\\prime}X)}^{-1}\\). Para establecer la Propiedad 2, a partir de la propiedad de la matriz de varianza, tenemos que \\[ \\small{ \\begin{array}{ll} \\mathrm{Var~}\\mathbf{b} &amp; = \\mathrm{Var~}\\left((\\mathbf{X^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathbf{y}\\right) = (\\mathbf{X^{\\prime}X)}^{-1} \\mathbf{X}^{\\prime} \\mathrm{Var~}\\mathbf{y} \\mathbf{X} (\\mathbf{X^{\\prime}X)}^{-1} \\\\ &amp;= (\\mathbf{X^{\\prime}X)}^{-1} \\mathbf{X}^{\\prime} \\sigma^2 \\mathbf{I} \\mathbf{X} (\\mathbf{X^{\\prime}X)}^{-1} = \\sigma^2 (\\mathbf{X^{\\prime}X)}^{-1}, \\end{array} } \\] como se requiere. Esta propiedad importante nos permitirá medir la precisión del estimador \\(\\mathbf{b}\\) cuando discutamos la inferencia estadística. Específicamente, por la definición de la varianza de un vector (ver Sección 2.11.4), \\[\\begin{equation} \\small{ \\mathrm{Var~}\\mathbf{b}= \\begin{pmatrix} \\mathrm{Var~}b_0 &amp; \\mathrm{Cov}(b_0,b_1) &amp; \\cdots &amp; \\mathrm{Cov}(b_0,b_k) \\\\ \\mathrm{Cov}(b_1,b_0) &amp; \\mathrm{Var~}b_1 &amp; \\cdots &amp; \\mathrm{Cov}(b_1,b_k) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathrm{Cov}(b_k,b_0) &amp; \\mathrm{Cov}(b_k,b_1) &amp; \\cdots &amp; \\mathrm{Var~}b_k \\end{pmatrix} = \\sigma^2 (\\mathbf{X^{\\prime}X)}^{-1}. } \\tag{3.6} \\end{equation}\\] Así, por ejemplo, \\(\\mathrm{Var~}b_j\\) es \\(\\sigma^2\\) veces la entrada diagonal \\((j+1)\\) de \\((\\mathbf{X^{\\prime}X)}^{-1}\\). Como otro ejemplo, \\(\\mathrm{Cov}(b_0,b_j)\\) es \\(\\sigma^2\\) veces el elemento en la primera fila y la columna \\((j+1)\\) de \\((\\mathbf{X^{\\prime}X)}^{-1}\\). Aunque existen métodos alternativos que son preferibles para aplicaciones específicas, los estimadores de mínimos cuadrados han demostrado ser efectivos para muchos análisis de datos rutinarios. Una característica deseable de los estimadores de regresión por mínimos cuadrados se resume en el siguiente resultado bien conocido. Teorema de Gauss-Markov: Consideremos el modelo de regresión y supongamos que se cumplen las Suposiciones F1-F4. Entonces, dentro de la clase de estimadores que son funciones lineales de las respuestas, el estimador de mínimos cuadrados \\(\\mathbf{b}\\) definido en la ecuación (3.4) es el estimador insesgado con la varianza mínima del vector de parámetros \\(\\boldsymbol{\\beta}\\). El teorema de Gauss-Markov establece que el estimador de mínimos cuadrados es el más preciso en el sentido de que tiene la menor varianza. Ya hemos visto en la Propiedad 1 que los estimadores de mínimos cuadrados son insesgados. El teorema de Gauss-Markov afirma que el estimador de mínimos cuadrados es el más preciso en el sentido de que tiene la menor varianza. (En un contexto matricial, “varianza mínima” significa que si \\(\\mathbf{b}^{\\ast}\\) es cualquier otro estimador, entonces la diferencia de las matrices de varianza, \\(\\mathrm{Var~} \\mathbf{b}^{\\ast} - \\mathrm{Var~}\\mathbf{b}\\), es semidefinida no negativa.) Una propiedad adicional importante se refiere a la distribución de los estimadores de regresión por mínimos cuadrados. Propiedad 3: Consideremos un modelo de regresión y supongamos que se cumplen las Suposiciones F1-F5. Entonces, el estimador de mínimos cuadrados \\(\\mathbf{b}\\) definido en la ecuación (3.4) está distribuido normalmente. Para establecer la Propiedad 3, definimos los vectores de peso, \\(\\mathbf{w}_i = (\\mathbf{X^{\\prime}X)}^{-1}(1, x_{i1}, \\ldots, x_{ik})^{\\prime}\\). Con esta notación, observamos que \\[ \\mathbf{b} = (\\mathbf{X^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathbf{y} = \\sum_{i=1}^{n} \\mathbf{w}_i y_i, \\] de modo que \\(\\mathbf{b}\\) es una combinación lineal de respuestas. Con la Suposición F5, las respuestas están distribuidas normalmente. Debido a que las combinaciones lineales de variables aleatorias normalmente distribuidas también están distribuidas normalmente, tenemos la conclusión de la Propiedad 3. Este resultado sustenta gran parte de la inferencia estadística que se presentará en las Secciones 3.4 y 4.2. 3.3 Estimación y Bondad de Ajuste Desviación Estándar Residual Se discutirán propiedades adicionales de los estimadores de los coeficientes de regresión cuando nos enfoquemos en la inferencia estadística. Ahora continuamos nuestra discusión sobre la estimación proporcionando un estimador del otro parámetro en el modelo de regresión lineal, \\(\\sigma^2\\). Nuestro estimador para \\(\\sigma^2\\) puede desarrollarse utilizando el principio de reemplazar expectativas teóricas por promedios muestrales. Examinando \\(\\sigma^2 = \\mathrm{E}\\left( y-\\mathrm{E~}y\\right)^2\\), al reemplazar la expectativa externa por un promedio muestral, se sugiere utilizar el estimador \\(n^{-1}\\sum_{i=1}^{n}(y_i-\\mathrm{E~}y_i)^2\\). Dado que no observamos \\(\\mathrm{E}~y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik}\\), utilizamos en su lugar la cantidad observada correspondiente \\(b_0 + b_1 x_{i1} + \\ldots + b_k x_{ik} = \\widehat{y}_i\\). Esto conduce a lo siguiente. Definición. Un estimador de \\(\\sigma^2\\), el error cuadrático medio (MSE), se define como \\[\\begin{equation} s^2 = \\frac{1}{n-(k+1)}\\sum_{i=1}^{n}\\left( y_i - \\widehat{y}_i \\right)^2. \\tag{3.7} \\end{equation}\\] La raíz cuadrada positiva, \\(s = \\sqrt{s^2}\\), se llama la desviación estándar residual. Esta expresión generaliza la definición en la ecuación (2.3), que es válida para \\(k=1\\). Resulta que, al usar \\(n-(k+1)\\) en lugar de \\(n\\) en el denominador de la ecuación (3.7), \\(s^2\\) es un estimador insesgado de \\(\\sigma^2\\). Esencialmente, al usar \\(\\widehat{y}_i\\) en lugar de \\(\\mathrm{E~}y_i\\) en la definición, hemos introducido algunas pequeñas dependencias entre las desviaciones de las respuestas \\(y_i - \\widehat{y}_i\\), reduciendo así la variabilidad total. Para compensar esta menor variabilidad, también reducimos el denominador en la definición de \\(s^2\\). Para proporcionar más intuición sobre la elección de \\(n-(k+1)\\) en la definición de \\(s^2\\), introducimos el concepto de residuales en el contexto de la regresión lineal múltiple. A partir de la Suposición E1, recordemos que los errores aleatorios pueden expresarse como \\(\\varepsilon_i = y_i - (\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik})\\). Dado que los parámetros \\(\\beta_0, \\ldots, \\beta_k\\) no se observan, los errores en sí mismos no se observan. En su lugar, examinamos los “errores estimados”, o residuales, definidos por \\(e_i = y_i - \\widehat{y}_i\\). A diferencia de los errores, existen ciertas dependencias entre los residuales. Una dependencia se debe al hecho algebraico de que el residual promedio es cero. Además, debe haber al menos \\(k+2\\) observaciones para que haya variación en el ajuste del plano. Si solo tenemos \\(k+1\\) observaciones, podríamos ajustar un plano a los datos perfectamente, resultando en ninguna variación en el ajuste. Por ejemplo, si \\(k=1\\), dado que dos observaciones determinan una línea, se requieren al menos tres observaciones para observar cualquier desviación de la línea. Debido a estas dependencias, solo tenemos \\(n-(k+1)\\) residuales libres, o no restringidos, para estimar la variabilidad en torno al plano de regresión. La raíz cuadrada positiva de \\(s^2\\) es nuestro estimador de \\(\\sigma\\). Usando los residuales, se puede expresar como \\[\\begin{equation} s = \\sqrt{\\frac{1}{n-(k+1)}\\sum_{i=1}^{n}e_i^2}. \\tag{3.8} \\end{equation}\\] Debido a que se basa en residuales, nos referimos a \\(s\\) como la desviación estándar residual. La cantidad \\(s\\) es una medida de nuestro “error típico”. Por esta razón, \\(s\\) también se llama el error estándar de la estimación. El Coeficiente de Determinación: \\(R^2\\) Para resumir la bondad de ajuste del modelo, como en el Capítulo 2, particionamos la variabilidad en partes que son “explicadas” y “no explicadas” por el ajuste de la regresión. Algebraicamente, los cálculos para la regresión utilizando muchas variables son similares al caso de usar solo una variable. Desafortunadamente, cuando se trata de muchas variables, perdemos la fácil interpretación gráfica como en la Figura 2.4. Comenzamos con la suma total de desviaciones cuadradas, \\(Total~SS = \\sum_{i=1}^{n}\\left( y_i - \\overline{y} \\right)^2\\), como nuestra medida de la variación total en el conjunto de datos. Como en la ecuación (2.1), podemos interpretar la ecuación \\[ \\small{ \\begin{array}{ccccc} \\underbrace{y_i - \\overline{y}} &amp; = &amp; \\underbrace{y_i - \\widehat{y}_i} &amp; + &amp; \\underbrace{\\widehat{y}_i - \\overline{y}} \\\\ \\text{desviación total} &amp; = &amp; \\text{desviación no explicada} &amp; + &amp; \\text{desviación explicada} \\\\ \\end{array} } \\] como “la desviación sin conocimiento de las variables explicativas es igual a la desviación no explicada por las variables explicativas más la desviación explicada por las variables explicativas”. Al cuadrar cada lado y sumar sobre todas las observaciones se obtiene \\[ Total~SS = Error~SS + Regression~SS \\] donde \\(Error~SS = \\sum_{i=1}^{n}\\left( y_i - \\widehat{y}_i \\right)^2\\) y \\(Regression~SS = \\sum_{i=1}^{n}\\left( \\widehat{y}_i - \\overline{y} \\right)^2\\). Como en la Sección 2.3 para el caso de una variable explicativa, la suma de los términos de producto cruzado resulta ser cero. Una estadística que resume esta relación es el coeficiente de determinación, \\[ R^2 = \\frac{Regression~SS}{Total~SS}. \\] Interpretamos \\(R^2\\) como la proporción de variabilidad explicada por la función de regresión. Si el modelo es adecuado para los datos, se esperaría una fuerte relación entre las respuestas observadas y las “esperadas” bajo el modelo, los valores ajustados. Un hecho algebraico interesante es el siguiente. Si uno eleva al cuadrado el coeficiente de correlación entre las respuestas y los valores ajustados, obtenemos el coeficiente de determinación, es decir, \\[ R^2 = \\left[ r \\left(y, \\widehat{y} \\right) \\right]^2. \\] Como resultado, \\(R\\), la raíz cuadrada positiva de \\(R^2\\), se llama el coeficiente de correlación múltiple. Se puede interpretar como la correlación entre la respuesta y la mejor combinación lineal de las variables explicativas, los valores ajustados. (Esta relación se desarrolla utilizando álgebra matricial en el apéndice técnico Sección 5.10.1.) La descomposición de la variabilidad también se resume utilizando la tabla de análisis de varianza, o ANOVA, como sigue. \\[ \\small{ \\begin{array}{l|lcl} \\hline \\text{Fuente} &amp; \\text{Suma de Cuadrados} &amp; df &amp; \\text{Cuadrado Medio} \\\\ \\hline \\text{Regresión} &amp; Regression~SS &amp; k &amp; Regression~MS \\\\ \\text{Error} &amp; Error~SS &amp; n - (k + 1) &amp; MSE \\\\ \\text{Total} &amp; Total~SS &amp; n - 1 &amp; \\\\ \\hline \\end{array} } \\] Las cifras de la columna de cuadrados medios se definen como las cifras de suma de cuadrados divididas por sus respectivos grados de libertad. Los grados de libertad del error denotan el número de residuales no restringidos. Es este número el que utilizamos en nuestra definición del “promedio” o error cuadrático medio. Es decir, definimos \\[ MSE = Error~MS = \\frac{Error~SS}{n - (k + 1)} = s^2. \\] De manera similar, los grados de libertad de la regresión son el número de variables explicativas. Esto da como resultado \\[ Regression~MS = \\frac{Regression~SS}{k}. \\] Al hablar del coeficiente de determinación, se puede establecer que siempre que se añade una variable explicativa al modelo, \\(R^2\\) nunca disminuye. Esto es cierto, independientemente de si la variable adicional es útil o no. Nos gustaría una medida de ajuste que disminuyera cuando se introducen variables inútiles en el modelo como variables explicativas. Para evitar esta anomalía, una estadística ampliamente utilizada es el coeficiente de determinación ajustado por grados de libertad, definido por \\[\\begin{equation} R_{a}^2 = 1 - \\frac{(Error~SS) / [n - (k + 1)]}{(Total~SS) / (n - 1)} = 1 - \\frac{s^2}{s_{y}^2}. \\tag{3.9} \\end{equation}\\] Para interpretar esta estadística, observe que \\(s_y^2\\) no depende del modelo ni de las variables del modelo. Por lo tanto, \\(s^2\\) y \\(R_a^2\\) son medidas equivalentes de ajuste del modelo. A medida que el ajuste del modelo mejora, \\(R_{a}^2\\) se hace más grande y \\(s^2\\) se hace más pequeño, y viceversa. Dicho de otro modo, elegir un modelo con el menor \\(s^2\\) es equivalente a elegir un modelo con el mayor \\(R_a^2\\). Ejemplo: Seguro de Vida a Término - Continuación. Para ilustrar, la Tabla 3.3 muestra las estadísticas resumen para la regresión de LNFACE sobre EDUCATION, NUMHH y LNINCOME. A partir de la columna de grados de libertad, recordamos que hay tres variables explicativas y 275 observaciones. Como medidas de ajuste del modelo, el coeficiente de determinación es \\(R^2 = 34.3\\%\\) (=\\(328.47 / 958.90\\)) y la desviación estándar residual es \\(s = 1.525\\) (=\\(\\sqrt{2.326}\\)). Si intentáramos estimar el monto nominal logarítmico sin conocimiento de las variables explicativas EDUCATION, NUMHH y LNINCOME, entonces el tamaño del error típico sería \\(s_y = 1.871\\) (=\\(\\sqrt{958.90 / 274}\\)). Así, al aprovechar nuestro conocimiento de las variables explicativas, hemos podido reducir el tamaño del error típico. La medida de ajuste del modelo que compara estas dos estimaciones de variabilidad es el coeficiente de determinación ajustado, \\(R_a^2 = 1 - 2.326 / 1.871^2 = 33.6\\%\\). Tabla 3.3: Tabla ANOVA para Seguro de Vida a Término Suma de Cuadrados \\(df\\) Cuadrado Medio Regresión 328.47 3 109.49 Error 630.43 271 2.326 Total 958.9 274 Ejemplo: ¿Por qué las Mujeres Viven Más que los Hombres? En un artículo con este título, Lemaire (2002) examinó lo que llamó la “ventaja femenina”, la diferencia en la esperanza de vida entre mujeres y hombres. Las esperanzas de vida son de interés porque se utilizan ampliamente como medidas de la salud de una nación. Lemaire examinó datos de \\(n = 169\\) países y encontró que la ventaja femenina promedio era de 4.51 años en todo el mundo. Buscó explicar esta diferencia basándose en 45 medidas de comportamiento, variables que capturan el grado de modernización económica de una nación, normas sociales/culturales/religiosas, posición geográfica y calidad de la atención médica disponible. Después de un análisis detallado, Lemaire reporta los coeficientes de un modelo de regresión que aparecen en la Tabla 3.4. Este modelo de regresión explica \\(R^2 = 61\\%\\) de la variabilidad. Es un modelo parsimonioso que consiste en solo \\(k = 4\\) de las 45 variables originales. Tabla 3.4. Coeficientes de Regresión de un Modelo de la Ventaja Femenina \\[ \\small{ \\begin{array}{l|rr} \\hline \\text{Variable} &amp; \\text{Coeficiente} &amp; t\\text{-estadística} \\\\ \\hline \\text{Intercepto} &amp; 9.904 &amp; 12.928 \\\\ \\text{Número Logarítmico de Personas por Médico} &amp; -0.473 &amp; -3.212 \\\\ \\text{Fertilidad} &amp; -0.444 &amp; -3.477 \\\\ \\text{Porcentaje de Hindúes y Budistas} &amp; -0.018 &amp; -3.196 \\\\ \\text{Indicador de la Unión Soviética} &amp; 4.922 &amp; 7.235 \\\\ \\hline \\end{array} } \\] Fuente: Lemaire (2002) Todas las variables fueron estadísticamente significativas. El número de personas por médico también estaba correlacionado con otras variables que capturan el grado de modernización económica de un país, como la urbanización, el número de automóviles y el porcentaje de personas que trabajan en la agricultura. La fertilidad, el número de nacimientos por mujer, estaba altamente correlacionada con las variables de educación en el estudio, incluyendo el analfabetismo femenino y la matriculación escolar femenina. El porcentaje de hindúes y budistas es una variable social/cultural/religiosa. El indicador de la Unión Soviética es una variable geográfica: caracteriza a los países de Europa del Este que pertenecían anteriormente a la Unión Soviética. Debido al alto grado de colinealidad entre las 45 variables candidatas, otros analistas podrían fácilmente elegir un conjunto alternativo de variables. No obstante, el punto importante de Lemaire fue que este modelo simple explica aproximadamente el 61% de la variabilidad basada únicamente en variables de comportamiento, no relacionadas con las diferencias biológicas entre sexos. 3.4 Inferencia Estadística para un Coeficiente Único 3.4.1 La Prueba t En muchas aplicaciones, una sola variable es de interés principal, y otras variables se incluyen en la regresión para controlar fuentes adicionales de variabilidad. Para ilustrar, un agente de ventas podría estar interesado en el efecto que tiene el ingreso sobre la cantidad de seguros demandados. En un análisis de regresión, también se podrían incluir otras variables explicativas como el género de un individuo, tipo de ocupación, edad, tamaño del hogar, nivel educativo, etc. Al incluir estas variables explicativas adicionales, esperamos obtener una mejor comprensión de la relación entre el ingreso y la demanda de seguros. Para llegar a conclusiones sensatas, necesitaremos algunas reglas para decidir si una variable es importante o no. Respondemos a la pregunta “¿Es \\(x_j\\) importante?” investigando si el parámetro de pendiente correspondiente, \\(\\beta_j\\), es igual a cero. La pregunta de si \\(\\beta_j\\) es cero se puede replantear en el marco de la prueba de hipótesis como “¿Es válida \\(H_0:\\beta_j=0\\)?” Examinamos la proximidad de \\(b_j\\) a cero para determinar si \\(\\beta_j\\) es cero o no. Dado que las unidades de \\(b_j\\) dependen de las unidades de \\(y\\) y \\(x_j\\), necesitamos estandarizar esta cantidad. A partir de la Propiedad 2 y la ecuación (3.6), vimos que \\(\\mathrm{Var~}b_j\\) es \\(\\sigma^2\\) multiplicado por el elemento diagonal \\((j+1)^{st}\\) de \\((\\mathbf{X^{\\prime}X})^{-1}\\). Reemplazando \\(\\sigma^2\\) por el estimador \\(s^2\\) y tomando raíces cuadradas, tenemos lo siguiente. Definición. El error estándar de \\(b_j\\) se puede expresar como \\[ se(b_j) = s \\sqrt{\\text{elemento diagonal (j+1)st de } (\\mathbf{X^{\\prime}X})^{-1}}. \\] Recuerda que un error estándar es una desviación estándar estimada. Para probar \\(H_0:\\beta_j=0\\), examinamos la razón \\(t\\), \\(t(b_j) = \\frac{b_j}{se(b_j)}\\). Interpretamos \\(t(b_j)\\) como el número de errores estándar que \\(b_j\\) está alejado de cero. Esta es la cantidad adecuada porque se puede demostrar que la distribución de muestreo de \\(t(b_j)\\) es la distribución \\(t\\) con \\(df=n-(k+1)\\) grados de libertad, bajo la hipótesis nula y con los supuestos del modelo de regresión lineal F1-F5. Esto nos permite construir pruebas de la hipótesis nula como el siguiente procedimiento: Procedimiento. La Prueba t para un Coeficiente de Regresión (\\(\\beta\\)). La hipótesis nula es \\(H_0:\\beta_j=0\\). La hipótesis alternativa es \\(H_{a}:\\beta_j \\neq 0\\). Establecer un nivel de significancia \\(\\alpha\\) (típicamente, pero no necesariamente, 5%). Construir la estadística, \\(t(b_j) = \\frac{b_j}{se(b_j)}\\). Procedimiento: Rechazar la hipótesis nula a favor de la alternativa si \\(|t(b_j)|\\) excede un valor \\(t\\). Aquí, este valor \\(t\\) es el percentil \\((1-\\alpha /2)^{th}\\) de la distribución \\(t\\) con \\(df=n-(k+1)\\) grados de libertad, denotado como \\(t_{n-(k+1),1-\\alpha /2}\\). En muchas aplicaciones, el tamaño de la muestra será lo suficientemente grande como para que podamos aproximar el valor \\(t\\) por el percentil correspondiente de la curva normal estándar. Al nivel de significancia del 5%, este percentil es 1.96. Así, como regla general, podemos interpretar que una variable es importante si su razón \\(t\\) excede dos en valor absoluto. Aunque es la más común, probar \\(H_0:\\beta_j=0\\) frente a \\(H_{a}:\\beta_j \\neq 0\\) es solo una de las muchas pruebas de hipótesis que se pueden realizar. Tabla 3.5 describe procedimientos alternativos para la toma de decisiones. Estos procedimientos son para probar \\(H_0:\\beta_j = d\\). Aquí, \\(d\\) es un valor prescrito por el usuario que puede ser igual a cero o cualquier otro valor conocido. Tabla 3.5. Procedimientos de Toma de Decisiones para Probar \\(H_0: \\beta_j = d\\) \\[ \\small{ \\begin{array}{cc} \\hline \\text{Hipótesis Alternativa }(H_{a}) &amp; \\text{Procedimiento: Rechazar } H_0 \\text{ en favor de } H_a \\text{ si }\\\\ \\hline \\beta_j &gt; d &amp; t-\\mathrm{ratio}&gt;t_{n-(k+1),1-\\alpha } \\\\ \\beta_j &lt; d &amp; t-\\mathrm{ratio}&lt;-t_{n-(k+1),1-\\alpha } \\\\ \\beta_j\\neq d &amp; |t-\\mathrm{ratio}\\mathit{|}&gt;t_{n-(k+1),1-\\alpha/2} \\end{array} \\\\ \\begin{array}{ll}\\hline \\textit{Notas:} &amp;\\text{ El nivel de significancia es } \\alpha. \\text{ Aquí, } t_{n-(k+1),1-\\alpha}\\text{ es el }(1-\\alpha)^{th}\\text{ percentil} \\\\ &amp;~~\\text{de la distribución }t-\\text{utilizando } df=n-(k+1)\\text{ grados de libertad.} \\\\ &amp;~~\\text{La estadística de prueba es }t-\\mathrm{ratio} = (b_j -d)/se(b_j) . \\\\ \\hline \\end{array} } \\] Alternativamente, se pueden construir valores \\(p\\) y compararlos con niveles de significancia dados. El valor \\(p\\) permite al lector del informe entender la fuerza de la desviación de la hipótesis nula. Tabla 3.6 resume el procedimiento para calcular los valores \\(p\\). Tabla 3.6. Valores de Probabilidad para Probar \\(H_0:\\beta_j =d\\) \\[ \\small{ \\begin{array}{cccc} \\hline \\text{Hipótesis} &amp; &amp; &amp; \\\\ \\text{Alternativa} (H_a ) &amp; \\beta_j &gt; d &amp; \\beta_j &lt; d &amp; \\beta_j \\neq d \\\\ \\hline p-valor &amp; \\Pr(t_{n-(k+1)}&gt;t-\\mathrm{ratio}) &amp; \\Pr(t_{n-(k+1)}&lt;t-\\mathrm{ratio}) &amp; \\Pr(|t_{n-(k+1)}|&gt;|t-\\mathrm{ratio}|) \\\\ \\end{array} \\\\ \\begin{array}{ll}\\hline \\textit{Notas:} &amp; \\text{ Aquí, } t_{n-(k+1)} \\text{ es una variable aleatoria con distribución }t\\text{ con }df=n-(k+1)\\text{ grados de libertad.} \\\\ &amp;~~\\text{La estadística de prueba es }t-\\mathrm{ratio} = (b_j -d)/se(b_j) . \\\\ \\hline \\end{array} } \\] Ejemplo: Seguro de Vida Temporal - Continuación. Una convención útil al reportar los resultados de un análisis estadístico es colocar el error estándar de una estadística entre paréntesis debajo de esa estadística. Así, por ejemplo, en nuestra regresión de LNFACE sobre EDUCATION, NUMHH, y LNINCOME, la ecuación de regresión estimada es: \\[ \\begin{array}{lccccc} \\widehat{LNFACE} = &amp;2.584 ~ + &amp;0.206~ \\text{EDUCATION} + &amp;0.306 ~\\text{NUMHH} + &amp;0.494 ~\\text{LNINCOME}. \\\\ \\text{error estándar} &amp;(0.846) &amp;(0.039) &amp;(0.063) &amp;(0.078). \\end{array} \\] Para ilustrar el cálculo de los errores estándar, primero notemos que, según la Tabla 3.3, tenemos que la desviación estándar residual es \\(s=1.525\\). Usando un paquete estadístico, tenemos \\[ \\small{ (\\mathbf{X^{\\prime}X})^{-1} = \\begin{pmatrix} 0.307975 &amp; -0.004633 &amp; -0.002131 &amp; -0.020697 \\\\ -0.004633 &amp; 0.000648 &amp; 0.000143 &amp; -0.000467 \\\\ -0.002131 &amp; 0.000143 &amp; 0.001724 &amp; -0.000453 \\\\ -0.020697 &amp; -0.000467 &amp; -0.000453 &amp; 0.002585 \\end{pmatrix}. } \\] Para ilustrar, podemos calcular \\(se(b_3)=s \\times \\sqrt{0.002585} = 0.078\\), como se indicó antes. El cálculo de los errores estándar, así como las correspondientes estadísticas \\(t\\), es parte del resultado estándar de los programas estadísticos y no necesita ser realizado por los usuarios. Nuestro objetivo aquí es ilustrar las ideas subyacentes a los cálculos rutinarios. Con esta información, podemos calcular inmediatamente las razones \\(t\\) para verificar si un coeficiente asociado con una variable individual es significativamente diferente de cero. Por ejemplo, la razón \\(t\\) para la variable LNINCOME es \\(t(b_3) = \\frac{0.494}{0.078} = 6.3\\). La interpretación es que \\(b_3\\) está más de cuatro errores estándar por encima de cero, y por lo tanto LNINCOME es una variable importante en el modelo. Más formalmente, podemos estar interesados en probar la hipótesis nula de que \\(H_0:\\beta_3 = 0\\) frente a \\(H_0:\\beta_3 \\neq 0\\). A un nivel de significancia del 5%, el valor \\(t\\) es 1.96, porque \\(df=275-(1+3)=271\\). Por lo tanto, rechazamos la hipótesis nula en favor de la hipótesis alternativa, que el ingreso logarítmico (LNINCOME) es importante para determinar el monto logarítmico del seguro. 3.4.2 Intervalos de Confianza Los intervalos de confianza para los parámetros son otra forma de describir la fuerza de la contribución de la \\(j\\)-ésima variable explicativa. La estadística \\(b_j\\) se llama una estimación puntual del parámetro \\(\\beta_j\\). Para proporcionar un rango de confianza, usamos el intervalo de confianza: \\[\\begin{equation} b_j \\pm t_{n-(k+1),1-\\alpha /2}~se(b_j). \\tag{3.10} \\end{equation}\\] Aquí, el valor \\(t\\), \\(t_{n-(k+1),1-\\alpha /2}\\), es un percentil de la distribución \\(t\\) con \\(df=n-(k+1)\\) grados de libertad. Usamos el mismo valor \\(t\\) que en la prueba de hipótesis bilateral. De hecho, hay una dualidad entre el intervalo de confianza y la prueba de hipótesis bilateral. Por ejemplo, no es difícil comprobar que si un valor hipotético cae fuera del intervalo de confianza, entonces \\(H_0\\) será rechazado en favor de \\(H_{a}\\). Además, el conocimiento del \\(p\\)-valor, la estimación puntual y el error estándar se puede utilizar para determinar un intervalo de confianza. 3.4.3 Gráficos de Variables Añadidas Para representar datos multivariados de forma gráfica, hemos visto que una matriz de dispersión es una herramienta útil. Sin embargo, la principal limitación de la matriz de dispersión es que solo captura relaciones entre pares de variables. Cuando los datos se pueden resumir utilizando un modelo de regresión, una herramienta gráfica que no tiene esta limitación es el gráfico de variables añadidas. El gráfico de variables añadidas también se llama gráfico de regresión parcial porque, como veremos, se construye en términos de los residuos de ciertos ajustes de regresión. También veremos que el gráfico de variables añadidas se puede resumir en términos de un coeficiente de correlación parcial, proporcionando así un vínculo entre correlación y regresión. Para introducir estas ideas, trabajamos en el contexto del siguiente ejemplo. Ejemplo: Precios de Refrigeradores. ¿Qué características de un refrigerador son importantes para determinar su precio (PRICE)? Aquí consideramos varias características de un refrigerador, incluyendo el tamaño del refrigerador en pies cúbicos (RSIZE), el tamaño del compartimento del congelador en pies cúbicos (FSIZE), la cantidad promedio de dinero gastado por año para operar el refrigerador (ECOST, por “costo de energía”), el número de estantes en las puertas del refrigerador y del congelador (SHELVES), y el número de características (FEATURES). La variable de características incluye estantes para latas, cajones transparentes, fabricantes de hielo, portahuevos, y así sucesivamente. Tanto los consumidores como los fabricantes están interesados en modelos de precios de refrigeradores. Otras cosas iguales, los consumidores generalmente prefieren refrigeradores más grandes con menores costos de energía y que tengan más características. Debido a las fuerzas de la oferta y la demanda, esperaríamos que los consumidores paguen más por estos refrigeradores. Un refrigerador más grande con menores costos de energía y que tiene más características a un precio similar se considera una ganga para el consumidor. ¿Cuánto estaría dispuesto a pagar el consumidor por este espacio adicional? Un modelo de precios para refrigeradores en el mercado proporciona algo de información sobre esta cuestión. Para este fin, analizamos datos de \\(n=37\\) refrigeradores. La Tabla 3.7 proporciona las estadísticas descriptivas básicas para la variable de respuesta PRICE y las cinco variables explicativas. De esta tabla, vemos que el precio promedio de los refrigeradores es \\(\\overline{y} = \\$626.40\\), con una desviación estándar de \\(s_{y} = \\$139.80\\). De manera similar, la cantidad promedio anual para operar un refrigerador, o ECOST promedio, es $70.51. knitr::kable(2, caption = &quot;Silly. Create a table just to update the counter...&quot;) Tabla 3.4: Silly. Create a table just to update the counter… x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Tabla 3.5: Silly. x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Tabla 3.6: Silly. x 2 Tabla 3.7: Estadísticas Descriptivas para cada variable para 37 Refrigeradores Media Mediana Desviación Estándar Mínimo Máximo ECOST 70.514 68.0 9.140 60.0 94.0 RSIZE 13.400 13.2 0.600 12.6 14.7 FSIZE 5.184 5.1 0.938 4.1 7.4 SHELVES 2.514 2.0 1.121 1.0 5.0 FEATURES 3.459 3.0 2.512 1.0 12.0 PRICE 626.351 590.0 139.790 460.0 1200.0 Para analizar las relaciones entre pares de variables, la Tabla 3.8 proporciona una matriz de coeficientes de correlación. En la tabla, vemos que hay relaciones lineales fuertes entre PRICE y tanto el espacio del congelador (FSIZE) como el número de FEATURES. Sorprendentemente, también hay una fuerte correlación positiva entre PRICE y ECOST. Recordemos que ECOST es el costo de energía; uno podría esperar que los refrigeradores de mayor precio deberían tener menores costos de energía. Tabla 3.8: Matriz de Coeficientes de Correlación ECOST RSIZE FSIZE SHELVES FEATURES RSIZE -0.033 FSIZE 0.855 -0.235 SHELVES 0.188 -0.363 0.251 FEATURES 0.334 -0.096 0.439 0.16 PRICE 0.522 -0.024 0.72 0.4 0.697 R Code to Produce Tables 3.7 and 3.8 Refrig &lt;- read.csv(&quot;CSVData/Refrigerator.csv&quot;, header=TRUE) # TABLE 3.7 SUMMARY STATISTICS varFrig &lt;- c(&quot;ECOST&quot;, &quot;RSIZE&quot;, &quot;FSIZE&quot;, &quot;SHELVES&quot;, &quot;FEATURES&quot;, &quot;PRICE&quot;) Refrig1 &lt;- Refrig[varFrig] tableMat &lt;- BookSummStats(Refrig1) colnames(tableMat) &lt;- c(&quot;Media&quot; , &quot;Mediana&quot; , &quot;Desviación Estándar&quot; , &quot;Mínimo&quot; , &quot;Máximo&quot;) rownames(tableMat) &lt;- varFrig # tableMat1[1:2,] &lt;- format(round(tableMat[1:2,], digits=0), big.mark = &#39;,&#39;) TableGen1(TableData=tableMat, TextTitle=&#39;Estadísticas Descriptivas para cada variable para 37 Refrigeradores&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:5, ColWidth = ColWidth5) tableCor &lt;- cor(Refrig1) tableCor &lt;- round(tableCor, digits = 3) tableCor[upper.tri(tableCor, diag = TRUE)] &lt;- &quot;&quot; tablePrint &lt;- tableCor[-1,] tablePrint &lt;- tablePrint[,-6] TableGen1(TableData=tablePrint, TextTitle=&#39;Matriz de Coeficientes de Correlación&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:5, ColWidth = ColWidth5) Se ajustó un modelo de regresión a los datos. La ecuación de regresión ajustada aparece en la Tabla 3.9, con \\(s=60.65\\) y \\(R^2=83.8\\%\\). Tabla 3.9: Modelo Ajustado de Precio de Refrigeradores Coeficiente Desviación Estándar \\(t\\)-Ratio Intercepto 798.00 271.400 -2.9 ECOST -6.96 2.275 -3.1 RSIZE 76.50 19.440 3.9 FSIZE 137.00 23.760 5.8 SHELVES 37.90 9.886 3.8 FEATURES 23.80 4.512 5.3 De la Tabla 3.9, las variables explicativas parecen ser buenos predictores de los precios de los refrigeradores. Juntas, estas variables explican el 83.8% de la variabilidad. Para entender los precios, el error típico ha disminuido de \\(s_{y} = \\$139.80\\) a \\(s = \\$60.65\\). Los \\(t\\)-ratios para cada una de las variables explicativas superan el valor absoluto de dos, lo que indica que cada variable es importante de manera individual. Lo que sorprende del ajuste de la regresión es el coeficiente negativo asociado con el costo de energía. Recordemos que podemos interpretar \\(b_{ECOST} = -6.96\\) como que, por cada aumento de un dólar en ECOST, esperamos que el PRICE disminuya en $6.96. Esta relación negativa está conforme con nuestra intuición económica. Sin embargo, es sorprendente que el mismo conjunto de datos nos haya mostrado que existe una relación positiva entre PRICE y ECOST. Esta aparente anomalía se debe a que la correlación solo mide las relaciones entre pares de variables, mientras que el ajuste de la regresión puede considerar varias variables simultáneamente. Para proporcionar más información sobre esta aparente anomalía, ahora introducimos el gráfico de variables añadidas. Producción de un Gráfico de Variables Añadidas El gráfico de variables añadidas proporciona vínculos adicionales entre la metodología de regresión y herramientas más fundamentales como los diagramas de dispersión y las correlaciones. Trabajamos en el contexto del Ejemplo del Precio de los Refrigeradores para demostrar la construcción de este gráfico. Procedimiento para producir un gráfico de variables añadidas. Realiza una regresión de PRICE sobre RSIZE, FSIZE, SHELVES y FEATURES, omitiendo ECOST. Calcula los residuos de esta regresión, los cuales etiquetamos como \\(e_1\\). Realiza una regresión de ECOST sobre RSIZE, FSIZE, SHELVES y FEATURES. Calcula los residuos de esta regresión, los cuales etiquetamos como \\(e_2\\). Grafica \\(e_1\\) versus \\(e_2\\). Este es el gráfico de variables añadidas de PRICE versus ECOST, controlando por los efectos de RSIZE, FSIZE, SHELVES y FEATURES. Este gráfico aparece en la Figura 3.4. Figura 3.4: Un gráfico de variables añadidas. Los residuos de la regresión de PRICE sobre las variables explicativas, omitiendo ECOST, están en el eje horizontal. En el eje vertical están los residuos de la regresión de ECOST sobre las demás variables explicativas. El coeficiente de correlación es -0.48. Código en R para producir la Figura 3.4 model.refrig1 &lt;- lm(PRICE ~ RSIZE + FSIZE + SHELVES + FEATURES, data = Refrig1) #summary(model.refrig1) model.refrig2 &lt;- lm(ECOST ~ RSIZE + FSIZE + SHELVES + FEATURES, data = Refrig1) #summary(model.refrig2) par(mar=c(4.1,4,.1,1), cex=1.1) plot(residuals(model.refrig2),residuals(model.refrig1), xlab=expression(e[1]),ylab=&quot;&quot;, las=1) mtext(expression(e[2]), side=2, at=0, line=3, las=1, cex=1.1) El error \\(\\varepsilon\\) puede interpretarse como la variación natural en una muestra. En muchas situaciones, esta variación natural es pequeña en comparación con los patrones evidentes en el componente de regresión no aleatorio. Por lo tanto, es útil pensar en el error, \\(\\varepsilon_i = y_i - \\left( \\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik} \\right)\\), como la respuesta después de controlar por los efectos de las variables explicativas. En la Sección 3.3, vimos que un error aleatorio puede aproximarse mediante un residuo, \\(e_i = y_i - \\left( b_0 + b_1 x_{i1} + \\cdots + b_k x_{ik} \\right)\\). De la misma manera, podemos pensar en un residuo como la respuesta después de “controlar” los efectos de las variables explicativas. Con esto en mente, podemos interpretar el eje vertical de la Figura 3.4 como el precio del refrigerador (PRICE) controlado por los efectos de RSIZE, FSIZE, SHELVES y FEATURES. De manera similar, podemos interpretar el eje horizontal como ECOST controlado por los efectos de RSIZE, FSIZE, SHELVES y FEATURES. El gráfico proporciona entonces una representación gráfica de la relación entre PRICE y ECOST, después de controlar por las demás variables explicativas. En comparación, un diagrama de dispersión de PRICE y ECOST (no mostrado aquí) no controla por las demás variables explicativas. Por lo tanto, es posible que la relación positiva entre PRICE y ECOST no se deba a una relación causal, sino más bien a una o más variables adicionales que hacen que ambas variables sean grandes. Por ejemplo, de la Tabla 3.8, vemos que el tamaño del congelador (FSIZE) está positivamente correlacionado tanto con ECOST como con PRICE. Ciertamente parece razonable que aumentar el tamaño de un congelador cause que tanto el costo de energía como el precio aumenten. Más bien, la correlación positiva puede deberse al hecho de que valores grandes de FSIZE significan valores grandes tanto de ECOST como de PRICE. Las variables omitidas en una regresión se llaman variables omitidas. Esta omisión podría causar un problema serio en el ajuste del modelo de regresión; los coeficientes de regresión podrían no solo ser significativamente fuertes cuando no deberían serlo, sino que también podrían tener el signo incorrecto. Seleccionar el conjunto adecuado de variables para incluir en el modelo de regresión es una tarea importante; es el tema de los Capítulos 5 y 6. 3.4.4 Coeficientes de Correlación Parcial Como vimos en el Capítulo 2, una estadística de correlación es una cantidad útil para resumir gráficos. La correlación en el gráfico de variables añadidas se llama coeficiente de correlación parcial. Se define como la correlación entre los residuos \\(e_1\\) y \\(e_2\\) y se denota por \\(r(y,x_j | x_1, \\ldots, x_{j-1}, x_{j+1}, \\ldots, x_k)\\). Debido a que resume un gráfico de variables añadidas, podemos interpretar \\(r(y,x_j | x_1, \\ldots, x_{j-1}, x_{j+1}, \\ldots, x_k)\\) como la correlación entre \\(y\\) y \\(x_j\\), en presencia de las otras variables explicativas. Por ejemplo, la correlación entre PRICE y ECOST en presencia de las otras variables explicativas es -0.48. El coeficiente de correlación parcial también se puede calcular utilizando \\[\\begin{equation} r(y,x_j | x_1, \\ldots, x_{j-1}, x_{j+1}, \\ldots, x_k) = \\frac{t(b_j)}{\\sqrt{t(b_j)^2 + n - (k + 1)}}. \\tag{3.11} \\end{equation}\\] Aquí, \\(t(b_j)\\) es el \\(t\\)-ratio para \\(b_j\\) de una regresión de \\(y\\) sobre \\(x_1, \\ldots, x_k\\) (incluyendo la variable \\(x_j\\)). Un aspecto importante de esta ecuación es que nos permite calcular coeficientes de correlación parcial ejecutando solo una regresión. Por ejemplo, en la Tabla 3.9, la correlación parcial entre PRICE y ECOST en presencia de las otras variables explicativas es \\(\\frac{-3.1}{\\sqrt{(-3.1)^2 + 37 - (5 + 1)}} \\approx -0.48\\). El cálculo de coeficientes de correlación parcial es más rápido cuando se usa la relación con el \\(t\\)-ratio, pero puede fallar en detectar relaciones no lineales. La información en la Tabla 3.9 nos permite calcular los cinco coeficientes de correlación parcial en el Ejemplo del Precio de los Refrigeradores después de ejecutar solo una regresión. El procedimiento de tres pasos para producir gráficos de variables añadidas requiere diez regresiones, dos para cada una de las cinco variables explicativas. Por supuesto, al producir gráficos de variables añadidas, podemos detectar relaciones no lineales que son omitidas por los coeficientes de correlación. Los coeficientes de correlación parcial proporcionan otra interpretación para los \\(t\\)-ratios. La ecuación muestra cómo calcular una estadística de correlación a partir de un \\(t\\)-ratio, proporcionando así otro vínculo entre la correlación y el análisis de regresión. Además, de la ecuación vemos que cuanto mayor es el \\(t\\)-ratio, mayor es el coeficiente de correlación parcial. Es decir, un \\(t\\)-ratio alto significa que existe una gran correlación entre la respuesta y la variable explicativa, controlando por las otras variables explicativas. Esto proporciona una respuesta parcial a la pregunta que suelen hacer los consumidores de análisis de regresión: “¿Cuál es la variable más importante?” 3.5 Algunas Variables Explicativas Especiales El modelo de regresión lineal es la base de una rica familia de modelos. Esta sección ofrece varios ejemplos para ilustrar la riqueza de esta familia. Estos ejemplos demuestran el uso de (i) variables binarias, (ii) transformación de variables explicativas y (iii) términos de interacción. Esta sección también sirve para subrayar el significado del adjetivo lineal en la frase “regresión lineal”; la función de regresión es lineal en los parámetros pero puede ser una función altamente no lineal de las variables explicativas. 3.5.1 Variables Binarias Las variables categóricas proporcionan una etiqueta numérica para mediciones de observaciones que caen en grupos distintos, o categorías. Debido a la agrupación, las variables categóricas son discretas y generalmente toman un número finito de valores. Comenzamos nuestra discusión con una variable categórica que puede tomar uno de solo dos valores, una variable binaria. Una discusión más detallada sobre las variables categóricas es el tema del Capítulo 4. Ejemplo: Seguro de Vida a Término - Continuación. Ahora consideramos el estado civil del encuestado. En la Encuesta de Finanzas del Consumidor, los encuestados pueden seleccionar entre varias opciones que describen su estado civil, incluyendo “casado”, “viviendo con una pareja”, “divorciado”, y así sucesivamente. El estado civil no se mide de manera continua, sino que toma valores que caen en grupos distintos. En este capítulo, agrupamos a los encuestados de acuerdo a si están solteros o no, definidos para incluir a aquellos que están separados, divorciados, viudos, nunca casados, y que no están casados ni viviendo con una pareja. El Capítulo 4 presentará un análisis más completo del estado civil incluyendo categorías adicionales. La variable binaria SINGLE se define como uno si el encuestado está soltero y 0 en caso contrario. La variable SINGLE también se conoce como una variable indicadora porque indica si el encuestado está soltero o no. Otro nombre para este tipo importante de variable es una variable dummy. Podríamos usar 0 y 100, o 20 y 36, o cualquier otro par de valores distintos. Sin embargo, 0 y 1 son convenientes para la interpretación de los valores de los parámetros, discutidos a continuación. Para simplificar la discusión, presentamos ahora un modelo utilizando solo LNINCOME y SINGLE como variables explicativas. Para nuestra muestra de \\(n = 275\\) hogares, 57 son solteros y los otros 218 no lo son. Para ver las relaciones entre LNFACE, LNINCOME y SINGLE, la Figura 3.5 introduce un gráfico de letras de LNFACE versus LNINCOME, con SINGLE como la variable de código. Podemos ver que la Figura 3.5 es un diagrama de dispersión de LNFACE versus LNINCOME, usando 50 hogares seleccionados aleatoriamente de nuestra muestra de 275 (para mayor claridad del gráfico). Sin embargo, en lugar de usar el mismo símbolo de gráfico para cada observación, hemos codificado los símbolos para que podamos entender fácilmente el comportamiento de una tercera variable, SINGLE. En otras aplicaciones, puede optar por usar otros símbolos de gráfico como \\(\\clubsuit\\), \\(\\heartsuit\\), \\(\\spadesuit\\), y así sucesivamente, o usar diferentes colores, para codificar información adicional. Para esta aplicación, se seleccionaron los códigos de letras “S” para solteros y “o” para otros porque recuerdan al lector la naturaleza del esquema de codificación. Independientemente del esquema de codificación, el punto importante es que un gráfico de letras es un dispositivo útil para representar gráficamente tres o más variables en dos dimensiones. La principal restricción es que la información adicional debe estar categorizada, como con las variables binarias, para que el esquema de codificación funcione. Figura 3.5: Gráfico de letras de LNFACE versus LNINCOME, con el código de letra ‘S’ para solteros y ‘o’ para otros. Las líneas de regresión ajustadas han sido superpuestas. La línea inferior es para solteros y la línea superior es para otros. La Figura 3.5 sugiere que LNFACE es más bajo para aquellos solteros que para otros para un nivel dado de ingreso. Por lo tanto, ahora consideramos un modelo de regresión, LNFACE = β_0 + β_1 LNINCOME + β_2 SINGLE + ϵ. La función de regresión se puede escribir como: \\[ \\text{E } y = \\begin{cases} \\beta_0 + \\beta_1 \\text{ LNINCOME} &amp; \\text{para otros encuestados} \\\\ \\beta_0 + \\beta_2 + \\beta_1 \\text{ LNINCOME} &amp; \\text{para encuestados solteros} \\end{cases} \\] La interpretación de los coeficientes del modelo difiere del caso de variables continuas. Para variables continuas como LNINCOME, interpretamos \\(\\beta_1\\) como el cambio esperado en \\(y\\) por unidad de cambio en el ingreso logarítmico, manteniendo fijas otras variables. Para variables binarias como SINGLE, interpretamos \\(\\beta_2\\) como el aumento esperado en \\(y\\) al pasar del nivel base de SINGLE (=0) al nivel alternativo. Así, aunque tenemos un modelo para ambos estados civiles, podemos interpretar el modelo usando dos ecuaciones de regresión, una para cada tipo de estado civil. Al escribir una ecuación separada para cada estado civil, hemos simplificado una complicada ecuación de regresión múltiple. A veces, es más fácil comunicar una serie de relaciones simples en comparación con una sola relación compleja. Aunque la interpretación para variables explicativas binarias difiere de la continua, el método de estimación de mínimos cuadrados ordinarios sigue siendo válido. Para ilustrar, la versión ajustada del modelo anterior es \\[ \\small{ \\begin{array}{ccccc} \\widehat{LNFACE} &amp; = &amp; 5.09 &amp; + 0.634 \\text{ LNINCOME} &amp; - 0.800 \\text{ SINGLE} .\\\\ \\text{error estándar} &amp; &amp; (0.89) &amp; ~~(0.078) &amp; ~(0.248) \\\\ \\end{array} } \\] Para interpretar \\(b_2 = -0.800\\), decimos que esperamos que el logaritmo de la cara sea menor en 0.80 para un encuestado que es soltero en comparación con la otra categoría. Esto asume que otras cosas, como el ingreso, permanecen constantes. Para una interpretación gráfica, las dos líneas de regresión ajustadas están superpuestas en la Figura 3.5. 3.5.2 Transformación de Variables Explicativas Los modelos de regresión tienen la capacidad de representar relaciones complejas y no lineales entre la respuesta esperada y las variables explicativas. Por ejemplo, los textos tempranos sobre regresión, como Plackett (1960, Capítulo 6), dedican un capítulo completo al modelo de regresión polinómica, \\[\\begin{equation} \\text{E } y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_p x^p. \\tag{3.12} \\end{equation}\\] Aquí, la idea es que un polinomio de orden \\(p\\) en \\(x\\) puede usarse para aproximar funciones generales y desconocidas no lineales de \\(x\\). El tratamiento moderno de la regresión polinómica no requiere un capítulo completo porque el modelo en la ecuación (3.12) puede expresarse como un caso especial del modelo de regresión lineal. Es decir, con la función de regresión en la ecuación (3.5), \\(\\text{E } y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k\\), podemos elegir \\(k = p\\) y \\(x_1 =x\\), \\(x_2 = x^2\\), \\(\\ldots\\), \\(x_p = x^p\\). Así, con estas elecciones de variables explicativas, podemos modelar una función altamente no lineal de \\(x\\). No estamos restringidos a potencias de \\(x\\) en nuestra elección de transformaciones. Por ejemplo, el modelo \\(\\text{E } y = \\beta_0 + \\beta_1 \\ln x\\), proporciona otra forma de representar una curva suavemente inclinada en \\(x\\). Este modelo puede escribirse como un caso especial del modelo de regresión lineal básico usando \\(x^{\\ast} = \\ln x\\) como la versión transformada de \\(x\\). Las transformaciones de las variables explicativas no tienen que ser funciones suaves. Para ilustrar, en algunas aplicaciones, es útil categorizar una variable explicativa continua. Por ejemplo, supongamos que \\(x\\) representa el número de años de educación, que varía de 0 a 17. Si nos basamos en información auto-reportada por nuestra muestra de personas mayores, puede haber una cantidad considerable de error en la medición de \\(x\\). Podríamos optar por usar una transformación menos informativa, pero más confiable, de \\(x\\) como \\(x^{\\ast}\\), una variable binaria para haber completado 13 años de escolaridad (terminar la secundaria). Formalmente, codificaríamos $x^{} = 1$ si \\(x \\geq 13\\) y \\(x^{\\ast} = 0\\) si \\(x &lt; 13\\). Así, hay varias formas en que las funciones no lineales de las variables explicativas pueden usarse en el modelo de regresión. Un ejemplo de un modelo de regresión no lineal es \\(y = \\beta_0 + \\exp (\\beta_1 x) + \\varepsilon.\\) Estos típicamente surgen en aplicaciones científicas de regresiones donde hay principios científicos fundamentales que guían el desarrollo del modelo complejo. 3.5.3 Términos de Interacción Hasta ahora hemos discutido cómo las variables explicativas, digamos \\(x_1\\) y \\(x_2\\), afectan la respuesta media de manera aditiva, es decir, \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). Aquí, esperamos que \\(y\\) aumente en \\(\\beta_1\\) por cada unidad que aumente \\(x_1\\), manteniendo \\(x_2\\) fijo. ¿Qué pasa si la tasa marginal de aumento de \\(\\mathrm{E}~y\\) difiere para valores altos de \\(x_2\\) en comparación con valores bajos de \\(x_2\\)? Una forma de representar esto es crear una variable de interacción \\(x_3 = x_1 \\times x_2\\) y considerar el modelo \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\). Con este modelo, el cambio en el esperado \\(y\\) por cada unidad de cambio en \\(x_1\\) ahora depende de \\(x_2\\). Formalmente, podemos evaluar pequeños cambios en la función de regresión como: \\[ \\frac{\\partial~ \\mathrm{E}~y}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} \\left(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\right) = \\beta_1 + \\beta_3 x_2 . \\] De esta manera, podemos permitir funciones más complicadas de \\(x_1\\) y \\(x_2\\). La Figura 3.6 ilustra esta estructura compleja. A partir de esta figura y los cálculos anteriores, vemos que los cambios parciales de \\(\\mathrm{E}~y\\) debido al movimiento de \\(x_1\\) dependen del valor de \\(x_2\\). De esta manera, decimos que los cambios parciales debidos a cada variable no son independientes, sino que “se mueven juntos.” Figura 3.6: Gráfico de \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\) versus \\(x_1\\) y \\(x_2\\). De manera más general, un término de interacción es una variable que se crea como una función no lineal de dos o más variables explicativas. Estos términos especiales, aunque nos permiten explorar una familia rica de funciones no lineales, pueden considerarse casos especiales del modelo de regresión lineal. Para hacer esto, simplemente creamos la variable de interés y tratamos este nuevo término como otra variable explicativa. Por supuesto, no todas las variables que creamos serán útiles. En algunos casos, la variable creada será tan similar a las variables ya presentes en nuestro modelo que no nos proporcionará nueva información. Afortunadamente, podemos usar pruebas \\(t\\) para verificar si la nueva variable es útil. Además, el Capítulo 4 presentará una prueba para decidir si un grupo de variables es útil. La función que usamos para crear una variable de interacción debe ser más que una combinación lineal de otras variables explicativas. Por ejemplo, si usamos \\(x_3 = x_1 + x_2\\), no podremos estimar todos los parámetros. El Capítulo 5 presentará algunas técnicas para ayudar a evitar situaciones en las que una variable es una combinación lineal de las demás. Para darles una idea de la amplia variedad de aplicaciones potenciales de variables explicativas especiales, ahora presentamos una serie de ejemplos breves. Ejemplo: Seguro de Vida a Término - Continuación. ¿Cómo interpretamos la interacción de una variable binaria con una variable continua? Para ilustrar, consideremos un modelo de regresión para Seguro de Vida a Término, \\[ \\mathrm{LNFACE} = \\beta_0 + \\beta_1 \\mathrm{LNINCOME} + \\beta_2 \\mathrm{SINGLE} + \\beta_3 \\mathrm{LNINCOME*SINGLE} + \\varepsilon . \\] En este modelo, hemos creado una tercera variable explicativa a través de la interacción de LNINCOME y SINGLE. La función de regresión se puede escribir como: \\[ \\mathrm{E}~y = \\begin{cases} \\beta_0 + \\beta_1 \\mathrm{LNINCOME}, &amp; \\text{para otros encuestados}, \\\\ \\beta_0 + \\beta_2 + (\\beta_1 + \\beta_3) \\mathrm{LNINCOME}, &amp; \\text{para encuestados solteros}. \\end{cases} \\] Así, a través de este único modelo con cuatro parámetros, podemos crear dos líneas de regresión separadas, una para los solteros y otra para los demás. La Figura 3.7 muestra las dos líneas de regresión ajustadas para nuestros datos. Figura 3.7: Gráfico de LNFACE versus LNINCOME, con el código de letra S para solteros y o para otros. Las líneas de regresión ajustadas han sido superpuestas. La línea inferior es para solteros y la línea superior es para otros. Ejemplo: Gastos de Compañías de Seguros de Vida. En una industria de seguros de vida bien desarrollada, minimizar los gastos es crucial para la posición competitiva de una compañía. Segal (2002) analizó datos contables anuales de más de 100 empresas para el período 1995-1998, inclusive, utilizando una base de datos de la Asociación Nacional de Comisionados de Seguros (NAIC) y otra información reportada. Segal modeló los gastos generales de la compañía como una función de la producción de la empresa y el precio de los insumos. La producción consiste en la producción de seguros, medida por \\(x_1\\) a \\(x_5\\), descritos en Tabla 3.10. Segal también consideró el cuadrado de cada salida, así como un término de interacción con una variable binaria \\(D\\) que indica si la empresa utiliza o no una sucursal para distribuir sus productos. (En una sucursal, los gerentes de campo son empleados de la empresa, no agentes independientes.) Tabla 3.10.Veintitrés Coeficientes de Regresión de un Modelo de Costos de Gastos \\[ \\small{ \\begin{array}{l|rrrr} \\hline &amp; \\text{Variable} &amp; &amp;\\text{Variable}&amp;\\text{ Cuadrada} \\\\ &amp; \\text{Base} &amp; \\text{Interacción} &amp; \\text{Base} &amp; \\text{Interacción} \\\\ &amp; &amp; \\text{con}~~ &amp; &amp; \\text{con}~~ \\\\ \\text{Variable} &amp; (D=0) &amp; (D=1) &amp; (D=0) &amp; (D=1)\\\\ \\hline \\text{Número de Pólizas de Vida Emitidas } (x_1) &amp; -0.454 &amp; 0.152 &amp; 0.032 &amp; -0.007 \\\\ \\text{Cantidad de Seguro de Vida a Término Vendido } (x_2) &amp; 0.112 &amp; -0.206 &amp; 0.002 &amp; 0.005 \\\\ \\text{Cantidad de Seguro de Vida Entera Vendido } (x_3) &amp; -0.184 &amp; 0.173 &amp; 0.008 &amp; -0.007 \\\\ \\text{Total de Consideraciones de Anualidades } (x_4) &amp; 0.098 &amp; -0.169 &amp; -0.003 &amp; 0.009 \\\\ \\text{Total de Primas de Accidentes y Salud } (x_5) &amp;-0.171 &amp; 0.014 &amp; 0.010 &amp; 0.002 \\\\ \\text{Intercepto } &amp; 7.726 &amp; &amp; &amp; \\\\ \\text{Precio del Trabajo (PL)} &amp; 0.553 &amp; &amp; &amp; \\\\ \\text{Precio del Capital (PC)} &amp; 0.102 &amp; &amp; &amp; \\\\ \\hline \\end{array} \\\\ \\begin{array}{l} \\textit{Nota: } x_1 \\text{ a } x_5 \\text{ están en unidades logarítmicas}.\\\\ \\textit{Fuente: Segal (2002)} \\end{array} } \\] Para los insumos de precios, el precio del trabajo (\\(PL\\)) se define como el costo total de empleados y agentes dividido por su número, en unidades logarítmicas. El precio del capital (\\(PC\\)) se aproxima por la relación del gasto en capital con el número de empleados y agentes, también en unidades logarítmicas. El precio de los materiales consiste en gastos distintos al trabajo y al capital divididos por el número de pólizas vendidas y terminadas durante el año. No aparece directamente como una variable explicativa. Más bien, Segal tomó la variable dependiente (\\(y\\)) como los gastos totales de la compañía divididos por el precio de los materiales, nuevamente en unidades logarítmicas. Con estas definiciones de variables, Segal estimó la siguiente función de regresión: \\[ \\mathrm{E~}y=\\beta_0 + \\sum_{j=1}^5 \\left( \\beta_j x_j + \\beta_{j+5} D x_j + \\beta_{j+10} x_j^2 + \\beta_{j+15}D x_j^2 \\right) + \\beta_{21} PL + \\beta_{22} PC. \\] Las estimaciones de los parámetros aparecen en Tabla 3.10. Por ejemplo, el cambio marginal en \\(\\mathrm{E}~y\\) por unidad de cambio en \\(x_1\\) es: \\[ \\frac{\\partial ~ \\mathrm{E}~y}{\\partial x_1}= \\beta_1 + \\beta_{6} D + 2 \\beta_{11} x_1 + 2 \\beta_{16}D x_1, \\] que se estima como \\(-0.454 + 0.152 D + (0.064 - 0.014 D) x_1\\). Para estos datos, el número mediano de pólizas emitidas fue \\(x_1=15,944\\). En este valor de \\(x_1\\), el cambio marginal estimado es \\(-0.454 + 0.152 D + (0.064 - 0.014 D) \\mathrm{ln}(15944) = 0.165 + 0.017 D,\\) o 0.165 para la base \\((D=0)\\) y 0.182 para compañías de sucursales \\((D=1)\\). Estas estimaciones son elasticidades, como se define en la Sección 3.2.2. Para interpretar estos coeficientes más a fondo, dejemos que \\(COST\\) represente los gastos generales totales de la compañía y \\(NUMPOL\\) represente el número de pólizas de vida emitidas. Entonces, para compañías de sucursales \\((D=1)\\), tenemos: \\[ 0.182 \\approx \\frac{\\partial y }{\\partial x_1 } = \\frac{\\partial ~ \\mathrm{ln}~COST}{\\partial ~ \\mathrm{ln}~NUMPOL}= \\frac{ \\frac{\\partial ~ COST}{\\partial ~NUMPOL}} {\\frac{COST}{NUMPOL}}, \\] o \\(\\frac{\\partial ~ COST}{\\partial ~NUMPOL} \\approx 0.182 \\frac{COST}{NUMPOL}\\). El costo mediano es $15,992,000, por lo que el costo marginal por póliza en estos valores medianos es \\(0.182 \\times (15992000/15944) =\\) $182.55. Caso Especial: Funciones de Respuesta Curvilíneas Podemos expandir las funciones polinómicas de una variable explicativa para incluir varias variables explicativas. Por ejemplo, la respuesta esperada, o función de respuesta, para un modelo de segundo orden con dos variables explicativas es: \\[ \\mathrm{E} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2. \\] La Figura 3.8 ilustra esta función de respuesta. De manera similar, la función de respuesta para un modelo de segundo orden con tres variables explicativas es: \\[ \\mathrm{E} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{33} x_3^2 + \\beta_{12} x_1 x_2 + \\beta_{13} x_1 x_3 + \\beta_{23} x_2 x_3. \\] Cuando hay más de una variable explicativa, los modelos de tercer orden y de órdenes superiores rara vez se utilizan en aplicaciones. Figura 3.8: Gráfico de \\(\\mathrm{E}~y = \\beta_0 + \\beta_1~x_1 + \\beta_2~x_2 + \\beta_{11}~x_1^2 + \\beta_{22}~x_2^2 + \\beta_{12}~x_1~x_2\\) versus \\(x_1\\) y \\(x_2\\). Código R para producir la Figura 3.8 # FIGURA 3.8 X1 &lt;- seq(3, 16, length=15) X2 &lt;- seq(5, 15, length=15) f &lt;- function(X1,X2) {y &lt;- 50 + 2*X1 + 3*X2 + 3*(X1-10)*(X2-10)- .4*(X1-10)^2+.4*(X2-10)^2} y &lt;- outer(X1, X2, f) par(mar=c(0,2.1,0,.1)) persp(X1, X2, y, theta = 30, phi = 30, expand = 0.5, ticktype=&quot;detailed&quot;) Caso Especial: Funciones No Lineales de una Variable Continua En algunas aplicaciones, esperamos que la respuesta tenga cambios abruptos en el comportamiento en ciertos valores de una variable explicativa, incluso si la variable es continua. Por ejemplo, supongamos que estamos tratando de modelar las contribuciones benéficas de un individuo (\\(y\\)) en función de sus ingresos (\\(x\\)). Para los datos de 2007, un modelo simple que podríamos considerar es el que se muestra en la Figura 3.9. Figura 3.9: El cambio marginal en \\(\\mathrm{E}~y\\) es menor por debajo de $97,500. El parámetro \\(\\beta_2\\) representa la diferencia en las pendientes. Código R para producir la Figura 3.9 # FIGURA 3.9 x &lt;- seq(90000,105000,20) Ey &lt;- 20 + 1*x + 2*(x-97500)*(x&gt;97500) par(mar=c(4.1,3.1,.1,1), cex=1.3) plot(x,Ey, type=&quot;l&quot;,yaxt=&quot;n&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;E y&quot;, side=2,las=1, line=1.5,cex=1.1) text(102000,102000,expression(beta[1]+beta[2]),cex=1.1) text(94000,92000,expression(beta[1]),cex=1.1) Una justificación para este modelo es que, en 2007, los individuos pagaban un 7.65% de sus ingresos en impuestos de Seguridad Social hasta 97,500. No se aplican impuestos de Seguridad Social a los ingresos que superan los 97,500. Así, una teoría es que, para los ingresos superiores a 97,500, los individuos tienen más ingresos disponibles por dólar y, por lo tanto, deberían estar más dispuestos a hacer contribuciones benéficas. Para modelar esta relación, definamos la variable binaria \\(z\\) que sea cero si \\(x &lt; 97,500\\) y uno si \\(x \\ge 97,500\\). Definamos la función de regresión como \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x + \\beta_2 z (x - 97,500)\\). Esto se puede escribir como: \\[ \\mathrm{E}~y = \\begin{cases} \\beta_0 + \\beta_1 x &amp; x &lt; 97,500 \\\\ \\beta_0 - \\beta_2(97,500) + (\\beta_1+\\beta_2) x &amp; x \\geq 97,500 \\end{cases} \\] Para estimar este modelo, realizaríamos una regresión de \\(y\\) sobre dos variables explicativas, \\(x_1 = x\\) y \\(x_2 = z \\times (x - 97,500)\\). Si \\(\\beta_2 &gt; 0\\), entonces la tasa marginal de contribuciones benéficas es mayor para ingresos que superan los $97,500. La Figura 3.9 ilustra esta relación, conocida como regresión lineal por tramos o a veces un modelo de “barra rota”. El cambio abrupto en la Figura 3.9 en \\(x = 97,500\\) se llama “cambio de pendiente”. Tenemos relaciones lineales por encima y por debajo del cambio de pendiente y hemos usado una variable binaria para unir las dos partes. No estamos restringidos a un solo cambio de pendiente. Por ejemplo, supongamos que deseamos hacer un estudio histórico de los ingresos gravables federales para los contribuyentes solteros de 1992. Entonces, había tres tramos impositivos: la tasa marginal por debajo de 21,450 era del 15%, por encima de 51,900 era del 31%, y en medio era del 28%. Para este ejemplo, usaríamos dos cambios de pendiente, en 21,450 y 51,900. Además, la regresión lineal por tramos no está restringida a funciones de respuesta continuas. Por ejemplo, supongamos que estamos estudiando las comisiones pagadas a los corredores de bolsa (\\(y\\)) en función del número de acciones compradas por un cliente (\\(x\\)). Podríamos esperar ver la relación ilustrada en la Figura 3.10. Aquí, la discontinuidad en \\(x = 100\\) refleja los gastos administrativos de comerciar en lotes irregulares, ya que se llaman transacciones de menos de 100 acciones. El menor costo marginal para transacciones superiores a 100 acciones simplemente refleja las economías de escala al hacer negocios en volúmenes mayores. Un modelo de regresión para esto es \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 z x\\) donde \\(z = 0\\) si \\(x &lt; 100\\) y \\(z = 1\\) si \\(x \\geq 100\\). La función de regresión representada en la Figura 3.10 es: \\[ \\mathrm{E}~y = \\begin{cases} \\beta_0 + \\beta_1 x_1 &amp; x &lt; 100 \\\\ \\beta_0 + \\beta_2 + (\\beta_1+\\beta_3) x_1 &amp; x \\geq 100 \\end{cases} \\] Figura 3.10: Gráfico de comisiones esperadas (\\(\\mathrm{E}~y\\)) versus número de acciones negociadas (\\(x\\)). El cambio en \\(x=100\\) refleja ahorros en gastos administrativos. La pendiente más baja para \\(x \\ge 100\\) refleja economías de escala en gastos. Código R para producir la Figura 3.10 # FIGURA 3.10 par(mar=c(4.1,4.5,.1,.1)) x &lt;- seq(50,150,.1) Ey &lt;- 20 + 2*x - 1*(x-100)*(x&gt;100) - 20*(x&gt;100) plot(x,Ey, type=&quot;p&quot;,ylab=&quot;&quot;, font.lab=1, cex.lab=1.1, cex=.25, las=1) mtext(&quot;E y&quot;, side=2,las=1, line=2.8,cex=1.1) 3.6 Lectura Adicional y Referencias Para las pruebas de los resultados del Capítulo 3, remitimos al lector a Goldberger (1991). Los modelos de regresión no lineales se discuten, por ejemplo, en Bates y Watts (1988). El Capítulo 3 ha introducido los fundamentos de la regresión lineal múltiple. El Capítulo 4 ampliará el alcance al introducir variables categóricas y métodos de inferencia estadística para manejar varios coeficientes simultáneamente. El Capítulo 5 presentará técnicas para ayudarte a seleccionar variables apropiadas en un modelo de regresión lineal múltiple. El Capítulo 6 es un capítulo de síntesis, que discute la interpretación del modelo, la selección de variables y la recolección de datos. Referencias del Capítulo Bates, Douglas M. y Watts, D. G. (1988). Nonlinear Regression Analysis and its Applications. John Wiley &amp; Sons, Nueva York. Lemaire, Jean (2002). Why do females live longer than males? North American Actuarial Journal, 6(4), 21-37. Goldberger, Arthur (1991). A Course in Econometrics. Harvard University Press, Cambridge. Plackett, R.L. (1960). Regression Analysis. Clarendon Press, Oxford, Inglaterra. Segal, Dan (2002). An economic analysis of life insurance company expenses. North American Actuarial Journal, 6(4), 81-94. 3.7 Ejercicios 3.1. Considera un conjunto de datos ficticio de \\(n = 100\\) observaciones con \\(s_y = 100\\). Realizamos una regresión con tres variables explicativas para obtener \\(s = 50\\). Calcula el coeficiente de determinación ajustado, \\(R_a^2\\). Completa la tabla de ANOVA. \\[ \\small{ \\begin{array}{|l|l|c|l|} \\hline \\text{Tabla de ANOVA} \\\\ \\hline \\text{Fuente} &amp; \\text{Suma de Cuadrados} &amp; \\text{df} &amp; \\text{Cuadrado Medio} \\\\ \\hline \\text{Regresión} &amp; &amp; &amp; \\\\ \\text{Error} &amp; &amp; &amp; \\\\ \\text{Total} &amp; &amp; &amp; \\\\ \\hline \\end{array} } \\] Calcula el coeficiente de determinación (no ajustado), \\(R^2\\). 3.2. Considera un conjunto de datos ficticio de \\(n = 100\\) observaciones con \\(s_y = 80\\). Realizamos una regresión con tres variables explicativas para obtener \\(s = 50\\). También obtenemos \\[ \\small{ \\left(\\mathbf{X^{\\prime} X} \\right)^{-1} = \\begin{pmatrix} 100 &amp; 20 &amp; 20 &amp; 20 \\\\ 20 &amp; 90 &amp; 30 &amp; 40 \\\\ 20 &amp; 30 &amp; 80 &amp; 50 \\\\ 20 &amp; 40 &amp; 50 &amp; 70 \\\\ \\end{pmatrix}. } \\] Determina el error estándar de \\(b_3\\), \\(se(b_3)\\). Determina la covarianza estimada entre \\(b_2\\) y \\(b_3\\). Determina la correlación estimada entre \\(b_2\\) y \\(b_3\\). Determina la varianza estimada de \\(4b_2 + 3b_3\\). 3.3. Considera el siguiente pequeño conjunto de datos ficticio. Ajustarás un modelo de regresión a \\(y\\) usando dos variables explicativas, \\(x_1\\) y \\(x_2\\). \\[ \\small{ \\begin{array}{c|cccc} \\hline i &amp; 1 &amp; 2 &amp; 3 &amp; 4 \\\\ \\hline x_{i,1} &amp; -1 &amp; 2 &amp; 4 &amp; 6 \\\\ x_{i,2} &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ y_i &amp; 0 &amp; 1 &amp; 5 &amp; 8 \\\\ \\hline \\end{array} } \\] A partir del modelo de regresión ajustado, tenemos \\(s = 1.373\\) y \\[ \\small{ \\mathbf{b} = \\begin{pmatrix} 0.1538 \\\\ 0.6923 \\\\ 2.8846 \\end{pmatrix} ~~~\\text{y}~~~ \\left(\\mathbf{X^{\\prime} X} \\right)^{-1} = \\begin{pmatrix} 0.53846 &amp; -0.07692 &amp; -0.15385 \\\\ -0.07692 &amp; 0.15385 &amp; -0.69231 \\\\ -0.15385 &amp; -0.69231 &amp; 4.11538 \\\\ \\end{pmatrix}. } \\] Escribe el vector de variables dependientes, \\(\\mathbf{y}\\), y la matriz de variables explicativas, \\(\\mathbf{X}\\). Determina el valor numérico para \\(\\widehat{y}_3\\), el valor ajustado para la tercera observación. Determina el valor numérico para \\(se(b_2)\\). Determina el valor numérico para \\(t(b_1)\\). 3.4. Lotería de Wisconsin. La Sección 2.1 describió una muestra de \\(n=50\\) áreas geográficas (códigos postales) que contenían datos de ventas de la lotería estatal de Wisconsin (\\(y = \\text{SALES}\\)). En esa sección, las ventas se analizaron usando un modelo de regresión lineal básico con \\(x = \\text{POP}\\), la población del área, como la variable explicativa. Este ejercicio amplía ese análisis al introducir variables explicativas adicionales dadas en la Tabla 3.11. \\[ \\text{Tabla 3.11: Características de lotería, económicas y demográficas } \\\\ \\text{ de cincuenta códigos postales de Wisconsin} \\] \\[ \\small{ \\begin{array}{ll} \\hline \\textbf{Características de la lotería} \\\\ \\hline \\text{SALES} &amp; \\text{Ventas de lotería en línea a consumidores individuales} \\\\ \\hline \\textbf{Características económicas y } \\\\ ~~~~~\\textbf{demográficas} \\\\\\hline \\text{PERPERHH} &amp; \\text{Personas por hogar} \\\\ \\text{MEDSCHYR} &amp; \\text{Años medianos de escolaridad} \\\\ \\text{MEDHVL} &amp; \\text{Valor medianos de viviendas en 1000s} \\\\ &amp; ~~~ \\text{ para viviendas ocupadas por sus propietarios} \\\\ \\text{PRCRENT} &amp; \\text{Porcentaje de viviendas ocupadas por arrendatarios} \\\\ \\text{PRC55P} &amp; \\text{Porcentaje de la población que tiene 55 años o más} \\\\ \\text{HHMEDAGE} &amp; \\text{Edad mediana del hogar} \\\\ \\text{MEDINC} &amp; \\text{Ingreso mediano estimado del hogar, en 1000s} \\\\ \\text{POP} &amp; \\text{Población} \\\\ \\hline \\end{array} } \\] Produce una tabla de estadísticas descriptivas para todas las variables. Un código postal (observación 11, código = 53211, Shorewood Wisconsin, un suburbio de Milwaukee) parece tener valores inusualmente grandes de MEDSCHYR y MEDHVL. Para esta observación, ¿cuántas desviaciones estándar está el valor de MEDSCHYR por encima de la media? Para esta observación, ¿cuántas desviaciones estándar está el valor de MEDHVL por encima de la media? Produce una tabla de correlaciones. ¿Cuáles son las tres variables más correlacionadas con SALES? Produce una matriz de gráficos de dispersión de todas las variables explicativas y SALES. En el gráfico de MEDSCHYR versus SALES, describe la posición de la observación 11. Ajusta un modelo lineal de SALES en las ocho variables explicativas. Resume el ajuste de este modelo citando la desviación estándar de los residuos, \\(s\\), el coeficiente de determinación, \\(R^2\\) y su versión ajustada, \\(R_a^2\\). Basado en el ajuste del modelo de la parte (d), ¿es MEDSCHYR una variable estadísticamente significativa? Para responder a esta pregunta, utiliza una prueba formal de hipótesis. Expón tus hipótesis nula y alternativa, criterio de decisión y regla de decisión. Ahora ajusta un modelo más parsimonioso, usando SALES como variable dependiente y MEDSCHYR, MEDHVL y POP como variables explicativas. Resume el ajuste de este modelo citando la desviación estándar de los residuos, \\(s\\), el coeficiente de determinación, \\(R^2\\) y su versión ajustada, \\(R_a^2\\). ¿Cómo se comparan estos valores con el ajuste del modelo en la parte (d)? Observa que el signo del coeficiente de regresión asociado con MEDSCHYR es negativo. Para ayudar a interpretar este coeficiente, calcula el coeficiente de correlación parcial correspondiente. ¿Cuál es la interpretación de este coeficiente? Para obtener más información sobre la relación entre MEDSCHYR y SALES, produce un gráfico de variable añadida controlando los efectos de MEDHVL y POP. Verifica que la correlación asociada con este gráfico concuerde con tu respuesta en la parte (g). Vuelve a ejecutar la regresión en la parte (f), después de eliminar la observación 11. Cita las estadísticas descriptivas básicas de esta regresión. Para este ajuste del modelo, ¿es MEDSCHYR una variable estadísticamente significativa? Para responder a esta pregunta, utiliza una prueba formal de hipótesis. Expón tus hipótesis nula y alternativa, criterio de decisión y regla de decisión. Vuelve a ejecutar la regresión en la parte (f), después de eliminar la observación 9. Cita las estadísticas descriptivas básicas de esta regresión. 3.5. Gastos de Compañías de Seguros. Este ejercicio considera los datos de compañías de seguros del NAIC y descritos en el Ejercicio 1.6. La Tabla 3.10 describe varias variables que pueden ser usadas para explicar los gastos. Al igual que en el estudio de Segal (2002) sobre los aseguradores de vida, los “outputs” de la empresa consisten en primas emitidas (para propiedad y accidentes, estas se subdividen en líneas personales y comerciales) así como pérdidas (subdivididas en líneas de corto y largo plazo). ASSETS y CASH son medidas comúnmente usadas del tamaño de una empresa. GROUP, STOCK y MUTUAL describen la estructura organizacional. Los “inputs” de la empresa se recopilaron del Bureau of Labor Statistics (BLS, del programa Occupational Employee Statistics). STAFFWAGE se calcula como el salario promedio en el estado donde está ubicada la compañía de seguros. AGENTWAGE se calcula como el salario promedio ponderado de la industria de corretaje, ponderado por el porcentaje de prima bruta escrita en cada estado. Tabla 3.10: Variables de Gastos de Aseguradoras Variable Descripción EXPENSES Gastos totales incurridos, en millones de dólares LOSSLONG Pérdidas incurridas para líneas de largo plazo, en millones de dólares LOSSSHORT Pérdidas incurridas para líneas de corto plazo, en millones de dólares GPWPERSONAL Prima bruta emitida para líneas personales, en millones de dólares GPWCOMM Prima bruta emitida para líneas comerciales, en millones de dólares ASSETS Activos netos admitidos, en millones de dólares CASH Efectivo y activos invertidos, en millones de dólares GROUP Indica si la empresa está afiliada STOCK Indica si la empresa es una compañía de acciones MUTUAL Indica si la empresa es una compañía mutua STAFFWAGE Salario promedio anual del personal administrativo de la aseguradora, en miles de dólares AGENTWAGE Salario promedio anual del agente de seguros, en miles de dólares Una inspección preliminar de los datos mostró que muchas empresas no informaron pérdidas de seguros en 2005. Para este ejercicio, consideramos las 384 compañías con algunas pérdidas en el archivo NAICExpense.csv. Produce estadísticas descriptivas de la variable de respuesta y las variables explicativas (no binarias). Nota el patrón de sesgo para cada variable. Observa que muchas variables tienen valores negativos. Transforma cada variable no binaria mediante la transformación logarítmica modificada, \\(\\ln(1+x)\\). Produce estadísticas descriptivas de estas variables explicativas no binarias modificadas. Denota LNEXPENSES (\\(= \\ln(1+\\text{EXPENSES})\\)) como la variable de gastos modificada. Para el análisis posterior, usa solo las variables modificadas descritas en la parte (b). Produce una tabla de correlaciones para las variables no binarias. ¿Cuáles son las tres variables más correlacionadas con LNEXPENSES? Proporciona un diagrama de caja de LNEXPENSES por nivel de GROUP. ¿Qué nivel de grupo tiene mayores gastos? Ajusta un modelo lineal de LNEXPENSES en las once variables explicativas. Resume el ajuste de este modelo citando la desviación estándar de los residuos, \\(s\\), el coeficiente de determinación, \\(R^2\\), y su versión ajustada, \\(R^2_a\\). Ajusta un modelo lineal de LNEXPENSES en un modelo reducido usando ocho variables explicativas, eliminando LNCASH, STOCK y MUTUAL. Para las variables explicativas, incluye ASSETS, GROUP, ambas versiones de pérdidas y primas brutas, así como las dos variables del BLS. f(i). Resume el ajuste de este modelo citando \\(s\\), \\(R^2\\), y \\(R^2_a\\). f(ii). Interpreta el coeficiente asociado con las primas brutas comerciales en la escala logarítmica. f(iii). Supón que GPWCOMM aumenta en $1, ¿cuánto esperamos que aumenten los EXPENSES? Usa tu respuesta en la parte f(ii) y los valores medianos de GPWCOMM y EXPENSES para esta pregunta. Eleva al cuadrado cada una de las dos variables de pérdidas y las dos de primas brutas. Ajusta un modelo lineal de LNEXPENSES en un modelo reducido usando doce variables explicativas, las ocho variables de la parte (f) y los cuatro términos cuadrados adicionales que acabas de crear. g(i). Resume el ajuste de este modelo citando \\(s\\), \\(R^2\\), y \\(R^2_a\\). g(ii). ¿Los términos cuadráticos parecen ser variables explicativas útiles? Ahora omite las dos variables del BLS, por lo que estás ajustando un modelo de LNEXPENSES en ASSETS, GROUP, ambas versiones de pérdidas y primas brutas, así como términos cuadráticos. Resume el ajuste de este modelo citando \\(s\\), \\(R^2\\), y \\(R^2_a\\). Comenta sobre el número de observaciones usadas para ajustar este modelo comparado con la parte (f). Términos de Interacción Elimina los términos cuadráticos de la parte (g) y agrega términos de interacción con la variable ficticia GROUP. Así, ahora hay once variables: ASSETS, GROUP, ambas versiones de pérdidas y primas brutas, así como interacciones de GROUP con ASSETS y ambas versiones de pérdidas y primas brutas. i(i). Resume el ajuste de este modelo citando \\(s\\), \\(R^2\\), y \\(R^2_a\\). i(ii). Supón que GPWCOMM aumenta en $1, ¿cuánto esperamos que aumenten los EXPENSES para las compañías GROUP=0? Usa los valores medianos de GPWCOMM y EXPENSES de las compañías GROUP=0 para esta pregunta. i(iii). Supón que GPWCOMM aumenta en $1, ¿cuánto esperamos que aumenten los EXPENSES para las compañías GROUP=1? Usa los valores medianos de GPWCOMM y EXPENSES de las compañías GROUP=1 para esta pregunta. 3.6 Expectativas de Vida Nacionales. Continuamos el análisis iniciado en los Ejercicios 1 y 2. Ahora ajusta un modelo de regresión en LIFEEXP usando tres variables explicativas: FERTILITY, PUBLICEDUCATION y lnHEALTH (la transformación logarítmica natural de PRIVATEHEALTH). Interpreta el coeficiente de regresión asociado con PUBLICEDUCATION. Interpreta el coeficiente de regresión asociado con los gastos en salud sin usar la escala logarítmica para los gastos. Basado en el ajuste del modelo, ¿es PUBLICEDUCATION una variable estadísticamente significativa? Para responder a esta pregunta, usa una prueba formal de hipótesis. Expón tus hipótesis nula y alternativa, criterio de decisión, y regla de decisión. El signo negativo del coeficiente de PUBLICEDUCATION es sorprendente, dado que el signo de la correlación entre PUBLICEDUCATION y LIFEEXP es positivo y la intuición sugiere una relación positiva. Para verificar este resultado, aparece un gráfico de variable añadida en la Figura 3.11. d(i). Para un gráfico de variable añadida, describe su propósito y un método para producirlo. d(ii). Calcula la correlación correspondiente al gráfico de variable añadida que aparece en la Figura 3.11. Figura 3.11: Gráfico de variable añadida de PUBLICEDUCATION versus LIFEEXP, controlando por FERTILITY y lnHEALTH Código R para producir la Figura 3.11 # FIGURA 3.11 LifeExp &lt;- read.csv(&quot;CSVData/UNLifeExpectancy.csv&quot;, header=TRUE) # ELIMINAR LIFEEXPs MISSING LifeExp3 &lt;- subset(LifeExp, !is.na(LIFEEXP) ) varLife &lt;- c(&quot;FERTILITY&quot;,&quot;PUBLICEDUCATION&quot;, &quot;REGION&quot;,&quot;COUNTRY&quot;,&quot;LIFEEXP&quot;, &quot;HEALTHEXPEND&quot;) LifeExp4 &lt;- LifeExp3[varLife] LifeExp4$lnHEALTH &lt;- log(LifeExp4$HEALTHEXPEND) LifeExp4.good &lt;- na.omit(LifeExp4) # GRÁFICO DE VARIABLE AÑADIDA model4a &lt;- lm(LIFEEXP ~ FERTILITY+lnHEALTH, data=LifeExp4.good) model4b &lt;- lm(PUBLICEDUCATION ~ FERTILITY+lnHEALTH, data=LifeExp4.good) plot(residuals(model4b),residuals(model4a), xlab=&quot;residuos(PUBLICEDUCATION)&quot;,ylab=&quot;residuos(LIFEEXP)&quot;) "],["C4MLRANOVA.html", "Capítulo 4 Regresión Lineal Múltiple - II 4.1 El Papel de las Variables Binarias 4.2 Inferencia Estadística para Varios Coeficientes 4.3 Modelo ANOVA de Un Factor 4.4 Combinando Variables Explicativas Categóricas y Continuas 4.5 Lecturas Adicionales y Referencias 4.6 Ejercicios 4.7 Suplemento Técnico - Expresiones Matriciales", " Capítulo 4 Regresión Lineal Múltiple - II Vista previa del capítulo. Este capítulo amplía la discusión sobre la regresión lineal múltiple al introducir la inferencia estadística para manejar varios coeficientes simultáneamente. Para motivar esta extensión, este capítulo considera los coeficientes asociados con variables categóricas. Estas variables nos permiten agrupar observaciones en distintas categorías. Este capítulo muestra cómo incorporar variables categóricas en funciones de regresión utilizando variables binarias, ampliando así considerablemente el alcance de las posibles aplicaciones del análisis de regresión. La inferencia estadística para varios coeficientes permite a los analistas tomar decisiones sobre variables categóricas, así como otras aplicaciones importantes. Las variables explicativas categóricas también proporcionan la base para un modelo de ANOVA, un tipo especial de modelo de regresión que permite un análisis e interpretación más sencillos. 4.1 El Papel de las Variables Binarias Las variables categóricas proporcionan etiquetas para observaciones, para denotar la pertenencia a grupos o categorías distintas. Una variable binaria es un caso especial de una variable categórica. Para ilustrar, una variable binaria puede indicarnos si alguien tiene o no seguro de salud. Una variable categórica podría indicarnos si alguien tiene: seguro privado grupal (ofrecido por empleadores y asociaciones), seguro privado individual (a través de compañías de seguros), seguro público (como Medicare o Medicaid), sin seguro de salud. Para las variables categóricas, puede o no existir un orden de los grupos. En el caso del seguro de salud, es difícil ordenar estas cuatro categorías y decir cuál es “mayor”: seguro privado grupal, seguro privado individual, seguro público o sin seguro de salud. En contraste, para la educación, podríamos agrupar a las personas en “baja,” “intermedia,” y “alta” según sus años de educación. En este caso, hay un orden entre los grupos basado en el nivel de logro educativo. Como veremos, este orden puede o no proporcionar información sobre la variable dependiente. Factor es otro término utilizado para una variable explicativa categórica no ordenada. Para las variables categóricas ordenadas, los analistas suelen asignar una puntuación numérica a cada resultado y tratar la variable como si fuera continua. Por ejemplo, si tuviéramos tres niveles de educación, podríamos emplear rangos y usar: \\[ \\small{ \\text{EDUCATION} = \\begin{cases} 1 &amp; \\text{para educación baja} \\\\ 2 &amp; \\text{para educación intermedia} \\\\ 3 &amp; \\text{para educación alta.} \\end{cases} } \\] Una alternativa sería utilizar una puntuación numérica que se aproxime a un valor subyacente de la categoría. Por ejemplo, podríamos usar: \\[ \\small{ \\text{EDUCATION} = \\begin{cases} 6 &amp; \\text{para educación baja} \\\\ 10 &amp; \\text{para educación intermedia} \\\\ 14 &amp; \\text{para educación alta.} \\end{cases} } \\] Esto da el número aproximado de años de escolaridad que completaron las personas en cada categoría. La asignación de puntuaciones numéricas y el tratamiento de la variable como continua tiene implicaciones importantes para la interpretación en la modelización de regresión. Recordemos que el coeficiente de regresión es el cambio marginal en la respuesta esperada; en este caso, el \\(\\beta\\) para EDUCATION evalúa el incremento en \\(\\mathrm{E }~y\\) por unidad de cambio en EDUCATION. Si registramos EDUCATION como un rango en un modelo de regresión, entonces el \\(\\beta\\) para EDUCATION corresponde al incremento en \\(\\mathrm{E }~y\\) al pasar de EDUCATION=1 a EDUCATION=2 (de baja a intermedia); este incremento es el mismo que al pasar de EDUCATION=2 a EDUCATION=3 (de intermedia a alta). ¿Queremos modelar este incremento como igual? Esta es una suposición que el analista hace con esta codificación de EDUCATION; puede ser o no válida, pero ciertamente necesita ser reconocida. Debido a esta interpretación de los coeficientes, los analistas rara vez usan rangos u otras puntuaciones numéricas para resumir variables categóricas no ordenadas. La forma más directa de manejar factores en la regresión es mediante el uso de variables binarias. Una variable categórica con \\(c\\) niveles puede representarse utilizando \\(c\\) variables binarias, una para cada categoría. Por ejemplo, supongamos que no estábamos seguros de la dirección del efecto de la educación y decidimos tratarla como un factor. Entonces, podríamos codificar \\(c=3\\) variables binarias: (1) una variable para indicar educación baja, (2) una para indicar educación intermedia, y (3) una para indicar educación alta. Estas variables binarias son a menudo conocidas como variables ficticias. En el análisis de regresión con un término de intercepción, utilizamos solo \\(c-1\\) de estas variables binarias; la variable restante entra implícitamente a través del término de intercepción. Al identificar una variable como un factor, la mayoría de los paquetes de software estadístico crearán automáticamente variables binarias para usted. A través del uso de variables binarias, no utilizamos el orden de las categorías dentro de un factor. Debido a que no se hace ninguna suposición sobre el orden de las categorías, para el ajuste del modelo no importa qué variable se omita con respecto al ajuste del modelo. Sin embargo, sí importa para la interpretación de los coeficientes de regresión. Consideremos el siguiente ejemplo. Ejemplo: Seguro de Vida Temporal - Continuado. Ahora volvemos al estado civil de los encuestados de la Encuesta de Finanzas del Consumidor (SCF). Recordemos que el estado civil no se mide de manera continua, sino que toma valores que caen en grupos distintos que tratamos como no ordenados. En el Capítulo 3, agrupamos a los encuestados según si eran o no “solteros”, donde ser soltero incluye nunca haberse casado, estar separado, divorciado, viudo, y no casado pero viviendo con una pareja. Ahora complementamos esto al considerar la variable categórica, MARSTAT, que representa el estado civil del encuestado. Esto puede ser: 1, para casado 2, para viviendo con una pareja 0, para otro (SCF desglosa aún más esta categoría en separado, divorciado, viudo, nunca casado, inaplicable, personas de 17 años o menos, y sin más personas). Como antes, la variable dependiente es \\(y =\\) LNFACE, la cantidad que la compañía pagará en caso de fallecimiento del asegurado nombrado (en dólares logarítmicos). La Tabla 4.1 resume la variable dependiente según el nivel de la variable categórica. Esta tabla muestra que el estado civil “casado” es el más prevalente en la muestra y que los casados eligen tener la mayor cobertura de seguro de vida. La Figura 4.1 da una visión más completa de la distribución de LNFACE para cada uno de los tres tipos de estado civil. La tabla y la figura también sugieren que aquellos que viven juntos tienen menos cobertura de seguro de vida que las otras dos categorías. Tabla 4.1: Estadísticas Resumidas de Logaritmo del Monto de Cobertura por Estado Civil MARSTAT Número Media Desviación Estándar Otro 0 57 10.958 1.566 Casado 1 208 12.329 1.822 Viviendo juntos 2 10 10.825 2.001 Total 275 11.990 1.871 Figura 4.1: Diagramas de Caja del Logaritmo del Monto de Cobertura, por Nivel de Estado Civil Código R para Producir la Tabla 4.1 y la Figura 4.1 tableout &lt;- data.frame( MARSTAT &lt;- c(0,1,2,&quot;&quot;), Número &lt;- c(57, 208, 10, 275), Media &lt;- c(10.958, 12.329, 10.825, 11.990), Desviación_Estándar &lt;- c(1.566, 1.822, 2.001, 1.871) ) colnames(tableout) &lt;- c(&quot;MARSTAT&quot;, &quot;Número&quot;, &quot;Media&quot;, &quot;Desviación Estándar&quot;) rownames(tableout) &lt;- c(&quot;Otro&quot;, &quot;Casado&quot;, &quot;Viviendo juntos&quot;, &quot;Total&quot;) TableGen1(TableData=tableout , TextTitle=&#39;Estadísticas Resumidas de Logaritmo del Monto de Cobertura por Estado Civil&#39;, Align=&#39;crrr&#39;, Digits=3, ColumnSpec=1:3, ColWidth = ColWidth4) %&gt;% kableExtra::column_spec(1, width = &quot;4cm&quot;) library(HH) Term &lt;- read.csv(&quot;CSVData/TermLife.csv&quot;, header=TRUE) Term2 &lt;- subset(Term, FACE &gt; 0) LNFACE &lt;- log(Term2$FACE) LNINCOME &lt;- log(Term2$INCOME) MAR0 &lt;- 1*(Term2$MARSTAT == 0) # FIGURA 4.1 par(mar=c(4.1,4,1,1), cex=1.1) boxplot(LNFACE ~ MARSTAT, data=Term2, ylab=&quot;&quot;, xlab=&quot;Estado Civil&quot;) mtext(&quot;LNFACE&quot;, side=2, at=17.2, las=1, cex=1.1, adj=.4) ¿Son las variables continuas y categóricas determinantes importantes de la respuesta? Para responder a esto, se realizó una regresión usando LNFACE como respuesta y cinco variables explicativas: tres continuas y dos binarias (para el estado civil). Recordemos que nuestras tres variables explicativas continuas son: LNINCOME (ingreso anual logarítmico), el número de años de EDUCATION del encuestado, y el número de miembros del hogar, NUMHH. Para las variables binarias, primero definimos MAR0 como la variable binaria que toma el valor de uno si MARSTAT=0 y cero en caso contrario. De manera similar, definimos MAR1 y MAR2 como variables binarias que indican si MARSTAT=1 y MARSTAT=2, respectivamente. Existe una dependencia lineal perfecta entre estas tres variables binarias, ya que MAR0 + MAR1 + MAR2 = 1 para cualquier encuestado. Por lo tanto, solo necesitamos dos de las tres. Sin embargo, no hay una dependencia perfecta entre cualquiera de dos de las tres. Resulta que cor(MAR0, MAR1) = -0.90, cor(MAR0, MAR2) = -0.10, y cor(MAR1, MAR2) = -0.34. Se realizó un modelo de regresión utilizando LNINCOME, EDUCATION, NUMHH, MAR0, y MAR2 como variables explicativas. La ecuación de regresión ajustada resulta ser: \\[ \\small{ \\widehat{y} = 3.395 + 0.452 \\text{ LNINCOME} + 0.205 \\text{ EDUCATION} + 0.248 \\text{ NUMHH} - 0.557 \\text{ MAR0} - 0.789 \\text{ MAR2}. } \\] Para interpretar los coeficientes de regresión asociados con el estado civil, consideremos un encuestado que está casado. En este caso, MAR0=0, MAR1=1, y MAR2=0, de manera que: \\[ \\small{ \\widehat{y}_m = 3.395 + 0.452 \\text{ LNINCOME} + 0.205 \\text{ EDUCATION} + 0.248 \\text{ NUMHH}. } \\] De manera similar, si el encuestado es codificado como viviendo juntos, entonces MAR0=0, MAR1=0, y MAR2=1, y: \\[ \\small{ \\widehat{y}_{lt} = 3.395 + 0.452 \\text{ LNINCOME} + 0.205 \\text{ EDUCATION} + 0.248 \\text{ NUMHH} - 0.789. } \\] La diferencia entre \\(\\widehat{y}_m\\) y \\(\\widehat{y}_{lt}\\) es \\(0.789.\\) Así, podemos interpretar el coeficiente de regresión asociado con MAR2, \\(-0.789\\), como la diferencia en los valores ajustados para alguien que vive juntos en comparación con una persona similar que está casada (la categoría omitida). De manera similar, podemos interpretar \\(-0.557\\) como la diferencia entre la categoría “otro” y la categoría de casados, manteniendo fijas las otras variables explicativas. Para la diferencia en los valores ajustados entre las categorías “otro” y “viviendo juntos”, podemos usar \\(-0.557 - (-0.789) = 0.232.\\) Aunque la regresión se realizó utilizando MAR0 y MAR2, cualquier dos de las tres producirían la misma Tabla ANOVA (Tabla 4.2). Sin embargo, la elección de las variables binarias sí afecta los coeficientes de regresión. La Tabla 4.3 muestra tres modelos, omitiendo MAR1, MAR2, y MAR0, respectivamente. Para cada ajuste, los coeficientes asociados con las variables continuas permanecen iguales. Como hemos visto, las interpretaciones de las variables binarias son con respecto a la categoría omitida, conocida como el nivel de referencia. Aunque cambian de un modelo a otro, su interpretación general sigue siendo la misma. Es decir, si queremos estimar la diferencia en la cobertura entre la categoría “otro” y la categoría “viviendo juntos”, la estimación sería \\(0.232\\), sin importar el modelo. Tabla 4.2: Seguro de Vida Temporal con Estado Civil - Tabla ANOVA Fuente Suma de Cuadrados \\(df\\) Cuadrado Medio Regresión 343.28 5 68.66 Error 615.62 269 2.29 Total 948.90 274 Aunque los tres modelos en la Tabla 4.3 son iguales excepto por diferentes elecciones de parámetros, parecen diferentes. En particular, los \\(t\\)-ratios difieren y muestran diferentes apariencias de significancia estadística. Por ejemplo, ambos \\(t\\)-ratios asociados con el estado civil en el Modelo 2 son menores que 2 en valor absoluto, lo que sugiere que el estado civil es poco importante. En contraste, tanto el Modelo 1 como el Modelo 3 tienen al menos una variable binaria de estado civil que supera 2 en valor absoluto, lo que sugiere significancia estadística. Por lo tanto, se puede influir en la apariencia de significancia estadística al alterar la elección del nivel de referencia. Para evaluar la importancia general del estado civil (no solo cada variable binaria), la Sección 4.2 introducirá pruebas de conjuntos de coeficientes de regresión. Tabla 4.3: Coeficientes de Regresión del Seguro de Vida Temporal con Estado Civil Coeficiente Modelo 1 Modelo 1 \\(t\\)-Ratio Coeficiente Modelo 2 Modelo 2 \\(t\\)-Ratio Coeficiente Modelo 3 Modelo 3 \\(t\\)-Ratio LNINCOME 0.452 5.74 0.452 5.74 0.452 5.74 EDUCATION 0.205 5.3 0.205 5.3 0.205 5.3 NUMHH 0.248 3.57 0.248 3.57 0.248 3.57 Intercepto 3.395 3.77 3.395 2.74 2.838 3.34 MAR0 -0.557 -2.15 0.232 0.44 MAR1 0.789 1.59 0.557 2.15 MAR2 -0.789 -1.59 -0.232 -0.44 Ejemplo: ¿Cómo afecta el Compartir Costos en Planes de Seguro Médico a los Gastos en Salud? En uno de los muchos estudios que resultaron del Experimento de Seguro de Salud de Rand (HIE) introducido en la Sección 1.5, Keeler y Rolph (1988) investigaron los efectos del compartir costos en los planes de seguro médico. Para este estudio, 14 planes de seguro médico fueron agrupados por la tasa de coaseguro (el porcentaje pagado como gastos de bolsillo que variaba en 0, 25, 50 y 95%). Uno de los planes con 95% limitaba los gastos anuales de bolsillo en atención ambulatoria a 150 por persona (450 por familia), proporcionando en efecto un deducible ambulatorio individual. Este plan se analizó como un grupo separado, de manera que había \\(c=5\\) categorías de planes de seguro. En la mayoría de los estudios de seguros, los individuos eligen planes de seguro, lo que hace difícil evaluar los efectos del compartir costos debido a la selección adversa. La selección adversa puede surgir porque los individuos con mala salud crónica son más propensos a elegir planes con menos compartir costos, lo que da la apariencia de que menos cobertura conduce a mayores gastos. En el HIE de Rand, los individuos fueron asignados aleatoriamente a los planes, eliminando así esta fuente potencial de sesgo. Keeler y Rolph (1988) organizaron los gastos de un individuo en episodios de tratamiento; cada episodio contiene gastos asociados con un determinado ataque de enfermedad, condición crónica o procedimiento. Los episodios se clasificaron como hospitalarios, dentales o ambulatorios; esta clasificación se basó principalmente en diagnósticos, no en la ubicación de los servicios. Así, por ejemplo, los servicios ambulatorios que preceden o siguen a una hospitalización, así como los medicamentos y pruebas relacionados, se incluyeron como parte de un episodio hospitalario. Para simplificar, aquí solo informamos resultados para episodios hospitalarios. Aunque las familias fueron asignadas aleatoriamente a los planes, Keeler y Rolph (1988) utilizaron métodos de regresión para controlar los atributos de los participantes y aislar los efectos del compartir costos en los planes. La Tabla 4.4 resume los coeficientes de regresión, basados en una muestra de \\(n=1,967\\) gastos por episodio. En esta regresión, el gasto logarítmico fue la variable dependiente. La variable categórica de compartir costos se descompuso en cinco variables binarias para que no se impusiera ninguna forma funcional en la respuesta al seguro. Estas variables son “Co-ins25,” “Co-ins50,” y “Co-ins95,” para tasas de coaseguro del 25, 50 y 95%, respectivamente, y “Deducible Indiv” para el plan con deducibles individuales. La variable omitida es el plan de seguro gratuito con 0% de coaseguro. El HIE se llevó a cabo en seis ciudades; una variable categórica para controlar la ubicación se representó con cinco variables binarias, Dayton, Fitchburg, Franklin, Charleston y Georgetown, siendo Seattle la variable omitida. Se utilizó un factor categórico con \\(c=6\\) niveles para la edad y el sexo; las variables binarias en el modelo consistieron en “Edad 0-2,” “Edad 3-5,” “Edad 6-17,” “Mujer edad 18-65,” y “Hombre edad 46-65,” siendo la categoría omitida “Hombre edad 18-45.” Otras variables de control incluyeron una escala de estado de salud, el estado socioeconómico, el número de visitas médicas en el año anterior al experimento en una escala logarítmica y la raza. La Tabla 4.4 resume los efectos de las variables. Como señalaron Keeler y Rolph, hubo grandes diferencias según la ubicación y la edad, aunque la regresión solo explicó \\(R^2=11\\%\\) de la variabilidad. Para las variables de compartir costos, solo “Co-ins95” fue estadísticamente significativa, y esto solo al nivel del 5%, no al nivel del 1%. El estudio de Keeler y Rolph (1988) examina otros tipos de gastos por episodio, así como la frecuencia de los gastos. Concluyeron que el compartir costos en los planes de seguro médico tiene poco efecto sobre la cantidad de gastos por episodio, aunque hay diferencias importantes en la frecuencia de episodios. Esto se debe a que un episodio de tratamiento está compuesto por dos decisiones. La cantidad de tratamiento es decidida conjuntamente entre el paciente y el médico y, en gran medida, no se ve afectada por el tipo de plan de seguro médico. La decisión de buscar tratamiento médico la toma el paciente; este proceso de toma de decisiones es más susceptible a los incentivos económicos en los aspectos de compartir costos de los planes de seguro médico. Tabla 4.4: Coeficientes de Gastos por Episodio del Rand HIE Variable Coeficiente de Regresión Variable Coeficiente de Regresión Intercepto 7.95 Dayton 0.13* Co-ins25 0.07 Fitchburg 0.12 Co-ins50 0.02 Franklin -0.01 Co-ins95 -0.13* Charleston 0.20* Deducible Indiv -0.03 Georgetown -0.18* Escala de Salud -0.02* Edad 0-2 -0.63** Estado Socioeconómico 0.03 Edad 3-5 -0.64** Visitas Médicas -0.03 Edad 6-17 -0.30** Examen -0.10* Mujer edad 18-65 0.11 Negro 0.14* Hombre edad 46-65 0.26 Nota: * significativo al 5%, ** significativo al 1% Fuente: Keeler y Rolph (1988) 4.2 Inferencia Estadística para Varios Coeficientes Puede ser útil examinar varios coeficientes de regresión al mismo tiempo. Por ejemplo, cuando se evalúa el efecto de una variable categórica con \\(c\\) niveles, necesitamos decir algo de manera conjunta sobre las \\(c-1\\) variables binarias que ingresan a la ecuación de regresión. Para hacer esto, la Sección 4.2.1 introduce un método para manejar combinaciones lineales de coeficientes de regresión. La Sección 4.2.2 muestra cómo probar varias combinaciones lineales, y la Sección 4.2.3 presenta otras aplicaciones de inferencia. 4.2.1 Conjuntos de Coeficientes de Regresión Recordemos que nuestros coeficientes de regresión se especifican como \\(\\boldsymbol{\\beta} = \\left( \\beta_0, \\beta_1, \\ldots, \\beta_k \\right)^{\\prime},\\) un vector de tamaño \\((k+1) \\times 1\\). Será conveniente expresar combinaciones lineales de los coeficientes de regresión utilizando la notación \\(\\mathbf{C} \\boldsymbol{\\beta},\\) donde \\(\\mathbf{C}\\) es una matriz de tamaño \\(p \\times (k+1)\\) que es especificada por el usuario y depende de la aplicación. Algunas aplicaciones involucran la estimación de \\(\\mathbf{C} \\boldsymbol{\\beta}\\). Otras involucran probar si \\(\\mathbf{C} \\boldsymbol{\\beta}\\) es igual a un valor específico conocido (denotado como \\(\\mathbf{d}\\)). Llamamos a \\(H_0:\\mathbf{C \\boldsymbol{\\beta} = d}\\) la hipótesis lineal general. Para demostrar la amplia variedad de aplicaciones en las que se pueden usar conjuntos de coeficientes de regresión, ahora presentamos una serie de casos especiales. Caso Especial 1: Un Coeficiente de Regresión. En la Sección 3.4, investigamos la importancia de un solo coeficiente, digamos \\(\\beta_j\\). Podemos expresar este coeficiente como \\(\\mathbf{C} \\boldsymbol{\\beta}\\) eligiendo \\(p=1\\) y \\(\\mathbf{C}\\) como un vector de \\(1 \\times (k+1)\\) con un uno en la columna \\((j+1)\\) y ceros en las demás posiciones. Estas elecciones resultan en \\[ \\mathbf{C \\boldsymbol{\\beta} =} \\left( 0~\\ldots~0~1~0~\\ldots~0\\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) = \\beta_j. \\] Caso Especial 2: Función de Regresión. Aquí, elegimos \\(p=1\\) y \\(\\mathbf{C}\\) como un vector de \\(1 \\times (k+1)\\) que representa la transpuesta de un conjunto de variables explicativas. Estas elecciones resultan en \\[ \\mathbf{C \\boldsymbol{\\beta} =} \\left( x_0, x_1, \\ldots, x_k \\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) = \\beta_0 x_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k = \\mathrm{E}~y, \\] que es la función de regresión. Caso Especial 3: Combinación Lineal de Coeficientes de Regresión. Cuando \\(p=1\\), usamos la convención de que las letras en minúscula en negrita son vectores y tomamos \\(\\mathbf{C = c^{\\prime}} = \\left( c_0, \\ldots, c_k \\right)^{\\prime}\\). En este caso, \\(\\mathbf{C} \\boldsymbol{\\beta}\\) es una combinación lineal genérica de los coeficientes de regresión \\[ \\mathbf{C} \\boldsymbol{\\beta} = \\mathbf{c}^{\\prime} {\\boldsymbol \\beta} = c_0 \\beta_0 + \\ldots + c_k \\beta_k . \\] Caso Especial 4: Prueba de Igualdad de Coeficientes de Regresión. Supongamos que el interés radica en probar \\(H_0: \\beta_1 = \\beta_2\\). Para este propósito, tomamos \\(p=1\\), \\(\\mathbf{c}^{\\prime} = \\left( 0, 1, -1, 0, \\ldots, 0\\right)\\), y \\(\\mathbf{d} = 0\\). Con estas elecciones, tenemos \\[ \\mathbf{C \\boldsymbol{\\beta} = c^{\\prime} \\boldsymbol{\\beta} =} \\left( 0, 1, -1, 0, \\ldots, 0\\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) = \\beta_1 - \\beta_2 = 0, \\] de modo que la hipótesis lineal general se reduce a \\(H_0: \\beta_1 = \\beta_2\\). Caso Especial 5: Adecuación del Modelo. Es costumbre en el análisis de regresión presentar una prueba de si alguna de las variables explicativas es útil para explicar la respuesta. Formalmente, esto es una prueba de la hipótesis nula \\(H_0:\\beta_1=\\beta_2=\\ldots=\\beta_k=0\\). Es importante notar que, como convención, no se prueba si el intercepto es cero. Para probar esto usando la hipótesis lineal general, elegimos \\(p=k\\), \\(\\mathbf{d}=\\left( 0~\\ldots~0\\right)^{\\prime}\\) como un vector de tamaño \\(k \\times 1\\) lleno de ceros y \\(\\mathbf{C}\\) como una matriz de tamaño \\(k \\times (k+1)\\) tal que \\[ \\small{ \\mathbf{C \\boldsymbol{\\beta} =}\\left( \\begin{array}{ccccc} 0 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{array} \\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) =\\left( \\begin{array}{c} \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) =\\left( \\begin{array}{c} 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right) =\\mathbf{d}. } \\] Caso Especial 6: Prueba de Partes del Modelo. Supongamos que estamos interesados en comparar una función de regresión completa \\[ \\mathrm{E~}y = \\beta_0 + \\beta_1 x_1 +\\ldots + \\beta_k x_k + \\beta_{k+1} x_{k+1} + \\ldots + \\beta_{k+p} x_{k+p} \\] con una función de regresión reducida, \\[ \\mathrm{E~}y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k. \\] Comenzando con la regresión completa, vemos que si se cumple la hipótesis nula \\(H_0:\\beta_{k+1} = \\ldots = \\beta_{k+p} = 0\\), entonces llegamos a la regresión reducida. Para ilustrar, las variables \\(x_{k+1}, \\ldots, x_{k+p}\\) pueden referirse a varias variables binarias que representan una variable categórica y nuestro interés radica en si la variable categórica es importante. Para probar la importancia de la variable categórica, queremos ver si las variables binarias \\(x_{k+1}, \\ldots, x_{k+p}\\) afectan conjuntamente a las variables dependientes. Para probar esto utilizando la hipótesis lineal general, elegimos \\(\\mathbf{d}\\) y \\(\\mathbf{C}\\) tal que \\[ \\small{ \\mathbf{C \\boldsymbol{\\beta} =}\\left( \\begin{array}{ccccccc} 0 &amp; \\cdots &amp; 0 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{array} \\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\\\ \\beta_{k+1} \\\\ \\vdots \\\\ \\beta_{k+p} \\end{array} \\right) =\\left( \\begin{array}{c} \\beta_{k+1} \\\\ \\vdots \\\\ \\beta_{k+p} \\end{array} \\right) =\\left( \\begin{array}{c} 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right) =\\mathbf{d}. } \\] De una lista de \\(k+p\\) variables \\(x_1, \\ldots, x_{k+p}\\), puedes eliminar cualquier \\(p\\) que consideres apropiado. Las variables adicionales no necesitan ser las últimas \\(p\\) en la especificación de la regresión. Eliminar \\(x_{k+1}, \\ldots, x_{k+p}\\) es solo por conveniencia notacional. 4.2.2 La Hipótesis Lineal General Para resumir, la hipótesis lineal general se puede expresar como \\(H_0:\\mathbf{C \\boldsymbol{\\beta} = d}\\). Aquí, \\(\\mathbf{C}\\) es una matriz de \\(p \\times (k+1)\\), \\(\\mathbf{d}\\) es un vector de \\(p \\times 1\\), y tanto \\(\\mathbf{C}\\) como \\(\\mathbf{d}\\) son especificados por el usuario y dependen de la aplicación en cuestión. Aunque \\(k+1\\) es el número de coeficientes de regresión, \\(p\\) es el número de restricciones bajo \\(H_0\\) sobre estos coeficientes. (Para aquellos lectores con conocimiento de álgebra matricial avanzada, \\(p\\) es el rango de \\(\\mathbf{C}\\)). Esta hipótesis nula se prueba contra la alternativa \\(H_a:\\mathbf{C \\boldsymbol{\\beta} \\neq d}\\). Esto puede ser obvio, pero requerimos que \\(p \\leq k+1\\) porque no podemos probar más restricciones que los parámetros libres. Para entender la base del procedimiento de prueba, primero recordemos algunas de las propiedades básicas de los estimadores de los coeficientes de regresión descritos en la Sección 3.3. Sin embargo, nuestro objetivo ahora es entender las propiedades de las combinaciones lineales de los coeficientes de regresión especificadas por \\(\\mathbf{C \\boldsymbol{\\beta}}\\). Un estimador natural de esta cantidad es \\(\\mathbf{Cb}\\). Es fácil ver que \\(\\mathbf{Cb}\\) es un estimador insesgado de \\(\\mathbf{C \\boldsymbol{\\beta}}\\), porque \\(\\mathrm{E~}\\mathbf{Cb = C}\\mathrm{E~}\\mathbf{b = C \\boldsymbol{\\beta}}\\). Además, la varianza es \\(\\mathrm{Var}\\left( \\mathbf{Cb}\\right) \\mathbf{= C}\\mathrm{Var}\\left( \\mathbf{b}\\right) \\mathbf{C}^{\\prime}\\) \\(=\\sigma^2 \\mathbf{C}\\left( \\mathbf{X^{\\prime}X}\\right)^{-1} \\mathbf{C}^{\\prime}\\). Para evaluar la diferencia entre \\(\\mathbf{d}\\), el valor hipotetizado de \\(\\mathbf{C \\boldsymbol{\\beta}}\\), y su valor estimado, \\(\\mathbf{Cb}\\), utilizamos la siguiente estadística: \\[ F-\\text{ratio}=\\frac{(\\mathbf{Cb-d)}^{\\prime}\\left( \\mathbf{C}\\left( \\mathbf{X^{\\prime}X} \\right)^{-1} \\mathbf{C}^{\\prime}\\right)^{-1}(\\mathbf{Cb-d)}}{ps_{full}^2}. \\tag{4.1} \\] Aquí, \\(s_{full}^2\\) es el error cuadrático medio del modelo de regresión completo. Usando la teoría de modelos lineales, se puede comprobar que la estadística \\(F\\)-ratio sigue una distribución \\(F\\) con grados de libertad del numerador \\(df_1=p\\) y grados de libertad del denominador \\(df_2=n-(k+1)\\). Tanto la estadística como la distribución teórica llevan el nombre de R. A. Fisher, un científico y estadístico renombrado que hizo mucho para avanzar la estadística como ciencia en la primera mitad del siglo XX. Al igual que la distribución normal y la distribución \\(t\\), la distribución \\(F\\) es una distribución continua. La distribución \\(F\\) es la distribución muestral para el \\(F\\)-ratio y es proporcional a la razón de dos sumas de cuadrados, cada una de las cuales es positiva o cero. Así, a diferencia de la distribución normal y la distribución \\(t\\), la distribución \\(F\\) solo toma valores no negativos. Recordemos que la distribución \\(t\\) está indexada por un único parámetro de grados de libertad. La distribución \\(F\\) está indexada por dos parámetros de grados de libertad: uno para el numerador, \\(df_1\\), y uno para el denominador, \\(df_2\\). El Apéndice A3.4 proporciona detalles adicionales. La estadística de prueba en la ecuación (4.1) es compleja en su forma. Afortunadamente, existe una alternativa que es más sencilla de implementar e interpretar; esta alternativa se basa en el principio de la suma de cuadrados adicional. Procedimiento para Probar la Hipótesis Lineal General Ejecuta la regresión completa y obtén la suma de cuadrados del error y el error cuadrático medio, los cuales etiquetamos como \\((Error~SS)_{full}\\) y \\(s_{full}^2\\), respectivamente. Considera el modelo asumiendo que la hipótesis nula es verdadera. Ejecuta una regresión con este modelo y obtén la suma de cuadrados del error, la cual etiquetamos como \\((Error~SS)_{reduced}\\). Calcula \\[ F-\\text{ratio}=\\frac{(Error~SS)_{reduced}-(Error~SS)_{full}}{ps_{full}^2}. \\tag{4.2} \\] Rechaza la hipótesis nula en favor de la alternativa si el \\(F\\)-ratio excede un valor \\(F\\). El valor \\(F\\) es un percentil de la distribución \\(F\\) con \\(df_1=p\\) y \\(df_2=n-(k+1)\\) grados de libertad. El percentil es uno menos el nivel de significancia de la prueba. Siguiendo nuestra notación con la distribución \\(t\\), denotamos este percentil como \\(F_{p,n-(k+1),1-\\alpha}\\), donde \\(\\alpha\\) es el nivel de significancia. Este procedimiento es comúnmente conocido como una prueba \\(F\\). La Sección 4.7.2 proporciona las bases matemáticas. Para entender el principio de la suma de cuadrados adicional, recordemos que la suma de cuadrados del error para el modelo completo se determina como el valor mínimo de \\[ SS(b_0^{\\ast}, \\ldots, b_k^{\\ast}) = \\sum_{i=1}^{n} \\left( y_i - \\left( b_0^{\\ast} + \\ldots + b_k^{\\ast} x_{i,k} \\right) \\right)^2. \\] Aquí, \\(SS(b_0^{\\ast}, \\ldots, b_k^{\\ast})\\) es una función de \\(b_0^{\\ast}, \\ldots, b_k^{\\ast}\\) y \\((Error~SS)_{full}\\) es el mínimo sobre todos los valores posibles de \\(b_0^{\\ast}, \\ldots, b_k^{\\ast}\\). De manera similar, \\((Error~SS)_{reduced}\\) es la mínima suma de cuadrados del error bajo las restricciones en la hipótesis nula. Debido a que hay menos posibilidades bajo la hipótesis nula, tenemos que \\[ (Error~SS)_{full} \\leq (Error~SS)_{reduced}. \\tag{4.3} \\] Para ilustrar, consideremos nuestro primer caso especial donde \\(H_0 : \\beta_j = 0\\). En este caso, la diferencia entre los modelos completo y reducido equivale a eliminar una variable. Una consecuencia de la ecuación (4.3) es que, al agregar variables a un modelo de regresión, la suma de cuadrados del error nunca aumenta (y, de hecho, generalmente disminuye). Por lo tanto, agregar variables a un modelo de regresión aumenta \\(R^2\\), el coeficiente de determinación. ¿Cuán grande debe ser una disminución en la suma de cuadrados del error para que sea estadísticamente significativa? Intuitivamente, se puede ver el cociente \\(F\\) como la diferencia en la suma de cuadrados del error dividida por el número de restricciones, \\(\\frac{(Error~SS)_{reduced}-(Error~SS)_{full}}{p}\\), y luego reescalada por la mejor estimación del término de varianza, el \\(s^2\\), del modelo completo. Bajo la hipótesis nula, esta estadística sigue una distribución \\(F\\) y podemos comparar la estadística de prueba con esta distribución para ver si es inusualmente grande. Usando la relación \\(Regression~SS = Total~SS - Error~SS\\), podemos re-expresar la diferencia en la suma de cuadrados del error como \\[ (Error~SS)_{reduced} - (Error~SS)_{full} = (Regression~SS)_{full} - (Regression~SS)_{reduced}. \\] Esta diferencia se conoce como una Suma de Cuadrados Tipo III. Al probar la importancia de un conjunto de variables explicativas, \\(x_{k+1}, \\ldots, x_{k+p}\\), en presencia de \\(x_1, \\ldots, x_k\\), encontrarás que muchos paquetes estadísticos calculan esta cantidad directamente en una sola ejecución de regresión. La ventaja de esto es que permite al analista realizar una prueba \\(F\\) usando una sola ejecución de regresión, en lugar de dos ejecuciones de regresión como en nuestro procedimiento de cuatro pasos descrito anteriormente. Ejemplo: Seguro de Vida Temporal - Continuación. Antes de discutir la lógica y las implicaciones de la prueba \\(F\\), vamos a ilustrar su uso. En el ejemplo del Seguro de Vida Temporal, supongamos que deseamos entender el impacto del estado civil. La Tabla 4.3 presentó un mensaje mixto en términos de cocientes \\(t\\); a veces eran estadísticamente significativos y otras veces no. Sería útil tener una prueba formal para dar una respuesta definitiva, al menos en términos de significancia estadística. Específicamente, consideramos un modelo de regresión utilizando LNINCOME, EDUCATION, NUMHH, MAR0, y MAR2 como variables explicativas. La ecuación del modelo es \\[ \\small{ \\begin{array}{ll} y &amp;= \\beta_0 + \\beta_1 \\text{LNINCOME} + \\beta_2 \\text{EDUCATION} + \\beta_3 \\text{NUMHH} \\\\ &amp; \\ \\ \\ \\ + \\beta_4 \\text{MAR0} + \\beta_5 \\text{MAR2}. \\end{array} } \\] Nuestro objetivo es probar \\(H_0: \\beta_4 = \\beta_5 = 0\\). Comenzamos ejecutando un modelo de regresión con todas las \\(k+p=5\\) variables. Los resultados se informaron en la Tabla 4.2, donde vimos que \\((Error~SS)_{full} = 615.62\\) y \\(s_{full}^2 = (1.513)^2 = 2.289\\). El siguiente paso es ejecutar el modelo reducido sin MAR0 y MAR2. Esto se hizo en la Tabla 3.3 del Capítulo 3, donde vimos que \\((Error~SS)_{reduced} = 630.43\\). Luego calculamos la estadística de prueba \\[ \\small{ F-\\text{ratio} = \\frac{(Error~SS)_{reduced} - (Error~SS)_{full}}{ps_{full}^2} = \\frac{630.43 - 615.62}{2 \\times 2.289} = 3.235. } \\] El cuarto paso compara la estadística de prueba con una distribución \\(F\\) con \\(df_1=p=2\\) y \\(df_2 = n-(k+p+1) = 269\\) grados de libertad. Usando un nivel de significancia del 5%, resulta que el percentil 95 es \\(F-\\text{ratio} \\approx 3.029\\). El valor \\(p\\) correspondiente es \\(\\Pr(F &gt; 3.235) = 0.0409\\). Al nivel de significancia del 5%, rechazamos la hipótesis nula \\(H_0: \\beta_4 = \\beta_5 = 0\\). Esto sugiere que es importante utilizar el estado civil para entender la cobertura de seguro de vida temporal, incluso en presencia de ingresos, educación y número de miembros del hogar. Algunos Casos Especiales La prueba de hipótesis lineales general está disponible cuando puedes expresar un modelo como un subconjunto de otro. Por esta razón, es útil pensar en ella como una herramienta para comparar modelos “más pequeños” con modelos “más grandes”. Sin embargo, el modelo más pequeño debe ser un subconjunto del modelo más grande. Por ejemplo, la prueba de hipótesis lineales general no se puede usar para comparar las funciones de regresión \\(\\mathrm{E~}y = \\beta_0 + \\beta_7 x_7\\) frente a \\(\\mathrm{E~}y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4\\). Esto se debe a que la primera, función más pequeña, no es un subconjunto de la segunda, función más grande. La prueba de hipótesis lineales general se puede usar en muchas ocasiones, aunque no siempre es necesaria. Por ejemplo, supongamos que deseamos probar \\(H_0:\\beta_k=0\\). Ya hemos visto que esta hipótesis nula se puede examinar usando la prueba del cociente \\(t\\). En este caso especial, resulta que \\((t-\\textrm{ratio})^2=F-\\textrm{ratio}\\). Así, estas pruebas son equivalentes para probar \\(H_0:\\beta_k=0\\) frente a \\(H_a:\\beta_k \\neq 0\\). La prueba \\(F\\) tiene la ventaja de que funciona para más de un predictor, mientras que la prueba \\(t\\) tiene la ventaja de que se pueden considerar alternativas unilaterales. Por lo tanto, ambas pruebas son consideradas útiles. Dividiendo el numerador y el denominador de la ecuación (4.2) por \\(Total~SS\\), la estadística de prueba también se puede escribir como: \\[ F-\\textrm{ratio}=\\frac{\\left( R_{full}^2-R_{reduced}^2\\right) /p}{\\left( 1-R_{full}^2\\right) / (n-(k+1))}. \\tag{4.4} \\] La interpretación de esta expresión es que el cociente \\(F\\) mide la disminución en el coeficiente de determinación, \\(R^2\\). La expresión en la ecuación (4.4) es particularmente útil para probar la adecuación del modelo, nuestro Caso Especial 5. En este caso, \\(p=k\\), y la suma de cuadrados de regresión bajo el modelo reducido es cero. Así, tenemos \\[ \\small{ F-\\textrm{ratio}=\\frac{\\left( (Regression~SS)_{full}\\right) /k}{s_{full}^2} =\\frac{(Regression~MS)_{full}}{(Error~SS)_{full}}. } \\] Esta estadística de prueba es una característica regular de la tabla ANOVA para muchos paquetes estadísticos. Por ejemplo, en nuestro ejemplo de Seguro de Vida Temporal, probar la adecuación del modelo significa evaluar \\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = 0\\). En la Tabla 4.2, el cociente \\(F\\) es 68.66 / 2.29 = 29.98. Con \\(df_1=5\\) y \\(df_2 = 269\\), el valor \\(F\\) es aproximadamente 2.248 y el valor \\(p\\) correspondiente es \\(\\Pr(F &gt; 29.98) \\approx 0\\). Esto nos lleva a rechazar firmemente la idea de que las variables explicativas no son útiles para entender la cobertura del seguro de vida temporal, reafirmando lo que aprendimos en el análisis gráfico y de correlación. Cualquier otro resultado sería sorprendente. Para otra expresión, dividiendo por \\(Total~SS\\), podemos escribir \\[ F-\\textrm{ratio}=\\frac{R^2}{1-R^2}\\frac{n-(k+1)}{k}. \\] Dado que tanto el cociente \\(F\\) como \\(R^2\\) son medidas del ajuste del modelo, parece intuitivamente plausible que estén relacionados de alguna manera. Una consecuencia de esta relación es que, a medida que \\(R^2\\) aumenta, también lo hace el cociente \\(F\\) y viceversa. El cociente \\(F\\) se usa porque su distribución muestral es conocida bajo una hipótesis nula, por lo que podemos hacer afirmaciones sobre significancia estadística. La medida \\(R^2\\) se usa debido a las interpretaciones fáciles asociadas con ella. 4.2.3 Estimando y Prediciendo Varios Coeficientes Estimación de Combinaciones Lineales de Coeficientes de Regresión En algunas aplicaciones, el principal interés es estimar una combinación lineal de los coeficientes de regresión. Para ilustrar, recordemos que en la Sección 3.5 desarrollamos una función de regresión para las contribuciones caritativas de un individuo (\\(y\\)) en términos de sus salarios (\\(x\\)). En esta función, hubo un cambio abrupto en la función en \\(x=97,500\\). Para modelarlo, definimos la variable binaria \\(z\\) para que sea cero si \\(x&lt;97,500\\) y uno si \\(x \\geq 97,500\\), y la función de regresión \\(\\mathrm{E~}y = \\beta_0 + \\beta_1 x + \\beta_2 z(x - 97,500)\\). Así, el cambio marginal esperado en las contribuciones por cambio en el salario para salarios superiores a \\(97,500\\) es \\(\\frac{\\partial \\left( \\mathrm{E~}y\\right)}{\\partial x} = \\beta_1 + \\beta_2\\). Para estimar \\(\\beta_1 + \\beta_2\\), un estimador razonable es \\(b_1 + b_2\\), el cual está disponible en el software estándar de regresión. Además, también nos gustaría calcular errores estándar para \\(b_1 + b_2\\) que se pueden utilizar, por ejemplo, para determinar un intervalo de confianza para \\(\\beta_1 + \\beta_2\\). Sin embargo, \\(b_1\\) y \\(b_2\\) suelen estar correlacionados, por lo que el cálculo del error estándar de \\(b_1 + b_2\\) requiere la estimación de la covarianza entre \\(b_1\\) y \\(b_2\\). La estimación de \\(\\beta_1 + \\beta_2\\) es un ejemplo de nuestro Caso Especial 3, que considera combinaciones lineales de coeficientes de regresión de la forma \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta = c_0 \\beta_0 + c_1 \\beta_1 + \\ldots + c_k \\beta_k\\). Para nuestro ejemplo de contribuciones caritativas, elegiríamos \\(c_1 = c_2 = 1\\) y los demás \\(c\\)’s igual a cero. Para estimar \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta\\), reemplazamos el vector de parámetros por el vector de estimadores y usamos \\(\\mathbf{c}^{\\prime} \\mathbf{b}\\). Para evaluar la fiabilidad de este estimador, como en la Sección 4.2.2, tenemos que \\(\\mathrm{Var}\\left( \\mathbf{c}^{\\prime} \\mathbf{b}\\right) = \\sigma^2 \\mathbf{c}^{\\prime}(\\mathbf{X^{\\prime} X})^{-1} \\mathbf{c}\\). Así, podemos definir la desviación estándar estimada, o error estándar, de \\(\\mathbf{c}^{\\prime} \\mathbf{b}\\) como \\[ se\\left( \\mathbf{c}^{\\prime} \\mathbf{b} \\right) = s \\sqrt{\\mathbf{c}^{\\prime} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{c}}. \\] Con esta cantidad, un intervalo de confianza del \\(100(1 - \\alpha) \\%\\) para \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta\\) es \\[ \\mathbf{c}^{\\prime} \\mathbf{b} \\pm t_{n - (k + 1), 1 - \\alpha / 2} ~ se(\\mathbf{c}^{\\prime} \\mathbf{b}). \\tag{4.5} \\] El intervalo de confianza en la ecuación (4.5) es válido bajo las Suposiciones F1-F5. Si elegimos que \\(\\mathbf{c}\\) tenga un “1” en la \\((j + 1)^{\\text{st}}\\) fila y ceros en los demás, entonces \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta = \\beta_j\\), \\(\\mathbf{c}^{\\prime} \\mathbf{b} = b_j\\), y \\[ se(b_j) = s \\sqrt{(j + 1)^{\\text{st}}~ \\textit{elemento diagonal de } (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1}}. \\] Por lo tanto, la ecuación (4.5) proporciona una base teórica para los intervalos de confianza de los coeficientes de regresión individuales introducidos en la ecuación (3.10) de la Sección 3.4 y lo generaliza a combinaciones lineales arbitrarias de los coeficientes de regresión. Otra aplicación importante de la ecuación (4.5) es la elección de \\(\\mathbf{c}\\) correspondiente a un conjunto de variables explicativas de interés, digamos, \\(\\mathbf{x}_{\\ast} = \\left( 1, x_{\\ast 1}, x_{\\ast 2}, \\ldots, x_{\\ast k} \\right)^{\\prime}\\). Estas pueden corresponder a una observación dentro del conjunto de datos o a un punto fuera de los datos disponibles. El parámetro de interés, \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta = \\mathbf{x}_{\\ast}^{\\prime} \\boldsymbol \\beta\\), es la respuesta esperada o la función de regresión en ese punto. Entonces, \\(\\mathbf{x}_{\\ast}^{\\prime} \\mathbf{b}\\) proporciona un estimador puntual y la ecuación (4.5) proporciona el intervalo de confianza correspondiente. Intervalos de Predicción La predicción es un objetivo inferencial que está estrechamente relacionado con la estimación de la función de regresión en un punto. Supongamos que, al considerar las contribuciones caritativas, conocemos los salarios de un individuo (y por lo tanto si los salarios superan los \\(97,500\\)) y deseamos predecir la cantidad de contribuciones caritativas. En general, asumimos que el conjunto de variables explicativas \\(\\mathbf{x}_{\\ast}\\) es conocido y deseamos predecir la respuesta correspondiente \\(y_{\\ast}\\). Esta nueva respuesta sigue las suposiciones descritas en la Sección 3.2. Específicamente, la respuesta esperada es \\(\\mathrm{E~}y_{\\ast} = \\mathbf{x}_{\\ast}^{\\prime} \\boldsymbol \\beta\\), \\(\\mathbf{x}_{\\ast}\\) es no estocástica, \\(\\mathrm{Var~}y_{\\ast} = \\sigma^2\\), \\(y_{\\ast}\\) es independiente de \\(\\{y_1, \\ldots, y_{n}\\}\\) y está distribuida normalmente. Bajo estas suposiciones, un intervalo de predicción del \\(100(1 - \\alpha)\\%\\) para \\(y_{\\ast}\\) es \\[ \\mathbf{x}_{\\ast}^{\\prime} \\mathbf{b} \\pm t_{n - (k + 1), 1 - \\alpha / 2} ~ s \\sqrt{1 + \\mathbf{x}_{\\ast}^{\\prime} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{x}_{\\ast}}. \\tag{4.6} \\] La ecuación (4.6) generaliza el intervalo de predicción introducido en la Sección 2.4. 4.3 Modelo ANOVA de Un Factor La Sección 4.1 mostró cómo incorporar variables categóricas no ordenadas, o factores, en un modelo de regresión lineal mediante el uso de variables binarias. Los factores son importantes en la investigación en ciencias sociales; pueden usarse para clasificar a las personas por género, etnia, estado civil, etc., o clasificar empresas por región geográfica, estructura organizativa, y así sucesivamente. En los estudios de seguros, las aseguradoras usan factores para categorizar a los asegurados según un “sistema de clasificación de riesgos.” Aquí, la idea es crear grupos de asegurados con características de riesgo similares que tendrán experiencias de reclamaciones similares. Estos grupos forman la base del precio del seguro, de modo que a cada asegurado se le cobra un monto adecuado a su categoría de riesgo. Este proceso a veces se conoce como “segmentación.” Aunque los factores pueden representarse como variables binarias en un modelo de regresión lineal, estudiamos los modelos de un factor como una unidad separada porque: El método de los mínimos cuadrados es mucho más simple, evitando la necesidad de invertir matrices de alta dimensión. Las interpretaciones resultantes de los coeficientes son más sencillas. El modelo de un factor sigue siendo un caso especial del modelo de regresión lineal. Por lo tanto, no se necesita teoría estadística adicional para establecer sus capacidades de inferencia estadística. Para establecer la notación para el modelo ANOVA de un factor, ahora consideramos el siguiente ejemplo. Ejemplo: Reclamaciones de Seguros de Automóviles. Examinamos la experiencia de reclamaciones de un gran asegurador de propiedades y accidentes del medio oeste de los Estados Unidos para seguros de automóviles particulares. La variable dependiente es la cantidad pagada en una reclamación cerrada, en dólares (reclamaciones que no se cerraron al final del año se manejan por separado). Las aseguradoras categorizan a los asegurados según un sistema de clasificación de riesgos. El sistema de clasificación de riesgos de esta aseguradora se basa en: Características del operador del automóvil (edad, género, estado civil y si es el conductor principal u ocasional de un automóvil). Características del vehículo (uso en la ciudad o en el campo, si el vehículo se usa para ir a la escuela o al trabajo, para negocios o placer, y si se usa para ir al trabajo, la distancia aproximada del trayecto). Estos factores se resumen en la variable categórica de clase de riesgo CLASS. La Tabla 4.5 muestra 18 clases de riesgo - no se proporciona información adicional de clasificación aquí para proteger los intereses propietarios de la aseguradora. La Tabla 4.5 resume los resultados de \\(n=6,773\\) reclamaciones para conductores de 50 años o más. Podemos ver que la reclamación mediana varía desde un mínimo de 707.40 (CLASE F7) hasta un máximo de 1,231.25 (CLASE C72). La distribución de las reclamaciones resulta ser asimétrica, por lo que consideramos \\(y\\) = reclamaciones logarítmicas. La tabla presenta medias, medianas y desviaciones estándar. Dado que la distribución de las reclamaciones logarítmicas es menos asimétrica, las medias están cerca de las medianas. La Figura 4.2 muestra la distribución de las reclamaciones logarítmicas por clase de riesgo. Tabla 4.5: Estadísticas Resumidas de Reclamaciones de Automóviles por Clase de Riesgo 1 2 3 4 5 6 Clase C1 C11 C1A C1B C1C C2 Número 726 1151 77 424 38 61 Mediana (dólares) 948.86 1,013.81 925.48 1,026.73 1,001.73 851.20 Mediana (en dólares log) 6.855 6.921 6.830 6.934 6.909 6.747 Media (en dólares log) 6.941 6.952 6.866 6.998 6.786 6.801 Desv. estándar (en dólares log) 1.064 1.074 1.072 1.068 1.110 0.948 Clase C6 C7 C71 C72 C7A C7B Número 911 913 1129 85 113 686 Mediana (dólares) 1,011.24 957.68 960.40 1,231.25 1,139.93 1,113.13 Mediana (en dólares log) 6.919 6.865 6.867 7.116 7.039 7.015 Media (en dólares log) 6.926 6.901 6.954 7.183 7.064 7.072 Desv. estándar (en dólares log) 1.115 1.058 1.038 0.988 1.021 1.103 Clase C7C F1 F11 F6 F7 F71 Número 81 29 40 157 59 93 Mediana (dólares) 1,200.00 1,078.04 774.79 1,105.04 707.40 1,118.73 Mediana (en dólares log) 7.090 6.983 6.652 7.008 6.562 7.020 Media (en dólares log) 7.244 7.004 6.804 6.910 6.577 6.935 Desv. estándar (en dólares log) 0.944 0.996 1.212 1.193 0.897 0.983 Figura 4.2: Diagramas de Caja de Reclamaciones Logarítmicas por Clase de Riesgo Código R para Producir la Tabla 4.5 y la Figura 4.2 AutoC &lt;- read.csv(&quot;CSVData/AutoClaims.csv&quot;, header=TRUE) # CREAR UNA TABLA DE MEDIAS Y DESVIACIONES ESTÁNDAR library(Hmisc) t1 &lt;- summarize(log(AutoC$PAID), AutoC$CLASS, length ) t2 &lt;- summarize(AutoC$PAID, AutoC$CLASS, median) t3 &lt;- summarize(log(AutoC$PAID), AutoC$CLASS, median) t4 &lt;- summarize(log(AutoC$PAID), AutoC$CLASS, mean) t5 &lt;- summarize(log(AutoC$PAID), AutoC$CLASS, sd) tablemat &lt;- cbind(t1, format(round(t2[2], digits = 2), big.mark = &#39;,&#39;), round(t3[2], digits = 3), round(t4[2], digits = 3), round(t5[2], digits = 3)) block1 &lt;- t(tablemat[1:6,]) block2 &lt;- t(tablemat[7:12,]) block3 &lt;- t(tablemat[13:18,]) bigblock &lt;- rbind(block1, block2, block3) temprow &lt;- c(&quot;Clase&quot;, &quot;Número&quot;, &quot;Mediana (dólares)&quot;, &quot;Mediana (en dólares log)&quot; ,&quot;Media (en dólares log)&quot;, &quot;Desv. estándar (en dólares log)&quot;) bigblock1 &lt;- cbind (c(temprow,temprow, temprow), bigblock) row.names(bigblock1) &lt;- NULL TableGen1(TableData=bigblock1 , TextTitle=&#39;Estadísticas Resumidas de Reclamaciones de Automóviles por Clase de Riesgo&#39;, Align=&#39;lrrrrrr&#39;, Digits=3, ColumnSpec=1:6, ColWidth = ColWidth4) %&gt;% kableExtra::column_spec(1, width = &quot;6cm&quot;) %&gt;% kableExtra::row_spec(6, extra_css = &quot;border-bottom: 2px solid black;&quot;) %&gt;% kableExtra::row_spec(12, extra_css = &quot;border-bottom: 2px solid black;&quot;) library(HH) AutoC &lt;- read.csv(&quot;CSVData/AutoClaims.csv&quot;, header=TRUE) # RECLAMACIONES DE AUTOMÓVILES # FIGURA 4.2 par(cex=0.6) boxplot(log(PAID) ~ CLASS,cex=.6, cex.labels=2, data = AutoC, xlab = &quot;&quot;, ylab = &quot;&quot;) Esta sección se centra en la clase de riesgo (CLASS) como la variable explicativa. Usamos la notación \\(y_{ij}\\) para referirnos a la \\(i\\)-ésima observación de la \\(j\\)-ésima clase de riesgo. Para la \\(j\\)-ésima clase de riesgo, asumimos que hay \\(n_j\\) observaciones. Existen \\(n=n_1+n_2+\\ldots +n_c\\) observaciones en total. Los datos son: \\[ \\begin{array}{cccccc} \\small{\\text{Datos para la clase de riesgo }}1 &amp; \\ \\ \\ \\ &amp; y_{11} &amp; y_{21} &amp; \\ldots &amp; y_{n_1,1} \\\\ \\small{\\text{Datos para la clase de riesgo }}2 &amp; &amp; y_{12} &amp; y_{22} &amp; \\ldots &amp; y_{n_2,1} \\\\ . &amp; &amp; . &amp; . &amp; \\ldots &amp; . \\\\ \\small{\\text{Datos para la clase de riesgo }} c &amp; &amp; y_{1c} &amp; y_{2c} &amp; \\ldots &amp; y_{n_c,c} \\end{array} \\] donde \\(c=18\\) es el número de niveles del factor CLASS. Debido a que cada nivel de un factor puede organizarse en una sola fila (o columna), otro término para este tipo de datos es una “clasificación de una vía.” Así, un modelo de una vía es otro término para un modelo de un factor. Una medida resumen importante para cada nivel del factor es el promedio de la muestra. Sea \\[ \\overline{y}_j=\\frac{1}{n_j}\\sum_{i=1}^{n_j}y_{ij} \\] el promedio de la \\(j\\)-ésima CLASS. Suposiciones del Modelo y Análisis La ecuación del modelo ANOVA de un factor es \\[ y_{ij}=\\mu_j+ \\varepsilon_{ij}\\ \\ \\ \\ \\ \\ i=1,\\ldots ,n_j,\\ \\ \\ \\ \\ j=1,\\ldots ,c. \\tag{4.7} \\] Al igual que con los modelos de regresión, se asume que las desviaciones aleatorias \\(\\{\\varepsilon_{ij} \\}\\) tienen una media cero con varianza constante (Suposición E3) y son independientes entre sí (Suposición E4). Dado que asumimos que el valor esperado de cada desviación es cero, tenemos \\(\\text{E}~y_{ij}=\\mu_j\\). Por lo tanto, interpretamos \\(\\mu_j\\) como el valor esperado de la respuesta \\(y_{ij}\\), es decir, la media \\(\\mu\\) varía según el nivel del factor \\(j\\). Para estimar los parámetros \\(\\{\\mu_j\\}\\), al igual que en la regresión, usamos el método de mínimos cuadrados, introducido en la Sección 2.1. Es decir, sea \\(\\mu^{\\ast}_j\\) una estimación “candidata” de \\(\\mu_j\\). La cantidad \\[ SS(\\mu^{\\ast}_1, \\ldots , \\mu^{\\ast}_{c}) = \\sum_{j=1}^{c} \\sum_{i=1}^{n_j} (y_{ij}-\\mu^{\\ast}_j)^2 \\] representa la suma de los cuadrados de las desviaciones de las respuestas respecto a estas estimaciones candidatas. A partir de algebra básica, el valor de \\(\\mu^{\\ast}_j\\) que minimiza esta suma de cuadrados es \\(\\bar{y}_j\\). Por lo tanto, \\(\\bar{y}_j\\) es la estimación por mínimos cuadrados de \\(\\mu_j\\). Para entender la confiabilidad de las estimaciones, podemos descomponer la variabilidad como en el caso de regresión, presentado en las Secciones 2.3.1 y 3.3. La suma mínima de los cuadrados de las desviaciones se llama suma de cuadrados del error y se define como \\[ Error ~SS = SS(\\bar{y}_1, \\ldots, \\bar{y}_{c}) = \\sum_{j=1}^{c} \\sum_{i=1}^{n_j} \\left(y_{ij}-\\bar{y}_j \\right)^2. \\] La variación total en el conjunto de datos se resume en la suma total de cuadrados, \\[ Total ~SS=\\sum_{j=1}^{c}\\sum_{i=1}^{n_j}(y_{ij}-\\bar{y})^2. \\] La diferencia, llamada suma de cuadrados del factor, se puede expresar como: \\[ \\begin{array}{ll} Factor~ SS &amp; = Total ~SS - Error ~SS \\\\ &amp; = \\sum_{j=1}^{c}\\sum_{i=1}^{n_j}(y_{ij}-\\bar{y})^2-\\sum_{j=1}^{c}\\sum_{i=1}^{n_j}(y_{ij}-\\bar{y}_j)^2 = \\sum_{j=1}^{c}\\sum_{i=1}^{n_j}(\\bar{y}_j-\\bar{y})^2 \\\\ &amp; = \\sum_{j=1}^{c}n_j(\\bar{y}_j-\\bar{y})^2. \\end{array} \\] Las dos últimas igualdades se derivan de la manipulación algebraica. El \\(Factor ~SS\\) desempeña el mismo papel que el \\(Regression ~SS\\) en los Capítulos 2 y 3. La descomposición de la variabilidad se resume en la Tabla 4.6. Tabla 4.6: Tabla ANOVA para el Modelo de Un Factor Fuente Suma de Cuadrados \\(df\\) Media Cuadrática Factor \\(Factor ~SS\\) \\(c-1\\) \\(Factor ~MS\\) Error \\(Error ~SS\\) \\(n-c\\) \\(Error ~MS\\) Total \\(Total ~SS\\) \\(n-1\\) Las convenciones para esta tabla son las mismas que en el caso de regresión. Es decir, la columna de media cuadrática (MS) se define dividiendo la columna de suma de cuadrados (SS) por la columna de grados de libertad (df). Por lo tanto, \\(Factor~MS \\equiv (Factor~SS)/(c-1)\\) y \\(Error~MS \\equiv (Error~SS)/(n-c)\\). Usamos \\[ s^2 = \\text{Error MS} = \\frac{1}{n-c} \\sum_{j=1}^{c}\\sum_{i=1}^{n_j} e_{ij}^2 \\] como nuestra estimación de \\(\\sigma^2\\), donde \\(e_{ij} = y_{ij} - \\bar{y}_j\\) es el residuo. Con este valor de \\(s\\), se puede mostrar que el intervalo de estimación para \\(\\mu_j\\) es \\[ \\bar{y}_j \\pm t_{n-c,1-\\alpha /2}\\frac{s}{\\sqrt{n_j}}. \\tag{4.8} \\] Aquí, el valor t \\(t_{n-c,1-\\alpha /2}\\) es un percentil de la distribución t con \\(df=n-c\\) grados de libertad. Ejemplo: Reclamaciones de Automóviles - Continuación. Para ilustrar, la tabla ANOVA que resume el ajuste para los datos de reclamaciones de automóviles se presenta en la Tabla 4.7. Aquí, vemos que la media cuadrática del error es \\(s^2 = 1.14.\\) Tabla 4.7: Tabla ANOVA para Reclamaciones de Automóviles Logarítmicas Fuente Suma de Cuadrados \\(df\\) Media Cuadrática CLASS 39.2 17 2.31 Error 7729.0 6755 1.14 Total 7768.2 6772 En la tarificación de automóviles, se usan los promedios de las reclamaciones para ayudar a fijar los precios de las coberturas de seguros. Como ejemplo, para la CLASS C72, el promedio de la reclamación logarítmica es 7.183. A partir de la ecuación (4.8), un intervalo de confianza del 95% es \\[ \\small{ 7.183 \\pm (1.96) \\frac{\\sqrt{1.14}}{\\sqrt{85}} = 7.183 \\pm 0.227 = (6.956 ,7.410). } \\] Cabe destacar que estas estimaciones están en unidades logarítmicas naturales. En dólares, nuestra estimación puntual es \\(e^{7.183} = 1,316.85\\) y nuestro intervalo de confianza del 95% es \\((e^{6.956} , e^{7.410}) \\text{ o } (\\$1,049.43, \\$1,652.43)\\). Una característica importante de la descomposición y estimación en un ANOVA de un factor es la facilidad de cálculo. Aunque la suma de cuadrados parece compleja, es importante señalar que no se requieren cálculos matriciales. En cambio, todos los cálculos se pueden realizar mediante promedios y sumas de cuadrados. Esto ha sido un aspecto importante históricamente, antes de la era de la computación de escritorio disponible. Además, las aseguradoras pueden segmentar sus carteras en cientos o incluso miles de clases de riesgo en lugar de las 18 utilizadas en nuestros datos de Reclamaciones de Automóviles. Por lo tanto, incluso hoy en día puede ser útil identificar una variable categórica como un factor y dejar que su software estadístico utilice técnicas de estimación ANOVA. Además, la estimación ANOVA también proporciona una interpretación directa de los resultados. Vínculo con la Regresión Esta subsección muestra cómo un modelo ANOVA de un factor se puede reescribir como un modelo de regresión. Para ello, hemos visto que tanto el modelo de regresión como el modelo ANOVA de un factor utilizan una estructura de error lineal con las Suposiciones E3 y E4 para errores idénticamente y distribuidos de manera independiente. De manera similar, ambos utilizan la suposición de normalidad E5 para resultados de inferencia seleccionados (como intervalos de confianza). Ambos emplean variables explicativas no estocásticas como en la Suposición E2. Ambos tienen un término de error aditivo (media cero), por lo que la principal diferencia aparente está en la respuesta esperada, \\(\\mathrm{E }~y\\). Para el modelo de regresión lineal, \\(\\mathrm{E }~y\\) es una combinación lineal de variables explicativas (Suposición F1). Para el modelo ANOVA de un factor, \\(\\mathrm{E}~y_] = \\mu_j\\) es una media que depende del nivel del factor. Para igualar estos dos enfoques, para el factor ANOVA con \\(c\\) niveles, definimos \\(c\\) variables binarias, \\(x_1, x_2, \\ldots, x_c\\). Aquí, \\(x_j\\) indica si una observación cae o no en el nivel \\(j\\)-ésimo. Con estas variables, podemos reescribir nuestro modelo ANOVA de un factor como \\[ y = \\mu_1 x_1 + \\mu_2 x_2 + \\ldots + \\mu_c x_c + \\varepsilon. \\tag{4.9} \\] Así, hemos reescrito la respuesta esperada del ANOVA de un factor como una función de regresión, aunque utilizando una forma sin intercepto (como en la ecuación (3.5)). El ANOVA de un factor es un caso especial de nuestro modelo de regresión habitual, utilizando variables binarias del factor como variables explicativas en la función de regresión. Como hemos visto, no se necesitan cálculos matriciales para la estimación por mínimos cuadrados. Sin embargo, siempre se pueden utilizar los procedimientos matriciales desarrollados en el Capítulo 3. La Sección 4.7.1 muestra cómo nuestra expresión matricial habitual para los coeficientes de regresión (\\(\\mathbf{b} = \\left(\\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{\\prime}\\mathbf{y}\\)) se reduce a las estimaciones simples \\(\\bar{y}_j\\) cuando se utiliza una sola variable categórica. Reparametrización Para incluir un término de intercepto, definimos \\(\\tau_j = \\mu_j - \\mu\\), donde \\(\\mu\\) es un parámetro aún no especificado. Como cada observación debe pertenecer a una de las \\(c\\) categorías, tenemos que \\(x_1 + x_2 + \\ldots + x_{c} = 1\\) para cada observación. Así, al usar \\(\\mu_j = \\tau_j + \\mu\\) en la ecuación (4.9), obtenemos \\[ y = \\mu + \\tau_1 x_1 + \\tau_2 x_2 + \\ldots + \\tau_{c} x_{c} + \\varepsilon, \\tag{4.10} \\] Así, hemos reescrito el modelo en lo que parece ser nuestro formato usual de regresión. Usamos \\(\\tau\\) en lugar de \\(\\beta\\) por razones históricas. Los modelos ANOVA fueron inventados por R.A. Fisher en relación con experimentos agrícolas. Aquí, la configuración típica es aplicar varios tratamientos a parcelas de tierra para cuantificar las respuestas de rendimiento de los cultivos. Así, la letra griega “t”, \\(\\tau\\), sugiere la palabra tratamiento, otro término utilizado para describir los niveles del factor de interés. Una versión más simple de la ecuación (4.10) se puede dar cuando identificamos el nivel del factor. Es decir, si sabemos que una observación pertenece al nivel \\(j\\)-ésimo, entonces solo \\(x_j\\) es uno y los otros \\(x\\) son 0. Por lo tanto, una expresión más simple de la ecuación (4.10) es \\[ y_{ij} = \\mu + \\tau_j + \\varepsilon_{ij}. \\] Al comparar las ecuaciones (4.9) y (4.10), vemos que el número de parámetros ha aumentado en uno. Es decir, en la ecuación (4.9) hay \\(c\\) parámetros, \\(\\mu_1, \\ldots, \\mu_c\\), mientras que en la ecuación (4.10) hay \\(c + 1\\) parámetros, \\(\\mu\\) y \\(\\tau_1, \\ldots, \\tau_c\\). Se dice que el modelo en la ecuación (4.10) está sobreparametrizado. Es posible estimar este modelo directamente, utilizando la teoría general de modelos lineales, resumida en la Sección 4.7.3. En esta teoría, los coeficientes de regresión no necesitan ser identificables. Alternativamente, se pueden hacer equivalentes estas dos expresiones restringiendo el movimiento de los parámetros en la ecuación (4.10). A continuación, presentamos dos formas de imponer restricciones. El primer tipo de restricción, generalmente utilizado en el contexto de regresión, es requerir que uno de los \\(\\tau\\) sea cero. Esto equivale a eliminar una de las variables explicativas. Por ejemplo, podríamos usar \\[ y = \\mu + \\tau_1 x_1 + \\tau_2 x_2 + \\ldots + \\tau_{c-1} x_{c-1} + \\varepsilon, \\tag{4.11} \\] eliminando \\(x_c\\). Con esta formulación, es fácil ajustar el modelo en la ecuación (4.11) utilizando rutinas de software de regresión, porque solo se necesita ejecutar la regresión con \\(c-1\\) variables explicativas. Sin embargo, se debe tener cuidado con la interpretación de los parámetros. Para igualar los modelos en las ecuaciones (4.9) y (4.10), necesitamos definir \\(\\mu \\equiv \\mu_c\\) y \\(\\tau_j = \\mu_j - \\mu_c\\) para \\(j=1,2,\\ldots,c-1\\). Es decir, el término de intercepto de la regresión es el nivel medio de la categoría eliminada, y cada coeficiente de regresión es la diferencia entre un nivel medio y el nivel medio eliminado. No es necesario eliminar el último nivel \\(c\\), y de hecho, se podría eliminar cualquier nivel. Sin embargo, la interpretación de los parámetros depende de la variable eliminada. Con esta restricción, los valores ajustados son \\(\\hat{\\mu} = \\hat{\\mu}_c = \\bar{y}_c\\) y \\(\\hat{\\tau}_j = \\hat{\\mu}_j - \\hat{\\mu}_c = \\bar{y}_j - \\bar{y}_c.\\) Recordemos que el símbolo de sombrero (\\(\\hat{\\cdot}\\)), o “hat,” representa un valor estimado o ajustado. El segundo tipo de restricción es interpretar \\(\\mu\\) como una media para toda la población. Para ello, el requisito usual es \\(\\mu \\equiv \\frac{1}{n} \\sum_{j=1}^c n_j \\mu_j\\), es decir, \\(\\mu\\) es un promedio ponderado de medias. Con esta definición, interpretamos \\(\\tau_j = \\mu_j - \\mu\\) como diferencias de tratamiento entre un nivel medio y la media poblacional. Otra forma de expresar esta restricción es \\(\\sum_{j=1}^{c} n_j \\tau_j = 0\\), es decir, la suma (ponderada) de las diferencias de tratamiento es cero. La desventaja de esta restricción es que no se puede implementar fácilmente con una rutina de regresión, y se necesita una rutina especial. La ventaja es que hay una simetría en las definiciones de los parámetros. No es necesario preocuparse por qué variable se está eliminando de la ecuación, lo cual es una consideración importante. Con esta restricción, los valores ajustados son \\[ \\hat{\\mu} = \\frac{1}{n} \\sum_{j=1}^{c} n_j \\hat{\\mu}_j = \\frac{1}{n} \\sum_{j=1}^{c} n_j \\bar{y}_j = \\bar{y} \\] y \\[ \\hat{\\tau}_j = \\hat{\\mu}_j - \\hat{\\mu} = \\bar{y}_j - \\bar{y}. \\] 4.4 Combinando Variables Explicativas Categóricas y Continuas Existen varias formas de combinar variables explicativas categóricas y continuas. Inicialmente, presentamos el caso de solo una variable categórica y una variable continua. Luego, presentamos brevemente el caso general, llamado modelo lineal general. Cuando se combinan modelos de variables categóricas y continuas, usamos la terminología factor para la variable categórica y covariable para la variable continua. Combinando un Factor y una Covariable Comencemos con los modelos más simples que utilizan un factor y una covariable. En la Sección 4.3, introdujimos el modelo de un solo factor \\(y_{ij} = \\mu_j + \\varepsilon_{ij}\\). En el Capítulo 2, introdujimos la regresión lineal básica en términos de una variable continua, o covariable, usando \\(y_{ij} = \\beta_0 + \\beta_1 x_{ij} + \\varepsilon_{ij}\\). La Tabla 4.8 resume diferentes enfoques que podrían usarse para representar combinaciones de un factor y una covariable. Tabla 4.8: Varios Modelos que Representan Combinaciones de Un Factor y Una Covariable Descripción del Modelo Notación ANOVA de un factor (modelo sin covariable) \\(y_{ij} = \\mu_j + \\varepsilon_{ij}\\) Regresión con intercepto y pendiente constante (modelo sin factor) \\(y_{ij} = \\beta_0 + \\beta_1 x_{ij} + \\varepsilon_{ij}\\) Regresión con intercepto variable y pendiente constante (modelo de análisis de covarianza) \\(y_{ij} = \\beta_{0j} + \\beta_1 x_{ij} + \\varepsilon_{ij}\\) Regresión con intercepto constante y pendiente variable \\(y_{ij} = \\beta_0 + \\beta_{1j} x_{ij} + \\varepsilon_{ij}\\) Regresión con intercepto y pendiente variable \\(y_{ij} = \\beta_{0j} + \\beta_{1j} x_{ij} + \\varepsilon_{ij}\\) Podemos interpretar la regresión con intercepto variable y pendiente constante como un modelo aditivo, porque estamos sumando el efecto del factor, \\(\\beta_{0j}\\), al efecto de la covariable, \\(\\beta_1 x_{ij}\\). Nótese que también se podría usar la notación \\(\\mu_j\\) en lugar de \\(\\beta_{0j}\\) para sugerir la presencia de un efecto de factor. Este también es conocido como un modelo de análisis de covarianza (ANCOVA). La regresión con intercepto y pendiente variables puede considerarse un modelo de interacción. Aquí, tanto el intercepto, \\(\\beta_{0j}\\), como la pendiente, \\(\\beta_{1j}\\), pueden variar según el nivel del factor. En este sentido, interpretamos que el factor y la covariable están “interactuando”. El modelo con intercepto constante y pendiente variable típicamente no se usa en la práctica; se incluye aquí por completitud. Con este modelo, el factor y la covariable interactúan solo a través de la pendiente variable. Las Figuras 4.3, 4.4 y 4.5 ilustran las respuestas esperadas de estos modelos. Figura 4.3: Gráfico de la respuesta esperada frente a la covariable para el modelo de regresión con intercepto variable y pendiente constante. Figura 4.4: Gráfico de la respuesta esperada frente a la covariable para el modelo de regresión con intercepto constante y pendiente variable. Figura 4.5: Gráfico de la respuesta esperada frente a la covariable para el modelo de regresión con intercepto variable y pendiente variable. Código R para Producir las Figuras 4.3, 4.4, y 4.5 ## Figura 4.3 x &lt;- seq(0, 100, length = 101) y &lt;- 0.15 * x par(mar = c(3.2, 3, .2, .2)) plot(x, y, type = &quot;l&quot;, xlim = c(10, 90), xaxt = &quot;n&quot;, ylim = c(1.5, 25), yaxt = &quot;n&quot;, ylab = &quot;&quot;, xlab = &quot;&quot;) mtext(&quot;y&quot;, side = 2, las = 1, line = 2, cex = 1.1) mtext(&quot;x&quot;, side = 1, line = 2, cex = 1.1) lines(x, y + 6) lines(x, y + 8) arrows(40, 5.5, 55, 5.5, code = 1, lwd = 2, angle = 15, length = 0.2) text(69, 5.5, expression(y == beta[&quot;0,3&quot;] + beta[1] * x), cex = 1.1) arrows(30, 13, 30, 16, code = 1, lwd = 2, angle = 15, length = 0.2) text(30, 17, expression(y == beta[&quot;0,2&quot;] + beta[1] * x), cex = 1.1) arrows(66, 15.5, 62, 12, code = 1, lwd = 2, angle = 15, length = 0.2) text(58, 11, expression(y == beta[&quot;0,1&quot;] + beta[1] * x), cex = 1.1) ## Figura 4.4 x &lt;- seq(0, 100, length = 101) y3 &lt;- 12 + 0.1 * x y2 &lt;- 12 - 0.1 * x y1 &lt;- 12 + 0.02 * x par(mar = c(3.2, 3, .2, .2)) plot(x, y1, type = &quot;l&quot;, xlim = c(3.5, 90), xaxt = &quot;n&quot;, ylim = c(1.5, 25), yaxt = &quot;n&quot;, ylab = &quot;&quot;, xlab = &quot;&quot;) mtext(&quot;y&quot;, side = 2, las = 1, line = 2, cex = 1.1) mtext(&quot;x&quot;, side = 1, line = 2, cex = 1.1) lines(x, y2) lines(x, y3) arrows(38, 8, 35, 4, code = 1, lwd = 2, angle = 15, length = 0.2) text(35, 3.5, expression(y == beta[0] + beta[&quot;1,2&quot;] * x), cex = 1.1) arrows(26, 15, 26, 18, code = 1, lwd = 2, angle = 15, length = 0.2) text(26, 19, expression(y == beta[0] + beta[&quot;1,3&quot;] * x), cex = 1.1) arrows(62, 13.5, 66, 16, code = 1, lwd = 2, angle = 15, length = 0.2) text(70, 16.5, expression(y == beta[0] + beta[&quot;1,1&quot;] * x), cex = 1.1) ## Figura 4.5 x1 &lt;- seq(50, 90, length = 51) y1 &lt;- -5 + 0.2 * x1 x2 &lt;- seq(30, 80, length = 41) y2 &lt;- 12 + 0.02 * x2 x3 &lt;- seq(10, 50, length = 41) y3 &lt;- 15 + 0.15 * x3 par(mar = c(3.2, 3, .2, .2)) plot(x1, y1, type = &quot;l&quot;, xlim = c(3.5, 90), xaxt = &quot;n&quot;, ylim = c(1.5, 25), ylab = &quot;&quot;, xlab = &quot;&quot;, yaxt = &quot;n&quot;) mtext(&quot;y&quot;, side = 2, las = 1, line = 2, cex = 1.1) mtext(&quot;x&quot;, side = 1, line = 2, cex = 1.1) lines(x2, y2) lines(x3, y3) arrows(68, 8, 65, 4, code = 1, lwd = 2, angle = 5, length = 0.2) text(65, 3.5, expression(y == beta[&quot;0,1&quot;] + beta[&quot;1,1&quot;] * x), cex = 1.1) arrows(50, 12.5, 30, 11.5, code = 1, lwd = 2, angle = 5, length = 0.2) text(30, 10.5, expression(y == beta[&quot;0,3&quot;] + beta[&quot;1,3&quot;] * x), cex = 1.1) arrows(30, 20, 20, 22, code = 1, lwd = 2, angle = 5, length = 0.2) text(20, 23, expression(y == beta[&quot;0,2&quot;] + beta[&quot;1,2&quot;] * x), cex = 1.1) Para cada modelo presentado en la Tabla 4.8, las estimaciones de los parámetros pueden calcularse utilizando el método de mínimos cuadrados. Como es habitual, esto significa escribir la respuesta esperada, \\(\\mathrm{E }~y_{ij}\\), como una función de variables conocidas y parámetros desconocidos. Para el modelo de regresión con intercepto variable y pendiente constante, las estimaciones de mínimos cuadrados pueden expresarse de manera compacta como: \\[ b_1 = \\frac{\\sum_{j=1}^{c}\\sum_{i=1}^{n_j} (x_{ij} - \\bar{x}_j) (y_{ij} - \\bar{y}_j)}{\\sum_{j=1}^{c}\\sum_{i=1}^{n_j} (x_{ij} - \\bar{x}_j)^2} \\] y \\(b_{0j} = \\bar{y}_j - b_1 \\bar{x}_j\\). De manera similar, las estimaciones de mínimos cuadrados para el modelo de regresión con intercepto y pendiente variables pueden expresarse como: \\[ b_{1j} = \\frac{\\sum_{i=1}^{n_j} (x_{ij} - \\bar{x}_j) (y_{ij} - \\bar{y}_j)}{\\sum_{i=1}^{n_j} (x_{ij} - \\bar{x}_j)^2} \\] y \\(b_{0j} = \\bar{y}_j - b_{1j} \\bar{x}_j\\). Con estas estimaciones de los parámetros, se pueden calcular los valores ajustados. Para cada modelo, los valores ajustados se definen como la respuesta esperada con los parámetros desconocidos reemplazados por sus estimaciones de mínimos cuadrados. Por ejemplo, para el modelo de regresión con intercepto variable y pendiente constante, los valores ajustados son \\(\\hat{y}_{ij} = b_{0j} + b_1 x_{ij}.\\) Ejemplo: Costos Hospitalarios en Wisconsin. Ahora estudiamos el impacto de varios predictores en los costos hospitalarios en el estado de Wisconsin. Identificar predictores de los costos hospitalarios puede proporcionar dirección a los hospitales, al gobierno, a las aseguradoras y a los consumidores en el control de estas variables, lo que a su vez lleva a un mejor control de los costos hospitalarios. Los datos para el año 1989 fueron obtenidos de la Oficina de Información de Salud, del Departamento de Salud y Servicios Humanos de Wisconsin. Se utilizan datos transversales, que detallan los costos de alta de 20 grupos relacionados con el diagnóstico (DRG) para hospitales en el estado de Wisconsin, desglosados en nueve áreas principales de servicios de salud y tres tipos de pagador (Pago por servicio, HMO y otros). Aunque hay 540 combinaciones potenciales de DRG, área y pagador (\\(20 \\times 9 \\times 3 = 540\\)), solo 526 combinaciones se realizaron realmente en el conjunto de datos de 1989. Otros predictores incluidos fueron el logaritmo del número total de altas (NO DSCHG) y el número total de camas hospitalarias (NUM BEDS) para cada combinación. La variable de respuesta es el logaritmo de los costos hospitalarios totales por número de altas (CHGNUM). Para simplificar la presentación, ahora consideramos solo los costos asociados con tres grupos relacionados con el diagnóstico (DRG): DRG #209, DRG #391, y DRG #430. La covariable, \\(x\\), es el logaritmo natural del número de altas. En entornos ideales, los hospitales con más pacientes disfrutan de menores costos debido a economías de escala. En entornos no ideales, los hospitales pueden no tener capacidad excedente y, por lo tanto, los hospitales con más pacientes tienen costos más altos. Uno de los propósitos de este análisis es investigar la relación entre los costos hospitalarios y la utilización hospitalaria. Recuerde que nuestra medida de los costos hospitalarios es el logaritmo de los costos por alta (\\(y\\)). El diagrama de dispersión en la Figura 4.6 da una idea preliminar de la relación entre \\(y\\) y \\(x\\). Notamos que parece haber una relación negativa entre \\(y\\) y \\(x\\). La relación negativa entre \\(y\\) y \\(x\\) sugerida por la Figura 4.6 es engañosa y está inducida por una variable omitida, la categoría del costo (DRG). Para ver el efecto conjunto de la variable categórica DRG y la variable continua \\(x\\), en la Figura 4.7 se muestra un gráfico de \\(y\\) versus \\(x\\) donde los símbolos de trazado son códigos para el nivel de la variable categórica. En este gráfico, vemos que el nivel de costo varía según el nivel del factor DRG. Además, para cada nivel de DRG, la pendiente entre \\(y\\) y \\(x\\) es cero o positiva. Las pendientes no son negativas, como sugiere la Figura 4.6. Figura 4.6: Gráfico del logaritmo natural del costo por alta versus logaritmo natural del número de altas. Este gráfico sugiere una relación negativa engañosa. Figura 4.7: Gráfico con letras del logaritmo natural del costo por alta versus logaritmo natural del número de altas según DRG. Aquí, A es para DRG #209, B es para DRG #391, y C es para DRG #430. Código R para Producir las Figuras 4.6 y 4.7 ## C5_HOSP.txt HospitalCosts &lt;- read.csv(&quot;CSVData/WiscHospCosts.csv&quot;, header=TRUE) # attach(HospitalCosts) # LNCHGNUM&lt;-log(CHG_NUM) ## Figura 4.6 par(mar=c(4.1,4,1,1), cex=1.1) plot(log(CHG_NUM) ~ log(NO_DSCHG), data=HospitalCosts, subset=DRG==209|DRG==391|DRG==430,type=&quot;p&quot;,pch=1,cex=0.6, xlim=c(2,9.2),xaxp=c(1.5,9,5),xlab=&quot;Número de Altas&quot;,ylim=c(5.9,9.8), yaxp=c(6.0,9.6,3),ylab=&quot;&quot;,las=1) mtext(&quot;CHGNUM&quot;, side=2, at=10.1, las=1, cex=1.1, adj=.4) ## Figura 4.7 DRGabc&lt;-character(length=length(HospitalCosts$DRG)) for (i in 1:length(HospitalCosts$DRG)) {DRGabc[i]&lt;-if (HospitalCosts$DRG[i]==209) &quot;A&quot; else if (HospitalCosts$DRG[i]==391) &quot;B&quot; else if (HospitalCosts$DRG[i]==430) &quot;C&quot; else &quot;D&quot;} par(mar=c(4.1,4,1,1), cex=1.1) plot(log(CHG_NUM) ~ log(NO_DSCHG), data=HospitalCosts, subset=DRG==209|DRG==391|DRG==430,type=&quot;p&quot;, pch=as.character(DRGabc),cex=0.8, xlim=c(2,9.2), xaxp=c(1.5,9,5),xlab=&quot;Número de Altas&quot;,ylim=c(5.9,9.8), yaxp=c(6.0,9.6,3),ylab=&quot;&quot;,las=1) mtext(&quot;CHGNUM&quot;, side=2, at=10.1, las=1, cex=1.1, adj=.4) Tabla 4.9: Bondad de Ajuste de los Modelos de Costos Hospitalarios en Wisconsin Descripción del Modelo Grados de libertad del modelo Grados de libertad del error Suma de cuadrados del error R-cuadrado (%) Media Cuadrática ANOVA de un factor 2 76 9.396 93.3 0.124 Regresión con intercepto y pendiente constantes 1 77 115.059 18.2 1.222 Regresión con intercepto variable y pendiente constante 3 75 7.482 94.7 0.100 Regresión con intercepto constante y pendiente variable 3 75 14.048 90.0 0.187 Regresión con intercepto y pendiente variables 5 73 5.458 96.1 0.075 Cada uno de los cinco modelos definidos en la Tabla 4.8 fue ajustado a este subconjunto del estudio de caso hospitalario. Las estadísticas resumen se encuentran en la Tabla 4.9. Para este conjunto de datos, hay \\(n = 79\\) observaciones y \\(c = 3\\) niveles del factor DRG. Para cada modelo, los grados de libertad del modelo son el número de parámetros del modelo menos uno. Los grados de libertad del error son el número de observaciones menos el número de parámetros del modelo. Usando variables binarias, cada uno de los modelos en la Tabla 4.8 puede escribirse en un formato de regresión. Como hemos visto en la Sección 4.2, cuando un modelo puede escribirse como un subconjunto de otro modelo más grande, tenemos procedimientos formales de prueba disponibles para decidir cuál modelo es más apropiado. Para ilustrar este procedimiento de prueba con nuestro ejemplo de DRG, a partir de la Tabla 4.9 y los gráficos asociados, parece claro que el factor DRG es importante. Además, una prueba \\(t\\), que no se presenta aquí, muestra que la covariable \\(x\\) es importante. Por lo tanto, comparemos el modelo completo \\(\\mathrm{E}~y_{ij} = \\beta_{0,j} + \\beta_{1,j}x\\) con el modelo reducido \\(\\mathrm{E}~y_{ij} = \\beta_{0,j} + \\beta_1x\\). En otras palabras, ¿hay una pendiente diferente para cada DRG? Usando la notación de la Sección 4.2, llamamos al intercepto y la pendiente variables el modelo completo. Bajo la hipótesis nula, \\(H_0: \\beta_{1,1} = \\beta_{1,2} = \\beta_{1,3}\\), obtenemos el modelo con intercepto variable y pendiente constante. Así, usando el cociente \\(F\\) en la ecuación (4.2), tenemos: \\[ \\small{ F\\text{-ratio} = \\frac{(Error~SS)_{reduced} - (Error~SS)_{full}}{ps_{full}^2} = \\frac{7.482 - 5.458}{2 \\times 0.075} = 13.535. } \\] El percentil 95 de la distribución \\(F\\) con \\(df_1 = p = 2\\) y \\(df_2 = (df)_{full} = 73\\) es aproximadamente 3.13. Por lo tanto, esta prueba nos lleva a rechazar la hipótesis nula y a declarar válida la alternativa, el modelo de regresión con intercepto variable y pendiente variable. Combinación de Dos Factores Hemos visto cómo combinar covariables, así como una covariable y un factor, tanto de manera aditiva como con interacciones. De la misma manera, supongamos que tenemos dos factores, como sexo (dos niveles: masculino/femenino) y edad (tres niveles: joven/mediana/anciana). Las variables binarias correspondientes serían \\(x_1\\) para indicar si la observación representa a una mujer, \\(x_2\\) para indicar si la observación representa a una persona joven, y \\(x_3\\) para indicar si la observación representa a una persona de mediana edad. Un modelo aditivo para estos dos factores puede usar la función de regresión \\[ \\mathrm{E }~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3. \\] Como hemos visto, este modelo es sencillo de interpretar. Por ejemplo, podemos interpretar \\(\\beta_1\\) como el efecto del sexo, manteniendo constante la edad. También podemos incorporar dos términos de interacción, \\(x_1 x_2\\) y \\(x_1 x_3\\). Usando las cinco variables explicativas, obtenemos la función de regresión \\[ \\mathrm{E }~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3. \\tag{4.12} \\] Aquí, las variables \\(x_1\\), \\(x_2\\), y \\(x_3\\) se conocen como los efectos principales. La Tabla 4.10 ayuda a interpretar esta ecuación. Específicamente, hay seis tipos de personas que podríamos encontrar: hombres y mujeres que son jóvenes, de mediana edad o ancianos. Tenemos seis parámetros en la ecuación (4.12). La Tabla 4.10 proporciona el enlace entre los parámetros y los tipos de personas. Al usar los términos de interacción, no imponemos ninguna especificación previa sobre los efectos aditivos de cada factor. De la Tabla 4.10, vemos que la interpretación de los coeficientes de regresión en la ecuación (4.12) no es directa. Sin embargo, usar el modelo aditivo con términos de interacción es equivalente a crear una nueva variable categórica con seis niveles, uno para cada tipo de persona. Si los términos de interacción son críticos en su estudio, puede ser conveniente crear un nuevo factor que incorpore los términos de interacción simplemente para facilitar la interpretación. Tabla 4.10: Función de Regresión para un Modelo de Dos Factores con Interacciones Sexo Edad \\(x_1\\) \\(x_2\\) \\(x_3\\) \\(x_4\\) \\(x_5\\) Función de Regresión Masculino Joven 0 1 0 0 0 \\(\\beta_0 + \\beta_2\\) Masculino Mediana 0 0 1 0 0 \\(\\beta_0 + \\beta_3\\) Masculino Anciana 0 0 0 0 0 \\(\\beta_0\\) Femenino Joven 1 1 0 1 0 \\(\\beta_0 + \\beta_1 + \\beta_2 + \\beta_4\\) Femenino Mediana 1 0 1 0 1 \\(\\beta_0 + \\beta_1 + \\beta_3 + \\beta_5\\) Femenino Anciana 1 0 0 0 0 \\(\\beta_0 + \\beta_1\\) Las extensiones a más de dos factores siguen de manera similar. Por ejemplo, supongamos que está examinando el comportamiento de empresas con sede en diez regiones geográficas, dos estructuras organizativas (con fines de lucro versus sin fines de lucro) con cuatro años de datos. Si decide tratar cada variable como un factor y desea modelar todos los términos de interacción, entonces esto es equivalente a un factor con \\(10 \\times 2 \\times 4 = 80\\) niveles. Los modelos con términos de interacción pueden tener un número considerable de parámetros y el analista debe ser prudente al especificar las interacciones a considerar. Modelo Lineal General El modelo lineal general extiende el modelo de regresión lineal de dos maneras. Primero, las variables explicativas pueden ser continuas, categóricas o una combinación. La única restricción es que entren linealmente de tal manera que la función de regresión resultante \\[ \\mathrm{E}~y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k \\tag{4.13} \\] sea una combinación lineal de coeficientes. Como hemos visto, podemos elevar al cuadrado las variables continuas o tomar otras transformaciones no lineales (como logaritmos) así como usar variables binarias para representar variables categóricas, por lo que esta “restricción”, como su nombre lo indica, permite una amplia clase de funciones generales para representar los datos. La segunda extensión es que las variables explicativas pueden ser combinaciones lineales unas de otras en el modelo lineal general. Debido a esto, en el caso del modelo lineal general, las estimaciones de los parámetros no tienen por qué ser únicas. Sin embargo, una característica importante del modelo lineal general es que los valores ajustados resultantes resultan ser únicos, utilizando el método de mínimos cuadrados. Por ejemplo, en la Sección 4.3 vimos que el modelo ANOVA de un factor podía expresarse como un modelo de regresión con \\(c\\) variables indicadoras. Sin embargo, si hubiéramos intentado estimar el modelo en la ecuación (4.10), el método de mínimos cuadrados no habría llegado a un conjunto único de estimaciones de los coeficientes de regresión. La razón es que, en la ecuación (4.10), cada variable explicativa puede expresarse como una combinación lineal de las otras. Por ejemplo, observe que \\(x_c = 1 - (x_1 + x_2 + \\ldots + x_{c-1})\\). El hecho de que las estimaciones de los parámetros no sean únicas es una desventaja, pero no una insuperable. La suposición de que las variables explicativas no sean combinaciones lineales unas de otras significa que podemos calcular estimaciones únicas de los coeficientes de regresión utilizando el método de mínimos cuadrados. En términos de matrices, porque las variables explicativas no son combinaciones lineales unas de otras, la matriz \\(\\mathbf{X}^{\\prime}\\mathbf{X}\\) no es invertible. Específicamente, supongamos que estamos considerando la función de regresión en la ecuación (4.13) y, utilizando el método de mínimos cuadrados, nuestras estimaciones de los coeficientes de regresión son \\(b_0^{o}, b_1^{o}, \\ldots, b_k^{o}\\). Este conjunto de estimaciones de los coeficientes de regresión minimiza nuestra suma de cuadrados de los errores, pero puede haber otros conjuntos de coeficientes que también minimicen la suma de cuadrados de los errores. Los valores ajustados se calculan como \\(\\hat{y}_i = b_0^{o} + b_1^{o} x_{i1} + \\ldots + b_k^{o} x_{ik}\\). Se puede demostrar que los valores ajustados resultantes son únicos, en el sentido de que cualquier conjunto de coeficientes que minimice la suma de cuadrados de los errores produce los mismos valores ajustados (ver Sección 4.7.3). Por lo tanto, para un conjunto de datos y un modelo lineal general especificado, los valores ajustados son únicos. Debido a que los residuos se calculan como las respuestas observadas menos los valores ajustados, tenemos que los residuos son únicos. Debido a que los residuos son únicos, tenemos que las sumas de cuadrados de los errores son únicas. Por lo tanto, parece razonable, y es cierto, que podemos usar la prueba general de hipótesis descrita en la Sección 4.2 para decidir si las colecciones de variables explicativas son importantes. En resumen, para los modelos lineales generales, las estimaciones de los parámetros pueden no ser únicas y, por lo tanto, no significativas. Una parte importante de los modelos de regresión es la interpretación de los coeficientes de regresión. Esta interpretación no está necesariamente disponible en el contexto del modelo lineal general. Sin embargo, para los modelos lineales generales, todavía podemos discutir la importancia de una variable individual o colección de variables a través de pruebas parciales F. Además, los valores ajustados, y el correspondiente ejercicio de predicción, funcionan en el contexto del modelo lineal general. La ventaja del contexto del modelo lineal general es que no necesitamos preocuparnos por el tipo de restricciones a imponer en los parámetros. Aunque no es el tema de este texto, esta ventaja es particularmente importante en los diseños experimentales complicados utilizados en las ciencias de la vida. El lector encontrará que las rutinas de estimación del modelo lineal general están ampliamente disponibles en los paquetes de software estadístico disponibles en el mercado hoy en día. 4.5 Lecturas Adicionales y Referencias Hay varios buenos libros sobre modelos lineales que se enfocan en variables categóricas y técnicas de análisis de variables. Hocking (2003) y Searle (1987) son buenos ejemplos. Referencias del Capítulo Hocking, Ronald R. (2003). Methods and Applications of Linear Models: Regression and the Analysis of Variance John Wiley and Sons, New York. Keeler, Emmett B., and John E. Rolph (1988). The demand for episodes of treatment in the Health Insurance Experiment. Journal of Health Economics 7: 337-367. Searle, Shayle R. (1987). Linear Models for Unbalanced Data. John Wiley &amp; Sons, New York. 4.6 Ejercicios 4.1. En este ejercicio, consideramos la relación entre dos estadísticas que resumen qué tan bien se ajusta un modelo de regresión, la razón \\(F\\) y \\(R^2\\), el coeficiente de determinación. (Aquí, la razón \\(F\\) es la estadística utilizada para probar la adecuación del modelo, no una estadística parcial \\(F\\)). Escribe \\(R^2\\) en términos de \\(Error ~SS\\) y \\(Regression ~SS\\). Escribe la razón \\(F\\) en términos de \\(Error ~SS\\), \\(Regression ~SS\\), \\(k\\), y \\(n\\). Establece la relación algebraica \\[ F\\text{-ratio} = \\frac{R^2}{1-R^2} \\frac{n-(k+1)}{k}. \\] Supongamos que \\(n = 40\\), \\(k = 5\\), y \\(R^2 = 0.20\\). Calcula la razón \\(F\\). Realiza la prueba habitual de adecuación del modelo para determinar si las cinco variables explicativas afectan significativamente de manera conjunta a la variable de respuesta. Supongamos que \\(n = 400\\) (no 40), \\(k = 5\\), y \\(R^2 = 0.20\\). Calcula la razón \\(F\\). Realiza la prueba habitual de adecuación del modelo para determinar si las cinco variables explicativas afectan significativamente de manera conjunta a la variable de respuesta. 4.2. Costos Hospitalarios. Este ejercicio considera los datos de gastos hospitalarios proporcionados por la Agencia de Investigación y Calidad de la Atención Médica de EE. UU. (AHRQ) y descritos en el Ejercicio 1.4. Produce un diagrama de dispersión, una correlación, y una regresión lineal de LNTOTCHG en función de AGE. ¿Es AGE un predictor significativo de LNTOTCHG? Te preocupa que los recién nacidos sigan un patrón diferente al de otras edades. Crea una variable binaria que indique si AGE es igual a cero o no. Ejecuta una regresión utilizando esta variable binaria y AGE como variables explicativas. ¿Es la variable binaria estadísticamente significativa? Ahora examina el efecto del género, usando la variable binaria FEMALE que es uno si el paciente es mujer y cero en caso contrario. Ejecuta una regresión utilizando AGE y FEMALE como variables explicativas. Realiza una segunda regresión que incluya estas dos variables con un término de interacción. Comenta si el efecto del género es importante en alguno de los modelos. Ahora considera el tipo de admisión, APRDRG, un acrónimo de “grupo de diagnóstico refinado para todos los pacientes”. Esta es una variable categórica explicativa que proporciona información sobre el tipo de admisión hospitalaria. Hay varios cientos de niveles de esta categoría. Por ejemplo, el nivel 640 representa la admisión de un recién nacido normal, con un peso neonatal mayor o igual a 2.5 kilogramos. Como otro ejemplo, el nivel 225 representa una admisión que resulta en una apendicectomía. d(i). Ejecuta un modelo ANOVA de un factor, utilizando APRDRG para predecir LNTOTCHG. Examina el \\(R^2\\) de este modelo y compáralo con el coeficiente de determinación del modelo de regresión lineal de LNTOTCHG en función de AGE. Basándote en esta comparación, ¿qué modelo crees que es preferido? d(ii). Para el modelo de un factor en la parte d(i), proporciona un intervalo de confianza del 95% para la media de LNTOTCHG para el nivel 225 correspondiente a una apendicectomía. Convierte tu respuesta final de dólares logarítmicos a dólares mediante exponenciación. d(iii). Ejecuta un modelo de regresión de APRDRG, FEMALE, y AGE en LNTOTCHG. Indica si AGE es un predictor estadísticamente significativo de LNTOTCHG. Indica si FEMALE es un predictor estadísticamente significativo de LNTOTCHG. 4.3. Utilización de Hogares de Ancianos. Este ejercicio considera los datos de hogares de ancianos proporcionados por el Departamento de Servicios de Salud y Familia de Wisconsin (DHFS) y descritos en los Ejercicios 1.2, 2.10, y 2.20. Además de las variables de tamaño, también tenemos información sobre varias variables binarias. La variable URBAN se utiliza para indicar la ubicación de la instalación. Es uno si la instalación se encuentra en un entorno urbano y cero en caso contrario. La variable MCERT indica si la instalación está certificada por Medicare. La mayoría, pero no todos, los hogares de ancianos están certificados para proporcionar atención financiada por Medicare. Hay tres estructuras organizativas para los hogares de ancianos: gobierno (estado, condados, municipios), empresas con fines de lucro y organizaciones exentas de impuestos. Periódicamente, las instalaciones pueden cambiar de propietario y, con menos frecuencia, de tipo de propiedad. Creamos dos variables binarias PRO y TAXEXEMPT para denotar empresas con fines de lucro y organizaciones exentas de impuestos, respectivamente. Algunos hogares de ancianos optan por no comprar cobertura de seguro privado para sus empleados. En cambio, estas instalaciones proporcionan directamente seguros y beneficios de pensión a sus empleados; esto se conoce como “auto-financiamiento del seguro”. Usamos la variable binaria SELFFUNDINS para denotarlo. Decides examinar la relación entre LOGTPY (\\(y\\)) y las variables explicativas. Usa los datos del año 2001 del informe de costos y realiza el siguiente análisis. Hay tres niveles de estructuras organizativas, pero solo utilizamos dos variables binarias (PRO y TAXEXEMPT). Explica por qué. Realiza un análisis de varianza de un factor usando TAXEXEMPT como el factor. Decide si exento de impuestos es o no un factor importante para determinar LOGTPY. Establece tu hipótesis nula, hipótesis alternativa y todos los componentes de la regla de toma de decisiones. Utiliza un nivel de significancia del 5%. Realiza un análisis de varianza de un factor usando MCERT como el factor. Decide si MCERT es o no un factor importante para determinar LOGTPY. c(i). Proporciona una estimación puntual de LOGTPY para una instalación de enfermería que no está certificada por Medicare. c(ii). Proporciona un intervalo de confianza del 95% para tu estimación puntual en la parte (i). Ejecuta un modelo de regresión usando las variables binarias, URBAN, PRO, TAXEXEMPT, SELFFUNDINS, y MCERT. Encuentra \\(R^2\\). ¿Qué variables son estadísticamente significativas? Ejecuta un modelo de regresión usando todas las variables explicativas, LOGNUMBED, LOGSQRFOOT, URBAN, PRO, TAXEXEMPT, SELFFUNDINS, y MCERT. Encuentra \\(R^2\\). ¿Qué variables son estadísticamente significativas? e(i). Calcula la correlación parcial entre LOGTPY y LOGSQRFOOT. Compárala con la correlación entre LOGTPY y LOGSQRFOOT. Explica por qué la correlación parcial es pequeña. e(ii). Compara el bajo nivel de los \\(t\\)-ratios (para probar la importancia de los coeficientes de regresión individuales) y el alto nivel de la razón \\(F\\) (para probar la adecuación del modelo). Describe la aparente inconsistencia y proporciona una explicación para esta inconsistencia. 4.4. Reclamaciones de Seguros de Automóviles. Consulta el Ejercicio 1.3. Ejecuta una regresión de LNPAID en AGE. ¿Es AGE una variable estadísticamente significativa? Para responder a esta pregunta, usa una prueba formal de hipótesis. Establece tu hipótesis nula y alternativa, el criterio de toma de decisiones y tu regla de toma de decisiones. También comenta sobre la bondad del ajuste de esta variable. Considera usar la clase como una sola variable explicativa. Usa el factor único para estimar el modelo y responde a las siguientes preguntas. b(i). ¿Cuál es la estimación puntual de las reclamaciones en la clase C7, conductores de 50-69 años, que conducen al trabajo o la escuela, menos de 30 millas por semana con un kilometraje anual inferior a 7500, en unidades logarítmicas naturales? b(ii). Determina el intervalo de confianza del 95% correspondiente de las reclamaciones esperadas, en unidades logarítmicas naturales. b(iii). Convierte el intervalo de confianza del 95% de las reclamaciones esperadas que determinaste en la parte b(ii) a dólares. Ejecuta una regresión de LNPAID en AGE, GENDER, y las variables categóricas STATE CODE y CLASS. c(i). ¿Es GENDER una variable estadísticamente significativa? Para responder a esta pregunta, usa una prueba formal de hipótesis. Establece tu hipótesis nula y alternativa, el criterio de toma de decisiones y tu regla de toma de decisiones. c(ii). ¿Es CLASS una variable estadísticamente significativa? Para responder a esta pregunta, usa una prueba formal de hipótesis. Establece tu hipótesis nula y alternativa, el criterio de toma de decisiones y tu regla de toma de decisiones. c(iii). Usa el modelo para proporcionar una estimación puntual de las reclamaciones en dólares (no en dólares logarítmicos) para un hombre de 60 años en el STATE 2 en la CLASS C7. c(iv). Escribe el coeficiente asociado con la CLASS C7 e interpreta este coeficiente. 4.5. Ventas de la Lotería de Wisconsin. Este ejercicio considera los datos de ventas de lotería del Estado de Wisconsin que se describieron en la Sección 2.1 y se examinaron en el Ejercicio 3.4. Parte 1: Decides examinar la relación entre SALES (\\(y\\)) y las ocho variables explicativas (PERPERHH, MEDSCHYR, MEDHVL, PRCRENT, PRC55P, HHMEDAGE, MEDINC, y POP). Ajusta un modelo de regresión de SALES en las ocho variables explicativas. Encuentra \\(R^2\\). b(i). Utilízalo para calcular el coeficiente de correlación entre los valores observados y los valores ajustados. b(ii). Quieres usar \\(R^2\\) para probar la adecuación del modelo en la parte (a). Usa una prueba formal de hipótesis. Establece tu hipótesis nula y alternativa, el criterio de toma de decisiones y tus reglas de toma de decisiones. Prueba si POP, MEDSCHYR, y MEDHVL son variables explicativas importantes de forma conjunta para comprender SALES. Parte 2: Después del análisis preliminar en la Parte 1, decides examinar la relación entre SALES (\\(y\\)) y POP, MEDSCHYR, y MEDHVL. Ajusta un modelo de regresión de SALES en estas tres variables explicativas. ¿Ha disminuido el coeficiente de determinación del modelo de regresión de ocho variables al modelo de tres variables? ¿Significa esto que el modelo no ha mejorado o proporciona poca información? Explica tu respuesta. Para determinar formalmente si se debe usar el modelo de tres o de ocho variables, utiliza una prueba parcial \\(F\\). Establece tu hipótesis nula y alternativa, el criterio de toma de decisiones y tus reglas de toma de decisiones. 4.6. Gastos de Compañías de Seguros. Este ejercicio considera los datos de compañías de seguros del NAIC y descritos en los Ejercicios 1.6 y 3.5. ¿Son importantes los términos cuadrados? Considera un modelo lineal de LNEXPENSES con doce variables explicativas. Para las variables explicativas, incluye ASSETS, GROUP, ambas versiones de pérdidas y primas brutas, así como las dos variables BLS. También incluye el cuadrado de cada una de las dos pérdidas y las dos primas brutas. Prueba si los cuatro términos cuadrados son conjuntamente estadísticamente significativos, usando una prueba parcial \\(F\\). Establece tus hipótesis nula y alternativa, el criterio de toma de decisiones y tus reglas de toma de decisiones. ¿Son importantes los términos de interacción con GROUP? Omite las dos variables BLS, de manera que ahora hay once variables: ASSETS, GROUP, ambas versiones de pérdidas y primas brutas, así como las interacciones de GROUP con ASSETS y ambas versiones de pérdidas y primas brutas. Prueba si los cinco términos de interacción son conjuntamente estadísticamente significativos, usando una prueba parcial \\(F\\). Establece tus hipótesis nula y alternativa, el criterio de toma de decisiones y tus reglas de toma de decisiones. Estás examinando una compañía que no está en la muestra con valores LONGLOSS = 0.025, SHORTLOSS = 0.040, GPWPERSONAL = 0.050, GPWCOMM = 0.120, ASSETS = 0.400, CASH = 0.350, y GROUP = 1. Usa el modelo de interacción de once variables de la parte (b) para producir un intervalo de predicción del 95% para esta compañía. 4.7. Expectativas de Vida Nacionales. Continuamos el análisis iniciado en los Ejercicios 1.7, 2.22, y 3.6. Considera la regresión utilizando tres variables explicativas, FERTILITY, PUBLICEDUCATION, y ln(HEALTH) que hiciste en el Ejercicio 3.6. Prueba si PUBLICEDUCATION y ln(HEALTH) son conjuntamente estadísticamente significativos, usando una prueba parcial \\(F\\). Establece tus hipótesis nula y alternativa, el criterio de toma de decisiones y tus reglas de toma de decisiones. (Sugerencia: Usa la forma del coeficiente de determinación para calcular el estadístico de la prueba.) Proporciona un valor aproximado de \\(p\\) para la prueba. Ahora introducimos la variable REGION, resumida en la Tabla 4.11. Un diagrama de caja de las expectativas de vida versus REGION se muestra en la Figura 4.8. Describe lo que aprendemos de la Tabla y el diagrama de caja sobre el efecto de REGION en LIFEEXP. Figura 4.8: Diagramas de Caja de LIFEEXP por REGION Tabla 4.11: Esperanza de Vida Promedio por Región Región Descripción de la Región Número Media 1 Estados Árabes 13 71.9 2 Asia Oriental y el Pacífico 17 69.1 3 América Latina y el Caribe 25 72.8 4 Asia del Sur 7 65.1 5 Europa Meridional 3 67.4 6 África Subsahariana 38 52.2 7 Europa Central y del Este 24 71.6 8 Altos Ingresos OCDE 23 79.6 Todos 150 67.4 Ajusta un modelo de regresión utilizando solo el factor REGION. ¿Es REGION un determinante estadísticamente significativo de LIFEEXP? Establece tus hipótesis nula y alternativa, el criterio de toma de decisiones y tus reglas de toma de decisiones. Ajusta un modelo de regresión utilizando tres variables explicativas, FERTILITY, PUBLICEDUCATION, y ln(HEALTH), así como la variable categórica REGION. d(i). Estás examinando un país que no está en la muestra con valores FERTILITY = 2.0, PUBLICEDUCATION = 5.0, y ln(HEALTH) = 1.0. Produce dos valores predichos de esperanza de vida asumiendo que el país es de (1) un estado árabe y (2) África Subsahariana. d(ii). Proporciona un intervalo de confianza del 95% para la diferencia en expectativas de vida entre un estado árabe y un país de África Subsahariana. d(iii). Proporciona la estimación puntual (usualmente de mínimos cuadrados ordinarios) para la diferencia en expectativas de vida entre un país de África Subsahariana y un país de altos ingresos de la OECD (Organización para la Cooperación y el Desarrollo Económico). 4.7 Suplemento Técnico - Expresiones Matriciales 4.7.1 Expresión de Modelos con Variables Categóricas en Forma Matricial El Capítulo 3 mostró cómo escribir la ecuación del modelo de regresión en la forma \\(\\mathbf{y = X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\) donde \\(\\mathbf{X}\\) es una matriz de variables explicativas. Esta forma permite el cálculo directo de los coeficientes de regresión, \\(\\mathbf{b} = \\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}\\). Esta sección muestra cómo el modelo y los cálculos se reducen a expresiones más simples cuando las variables explicativas son categóricas. Modelo con una Variable Categórica. Considera el modelo con una variable categórica introducido en la Sección 4.3 con \\(c\\) niveles de la variable categórica. A partir de la ecuación (4.9), este modelo se puede escribir como \\[ \\mathbf{y} = \\begin{bmatrix} y_{1,1} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ y_{n_1,1} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ y_{1,c} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ y_{n_{c},c} \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ 1 &amp; 0 &amp; \\cdots &amp; \\cdots \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} \\mu_1 \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\mu_c \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_{1,1} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\varepsilon_{n_1,1} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\varepsilon_{1,c} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\varepsilon_{n_{c},c} \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\] Para hacer la notación más compacta, escribimos \\(\\mathbf{0}\\) y \\(\\mathbf{1}\\) para una columna de ceros y unos, respectivamente. Con esta convención, otra forma de expresarlo es \\[ \\mathbf{y} = \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdots &amp; \\mathbf{0}_1 \\\\ \\mathbf{0}_2 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0}_c &amp; \\mathbf{0}_c &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix} \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_c \\end{bmatrix} + \\boldsymbol{\\varepsilon} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\tag{4.14} \\] Aquí, \\(\\mathbf{0}_1\\) y \\(\\mathbf{1}_1\\) representan columnas de vectores de longitud \\(n_1\\) de ceros y unos, respectivamente, y de manera similar para \\(\\mathbf{0}_2, \\mathbf{1}_2, \\ldots, \\mathbf{0}_c, \\mathbf{1}_c\\). La ecuación (4.14) nos permite aplicar la maquinaria desarrollada para el modelo de regresión al modelo con una variable categórica. Como cálculo intermedio, tenemos \\[ \\begin{array}{ll} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} &amp;= \\left( \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_2 &amp; \\cdots &amp; \\mathbf{0}_c \\\\ \\mathbf{0}_1 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_c \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0}_1 &amp; \\mathbf{0}_2 &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix}^{\\prime} \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdots &amp; \\mathbf{0}_1 \\\\ \\mathbf{0}_2 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0}_c &amp; \\mathbf{0}_c &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix} \\right)^{-1}\\\\ &amp;= \\begin{bmatrix} n_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; n_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; n_c \\end{bmatrix}^{-1} = \\begin{bmatrix} \\frac{1}{n_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\frac{1}{n_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\frac{1}{n_c} \\end{bmatrix} . \\tag{4.15} \\end{array} \\] Así, las estimaciones de los parámetros son \\[ \\mathbf{b} = \\begin{bmatrix} \\hat{\\mu}_1 \\\\ \\vdots \\\\ \\hat{\\mu}_c \\end{bmatrix} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{X}^{\\prime} \\mathbf{y} = \\begin{bmatrix} \\frac{1}{n_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\frac{1}{n_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\frac{1}{n_c} \\end{bmatrix} \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_2 &amp; \\cdots &amp; \\mathbf{0}_c \\\\ \\mathbf{0}_1 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_c \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0}_1 &amp; \\mathbf{0}_2 &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix}^{\\prime} \\begin{bmatrix} y_{1,1} \\\\ \\vdots \\\\ y_{n_1,1} \\\\ \\vdots \\\\ y_{1,c} \\\\ \\vdots \\\\ y_{n_{c},c} \\end{bmatrix} \\] \\[ = \\begin{bmatrix} \\frac{1}{n_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\frac{1}{n_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\frac{1}{n_c} \\end{bmatrix} \\begin{bmatrix} \\sum_{i=1}^{n_1} y_{i1} \\\\ \\vdots \\\\ \\sum_{i=1}^{n_c} y_{ic} \\end{bmatrix} = \\begin{bmatrix} \\bar{y}_1 \\\\ \\vdots \\\\ \\bar{y}_c \\end{bmatrix} \\tag{4.16} \\] Hemos visto que la estimación por mínimos cuadrados de \\(\\mu_j\\), \\(\\bar{y}_j\\), se puede obtener directamente de la ecuación (4.9). Al reescribir el modelo en notación de regresión matricial, podemos recurrir a los resultados del modelo de regresión y no necesitamos probar las propiedades de los modelos con variables categóricas desde los principios básicos. Es decir, dado que este modelo está en formato de regresión, tenemos inmediatamente todas las propiedades del modelo de regresión. Indicar a tu software que una variable es categórica puede significar cálculos más eficientes, como en los cálculos de las estimaciones de regresión por mínimos cuadrados en la ecuación (4.16). Además, el cálculo de otras cantidades también se puede hacer de manera más directa. Como otro ejemplo, tenemos que el error estándar de \\(\\hat{\\mu}_j\\) es \\[ se(\\hat{\\mu}_j) = s ~ \\sqrt{j \\text{ th } \\textit{ diagonal element of } \\mathbf{(X}^{\\prime}\\mathbf{X)}^{-1}} = s/\\sqrt{n_j}. \\] Modelo con Una Variable Categórica y Una Continua Como otra ilustración, consideramos el modelo de intercepto variable y pendiente constante. Este modelo se resume como \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta + \\boldsymbol \\varepsilon\\) donde \\[ \\mathbf{X} = \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdots &amp; \\mathbf{0}_1 &amp; \\mathbf{x}_1 \\\\ \\mathbf{0}_2 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_2 &amp; \\mathbf{x}_2 \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ \\mathbf{0}_{c} &amp; \\mathbf{0}_{c} &amp; \\cdots &amp; \\mathbf{1}_c &amp; \\mathbf{x}_{c} \\end{bmatrix} \\text{ y } \\boldsymbol \\beta = \\begin{bmatrix} \\beta_{01} \\\\ \\beta_{02} \\\\ \\cdots \\\\ \\cdots \\\\ \\cdots \\\\ \\beta_{0c} \\\\ \\beta_1 \\end{bmatrix}. \\tag{4.17} \\] Aquí, \\(\\mathbf{0}_j\\) y \\(\\mathbf{1}_j\\) representan columnas de vectores de longitud \\(n_j\\) de ceros y unos, respectivamente, y \\(\\mathbf{x}_j = (x_{1j}, x_{2j}, \\ldots, x_{n_j, j})^{\\prime}\\) es la columna de la variable continua en el nivel \\(j\\). Técnicas de álgebra matricial sencillas proporcionan las estimaciones por mínimos cuadrados. 4.7.2 Cálculo Recursivo de Mínimos Cuadrados Cuando se calculan los coeficientes de regresión utilizando mínimos cuadrados, \\(\\mathbf{b} = \\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}\\), para algunas aplicaciones, la dimensión de \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\) puede ser grande, lo que causa dificultades computacionales. Afortunadamente, para algunos problemas, los cálculos se pueden dividir en problemas más pequeños que se pueden resolver de forma recursiva. Cálculo Recursivo de Mínimos Cuadrados. Supongamos que la función de regresión se puede escribir como \\[ \\mathrm{E}~\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta = \\left( \\mathbf{X}_1 : \\mathbf{X}_2 \\right) \\left( \\begin{array}{c} \\boldsymbol \\beta_1 \\\\ \\boldsymbol \\beta_2 \\\\ \\end{array} \\right), \\tag{4.18} \\] donde \\(\\mathbf{X}_1\\) tiene dimensiones \\(n \\times k_1\\), \\(\\mathbf{X}_2\\) tiene dimensiones \\(n \\times k_2\\), \\(k_1 + k_2 = k\\), \\(\\boldsymbol \\beta_1\\) tiene dimensiones \\(k_1 \\times 1\\), y \\(\\boldsymbol \\beta_2\\) tiene dimensiones \\(k_2 \\times 1\\). Definimos \\(\\mathbf{Q}_1 = \\mathbf{I} - \\mathbf{X}_1 \\left(\\mathbf{X}_1^{\\prime} \\mathbf{X}_1\\right)^{-1} \\mathbf{X}_1^{\\prime}\\). Entonces, el estimador por mínimos cuadrados se puede calcular como: \\[ \\mathbf{b} = \\left( \\begin{array}{c} \\mathbf{b}_1 \\\\ \\mathbf{b}_2 \\\\ \\end{array} \\right) = \\left( \\begin{array}{c} (\\mathbf{X}_1^{\\prime} \\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime} \\left( \\mathbf{y} - \\mathbf{X}_2 \\mathbf{b}_2 \\right) \\\\ \\left(\\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2\\right)^{-1} \\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{y} \\\\ \\end{array} \\right). \\tag{4.19} \\] La ecuación (4.19) proporciona el primer paso en la recursión. Puede ser iterada fácilmente para permitir una descomposición más detallada de \\(\\mathbf{X}\\). Caso Especial: Modelo con Una Variable Categórica y Una Continua. Para ilustrar la relevancia de la ecuación (4.19), consideremos el modelo resumido en la ecuación (4.17). Aquí, la dimensión de \\(\\mathbf{X}\\) es \\(n \\times (c+1)\\) y, por lo tanto, la dimensión de \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\) es \\((c+1) \\times (c+1)\\). Tomar la inversa de esta matriz podría ser difícil si \\(c\\) es grande. Para aplicar la ecuación (4.19), definimos \\[ \\mathbf{X}_1 = \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdots &amp; \\mathbf{0}_1 \\\\ \\mathbf{0}_2 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_2 \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ \\mathbf{0}_c &amp; \\mathbf{0}_c &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix} ~~~~~\\text{ y }~~~~ \\mathbf{X}_2 = \\begin{bmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\cdots \\\\ \\cdots \\\\ \\cdots \\\\ \\mathbf{x}_c \\end{bmatrix}. \\] En este caso, hemos visto en la ecuación (4.15) cómo es sencillo calcular \\(\\left(\\mathbf{X}_1^{\\prime} \\mathbf{X}_1\\right)^{-1}\\) sin necesidad de invertir matrices. Esto hace que el cálculo de \\(\\mathbf{Q}_1\\) sea directo. Con esto, podemos calcular \\(\\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2\\) y, dado que es un escalar, obtener inmediatamente su inversa. Esto proporciona \\(\\mathbf{b}_2\\), que luego se usa para calcular \\(\\mathbf{b}_1\\). Aunque este procedimiento no es tan directo como \\(\\mathbf{b} = \\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}\\), puede ser computacionalmente eficiente. Resultados de Matrices Particionadas Para establecer la ecuación (4.19), utilizamos resultados estándar en álgebra matricial respecto a las inversas de matrices particionadas. Resultados de Matrices Particionadas. Supongamos que podemos particionar la matriz \\((p+q) \\times (p+q)\\) \\(\\mathbf{B}\\) como \\[ \\mathbf{B} = \\begin{bmatrix} \\mathbf{B}_{11} &amp; \\mathbf{B}_{12} \\\\ \\mathbf{B}_{12}^{\\prime} &amp; \\mathbf{B}_{22} \\end{bmatrix}, \\] donde \\(\\mathbf{B}_{11}\\) es una matriz \\(p \\times p\\) invertible, \\(\\mathbf{B}_{22}\\) es una matriz \\(q \\times q\\) invertible, y \\(\\mathbf{B}_{12}\\) es una matriz \\(p \\times q\\). Entonces \\[ \\mathbf{B}^{-1} = \\begin{bmatrix} \\mathbf{C}_{11}^{-1} &amp; - \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{C}_{22}^{-1} \\\\ - \\mathbf{C}_{22}^{-1} \\mathbf{B}_{12}^{\\prime} \\mathbf{B}_{11}^{-1} &amp; \\mathbf{C}_{22}^{-1} \\end{bmatrix}, \\tag{4.20} \\] donde \\(\\mathbf{C}_{11} = \\mathbf{B}_{11} - \\mathbf{B}_{12} \\mathbf{B}_{22}^{-1} \\mathbf{B}_{12}^{\\prime}\\) y \\(\\mathbf{C}_{22} = \\mathbf{B}_{22} - \\mathbf{B}_{12}^{\\prime} \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12}.\\) Para verificar la ecuación (4.20), multiplique \\(\\mathbf{B}^{-1}\\) por \\(\\mathbf{B}\\) para obtener la matriz identidad \\(\\mathbf{I}\\). Además, \\[ \\mathbf{C}_{11}^{-1} = \\mathbf{B}_{11}^{-1} + \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{C}_{22}^{-1} \\mathbf{B}_{12}^{\\prime} \\mathbf{B}_{11}^{-1}. \\tag{4.21} \\] Ahora, primero escribimos el estimador de mínimos cuadrados como \\[ \\begin{array}{ll} \\mathbf{b} &amp;= \\left( \\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y} = \\left( \\left( \\begin{array}{c} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{X}_2^{\\prime} \\\\ \\end{array} \\right) \\left( \\mathbf{X}_1 : \\mathbf{X}_2 \\right)\\right)^{-1} \\left( \\begin{array}{c} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{X}_2^{\\prime} \\\\ \\end{array} \\right) \\mathbf{y} \\\\ &amp; = \\left( \\begin{array}{cc} \\mathbf{X}_1^{\\prime} \\mathbf{X}_1 &amp; \\mathbf{X}_1^{\\prime} \\mathbf{X}_2 \\\\ \\mathbf{X}_2^{\\prime} \\mathbf{X}_1 &amp; \\mathbf{X}_2^{\\prime} \\mathbf{X}_2 \\\\ \\end{array} \\right)^{-1} \\left( \\begin{array}{c} \\mathbf{X}_1^{\\prime} \\mathbf{y} \\\\ \\mathbf{X}_2^{\\prime} \\mathbf{y} \\\\ \\end{array} \\right) = \\left( \\begin{array}{c} \\mathbf{b}_1 \\\\ \\mathbf{b}_2 \\\\ \\end{array} \\right). \\end{array} \\] Para aplicar los resultados de matrices particionadas, definimos \\[ \\mathbf{Q}_j = \\mathbf{I} - \\mathbf{X}_j \\left(\\mathbf{X}_j^{\\prime}\\mathbf{X}_j \\right)^{-1} \\mathbf{X}_j^{\\prime}, \\] donde \\(j=1,2\\), y \\(\\mathbf{B}_{j,k} = \\mathbf{X}_j^{\\prime}\\mathbf{X}_k\\) para \\(j,k=1,2.\\) Esto significa que \\(\\mathbf{C}_{11}=\\mathbf{X}_1^{\\prime}\\mathbf{X}_1 - \\mathbf{X}_1^{\\prime}\\mathbf{X}_2 (\\mathbf{X}_2^{\\prime}\\mathbf{X}_2)^{-1} \\mathbf{X}_2^{\\prime}\\mathbf{X}_1^{\\prime }\\) \\(= \\mathbf{X}_1^{\\prime} \\mathbf{Q}_2 \\mathbf{X}_1\\) y de manera similar \\(\\mathbf{C}_{22} = \\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2\\). A partir de la segunda fila, tenemos \\[ \\begin{array}{ll} \\mathbf{b}_2 &amp;= \\mathbf{C}_{22}^{-1} \\left( -\\mathbf{B}_{12}^{\\prime} \\mathbf{B}_{11}^{-1}\\mathbf{X}_1^{\\prime} \\mathbf{y} + \\mathbf{X}_2^{\\prime} \\mathbf{y} \\right) \\\\ &amp;= \\left(\\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2 \\right)^{-1} \\left( - \\mathbf{X}_2^{\\prime} \\mathbf{X}_1 (\\mathbf{X}_1^{\\prime}\\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y} + \\mathbf{X}_2^{\\prime}\\mathbf{y} \\right) \\\\ &amp;= \\left(\\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2 \\right)^{-1} \\mathbf{X}_2^{\\prime} \\mathbf{Q}_1\\mathbf{y}. \\end{array} \\] A partir de la primera fila, \\[ \\begin{array}{ll} \\mathbf{b}_1 &amp;= \\mathbf{C}_{11}^{-1} \\mathbf{X}_1^{\\prime}\\mathbf{y} - \\mathbf{B}_{11}^{-1}\\mathbf{B}_{12}\\mathbf{C}_{22}^{-1} \\mathbf{X}_2^{\\prime}\\mathbf{y} \\\\ &amp;= \\left( \\mathbf{B}_{11}^{-1} + \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{C}_{22}^{-1} \\mathbf{B}_{21} \\mathbf{B}_{11}^{-1} \\right) \\mathbf{X}_1^{\\prime}\\mathbf{y} - \\mathbf{B}_{11}^{-1}\\mathbf{B}_{12}\\mathbf{C}_{22}^{-1} \\mathbf{X}_2^{\\prime}\\mathbf{y} \\\\ &amp;= \\mathbf{B}_{11}^{-1}\\mathbf{X}_1^{\\prime}\\mathbf{y} - \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{C}_{22}^{-1} \\left( -\\mathbf{B}_{21} \\mathbf{B}_{11}^{-1} \\mathbf{X}_1^{\\prime}\\mathbf{y} + \\mathbf{X}_2^{\\prime}\\mathbf{y} \\right)\\\\ &amp;= \\mathbf{B}_{11}^{-1}\\mathbf{X}_1^{\\prime}\\mathbf{y} - \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{b}_2 \\\\ &amp;= (\\mathbf{X}_1^{\\prime}\\mathbf{X}_1)^{-1}\\mathbf{X}_1^{\\prime}\\mathbf{y} - (\\mathbf{X}_1^{\\prime}\\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime}\\mathbf{X}_2 \\mathbf{b}_2 \\\\ &amp;= (\\mathbf{X}_1^{\\prime}\\mathbf{X}_1)^{-1}\\mathbf{X}_1^{\\prime} \\left( \\mathbf{y} - \\mathbf{X}_2 \\mathbf{b}_2 \\right). \\end{array} \\] Esto establece la ecuación (4.19). Modelo Reparametrizado. Para la función de regresión particionada en la ecuación (4.18), definimos \\(\\mathbf{A}= \\left( \\mathbf{X}_1^{\\prime} \\mathbf{X}_1 \\right)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{X}_2\\) y \\(\\mathbf{E}_2 = \\mathbf{X}_2 - \\mathbf{X}_1 \\mathbf{A}\\). Si se realizara una regresión “multivariada” usando \\(\\mathbf{X}_2\\) como la respuesta y \\(\\mathbf{X}_1\\) como variables explicativas, entonces las estimaciones de parámetros serían \\(\\mathbf{A}\\) y los residuos \\(\\mathbf{E}_2\\). Con estas definiciones, use la ecuación (4.18) para definir el modelo de regresión reparametrizado \\[ \\begin{array}{ll} \\mathbf{y} &amp; = \\mathbf{X}_1 \\boldsymbol \\beta_1 + \\mathbf{X}_2 \\boldsymbol \\beta_2 + \\boldsymbol \\varepsilon = \\mathbf{X}_1 \\boldsymbol \\beta_1 + (\\mathbf{E}_2 + \\mathbf{X}_1 \\mathbf{A})\\boldsymbol \\beta_2 + \\boldsymbol \\varepsilon \\\\ &amp; = \\mathbf{X}_1 \\boldsymbol \\alpha_1 + \\mathbf{E}_2 \\boldsymbol \\beta_2 + \\boldsymbol \\varepsilon, \\end{array} \\tag{4.22} \\] donde \\(\\boldsymbol \\alpha_1 = \\boldsymbol \\beta_1 + \\mathbf{A}\\boldsymbol \\beta_2\\) es un nuevo vector de parámetros. La razón para introducir esta nueva parametrización es que ahora el vector de variables explicativas es ortogonal a las otras variables explicativas, es decir, el álgebra directa muestra que \\(\\mathbf{X}_1^{\\prime }\\mathbf{E}_2=\\mathbf{0}\\). Según la ecuación (4.19), el vector de estimaciones de mínimos cuadrados es \\[ \\begin{array}{ll} \\mathbf{a} = \\begin{bmatrix} \\mathbf{a}_1 \\\\ \\mathbf{b}_2 \\end{bmatrix} = \\left( \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{E}_2^{\\prime} \\end{bmatrix} \\begin{bmatrix} \\mathbf{X}_1 &amp; \\mathbf{E}_2 \\end{bmatrix} \\right) ^{-1} \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{E}_2^{\\prime} \\end{bmatrix} \\mathbf{y} = \\begin{bmatrix} \\left( \\mathbf{X}_1^{\\prime} \\mathbf{X}_1 \\right)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y} \\\\ \\left( \\mathbf{E}_2^{\\prime}\\mathbf{E}_2\\right) ^{-1} \\mathbf{E}_2^{\\prime}\\mathbf{y} \\end{bmatrix}. \\end{array} \\tag{4.23} \\] Suma Extra de Cuadrados. Supongamos que queremos considerar el aumento en la suma de cuadrados del error al pasar de un modelo reducido \\[ \\mathbf{y} = \\mathbf{X}_1 \\boldsymbol \\beta_1 + \\boldsymbol \\varepsilon \\] a un modelo completo \\[ \\mathbf{y} = \\mathbf{X}_1 \\boldsymbol \\beta_1 + \\mathbf{X}_2 \\boldsymbol \\beta_2 + \\boldsymbol \\varepsilon. \\] Para el modelo reducido, la suma de cuadrados del error es \\[ (Error ~ SS)_{reduced} = \\mathbf{y}^{\\prime} \\mathbf{y} - \\mathbf{y}^{\\prime} \\mathbf{X}_1 (\\mathbf{X}_1^{\\prime} \\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y}. \\tag{4.24} \\] Usando la versión reparametrizada del modelo completo, la suma de cuadrados del error es \\[ \\begin{array}{ll} (Error ~ SS)_{full} &amp;= \\mathbf{y}^{\\prime} \\mathbf{y} - \\mathbf{a}^{\\prime} \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{E}_2^{\\prime} \\end{bmatrix} \\mathbf{y} = \\mathbf{y}^{\\prime}\\mathbf{y} - \\begin{bmatrix} \\left( \\mathbf{X}_1^{\\prime} \\mathbf{X}_1 \\right)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y} \\\\ \\left( \\mathbf{E}_2^{\\prime}\\mathbf{E}_2\\right)^{-1} \\mathbf{E}_2^{\\prime} \\mathbf{y} \\end{bmatrix}^{\\prime} \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\mathbf{y} \\\\ \\mathbf{E}_2^{\\prime}\\mathbf{y} \\end{bmatrix} \\\\ &amp;= \\mathbf{y}^{\\prime} \\mathbf{y} - \\mathbf{y}^{\\prime} \\mathbf{X}_1 (\\mathbf{X}_1^{\\prime} \\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y} - \\mathbf{y}^{\\prime} \\mathbf{E}_2 (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1} \\mathbf{E}_2^{\\prime} \\mathbf{y}. \\tag{4.25} \\end{array} \\] Así, la reducción en la suma de cuadrados del error al agregar \\(\\mathbf{X}_2\\) al modelo es \\[ (Error ~SS)_{reduced} - (Error ~SS)_{full} = \\mathbf{y}^{\\prime} \\mathbf{E}_2 (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1} \\mathbf{E}_2^{\\prime} \\mathbf{y}. \\tag{4.26} \\] Como se mencionó en la Sección 4.3, la cantidad \\((Error ~SS)_{reduced} - (Error ~SS)_{full}\\) se llama suma extra de cuadrados, o suma de cuadrados Tipo III. Esto se produce automáticamente por algunos paquetes de software estadístico, evitando así la necesidad de ejecutar regresiones separadas. 4.7.3 Modelo Lineal General Recuerda el modelo lineal general de la Sección 4.4. Es decir, usamos \\[ y_i = \\beta_0 x_{i0} + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik} + \\varepsilon_i, \\] o, en notación de matrices, \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta + \\boldsymbol \\varepsilon\\). Como antes, utilizamos las Suposiciones F1-F4 (o E1-E4) para que los términos de perturbación sean i.i.d. con media cero y varianza común \\(\\sigma^2\\), y las variables explicativas \\(\\{x_{i0},x_{i1},x_{i2},\\ldots,x_{ik}\\}\\) sean no estocásticas. En el modelo lineal general, no requerimos que \\(\\mathbf{X}^{\\prime}\\mathbf{X}\\) sea invertible. Como hemos visto en el Capítulo 4, una razón importante para esta generalización se relaciona con el manejo de variables categóricas. Es decir, para usar variables categóricas, generalmente se recodifican utilizando variables binarias. Para esta recodificación, generalmente se deben hacer algunos tipos de restricciones en el conjunto de parámetros asociados con las variables indicadoras. Sin embargo, no siempre está claro qué tipo de restricciones son las más intuitivas. Al expresar el modelo sin requerir que \\(\\mathbf{X}^{\\prime}\\mathbf{X}\\) sea invertible, las restricciones pueden imponerse después de que se realice la estimación, no antes. Ecuaciones Normales. Incluso cuando \\(\\mathbf{X}^{\\prime}\\mathbf{X}\\) no es invertible, las soluciones a las ecuaciones normales aún proporcionan estimaciones de mínimos cuadrados de \\(\\boldsymbol \\beta\\). Es decir, la suma de cuadrados es \\[ SS(\\mathbf{b}^{\\ast}) = \\mathbf{(y - Xb}^{\\ast}\\mathbf{)}^{\\prime}\\mathbf{(y - Xb}^{\\ast}\\mathbf{)}, \\] donde \\(\\mathbf{b}^{\\ast} = (b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast})^{\\prime}\\) es un vector de estimaciones candidatas. Las soluciones de las ecuaciones normales son aquellos vectores \\(\\mathbf{b}^{\\circ }\\) que satisfacen las ecuaciones normales \\[ \\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\circ } = \\mathbf{X}^{\\prime}\\mathbf{y}. \\tag{4.27} \\] Usamos la notación \\(^{\\circ }\\) para recordarnos que \\(\\mathbf{b}^{\\circ }\\) no tiene que ser único. Sin embargo, es un minimizador de la suma de cuadrados. Para ver esto, considera otro vector candidato \\(\\mathbf{b}^{\\ast}\\) y nota que \\[ SS(\\mathbf{b}^{\\ast}) = \\mathbf{y}^{\\prime}\\mathbf{y} - 2\\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{y} + \\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\ast}. \\] Luego, usando la ecuación (4.27), tenemos \\[ SS(\\mathbf{b}^{\\ast}) - SS(\\mathbf{b}^{\\circ }) = -2\\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{y} + \\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\ast} - (-2\\mathbf{b}^{\\circ \\prime }\\mathbf{Xy} + \\mathbf{b}^{\\circ \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\circ }) \\] \\[ = -2\\mathbf{b}^{\\ast \\prime }\\mathbf{Xb}^{\\circ } + \\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\ast} + \\mathbf{b}^{\\circ \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\circ } \\] \\[ = \\mathbf{(b}^{\\ast} - \\mathbf{b}^{\\circ})^{\\prime} \\mathbf{X}^{\\prime} \\mathbf{X} (\\mathbf{b}^{\\ast} - \\mathbf{b}^{\\circ}) = \\mathbf{z}^{\\prime} \\mathbf{z} \\geq 0, \\] donde \\(\\mathbf{z} = \\mathbf{X} (\\mathbf{b}^{\\ast} - \\mathbf{b}^{\\circ})\\). Así, cualquier otro candidato \\(\\mathbf{b}^{\\ast}\\) produce una suma de cuadrados al menos tan grande como \\(SS(\\mathbf{b}^{\\circ})\\). Valores Ajustados Únicos A pesar de que puede haber (infinitas) soluciones a las ecuaciones normales, los valores ajustados resultantes, \\(\\mathbf{\\hat{y}} = \\mathbf{Xb}^{\\circ}\\), son únicos. Para ver esto, supongamos que \\(\\mathbf{b}_1^{\\circ}\\) y \\(\\mathbf{b}_2^{\\circ}\\) son dos soluciones diferentes de la ecuación (4.27). Sea \\(\\mathbf{\\hat{y}}_1 = \\mathbf{Xb}_1^{\\circ}\\) y \\(\\mathbf{\\hat{y}}_2 = \\mathbf{Xb}_2^{\\circ}\\) los vectores de valores ajustados generados por estas estimaciones. Entonces, \\[ \\mathbf{(\\hat{y}}_1 - \\mathbf{\\hat{y}}_2)^{\\prime} (\\mathbf{\\hat{y}}_1 - \\mathbf{\\hat{y}}_2) = \\mathbf{(b}_1^{\\circ} - \\mathbf{b}_2^{\\circ})^{\\prime} \\mathbf{X}^{\\prime} \\mathbf{X} (\\mathbf{b}_1^{\\circ} - \\mathbf{b}_2^{\\circ}) = 0 \\] porque \\(\\mathbf{X}^{\\prime} \\mathbf{X} (\\mathbf{b}_1^{\\circ} - \\mathbf{b}_2^{\\circ}) = \\mathbf{X}^{\\prime} \\mathbf{y} - \\mathbf{X}^{\\prime} \\mathbf{y} = \\mathbf{0}\\), de la ecuación (4.27). Por lo tanto, tenemos que \\(\\mathbf{\\hat{y}}_1 = \\mathbf{\\hat{y}}_2\\) para cualquier elección de \\(\\mathbf{b}_1^{\\circ}\\) y \\(\\mathbf{b}_2^{\\circ}\\), estableciendo así la unicidad de los valores ajustados. Debido a que los valores ajustados son únicos, los residuos también son únicos. Por lo tanto, la suma de cuadrados de los errores y las estimaciones de la variabilidad (como \\(s^2\\)) también son únicas. Inversas Generalizadas Una inversa generalizada de una matriz \\(\\mathbf{A}\\) es una matriz \\(\\mathbf{B}\\) tal que \\(\\mathbf{ABA = A}\\). Usamos la notación \\(\\mathbf{A}^{\\mathbf{-}}\\) para denotar la inversa generalizada de \\(\\mathbf{A}\\). En el caso de que \\(\\mathbf{A}\\) sea invertible, entonces \\(\\mathbf{A}^{\\mathbf{-}}\\) es única e igual a \\(\\mathbf{A}^{\\mathbf{-1}}\\). Aunque existen varias definiciones de inversas generalizadas, la definición anterior es suficiente para nuestros propósitos. Ver Searle (1987) para una discusión más profunda sobre definiciones alternativas de inversas generalizadas. Con esta definición, se puede mostrar que una solución a la ecuación \\(\\mathbf{Ab = c}\\) puede expresarse como \\(\\mathbf{b = A}^{-} \\mathbf{c}\\). Así, podemos expresar una estimación de mínimos cuadrados de \\(\\boldsymbol \\beta\\) como \\(\\mathbf{b}^{\\circ} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-} \\mathbf{X}^{\\prime} \\mathbf{y}\\). Los paquetes de software estadístico pueden calcular versiones de \\((\\mathbf{X}^{\\prime} \\mathbf{X})^{-}\\) y así generar \\(\\mathbf{b}^{\\circ}\\). Funciones Estimables Anteriormente, vimos que cada valor ajustado \\(\\hat{y}_i\\) es único. Dado que los valores ajustados son simplemente combinaciones lineales de estimaciones de parámetros, parece razonable preguntar qué otras combinaciones lineales de estimaciones de parámetros son únicas. Con este fin, decimos que \\(\\mathbf{C \\boldsymbol \\beta}\\) es una función estimable de los parámetros si \\(\\mathbf{Cb}^{\\circ}\\) no depende (es invariante) de la elección de \\(\\mathbf{b}^{\\circ}\\). Debido a que los valores ajustados son invariantes a la elección de \\(\\mathbf{b}^{\\circ}\\), tenemos que \\(\\mathbf{C = X}\\) produce un tipo de función estimable. Curiosamente, resulta que todas las funciones estimables son de la forma \\(\\mathbf{LXb}^{\\circ}\\), es decir, \\(\\mathbf{C} = \\mathbf{LX}\\). Ver Searle (1987, página 284) para una demostración de esto. Así, todas las funciones estimables son combinaciones lineales de valores ajustados, es decir, \\(\\mathbf{LXb}^{\\circ} = \\mathbf{L\\hat{y}}\\). Las funciones estimables son insesgadas y tienen una varianza que no depende de la elección de la inversa generalizada. Es decir, se puede mostrar que \\(\\text{E } \\mathbf{Cb}^{\\circ} = \\mathbf{C \\boldsymbol \\beta}\\) y \\(\\text{Var } \\mathbf{Cb}^{\\circ} = \\sigma^2 \\mathbf{C(X}^{\\prime} \\mathbf{X})^{-} \\mathbf{C}^{\\prime}\\) no depende de la elección de \\(\\mathbf{(X}^{\\prime} \\mathbf{X})^{-}\\). Hipótesis Comprobables Como se describe en la Sección 4.2, a menudo interesa probar \\(H_0\\): \\(\\mathbf{C \\boldsymbol \\beta} = \\mathbf{d}\\), donde \\(\\mathbf{d}\\) es un vector especificado. Esta hipótesis se dice que es comprobable si \\(\\mathbf{C \\boldsymbol \\beta}\\) es una función estimable, \\(\\mathbf{C}\\) es de rango completo en filas, y el rango de \\(\\mathbf{C}\\) es menor que el rango de \\(\\mathbf{X}\\). Para ser consistentes con la notación de la Sección 4.2, sea \\(p\\) el rango de \\(\\mathbf{C}\\) y \\(k + 1\\) el rango de \\(\\mathbf{X}\\). Recordemos que el rango de una matriz es el menor entre el número de filas linealmente independientes y el número de columnas linealmente independientes. Cuando decimos que \\(\\mathbf{C}\\) tiene rango completo en filas, nos referimos a que hay \\(p\\) filas en \\(\\mathbf{C}\\), de modo que el número de filas es igual al rango. Hipótesis Lineal General Como en la Sección 4.2, la estadística de prueba para examinar \\(H_0\\): \\(\\mathbf{C \\boldsymbol \\beta} = \\mathbf{d}\\) es \\[ F\\text{-ratio} = \\frac{\\mathbf{(Cb}^{\\circ} - \\mathbf{d})^{\\prime} \\mathbf{(C(X}^{\\prime} \\mathbf{X})^{-} \\mathbf{C}^{\\prime})^{-1} \\mathbf{(Cb}^{\\circ} - \\mathbf{d})}{ps_{full}^2}. \\] Note que la estadística \\(F\\)-ratio no depende de la elección de \\(\\mathbf{b}^{\\circ}\\) porque \\(\\mathbf{C b}^{\\circ}\\) es invariante a \\(\\mathbf{b}^{\\circ}\\). Si \\(H_0\\): \\(\\mathbf{C \\boldsymbol \\beta} = \\mathbf{d}\\) es una hipótesis comprobable y los errores \\(\\varepsilon_i\\) son i.i.d. \\(\\text{N}(0, \\sigma^2)\\), entonces el \\(F\\)-ratio tiene una distribución \\(F\\) con \\(df_1 = p\\) y \\(df_2 = n - (k + 1)\\). Modelo de Una Variable Categórica Ahora ilustramos el modelo lineal general considerando una versión sobre-parametr izada del modelo de un factor que aparece en la ecuación (4.10) usando \\[ y_{ij} = \\mu + \\tau_j + e_{ij} = \\mu + \\tau_1 x_{i1} + \\tau_2 x_{i2} + \\ldots + \\tau_c x_{ic} + \\varepsilon_{ij}. \\] En este punto, no imponemos restricciones adicionales en los parámetros. Como en la ecuación (4.13), esto se puede escribir en forma matricial como \\[ \\mathbf{y} = \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdots &amp; \\mathbf{0}_1 \\\\ \\mathbf{1}_2 &amp; \\mathbf{0}_2 &amp; \\mathbf{1}_{\\mathbf{2}} &amp; \\cdots &amp; \\mathbf{0}_2 \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\mathbf{1}_c &amp; \\mathbf{0}_c &amp; \\mathbf{0}_c &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix} \\begin{bmatrix} \\mu \\\\ \\tau_1 \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\tau_c \\end{bmatrix} + \\boldsymbol \\varepsilon = \\mathbf{X \\boldsymbol \\beta + \\boldsymbol \\varepsilon}. \\] Así, la matriz \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\) es \\[ \\mathbf{X}^{\\prime} \\mathbf{X} = \\begin{bmatrix} n &amp; n_1 &amp; n_2 &amp; \\cdots &amp; n_c \\\\ n_1 &amp; n_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ n_2 &amp; 0 &amp; n_2 &amp; \\cdots &amp; 0 \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ n_c &amp; 0 &amp; 0 &amp; \\cdots &amp; n_c \\end{bmatrix}. \\] donde \\(n = n_1 + n_2 + \\ldots + n_{c}\\). Esta matriz no es invertible. Para ver esto, note que al sumar las últimas \\(c\\) filas juntas se obtiene la primera fila. Por lo tanto, las últimas \\(c\\) filas son una combinación lineal exacta de la primera fila, lo que significa que la matriz no tiene rango completo. Las estimaciones (no únicas) de mínimos cuadrados se pueden expresar como \\[ \\mathbf{b}^{\\circ} = \\begin{bmatrix} \\mu^{\\circ} \\\\ \\tau_1^{\\circ} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\tau_c^{\\circ} \\end{bmatrix} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-} \\mathbf{X}^{\\prime} \\mathbf{y}. \\] Las funciones estimables son combinaciones lineales de los valores ajustados. Dado que los valores ajustados son \\(\\hat{y}_{ij} = \\bar{y}_j\\), las funciones estimables pueden expresarse como \\(L = \\sum_{j=1}^{c} a_j \\bar{y}_j\\) donde \\(a_1, \\ldots, a_{c}\\) son constantes. Esta combinación lineal de los valores ajustados es un estimador insesgado de \\(\\text{E } L = \\sum_{i=1}^{c} a_i (\\mu + \\tau_i)\\). Por lo tanto, por ejemplo, eligiendo \\(a_1 = 1\\) y los otros \\(a_i = 0\\), vemos que \\(\\mu + \\tau_1\\) es estimable. Como otro ejemplo, eligiendo \\(a_1 = 1\\), \\(a_2 = -1\\), y los otros \\(a_i = 0\\), vemos que \\(\\tau_1 - \\tau_2\\) es estimable. Se puede demostrar que \\(\\mu\\) no es un parámetro estimable sin restricciones adicionales sobre \\(\\tau_1, \\ldots, \\tau_c\\). "],["C5VarSelect.html", "Capítulo 5 Selección de Variables 5.1 Un Enfoque Iterativo para el Análisis de Datos y Modelado 5.2 Procedimientos Automáticos de Selección de Variables 5.3 Análisis de Residuales 5.4 Puntos Influyentes 5.5 Colinealidad 5.6 Criterios de Selección 5.7 Heterocedasticidad 5.8 Lectura Adicional y Referencias 5.9 Ejercicios 5.10 Suplementos Técnicos para el Capítulo 5", " Capítulo 5 Selección de Variables Vista previa del capítulo. Este capítulo describe herramientas y técnicas para ayudar a seleccionar las variables a incluir en un modelo de regresión lineal, comenzando con un proceso iterativo de selección de modelos. En aplicaciones con muchas variables explicativas potenciales, los procedimientos automáticos de selección de variables ayudan a evaluar rápidamente muchos modelos. Sin embargo, los procedimientos automáticos tienen serias limitaciones, incluida la incapacidad de manejar adecuadamente las no linealidades como el impacto de puntos inusuales; este capítulo amplía la discusión del Capítulo 2 sobre puntos inusuales. También se describe la colinealidad, una característica común de los datos de regresión donde las variables explicativas están linealmente relacionadas entre sí. Otros temas que afectan la selección de variables, como la heterocedasticidad y la validación fuera de muestra, también se introducen. 5.1 Un Enfoque Iterativo para el Análisis de Datos y Modelado En nuestra introducción a la regresión lineal básica en el Capítulo 2, examinamos los datos gráficamente, formulamos una hipótesis sobre la estructura del modelo y comparamos los datos con un modelo candidato para formular un modelo mejorado. Box (1980) describe esto como un proceso iterativo, que se muestra en la Figura 5.1. Figura 5.1: El proceso iterativo de especificación del modelo Este proceso iterativo proporciona una receta útil para estructurar la tarea de especificar un modelo que represente un conjunto de datos. El primer paso, la etapa de formulación del modelo, se realiza examinando los datos gráficamente y utilizando el conocimiento previo de las relaciones, como de la teoría económica o de la práctica estándar de la industria. El segundo paso en la iteración se basa en los supuestos del modelo especificado. Estos supuestos deben ser consistentes con los datos para hacer un uso válido del modelo. El tercer paso, verificación diagnóstica, también se conoce como crítica de datos y modelo; los datos y el modelo deben ser consistentes entre sí antes de que se puedan hacer inferencias adicionales. La verificación diagnóstica es una parte importante de la formulación del modelo; puede revelar errores cometidos en pasos anteriores y proporcionar formas de corregir estos errores. 5.2 Procedimientos Automáticos de Selección de Variables Las relaciones en negocios y economía son complicadas; típicamente hay muchas variables que podrían servir como predictores útiles de la variable dependiente. Al buscar una relación adecuada, hay una gran cantidad de modelos potenciales que se basan en combinaciones lineales de variables explicativas y un número infinito de modelos que pueden formarse a partir de combinaciones no lineales. Para buscar entre los modelos basados en combinaciones lineales, existen varios procedimientos automáticos para seleccionar las variables que se incluirán en el modelo. Estos procedimientos automáticos son fáciles de usar y sugerirán uno o más modelos que se pueden explorar con mayor detalle. Para ilustrar cuán grande es el número potencial de modelos lineales, supongamos que solo hay cuatro variables, \\(x_{1}, x_2, x_3\\) y \\(x_4\\), bajo consideración para ajustar un modelo a \\(y\\). Sin considerar la multiplicación u otras combinaciones no lineales de las variables explicativas, ¿cuántos modelos posibles hay? La Tabla 5.1 muestra que la respuesta es 16. Tabla 5.1: Dieciséis Modelos Posibles Expression Combinations Models E \\(y=\\beta_0\\) 1 modelo sin variables independientes E \\(y=\\beta_0+\\beta_1x_i\\) \\(i\\) = 1,2,3,4 4 modelos con una variable independiente E \\(y = \\beta_0 + \\beta_1 x_i + \\beta_2 x_j\\) (\\(i,j\\)) = (1,2),(1,3),(1,4),(2,3),(2,4),(3,4) 6 modelos con dos variables independientes E \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_j +\\beta_3x_{k}\\) (\\(i,j,k\\)) = (1,2,3),(1,2,4),(1,3,4),(2,3,4) 4 modelos con tres variables independientes E \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +\\beta_3 x_3 + \\beta_4 x_4\\) 1 modelo con todas las variables independientes Si solo hubiera tres variables explicativas, entonces se puede usar la misma lógica para verificar que hay ocho modelos posibles. Extrapolando a partir de estos dos ejemplos, ¿cuántos modelos lineales habrá si hay diez variables explicativas? La respuesta es 1,024, lo cual es bastante. En general, la respuesta es \\(2^k\\), donde \\(k\\) es el número de variables explicativas. Por ejemplo, \\(2^3\\) es 8, \\(2^4\\) es 16, y así sucesivamente. En cualquier caso, para un número moderadamente grande de variables explicativas, hay muchos modelos potenciales que se basan en combinaciones lineales de variables explicativas. Nos gustaría tener un procedimiento para buscar rápidamente entre estos modelos potenciales y darnos más tiempo para pensar en otros aspectos interesantes de la selección de modelos. La regresión por pasos son procedimientos que emplean pruebas \\(t\\) para verificar la “significancia” de las variables explicativas que se incluyen o eliminan del modelo. Para comenzar, en la versión de selección hacia adelante de la regresión por pasos, las variables se agregan una a la vez. En la primera etapa, de todas las variables candidatas, se agrega al modelo la que es más estadísticamente significativa. En la siguiente etapa, con la variable de la primera etapa ya incluida, se agrega la siguiente variable más estadísticamente significativa. Este procedimiento se repite hasta que se hayan agregado todas las variables estadísticamente significativas. Aquí, la significancia estadística generalmente se evalúa utilizando el cociente \\(t\\) de una variable; el umbral para la significancia estadística es típicamente un valor \\(t\\) predefinido (como dos, que corresponde a un nivel de significancia aproximado del 95%). La versión de selección hacia atrás funciona de manera similar, excepto que todas las variables se incluyen en la etapa inicial y luego se eliminan una a la vez (en lugar de agregarse). Más generalmente, un algoritmo que agrega y elimina variables en cada etapa a veces se conoce como el algoritmo de regresión por pasos. Algoritmo de Regresión por Pasos. Suponga que el analista ha identificado una variable como la respuesta, \\(y\\), y \\(k\\) variables explicativas potenciales, \\(x_1, x_2, \\ldots, x_k\\). Considere todas las regresiones posibles usando una variable explicativa. Para cada una de las \\(k\\) regresiones, calcule \\(t(b_1)\\), el cociente \\(t\\) para la pendiente. Elija la variable con el cociente \\(t\\) más grande. Si el cociente \\(t\\) no supera un valor \\(t\\) predefinido (como dos), entonces no elija ninguna variable y detenga el procedimiento. Agregue una variable al modelo del paso anterior. La variable a ingresar es la que hace la contribución más significativa. Para determinar el tamaño de la contribución, use el valor absoluto del cociente \\(t\\) de la variable. Para ingresar, el cociente \\(t\\) debe superar un valor \\(t\\) especificado en valor absoluto. Elimine una variable del modelo del paso anterior. La variable a eliminar es la que hace la menor contribución. Para determinar el tamaño de la contribución, use el valor absoluto del cociente \\(t\\) de la variable. Para ser eliminada, el cociente \\(t\\) debe ser menor que un valor \\(t\\) especificado en valor absoluto. Repita los pasos (ii) y (iii) hasta que se realicen todas las posibles adiciones y eliminaciones. Al implementar esta rutina, algunos paquetes de software estadístico usan una prueba \\(F\\) en lugar de pruebas \\(t\\). Recuerde que, cuando solo se considera una variable, \\((t\\text{-cociente})^2 = F\\)-cociente, y por lo tanto, estos procedimientos son equivalentes. Este algoritmo es útil porque busca rápidamente entre varios modelos candidatos. Sin embargo, presenta varias desventajas: El procedimiento “husmea” entre un gran número de modelos y puede ajustar los datos “demasiado bien.” No hay garantía de que el modelo seleccionado sea el mejor. El algoritmo no considera modelos que se basan en combinaciones no lineales de variables explicativas. También ignora la presencia de valores atípicos y puntos de alta influencia. Además, el algoritmo no busca todos los \\(2^{k}\\) regresiones lineales posibles. El algoritmo utiliza un criterio, un cociente \\(t\\), y no considera otros criterios como \\(s\\), \\(R^2\\), \\(R_a^2\\), y así sucesivamente. Hay una secuencia de pruebas de significancia involucradas. Por lo tanto, el nivel de significancia que determina el valor \\(t\\) no es significativo. Al considerar cada variable por separado, el algoritmo no toma en cuenta el efecto conjunto de las variables explicativas. Los procedimientos puramente automáticos pueden no tener en cuenta el conocimiento especial de un investigador. Muchas de las críticas al algoritmo básico de regresión paso a paso pueden abordarse con software de computación moderno que ahora está ampliamente disponible. Ahora consideraremos cada inconveniente, en orden inverso. Para responder a la desventaja número (7), muchas rutinas de software estadístico tienen opciones para forzar la inclusión de variables en una ecuación de modelo. De esta manera, si otras evidencias indican que una o más variables deben incluirse en el modelo, el investigador puede forzar la inclusión de estas variables. Para la desventaja número (6), en la Sección 5.5.4 sobre variables supresoras, proporcionaremos ejemplos de variables que no tienen efectos individuales importantes pero son importantes cuando se consideran en conjunto. Estas combinaciones de variables pueden no ser detectadas con el algoritmo básico, pero serán detectadas con el algoritmo de selección hacia atrás. Dado que el procedimiento de selección hacia atrás comienza con todas las variables, detectará y conservará las variables que son importantes en conjunto. La desventaja número (5) es realmente una sugerencia sobre la forma de utilizar la regresión paso a paso. Bendel y Afifi (1977) sugirieron usar un valor de corte más pequeño del que normalmente se usaría. Por ejemplo, en lugar de usar un \\(t\\)-valor = 2 que corresponde aproximadamente a un nivel de significancia del 5%, considere usar un \\(t\\)-valor = 1.645 que corresponde aproximadamente a un nivel de significancia del 10%. De esta manera, hay menos posibilidad de excluir variables que pueden ser importantes. Un límite inferior, pero aún una buena opción para trabajo exploratorio, es un corte tan pequeño como \\(t\\)-valor = 1. Esta elección está motivada por un resultado algebraico: cuando una variable entra en un modelo, \\(s\\) disminuirá si el \\(t\\)-ratio excede uno en valor absoluto. Para abordar las desventajas número (3) y (4), ahora introducimos la rutina de mejores regresiones. Las mejores regresiones es un algoritmo útil que ahora está ampliamente disponible en paquetes de software estadístico. El algoritmo de mejor regresión busca en todas las combinaciones posibles de variables explicativas, a diferencia de la regresión paso a paso, que agrega y elimina una variable a la vez. Por ejemplo, suponga que hay cuatro posibles variables explicativas, \\(x_1\\), \\(x_2\\), \\(x_3\\) y \\(x_4\\), y el usuario desea saber cuál es el mejor modelo de dos variables. El algoritmo de mejor regresión busca entre los seis modelos de la forma \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x_i + \\beta_2 x_j\\). Típicamente, una rutina de mejor regresión recomienda uno o dos modelos para cada modelo con coeficiente \\(p\\), donde p es un número especificado por el usuario. Debido a que se ha especificado el número de coeficientes que entrarán en el modelo, no importa qué criterio usemos: \\(R^2\\), \\(R_a^2\\) o \\(s\\). El algoritmo de mejor regresión realiza su búsqueda mediante un uso ingenioso del hecho algebraico de que, cuando se añade una variable al modelo, la suma de cuadrados del error no aumenta. Debido a este hecho, ciertas combinaciones de variables incluidas en el modelo no necesitan ser calculadas. Un inconveniente importante de este algoritmo es que puede tomar mucho tiempo cuando el número de variables consideradas es grande. Los usuarios de la regresión no siempre aprecian la profundidad del inconveniente número (1), data-snooping (exploración de datos). La exploración de datos ocurre cuando el analista ajusta un gran número de modelos a un conjunto de datos. Abordaremos el problema de la exploración de datos en la Sección 5.6.2 sobre validación de modelos. Aquí, ilustraremos el efecto de la exploración de datos en la regresión paso a paso. Ejemplo: Exploración de Datos en Regresión Paso a Paso. La idea de esta ilustración es de Rencher y Pun (1980). Considere \\(n = 100\\) observaciones de \\(y\\) y cincuenta variables explicativas, \\(x_1, x_2, \\ldots, x_{50}\\). Los datos que consideramos aquí se simularon usando variables aleatorias normales estándar independientes. Debido a que las variables se simularon de manera independiente, estamos trabajando bajo la hipótesis nula de que no hay relación entre la respuesta y las variables explicativas, es decir, \\(H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_{50} = 0\\). De hecho, cuando se ajustó el modelo con las cincuenta variables explicativas, resultó que \\(s = 1.142\\), \\(R^2 = 46.2\\%\\), y el \\(F\\)-ratio = \\(\\frac{Regression~MS}{Error~MS} = 0.84\\). Usando una distribución \\(F\\) con \\(df_1 = 50\\) y \\(df_2 = 49\\), el percentil 95 es 1.604. De hecho, 0.84 es el percentil 27 de esta distribución, lo que indica que el valor \\(p\\) es 0.73. Por lo tanto, como era de esperar, los datos están en congruencia con \\(H_0\\). A continuación, se realizó una regresión paso a paso con \\(t\\)-valor = 2. Dos variables fueron retenidas por este procedimiento, lo que resultó en un modelo con \\(s = 1.05\\), \\(R^2 = 9.5\\%\\) y \\(F\\)-ratio = 5.09. Para una distribución \\(F\\) con \\(df_1 = 2\\) y \\(df_2 = 97\\), el percentil 95 es un \\(F\\)-valor = 3.09. Esto indica que las dos variables son predictores estadísticamente significativos de \\(y\\). A primera vista, este resultado es sorprendente. Los datos se generaron de manera que \\(y\\) no estuviera relacionado con las variables explicativas. Sin embargo, debido a que \\(F\\)-ratio \\(&gt;\\) \\(F\\)-valor, la prueba \\(F\\) indica que dos variables explicativas están significativamente relacionadas con \\(y\\). La razón es que la regresión paso a paso ha realizado muchas pruebas de hipótesis en los datos. Por ejemplo, en el Paso 1, se realizaron cincuenta pruebas para encontrar variables significativas. Recuerde que un nivel del 5% significa que esperamos cometer aproximadamente un error en 20. Por lo tanto, con cincuenta pruebas, esperamos encontrar \\(50 \\times 0.05 = 2.5\\) variables “significativas”, incluso bajo la hipótesis nula de que no hay relación entre \\(y\\) y las variables explicativas. Para continuar, se realizó una regresión paso a paso con \\(t\\)-valor = 1.645. Seis variables fueron retenidas por este procedimiento, lo que resultó en un modelo con \\(s = 0.99\\), \\(R^2 = 22.9\\%\\) y \\(F\\)-ratio = 4.61. Como antes, una prueba \\(F\\) indica una relación significativa entre la respuesta y estas seis variables explicativas. Para resumir, utilizando simulación, construimos un conjunto de datos de manera que las variables explicativas no tuvieran relación con la respuesta. Sin embargo, al utilizar la regresión paso a paso para examinar los datos, “encontramos” relaciones aparentemente significativas entre la respuesta y ciertos subconjuntos de las variables explicativas. Este ejemplo ilustra una advertencia general en la selección de modelos: cuando las variables explicativas se seleccionan utilizando los datos, los \\(t\\)-ratios y los \\(F\\)-ratios serán demasiado grandes, exagerando así la importancia de las variables en el modelo. La regresión paso a paso y las mejores regresiones son ejemplos de procedimientos automáticos de selección de variables. En su trabajo de modelado, encontrará que estos procedimientos son útiles porque pueden buscar rápidamente entre varios modelos candidatos. Sin embargo, estos procedimientos ignoran alternativas no lineales, así como el efecto de los valores atípicos y los puntos de alta influencia. El objetivo principal de estos procedimientos es mecanizar ciertas tareas rutinarias. Este enfoque de selección automática se puede extender, y de hecho, hay varios “sistemas expertos” disponibles en el mercado. Por ejemplo, hay algoritmos disponibles que manejan “automáticamente” puntos inusuales como valores atípicos y puntos de alta influencia. Un modelo sugerido por los procedimientos automáticos de selección de variables debe estar sujeto a los mismos procedimientos cuidadosos de verificación diagnóstica que un modelo obtenido por cualquier otro medio. 5.3 Análisis de Residuales Recuerde el papel de un residual en el modelo de regresión lineal introducido en la Sección 2.6. Un residual es una respuesta menos el valor ajustado correspondiente bajo el modelo. Dado que el modelo resume el efecto lineal de varias variables explicativas, podemos pensar en un residual como una respuesta controlada por los valores de las variables explicativas. Si el modelo es una representación adecuada de los datos, entonces los residuales deberían aproximarse a errores aleatorios. Los errores aleatorios se utilizan para representar la variación natural en el modelo; representan el resultado de un mecanismo impredecible. Por lo tanto, en la medida en que los residuales se parezcan a errores aleatorios, no debería haber patrones discernibles en los residuales. Los patrones en los residuales indican la presencia de información adicional que esperamos incorporar en el modelo. La ausencia de patrones en los residuales indica que el modelo parece explicar las relaciones principales en los datos. 5.3.1 Residuales Hay al menos cuatro tipos de patrones que pueden descubrirse a través del análisis de residuales. En esta sección, discutimos los dos primeros: residuales que son inusuales y aquellos que están relacionados con otras variables explicativas. Luego introducimos el tercer tipo, residuales que muestran un patrón heterocedástico, en la Sección 5.7. En nuestro estudio de datos de series temporales que comienza en el Capítulo 7, introduciremos el cuarto tipo, residuales que muestran patrones a lo largo del tiempo. Al examinar los residuales, generalmente es más fácil trabajar con un residual estandarizado, un residual que ha sido reescalado para no tener dimensiones. Generalmente trabajamos con residuales estandarizados porque así logramos transferir cierta experiencia de un conjunto de datos a otro y podemos enfocarnos en relaciones de interés. Al usar residuales estandarizados, podemos entrenarnos para observar una variedad de gráficos de residuales y reconocer inmediatamente un punto inusual al trabajar en unidades estándar. Hay varias formas de definir un residual estandarizado. Usando \\(e_i = y_i - \\hat{y}_i\\) como el \\(i\\)-ésimo residual, aquí hay tres definiciones comúnmente usadas: \\[\\begin{equation} \\text{(a) }\\frac{e_i}{s}, \\quad \\text{(b) }\\frac{e_i}{s\\sqrt{1 - h_{ii}}}, \\quad \\text{(c) }\\frac{e_i}{s_{(i)}\\sqrt{1 - h_{ii}}}. \\tag{5.1} \\end{equation}\\] Aquí, \\(h_{ii}\\) es la influencia del \\(i\\)-ésimo punto. Se calcula en función de los valores de las variables explicativas y se definirá en la Sección 5.4.1. Recuerde que \\(s\\) es la desviación estándar de los residuales (definida en la ecuación 3.8). De manera similar, definimos \\(s_{(i)}\\) como la desviación estándar de los residuales al ejecutar una regresión después de eliminar la \\(i\\)-ésima observación. Ahora, la primera definición en (a) es simple y fácil de explicar. Un cálculo simple muestra que la desviación estándar de la muestra de los residuales es aproximadamente \\(s\\) (una razón por la que \\(s\\) a menudo se denomina desviación estándar de los residuales). Por lo tanto, parece razonable estandarizar los residuales dividiendo por \\(s\\). La segunda opción presentada en (b), aunque más compleja, es más precisa. La varianza del \\(i\\)-ésimo residual es \\[ \\text{Var}(e_i) = \\sigma^2(1 - h_{ii}). \\] Este resultado se establecerá en la ecuación (5.15) de la Sección 5.10. Tenga en cuenta que esta varianza es menor que la varianza del término de error, Var\\((\\varepsilon_i) = \\sigma^2\\). Ahora, podemos reemplazar \\(\\sigma\\) por su estimación, \\(s\\). Entonces, este resultado lleva a usar la cantidad \\(s\\sqrt{1 - h_{ii}}\\) como una desviación estándar estimada, o error estándar, para \\(e_i\\). Por lo tanto, definimos el error estándar de \\(e_i\\) como \\[ \\text{se}(e_i) = s \\sqrt{1 - h_{ii}}. \\] Siguiendo las convenciones introducidas en la Sección 2.6, en este texto usamos \\(e_i / \\text{se}(e_i)\\) como nuestro residual estandarizado. La tercera opción presentada en (c) es una modificación de (b) y se conoce como un residual studentizado. Como se enfatiza en la Sección 5.3.2, un uso importante de los residuales es identificar respuestas inusualmente grandes. Ahora, supongamos que la \\(i\\)-ésima respuesta es inusualmente grande y que esto se mide a través de su residual. Este residual inusualmente grande también hará que el valor de \\(s\\) sea grande. Debido a que el efecto grande aparece tanto en el numerador como en el denominador, el residual estandarizado puede no detectar esta respuesta inusual. Sin embargo, esta respuesta grande no inflará \\(s_{(i)}\\) porque se construye después de eliminar la \\(i\\)-ésima observación. Por lo tanto, al usar residuales studentizados, obtenemos una mejor medida de las observaciones que tienen residuales inusualmente grandes. Al omitir esta observación de la estimación de \\(\\sigma\\), el tamaño de la observación solo afecta al numerador \\(e_i\\) y no al denominador \\(s_{(i)}\\). Como otra ventaja, los residuales studentizados siguen una distribución \\(t\\) con \\(n - (k + 1)\\) grados de libertad, asumiendo que los errores están distribuidos normalmente (suposición E5). Este conocimiento de la distribución precisa nos ayuda a evaluar el grado de ajuste del modelo y es particularmente útil en muestras pequeñas. Es esta relación con la distribución \\(t\\) de “Student” la que sugiere el nombre de “residuales studentizados”. 5.3.2 Uso de los Residuales para Identificar Valores Atípicos Una función importante del análisis de residuales es identificar valores atípicos. Un valor atípico es una observación que no se ajusta bien al modelo; son observaciones donde el residual es inusualmente grande. Una regla general utilizada por muchos paquetes estadísticos es que una observación se marca como un valor atípico si el residual estandarizado excede dos en valor absoluto. En la medida en que la distribución de los residuales estandarizados imite la curva normal estándar, esperamos que solo una de cada 20 observaciones, o el 95%, exceda dos en valor absoluto y muy pocas observaciones excedan tres. Los valores atípicos proporcionan una señal de que una observación debe investigarse para entender las causas especiales asociadas con este punto. Un valor atípico es una observación que parece inusual con respecto al resto del conjunto de datos. A menudo sucede que la razón de este comportamiento atípico puede descubrirse después de una investigación adicional. De hecho, este puede ser el propósito principal del análisis de regresión de un conjunto de datos. Consideremos un ejemplo simple de lo que se llama análisis de desempeño. Supongamos que tenemos disponible una muestra de \\(n\\) vendedores y estamos tratando de entender las ventas de cada persona en el segundo año en función de sus ventas en el primer año. Hasta cierto punto, esperamos que las ventas más altas en el primer año estén asociadas con ventas más altas en el segundo año. Las altas ventas pueden deberse a la habilidad natural del vendedor, ambición, buen territorio, etc. Las ventas del primer año pueden considerarse como una variable proxy que resume estos factores. Esperamos variación en el desempeño de ventas tanto de manera transversal como a lo largo de los años. Es interesante cuando un vendedor tiene un desempeño inusualmente bueno (o malo) en el segundo año en comparación con su desempeño en el primer año. Los residuales proporcionan un mecanismo formal para evaluar las ventas del segundo año después de controlar los efectos de las ventas del primer año. Hay varias opciones disponibles para manejar valores atípicos. Opciones para Manejar Valores Atípicos Incluir la observación en las estadísticas resumen habituales pero comentar sobre sus efectos. Un valor atípico puede ser grande pero no tan grande como para sesgar los resultados de todo el análisis. Si no se pueden determinar causas especiales para esta observación inusual, entonces esta observación puede simplemente reflejar la variabilidad en los datos. Eliminar la observación del conjunto de datos. Puede determinarse que la observación no es representativa de la población de la cual se extrae la muestra. Si este es el caso, entonces puede haber poca información contenida en la observación que pueda usarse para hacer afirmaciones generales sobre la población. Esta opción implica que omitiríamos la observación de las estadísticas resumen de la regresión y la discutiríamos en nuestro informe como un caso separado. Crear una variable binaria para indicar la presencia de un valor atípico. Si se han identificado una o varias causas especiales para explicar un valor atípico, entonces estas causas podrían introducirse formalmente en el procedimiento de modelado mediante la introducción de una variable que indique la presencia (o ausencia) de estas causas. Este enfoque es similar a la eliminación de puntos, pero permite que el valor atípico se incluya formalmente en la formulación del modelo, de modo que, si surgen observaciones adicionales afectadas por las mismas causas, se puedan manejar de forma automática. 5.3.3 Uso de los Residuales para Seleccionar Variables Explicativas Otra función importante del análisis de residuales es ayudar a identificar variables explicativas adicionales que puedan usarse para mejorar la formulación del modelo. Si hemos especificado el modelo correctamente, entonces los residuales deberían parecerse a errores aleatorios y no contener patrones discernibles. Por lo tanto, al comparar residuales con variables explicativas, no esperamos ninguna relación. Si detectamos una relación, esto sugiere la necesidad de controlar esta variable adicional. Esto se puede lograr introduciendo la variable adicional en el modelo de regresión. Las relaciones entre los residuales y las variables explicativas pueden establecerse rápidamente utilizando estadísticas de correlación. Sin embargo, si una variable explicativa ya está incluida en el modelo de regresión, entonces la correlación entre los residuales y una variable explicativa será cero (ver Sección 5.10.1 para la demostración algebraica). Es una buena idea reforzar esta correlación con un diagrama de dispersión. Un gráfico de residuales frente a variables explicativas no solo reforzará gráficamente la estadística de correlación, sino que también servirá para detectar posibles relaciones no lineales. Por ejemplo, una relación cuadrática puede detectarse utilizando un diagrama de dispersión, no una estadística de correlación. Si detecta una relación entre los residuales de un ajuste de modelo preliminar y una variable explicativa adicional, introducir esta variable adicional no siempre mejorará la especificación de su modelo. La razón es que la variable adicional puede estar relacionada linealmente con las variables que ya están en el modelo. Si desea una garantía de que agregar una variable adicional mejorará su modelo, entonces construya un gráfico de variables añadidas (ver Sección 3.4.3). En resumen, después de un ajuste preliminar del modelo, debe: Calcular estadísticas resumen y mostrar la distribución de los residuales (estandarizados) para identificar valores atípicos. Calcular la correlación entre los residuales (estandarizados) y las variables explicativas adicionales para buscar relaciones lineales. Crear gráficos de dispersión entre los residuales (estandarizados) y las variables explicativas adicionales para buscar relaciones no lineales. Ejemplo: Liquidez del Mercado de Valores. La decisión de un inversor de comprar una acción generalmente se toma teniendo en cuenta varios criterios. Primero, los inversores suelen buscar un alto rendimiento esperado. Un segundo criterio es el riesgo de una acción, que puede medirse mediante la variabilidad de los rendimientos. Tercero, muchos inversores están preocupados por el tiempo que están comprometiendo su capital con la compra de un valor. Muchas acciones de ingresos, como las de servicios públicos, devuelven regularmente partes de las inversiones de capital en forma de dividendos. Otras acciones, particularmente las de crecimiento, no devuelven nada hasta la venta del valor. Por lo tanto, la duración promedio de la inversión en un valor es otro criterio. Cuarto, a los inversores les preocupa la capacidad de vender la acción en cualquier momento que sea conveniente para ellos. Nos referimos a este cuarto criterio como la liquidez de la acción. Cuanto más líquida sea la acción, más fácil será venderla. Para medir la liquidez, en este estudio utilizamos el número de acciones negociadas en una bolsa durante un período de tiempo específico (llamado VOLUME). Estamos interesados en estudiar la relación entre el volumen y otras características financieras de una acción. Comenzamos este estudio con 126 empresas cuyas opciones se negociaron el 3 de diciembre de 1984. Los datos de las acciones fueron obtenidos de Francis Emory Fitch, Inc. para el período del 3 de diciembre de 1984 al 28 de febrero de 1985. Para las variables de actividad comercial, examinamos: El volumen total de negociación de tres meses (VOLUME, en millones de acciones), El número total de transacciones de tres meses (NTRAN), y El tiempo promedio entre transacciones (AVGT, medido en minutos). Para las variables de tamaño de la empresa, utilizamos: El precio de apertura de la acción el 2 de enero de 1985 (PRICE), El número de acciones en circulación el 31 de diciembre de 1984 (SHARE, en millones de acciones), y El valor de mercado del capital (VALUE, en miles de millones de dólares) obtenido al tomar el producto de PRICE y SHARE. Finalmente, para el apalancamiento financiero, examinamos la relación deuda-capital (DEB_EQ) obtenida de la Cinta Industrial de Compustat y el manual de Moody’s. Los datos en SHARE se obtienen de la cinta mensual del Centro de Investigación en Precios de Seguridad (CRSP). Después de examinar algunas estadísticas resumen preliminares de los datos, se eliminaron tres empresas porque tenían un volumen inusualmente alto o un precio elevado. Estas son Teledyne y Capital Cities Communication, cuyos precios eran más de cuatro veces el precio promedio de las demás empresas, y American Telephone and Telegraph, cuyo volumen total era más de siete veces el volumen total promedio de las demás empresas. Basado en una investigación adicional, cuyos detalles no se presentan aquí, estas empresas fueron eliminadas porque parecían representar circunstancias especiales que no deseábamos modelar. La Tabla 5.2 resume las estadísticas descriptivas basadas en las \\(n = 123\\) empresas restantes. Por ejemplo, en la Tabla 5.2, vemos que el tiempo promedio entre transacciones es de aproximadamente cinco minutos y este tiempo varía desde un mínimo de menos de 1 minuto hasta un máximo de aproximadamente 20 minutos. Tabla 5.2: Estadísticas Resumen de las Variables de Liquidez de las Acciones Media Mediana Desviación Estándar Mínimo Máximo VOLUME 13.423 11.556 10.632 0.658 64.572 AVGT 5.441 4.284 3.853 0.590 20.772 NTRAN 6436.000 5071.000 5310.000 999.000 36420.000 PRICE 38.800 34.380 21.370 9.120 122.380 SHARE 94.730 53.830 115.100 6.740 783.050 VALUE 4.116 2.065 8.157 0.115 75.437 DEBEQ 2.697 1.105 6.509 0.185 53.628 Fuente: Francis Emory Fitch, Inc., Standard &amp; Poor’s Compustat, y el Centro de Investigación de Precios de Valores de la Universidad de Chicago. La Tabla 5.3 reporta los coeficientes de correlación y la Figura 5.2 proporciona la matriz de dispersión correspondiente. Si tienes conocimientos en finanzas, te resultará interesante notar que el apalancamiento financiero, medido por DEB_EQ, no parece estar relacionado con las otras variables. A partir del diagrama de dispersión y la matriz de correlación, vemos una fuerte relación entre VOLUME y el tamaño de la empresa, medido por SHARE y VALUE. Además, las tres variables de actividad de negociación, VOLUME, AVGT y NTRAN, están altamente relacionadas entre sí. Tabla 5.3: Matriz de Correlación de la Liquidez de las Acciones AVGT NTRAN PRICE SHARE VALUE DEB_EQ VOLUME AVGT 1.000 -0.668 -0.128 -0.429 -0.318 0.094 -0.674 NTRAN -0.668 1.000 0.190 0.817 0.760 -0.092 0.913 PRICE -0.128 0.190 1.000 0.177 0.457 -0.038 0.168 SHARE -0.429 0.817 0.177 1.000 0.829 -0.077 0.773 VALUE -0.318 0.760 0.457 0.829 1.000 -0.077 0.702 DEB_EQ 0.094 -0.092 -0.038 -0.077 -0.077 1.000 -0.052 VOLUME -0.674 0.913 0.168 0.773 0.702 -0.052 1.000 Código R para Producir las Tablas 5.2 y 5.3 liquidity &lt;- read.csv(&quot;CSVData/Liquidity.csv&quot;, header=TRUE) varLiquid &lt;- c(&quot;AVGT&quot;, &quot;NTRAN&quot;, &quot;PRICE&quot;, &quot;SHARE&quot;, &quot;VALUE&quot;, &quot;DEBEQ&quot;, &quot;VOLUME&quot;) liquidMat &lt;- data.frame(liquidity[varLiquid]) names(liquidMat)[names(liquidMat) == &quot;DEBEQ&quot;] &lt;- &quot;DEB_EQ&quot; # TABLA 5.2 ESTADÍSTICAS RESUMEN BookSummStats &lt;- function(Xymat){ meanSummary &lt;- sapply(Xymat, mean, na.rm=TRUE) sdSummary &lt;- sapply(Xymat, sd, na.rm=TRUE) minSummary &lt;- sapply(Xymat, min, na.rm=TRUE) maxSummary &lt;- sapply(Xymat, max, na.rm=TRUE) medSummary &lt;- sapply(Xymat, median,na.rm=TRUE) tableMat &lt;- cbind(meanSummary, medSummary, sdSummary, minSummary, maxSummary) return(tableMat) } liquidMat1 &lt;- liquidMat[,c(7,1:6)] tableMat &lt;- BookSummStats(liquidMat1) colnames(tableMat) &lt;- c(&quot;Media&quot;, &quot;Mediana&quot;, &quot;Desviación Estándar&quot;, &quot;Mínimo&quot;, &quot;Máximo&quot;) rownames(tableMat) &lt;- varLiquid[c(7,1:6)] tableMat &lt;- round(tableMat, digits = 3) tableMat[3,] &lt;- round(tableMat[3,], digits = 0) tableMat[4:5,] &lt;- round(tableMat[4:5,], digits = 2) TableGen1(TableData=tableMat, TextTitle=&#39;Estadísticas Resumen de las Variables de Liquidez de las Acciones&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:5, ColWidth = ColWidth5) cor_matrix &lt;- cor(liquidMat) rownames(cor_matrix) &lt;- colnames(cor_matrix) &lt;- c(&quot;AVGT&quot;, &quot;NTRAN&quot;, &quot;PRICE&quot;, &quot;SHARE&quot;, &quot;VALUE&quot;, &quot;DEB_EQ&quot;, &quot;VOLUME&quot;) TableGen1(TableData=cor_matrix, TextTitle=&#39;Matriz de Correlación de la Liquidez de las Acciones&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:6, ColWidth = ColWidth6) La Figura 5.2 muestra que la variable AVGT está inversamente relacionada con VOLUME y NTRAN está inversamente relacionada con AVGT. De hecho, resultó que la correlación entre el tiempo promedio entre transacciones y el recíproco del número de transacciones fue del \\(99.98\\%!\\) Esto no es tan sorprendente cuando se piensa en cómo se podría calcular AVGT. Por ejemplo, en la Bolsa de Valores de Nueva York, el mercado está abierto de 10:00 A.M. a 4:00 P.M. Para cada acción en un día particular, el tiempo promedio entre transacciones multiplicado por el número de transacciones es casi igual a 360 minutos (= 6 horas). Por lo tanto, excepto por errores de redondeo porque las transacciones solo se registran al minuto más cercano, hay una relación lineal perfecta entre AVGT y el recíproco de NTRAN. Figura 5.2: Matriz de dispersión para las variables de liquidez de las acciones. La variable del número de transacciones (NTRAN) parece estar fuertemente relacionada con el VOLUME de acciones negociadas e inversamente relacionada con AVGT. Para comenzar a entender la medida de liquidez VOLUME, primero ajustamos un modelo de regresión utilizando NTRAN como una variable explicativa. El modelo de regresión ajustado es: \\[ \\small{ \\begin{array}{lcc} \\text{VOLUME} &amp;= 1.65 &amp;+ 0.00183 \\text{ NTRAN} \\\\ \\text{errores estándar} &amp; (0.6173) &amp; (0.000074) \\end{array} } \\] con \\(R^2 = 83.4\\%\\) y \\(s = 4.35\\). Note que el cociente \\(t\\) para la pendiente asociada con NTRAN es \\[ t(b_1) = \\frac{b_1}{se(b_1)} = \\frac{0.00183}{0.000074} = 24.7 \\] indicando una fuerte significancia estadística. Los residuos se calcularon utilizando este modelo estimado. Para ver si los residuos están relacionados con otras variables explicativas, la Tabla 5.4 muestra las correlaciones. Tabla 5.4: Primera Tabla de Correlaciones Variable AVGT PRICE SHARE VALUE DEB_EQ RESID -0.159 -0.014 0.064 0.018 0.078 Nota: Los residuos se crearon a partir de una regresión de VOLUME sobre NTRAN. La correlación entre el residuo y AVGT y el diagrama de dispersión (no mostrado aquí) indica que puede haber alguna información en la variable AVGT en el residuo. Por lo tanto, parece razonable usar AVGT directamente en el modelo de regresión. Recuerde que estamos interpretando el residuo como el valor de VOLUME habiendo controlado el efecto de NTRAN. A continuación, ajustamos un modelo de regresión utilizando NTRAN y AVGT como variables explicativas. El modelo de regresión ajustado es: \\[ \\small{ \\begin{array}{lccc} \\text{VOLUME} &amp;= 4.41 &amp;- 0.322 \\text{ AVGT} &amp;+ 0.00167 \\text{ NTRAN} \\\\ \\text{errores estándar} &amp; (1.30)&amp; (0.135)&amp; (0.000098) \\end{array} } \\] con \\(R^2 = 84.2\\%\\) y \\(s = 4.26\\). Basado en el cociente \\(t\\) para AVGT, \\(t(b_{AVGT}) = \\frac{-0.322}{0.135} = -2.39\\), parece que AVGT es una variable explicativa útil en el modelo. Note también que \\(s\\) ha disminuido, lo que indica que \\(R_a^2\\) ha aumentado. La Tabla 5.5 proporciona correlaciones entre los residuos del modelo y otras posibles variables explicativas e indica que no parece haber mucha información adicional en las variables explicativas. Esto se reafirma por la tabla correspondiente de diagramas de dispersión en la Figura 5.3. Los histogramas en la Figura 5.3 sugieren que, aunque la distribución de los residuos es bastante simétrica, la distribución de cada variable explicativa está sesgada. Debido a esto, se exploraron transformaciones de las variables explicativas. Esta línea de pensamiento no proporcionó mejoras reales y, por lo tanto, no se proporcionan detalles aquí. Figura 5.3: Matriz de dispersión de los residuos de la regresión de VOLUME sobre NTRAN y AVGT en el eje vertical y las variables predictoras restantes en los ejes horizontales. Tabla 5.5: Segunda Tabla de Correlaciones Variable PRICE SHARE VALUE DEB_EQ RESID -0.015 0.100 0.074 0.089 Nota: Los residuos se crearon a partir de una regresión de VOLUME sobre NTRAN y AVGT. 5.4 Puntos Influyentes No todos los puntos son creados iguales; en esta sección veremos que ciertas observaciones pueden tener un efecto desproporcionado en el ajuste general de la regresión. A estos puntos los llamaremos “influyentes.” Esto no es tan sorprendente; ya hemos visto que las estimaciones de los coeficientes de regresión son sumas ponderadas de respuestas (ver Sección 3.2.4). Algunas observaciones tienen pesos mayores que otras y, por lo tanto, tienen una mayor influencia en las estimaciones de los coeficientes de regresión. Por supuesto, el hecho de que una observación sea influyente no significa que sea incorrecta o que su impacto en el modelo sea engañoso. Como analistas, simplemente nos gustaría saber si nuestro modelo ajustado es sensible a cambios leves, como la eliminación de un solo punto, para sentirnos cómodos al generalizar nuestros resultados de la muestra a una población más grande. Para evaluar la influencia, pensamos en observaciones como respuestas inusuales, dadas un conjunto de variables explicativas, o que tienen un conjunto inusual de valores de variables explicativas. Ya hemos visto en la Sección 5.3 cómo evaluar respuestas inusuales utilizando residuos. Esta sección se centra en conjuntos inusuales de valores de variables explicativas. 5.4.1 Apalancamiento Introdujimos este tema en la Sección 2.6, donde llamamos a una observación con una variable explicativa inusual un “punto de alto apalancamiento.” Con más de una variable explicativa, determinar si una observación es un punto de alto apalancamiento no es tan sencillo. Por ejemplo, es posible que una observación “no sea inusual” para ninguna variable individual y, sin embargo, sea inusual en el espacio de variables explicativas. Considere el conjunto de datos ficticio representado en la Figura 5.4. Visualmente, parece claro que el punto marcado en la esquina superior derecha es inusual. Sin embargo, no es inusual cuando se examina el histograma de \\(x_1\\) o de \\(x_2\\). Es inusual solo cuando se consideran las variables explicativas de manera conjunta. Figura 5.4: El elipsoide representa la mayoría de los datos. La flecha marca un punto inusual. Para dos variables explicativas, esto es evidente al examinar los datos gráficamente. Debido a que es difícil examinar gráficamente los datos con más de dos variables explicativas, necesitamos un procedimiento numérico para evaluar el apalancamiento. Para definir el concepto de apalancamiento en la regresión lineal múltiple, utilizamos algunos conceptos de álgebra matricial. Específicamente, en la Sección 3.1 mostramos que el vector de coeficientes de regresión de mínimos cuadrados se puede calcular usando \\(\\mathbf{b} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}\\). Así, podemos expresar el vector de valores ajustados \\(\\hat{\\mathbf{y}} = (\\hat{y}_1, \\ldots, \\hat{y}_n)^{\\prime}\\) como \\[\\begin{equation} \\mathbf{\\hat{y}} = \\mathbf{Xb} . \\tag{5.2} \\end{equation}\\] De manera similar, el vector de residuos es el vector de respuesta menos el vector de valores ajustados, es decir, \\(\\mathbf{e} = \\mathbf{y - \\hat{y}}\\). A partir de la expresión para los coeficientes de regresión \\(\\mathbf{b}\\) en la ecuación (3.4), tenemos \\[ \\mathbf{\\hat{y}} = \\mathbf{X} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{X}^{\\prime} \\mathbf{y} \\] Esta ecuación sugiere definir \\[ \\mathbf{H} = \\mathbf{X} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{X}^{\\prime} \\] de modo que \\[ \\mathbf{\\hat{y}} = \\mathbf{Hy} \\] A partir de esto, se dice que la matriz \\(\\mathbf{H}\\) proyecta el vector de respuestas \\(\\mathbf{y}\\) en el vector de valores ajustados \\(\\mathbf{\\hat{y}}\\). Alternativamente, puede pensar en \\(\\mathbf{H}\\) como la matriz que pone el “sombrero,” o circunflejo, en \\(\\mathbf{y}\\). A partir de la \\(i\\)-ésima fila de la ecuación vectorial \\(\\mathbf{\\hat{y}} = \\mathbf{Hy}\\), tenemos \\[ \\hat{y}_i = h_{i1} y_1 + h_{i2} y_2 + \\cdots + h_{ii} y_i + \\cdots + h_{in} y_n \\] Aquí, \\(h_{ij}\\) es el número en la \\(i\\)-ésima fila y \\(j\\)-ésima columna de \\(\\mathbf{H}\\). A partir de esta expresión, vemos que cuanto mayor sea \\(h_{ii}\\), mayor será el efecto que la \\(i\\)-ésima respuesta \\((y_i)\\) tiene en el valor ajustado correspondiente \\((\\hat{y}_i)\\). Por lo tanto, llamamos a \\(h_{ii}\\) el apalancamiento para la \\(i\\)-ésima observación. Debido a que \\(h_{ii}\\) es el elemento diagonal \\(i\\)-ésimo de \\(\\mathbf{H}\\), una expresión directa para \\(h_{ii}\\) es \\[\\begin{equation} h_{ii} = \\mathbf{x}_i^{\\prime} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{x}_i \\tag{5.3} \\end{equation}\\] donde \\(\\mathbf{x}_i = (x_{i0}, x_{i1}, \\ldots, x_{ik})^{\\prime}\\). Debido a que los valores de \\(h_{ii}\\) se calculan en base a las variables explicativas, los valores de la variable de respuesta no afectan el cálculo de los apalancamientos. Los valores altos de apalancamiento indican que una observación puede tener un efecto desproporcionado en el ajuste, esencialmente porque está distante de las otras observaciones (al observar el espacio de variables explicativas). ¿Qué tan grande es grande? Existen algunas pautas de álgebra matricial, donde tenemos que \\[ \\frac{1}{n} \\leq h_{ii} \\leq 1 \\] y \\[ \\bar{h} = \\frac{1}{n} \\sum_{i=1}^{n} h_{ii} = \\frac{k+1}{n}. \\] Por lo tanto, cada apalancamiento está limitado por \\(n^{-1}\\) y \\(1\\), y el apalancamiento promedio es igual al número de coeficientes de regresión dividido por el número de observaciones. A partir de estos y argumentos relacionados, utilizamos una convención ampliamente adoptada y declaramos que una observación es un punto de alto apalancamiento si el apalancamiento supera tres veces el promedio, es decir, si \\[ h_{ii} &gt; \\frac{3(k+1)}{n}. \\] Una vez identificados los puntos de alto apalancamiento, al igual que con los valores atípicos, es importante que el analista busque causas especiales que puedan haber producido estos puntos inusuales. Para ilustrar, en la Sección 2.7 identificamos el colapso del mercado de 1987 como la razón detrás del punto de alto apalancamiento. Además, los puntos de alto apalancamiento a menudo se deben a errores administrativos al codificar los datos, que pueden o no ser fáciles de rectificar. En general, las opciones para manejar puntos de alto apalancamiento son similares a las disponibles para tratar con valores atípicos. Opciones para Manejar Puntos de Alto Apalancamiento Incluir la observación en las estadísticas resumidas pero comentar sobre su efecto. Por ejemplo, una observación puede apenas superar un límite y su efecto puede no ser importante en el análisis general. Eliminar la observación del conjunto de datos. Nuevamente, la justificación básica para esta acción es que se considera que la observación no es representativa de una población más grande. Una opción intermedia entre (1) y (2) es presentar el análisis tanto con como sin el punto de alto apalancamiento. De esta manera, se demuestra completamente el impacto del punto y el lector de su análisis puede decidir cuál opción es más adecuada. Elegir otra variable para representar la información. En algunos casos, otra variable explicativa estará disponible para servir como reemplazo. Por ejemplo, en un ejemplo de alquileres de apartamentos, podríamos usar el número de habitaciones para reemplazar una variable de metros cuadrados como medida del tamaño del apartamento. Aunque los metros cuadrados de un apartamento pueden ser inusualmente grandes, lo que lo convierte en un punto de alto apalancamiento, puede tener una, dos o tres habitaciones, dependiendo de la muestra examinada. Usar una transformación no lineal de una variable explicativa. Para ilustrar, con nuestro ejemplo de Liquidez de Acciones en la Sección 5.5.3, podemos transformar la variable continua de razón deuda a capital DEB_EQ en una variable que indique la presencia de “alta” razón deuda a capital. Por ejemplo, podríamos codificar DE_IND = 1 si DEB_EQ &gt; 5 y DE_IND = 0 si DEB_EQ ≤ 5. Con esta recodificación, aún conservamos información sobre el apalancamiento financiero de una empresa sin permitir que los valores grandes de DEB_EQ influyan en el ajuste de la regresión. Algunos analistas usan metodologías de estimación “robustas” como alternativa a la estimación de mínimos cuadrados. La idea básica de estas técnicas es reducir el efecto de cualquier observación en particular. Estas técnicas son útiles para reducir el efecto tanto de valores atípicos como de puntos de alto apalancamiento. Esta táctica puede considerarse intermedia entre un procedimiento extremo, ignorando el efecto de puntos inusuales, y otro extremo, dando plena credibilidad a los puntos inusuales al eliminarlos del conjunto de datos. La palabra robusto sugiere que estas metodologías de estimación son “saludables” incluso cuando son atacadas por una observación ocasionalmente mala (un germen). Hemos visto que esto no es cierto para la estimación de mínimos cuadrados. 5.4.2 Distancia de Cook Para cuantificar la influencia de un punto, una medida que considera tanto las variables de respuesta como las explicativas es la Distancia de Cook. Esta distancia, \\(D_i\\), se define como \\[\\begin{equation} \\begin{array}{ll} D_i &amp;= \\frac{\\sum_{j=1}^{n} (\\hat{y}_j - \\hat{y}_{j(i)})^2}{(k+1) s^2} \\tag{5.4} \\\\ &amp;= \\left( \\frac{e_i}{se(e_i)} \\right)^2 \\frac{h_{ii}}{(k+1)(1 - h_{ii})}. \\end{array} \\end{equation}\\] La primera expresión proporciona una definición. Aquí, \\(\\hat{y}_{j(i)}\\) es la predicción de la \\(j\\)-ésima observación, calculada excluyendo la \\(i\\)-ésima observación del ajuste de regresión. Para medir el impacto de la \\(i\\)-ésima observación, comparamos los valores ajustados con y sin la \\(i\\)-ésima observación. Cada diferencia se eleva al cuadrado y se suma en todas las observaciones para resumir el impacto. La segunda ecuación proporciona otra interpretación de la distancia \\(D_i\\). La primera parte, \\(\\left( \\frac{e_i}{se(e_i)} \\right)^2\\), es el cuadrado del residuo estandarizado \\(i\\)-ésimo. La segunda parte, \\(\\frac{h_{ii}}{(k+1)(1 - h_{ii})}\\), se atribuye únicamente al apalancamiento. Así, la distancia \\(D_i\\) se compone de una medida para valores atípicos multiplicada por una medida de apalancamiento. De esta manera, la distancia de Cook tiene en cuenta tanto las variables de respuesta como las explicativas. La Sección 5.10.3 establece la validez de la ecuación (5.4). Para tener una idea del tamaño esperado de \\(D_i\\) para un punto que no es inusual, recuerde que esperamos que los residuos estandarizados sean aproximadamente uno y que el apalancamiento \\(h_{ii}\\) sea aproximadamente \\(\\frac{k+1}{n}\\). Por lo tanto, anticipamos que \\(D_i\\) debería ser aproximadamente \\(\\frac{1}{n}\\). Otra regla general es comparar \\(D_i\\) con una distribución \\(F\\) con \\(df_1 = k+1\\) y \\(df_2 = n - (k+1)\\) grados de libertad. Los valores de \\(D_i\\) que son grandes en comparación con esta distribución merecen atención. Ejemplo: Valores Atípicos y Puntos de Alto Apalancamiento - Continuación. Para ilustrar, volvemos a nuestro ejemplo de la Sección 2.6. En este ejemplo, consideramos 19 puntos “buenos” o base, más cada uno de los tres tipos de puntos inusuales, etiquetados como A, B y C. La Tabla 5.6 resume los cálculos. Tabla 5.6: Medidas de Tres Tipos de Puntos Inusuales Observación Residuo Estandarizado \\(e / se(e)\\) Leverage \\(h\\) Distancia de Cook \\(D\\) A 4.00 0.067 0.577 B 0.77 0.550 0.363 C -4.01 0.550 9.832 Como se mencionó en la Sección 2.6, de la columna de residuos estandarizados vemos que tanto los puntos A como C son valores atípicos. Para juzgar el tamaño de los apalancamientos, dado que hay \\(n=20\\) puntos, los apalancamientos están limitados por 0.05 y 1.00, con el apalancamiento promedio siendo \\(\\bar{h} = \\frac{2}{20} = 0.10\\). Usando 0.3 (\\(= 3 \\times \\bar{h}\\)) como un umbral, tanto los puntos B como C son puntos de alto apalancamiento. Nótese que sus valores son los mismos. Esto se debe a que, según la Figura 2.7, los valores de las variables explicativas son los mismos y solo la variable de respuesta ha cambiado. La columna de la distancia de Cook captura ambos tipos de comportamiento inusual. Dado que el valor típico de \\(D_i\\) es \\(\\frac{1}{n}\\) o 0.05, la distancia de Cook proporciona una estadística para alertarnos de que cada punto es inusual en un aspecto u otro. En particular, el punto C tiene un \\(D_i\\) muy grande, lo que refleja el hecho de que es tanto un valor atípico como un punto de alto apalancamiento. El percentil 95 de una distribución \\(F\\) con \\(df_1 = 2\\) y \\(df_2 = 18\\) es 3.555. El hecho de que el punto C tenga un valor de \\(D_i\\) que supera con creces este umbral indica la influencia sustancial de este punto. 5.5 Colinealidad 5.5.1 ¿Qué es la Colinealidad? Colinealidad, o multicolinealidad, ocurre cuando una variable explicativa es, o casi es, una combinación lineal de las otras variables explicativas. Intuitivamente, con datos colineales, es útil pensar en las variables explicativas como altamente correlacionadas entre sí. Si una variable explicativa es colineal, surge la pregunta de si es redundante, es decir, si la variable proporciona poca información adicional sobre la información que ya está en las otras variables explicativas. Las preguntas son: ¿Es importante la colinealidad? Si es así, ¿cómo afecta el ajuste de nuestro modelo y cómo la detectamos? Para abordar la primera pregunta, considere un ejemplo algo patológico. Ejemplo: Variables Explicativas Perfectamente Correlacionadas. Joe Finance fue solicitado para ajustar el modelo \\(\\mathrm{E} ~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) a un conjunto de datos. Su modelo ajustado resultante fue \\(\\hat{y} = -87 + x_1 + 18 x_2.\\) El conjunto de datos considerado es: \\[ \\begin{array}{l|cccc} \\hline i &amp; 1 &amp; 2 &amp; 3 &amp; 4 \\\\ \\hline y_i &amp; 23 &amp; 83 &amp; 63 &amp; 103 \\\\ x_{i1} &amp; 2 &amp; 8 &amp;6 &amp; 10 \\\\ x_{i2} &amp; 6 &amp; 9 &amp; 8 &amp; 10 \\\\ \\hline \\end{array} \\] Joe verificó el ajuste para cada observación. Joe estaba muy contento porque ajustó los datos perfectamente. Por ejemplo, para la tercera observación, el valor ajustado es \\(\\hat{y}_3 = -87 + 6 + 18 \\times 8 = 63\\), que es igual a la tercera respuesta, \\(y_3\\). Debido a que la respuesta es igual al valor ajustado, el residuo es cero. Puede verificar que esto es cierto para cada observación, y así, el \\(R^2\\) resultó ser \\(100\\%\\). Sin embargo, Jane Actuary llegó y ajustó el modelo \\(\\hat{y} = -7 + 9 x_1 + 2 x_2.\\) Jane realizó las mismas comprobaciones cuidadosas que Joe hizo y también obtuvo un ajuste perfecto (\\(R^2 = 1\\)). ¿Quién tiene razón? La respuesta es ambos y ninguno. De hecho, hay un número infinito de ajustes. Esto se debe a la relación perfecta \\(x_2 = 5 + \\frac{x_1}{2}\\) entre las dos variables explicativas. Este ejemplo ilustra algunos hechos importantes sobre la colinealidad. Hechos sobre la Colinealidad La colinealidad no nos impide obtener buenos ajustes ni hacer predicciones de nuevas observaciones. Nótese que en el ejemplo anterior obtuvimos ajustes perfectos. Las estimaciones de las varianzas de error y, por lo tanto, las pruebas de adecuación del modelo, siguen siendo fiables. En casos de colinealidad severa, los errores estándar de los coeficientes de regresión individuales son mayores que en los casos en que, ceteris paribus, no existe colinealidad severa. Con errores estándar grandes, los coeficientes de regresión individuales pueden no ser significativos. Además, debido a que un error estándar grande significa que el correspondiente cociente \\(t\\) es pequeño, es difícil detectar la importancia de una variable. Para detectar la colinealidad, comience con una matriz de coeficientes de correlación de las variables explicativas. Esta matriz es fácil de crear, fácil de interpretar y captura rápidamente las relaciones lineales entre pares de variables. Una matriz de diagramas de dispersión proporciona un refuerzo visual de las estadísticas resumidas en la matriz de correlación. 5.5.2 Factores de Inflación de Varianza Las matrices de correlación y diagramas de dispersión capturan solo las relaciones entre pares de variables. Para capturar relaciones más complejas entre varias variables, introducimos el factor de inflación de varianza (VIF). Para definir un VIF, suponga que el conjunto de variables explicativas está etiquetado como \\(x_1, x_2, \\ldots, x_{k}\\). Ahora, ejecute la regresión utilizando \\(x_j\\) como la “respuesta” y los otros \\(x\\) (\\(x_1, x_2, \\ldots, x_{j-1}, x_{j+1}, \\ldots, x_{k}\\)) como las variables explicativas. Denote el coeficiente de determinación de esta regresión por \\(R_j^2\\). Interpretamos \\(R_j = \\sqrt{R_j^2}\\) como el coeficiente de correlación múltiple entre \\(x_j\\) y las combinaciones lineales de los otros \\(x\\). A partir de este coeficiente de determinación, definimos el factor de inflación de varianza \\[ VIF_j = \\frac{1}{1 - R_j^2}, \\text{ para } j = 1, 2, \\ldots, k. \\] Un mayor \\(R_j^2\\) resulta en un mayor \\(VIF_j\\); esto significa una mayor colinealidad entre \\(x_j\\) y los otros \\(x\\). Ahora, \\(R_j^2\\) por sí solo es suficiente para capturar la relación lineal de interés. Sin embargo, usamos \\(VIF_j\\) en lugar de \\(R_j^2\\) como nuestra medida de colinealidad debido a la relación algebraica \\[\\begin{equation} se(b_j) = s \\frac{\\sqrt{VIF_j}}{s_{x_j} \\sqrt{n - 1}}. \\tag{5.5} \\end{equation}\\] Aquí, \\(se(b_j)\\) y \\(s\\) son errores estándar y la desviación estándar residual de un ajuste completo de regresión de \\(y\\) sobre \\(x_1, \\ldots, x_{k}\\). Además, \\(s_{x_j} = \\sqrt{(n - 1)^{-1} \\sum_{i=1}^{n} (x_{ij} - \\bar{x}_j)^2 }\\) es la desviación estándar muestral de la \\(j\\)-ésima variable \\(x_j\\). La Sección 5.10.3 proporciona una verificación de la ecuación (5.5). Así, un mayor \\(VIF_j\\) resulta en un mayor error estándar asociado con la pendiente \\(j\\)-ésima, \\(b_j\\). Recuerde que \\(se(b_j)\\) es \\(s\\) veces la raíz cuadrada del \\((j+1)\\)-ésimo elemento diagonal de \\((\\mathbf{X}^{\\prime} \\mathbf{X})^{-1}\\). La idea es que cuando ocurre colinealidad, la matriz \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\) tiene propiedades similares al número cero. Cuando intentamos calcular la inversa de \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\), esto es análogo a dividir por cero en números escalares. Como regla general, cuando \\(VIF_j\\) supera 10 (lo cual es equivalente a \\(R_j^2 &gt; 90\\%\\)), decimos que existe colinealidad severa. Esto puede indicar la necesidad de acción. Tolerancia, definida como el recíproco del factor de inflación de varianza, es otra medida de colinealidad utilizada por algunos analistas. Por ejemplo, con \\(k = 2\\) variables explicativas en el modelo, entonces \\(R_1^2\\) es la correlación cuadrada entre las dos variables explicativas, digamos \\(r_{12}^2\\). Entonces, a partir de la ecuación anterior, tenemos que \\[ se(b_j) = s \\left(s_{x_j} \\sqrt{n - 1} \\right)^{-1} \\left(1 - r_{12}^2 \\right)^{-1/2}, \\text{ para } j = 1, 2. \\] A medida que la correlación se acerca a uno en valor absoluto, \\(|r_{12}| \\rightarrow 1\\), entonces el error estándar se vuelve grande, lo que significa que el estadístico \\(t\\) correspondiente se vuelve pequeño. En resumen, un alto \\(VIF\\) puede significar pequeños estadísticos \\(t\\) a pesar de que las variables sean importantes. Además, se puede verificar que la correlación entre \\(b_1\\) y \\(b_2\\) es \\(-r_{12}\\), indicando que las estimaciones de los coeficientes están altamente correlacionadas. Ejemplo: Liquidez del Mercado de Valores - Continuación. Como ejemplo, considere una regresión de VOLUME sobre PRICE, SHARE y VALUE. A diferencia de las variables explicativas consideradas en la Sección 5.5.3, estas tres variables explicativas no son medidas de actividad de trading. A partir de un ajuste de regresión, tenemos \\(R^2 = 61\\%\\) y \\(s = 6.72\\). Las estadísticas asociadas con los coeficientes de regresión están en la Tabla 5.7. Tabla 5.7: Estadísticas de una regresión de VOLUME sobre PRICE, SHARE y VALUE \\(x_j\\) \\(s_{x_j}\\) \\(b_j\\) \\(se(b_j)\\) \\(t(b_j)\\) \\(VIF_j\\) PRICE 21.370 -0.022 0.035 -0.63 1.5 SHARE 115.100 0.054 0.010 5.19 3.8 VALUE 8.157 0.313 0.162 1.94 4.7 Puede verificar que la relación en la ecuación (5.5) es válida para cada una de las variables explicativas en la Tabla 5.7. Dado que cada estadístico \\(VIF\\) es menor a diez, hay poca razón para sospechar colinealidad severa. Esto es interesante porque puede recordar que existe una relación perfecta entre PRICE, SHARE y VALUE en el sentido de que definimos el valor de mercado como VALUE = PRICE \\(\\times\\) SHARE. Sin embargo, la relación es multiplicativa y, por lo tanto, es no lineal. Debido a que las variables no están relacionadas linealmente, es válido incluir las tres en el modelo de regresión. Desde una perspectiva financiera, la variable VALUE es importante porque mide el valor de una empresa. Desde una perspectiva estadística, la variable VALUE cuantifica la interacción entre PRICE y SHARE (las variables de interacción se introdujeron en la Sección 3.5.3). Para la colinealidad, solo nos interesa detectar tendencias lineales, por lo que las relaciones no lineales entre variables no son un problema aquí. Por ejemplo, hemos visto que a veces es útil mantener tanto una variable explicativa \\(x\\) como su cuadrado \\(x^2\\), a pesar de que existe una relación perfecta (no lineal) entre las dos. Sin embargo, debemos verificar que las relaciones no lineales no sean aproximadamente lineales en la región de muestreo. Aunque la relación es teóricamente no lineal, si es cercana a lineal para nuestra muestra disponible, pueden surgir problemas de colinealidad. La Figura 5.5 ilustra esta situación. Figura 5.5: La relación entre \\(x_1\\) y \\(x_2\\) es no lineal. Sin embargo, en la región muestreada, las variables tienen una relación casi lineal. ¿Qué podemos hacer en presencia de colinealidad? Una opción es centrar cada variable, restando su promedio y dividiendo por su desviación estándar. Por ejemplo, crear una nueva variable \\(x_{ij}^{\\ast} = (x_{ij} - \\bar{x}_j) / s_{x_j}\\). A veces, una variable aparece en millones de unidades y otra en fracciones de unidades. Comparado con la primera variable, la segunda parece ser casi una columna constante de ceros (dado que las computadoras retienen típicamente un número finito de dígitos). Si esto es cierto, entonces la segunda variable se parece mucho a un desplazamiento lineal de la columna constante de unos correspondiente al intercepto. Esto es un problema porque, con las operaciones de mínimos cuadrados, estamos implícitamente elevando al cuadrado números que pueden hacer que estas columnas parezcan aún más similares. Este problema es simplemente computacional y es fácil de corregir. Simplemente recodifique las variables para que las unidades sean de magnitud similar. Algunos analistas de datos centran automáticamente todas las variables para evitar estos problemas. Este es un enfoque legítimo porque las técnicas de regresión buscan relaciones lineales; los desplazamientos en ubicación y escala no afectan las relaciones lineales. Otra opción es simplemente no tener en cuenta explícitamente la colinealidad en el análisis, pero discutir algunas de sus implicaciones al interpretar los resultados del análisis de regresión. Este enfoque es probablemente el más comúnmente adoptado. Es un hecho que, al tratar con datos de negocios y económicos, la colinealidad tiende a existir entre las variables. Dado que los datos tienden a ser observacionales en lugar de experimentales, hay poco que el analista pueda hacer para evitar esta situación. En la mejor de las situaciones, una variable auxiliar que proporcione información similar y que facilite el problema de colinealidad está disponible para reemplazar una variable. Similar a nuestra discusión sobre puntos de alta influencia, una versión transformada de la variable explicativa también puede ser un sustituto útil. En algunas situaciones, un reemplazo ideal no está disponible y nos vemos obligados a eliminar una o más variables. Decidir qué variables eliminar es una elección difícil. Al decidir entre variables, a menudo la elección estará dictada por el juicio del investigador sobre cuál es el conjunto de variables más relevante. 5.5.3 Colinealidad e Influencia Las medidas de colinealidad e influencia comparten características comunes y, sin embargo, están diseñadas para capturar diferentes aspectos de un conjunto de datos. Ambas son útiles para la crítica de datos y del modelo; se aplican después de un ajuste preliminar del modelo con el objetivo de mejorar la especificación del modelo. Además, ambas se calculan utilizando solo las variables explicativas; los valores de las respuestas no entran en ninguno de los cálculos. Nuestra medida de colinealidad, el factor de inflación de la varianza, está diseñada para ayudar con la crítica del modelo. Es una medida calculada para cada variable explicativa, diseñada para explicar la relación con otras variables explicativas. La estadística de influencia está diseñada para ayudarnos con la crítica de datos. Es una medida calculada para cada observación para ayudarnos a explicar cuán inusual es una observación con respecto a otras observaciones. La colinealidad puede estar enmascarada o inducida por puntos de alta influencia, como lo señalaron Mason y Gunst (1985) y Hadi (1988). Las Figuras 5.6 y 5.7 proporcionan ilustraciones de cada caso. Estos ejemplos simples subrayan un punto importante: la crítica de datos y la crítica del modelo no son ejercicios separados. Figura 5.6: Con la excepción del punto marcado, \\(x_1\\) y \\(x_2\\) están altamente relacionados linealmente. Figura 5.7: La relación lineal altamente entre \\(x_1\\) y \\(x_2\\) es principalmente debido al punto marcado. Los ejemplos en las Figuras 5.6 y 5.7 también nos ayudan a ver una forma en que los puntos de alta influencia pueden afectar los errores estándar de los coeficientes de regresión. Recuerde que, en la Sección 5.4.1, vimos que los puntos de alta influencia pueden afectar los valores ajustados del modelo. En las Figuras 5.6 y 5.7, vemos que los puntos de alta influencia afectan la colinealidad. Por lo tanto, a partir de la ecuación (5.5), tenemos que los puntos de alta influencia también pueden afectar nuestros errores estándar de los coeficientes de regresión. 5.5.4 Variables Suprensoras Como hemos visto, la colinealidad severa puede inflar seriamente los errores estándar de los coeficientes de regresión. Dado que dependemos de estos errores estándar para evaluar la utilidad de las variables explicativas, nuestros procedimientos de selección de modelos e inferencias pueden ser deficientes en presencia de colinealidad severa. A pesar de estos inconvenientes, la colinealidad leve en un conjunto de datos no debe considerarse una deficiencia del conjunto de datos; es simplemente una característica de las variables explicativas disponibles. Incluso si una variable explicativa es casi una combinación lineal de las demás, eso no significa necesariamente que la información que proporciona sea redundante. Para ilustrar, ahora consideramos una variable suprensora, una variable explicativa que aumenta la importancia de otras variables explicativas cuando se incluye en el modelo. Ejemplo: Variable Suprensora. La Figura 5.8 muestra una matriz de dispersión de un conjunto de datos hipotético con cincuenta observaciones. Este conjunto de datos contiene una variable dependiente y dos variables explicativas. La Tabla 5.8 proporciona la matriz de coeficientes de correlación correspondiente. Aquí, vemos que las dos variables explicativas están altamente correlacionadas. Ahora recuerde que, para una regresión con una variable explicativa, el coeficiente de correlación al cuadrado es el coeficiente de determinación. Así, usando la Tabla 5.8, para una regresión de \\(y\\) sobre \\(x_1\\), el coeficiente de determinación es \\((0.188)^2 = 3.5\\%\\). De manera similar, para una regresión de \\(y\\) sobre \\(x_2\\), el coeficiente de determinación es \\((-0.022)^2 = 0.04\\%\\). Sin embargo, para una regresión de \\(y\\) sobre \\(x_1\\) y \\(x_2\\), el coeficiente de determinación resulta ser sorprendentemente alto, \\(80.7\\%\\). La interpretación es que, individualmente, tanto \\(x_1\\) como \\(x_2\\) tienen poco impacto en \\(y\\). Sin embargo, cuando se toman conjuntamente, las dos variables explicativas tienen un efecto significativo en \\(y\\). Aunque la Tabla 5.8 muestra que \\(x_1\\) y \\(x_2\\) están fuertemente relacionados linealmente, esta relación no significa que \\(x_1\\) y \\(x_2\\) proporcionen la misma información. De hecho, en este ejemplo, las dos variables se complementan entre sí. Figura 5.8: Matriz de dispersión de una variable dependiente y dos variables explicativas para el ejemplo de variable suprensora Tabla 5.8: Matriz de Correlación para el Ejemplo de Suprensor \\(x_1\\) \\(x_2\\) \\(x_2\\) 0.972 \\(y\\) 0.188 -0.022 5.5.5 Variables Ortogonales Otra forma de entender el impacto de la colinealidad es estudiar el caso en el que no hay relaciones entre conjuntos de variables explicativas. Matemáticamente, se dice que dos matrices \\(\\mathbf{X}_1\\) y \\(\\mathbf{X}_2\\) son ortogonales si \\(\\mathbf{X}_1^{\\prime} \\mathbf{X}_2 = \\mathbf{0}\\). Intuitivamente, dado que generalmente trabajamos con variables centradas (con medias cero), esto significa que cada columna de \\(\\mathbf{X}_1\\) no está correlacionada con cada columna de \\(\\mathbf{X}_2\\). Aunque es poco probable que ocurra con datos observacionales en las ciencias sociales, al diseñar tratamientos experimentales o construir polinomios de alto grado, las aplicaciones de variables ortogonales se utilizan regularmente (véase, por ejemplo, Hocking, 2003). Para nuestros propósitos, trabajaremos con variables ortogonales simplemente para entender las consecuencias lógicas de una ausencia total de colinealidad. Supongamos que \\(\\mathbf{x}_2\\) es una variable explicativa que es ortogonal a \\(\\mathbf{X}_1\\), donde \\(\\mathbf{X}_1\\) es una matriz de variables explicativas que incluye la intersección. Entonces, es sencillo comprobar que la adición de \\(\\mathbf{x}_2\\) a la ecuación de regresión no cambia el ajuste para los coeficientes correspondientes a \\(\\mathbf{X}_1\\). Es decir, sin \\(\\mathbf{x}_2\\), los coeficientes correspondientes a \\(\\mathbf{X}_1\\) se calcularían como \\(\\mathbf{b}_1 = \\left(\\mathbf{X}_1^{\\prime} \\mathbf{X}_1 \\right)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y}\\). Usar el ortogonal \\(\\mathbf{x}_2\\) como parte del cálculo de mínimos cuadrados no cambiaría el resultado para \\(\\mathbf{b}_1\\) (véase el cálculo recursivo de mínimos cuadrados en la Sección 4.7.2). Además, el factor de inflación de la varianza para \\(\\mathbf{x}_2\\) es 1, lo que indica que el error estándar no se ve afectado por las otras variables explicativas. De manera similar, la reducción en la suma de errores al agregar la variable ortogonal \\(\\mathbf{x}_2\\) se debe únicamente a esa variable, y no a su interacción con otras variables en \\(\\mathbf{X}_1\\). Las variables ortogonales pueden ser creadas para datos observacionales en ciencias sociales (así como otros datos colineales) utilizando el método de componentes principales. Con este método, se utiliza una transformación lineal de la matriz de variables explicativas de la forma, \\(\\mathbf{X}^{\\ast} = \\mathbf{X} \\mathbf{P}\\), de manera que la matriz resultante \\(\\mathbf{X}^{\\ast}\\) esté compuesta por columnas ortogonales. La función de regresión transformada es \\(\\mathrm{E~}\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta = \\mathbf{X} \\mathbf{P} \\mathbf{P}^{-1} \\boldsymbol \\beta = \\mathbf{X}^{\\ast} \\boldsymbol \\beta^{\\ast}\\), donde \\(\\boldsymbol \\beta^{\\ast} = \\mathbf{P}^{-1} \\boldsymbol \\beta\\) es el conjunto de nuevos coeficientes de regresión. La estimación procede como antes, con el conjunto ortogonal de variables explicativas. Al elegir la matriz \\(\\mathbf{P}\\) apropiadamente, cada columna de \\(\\mathbf{X}^{\\ast}\\) tiene una contribución identificable. Así, podemos usar técnicas de selección de variables para identificar las porciones de “componentes principales” de \\(\\mathbf{X}^{\\ast}\\) para usar en la ecuación de regresión. La regresión por componentes principales es un método ampliamente utilizado en algunas áreas de aplicación, como la psicología. Puede abordar fácilmente datos altamente colineales de manera disciplinada. La principal desventaja de esta técnica es que las estimaciones de parámetros resultantes son difíciles de interpretar. 5.6 Criterios de Selección 5.6.1 Bondad de Ajuste ¿Qué tan bien se ajusta el modelo a los datos? Los criterios que miden la proximidad entre el modelo ajustado y los datos reales se conocen como estadísticas de bondad de ajuste. Específicamente, interpretamos el valor ajustado \\(\\hat{y}_i\\) como la mejor aproximación del modelo para la \\(i\\)-ésima observación y lo comparamos con el valor real \\(y_i\\). En la regresión lineal, examinamos la diferencia a través del residuo \\(e_i = y_i - \\hat{y}_i\\); residuos pequeños implican un buen ajuste del modelo. Hemos cuantificado esto a través del tamaño del error típico \\((s)\\), incluyendo el coeficiente de determinación \\((R^2)\\) y una versión ajustada \\((R_{a}^2)\\). Para modelos no lineales, necesitaremos medidas adicionales, y es útil introducir estas medidas en este caso lineal más simple. Una de estas medidas es el Criterio de Información de Akaike que se definirá en términos de ajustes de verosimilitud en la Sección 11.9.4. Para la regresión lineal, se reduce a \\[\\begin{equation} AIC = n \\ln (s^2) + n \\ln (2 \\pi) + n + 3 + k. \\tag{5.6} \\end{equation}\\] Para la comparación de modelos, cuanto menor sea el \\(AIC\\), mejor es el ajuste. Comparar modelos con el mismo número de variables (\\(k\\)) significa que seleccionar un modelo con valores bajos de \\(AIC\\) lleva a la misma elección que seleccionar un modelo con valores bajos de la desviación estándar de los residuos \\(s\\). Además, un pequeño número de parámetros implica un valor bajo de \\(AIC\\), manteniéndose todo lo demás constante. La idea es que esta medida equilibra el ajuste (\\(n \\ln (s^2)\\)) con una penalización por complejidad (el número de parámetros, \\(k+2\\)). Los paquetes estadísticos a menudo omiten constantes como \\(n \\ln (2 \\pi)\\) y \\(n+3\\) al reportar \\(AIC\\) porque no importan al comparar modelos. La Sección 11.9.4 presentará otra medida, el Criterio de Información de Bayes (\\(BIC\\)), que da un peso menor a la penalización por complejidad. Una tercera medida de bondad de ajuste que se usa en modelos de regresión lineal es la estadística \\(C_p\\). Para definir esta estadística, supongamos que tenemos disponibles \\(k\\) variables explicativas \\(x_1, ..., x_{k}\\) y realizamos una regresión para obtener \\(s_{full}^2\\) como el error cuadrático medio. Ahora, supongamos que consideramos usar solo \\(p-1\\) variables explicativas de modo que haya \\(p\\) coeficientes de regresión. Con estas \\(p-1\\) variables explicativas, realizamos una regresión para obtener la suma de cuadrados del error \\((Error~SS)_p\\). Así, estamos en posición de definir \\[ C_{p} = \\frac{(Error~SS)_p}{s_{full}^2} - n + 2p. \\] Como criterio de selección, elegimos el modelo con un coeficiente \\(C_{p}\\) “pequeño”, donde pequeño se entiende en relación con \\(p\\). En general, los modelos con valores más pequeños de \\(C_{p}\\) son más deseables. Al igual que las estadísticas \\(AIC\\) y \\(BIC\\), la estadística \\(C_{p}\\) busca un equilibrio entre el ajuste del modelo y la complejidad. Es decir, cada estadística resume el compromiso entre el ajuste del modelo y la complejidad, aunque con diferentes pesos. Para la mayoría de los conjuntos de datos, recomiendan el mismo modelo, por lo que un analista puede reportar cualquiera o todas las tres estadísticas. Sin embargo, para algunas aplicaciones, llevan a diferentes modelos recomendados. En este caso, el analista necesita confiar más en criterios no basados en datos para la selección del modelo (los cuales siempre son importantes en cualquier aplicación de regresión). 5.6.2 Validación del Modelo La validación del modelo es el proceso de confirmar que nuestro modelo propuesto es apropiado, especialmente a la luz de los propósitos de la investigación. Recuerda el proceso iterativo de formulación y selección de modelos descrito en la Sección 5.1. Una crítica importante a este proceso iterativo es que es culpable de búsqueda de datos, es decir, ajustar un gran número de modelos a un solo conjunto de datos. Como vimos en la Sección 5.2 sobre la búsqueda de datos en la regresión paso a paso, al mirar una gran cantidad de modelos podemos sobreajustar los datos y subestimar la variación natural en nuestra representación. Podemos responder a esta crítica utilizando una técnica llamada validación fuera de muestra. La situación ideal es tener disponibles dos conjuntos de datos, uno para el desarrollo del modelo y otro para la validación del modelo. Inicialmente desarrollamos uno o varios modelos en el primer conjunto de datos. Los modelos desarrollados a partir del primer conjunto de datos se llaman nuestros modelos candidatos. Luego, el rendimiento relativo de los modelos candidatos podría medirse en un segundo conjunto de datos. De esta manera, los datos utilizados para validar el modelo no se ven afectados por los procedimientos utilizados para formular el modelo. Desafortunadamente, rara vez estarán disponibles dos conjuntos de datos para el investigador. Sin embargo, podemos implementar el proceso de validación dividiendo el conjunto de datos en dos submuestras. A estas las llamamos las submuestras de desarrollo del modelo y submuestras de validación, respectivamente. También se conocen como muestras de entrenamiento y prueba, respectivamente. Para ver cómo funciona el proceso en el contexto de la regresión lineal, considera el siguiente procedimiento. Procedimiento de Validación Fuera de Muestra Comienza con un tamaño de muestra de \\(n\\) y divídelo en dos submuestras, llamadas la submuestra de desarrollo del modelo y la submuestra de validación. Sea \\(n_1\\) y \\(n_2\\) el tamaño de cada submuestra. En regresión transversal, realiza esta división usando un mecanismo de muestreo aleatorio. Usa la notación \\(i=1,...,n_1\\) para representar las observaciones de la submuestra de desarrollo del modelo y \\(i=n_1+1,...,n_1+n_2=n\\) para las observaciones de la submuestra de validación. La Figura 5.9 ilustra este procedimiento. Usando la submuestra de desarrollo del modelo, ajusta un modelo candidato al conjunto de datos \\(i=1,...,n_1\\). Usando el modelo creado en el Paso (ii) y las variables explicativas de la submuestra de validación, “predice” las variables dependientes en la submuestra de validación, \\(\\hat{y}_i\\), donde \\(i=n_1+1,...,n_1+n_2\\). (Para obtener estas predicciones, puede que necesites transformar las variables dependientes de nuevo a la escala original.) Evalúa la proximidad de las predicciones a los datos retenidos. Una medida es la suma de errores cuadráticos de predicción \\[\\begin{equation} SSPE = \\sum_{i=n_1+1}^{n_1+n_2} (y_i - \\hat{y}_i)^2 . \\tag{5.7} \\end{equation}\\] Repite los Pasos (ii) a (iv) para cada modelo candidato. Elige el modelo con el menor SSPE. Figura 5.9: Para la validación del modelo, un conjunto de datos de tamaño \\(n\\) se divide aleatoriamente en dos submuestras Existen varias críticas a la SSPE. Primero, es evidente que calcular esta estadística para cada uno de varios modelos candidatos lleva una cantidad considerable de tiempo y esfuerzo. Sin embargo, como ocurre con muchas técnicas estadísticas, esto es simplemente una cuestión de tener disponible software estadístico especializado para realizar los pasos descritos anteriormente. Segundo, dado que la estadística en sí se basa en un subconjunto aleatorio de la muestra, su valor variará de un analista a otro. Esta objeción podría superarse utilizando las primeras \\(n_1\\) observaciones de la muestra. En la mayoría de las aplicaciones, esto no se hace por si hay una relación oculta en el orden de las observaciones. Tercero, y quizás lo más importante, es el hecho de que la elección de los tamaños relativos de los subconjuntos, \\(n_1\\) y \\(n_2\\), no está clara. Varios investigadores recomiendan diferentes proporciones para la asignación. Snee (1977) sugiere que la división de datos no se realice a menos que el tamaño de la muestra sea moderadamente grande, específicamente, \\(n \\geq 2(k+1) + 20\\). Las directrices de Picard y Berk (1990) muestran que cuanto mayor es el número de parámetros a estimar, mayor es la proporción de observaciones necesarias para la submuestra de desarrollo del modelo. Como regla general, para conjuntos de datos con 100 observaciones o menos, usa alrededor del 25-35% de la muestra para validación fuera de muestra. Para conjuntos de datos con 500 o más observaciones, usa el 50% de la muestra para validación fuera de muestra. Hastie, Tibshirani y Friedman (2001) señalan que una división típica es 50% para desarrollo/entrenamiento, 25% para validación, y el 25% restante para una tercera etapa de validación adicional que ellos llaman prueba. Debido a estas críticas, los analistas utilizan varias variantes del proceso básico de validación fuera de muestra. Aunque no existe un procedimiento teóricamente mejor, se acuerda ampliamente que la validación del modelo es una parte importante para confirmar la utilidad de un modelo. 5.6.3 Validación Cruzada La validación cruzada es una técnica de validación de modelos que divide los datos en dos conjuntos disjuntos. La Sección 5.6.2 discutió la validación fuera de muestra, donde los datos se dividieron aleatoriamente en dos subconjuntos, ambos conteniendo un porcentaje considerable de los datos. Otro método popular es la validación cruzada de dejar uno fuera, donde la muestra de validación consiste en una sola observación y la muestra de desarrollo se basa en el resto del conjunto de datos. Especialmente para tamaños de muestra pequeños, una estadística atractiva de validación cruzada de dejar uno fuera es PRESS, la Suma de Cuadrados de Residuos Predichos. Para definir la estadística, considera el siguiente procedimiento donde suponemos que hay un modelo candidato disponible. Procedimiento de Validación PRESS Desde la muestra completa, omite el \\(i\\)-ésimo punto y usa las \\(n-1\\) observaciones restantes para calcular los coeficientes de regresión. Usa los coeficientes de regresión calculados en el primer paso y las variables explicativas para la \\(i\\)-ésima observación para calcular la respuesta predicha, \\(\\hat{y}_{(i)}\\). Esta parte del procedimiento es similar al cálculo de la estadística SSPE con \\(n_1=n-1\\) y \\(n_2=1\\). Ahora, repite (i) y (ii) para \\(i=1,...,n\\). Resumiendo, define \\[\\begin{equation} PRESS = \\sum_{i=1}^{n} (y_i - \\hat{y}_{(i)})^2 . \\tag{5.8} \\end{equation}\\] Al igual que con SSPE, esta estadística se calcula para cada uno de varios modelos competidores. Bajo este criterio, elegimos el modelo con el PRESS más pequeño. Basado en esta definición, la estadística parece ser muy intensiva en cálculos ya que requiere \\(n\\) ajustes de regresión para evaluarla. Para abordar esto, los lectores interesados encontrarán que la Sección 5.10.2 establece \\[\\begin{equation} y_i - \\hat{y}_{(i)} = \\frac{e_i}{1 - h_{ii}} . \\tag{5.9} \\end{equation}\\] Aquí, \\(e_i\\) y \\(h_{ii}\\) representan el \\(i\\)-ésimo residuo y la influencia del ajuste de regresión utilizando el conjunto de datos completo. Esto da lugar a \\[\\begin{equation} PRESS = \\sum_{i=1}^{n} \\left( \\frac{e_i}{1 - h_{ii}} \\right)^2 , \\tag{5.10} \\end{equation}\\] lo cual es una fórmula computacionalmente mucho más fácil. Así, la estadística PRESS es menos intensiva en cálculos que SSPE. Otra ventaja importante de esta estadística, en comparación con SSPE, es que no necesitamos hacer una elección arbitraria sobre los tamaños relativos de los subconjuntos. De hecho, dado que estamos realizando una validación “fuera de muestra” para cada observación, se puede argumentar que este procedimiento es más eficiente, una consideración especialmente importante cuando el tamaño de la muestra es pequeño (por ejemplo, menos de 50 observaciones). Una desventaja es que, dado que el modelo se vuelve a ajustar para cada punto eliminado, PRESS no goza de la apariencia de independencia entre los aspectos de estimación y predicción, a diferencia de SSPE. 5.7 Heterocedasticidad En la mayoría de las aplicaciones de regresión, el objetivo es entender los determinantes de la función de regresión \\(\\mathrm{E~}y_i = \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta = \\mu_i\\). Nuestra capacidad para entender la media está fuertemente influenciada por la cantidad de dispersión respecto a la media, que cuantificamos usando la varianza \\(\\mathrm{E}\\left(y_i - \\mu_i\\right)^2\\). En algunas aplicaciones, como cuando me peso en una balanza, hay relativamente poca variabilidad; las mediciones repetidas dan casi el mismo resultado. En otras aplicaciones, como el tiempo que me toma volar a Nueva York, las mediciones repetidas muestran una variabilidad sustancial y están llenas de incertidumbre inherente. La cantidad de incertidumbre también puede variar de un caso a otro. Denotamos el caso de “variabilidad variable” con la notación \\(\\sigma_i^2 = \\mathrm{E}\\left(y_i - \\mu_i\\right)^2\\). Cuando la variabilidad varía según la observación, esto se conoce como heterocedasticidad, que significa “dispersión diferente”. En contraste, la suposición habitual de variabilidad común (suposición E3/F3 en la Sección 3.2) se llama homocedasticidad, lo que significa “misma dispersión”. Nuestras estrategias de estimación dependen del grado de heterocedasticidad. Para conjuntos de datos con solo una ligera heterocedasticidad, se puede usar mínimos cuadrados para estimar los coeficientes de regresión, tal vez combinado con un ajuste para los errores estándar (descritos en la Sección 5.7.2). Esto se debe a que los estimadores de mínimos cuadrados son insesgados incluso en presencia de heterocedasticidad (ver Propiedad 1 en la Sección 3.2). Sin embargo, con variables dependientes heterocedásticas, el teorema de Gauss-Markov ya no se aplica, por lo que los estimadores de mínimos cuadrados no están garantizados como óptimos. En casos de heterocedasticidad severa, se utilizan estimadores alternativos, siendo los más comunes aquellos basados en transformaciones de la variable dependiente, como se describirá en la Sección 5.7.4. 5.7.1 Detección de Heterocedasticidad Para decidir una estrategia para manejar la posible heterocedasticidad, primero debemos evaluar o detectar su presencia. Para detectar heterocedasticidad de manera gráfica, una buena idea es realizar un ajuste preliminar de regresión de los datos y trazar los residuos frente a los valores ajustados. Para ilustrar, la Figura 5.10 muestra un gráfico de un conjunto de datos ficticio con una variable explicativa donde la dispersión aumenta a medida que aumenta la variable explicativa. Se realizó una regresión por mínimos cuadrados: se calcularon los residuos y los valores ajustados. La Figura 5.11 es un ejemplo de un gráfico de residuos frente a valores ajustados. El ajuste preliminar de regresión elimina muchos de los patrones principales en los datos y deja al ojo libre para concentrarse en otros patrones que pueden influir en el ajuste. Trazamos los residuos frente a los valores ajustados porque los valores ajustados son una aproximación del valor esperado de la respuesta y, en muchas situaciones, la variabilidad crece con la respuesta esperada. Figura 5.10: El área sombreada representa los datos. Figura 5.11: Residuos trazados frente a los valores ajustados para los datos en la Figura 5.10. Más pruebas formales de heterocedasticidad también están disponibles en la literatura de regresión. Por ejemplo, consideremos una prueba de Breusch y Pagan (1980). Específicamente, esta prueba examina la hipótesis alternativa \\(H_a\\): \\(\\mathrm{Var~} y_i = \\sigma^2 + \\mathbf{z}_i^{\\prime} \\boldsymbol \\gamma\\), donde \\(\\mathbf{z}_i\\) es un vector conocido de variables y \\(\\boldsymbol \\gamma\\) es un vector de parámetros de dimensión \\(p\\). Así, la hipótesis nula es \\(H_0:~ \\boldsymbol \\gamma = \\mathbf{0}\\), que es equivalente a homocedasticidad, \\(\\mathrm{Var~} y_i = \\sigma^2.\\) Procedimiento para la Prueba de Heterocedasticidad Ajuste un modelo de regresión y calcule los residuos del modelo, \\(e_i\\). Calcule los residuos estandarizados al cuadrado, \\(e_i^{\\ast 2} = e_i^2 / s^2\\). Ajuste un modelo de regresión de \\(e_i^{\\ast 2}\\) sobre \\(\\mathbf{z}_i\\). La estadística de la prueba es \\(LM = \\frac{\\text{Regress~SS}_z}{2}\\), donde \\(Regress~SS_z\\) es la suma de cuadrados de la regresión del ajuste del modelo en el paso (iii). Rechace la hipótesis nula si \\(LM\\) excede un percentil de una distribución chi-cuadrado con \\(p\\) grados de libertad. El percentil es uno menos el nivel de significancia de la prueba. Aquí usamos \\(LM\\) para denotar la estadística de la prueba porque Breusch y Pagan la derivaron como una estadística de multiplicador de Lagrange; consulte Breusch y Pagan (1980) para más detalles. 5.7.2 Errores Estándar Consistentes con Heterocedasticidad Para conjuntos de datos con solo una leve heterocedasticidad, una estrategia sensata es emplear estimadores de mínimos cuadrados de los coeficientes de regresión y ajustar el cálculo de errores estándar para tener en cuenta la heterocedasticidad. En la Sección 3.2 sobre propiedades, vimos que los coeficientes de regresión por mínimos cuadrados pueden escribirse como \\(\\mathbf{b} = \\sum_{i=1}^n \\mathbf{w}_i y_i,\\) donde \\(\\mathbf{w}_i = \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1} \\mathbf{x}_i\\). Así, con \\(\\sigma_i^2 = \\mathrm{Var~} y_i\\), tenemos \\[\\begin{equation} \\mathrm{Var~}\\mathbf{b} = \\sum_{i=1}^n \\mathbf{w}_i \\mathbf{w}_i^{\\prime} \\sigma_i^2 = \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1} \\left( \\sum_{i=1}^n \\sigma_i^2 \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\right) \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1}. \\tag{5.11} \\end{equation}\\] Esta cantidad es conocida excepto por \\(\\sigma_i^2\\). Podemos calcular los residuos usando los coeficientes de regresión por mínimos cuadrados como \\(e_i = y_i - \\mathbf{x}_i^{\\prime} \\mathbf{b}\\). Con estos, podemos definir la estimación empírica, o robusta, de la matriz varianza-covarianza como \\[ \\widehat{\\mathrm{Var~}\\mathbf{b}} = \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1} \\left( \\sum_{i=1}^n e_i^2 \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\right) \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1}. \\] Los correspondientes errores estándar “consistentes con heterocedasticidad” son \\[\\begin{equation} se_r(b_j) = \\sqrt{(j+1)^{\\text{er}}~ \\text{elemento diagonal de }\\widehat{\\mathrm{Var~}\\mathbf{b}}}. \\tag{5.12} \\end{equation}\\] La lógica detrás de este estimador es que cada residual al cuadrado, \\(e_i^2\\), puede ser una mala estimación de \\(\\sigma_i^2\\). Sin embargo, nuestro interés es estimar una (ponderada) suma de varianzas en la ecuación (5.11); estimar la suma es una tarea mucho más fácil que estimar cualquier estimación de varianza individual. Los errores estándar robustos, o consistentes con heterocedasticidad, están ampliamente disponibles en paquetes de software estadístico. Aquí, también verá definiciones alternativas de residuos empleados, como en la Sección 5.3.1. Si su paquete estadístico ofrece opciones, el estimador robusto que utiliza residuos studentizados es generalmente preferido. 5.7.3 Mínimos Cuadrados Ponderados Los estimadores de mínimos cuadrados son menos útiles para conjuntos de datos con heterocedasticidad severa. Una estrategia es usar una variación de la estimación por mínimos cuadrados mediante el ponderado de las observaciones. La idea es que, al minimizar la suma de errores cuadrados usando datos heterocedásticos, la variabilidad esperada de algunas observaciones es menor que la de otras. Intuitivamente, parece razonable que, cuanto menor es la variabilidad de la respuesta, más confiable es esa respuesta y mayor peso debería recibir en el procedimiento de minimización. Los mínimos cuadrados ponderados son una técnica que tiene en cuenta esta “variabilidad variable”. Específicamente, usamos los supuestos E1, E2 y E4 de la Sección 3.2.3, con E3 reemplazado por E \\(\\varepsilon_i = 0\\) y \\(\\text{Var} \\varepsilon_i = \\sigma^2 / w_i\\), de modo que la variabilidad es proporcional a un peso conocido \\(w_i\\). Por ejemplo, si la unidad de análisis \\(i\\) representa una entidad geográfica como un estado, podrías usar el número de personas en el estado como peso. O, si \\(i\\) representa una empresa, podrías usar los activos de la empresa para la variable de ponderación. Valores mayores de \\(w_i\\) indican una variable de respuesta más precisa a través de una menor variabilidad. En aplicaciones actuariales, se usan pesos para tener en cuenta una exposición, como el monto de la prima de seguro, el número de empleados, el tamaño de la nómina, el número de vehículos asegurados, etc. (una discusión más detallada está en el Capítulo 18). Este modelo puede convertirse fácilmente en el problema de “mínimos cuadrados ordinarios” multiplicando todas las variables de regresión por \\(\\sqrt{w_i}\\). Es decir, si definimos \\(y_i^{\\ast} = y_i \\times \\sqrt{w_i}\\) y \\(x_{ij}^{\\ast} = x_{ij} \\times \\sqrt{w_i}\\), entonces, a partir del supuesto E1, tenemos \\[ \\begin{array}{ll} y_i^{\\ast} &amp; = y_i \\times \\sqrt{w_i} = \\left( \\beta_0 x_{i0} + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik} + \\varepsilon_i \\right) \\sqrt{w_i} \\\\ &amp;= \\beta_0 x_{i0}^{\\ast} + \\beta_1 x_{i1}^{\\ast} + \\ldots + \\beta_k x_{ik}^{\\ast} + \\varepsilon_i^{\\ast} \\end{array} \\] donde \\(\\varepsilon_i^{\\ast} = \\varepsilon_i \\times \\sqrt{w_i}\\) tiene una varianza homocedástica \\(\\sigma^2\\). Así, con las variables reescaladas, toda la inferencia puede proceder como antes. Este trabajo ha sido automatizado en paquetes estadísticos donde el usuario simplemente especifica los pesos \\(w_i\\) y el paquete hace el resto. En términos de álgebra de matrices, este procedimiento se puede llevar a cabo definiendo una matriz de pesos \\(n \\times n\\) \\(\\mathbf{W} = \\text{diag}(w_i)\\), de modo que el elemento diagonal \\(i\\)-ésimo de \\(\\mathbf{W}\\) sea \\(w_i\\). Ampliando la ecuación (3.14), por ejemplo, las estimaciones de mínimos cuadrados ponderados se pueden expresar como \\[\\begin{equation} \\mathbf{b}_{WLS} = \\left( \\mathbf{X}^{\\prime} \\mathbf{W} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{W} \\mathbf{y}. \\tag{5.13} \\end{equation}\\] Discuciones adicionales sobre la estimación de mínimos cuadrados ponderados se presentarán en la Sección 15.1.1. 5.7.4 Transformaciones Otro enfoque que maneja la heterocedasticidad severa, introducido en la Sección 1.3, es transformar la variable dependiente, típicamente con una transformación logarítmica de la forma \\(y^{\\ast} = \\ln y\\). Como vimos en la Sección 1.3, las transformaciones pueden servir para “reducir” la dispersión de los datos y simetrizar una distribución. A través de un cambio de escala, una transformación también cambia la variabilidad, potencialmente alterando un conjunto de datos heterocedástico en uno homocedástico. Esta es tanto una fortaleza como una limitación del enfoque de transformación: una transformación afecta simultáneamente tanto la distribución como la heterocedasticidad. Las transformaciones de potencia, como la transformación logarítmica, son más útiles cuando la variabilidad de los datos crece con la media. En este caso, la transformación servirá para “reducir” los datos a una escala que parece ser homocedástica. Por el contrario, dado que las transformaciones son funciones monótonas, no ayudarán con patrones de variabilidad que son no monótonos. Además, si tus datos son razonablemente simétricos pero heterocedásticos, una transformación no será útil porque cualquier elección que mitigue la heterocedasticidad sesgará la distribución. Cuando los datos no son positivos, es común agregar una constante a cada observación para que todas las observaciones sean positivas antes de la transformación. Por ejemplo, la transformación \\(\\ln(1+y)\\) acomoda la presencia de ceros. También se puede multiplicar por una constante para que se mantengan las unidades originales aproximadas. Por ejemplo, la transformación \\(100 \\ln(1 + y/100)\\) puede aplicarse a datos porcentuales donde a veces aparecen porcentajes negativos. Nuestras discusiones sobre transformaciones se han centrado en transformar variables dependientes. Como se señaló en la Sección 3.5, también es posible transformar variables explicativas. Esto se debe a que los supuestos de regresión condicionan las variables explicativas (Sección 3.2.3). Algunos analistas prefieren transformar variables para aproximarse a la normalidad, considerando las distribuciones normales multivariadas como una base para el análisis de regresión. Otros son reacios a transformar variables explicativas debido a las dificultades para interpretar los modelos resultantes. El enfoque aquí es usar transformaciones que sean fácilmente interpretables, como las introducidas en la Sección 3.5. Otras transformaciones son ciertamente candidatas para incluir en un modelo seleccionado, pero deben proporcionar dividendos sustanciales en términos de ajuste o poder predictivo si son difíciles de comunicar. 5.8 Lectura Adicional y Referencias Long y Ervin (2000) reúnen pruebas convincentes sobre el uso de estimadores alternativos consistentes con la heterocedasticidad para los errores estándar que tienen un mejor rendimiento en muestras finitas que las versiones clásicas. Las propiedades de gran muestra de los estimadores empíricos han sido establecidas por Eicker (1967), Huber (1967) y White (1980) en el caso de la regresión lineal. Para el caso de la regresión lineal, MacKinnon y White (1985) sugieren alternativas que proporcionan mejores propiedades en muestras pequeñas. Para muestras pequeñas, la evidencia se basa en (1) el sesgo de los estimadores, (2) su motivación como estimadores jackknife y (3) su rendimiento en estudios de simulación. Otros métodos para medir la colinealidad basados en conceptos de álgebra de matrices que involucran valores propios, como los números de condición y los índices de condición, son utilizados por algunos analistas. Consulta a Belsey, Kuh y Welsch (1980) para un tratamiento sólido de la colinealidad y los diagnósticos de regresión. Hocking (2003) proporciona lecturas adicionales sobre colinealidad y componentes principales. Consulta a Carroll y Ruppert (1988) para más discusiones sobre transformaciones en la regresión. Hastie, Tibshirani y Friedman (2001) ofrecen una discusión avanzada sobre problemas de selección de modelos, centrándose en los aspectos predictivos de los modelos en el lenguaje del aprendizaje automático. Referencias del Capítulo Belseley, David A., Edwin Kuh and Roy E. Welsch (1980). Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. Wiley, New York. Bendel, R. B. and Afifi, A. A. (1977). Comparison of stopping rules in forward “stepwise” regression. Journal of the American Statistical Association 72, 46-53. Box, George E. P. (1980). Sampling and Bayes inference in scientific modeling and robustness (with discussion). Journal of the Royal Statistical Society, Series A, 143, 383-430. Breusch, T. S. and A. R. Pagan (1980). The Lagrange multiplier test and its applications to model specification in econometrics. Review of Economic Studies, 47, 239-53. Carroll, Raymond J. and David Ruppert (1988). Transformation and Weighting in Regression, Chapman-Hall. Eicker, F. (1967). Limit theorems for regressions with unequal and dependent errors. Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability 1, LeCam, L. M. and J. Neyman, editors, University of California Press, pp, 59-82. Hadi, A. S. (1988). Diagnosing collinearity-influential observations. Computational Statistics and Data Analysis 7, 143-159. Hastie, Trevor, Robert Tibshirani and Jerome Friedman (2001). The Elements of Statistical Learning: Data Mining, Inference and Prediction. Springer-Verlag, New York. Hocking, Ronald R. (2003). Methods and Applications of Linear Models: Regression and the Analysis of Variance. Wiley, New York. Huber, P. J. (1967). The behaviour of maximum likelihood estimators under non-standard conditions. Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability 1, LeCam, L. M. and Neyman, J. editors, University of California Press, pp, 221-33. Long, J.S. and L.H. Ervin (2000). Using heteroscedasticity consistent standard errors in the linear regression model. American Statistician 54, 217-224. MacKinnon, J.G. and H. White (1985). Some heteroskedasticity consistent covariance matrix estimators with improved finite sample properties. Journal of Econometrics 29, 53-57. Mason, R. L. and Gunst, R. F. (1985). Outlier-induced collinearities. Technometrics 27, 401-407. Picard, R. R. and Berk, K. N. (1990). Data splitting. The American Statistician 44, 140-147. Rencher, A. C. and Pun, F. C. (1980). Inflation of \\(R^2\\) in best subset regression. Technometrics 22, 49-53. Snee, R. D. (1977). Validation of regression models. Methods and examples. Technometrics 19, 415-428. 5.9 Ejercicios 5.1. Estás realizando una regresión con una variable explicativa, por lo que considera el modelo de regresión lineal básico \\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\). Muestra que el apalancamiento \\(i\\)-ésimo se puede simplificar a \\[ h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\overline{x})^2}{(n-1) s_x^2}. \\] Muestra que \\(\\overline{h} = 2 / n\\). Supón que \\(h_{ii} = 6/n\\). ¿Cuántas desviaciones estándar está \\(x_i\\) alejado (ya sea por encima o por debajo) de la media? 5.2. Considera los resultados de una regresión usando una variable explicativa con \\(n=3\\) observaciones. Los residuos y los apalancamientos son: \\[ \\small{ \\begin{array}{l|ccc} \\hline i &amp; 1 &amp; 2 &amp; 3 \\\\ \\hline \\text{Residuos } e_i &amp; 3.181 &amp; -6.362 &amp; 3.181 \\\\ \\text{Apalancamientos } h_{ii} &amp; 0.8333 &amp; 0.3333 &amp; 0.8333 \\\\ \\hline \\end{array} } \\] Calcula el estadístico \\(PRESS\\). 5.3. Expectativas de Vida Nacionales. Continuamos con el análisis iniciado en los Ejercicios 1.7, 2.22, 3.6 y 4.7. El enfoque de este ejercicio es la selección de variables. Comienza con los datos de \\(n=185\\) países de todo el mundo que tienen expectativas de vida válidas (no faltantes). Grafica la expectativa de vida frente al producto interno bruto y los gastos privados en salud. A partir de estos gráficos, describe por qué es deseable utilizar transformaciones logarítmicas, lnGDP y lnHEALTH, respectivamente. También grafica la expectativa de vida frente a lnGDP y lnHEALTH para confirmar tu intuición. Utiliza un algoritmo de regresión paso a paso para ayudarte a seleccionar un modelo. No consideres las variables RESEARCHERS, SMOKING y FEMALEBOSS ya que tienen muchos valores faltantes. Para las variables restantes, utiliza solo las observaciones sin valores faltantes. Hazlo dos veces, con y sin la variable categórica REGION. Regresa al conjunto de datos completo de \\(n=185\\) países y ejecuta un modelo de regresión usando FERTILITY, PUBLICEDUCATION y lnHEALTH como variables explicativas. c(i). Proporciona histogramas de residuos estandarizados y apalancamientos. c(ii). Identifica el residuo estandarizado y el apalancamiento asociados con Lesoto, anteriormente Basutolandia, un reino rodeado por Sudáfrica. ¿Es esta observación un valor atípico, un punto de alto apalancamiento, o ambos? c(iii). Repite la regresión sin Lesoto. Cita cualquier diferencia en los coeficientes estadísticos entre este modelo y el del apartado c(i). 5.4. Seguro de Vida a Término. Continuamos con nuestro estudio de la Demanda de Seguro de Vida a Término de los Capítulos 3 y 4. Específicamente, examinamos la Encuesta de Finanzas del Consumidor (SCF) de 2004, una muestra representativa a nivel nacional que contiene información extensa sobre activos, pasivos, ingresos y características demográficas de los encuestados (potenciales clientes de EE.UU.). Estudiamos una muestra aleatoria de 500 familias con ingresos positivos. De la muestra de 500, inicialmente consideramos una submuestra de \\(n=275\\) familias que compraron seguro de vida a término. Considera una regresión lineal de LNINCOME, EDUCATION, NUMHH, MARSTAT, AGE y GENDER sobre LNFACE. Colinealidad. No todas las variables resultaron ser estadísticamente significativas. Para investigar una posible explicación, calcula los factores de inflación de la varianza. a(i). Explica brevemente la idea de colinealidad y un factor de inflación de la varianza. a(ii). ¿Qué constituye un gran factor de inflación de la varianza? a(iii). Si se detecta un gran factor de inflación de la varianza, ¿qué posibles acciones podemos tomar para abordar este aspecto de los datos? a(iv). Complementa las estadísticas de los factores de inflación de la varianza con una tabla de correlaciones de las variables explicativas. Basado en estas estadísticas, ¿es la colinealidad un problema con este modelo ajustado? ¿Por qué o por qué no? Puntos Inusuales. A veces, un ajuste deficiente del modelo puede deberse a puntos inusuales. b(i). Define la idea de apalancamiento para una observación. b(ii). Para este modelo ajustado, da reglas generales para identificar puntos con apalancamiento inusual. Identifica cualquier punto inusual. b(iii). Un analista está preocupado por los valores de apalancamiento de este modelo ajustado y sugiere usar FACE como la variable dependiente en lugar de LNFACE. Describe cómo cambiarían los valores de apalancamiento usando esta variable dependiente alternativa. Análisis de Residuos. Podemos aprender cómo mejorar los ajustes del modelo a partir de los análisis de residuos. c(i). Proporciona un gráfico de residuos frente a valores ajustados. ¿Qué esperamos aprender de este tipo de gráfico? ¿Este gráfico muestra alguna inadecuación del modelo? c(ii). Proporciona un gráfico \\(qq\\) de residuos. ¿Qué esperamos aprender de este tipo de gráfico? ¿Este gráfico muestra alguna inadecuación del modelo? c(iii). Proporciona un gráfico de residuos frente a apalancamientos. ¿Qué esperamos aprender de este tipo de gráfico? ¿Este gráfico muestra alguna inadecuación del modelo? Regresión Paso a Paso. Ejecuta un algoritmo de regresión paso a paso. Supón que este algoritmo sugiere un modelo utilizando LNINCOME, EDUCATION, NUMHH y GENDER como variables explicativas para predecir la variable dependiente LNFACE. d(i). ¿Cuál es el propósito de la regresión paso a paso? d(ii). Describe dos desventajas importantes de los algoritmos de regresión paso a paso. 5.10 Suplementos Técnicos para el Capítulo 5 5.10.1 Matriz de Proyección Matriz de Sombrero. Definimos la matriz de sombrero como \\(\\mathbf{H} = \\mathbf{X(X}^{\\prime}\\mathbf{X)}^{-1} \\mathbf{X}^{\\prime}\\), de manera que \\(\\mathbf{\\hat{y}} = \\mathbf{X b} = \\mathbf{Hy}\\). De esto, se dice que la matriz \\(\\mathbf{H}\\) proyecta el vector de respuestas \\(\\mathbf{y}\\) sobre el vector de valores ajustados \\(\\mathbf{\\hat{y}}\\). Dado que \\(\\mathbf{H}^{\\prime} = \\mathbf{H}\\), la matriz de sombrero es simétrica. Además, también es una matriz idempotente debido a la propiedad de que \\(\\mathbf{HH} = \\mathbf{H}\\). Para ver esto, tenemos que \\[ \\begin{array}{ll} \\mathbf{HH} &amp;= \\mathbf{(X(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathbf{)(X(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathbf{)} \\\\ &amp;= \\mathbf{X(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{(\\mathbf{X}^{\\prime}X)(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime} = \\mathbf{X(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime} = \\mathbf{H}. \\end{array} \\] De manera similar, es fácil verificar que \\(\\mathbf{I-H}\\) es idempotente. Dado que \\(\\mathbf{H}\\) es idempotente, a partir de algunos resultados en álgebra de matrices, es sencillo mostrar que \\[ \\sum_{i=1}^{n} h_{ii} = k + 1. \\] Como se discutió en la Sección 5.4.1, usamos nuestros límites y el apalancamiento promedio, \\(\\bar{h} = (k + 1)/n\\), para ayudar a identificar observaciones con apalancamiento inusualmente alto. Varianza de los Residuos. Usando la ecuación del modelo \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\), podemos expresar el vector de residuos como \\[\\begin{equation} \\mathbf{e} = \\mathbf{y} - \\mathbf{\\hat{y}} = \\mathbf{y - Hy} = \\mathbf{(I-H)(X \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon})} = \\mathbf{(I-H) \\boldsymbol{\\varepsilon}}. \\tag{5.14} \\end{equation}\\] La última igualdad se debe al hecho de que \\(\\mathbf{(I-H)X} = \\mathbf{X - HX} = \\mathbf{X - X} = \\mathbf{0}\\). Usando \\(\\text{Var }~ \\boldsymbol{\\varepsilon} = \\sigma^2 \\mathbf{I}\\), tenemos \\[ \\begin{array}{ll} \\text{Var } \\mathbf{e} &amp;= \\text{Var }\\left[ \\mathbf{(I-H)\\boldsymbol{\\varepsilon}} \\right] = \\mathbf{(I-H)} \\text{Var } \\boldsymbol{\\varepsilon} \\mathbf{(I-H)} \\\\ &amp;= \\sigma^2 \\mathbf{(I-H)} \\mathbf{I} \\mathbf{(I-H)} = \\sigma^2 \\mathbf{(I-H)}. \\end{array} \\] La última igualdad proviene del hecho de que \\(\\mathbf{I-H}\\) es idempotente. Así, tenemos que \\[\\begin{equation} \\text{Var } e_i = \\sigma^2 (1 - h_{ii}) \\text{ y Cov } (e_i, e_j) = -\\sigma^2 h_{ij}. \\tag{5.15} \\end{equation}\\] Así, aunque los errores verdaderos \\(\\boldsymbol{\\varepsilon}\\) son no correlacionados, hay una pequeña correlación negativa entre los residuos \\(\\mathbf{e}\\). Dominio del Error en el Residuo. Examinando la fila \\(i\\)-ésima de la ecuación (5.14), tenemos que el residuo \\(i\\)-ésimo \\[\\begin{equation} e_i = \\varepsilon_i - \\sum_{j=1}^{n} h_{ij} \\varepsilon_j \\tag{5.16} \\end{equation}\\] se puede expresar como una combinación lineal de errores independientes. La relación \\(\\mathbf{H} = \\mathbf{HH}\\) da lugar a \\[\\begin{equation} h_{ii} = \\sum_{j=1}^{n} h_{ij}^2. \\tag{5.17} \\end{equation}\\] Dado que \\(h_{ii}\\) es, en promedio, \\((k + 1)/n\\), esto indica que cada \\(h_{ij}\\) es pequeño en relación con 1. Así, al interpretar la ecuación (5.16), decimos que la mayor parte de la información en \\(e_i\\) se debe a \\(\\varepsilon_i\\). Correlaciones con los Residuos. Primero define \\(\\mathbf{x}^j = (x_{1j}, x_{2j}, \\dots, x_{nj})^{\\prime}\\) como la columna que representa la \\(j\\)-ésima variable. Con esta notación, podemos particionar la matriz de variables explicativas como \\(\\mathbf{X} = \\left( \\mathbf{x}^{0}, \\mathbf{x}^{1}, \\dots, \\mathbf{x}^{k} \\right)\\). Ahora, examinando la columna \\(j\\)-ésima de la relación \\(\\mathbf{(I-H)X} = \\mathbf{0}\\), tenemos \\(\\mathbf{(I-H)x}^{j} = \\mathbf{0}\\). Con \\(\\mathbf{e} = \\mathbf{(I-H) \\boldsymbol{\\varepsilon}}\\), esto da \\[ \\mathbf{e}^{\\prime} \\mathbf{x}^{j} = \\boldsymbol{\\varepsilon}^{\\prime} \\mathbf{(I-H)x}^{j} = 0, \\] para \\(j = 0, 1, \\ldots, k.\\) Este resultado tiene varias implicaciones. Si el intercepto está en el modelo, entonces \\(\\mathbf{x}^{0} = (1, 1, \\ldots, 1)^{\\prime}\\) es un vector de unos. Aquí, \\(\\mathbf{e}^{\\prime} \\mathbf{x}^{0} = 0\\) significa que \\(\\sum_{i=1}^{n} e_i = 0\\) o, el residuo promedio es cero. Además, dado que \\(\\mathbf{e}^{\\prime} \\mathbf{x}^{j} = 0\\), es fácil verificar que la correlación muestral entre \\(\\mathbf{e}\\) y \\(\\mathbf{x}^{j}\\) es cero. En la misma línea, también tenemos que \\(\\mathbf{e}^{\\prime} \\mathbf{\\hat{y}} = \\mathbf{e}^{\\prime} \\mathbf{(I-H)Xb} = \\mathbf{0}\\). Así, usando el mismo argumento que antes, la correlación muestral entre \\(\\mathbf{e}\\) y \\(\\mathbf{\\hat{y}}\\) es cero. Coeficiente de Correlación Múltiple. Para un ejemplo de una correlación diferente de cero, considera \\(r(\\mathbf{y, \\hat{y}})\\), la correlación muestral entre \\(\\mathbf{y}\\) y \\(\\mathbf{\\hat{y}}\\). Dado que \\(\\mathbf{(I-H)x}^{0} = \\mathbf{0}\\), tenemos \\(\\mathbf{x}^{0} = \\mathbf{Hx}^{0}\\) y, por lo tanto, \\(\\mathbf{\\hat{y}}^{\\prime} \\mathbf{x}^{0} = \\mathbf{y}^{\\prime} \\mathbf{Hx}^{0} = \\mathbf{y^{\\prime} x}^{0}\\). Asumiendo que \\(\\mathbf{x}^{0} = (1, 1, \\ldots, 1)^{\\prime}\\), esto significa que \\(\\sum_{i=1}^{n} \\hat{y}_i = \\sum_{i=1}^{n} y_i\\), por lo que el valor promedio ajustado es \\(\\bar{y}\\). \\[ r(\\mathbf{y, \\hat{y}}) = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(\\hat{y}_i - \\bar{y})}{(n-1) s_y s_{\\hat{y}}}. \\] Recuerda que \\((n-1) s_y^2 = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = Total ~SS\\) y \\((n-1) s_{\\hat{y}}^2 = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2 = Regress ~SS\\). Además, con \\(\\mathbf{x}^0 = (1, 1, \\ldots, 1)^{\\prime}\\), \\[ \\begin{array}{ll} \\sum_{i=1}^{n} (y_i - \\bar{y})(\\hat{y}_i - \\bar{y}) &amp;= (\\mathbf{y} - \\bar{y} \\mathbf{x}^0)^{\\prime} (\\mathbf{\\hat{y}} - \\bar{y} \\mathbf{x}^0) = \\mathbf{y}^{\\prime} \\mathbf{\\hat{y}} - \\bar{y}^2 \\mathbf{x}^{0 \\prime} \\mathbf{x}^0 \\\\ &amp;= \\mathbf{y}^{\\prime} \\mathbf{Xb} - n \\bar{y}^2 = Regress ~SS. \\end{array} \\] Esto da \\[\\begin{equation} r(\\mathbf{y, \\hat{y}}) = \\frac{Regress ~SS}{\\sqrt{\\left( Total ~SS \\right) \\left( Regress ~SS \\right)}} = \\sqrt{\\frac{Regress ~SS}{Total ~SS}} = \\sqrt{R^2}. \\tag{5.18} \\end{equation}\\] Es decir, el coeficiente de determinación se puede interpretar como la raíz cuadrada de la correlación entre las respuestas observadas y las ajustadas. 5.10.2 Estadísticas Leave-One-Out Notación. Para probar la sensibilidad de las cantidades de regresión, hay varias estadísticas de interés que se basan en la noción de “dejar fuera” u omitir una observación. Con este fin, la notación de subíndice \\((i)\\) significa dejar fuera la \\(i\\)-ésima observación. Por ejemplo, omitir la fila de variables explicativas \\(\\mathbf{x}_i^{\\prime} = (x_{i0}, x_{i1}, \\dots, x_{ik})\\) de \\(\\mathbf{X}\\) da lugar a \\(\\mathbf{X}_{(i)}\\), una matriz de \\((n-1) \\times (k+1)\\) de variables explicativas. De manera similar, \\(\\mathbf{y}_{(i)}\\) es un vector de \\((n-1) \\times 1\\), basado en eliminar la \\(i\\)-ésima fila de \\(\\mathbf{y}\\). Resultado Básico de Matrices. Supongamos que \\(\\mathbf{A}\\) es una matriz invertible de \\(p \\times p\\) y \\(\\mathbf{z}\\) es un vector de \\(p \\times 1\\). El siguiente resultado de álgebra de matrices proporciona una herramienta importante para entender las estadísticas leave-one-out en el análisis de regresión lineal. \\[\\begin{equation} \\left( \\mathbf{A - zz}^{\\prime} \\right)^{-1} = \\mathbf{A}^{-1} + \\frac{\\mathbf{A}^{-1} \\mathbf{zz}^{\\prime} \\mathbf{A}^{-1}}{1 - \\mathbf{z}^{\\prime} \\mathbf{A}^{-1} \\mathbf{z}}. \\tag{5.19} \\end{equation}\\] Para verificar este resultado, simplemente multiplica \\(\\mathbf{A - zz}^{\\prime}\\) por el lado derecho de la ecuación para obtener \\(\\mathbf{I}\\), la matriz identidad. Vector de Coeficientes de Regresión. Al omitir la \\(i\\)-ésima observación, nuestro nuevo vector de coeficientes de regresión es \\(\\mathbf{b}_{(i)} = \\left( \\mathbf{X}_{(i)}^{\\prime} \\mathbf{X}_{(i)} \\right)^{-1} \\mathbf{X}_{(i)}^{\\prime} \\mathbf{y}_{(i)}.\\) Una expresión alternativa para \\(\\mathbf{b}_{(i)}\\) que resulta ser más simple de calcular es \\[\\begin{equation} \\mathbf{b}_{(i)} = \\mathbf{b} - \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}}. \\tag{5.20} \\end{equation}\\] Para verificar esto, primero usa el resultado de inversión de matrices con \\(\\mathbf{A} = \\mathbf{X}^{\\prime} \\mathbf{X}\\) y \\(\\mathbf{z} = \\mathbf{x}_i\\) para obtener \\[ \\left( \\mathbf{X}_{(i)}^{\\prime} \\mathbf{X}_{(i)} \\right)^{-1} = (\\mathbf{X}^{\\prime} \\mathbf{X} - \\mathbf{x}_i \\mathbf{x}_i^{\\prime})^{-1} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} + \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1}}{1 - h_{ii}}, \\] donde, a partir del resultado de apalancamiento, tenemos \\(h_{ii} = \\mathbf{x}_i^{\\prime} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{x}_i\\). Multiplicando cada lado por \\[ \\mathbf{X}_{(i)}^{\\prime} \\mathbf{y}_{(i)} = \\mathbf{X}^{\\prime} \\mathbf{y} - \\mathbf{x}_i y_i \\] da \\[ \\begin{array}{ll} \\mathbf{b}_{(i)} &amp;= \\left( \\mathbf{X}_{(i)}^{\\prime} \\mathbf{X}_{(i)} \\right)^{-1} \\mathbf{X}_{(i)}^{\\prime} \\mathbf{y}_{(i)} \\\\ &amp;= \\left( (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} + \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1}}{1 - h_{ii}} \\right) \\left( \\mathbf{X}^{\\prime} \\mathbf{y} - \\mathbf{x}_i y_i \\right) \\\\ &amp;= \\mathbf{b} - \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i y_i + \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\mathbf{b} - \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i y_i}{1 - h_{ii}} \\\\ &amp;= \\mathbf{b} - \\frac{\\left( 1 - h_{ii} \\right) \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i y_i - \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\mathbf{b} - \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i h_{ii} y_i}{1 - h_{ii}} \\\\ &amp;= \\mathbf{b} - \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i y_i - \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\mathbf{b}}{1 - h_{ii}} \\\\ &amp;= \\mathbf{b} - \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}}. \\end{array} \\] Esto establece el resultado. Distancia de Cook. Para medir el efecto, o influencia, de omitir la \\(i\\)-ésima observación, Cook examinó la diferencia entre los valores ajustados con y sin la observación. Definimos la Distancia de Cook como \\[ D_i = \\frac{\\left( \\mathbf{\\hat{y} - \\hat{y}}_{(i)} \\right)^{\\prime} \\left( \\mathbf{\\hat{y} - \\hat{y}}_{(i)} \\right)}{(k+1) s^2} \\] donde \\(\\mathbf{\\hat{y}}_{(i)} = \\mathbf{Xb}_{(i)}\\) es el vector de valores ajustados calculado omitiendo el punto \\(i\\)-ésimo. Usando la ecuación (5.20) y \\(\\mathbf{\\hat{y}} = \\mathbf{Xb}\\), una expresión alternativa para la Distancia de Cook es \\[ \\begin{array}{ll} D_i &amp;= \\frac{\\left( \\mathbf{b - b}_{(i)} \\right)^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right) \\left( \\mathbf{b - b}_{(i)} \\right)}{(k+1) s^2} \\\\ &amp;= \\frac{e_i^2}{(1 - h_{ii})^2} \\frac{\\mathbf{x}_i^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right) \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i}{(k+1) s^2} \\\\ &amp; = \\frac{e_i^2}{(1 - h_{ii})^2} \\frac{h_{ii}}{(k+1) s^2} \\\\ &amp;= \\left( \\frac{e_i}{s \\sqrt{1 - h_{ii}}} \\right)^2 \\frac{h_{ii}}{(k+1) (1 - h_{ii})}. \\end{array} \\] Este resultado no solo es útil computacionalmente, sino que también sirve para descomponer la estadística en la parte debida al residuo estandarizado, \\((e_i/(s \\sqrt{1 - h_{ii}}))^2\\), y en la parte debida al apalancamiento, \\(\\frac{h_{ii}}{(k+1) (1 - h_{ii})}\\). Residuo Leave-One-Out. El residuo leave-one-out se define como \\(e_{(i)} = y_i - \\mathbf{x}_i^{\\prime} \\mathbf{b}_{(i)}\\). Se usa en el cálculo de la estadística PRESS, descrita en la Sección 5.6.3. Una expresión computacional simple es \\(e_{(i)} = \\frac{e_i}{1 - h_{ii}}\\). Para verificar esto, usa la ecuación (5.20) para obtener \\[ e_{(i)} = y_i - \\mathbf{x}_i^{\\prime} \\mathbf{b}_{(i)} = y_i - \\mathbf{x}_i^{\\prime} \\left( \\mathbf{b} - \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}} \\right) \\] \\[ = e_i + \\frac{\\mathbf{x}_i \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}} = e_i + \\frac{h_{ii} e_i}{1 - h_{ii}} = \\frac{e_i}{1 - h_{ii}}. \\] Estimación de Varianza Leave-One-Out. La estimación leave-one-out de la varianza se define como \\[ s_{(i)}^2 = \\frac{((n - 1) - (k + 1))^{-1} \\sum_{j \\ne i} \\left( y_j - \\mathbf{x}_j^{\\prime} \\mathbf{b}_{(i)} \\right)^2}{(n - 1) - (k + 1)}. \\] Se usa en la definición del residuo estandarizado, definido en la Sección 5.3.1. Una expresión computacional simple está dada por \\[\\begin{equation} s_{(i)}^2 = \\frac{(n - (k + 1)) s^2 - \\frac{e_i^2}{1 - h_{ii}}}{(n - 1) - (k + 1)}. \\tag{5.21} \\end{equation}\\] Para ver esto, primero nota que, a partir de la ecuación (5.14), tenemos \\(\\mathbf{He} = \\mathbf{H(I - H) \\boldsymbol{\\varepsilon}} = \\mathbf{0}\\), porque \\(\\mathbf{H} = \\mathbf{HH}\\). En particular, desde la fila \\(i\\)-ésima de \\(\\mathbf{He} = \\mathbf{0}\\), tenemos \\(\\sum_{j=1}^{n} h_{ij} e_j = 0\\). Ahora, usando las ecuaciones (5.17) y (5.20), tenemos \\[ \\begin{array}{ll} \\sum_{j \\ne i} \\left( y_j - \\mathbf{x}_j^{\\prime} \\mathbf{b}_{(i)} \\right)^2 &amp;= \\sum_{j=1}^{n} \\left( y_j - \\mathbf{x}_j^{\\prime} \\mathbf{b}_{(i)} \\right)^2 - \\left( y_i - \\mathbf{x}_i^{\\prime} \\mathbf{b}_{(i)} \\right)^2 \\\\ &amp;= \\sum_{j=1}^{n} \\left( y_j - \\mathbf{x}_j^{\\prime} \\mathbf{b} + \\frac{\\mathbf{x}_j^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}} \\right) - e_{(i)}^2 \\\\ &amp;= \\sum_{j=1}^{n} \\left( e_j + \\frac{h_{ij} e_i}{1 - h_{ii}} \\right)^2 - \\frac{e_i^2}{(1 - h_{ii})^2} \\\\ &amp;= \\sum_{j=1}^{n} e_j^2 + 0 + \\frac{e_i^2}{(1 - h_{ii})^2} h_{ii} - \\frac{e_i^2}{(1 - h_{ii})^2} \\\\ &amp;= \\sum_{j=1}^{n} e_j^2 - \\frac{e_i^2}{1 - h_{ii}} = (n - (k + 1)) s^2 - \\frac{e_i^2}{1 - h_{ii}}. \\end{array} \\] Esto establece la ecuación (5.21). 5.10.3 Omisión de Variables Notación. Para medir el efecto en las cantidades de regresión, hay una serie de estadísticas de interés basadas en la noción de omitir una variable explicativa. A tal fin, la notación de superíndice \\((j)\\) significa omitir la \\(j\\)-ésima variable, donde \\(j=0,1,\\ldots,k\\). Primero, recuerda que \\(\\mathbf{x}^{j} = (x_{1j}, x_{2j}, \\ldots, x_{nj})^{\\prime}\\) es la columna que representa la \\(j\\)-ésima variable. Además, define \\(\\mathbf{X}^{(j)}\\) como la matriz \\(n \\times k\\) de variables explicativas definida al eliminar \\(\\mathbf{x}^{j}\\) de \\(\\mathbf{X}\\). Por ejemplo, tomando \\(j=k\\), a menudo particionamos \\(\\mathbf{X}\\) como \\(\\mathbf{X} = \\left( \\mathbf{X}^{(k)}: \\mathbf{x}^k \\right)\\). Usando los resultados de la Sección 4.7.2, utilizaremos \\(\\mathbf{X}^{(k)} = \\mathbf{X}_1\\) y \\(\\mathbf{x}^k = \\mathbf{X}_2\\). Factor de Inflación de la Varianza. Primero, nos gustaría establecer la relación entre la definición del error estándar de \\(b_j\\) dada por \\[ se(b_j) = s \\sqrt{(j+1)\\text{ésimo elemento diagonal de }(\\mathbf{X}^{\\prime}\\mathbf{X})^{-1}} \\] y la relación que involucra el factor de inflación de la varianza, \\[ se(b_j) = s \\frac{\\sqrt{VIF_j}}{s_{x_j}\\sqrt{n-1}}. \\] Por simetría de las variables independientes, solo necesitamos considerar el caso donde \\(j=k\\). Así, nos gustaría establecer \\[\\begin{equation} (k+1)\\text{ésimo elemento diagonal de }(\\mathbf{X}^{\\prime}\\mathbf{X})^{-1} = \\frac{VIF_{k}}{(n-1) s_{x_{k}}^2}. \\tag{5.22} \\end{equation}\\] Primero considera el modelo reparametrizado en la ecuación (4.22). A partir de la ecuación (4.23), podemos expresar la estimación del coeficiente de regresión \\[ b_{k} = \\frac{\\mathbf{e}_1^{\\prime}\\mathbf{y}}{\\mathbf{e}_1^{\\prime}\\mathbf{e}_1}. \\] De la ecuación (4.23), tenemos que \\(\\text{Var} \\, b_{k} = \\sigma^2 (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1}\\) y así \\[\\begin{equation} se(b_{k}) = s (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1/2}. \\tag{5.23} \\end{equation}\\] Así, \\((\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1}\\) es el \\((k+1)\\)-ésimo elemento diagonal de \\[ \\left( \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{E}_2^{\\prime} \\end{bmatrix} \\begin{bmatrix} \\mathbf{X}_1 &amp; \\mathbf{E}_2 \\end{bmatrix} \\right)^{-1} \\] y también es el \\((k+1)\\)-ésimo elemento diagonal de \\((\\mathbf{X}^{\\prime} \\mathbf{X})^{-1}\\). Alternativamente, esto se puede verificar directamente utilizando la inversa de la matriz particionada en la ecuación (4.19). Ahora, supongamos que realizamos una regresión usando \\(\\mathbf{x}^{k} = \\mathbf{X}_2\\) como el vector de respuesta y \\(\\mathbf{X}^{(k)} = \\mathbf{X}_1\\) como la matriz de variables explicativas. Como se anotó arriba en la ecuación (4.22), \\(\\mathbf{E}_2\\) representa los “residuos” de esta regresión y así \\(\\mathbf{E}_2^{\\prime} \\mathbf{E}_2\\) representa la suma de cuadrados del error. Para esta regresión, la suma total de cuadrados es \\[ \\sum_{i=1}^{n} (x_{ik} - \\bar{x}_{k})^2 = (n-1) s_{x_{k}}^2 \\] y el coeficiente de determinación es \\(R_{k}^2\\). Así, \\[ \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 = Error ~SS = Total ~SS (1 - R_{k}^2) = \\frac{(n-1) s_{x_{k}}^2}{VIF_{k}}. \\] Esto establece el resultado. Estableciendo \\(t^2 = F\\). Para probar la hipótesis nula \\(H_0\\): \\(\\beta_{k} = 0\\), el material en la Sección 3.4.1 proporciona una descripción de una prueba basada en el estadístico \\(t\\), \\(t(b_{k}) = \\frac{b_{k}}{se(b_{k})}\\). Un procedimiento alternativo de prueba, descrito en las Secciones 4.2.2, utiliza el estadístico de prueba \\[ F-\\text{ratio} = \\frac{(Error ~SS)_{reducido} - (Error ~SS)_{completo}}{p \\times (Error~MS)_{completo}} = \\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right)^2}{s^2 \\mathbf{E}_2^{\\prime} \\mathbf{E}_2} \\] de la ecuación (4.26). Alternativamente, a partir de las ecuaciones (4.23) y (5.23), tenemos \\[\\begin{equation} t(b_{k}) = \\frac{b_{k}}{se(b_{k})} = \\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right) / \\left( \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 \\right)}{s / \\sqrt{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2}} = \\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right)}{s \\sqrt{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2}}. \\tag{5.24} \\end{equation}\\] Así, \\(t(b_{k})^2 = F\\)-ratio. Coeficientes de Correlación Parcial. A partir del modelo de regresión completo \\[ \\mathbf{y} = \\mathbf{X}^{(k)} \\boldsymbol{\\beta}^{(k)} + \\mathbf{x}_{k} \\beta_{k} + \\boldsymbol{\\varepsilon}, \\] considera dos regresiones separadas. Una regresión usando \\(\\mathbf{x}^{k}\\) como el vector de respuesta y \\(\\mathbf{X}^{(k)}\\) como la matriz de variables explicativas produce los residuos \\(\\mathbf{E}_2\\). De manera similar, una regresión con \\(\\mathbf{y}\\) como el vector de respuesta y \\(\\mathbf{X}^{(k)}\\) como la matriz de variables explicativas produce los residuos \\[ \\mathbf{E}_1 = \\mathbf{y} - \\mathbf{X}^{(k)} \\left( \\mathbf{X}^{(k)\\prime} \\mathbf{X}^{(k)} \\right)^{-1} \\mathbf{X}^{(k)} \\mathbf{y}. \\] Si \\(x^{0} = (1,1,\\ldots,1)^{\\prime}\\), entonces el promedio de \\(\\mathbf{E}_1\\) y \\(\\mathbf{E}_2\\) es cero. En este caso, la correlación muestral entre \\(\\mathbf{E}_1\\) y \\(\\mathbf{E}_2\\) es \\[ r(\\mathbf{E}_1, \\mathbf{E}_2) = \\frac{\\sum_{i=1}^{n} E_{1i} E_{2i}}{\\sqrt{\\left( \\sum_{i=1}^{n} E_{1i}^2 \\right) \\left( \\sum_{i=1}^{n} E_{2i}^2 \\right)}} = \\frac{\\mathbf{E}_1^{\\prime} \\mathbf{E}_2}{\\sqrt{\\left( \\mathbf{E}_1^{\\prime} \\mathbf{E}_1 \\right) \\left( \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 \\right)}}. \\] Como \\(\\mathbf{E}_2\\) es un vector de residuos usando \\(\\mathbf{X}^{(k)}\\) como la matriz de variables explicativas, tenemos que \\(\\mathbf{E}_2^{\\prime} \\mathbf{X}^{(k)} = 0\\). Así, para el numerador, tenemos \\[ \\mathbf{E}_2^{\\prime} \\mathbf{E}_1 = \\mathbf{E}_2^{\\prime} \\left( \\mathbf{y} - \\mathbf{X}^{(k)} \\left( \\mathbf{X}^{(k)\\prime} \\mathbf{X}^{(k)} \\right)^{-1} \\mathbf{X}^{(k)} \\mathbf{y} \\right) = \\mathbf{E}_2^{\\prime} \\mathbf{y}. \\] A partir de las ecuaciones (4.24) y (4.25), tenemos que \\[ (n - (k+1)) s^2 = (Error ~SS)_{completo} = \\mathbf{E}_1^{\\prime} \\mathbf{E}_1 - \\frac{\\left( \\mathbf{E}_1^{\\prime} \\mathbf{y} \\right)^2}{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2} = \\mathbf{E}_1^{\\prime} \\mathbf{E}_1 - \\frac{\\left( \\mathbf{E}_1^{\\prime} \\mathbf{E}_2 \\right)^2}{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2}. \\] Así, a partir de la ecuación (5.24) \\[ \\begin{array}{ll} \\frac{t(b_{k})}{\\sqrt{t(b_{k})^2 + n - (k+1)}} &amp;= \\frac{\\mathbf{E}_2^{\\prime} \\mathbf{y} / \\left(s \\sqrt{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2}\\right)}{\\sqrt{\\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right)^2}{s^2 \\mathbf{E}_2^{\\prime} \\mathbf{E}_2} + n - (k+1)}} \\\\ &amp; = \\frac{\\mathbf{E}_2^{\\prime} \\mathbf{y}}{\\sqrt{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right)^2 + \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 s^2 \\left(n - (k+1) \\right)}} \\\\ &amp; = \\frac{\\mathbf{E}_2^{\\prime} \\mathbf{E}_1}{\\sqrt{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{E}_1 \\right)^2 + \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 \\left( \\mathbf{E}_1^{\\prime} \\mathbf{E}_1 - \\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{E}_1 \\right)^2}{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2} \\right)}} \\\\ &amp;= \\frac{\\mathbf{E}_1^{\\prime} \\mathbf{E}_2}{\\sqrt{(\\mathbf{E}_1^{\\prime} \\mathbf{E}_1) (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)}} = r(\\mathbf{E}_1, \\mathbf{E}_2). \\end{array} \\] Esto establece la relación entre el coeficiente de correlación parcial y el estadístico \\(t\\)-ratio. "],["interpretación-de-resultados-de-regresión.html", "Capítulo 6 Interpretación de Resultados de Regresión 6.1 Lo que nos dice el proceso de modelado 6.2 La Importancia de la Selección de Variables 6.3 La Importancia de la Recolección de Datos 6.4 Modelos de Datos Faltantes 6.5 Aplicación: Eficiencia en el Costo de los Gestores de Riesgos 6.6 Lecturas Adicionales y Referencias 6.7 Ejercicios 6.8 Suplementos Técnicos para el Capítulo 6", " Capítulo 6 Interpretación de Resultados de Regresión Vista previa del capítulo. Un analista de regresión recopila datos, selecciona un modelo y luego informa sobre los hallazgos del estudio, en ese orden. Este capítulo considera estos tres temas en orden inverso, enfatizando cómo cada etapa del estudio está influenciada por los pasos precedentes. Una aplicación, determinar las características de una empresa que influyen en su efectividad para gestionar el riesgo, ilustra el proceso de modelado de regresión de principio a fin. Estudiar un problema utilizando un proceso de modelado de regresión implica un compromiso sustancial de tiempo y energía. Primero, uno debe adoptar el concepto de pensamiento estadístico, es decir, estar dispuesto a utilizar los datos activamente como parte de un proceso de toma de decisiones. En segundo lugar, uno debe apreciar la utilidad de un modelo que se usa para aproximar una situación real. Después de hacer este compromiso sustancial, hay una tendencia natural a “sobrevender” los resultados de métodos estadísticos como el análisis de regresión. Al sobrevender cualquier conjunto de ideas, los consumidores eventualmente se sienten decepcionados cuando los resultados no cumplen con sus expectativas. Este capítulo comienza en la Sección 6.1 resumiendo lo que podemos esperar aprender razonablemente del modelado de regresión. Los modelos están diseñados para ser mucho más simples que las relaciones entre entidades que existen en el mundo real. Un modelo es simplemente una aproximación de la realidad. Como dijo George Box (1979), “Todos los modelos son incorrectos, pero algunos son útiles”. Desarrollar el modelo, el tema del Capítulo 5, es parte del arte de la estadística. Aunque los principios de la selección de variables son ampliamente aceptados, la aplicación de estos principios puede variar considerablemente entre los analistas. El producto resultante tiene ciertos valores estéticos y de ninguna manera está predeterminado. La estadística se puede considerar como el arte de razonar con datos. La Sección 6.2 subrayará la importancia de la selección de variables. La formulación del modelo y la recopilación de datos forman la primera etapa del proceso de modelado. Los estudiantes de estadística suelen sorprenderse por la dificultad de relacionar ideas sobre relaciones con los datos disponibles. Estas dificultades incluyen la falta de datos fácilmente disponibles y la necesidad de usar ciertos datos como sustitutos de la información ideal que no está disponible numéricamente. La Sección 6.3 describirá varios tipos de dificultades que pueden surgir al recopilar datos. La Sección 6.4 describirá algunos modelos para aliviar estas dificultades. 6.1 Lo que nos dice el proceso de modelado La inferencia del modelo es la etapa final del proceso de modelado. Al estudiar el comportamiento de los modelos, esperamos aprender algo sobre el mundo real. Los modelos sirven para imponer un orden en la realidad y proporcionar una base para entender la realidad a través de la naturaleza del orden impuesto. Además, los modelos estadísticos se basan en el razonamiento con los datos disponibles de una muestra. Por lo tanto, los modelos sirven como una guía importante para predecir el comportamiento de observaciones fuera de la muestra disponible. 6.1.1 Interpretación de efectos individuales Al interpretar los resultados de una regresión múltiple, el objetivo principal es a menudo transmitir la importancia de las variables individuales, o efectos, sobre un resultado de interés. La interpretación depende de si los efectos son o no significativamente sustantivos, estadísticamente significativos y causales. Significado Sustantivo. Los lectores de un estudio de regresión primero quieren entender la dirección y magnitud de los efectos individuales. ¿Las mujeres tienen más o menos reclamaciones que los hombres en un estudio de reclamaciones de seguros? Si es menos, ¿cuánto menos? Puedes responder a estas preguntas a través de una tabla de coeficientes de regresión. Además, para dar una idea de la fiabilidad de las estimaciones, también puede ser útil incluir el error estándar o un intervalo de confianza, como se introdujo en la Sección 3.4.2. Recuerda que los coeficientes de regresión son estimaciones de las derivadas parciales de la función de regresión \\[ \\mathrm{E~}y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k. \\] Al interpretar los coeficientes para variables explicativas continuas, es útil hacerlo en términos de cambios significativos de cada \\(x\\). Por ejemplo, si la población es una variable explicativa, podemos hablar del cambio esperado en \\(y\\) por cada cambio de 1,000 o un millón en la población. Además, al interpretar los coeficientes de regresión, comenta sobre su significado “sustantivo”. Por ejemplo, supongamos que encontramos una diferencia en las reclamaciones entre hombres y mujeres, pero la diferencia estimada es solo del 1% de las reclamaciones esperadas. Esta diferencia puede ser estadísticamente significativa pero no económicamente relevante. La significación sustantiva se refiere a la importancia en el campo de estudio; en ciencia actuarial, esto es típicamente de relevancia financiera o económica, pero también podría ser no monetaria, como los efectos sobre la esperanza de vida futura. Significado Estadístico. ¿Son los efectos debidos al azar? La maquinaria de pruebas de hipótesis introducida en la Sección 3.4.1 proporciona un mecanismo formal para responder a esta pregunta. Las pruebas de hipótesis son útiles porque proporcionan un estándar formal y acordado para decidir si una variable hace una contribución importante a una respuesta esperada. Al interpretar los resultados, los investigadores suelen citar un \\(t\\)-ratio o un \\(p\\)-valor para demostrar significación estadística. En algunas situaciones, es de interés comentar sobre las variables que no son estadísticamente significativas. Los efectos que no son estadísticamente significativos tienen errores estándar que son grandes en relación con los coeficientes de regresión. En la Sección 5.5.2, expresamos este error estándar como \\[\\begin{equation} se(b_{j}) = s \\frac{\\sqrt{VIF_{j}}}{s_{x_{j}} \\sqrt{n-1}}. \\tag{6.1} \\end{equation}\\] Una posible explicación para la falta de significancia estadística es una gran variación en el término de perturbación. Al expresar el error estándar en esta forma, vemos que cuanto mayor es la variación natural, medida por \\(s\\), más difícil es rechazar la hipótesis nula de ningún efecto (\\(H_0\\)), manteniendo todo lo demás constante. Una segunda posible explicación para la falta de significancia estadística es la alta colinealidad, medida por \\(VIF_j\\). Una variable puede estar confundida con otras variables de manera que, a partir de los datos que se están analizando, sea imposible distinguir los efectos de una variable de otra. Una tercera posible explicación es el tamaño de la muestra. Supongamos que se utiliza un mecanismo similar a extracciones de una población estable para observar las variables explicativas. Entonces, la desviación estándar de \\(x_j\\), \\(s_{x_j}\\), debería ser estable a medida que aumenta el número de extracciones. De manera similar, también deberían serlo \\(R_j^2\\) y \\(s^2\\). Entonces, el error estándar \\(se(b_j)\\) debería disminuir a medida que el tamaño de la muestra, \\(n\\), aumenta. Por el contrario, un tamaño de muestra más pequeño significa un error estándar mayor, manteniendo todo lo demás constante. Esto significa que es posible que no podamos detectar la importancia de las variables en muestras de tamaño pequeño o moderado. Por lo tanto, en un mundo ideal, si no se detecta significancia estadística donde se había hipotetizado (y completamente esperado), se podría: (i) obtener una medida más precisa de \\(y\\), reduciendo así su variabilidad natural, (ii) rediseñar el esquema de recolección de muestras para que las variables explicativas relevantes sean menos redundantes y (iii) recopilar más datos. Normalmente, estas opciones no están disponibles con datos observacionales, pero puede ser útil señalar los próximos pasos en un programa de investigación. Los analistas ocasionalmente observan relaciones estadísticamente significativas que no se anticiparon; esto podría deberse a un tamaño de muestra grande. Anteriormente, mencionamos que una muestra pequeña puede no proporcionar suficiente información para detectar relaciones significativas. La otra cara de este argumento es que, para muestras grandes, tenemos la oportunidad de detectar la importancia de variables que podrían pasar desapercibidas en muestras de tamaño pequeño o incluso moderado. Desafortunadamente, esto también significa que las variables con coeficientes de parámetro pequeños, que contribuyen poco a entender la variación en la respuesta, pueden ser juzgadas como significativas utilizando nuestros procedimientos de toma de decisiones. Esto sirve para resaltar la diferencia entre significancia sustantiva y estadística: en particular, para muestras grandes, los investigadores encuentran variables que son estadísticamente significativas pero prácticamente poco importantes. En estos casos, puede ser prudente que el investigador omita variables de la especificación del modelo cuando su presencia no esté de acuerdo con la teoría aceptada, incluso si se consideran estadísticamente significativas. Efectos Causales. Si cambiamos \\(x\\), ¿cambiaría \\(y\\)? Como estudiantes de ciencias básicas, aprendimos principios que involucran acciones y reacciones. Agregar masa a una bola en movimiento aumenta la fuerza de su impacto contra una pared. Sin embargo, en las ciencias sociales, las relaciones son probabilísticas, no deterministas, y por lo tanto más sutiles. Por ejemplo, a medida que la edad (\\(x\\)) aumenta, la probabilidad de morir en un año (\\(y\\)) aumenta para la mayoría de las curvas de mortalidad humana. Comprender la causalidad, incluso la probabilística, es la raíz de toda la ciencia y proporciona la base para la toma de decisiones informada. Es importante reconocer que los procesos causales generalmente no pueden demostrarse exclusivamente a partir de los datos; los datos solo pueden presentar evidencia empírica relevante que sirva como un eslabón en una cadena de razonamiento sobre los mecanismos causales. Para la causalidad, hay tres condiciones necesarias: (i) asociación estadística entre variables, (ii) orden temporal apropiado y (iii) la eliminación de hipótesis alternativas o el establecimiento de un mecanismo causal formal. Como ejemplo, recordemos el estudio de Galton en la Sección 1.1, que relaciona la altura de los hijos adultos (\\(y\\)) con un índice de la altura de los padres (\\(x\\)). Para este estudio, estaba claro que hay una fuerte asociación estadística entre \\(x\\) e \\(y\\). La demografía también deja claro que las mediciones de los padres (\\(x\\)) preceden a las mediciones de los hijos (\\(y\\)). Lo que no es seguro es el mecanismo causal. Por ejemplo, en la Sección 1.5, mencionamos la posibilidad de que una variable omitida, como la dieta familiar, podría estar influyendo tanto en \\(x\\) como en \\(y\\). Se necesitan evidencias y teorías de la biología humana y la genética para establecer un mecanismo causal formal. Ejemplo: Raza, Redlining y Precios del Seguro de Automóviles. En un artículo con este título, Harrington y Niehaus (1998) investigaron si las compañías de seguros participaban en conductas discriminatorias (raciales), conocidas comúnmente como redlining. La discriminación racial es ilegal y las compañías de seguros no pueden usar la raza para determinar los precios. El término redlining se refiere a la práctica de trazar líneas rojas en un mapa para indicar áreas que las aseguradoras no cubrirán, áreas que típicamente contienen una alta proporción de minorías. Para investigar si existe o no discriminación racial en los precios del seguro, Harrington y Niehaus recopilaron datos de primas y reclamaciones de seguros de automóviles de pasajeros privados del Departamento de Seguros de Missouri para el período 1988-1992. Aunque las compañías de seguros no mantienen información sobre raza/etnicidad en sus datos de primas y reclamaciones, dicha información está disponible a nivel de código postal en la Oficina del Censo de EE. UU. Al agregar las primas y las reclamaciones al nivel de código postal, Harrington y Niehaus pudieron evaluar si las áreas con un mayor porcentaje de población negra pagaban más por el seguro (PCTBLACK). Una medida de precios ampliamente utilizada es la razón de siniestralidad, definida como la relación entre reclamaciones y primas. Esta medida la rentabilidad de las aseguradoras; si existe discriminación racial en los precios, se esperaría ver una baja razón de siniestralidad en áreas con una alta proporción de minorías. Harrington y Niehaus usaron esto como la variable dependiente, después de tomar logaritmos para abordar la asimetría en la distribución de la razón de siniestralidad. Harrington y Niehaus (1998) estudiaron 270 códigos postales alrededor de seis ciudades principales en Missouri, donde había grandes concentraciones de minorías. La Tabla 6.1 presenta los hallazgos de la cobertura comprensiva, aunque los autores también investigaron la cobertura de colisión y de responsabilidad civil. Además de la variable principal de interés, PCTBLACK, se introdujeron algunas variables de control relacionadas con la distribución por edades (PCT1824 y PCT55UP), estado civil (MARRIED), población (ln TOTPOP) y empleo (PCTUNEMP). El tamaño de la póliza se midió indirectamente a través del valor promedio del automóvil (ln AVCARV). La Tabla 6.1 informa que solo el tamaño de la póliza y la población son determinantes estadísticamente significativos de las razones de siniestralidad. De hecho, el coeficiente asociado con PCTBLACK tiene un signo positivo, lo que indica que las primas son más bajas en áreas con altas concentraciones de minorías (aunque no significativo). En un mercado de seguros eficiente, esperaríamos que los precios estuvieran estrechamente alineados con las reclamaciones y que existieran pocos patrones generales. Tabla 6.1: Resultados de la Regresión de la Razón de Siniestralidad Variable Descripción Coeficiente de Regresión \\(t\\)-Estadístico Intercept 1.98 2.73 PCTBLACK Proporción de la población negra 0.11 0.63 ln TOTPOP Logaritmo de la población total -0.1 -4.43 PCT1824 Porcentaje de la población entre 18 y 24 años -0.23 -0.5 PCT55UP Porcentaje de la población de 55 años o más -0.47 -1.76 MARRIED Porcentaje de la población casada -0.32 -0.9 PCTUNEMP Porcentaje de la población desempleada 0.11 0.1 ln AVCARV Logaritmo del valor promedio del automóvil asegurado -0.87 -3.26 Fuente: Harrington y Niehaus (1998) \\(R_a^2\\) 0.11 Ciertamente, los hallazgos de Harrington y Niehaus (1998) son inconsistentes con la hipótesis de discriminación racial en los precios. Establecer una falta de significancia estadística suele ser más difícil que establecer significancia. En el artículo de Harrington y Niehaus (1998), hay muchas especificaciones de modelos alternativas que evalúan la robustez de sus hallazgos frente a diferentes procedimientos de selección de variables y diferentes subconjuntos de datos. La Tabla 6.1 presenta los estimadores de los coeficientes y los \\(t\\)-ratios calculados utilizando mínimos cuadrados ponderados, con el tamaño de la población como pesos. Los autores también utilizaron mínimos cuadrados (ordinarios) con errores estándar robustos, obteniendo resultados similares. 6.1.2 Otras Interpretaciones Cuando se toman colectivamente, las combinaciones lineales de los coeficientes de regresión pueden interpretarse como la función de regresión: \\[ \\mathrm{E~}y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k. \\] Al presentar los resultados de regresión, los lectores quieren saber qué tan bien el modelo se ajusta a los datos. La Sección 5.6.1 resumió varias estadísticas de bondad de ajuste que se informan rutinariamente en investigaciones de regresión. Función de Regresión y Precios. Al evaluar los datos de reclamaciones de seguros, la función de regresión representa las reclamaciones esperadas y, por lo tanto, forma la base de la función de precios. (Vea el ejemplo en el Capítulo 4). En este caso, la forma de la función de regresión y los niveles para combinaciones clave de variables explicativas son de interés. Estudios de Referencia (Benchmarking). En algunas investigaciones, el propósito principal puede ser determinar si una observación específica está “en línea” con las otras disponibles. Por ejemplo, en el Capítulo 20 examinaremos los salarios de los CEOs. El propósito principal de dicho análisis podría haber sido ver si el salario de una persona es alto o bajo en comparación con otros en la muestra, controlando por características como la industria y los años de experiencia. El residuo resume la desviación de la respuesta respecto a la esperada según el modelo. Si el residuo es inusualmente grande o pequeño, entonces interpretamos esto como que existen circunstancias inusuales asociadas con esta observación. Este análisis no sugiere la naturaleza ni las causas de estas circunstancias; simplemente indica que la observación es inusual en comparación con las demás en la muestra. Para algunas investigaciones, como en litigios relacionados con paquetes de compensación, esta es una declaración poderosa. Predicción. Muchas aplicaciones actuariales conciernen a la predicción, donde el interés radica en describir la distribución de una variable aleatoria que aún no se ha realizado. Al establecer reservas, los actuarios de compañías de seguros están estableciendo pasivos para futuras reclamaciones que predicen que se realizarán y, por lo tanto, se convertirán en gastos eventuales de la compañía. La predicción, o pronóstico, es la principal motivación de la mayoría de los análisis de datos de series temporales, que se trata en los Capítulos 7-10. La predicción de una sola variable aleatoria en el contexto de la regresión lineal múltiple se introdujo en la Sección 4.2.3. Aquí, asumimos que tenemos disponible un conjunto dado de características, \\(\\mathbf{x}_{\\ast}=(1,x_{\\ast 1},\\ldots,x_{\\ast k})^{\\prime }\\). Según nuestro modelo, la nueva respuesta es: \\[ y_{\\ast}=\\beta_0 + \\beta_1 x_{\\ast 1} + \\cdots + \\beta_k x_{\\ast k} + \\varepsilon_{\\ast}. \\] Utilizamos como nuestro predictor puntual: \\[ \\hat{y}_{\\ast}=b_{0} + b_{1} x_{\\ast 1} + \\cdots + b_{k} x_{\\ast k}. \\] Como en la Sección 2.5.3, podemos descomponer el error de predicción en el error de estimación más el error aleatorio, de la siguiente manera: \\[ \\begin{array}{ccccc} \\underbrace{y^{\\ast}-\\widehat{y}^{\\ast}} &amp; = &amp; \\underbrace{\\beta_0 - b_{0} + (\\beta_1 - b_{1})x_{\\ast 1} + \\cdots + (\\beta_k - b_{k})x_{\\ast k}} &amp; + &amp; \\underbrace{\\varepsilon ^{\\ast}} \\\\ {\\small \\text{error de predicción}} &amp; {\\small =} &amp; {\\small \\text{error en la estimación de la} } &amp; {\\small +} &amp; {\\small \\text{desviación} }\\\\ &amp; &amp; {\\small \\text{función de regresión en } x_{\\ast 1}, \\ldots, x_{\\ast k}} &amp; &amp; {\\small \\text{adicional} } \\end{array} \\] Esta descomposición nos permite proporcionar una distribución para el error de predicción. Es habitual asumir una normalidad aproximada. Con esta suposición adicional, resumimos esta distribución utilizando un intervalo de predicción \\[\\begin{equation} \\hat{y}_{\\ast} \\pm t_{n-(k+1),1-\\alpha /2} ~ se(pred), \\tag{6.2} \\end{equation}\\] donde \\[ se(pred) = s \\sqrt{1 + \\mathbf{x}_{\\ast}^{\\prime }(\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{x}_{\\ast}}. \\] Aquí, el valor \\(t\\) \\(t_{n-(k+1),1-\\alpha /2}\\) es un percentil de la distribución \\(t\\) con \\(df=n-(k+1)\\) grados de libertad. Esto extiende la ecuación (2.7). Comunicar el rango de resultados probables es un objetivo importante. Al analizar datos, puede haber varias técnicas alternativas de predicción disponibles. Incluso dentro de la clase de modelos de regresión, cada uno de los varios modelos candidatos producirá una predicción diferente. Es importante proporcionar una distribución o rango de posibles errores. Los consumidores ingenuos pueden desilusionarse fácilmente con los resultados de las predicciones de los modelos de regresión. A estos consumidores se les dice (correctamente) que el modelo de regresión es óptimo, basado en ciertos criterios bien definidos, y luego se les proporciona una predicción puntual, como \\(\\hat{y}_{\\ast}\\). Sin conocimiento de un intervalo, el consumidor tiene expectativas sobre el rendimiento de la predicción, generalmente más altas de lo que justifica la información disponible en la muestra. Un intervalo de predicción no solo proporciona una única predicción óptima puntual, sino también un rango de fiabilidad. Al hacer las predicciones, hay una suposición importante: la nueva observación sigue el mismo modelo que se utilizó en la muestra. Por lo tanto, las condiciones básicas sobre la distribución de los errores deben permanecer sin cambios para las nuevas observaciones. También es importante que el nivel de las variables predictoras, \\(x_{\\ast 1},\\ldots,x_{\\ast k}\\), sea similar al de las observaciones disponibles en la muestra. Si una o varias de las variables predictoras difieren drásticamente de las de la muestra disponible, entonces la predicción resultante puede ser inadecuada. Por ejemplo, sería imprudente usar el modelo desarrollado en las Secciones 2.1 a 2.3 para predecir la lotería de una región con una población de \\(x_{\\ast}=400,000\\), más de diez veces la mayor población en nuestra muestra. Aunque sería fácil introducir \\(x_{\\ast}=400,000\\) en nuestras fórmulas, el resultado tendría poco sentido intuitivo. Extrapolar relaciones más allá de los datos observados requiere experiencia tanto en la naturaleza de los datos como en la metodología estadística. En la Sección 6.3, identificaremos este problema como un sesgo potencial debido a la región de muestreo. 6.2 La Importancia de la Selección de Variables Por un lado, elegir un modelo teórico que represente exactamente los eventos del mundo real es probablemente una tarea imposible. Por otro lado, elegir un modelo que represente aproximadamente el mundo real es un asunto práctico importante. Cuanto más cerca esté nuestro modelo del mundo real, más precisas serán las afirmaciones que hagamos, sugeridas por el modelo. Aunque no podemos obtener el modelo correcto, podemos seleccionar un modelo útil o al menos adecuado. Los usuarios de la estadística, desde el principiante hasta el experto experimentado, siempre seleccionarán un modelo inadecuado de vez en cuando. La pregunta clave es: ¿Qué tan importante es seleccionar un modelo adecuado? Aunque no se puede prever cada tipo de error, hay algunos principios orientadores que son útiles tener en cuenta al seleccionar un modelo. 6.2.1 Sobreajuste del Modelo Este tipo de error ocurre cuando se añaden variables superfluas o extrañas al modelo especificado. Si solo se añaden un pequeño número de variables extrañas, como una o dos, entonces este tipo de error probablemente no distorsionará de manera significativa la mayoría de los tipos de conclusiones que puedan alcanzarse con el modelo ajustado. Por ejemplo, sabemos que cuando añadimos una variable al modelo, la suma de cuadrados de los errores no aumenta. Si la variable es extraña, entonces la suma de cuadrados de los errores tampoco disminuirá de manera apreciable. De hecho, añadir una variable extraña puede aumentar \\(s^2\\) porque el denominador es más pequeño por un grado de libertad. Sin embargo, para conjuntos de datos de tamaño de muestra moderado, el efecto es mínimo. Sin embargo, añadir varias variables extrañas puede inflar \\(s^{2}\\) de manera apreciable. Además, existe la posibilidad de que añadir variables explicativas extrañas induzca, o empeore, la presencia de colinealidad. Un punto más importante es que, al añadir variables extrañas, nuestras estimaciones de los coeficientes de regresión permanecen insesgadas. Considere el siguiente ejemplo. Ejemplo: Regresión usando una Variable Explicativa. Suponga que el modelo verdadero de las respuestas es \\[ y_i = \\beta_0 + \\varepsilon_i, \\quad i = 1, \\ldots, n. \\] Bajo este modelo, el nivel de una variable explicativa genérica \\(x\\) no afecta el valor de la respuesta \\(y\\). Si fuéramos a predecir la respuesta en cualquier nivel de \\(x\\), la predicción tendría un valor esperado de \\(\\beta_0\\). Sin embargo, supongamos que equivocadamente ajustamos el modelo \\[ y_i = \\beta_0^{\\ast} + \\beta_1^{\\ast}x_i + \\varepsilon_i^{\\ast}. \\] Con este modelo, la predicción en un nivel genérico \\(x\\) es \\(b_{0}^{\\ast} + b_{1}^{\\ast}x\\) donde \\(b_{0}^{\\ast}\\) y \\(b_{1}^{\\ast}\\) son las estimaciones de mínimos cuadrados ordinarios de \\(\\beta_0^{\\ast}\\) y \\(\\beta_1^{\\ast}\\), respectivamente. No es demasiado difícil confirmar que \\[ \\text{Sesgo} = \\text{E}(b_{0}^{\\ast} + b_{1}^{\\ast}x) - \\text{E}y = 0, \\] donde las esperanzas se calculan usando el modelo verdadero. Por lo tanto, al usar un modelo ligeramente más grande del que deberíamos, no pagamos en términos de cometer un error persistente a largo plazo, como el representado por el sesgo. El precio de cometer este error es que nuestro error estándar es ligeramente mayor de lo que sería si hubiéramos elegido el modelo correcto. 6.2.2 Subajuste del Modelo Este tipo de error ocurre cuando se omiten variables importantes en la especificación del modelo; es más grave que el sobreajuste. Omitir variables importantes puede causar cantidades apreciables de sesgo en nuestras estimaciones resultantes. Además, debido a este sesgo, las estimaciones resultantes de \\(s^{2}\\) son más grandes de lo necesario. Un \\(s\\) más grande infla nuestros intervalos de predicción y produce pruebas inexactas de hipótesis sobre la importancia de las variables explicativas. Para ver los efectos de subajustar un modelo, volvemos al ejemplo anterior. Ejemplo: Regresión usando una Variable Explicativa - Continuación. Ahora invertimos los roles de los modelos descritos antes. Supongamos que el modelo verdadero es \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\] y que ajustamos erróneamente el modelo, \\[ y_i = \\beta_0^{\\ast} + \\varepsilon_i^{\\ast}. \\] Por lo tanto, hemos omitido inadvertidamente los efectos de la variable explicativa \\(x\\). Con el modelo ajustado, usaríamos \\(\\bar{y}\\) para nuestra predicción en un nivel genérico de \\(x\\). A partir del modelo verdadero, tenemos \\(\\bar{y} = \\beta_0 + \\beta_1 \\bar{x} + \\bar{\\varepsilon}\\). El sesgo de la predicción en \\(x\\) es \\[ \\begin{array}{ll} \\text{Sesgo} &amp;= \\text{E} \\bar{y} - \\text{E} (\\beta_0 + \\beta_1 x + \\varepsilon) \\\\ &amp;= \\text{E} (\\beta_0 + \\beta_1 \\bar{x} + \\bar{\\varepsilon}) - (\\beta_0 + \\beta_1 x) \\\\ &amp;= \\beta_1 (\\bar{x} - x). \\end{array} \\] Si \\(\\beta_1\\) es positivo, entonces subestimamos para valores grandes de \\(x\\), resultando en un sesgo negativo, y sobreestimamos para valores pequeños de \\(x\\) (en relación con \\(\\overline{x}\\)). Así, hay un error persistente a largo plazo al omitir la variable explicativa \\(x\\). De manera similar, se puede verificar que este tipo de error produce estimaciones sesgadas de los parámetros de regresión y un valor inflado de \\(s^{2}\\). Por supuesto, nadie quiere sobreajustar o subajustar el modelo. Sin embargo, los datos de las ciencias sociales a menudo son desordenados y puede ser difícil saber si incluir o no una variable en el modelo. Al seleccionar variables, los analistas a menudo se guían por el principio de parsimonia, también conocido como la Navaja de Occam, que establece que cuando hay varias explicaciones posibles para un fenómeno, se debe usar la más simple. Hay varios argumentos para preferir modelos más simples: Una explicación más simple es más fácil de interpretar. Los modelos simples, también conocidos como modelos “parsimoniosos”, a menudo funcionan bien con datos fuera de la muestra. Las variables superfluas pueden causar problemas de colinealidad, lo que dificulta la interpretación de los coeficientes individuales. El punto de vista opuesto se puede resumir en una cita a menudo atribuida a Albert Einstein, que dice que debemos usar “el modelo más simple posible, pero no más simple”. Esta sección demuestra que subajustar un modelo, omitiendo variables importantes, es típicamente un error más grave que incluir variables superfluas que aportan poco a nuestra capacidad de explicar los datos. Incluir variables superfluas disminuye los grados de libertad y aumenta la estimación de la variabilidad, lo cual suele ser de menor preocupación en las aplicaciones actuariales. En caso de duda, deje la variable en el modelo. 6.3 La Importancia de la Recolección de Datos El proceso de modelado de regresión comienza con la recolección de datos. Habiendo estudiado los resultados y el proceso de selección de variables, ahora podemos discutir las entradas al proceso. No es sorprendente que haya una larga lista de posibles dificultades que se encuentran con frecuencia al recolectar datos para regresión. En esta sección, identificamos las principales dificultades potenciales y proporcionamos algunas vías para evitar estas dificultades. 6.3.1 Error en el Marco Muestral y Selección Adversa El error en el marco muestral ocurre cuando el marco muestral, la lista de la cual se extrae la muestra, no es una aproximación adecuada de la población de interés. Al final, una muestra debe ser un subconjunto representativo de una población más grande, o universo, de interés. Si la muestra no es representativa, tomar una muestra más grande no elimina el sesgo; simplemente repite el mismo error una y otra vez. Ejemplo: Encuesta de Literary Digest. Quizás el ejemplo más conocido de error en el marco muestral es de la encuesta de Literary Digest en 1936. Esta encuesta se realizó para predecir el ganador de las elecciones presidenciales de Estados Unidos en 1936. Los dos principales candidatos eran Franklin D. Roosevelt, el demócrata, y Alfred Landon, el republicano. Literary Digest, una revista prominente en ese momento, realizó una encuesta a diez millones de votantes. De los encuestados, 2.4 millones respondieron, prediciendo una victoria “arrolladora” de Landon por un margen de 57% a 43%. Sin embargo, la elección real resultó en una victoria abrumadora de Roosevelt, por un margen de 62% a 38%. ¿Qué salió mal? Hubo varios problemas con la encuesta de Literary Digest. Quizás el más importante fue el error en el marco muestral. Para desarrollar su marco muestral, Literary Digest utilizó direcciones de guías telefónicas y listas de membresía de clubes. En 1936, Estados Unidos estaba en el fondo de la Gran Depresión; los teléfonos y las membresías de clubes eran un lujo que solo las personas de altos ingresos podían permitirse. Así, la lista de Literary Digest incluía una cantidad no representativa de personas de altos ingresos. En elecciones presidenciales anteriores realizadas por Literary Digest, los ricos y los pobres tendían a votar de manera similar y esto no era un problema. Sin embargo, los problemas económicos fueron los principales temas políticos en las elecciones presidenciales de 1936. Como resultó ser, los pobres tendían a votar por Roosevelt y los ricos por Landon. Como resultado, los resultados de la encuesta de Literary Digest fueron gravemente erróneos. Tomar una muestra grande, incluso de tamaño 2.4 millones, no ayudó; el error básico se repitió una y otra vez. El sesgo en el marco muestral ocurre cuando la muestra no es un subconjunto representativo de la población de interés. Al analizar los datos de las compañías de seguros, este sesgo puede surgir debido a la selección adversa. En muchos mercados de seguros, las compañías diseñan y fijan los precios de los contratos y los asegurados deciden si desean o no entrar en un acuerdo contractual (de hecho, los asegurados “solicitan” un seguro, por lo que los aseguradores también tienen el derecho de no entrar en el acuerdo). Por lo tanto, alguien es más probable que entre en un acuerdo si cree que la aseguradora está subestimando su riesgo, especialmente a la luz de las características del asegurado que no son observadas por la aseguradora. Por ejemplo, es bien sabido que la experiencia de mortalidad de una muestra de compradores de rentas vitalicias no es representativa de la población en general; las personas que compran rentas vitalicias tienden a ser saludables en relación con la población general. No compraría una renta vitalicia que pague un beneficio periódico mientras esté vivo si tuviera mala salud y pensara que su probabilidad de una larga vida es baja. La selección adversa surge porque los “malos riesgos”, aquellos con reclamos mayores de los esperados, son más propensos a entrar en contratos que los “buenos riesgos” correspondientes. Aquí, la expectativa se desarrolla en función de características (variables explicativas) que pueden ser observadas por la aseguradora. Por supuesto, existe un gran mercado para las rentas vitalicias y otras formas de seguros en las que existe selección adversa. Las compañías de seguros pueden fijar precios adecuados para estos mercados redefiniendo su “población de interés” para que no sea la población general, sino la población de asegurados potenciales. Así, por ejemplo, al fijar el precio de las rentas vitalicias, las aseguradoras utilizan datos de mortalidad de rentistas, no datos de la población general. De esta manera, pueden evitar posibles desajustes entre la población y la muestra. Más generalmente, la experiencia de casi cualquier compañía difiere de la población general debido a los estándares de suscripción y las filosofías de ventas. Algunas compañías buscan “riesgos preferidos” ofreciendo descuentos educativos, bonos por buen manejo, etc., mientras que otras buscan asegurados de alto riesgo. La muestra de asegurados de la compañía diferirá de la población general y el grado de la diferencia puede ser un aspecto interesante para cuantificar en un análisis. El sesgo en el marco muestral puede ser particularmente importante cuando una compañía busca comercializar un nuevo producto para el cual no tiene datos de experiencia. Identificar un mercado objetivo y su relación con la población general es un aspecto importante de un plan de desarrollo de mercado. 6.3.2 Regiones de Muestreo Limitadas Una región de muestreo limitada puede dar lugar a un sesgo potencial cuando intentamos extrapolar fuera de la región de muestreo. Para ilustrarlo, considere la Figura 6.1. Aquí, con base en los datos de la región de muestreo, una línea puede parecer una representación apropiada. Sin embargo, si una curva cuadrática es la verdadera respuesta esperada, cualquier pronóstico que esté lejos de la región de muestreo estará seriamente sesgado. Figura 6.1: La extrapolación fuera de la región de muestreo puede estar sesgada Otro problema debido a una región de muestreo limitada, aunque no sea un sesgo, que puede surgir es la dificultad para estimar un coeficiente de regresión. En el Capítulo 5, vimos que una menor dispersión de una variable, ceteris paribus, significa una estimación menos confiable del coeficiente de pendiente asociado con esa variable. Es decir, a partir de la Sección 5.5.2 o de la ecuación (6.1), vemos que cuanto menor es la dispersión de \\(x_{j}\\), medida por \\(s_{x_{j}}\\), mayor es el error estándar de \\(b_{j},se(b_{j})\\). Llevado al extremo, donde \\(s_{x_{j}}=0\\), podríamos tener una situación como la ilustrada en la Figura 6.2. Para la situación extrema ilustrada en la Figura 6.2, no hay suficiente variación en \\(x\\) para estimar el parámetro de pendiente correspondiente. Figura 6.2: La falta de variación en \\(x\\) significa que no podemos ajustar una línea única que relacione \\(x\\) y \\(y\\). 6.3.3 Variables Dependientes Limitadas, Censura y Truncamiento En algunas aplicaciones, la variable dependiente está limitada a ciertos rangos. Para entender por qué esto es un problema, primero recordemos que bajo el modelo de regresión lineal, la variable dependiente es igual a la función de regresión más un error aleatorio. Normalmente, se supone que el error aleatorio se distribuye aproximadamente de manera normal, por lo que la respuesta varía de forma continua. Sin embargo, si los resultados de la variable dependiente están restringidos o limitados, entonces los resultados no son puramente continuos. Esto significa que nuestra suposición de errores normales no es estrictamente correcta y puede no ser una buena aproximación. Para ilustrar esto, la Figura 6.3 muestra un gráfico del ingreso de un individuo (\\(x\\)) versus la cantidad de seguro adquirido (\\(y\\)). La muestra en este gráfico representa dos submuestras: aquellos que compraron seguro, correspondientes a \\(y&gt;0\\), y aquellos que no lo hicieron, correspondientes al “precio” \\(y=0\\). Ajustar una única línea a estos datos desinformaría a los usuarios sobre los efectos de \\(x\\) sobre \\(y\\). Figura 6.3: Cuando los individuos no compran nada, se registran como ventas de \\(y=0\\). Figura 6.4: Si se omiten las respuestas por debajo de la línea horizontal en \\(y=d\\), la línea de regresión ajustada puede ser muy diferente de la verdadera línea de regresión. Si consideráramos solo a aquellos que compraron seguro, entonces todavía tendríamos un límite inferior implícito de cero (si un precio de seguro debe ser mayor que cero). Sin embargo, los precios deben estar cerca de este límite para una región de muestreo dada y, por lo tanto, no representar un problema práctico importante. Al incluir a varias personas que no compraron seguro (y, por lo tanto, gastaron $0 en seguro), nuestra región de muestreo ahora claramente incluye este límite inferior. Hay varias maneras en las que las variables dependientes pueden estar restringidas o censuradas. La Figura 6.3 ilustra el caso en el que el valor de \\(y\\) no puede ser inferior a cero. Como otro ejemplo, los siniestros de seguros a menudo están restringidos a ser menores o iguales a un límite superior especificado en la póliza de seguro. Si la censura es severa, los mínimos cuadrados ordinarios producen resultados sesgados. En el Capítulo 15 se describen enfoques especializados, conocidos como modelos de regresión censurada, para manejar este problema. La Figura 6.4 ilustra otra limitación comúnmente encontrada en el valor de la variable dependiente. Para esta ilustración, suponga que \\(y\\) representa una pérdida asegurada y que \\(d\\) representa el deducible de una póliza de seguro. En este escenario, es práctica común que las aseguradoras no registren pérdidas por debajo de \\(d\\) (generalmente no son reportadas por los asegurados). En este caso, se dice que los datos están truncados. No es sorprendente que existan modelos de regresión truncada para manejar esta situación. Como regla general, los datos truncados representan una fuente de sesgo más grave que los datos censurados. Cuando los datos están truncados, no tenemos valores de las variables dependientes y, por lo tanto, tenemos menos información que cuando los datos están censurados. Consulte el Capítulo 15 para obtener más detalles. 6.3.4 Variables Omitidas y Endógenas Por supuesto, los analistas prefieren incluir todas las variables importantes. Sin embargo, un problema común es que puede que no tengamos los recursos ni la previsión para recopilar y analizar todos los datos relevantes. Además, a veces se nos prohíbe incluir ciertas variables. Por ejemplo, en la tarificación de seguros, generalmente se nos impide usar la etnia como una variable de tarificación. Además, hay muchas tablas de mortalidad y otras tablas de decremento que son “unisex”, es decir, no distinguen entre géneros. Omitir variables importantes puede afectar nuestra capacidad para ajustar la función de regresión; esto puede afectar el rendimiento dentro de la muestra (explicación) así como fuera de la muestra (predicción). Si la variable omitida no está correlacionada con otras variables explicativas, entonces la omisión no afectará la estimación de los coeficientes de regresión. Sin embargo, típicamente este no es el caso. La Sección 3.4.3, el Ejemplo del Refrigerador, ilustra un caso grave en el que la dirección de un resultado estadísticamente significativo se invirtió con la presencia de una variable explicativa. En este ejemplo, encontramos que en una muestra transversal de refrigeradores existía una correlación significativamente positiva entre el precio y el costo anual de energía para operar el refrigerador. Esta correlación positiva era contraintuitiva porque uno esperaría que precios más altos significaran menores gastos anuales en operar un refrigerador. Sin embargo, cuando incluimos varias variables adicionales, en particular, medidas del tamaño de un refrigerador, encontramos una relación significativamente negativa entre el precio y los costos de energía. Nuevamente, al omitir estas variables adicionales, hubo un sesgo importante al usar la regresión para entender la relación entre el precio y los costos de energía. Las variables omitidas pueden llevar a la presencia de variables explicativas endógenas. Una variable exógena es aquella que se puede considerar “dada” para los propósitos en cuestión. Una variable endógena es aquella que no cumple con el requisito de exogeneidad. Una variable omitida puede afectar tanto a \\(y\\) como a \\(x\\) y, en este sentido, inducir una relación entre las dos variables. Si la relación entre \\(x\\) y \\(y\\) se debe a una variable omitida, es difícil condicionar en \\(x\\) al estimar un modelo para \\(y\\). Hasta ahora, las variables explicativas han sido tratadas como no estocásticas. Para muchas aplicaciones en ciencias sociales, es más intuitivo considerar que las \\(x\\) son estocásticas y realizar inferencias condicionales a sus realizaciones. Por ejemplo, bajo esquemas de muestreo comunes, podemos estimar la función de regresión condicional \\[ \\mathrm{E~}\\left(y|x_1, \\ldots, x_k \\right) = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k. \\] Esto se conoce como un modelo “basado en el muestreo”. En la literatura económica, Goldberger (1972) define un modelo estructural como un modelo estocástico que representa una relación causal, no una relación que simplemente captura asociaciones estadísticas. Los modelos estructurales pueden contener fácilmente variables explicativas endógenas. Para ilustrar, consideremos un ejemplo que relaciona reclamaciones y primas. Para muchas líneas de negocio, las clases de primas son simplemente funciones no lineales de factores exógenos como la edad, el género, etc. Para otras líneas de negocio, las primas cobradas son una función del historial de reclamaciones previo. Consideremos las ecuaciones del modelo que relacionan las reclamaciones (\\(y_{it}, t=1, 2\\)) con las primas (\\(x_{it}, t=1, 2\\)): \\[\\begin{eqnarray*} y_{i2} = \\beta_{0,C} + \\beta_{1,C} y_{i1} + \\beta_{2,C} x_{i2} + \\varepsilon_{i1} \\\\ x_{i2} = \\beta_{0,P} + \\beta_{1,P} y_{i1} + \\beta_{2,P} x_{i1} + \\varepsilon_{i2}. \\end{eqnarray*}\\] En este modelo, las reclamaciones y las primas del período actual (\\(t=2\\)) se ven afectadas por las reclamaciones y primas del período anterior. Este es un ejemplo de un modelo de ecuaciones estructurales que requiere técnicas especiales de estimación. ¡Nuestros procedimientos de estimación habituales están sesgados! Ejemplo: Raza, Discriminación y Precios de Seguros de Automóviles - Continuación. Aunque Harrington y Niehaus (1998) no encontraron discriminación racial en la tarificación de seguros, sus resultados sobre el acceso al seguro fueron inconclusos. Los aseguradores ofrecen contratos de riesgo “estándar” y “preferido” a los solicitantes que cumplen con estándares restrictivos de suscripción, en comparación con los contratos de riesgo “subestándar” donde los estándares de suscripción son más relajados. Los reclamos esperados son más bajos para los contratos de riesgo estándar y preferido, y por lo tanto, las primas son más bajas, que para los contratos subestándar. Harrington y Niehaus examinaron la proporción de solicitantes a quienes se les ofrecieron contratos subestándar, NSSHARE, y encontraron que estaba significativamente relacionada positivamente con PCTBLACK, la proporción de la población negra. Esto sugiere evidencia de discriminación racial; ellos afirman que esta es una interpretación inapropiada debido al sesgo por variables omitidas. Harrington y Niehaus argumentan que la proporción de solicitantes a quienes se les ofrecieron contratos subestándar debería estar positivamente relacionada con los costos esperados de los reclamos. Además, los costos esperados de los reclamos están fuertemente relacionados con PCTBLACK, porque las minorías en la muestra tendían a tener ingresos más bajos. Así, las variables no observadas, como los ingresos, tienden a impulsar la relación positiva entre NSSHARE y PCTBLACK. Debido a que los datos se analizan a nivel de código postal y no a nivel individual, el potencial sesgo por variables omitidas hizo que el análisis fuera inconcluso. 6.3.5 Datos Faltantes En los ejemplos de datos, ilustraciones, estudios de caso y ejercicios de este texto, hay muchas instancias en las que ciertos datos están faltantes o no disponibles para el análisis. En cada caso, los datos no se perdieron de manera descuidada, sino que no estaban disponibles debido a razones sustantivas asociadas con la recolección de datos. Por ejemplo, cuando examinamos los rendimientos de acciones de una muestra de empresas, vimos que algunas empresas no tenían un promedio de ganancias por acción de cinco años. La razón era simplemente que no habían estado en existencia durante cinco años. Como otro ejemplo, al examinar las esperanzas de vida, algunos países no reportaron la tasa de fertilidad total porque les faltaban recursos administrativos para capturar estos datos. Los datos faltantes son un aspecto inescapable al analizar datos en las ciencias sociales. Cuando la razón para la falta de disponibilidad de datos no está relacionada con los valores reales de los datos, se dice que los datos están faltantes al azar. Existen varias técnicas para manejar datos faltantes al azar, ninguna de las cuales es claramente superior a las otras. Una “técnica” es simplemente ignorar el problema. Por lo tanto, faltar al azar a veces se denomina el caso ignorable de datos faltantes. Si hay solo unos pocos datos faltantes, en comparación con el número total disponible, una estrategia ampliamente empleada es eliminar las observaciones correspondientes a los datos faltantes. Suponiendo que los datos están faltantes al azar, se pierde poca información al eliminar una pequeña porción de los datos. Además, con esta estrategia, no necesitamos hacer suposiciones adicionales sobre las relaciones entre los datos. Si los datos faltantes provienen principalmente de una variable, podemos considerar omitir esta variable. Aquí, la motivación es que perdemos menos información al omitir esta variable en comparación con retener la variable pero perder las observaciones asociadas con los datos faltantes. Otra estrategia es completar, o imputar, los datos faltantes. Hay muchas variaciones de la estrategia de imputación. Todas asumen algún tipo de relaciones entre las variables además de las suposiciones del modelo de regresión. Aunque estos métodos producen resultados razonables, hay que tener en cuenta que cualquier tipo de valores imputados no presenta la misma variabilidad inherente que los datos reales. Así, los resultados de los análisis basados en valores imputados a menudo reflejan menos variabilidad que aquellos con datos reales. Ejemplo: Gastos de Compañías de Seguros - Continuación. Al examinar la información financiera de las compañías, los analistas a menudo se ven obligados a omitir una cantidad considerable de información al utilizar modelos de regresión para buscar relaciones. Para ilustrar, Segal (2002) examinó los estados financieros de seguros de vida a partir de datos proporcionados por la Asociación Nacional de Comisionados de Seguros (NAIC). Inicialmente, consideró 733 observaciones de empresas-año durante el período 1995-1998. Sin embargo, 154 observaciones fueron excluidas debido a primas, beneficios y otras variables explicativas inconsistentes o negativas. También se excluyeron las pequeñas empresas que representaban 131 observaciones. Las pequeñas empresas consisten en menos de 10 empleados y agentes, costos operativos menores a $1 millón o menos de 1,000 pólizas de vida vendidas. La muestra resultante fue de \\(n=448\\) observaciones. Las restricciones de muestra se basaron en variables explicativas; este procedimiento no necesariamente sesga los resultados. Segal argumentó que su muestra final seguía siendo representativa de la población de interés. Hubo alrededor de 110 empresas en cada uno de los años 1995-1998. En 1998, los activos agregados de las empresas en la muestra representaban aproximadamente $650 mil millones, un tercio de la industria de seguros de vida. 6.4 Modelos de Datos Faltantes Para entender los mecanismos que conducen a respuestas no planificadas, los modelamos de manera estocástica. Sea \\(r_i\\) una variable binaria para la \\(i\\)-ésima observación, con un uno indicando que esta respuesta se observa y un cero indicando que la respuesta está faltante. Sea \\(\\mathbf{r} = (r_1, \\ldots, r_n)^{\\prime}\\) que resume la disponibilidad de datos para todos los sujetos. El interés radica en si las respuestas influyen en el mecanismo de datos faltantes. Para la notación, usamos \\(\\mathbf{Y} = (y_1, \\ldots, y_n)^{\\prime}\\) para ser la colección de todas las respuestas potencialmente observadas. 6.4.1 Faltante al Azar En el caso en que \\(\\mathbf{Y}\\) no afecta la distribución de \\(\\mathbf{r}\\), seguimos a Rubin (1976) y llamamos a este caso faltante completamente al azar (MCAR). Específicamente, los datos faltantes son MCAR si \\(\\mathrm{f}(\\mathbf{r} | \\mathbf{Y}) = \\mathrm{f}(\\mathbf{r})\\), donde f(.) es una función genérica de masa de probabilidad. Una extensión de esta idea se encuentra en Little (1995), donde se añade el adjetivo “dependiente de covariables” cuando \\(\\mathbf{Y}\\) no afecta la distribución de \\(\\mathbf{r}\\), condicionado a las covariables. Si las covariables se resumen como \\(\\mathbf{X}\\), entonces la condición corresponde a la relación \\(\\mathrm{f}(\\mathbf{r} | \\mathbf{Y, X}) = \\mathrm{f}(\\mathbf{r | X})\\). Para ilustrar este punto, considere un ejemplo de Little y Rubin (1987) donde \\(\\mathbf{X}\\) corresponde a la edad y \\(\\mathbf{Y}\\) corresponde a los ingresos de todas las observaciones potenciales. Si la probabilidad de estar faltante no depende de los ingresos, entonces los datos faltantes son MCAR. Si la probabilidad de estar faltante varía según la edad pero no por ingresos entre las observaciones dentro de un grupo de edad, entonces los datos faltantes son MCAR dependientes de covariables. Bajo esta última especificación, es posible que los datos faltantes varíen según los ingresos. Por ejemplo, las personas más jóvenes pueden ser menos propensas a responder una encuesta. Esto muestra que la característica “faltante al azar” depende del propósito del análisis. Específicamente, es posible que un análisis de los efectos conjuntos de edad e ingresos pueda encontrar patrones graves de datos faltantes, mientras que un análisis de ingresos controlado por edad no sufre patrones graves de sesgo. Little y Rubin (1987) abogan por modelar los mecanismos de datos faltantes. Para ilustrar, considere un enfoque de máxima verosimilitud utilizando un modelo de selección para el mecanismo de datos faltantes. Ahora, particione \\(\\mathbf{Y}\\) en componentes observados y faltantes utilizando la notación \\(\\mathbf{Y} =\\{\\mathbf{Y}_{obs}, \\mathbf{Y}_{miss}\\}\\). Con el enfoque de máxima verosimilitud, basamos la inferencia en las variables aleatorias observadas. Por lo tanto, usamos una verosimilitud proporcional a la función conjunta \\(\\mathrm{f}(\\mathbf{r}, \\mathbf{Y}_{obs})\\). También especificamos un modelo de selección especificando la función de masa condicional \\(\\mathrm{f}(\\mathbf{r} | \\mathbf{Y})\\). Supongamos que las respuestas observadas y las distribuciones del modelo de selección se caracterizan por vectores de parámetros \\(\\boldsymbol \\theta\\) y \\(\\boldsymbol \\psi\\), respectivamente. Entonces, con la relación \\(\\mathrm{f}(\\mathbf{r}, \\mathbf{Y}_{obs},\\boldsymbol \\theta, \\boldsymbol \\psi) = \\mathrm{f}(\\mathbf{Y}_{obs}, \\boldsymbol \\theta) \\times \\mathrm{f}(\\mathbf{r} | \\mathbf{Y}_{obs}, \\boldsymbol \\psi)\\), podemos expresar la verosimilitud logarítmica de las variables aleatorias observadas como \\[ L(\\boldsymbol \\theta, \\boldsymbol \\psi) = \\mathrm{ln~} \\mathrm{f}(\\mathbf{r}, \\mathbf{Y}_{obs}, \\boldsymbol \\theta, \\boldsymbol \\psi) = \\mathrm{ln~} \\mathrm{f}(\\mathbf{Y}_{obs}, \\boldsymbol \\theta) + \\mathrm{ln~} \\mathrm{f}(\\mathbf{r} | \\mathbf{Y}_{obs}, \\boldsymbol \\psi). \\] (Véase la Sección 11.9 si desea un repaso sobre la inferencia de verosimilitud). En el caso en que los datos son MCAR, entonces \\(\\mathrm{f}(\\mathbf{r} | \\mathbf{Y}_{obs}, \\boldsymbol \\psi) = \\mathrm{f}(\\mathbf{r} | \\boldsymbol \\psi)\\) no depende de \\(\\mathbf{Y}_{obs}\\). Little y Rubin (1987) también consideran el caso en que la distribución del modelo del mecanismo de selección no depende de \\(\\mathbf{Y}_{miss}\\) pero puede depender de \\(\\mathbf{Y}_{obs}\\). En este caso, lo llaman datos faltantes al azar (MAR). En los casos tanto de MAR como de MCAR, vemos que la verosimilitud puede maximizarse sobre los parámetros, por separado para cada caso. En particular, si uno está interesado únicamente en el estimador de máxima verosimilitud de \\(\\boldsymbol \\theta\\), entonces el mecanismo del modelo de selección puede ser “ignorado”. Por lo tanto, ambas situaciones a menudo se denominan caso ignorables. Ejemplo: Gastos Dentales. Sea \\(y\\) el gasto anual en dentista de un hogar y \\(x\\) el ingreso. Considere los siguientes cinco mecanismos de selección. El hogar no es seleccionado (faltante) con una probabilidad sin tener en cuenta el nivel de gasto dental. En este caso, el mecanismo de selección es MCAR. El hogar no es seleccionado si el gasto dental es menor a $100. En este caso, el mecanismo de selección depende de la respuesta observada y faltante. El mecanismo de selección no puede ser ignorado. El hogar no es seleccionado si el ingreso es menor a $20,000. En este caso, el mecanismo de selección es MCAR, dependiente de covariables. Es decir, suponiendo que el propósito del análisis es entender los gastos dentales condicionado al conocimiento del ingreso, estratificar según el ingreso no sesga gravemente el análisis. La probabilidad de que un hogar sea seleccionado aumenta con el gasto dental. Por ejemplo, supongamos que la probabilidad de ser seleccionado es una función lineal de \\(\\exp(\\psi y_i)/(1+ \\exp(\\psi y_i))\\). En este caso, el mecanismo de selección depende de la respuesta observada y faltante. El mecanismo de selección no puede ser ignorado. El hogar es seguido durante \\(T\\) = 2 períodos. En el segundo período, un hogar no es seleccionado si el gasto del primer período es menor a $100. En este caso, el mecanismo de selección es MAR. Es decir, el mecanismo de selección se basa en una respuesta observada. Los segundos y cuartos mecanismos de selección representan situaciones donde el mecanismo de selección debe ser modelado explícitamente; estos son casos no ignorables. En estas situaciones, sin ajustes explícitos, los procedimientos que ignoran el efecto de selección pueden producir resultados gravemente sesgados. Para ilustrar una corrección para el sesgo de selección en un caso simple, presentamos un ejemplo de Little y Rubin (1987). La Sección 6.4.2 describe mecanismos adicionales. Ejemplo: Alturas Históricas. Little y Rubin (1987) discuten datos de Wachter y Trusell (1982) sobre \\(y\\), la altura de hombres reclutados para servir en el ejército. La muestra está sujeta a censura en el sentido de que se impusieron estándares mínimos de altura para la admisión en el ejército. Así, el mecanismo de selección es \\[ r_i = \\left\\{ \\begin{array}{ll} 1 &amp; y_i &gt; c_i \\\\ 0 &amp; \\mathrm{de lo contrario} \\\\ \\end{array} \\right. , \\] donde \\(c_i\\) es el estándar mínimo de altura conocido impuesto en el momento del reclutamiento. El mecanismo de selección es no ignorables porque depende de la altura del individuo, \\(y\\). Para este ejemplo, se dispone de información adicional para proporcionar inferencia de modelos confiables. Específicamente, basándonos en otros estudios de alturas masculinas, podemos suponer que la población de alturas sigue una distribución normal. Así, la verosimilitud de las observaciones puede escribirse y la inferencia puede proceder directamente. Para ilustrar, supongamos que \\(c_i = c\\) es constante. Sean \\(\\mu\\) y \\(\\sigma\\) la media y la desviación estándar de \\(y\\). Supongamos además que tenemos una muestra aleatoria de \\(n + m\\) hombres en la que \\(m\\) hombres están por debajo del estándar mínimo de altura \\(c\\) y observamos \\(\\mathbf{Y}_{obs} = (y_1, \\ldots, y_n)^{\\prime}\\). La distribución conjunta para los observables es \\[\\begin{eqnarray*} \\mathrm{f}(\\mathbf{r}, \\mathbf{Y}_{obs}, \\mu, \\sigma) &amp;=&amp; \\mathrm{f}(\\mathbf{Y}_{obs}, \\mu, \\sigma) \\times \\mathrm{f}(\\mathbf{r} | \\mathbf{Y}_{obs}) \\\\ &amp;=&amp; \\left\\{ \\prod_{i=1}^n \\mathrm{f}(y_i | y_i &gt; c) \\times \\mathrm{Pr}(y_i &gt; c) \\right\\} \\times \\left\\{\\mathrm{Pr}(y_i \\leq c)\\right\\}^m. \\end{eqnarray*}\\] Ahora, sean \\(\\phi\\) y \\(\\Phi\\) la densidad y la función de distribución para la distribución normal estándar. Así, la verosimilitud logarítmica es \\[\\begin{eqnarray*} L(\\mu, \\sigma) &amp;=&amp; \\mathrm{ln~} \\mathrm{f}(\\mathbf{r}, \\mathbf{Y}_{obs}, \\mu, \\sigma) \\\\ &amp;=&amp; \\sum_{i=1}^n \\mathrm{ln}\\left\\{ \\frac{1}{\\sigma} \\phi \\left( \\frac{y_i-\\mu}{\\sigma} \\right) \\right\\} + m~ \\mathrm{ln}\\left\\{ \\Phi \\left( \\frac{c-\\mu}{\\sigma}\\right) \\right\\} . \\end{eqnarray*}\\] Esto es fácil de maximizar en \\(\\mu\\) y \\(\\sigma\\). Si se ignoraran los mecanismos de censura, entonces se derivarían estimaciones de los datos observados a partir de la “verosimilitud logarítmica,” \\[ \\sum_{i=1}^n \\mathrm{ln}\\left\\{ \\frac{1}{\\sigma} \\phi \\left( \\frac{y_i-\\mu}{\\sigma} \\right) \\right\\}, \\] lo que daría resultados diferentes y sesgados. 6.4.2 Datos Faltantes No Ignorables Para los datos faltantes no ignorables, Little (1995) recomienda: Evitar respuestas faltantes siempre que sea posible utilizando procedimientos de seguimiento adecuados. Recoger covariables que sean útiles para predecir los valores faltantes. Recoger la mayor cantidad de información posible sobre la naturaleza del mecanismo de datos faltantes. Para el último punto, si se sabe poco sobre el mecanismo de datos faltantes, es difícil emplear un procedimiento estadístico robusto para corregir el sesgo de selección. Existen muchos modelos de mecanismos de datos faltantes. Una visión general aparece en Little y Rubin (1987). Little (1995) examina el problema de la deserción. En lugar de revisar esta literatura en desarrollo, presentamos un modelo ampliamente utilizado para datos faltantes no ignorables. Procedimiento de Dos Etapas de Heckman Heckman (1976) asume que el mecanismo de respuesta de muestreo está gobernado por la variable latente (no observada) \\(r_i^{\\ast}\\) donde \\[ r_i^{\\ast} = \\mathbf{z}_i^{\\prime} \\boldsymbol \\gamma + \\eta_i. \\] Las variables en \\(\\mathbf{z}_i\\) pueden o no incluir las variables en \\(\\mathbf{x}_i\\). Observamos \\(y_i\\) si \\(r_i^{\\ast}&gt;0\\), es decir, si \\(r_i^{\\ast}\\) cruza el umbral 0. Así, observamos \\[ r_i = \\left\\{ \\begin{array}{ll} 1 &amp; r_i^{\\ast}&gt;0 \\\\ 0 &amp; \\mathrm{de lo contrario} \\\\ \\end{array} \\right. . \\] Para completar la especificación, asumimos que {(\\(\\varepsilon_i,\\eta_i\\))} son distribuidos idénticamente e independientemente, y que la distribución conjunta de {(\\(\\varepsilon_i,\\eta_i\\))} es bivariada normal con medias cero, varianzas \\(\\sigma^2\\) y \\(\\sigma_{\\eta}^2\\), y correlación \\(\\rho\\). Note que si el parámetro de correlación \\(\\rho\\) es igual a cero, entonces los modelos de respuesta y selección son independientes. En este caso, los datos son MCAR y los procedimientos de estimación habituales son no sesgados y asintóticamente eficientes. Bajo estas suposiciones, cálculos básicos de la normal multivariada muestran que \\[ \\mathrm{E~}(y_i | r_i^{\\ast}&gt;0) = \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta + \\beta_{\\lambda} \\lambda(\\mathbf{z}_i^{\\prime} \\boldsymbol \\gamma), \\] donde \\(\\beta_{\\lambda} = \\rho \\sigma\\) y \\(\\lambda(a)=\\phi(a)/\\Phi(a)\\). Aquí, \\(\\lambda(.)\\) es la inversa de la llamada “razón de Mills.” Este cálculo sugiere el siguiente procedimiento en dos etapas para estimar los parámetros de interés. Procedimiento de Dos Etapas de Heckman Utilice los datos {(\\(r_i, \\mathbf{z}_i\\))} y un modelo de regresión probit para estimar \\(\\boldsymbol \\gamma\\). Llame a este estimador \\(\\mathbf{g}_H\\). Utilice el estimador de la etapa (1) para crear una nueva variable explicativa, \\(x_{i,K+1} = \\lambda(\\mathbf{z}_i^{\\prime}\\mathbf{g}_H)\\). Realice un modelo de regresión utilizando las \\(K\\) variables explicativas \\(\\mathbf{x}_i\\), así como la variable explicativa adicional \\(x_{i,K+1}\\). Use \\(\\mathbf{b}_H\\) y \\(b_{\\lambda,H}\\) para denotar los estimadores de \\(\\boldsymbol \\beta\\) y \\(\\beta_{\\lambda}\\), respectivamente. El Capítulo 11 introducirá las regresiones probit. También observamos que el método de dos etapas no funciona en ausencia de covariables para predecir la respuesta y, para fines prácticos, requiere variables en \\(\\mathbf{z}\\) que no están en \\(\\mathbf{x}\\) (ver Little y Rubin, 1987). Para probar el sesgo de selección, podemos probar la hipótesis nula \\(H_0:\\beta_{\\lambda}=0\\) en la segunda etapa debido a la relación \\(\\beta_{\\lambda}= \\rho \\sigma\\). Al realizar esta prueba, se deben usar errores estándar corregidos por heterocedasticidad. Esto se debe a que la varianza condicional \\(\\mathrm{Var}(y_i | r_i^{\\ast}&gt;0)\\) depende de la observación \\(i\\). Específicamente, \\(\\mathrm{Var}(y_i | r_i^{\\ast}&gt;0) = \\sigma^2 (1-\\rho^2 \\delta_i),\\) donde \\(\\delta_i= \\lambda_i(\\lambda_i + \\mathbf{z}_i^{\\prime} \\boldsymbol \\gamma)\\) y \\(\\lambda_i = \\phi(\\mathbf{z}_i^{\\prime} \\boldsymbol \\gamma)/\\Phi(\\mathbf{z}_i^{\\prime} \\boldsymbol \\gamma).\\) Este procedimiento asume normalidad para las variables latentes de selección para formar las variables aumentadas. Existen otras formas de distribución en la literatura, incluyendo las distribuciones logística y uniforme. Una crítica más profunda, planteada por Little (1985), es que el procedimiento se basa en suposiciones que no se pueden probar utilizando los datos disponibles. Esta crítica es análoga al ejemplo de alturas históricas donde nos basamos en gran medida en la curva normal para inferir la distribución de alturas por debajo del punto de censura. A pesar de estas críticas, el procedimiento de Heckman se usa ampliamente en las ciencias sociales. Algoritmo EM La Sección 6.4.2 se ha centrado en introducir modelos específicos de no respuesta no ignorables. Los modelos generales robustos de no respuesta no están disponibles. En cambio, una estrategia más apropiada es centrarse en una situación específica, recoger la mayor cantidad de información posible sobre la naturaleza del problema de selección y luego desarrollar un modelo para este problema de selección específico. El algoritmo EM es un dispositivo computacional para calcular los parámetros del modelo. Aunque es específico para cada modelo, ha encontrado aplicaciones en una amplia variedad de modelos que involucran datos faltantes. Computacionalmente, el algoritmo itera entre los pasos de “E”, para la expectativa condicional, y “M”, para la maximización. El paso E encuentra la expectativa condicional de los datos faltantes dado los datos observados y los valores actuales de los parámetros estimados. Esto es análogo a la tradición de imputar datos faltantes. Una innovación clave del algoritmo EM es que se imputan estadísticas suficientes para los valores faltantes, no los puntos de datos individuales. Para el paso M, se actualizan las estimaciones de los parámetros maximizando una log-verosimilitud observada. Tanto las estadísticas suficientes como la log-verosimilitud dependen de la especificación del modelo. Existen muchas introducciones al algoritmo EM en la literatura. Little y Rubin (1987) proporcionan un tratamiento detallado. 6.5 Aplicación: Eficiencia en el Costo de los Gestores de Riesgos Esta sección examina datos de una encuesta sobre la eficiencia en el costo de las prácticas de gestión de riesgos. Las prácticas de gestión de riesgos son actividades llevadas a cabo por una empresa para minimizar el costo potencial de futuras pérdidas, como el evento de un incendio en un almacén o un accidente que lesione a los empleados. Esta sección desarrolla un modelo que se puede utilizar para hacer afirmaciones sobre el costo de la gestión de riesgos. Un esquema del proceso de modelado de regresión es el siguiente. Comenzamos proporcionando una introducción al problema y dando un breve contexto sobre los datos. Ciertas teorías previas nos llevarán a presentar un ajuste preliminar del modelo. Usando técnicas diagnósticas, será evidente que varias suposiciones que sustentan este modelo no están en acuerdo con los datos. Esto nos llevará a volver al principio y comenzar el análisis desde cero. Lo que aprendemos de un examen detallado de los datos nos llevará a postular algunos modelos revisados. Finalmente, para comunicar ciertos aspectos del nuevo modelo, exploraremos presentaciones gráficas del modelo recomendado. Introducción Los datos para este estudio fueron proporcionados por la Profesora Joan Schmit y se discuten en más detalle en el artículo “Cost effectiveness of risk management practices,” Schmit y Roth (1990). Los datos provienen de un cuestionario enviado a 374 gestores de riesgos de grandes organizaciones con sede en EE. UU. El propósito del estudio fue relacionar la eficacia en costos con la filosofía de gestión de controlar la exposición de la empresa a diversas pérdidas por propiedad y accidentes, después de ajustar por efectos de la empresa como el tamaño y el tipo de industria. Primero, algunas advertencias. Los datos de encuestas a menudo se basan en muestras de conveniencia, no en muestras probabilísticas. Al igual que con todos los conjuntos de datos observacionales, la metodología de regresión es una herramienta útil para resumir los datos. Sin embargo, debemos ser cautelosos al hacer inferencias basadas en este tipo de conjunto de datos. Para esta encuesta en particular, 162 gestores devolvieron encuestas completas, resultando en una buena tasa de respuesta del \\(43\\%\\). Sin embargo, para las variables incluidas en el análisis (definidas más adelante), solo se completaron 73 formularios, resultando en una tasa de respuesta completa del \\(20\\%\\). ¿Por qué una diferencia tan dramática? Los gestores, al igual que la mayoría de las personas, generalmente no tienen problemas en responder a consultas sobre sus actitudes u opiniones acerca de diversos temas. Cuando se les pregunta sobre hechos concretos, en este caso el tamaño de los activos de la empresa o las primas de seguros, o bien consideran que la información es confidencial y son reacios a responder incluso cuando se les garantiza anonimato, o simplemente no están dispuestos a tomarse el tiempo para buscar la información. Desde el punto de vista del encuestador, esto es desafortunado porque, por lo general, los datos “actitudinales” son imprecisos (alta varianza en comparación con la media) en comparación con los datos financieros concretos. El inconveniente es que estos últimos datos a menudo son difíciles de obtener. De hecho, para esta encuesta, se enviaron varios cuestionarios previos para determinar la disposición de los gestores a responder preguntas específicas. A partir de los cuestionarios previos, los investigadores redujeron drásticamente el número de preguntas financieras que planeaban hacer. Una medida de la eficacia en el costo de la gestión de riesgos, FIRMCOST, es la variable dependiente. Esta variable se define como las primas totales por propiedad y accidentes y las pérdidas no aseguradas como un porcentaje de los activos totales. Es un proxy para los gastos anuales asociados con eventos asegurables, estandarizados por el tamaño de la empresa. Aquí, para las variables financieras, ASSUME es el monto de retención por ocurrencia como porcentaje de los activos totales, CAP indica si la empresa posee una compañía de seguros cautiva, SIZELOG es el logaritmo de los activos totales e INDCOST es una medida del riesgo de la industria de la empresa. Las variables actitudinales incluyen CENTRAL, una medida de la importancia de los gestores locales en la elección de la cantidad de riesgo a retener, y SOPH, una medida del grado de importancia en el uso de herramientas analíticas, como la regresión, en la toma de decisiones de gestión de riesgos. En el artículo, los investigadores describieron varias debilidades de las definiciones utilizadas, pero argumentan que estas definiciones proporcionan información útil, basada en la disposición de los gestores de riesgos a obtener información confiable. Los investigadores también describieron varias teorías sobre relaciones que podrían ser confirmadas por los datos. Específicamente, hipotetizaron: Existe una relación inversa entre la retención de riesgo (ASSUME) y el costo (FIRMCOST). La idea detrás de esta teoría es que mayores montos de retención deberían significar menores gastos para una empresa, resultando en menores costos. El uso de una compañía de seguros cautiva (CAP) resulta en menores costos. Presumiblemente, una cautiva se usa solo cuando es costo-efectiva y, en consecuencia, esta variable debería indicar menores costos si se utiliza efectivamente. Existe una relación inversa entre la medida de centralización (CENTRAL) y el costo (FIRMCOST). Presumiblemente, los gestores locales podrían tomar decisiones más costo-efectivas porque están más familiarizados con las circunstancias locales relacionadas con la gestión de riesgos que los gestores ubicados centralmente. Existe una relación inversa entre la medida de sofisticación (SOPH) y el costo (FIRMCOST). Presumiblemente, herramientas analíticas más sofisticadas ayudan a las empresas a gestionar el riesgo de manera más eficaz, resultando en menores costos. Análisis Preliminar Para probar las teorías descritas anteriormente, se puede utilizar el marco de análisis de regresión. Para ello, se plantea el modelo \\[ \\small{ \\begin{array}{ll} \\text{FIRMCOST} &amp;=&amp;\\beta_0 +\\beta_1 \\text{ ASSUME}+\\beta_{2}\\text{ CAP} +\\beta_{3}\\text{ SIZELOG}+\\beta_4\\text{ INDCOST} \\\\ &amp;&amp;+\\beta_5\\text{ CENTRAL}+\\beta_6\\text{ SOPH}+ \\varepsilon. \\end{array} } \\] Con este modelo, cada teoría se puede interpretar en términos de los coeficientes de regresión. Por ejemplo, \\(\\beta_1\\) se puede interpretar como el cambio esperado en el costo por unidad de cambio en el nivel de retención (ASSUME). Así, si la primera hipótesis es verdadera, esperamos que \\(\\beta_1\\) sea negativo. Para probar esto, podemos estimar \\(b_{1}\\) y usar nuestras pruebas de hipótesis para decidir si \\(b_{1}\\) es significativamente menor que cero. Las variables SIZELOG e INDCOST se incluyen en el modelo para controlar los efectos de estas variables. Estas variables no están directamente bajo el control del gestor de riesgos y, por lo tanto, no son de interés principal. Sin embargo, la inclusión de estas variables puede explicar una parte importante de la variabilidad. Los datos de 73 gestores fueron ajustados utilizando este modelo de regresión. La Tabla 6.2 resume el modelo ajustado. Tabla 6.2: Resultados de la Regresión del Ajuste del Modelo Preliminar Coeficiente Error Estándar \\(t\\)-Estadístico (Intercept) 59.765 19.065 3.135 ASSUME -0.300 0.222 -1.353 CAP 5.498 3.848 1.429 SIZELOG -6.836 1.923 -3.555 INDCOST 23.078 8.304 2.779 CENTRAL 0.133 1.441 0.092 SOPH -0.137 0.347 -0.394 El coeficiente de determinación ajustado es \\(R_{a}^{2}=18.8\\%\\), el ratio \\(F\\) es 3.78 y la desviación estándar residual es \\(s=14.56\\). Con base en las estadísticas resumidas del modelo de regresión, podemos concluir que las medidas de centralización y sofisticación no tienen un impacto en nuestra medida de eficacia en costos. Para ambas variables, el ratio \\(t\\) es bajo, menos de 1.0 en valor absoluto. El efecto de la retención de riesgo parece ser solo algo importante. El coeficiente tiene el signo apropiado, aunque está solo 1.35 errores estándar por debajo de cero. Esto no se consideraría estadísticamente significativo al nivel del 5%, aunque sí al nivel del 10% (el valor \\(p\\) es 9%). Quizás lo más desconcertante es el coeficiente asociado con la variable CAP. Teorizamos que este coeficiente sería negativo. Sin embargo, en nuestro análisis de los datos, el coeficiente resulta ser positivo y está 1.43 errores estándar por encima de cero. Esto no solo nos lleva a desmentir nuestra teoría, sino también a buscar nuevas ideas que estén en concordancia con la información aprendida de los datos. Schmit y Roth sugieren razones que pueden ayudarnos a interpretar los resultados de nuestras pruebas de hipótesis. Por ejemplo, sugieren que los gestores en la muestra pueden no tener las herramientas más sofisticadas disponibles cuando gestionan riesgos, lo que resulta en un coeficiente no significativo asociado con SOPH. También discutieron sugerencias alternativas, así como interpretaciones para los otros resultados de las pruebas de hipótesis. ¿Qué tan robusto es este modelo? La Sección 6.2 enfatizó algunos de los peligros de trabajar con un modelo inadecuado. Algunos lectores pueden sentirse incómodos con el modelo seleccionado anteriormente porque dos de las seis variables tienen \\(t\\)-ratios menores a 1 en valor absoluto y cuatro de las seis tienen \\(t\\)-ratios menores a 1.5 en valor absoluto. Quizás aún más importante, los histogramas de los residuos estandarizados y las palancas, en la Figura 6.5, muestran que varias observaciones son puntos atípicos y de alta influencia. Para ilustrar, el residuo más grande resulta ser \\(e_{15}=83.73\\). La suma de cuadrados del error es \\(Error~SS\\) = \\((n-(k+1))s^{2}\\) = \\((73-7)(14.56)^{2}=13,987\\). Así, la 15ª observación representa el 50.1% de la suma de cuadrados del error \\((=83.73^{2}/13,987)\\), sugiriendo que esta única observación de las 73 tiene un impacto dominante en el ajuste del modelo. Además, los gráficos de residuos estandarizados versus valores ajustados, no presentados aquí, mostraron evidencia de residuos heterocedásticos. Con base en estas observaciones, parece razonable evaluar la robustez del modelo. Figura 6.5: Histogramas de residuos estandarizados y palancas del ajuste preliminar del modelo de regresión. Código R para producir la Tabla 6.2 y la Figura 6.5 ## Análisis del Gestor de Riesgos # Tabla 6.2 survey &lt;- read.csv(&quot;CSVData/RiskSurvey.csv&quot;, header=TRUE) varSurvey &lt;- c(&quot;FIRMCOST&quot;,&quot;ASSUME&quot;,&quot;CAP&quot;,&quot;SIZELOG&quot;,&quot;INDCOST&quot;,&quot;CENTRAL&quot;,&quot;SOPH&quot;) survey1 &lt;- data.frame(survey[varSurvey]) # PRIMERA REGRESIÓN lmsurvey1&lt;-lm(FIRMCOST~ASSUME+CAP+SIZELOG+INDCOST+CENTRAL+SOPH,data=survey1) sum1 &lt;- summary(lmsurvey1) tableout &lt;- sum1$coefficients[,1:3] colnames(tableout) &lt;- c(&quot;Coeficiente&quot;, &quot;Error Estándar&quot;, &quot;$t$-Estadístico&quot;) TableGen1(TableData=tableout , TextTitle=&#39;Resultados de la Regresión del Ajuste del Modelo Preliminar&#39;, Align=&#39;r&#39;, ColumnSpec=1:3, Digits = 3, ColWidth = &quot;3cm&quot;) ri1 &lt;- rstandard(lmsurvey1) hii1 &lt;- hatvalues(lmsurvey1) # FIGURA 6.5 par(mfrow=c(1, 2),mar=c(4.1,4.5,.2,.2)) hist(ri1, nclass=16, main=&quot;&quot;, xlab=&quot;Residuos Estandarizados&quot;,las=1,cex.lab=1.5) hist(hii1, nclass=16, main=&quot;&quot;, xlab=&quot;Palancas&quot;,las=1,cex.lab=1.5) Volviendo a lo Básico Para entender mejor los datos, comenzamos examinando las estadísticas resumen básicas en la Tabla 6.3 y los histogramas correspondientes en la Figura 6.6. En la Tabla 6.3, el valor más alto de FIRMCOST es 97.55, que está más de cinco desviaciones estándar por encima de la media \\([10.97+5(16.16)=91.77]\\). Un examen de los datos muestra que este punto es la observación 15, la misma observación que fue un punto atípico en el ajuste preliminar de la regresión. Sin embargo, el histograma de FIRMCOST en la Figura 6.6 revela que este no es el único punto inusual. Otras dos observaciones tienen valores inusualmente altos de FIRMCOST, resultando en una distribución sesgada hacia la derecha. El histograma, en la Figura 6.6, de la variable ASSUME muestra que esta distribución también está sesgada hacia la derecha, posiblemente debido únicamente a dos grandes observaciones. De las estadísticas resumen básicas en la Tabla 6.3, vemos que el valor más alto de ASSUME está más de siete desviaciones estándar por encima de la media. Esta observación podría resultar influyente en el ajuste posterior del modelo de regresión. El diagrama de dispersión de FIRMCOST versus ASSUME en la Figura 6.6 nos dice que la observación con el valor más alto de FIRMCOST no es la misma que la observación con el valor más alto de ASSUME. Tabla 6.3: Estadísticas Resumen de \\(n=73\\) Encuestas sobre Gestión de Riesgos Media Mediana Desviación Estándar Mínimo Máximo FIRMCOST 10.973 6.08 16.159 0.20 97.55 ASSUME 2.574 0.51 8.445 0.00 61.82 CAP 0.342 0.00 0.478 0.00 1.00 SIZELOG 8.332 8.27 0.963 5.27 10.60 INDCOST 0.418 0.34 0.216 0.09 1.22 CENTRAL 2.247 2.00 1.256 1.00 5.00 SOPH 21.192 23.00 5.304 5.00 31.00 Fuente: Schmit y Roth, (1990) A partir de los histogramas de SIZELOG, INDCOST, CENTRAL y SOPH, vemos que estas distribuciones no están fuertemente sesgadas. Tomar logaritmos del tamaño de los activos totales de la empresa ha servido para hacer la distribución más simétrica que en las unidades originales. A partir del histograma y las estadísticas resumen, vemos que CENTRAL es una variable discreta, que toma valores del uno al cinco. La otra variable discreta es CAP, una variable binaria que toma solo valores cero y uno. El histograma y el gráfico de dispersión correspondiente a CAP no se presentan aquí. Es más informativo proporcionar una tabla de medias de cada variable por niveles de CAP, como en la Tabla 6.4. A partir de esta tabla, vemos que 25 de las 73 empresas encuestadas tienen aseguradoras cautivas. Además, por un lado, el FIRMCOST promedio para las empresas con aseguradoras cautivas \\((CAP = 1)\\) es mayor que para las que no tienen \\((CAP = 0)\\). Por otro lado, al pasar a la escala logarítmica, sucede lo contrario; es decir, el COSTLOG promedio para las empresas con aseguradoras cautivas \\((CAP = 1)\\) es mayor que para las que no tienen \\((CAP = 0)\\). Tabla 6.4: Tabla de Medias por Nivel de CAP \\(n\\) FIRMCOST ASSUME SIZELOG INDCOST CENTRAL SOPH COSTLOG CAP = 0 48 9.954 1.175 8.196 0.399 2.250 21.521 1.820 CAP = 1 25 12.931 5.258 8.592 0.455 2.240 20.560 1.595 TOTAL 73 10.973 2.574 8.332 0.418 2.247 21.192 1.743 Figura 6.6: Histogramas y gráficos de dispersión de FIRMCOST y varias variables explicativas. Las distribuciones de FIRMCOST y ASSUME están fuertemente sesgadas hacia la derecha. Hay una relación negativa entre FIRMCOST y SIZELOG, aunque no lineal. Código R para producir las Tablas 6.3 y 6.4 y la Figura 6.6 # TABLA 6.3 ESTADÍSTICAS RESUMEN BookSummStats &lt;- function(Xymat){ meanSummary &lt;- sapply(Xymat, mean, na.rm=TRUE) sdSummary &lt;- sapply(Xymat, sd, na.rm=TRUE) minSummary &lt;- sapply(Xymat, min, na.rm=TRUE) maxSummary &lt;- sapply(Xymat, max, na.rm=TRUE) medSummary &lt;- sapply(Xymat, median,na.rm=TRUE) tableMat &lt;- cbind(meanSummary, medSummary, sdSummary, minSummary, maxSummary) return(tableMat) } tableMat &lt;- BookSummStats(survey1) colnames(tableMat) &lt;- c(&quot;Media&quot;, &quot;Mediana&quot;, &quot;Desviación Estándar&quot;, &quot;Mínimo&quot;, &quot;Máximo&quot;) rownames(tableMat) &lt;- varSurvey TableGen1(TableData=tableMat, TextTitle=&#39;Estadísticas Resumen de $n=73$ Encuestas sobre Gestión de Riesgos&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:5, ColWidth = ColWidth5) # Tabla 6.4 varSum &lt;-function(var){ rows12 &lt;- Hmisc::summarize(var, survey1$CAP, mean )[[2]] row3 &lt;- mean(var) return( c(rows12, row3) ) } tableout1 &lt;- cbind( varSum(survey[,1]), varSum(survey[,2]), varSum(survey[,4]), varSum(survey[,5]), varSum(survey[,6]), varSum(survey[,7]), varSum(log(survey[,1])) ) num.cap &lt;- c(Hmisc::summarize(survey[,1], survey1$CAP, length )[[2]] , length(survey[,1]) ) tableout &lt;- cbind(num.cap, tableout1) colnames(tableout) &lt;- c(&quot;$n$&quot;, &quot;FIRMCOST&quot;,&quot;ASSUME&quot;,&quot;SIZELOG&quot;,&quot;INDCOST&quot;,&quot;CENTRAL&quot;,&quot;SOPH&quot;,&quot;COSTLOG&quot;) rownames(tableout) &lt;- c(&quot;CAP = 0&quot;, &quot;CAP = 1&quot;, &quot;TOTAL&quot;) TableGen1(TableData=tableout , TextTitle=&#39;Tabla de Medias por Nivel de CAP&#39;, Align=&#39;r&#39;, ColumnSpec=1:7, Digits = 3) # FIGURA 6.6 layout(matrix(c(1,2,3,4,5,6,7,8,9,10,11,12),byrow=TRUE,ncol=6)) par(&quot;oma&quot;=c(3,3,3,3),&quot;mai&quot;=c(0,0,0.1,0)) plot.new() hist(survey1$ASSUME,breaks=18,main=&quot;ASSUME&quot;,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) hist(survey1$SIZELOG,breaks=18,main=&quot;SIZELOG&quot;,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) hist(survey1$INDCOST,breaks=18,main=&quot;INDCOST&quot;,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) hist(survey1$CENTRAL,breaks=18,main=&quot;CENTRAL&quot;,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) hist(survey1$SOPH,breaks=18,main=&quot;SOPH&quot;,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) hist(survey1$FIRMCOST,breaks=18,main=&quot;FIRMCOST&quot;,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) plot(survey1$ASSUME,survey1$FIRMCOST,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) plot(survey1$SIZELOG,survey1$FIRMCOST,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) plot(survey1$INDCOST,survey1$FIRMCOST,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) plot(survey1$CENTRAL,survey1$FIRMCOST,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) plot(survey1$SOPH,survey1$FIRMCOST,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) Al examinar las relaciones entre pares de variables, en la Figura 6.6 observamos algunas de las relaciones que fueron evidentes en el ajuste preliminar de la regresión. Hay una relación inversa entre FIRMCOST y SIZELOG, y el gráfico de dispersión sugiere que esta relación podría ser no lineal. También hay una relación positiva leve entre FIRMCOST e INDCOST y no se observan relaciones aparentes entre FIRMCOST y ninguna de las otras variables explicativas. Estas observaciones se refuerzan con la tabla de correlaciones dada en la Tabla 6.4. Nota que la tabla oculta una característica que es evidente en los gráficos de dispersión: el efecto de las observaciones inusualmente grandes. Tabla 6.5: Matriz de Correlaciones COSTLOG FIRMCOST ASSUME CAP SIZELOG INDCOST CENTRAL FIRMCOST 0.713 ASSUME 0.165 0.039 CAP -0.088 0.088 0.231 SIZELOG -0.637 -0.366 -0.209 0.196 INDCOST 0.395 0.326 0.249 0.122 -0.102 CENTRAL -0.054 0.014 -0.068 -0.004 -0.08 -0.085 SOPH 0.144 0.048 0.062 -0.087 -0.209 0.093 0.283 Debido al sesgo de la distribución y al efecto de las observaciones inusualmente grandes, una transformación de la variable de respuesta podría llevar a resultados más útiles. La Figura 6.7 muestra el histograma de COSTLOG, definido como el logaritmo de FIRMCOST. La distribución es mucho menos sesgada que la distribución de FIRMCOST. La variable COSTLOG también se incluyó en la matriz de correlaciones en la Tabla 6.4. A partir de esta tabla, la relación entre SIZELOG parece ser más fuerte con COSTLOG que con FIRMCOST. La Figura 6.8 muestra varios gráficos de dispersión que ilustran la relación entre COSTLOG y las variables explicativas. La relación entre COSTLOG y SIZELOG parece ser lineal. Es más fácil interpretar estos gráficos de dispersión que los de la Figura 6.6 debido a la ausencia de los grandes valores inusuales de la variable dependiente. Figura 6.7: Histograma de COSTLOG (el logaritmo natural de FIRMCOST). La distribución de COSTLOG es menos sesgada que la de FIRMCOST. Figura 6.8: Gráficos de dispersión de COSTLOG versus varias variables explicativas. Hay una relación negativa entre COSTLOG y SIZELOG y una relación positiva leve entre COSTLOG e INDCOST. Código R para producir la Tabla 6.5 y las Figuras 6.7 y 6.8 # Tabla 6.5 survey2 &lt;- cbind(log(survey1[,1]), survey1) names(survey2) &lt;- c(&quot;COSTLOG&quot;, varSurvey) tableCor &lt;- cor(survey2) tableCor &lt;- round(tableCor, digits = 3) tableCor[upper.tri(tableCor, diag = TRUE)] &lt;- &quot;&quot; tablePrint &lt;- tableCor[-1,] tablePrint &lt;- tablePrint[,-8] TableGen1(TableData=tablePrint, TextTitle=&#39;Matriz de Correlaciones&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:3, ColWidth = ColWidth5) # FIGURA 6.7 COSTLOG &lt;- log(survey1$FIRMCOST) par(mar=c(4,4.4,.2,.2)) hist(COSTLOG, nclass=16, main=&quot;&quot;, xlab=&quot;COSTLOG&quot;,las=1,cex.lab=2) # FIGURA 6.8 layout(matrix(c(1,2,3,4,5),byrow=TRUE,ncol=5)) par(&quot;oma&quot;=c(3,5,3,3),&quot;mai&quot;=c(0,0,0.2,0)) plot(survey1$ASSUME,COSTLOG,main=&quot;ASSUME&quot;, xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;,las=1) plot(survey1$SIZELOG,COSTLOG,main=&quot;SIZELOG&quot;,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) plot(survey1$INDCOST,COSTLOG,main=&quot;INDCOST&quot;,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) plot(survey1$CENTRAL,COSTLOG,main=&quot;CENTRAL&quot;,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) plot(survey1$SOPH,COSTLOG, main=&quot;SOPH&quot;, xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;) Algunos Modelos Nuevos Ahora, exploramos el uso de COSTLOG como la variable dependiente. Este enfoque se basa en el trabajo de la subsección anterior y en los gráficos de residuos del ajuste preliminar de la regresión. Como primer paso, ajustamos un modelo con todas las variables explicativas. Por lo tanto, este modelo es el mismo que el ajuste preliminar de la regresión, excepto que usa COSTLOG en lugar de FIRMCOST como la variable dependiente. Este modelo sirve como un punto de referencia útil para nuestro trabajo posterior. La Tabla 6.6 resume el ajuste. Tabla 6.6: Resultados de la Regresión: COSTLOG como Variable Dependiente Coeficiente Error Estándar \\(t\\)-Estadístico (Intercept) 7.643 1.155 6.617 ASSUME -0.008 0.013 -0.609 CAP 0.015 0.233 0.064 SIZELOG -0.787 0.116 -6.752 INDCOST 1.905 0.503 3.787 CENTRAL -0.080 0.087 -0.916 SOPH 0.002 0.021 0.116 Aquí, \\(R_{a}^{2}=48\\%\\), \\(F\\)-ratio \\(=12.1\\) y \\(s=0.882\\). La Figura 6.9 muestra que la distribución de los residuos estandarizados es menos sesgada que la correspondiente en la Figura 6.5. La distribución de los apalancamientos muestra que todavía hay observaciones altamente influyentes. (De hecho, la distribución de los apalancamientos parece ser la misma que en la Figura 6.5. ¿Por qué?) Cuatro de las seis variables tienen \\(t\\)-ratios menores que uno en valor absoluto, lo que sugiere que debemos continuar buscando un mejor modelo. Figura 6.9: Histogramas de residuos estandarizados y apalancamientos utilizando COSTLOG como la variable dependiente. Para continuar con la búsqueda, se realizó una regresión por pasos (aunque la salida no se reproduce aquí). La salida de esta técnica de búsqueda, así como el modelo de regresión ajustado arriba, sugiere usar las variables SIZELOG e INDCOST para explicar la variable dependiente COSTLOG. Podemos realizar una regresión usando SIZELOG e INDCOST como variables explicativas. En la Figura 6.10, vemos que el tamaño y la forma de la distribución de los residuos estandarizados son similares a los de la Figura 6.9. Los apalancamientos son mucho menores, lo que refleja la eliminación de varias variables explicativas del modelo. Recuerda que el apalancamiento promedio es \\(\\bar{h} =(k+1)/n=3/73\\approx 0.04\\). Así, todavía tenemos tres puntos que superan tres veces el promedio y, por lo tanto, se consideran puntos de alto apalancamiento. Figura 6.10: Histogramas de residuos estandarizados y apalancamientos usando SIZELOG e INDCOST como variables explicativas. Código R para producir la Tabla 6.6 y las Figuras 6.9 y 6.10 # SEGUNDA REGRESIÓN # Tabla 6.6 lmsurvey2 &lt;- lm(COSTLOG ~ ASSUME + CAP + SIZELOG + INDCOST + CENTRAL + SOPH, data=survey2) sum2 &lt;- summary(lmsurvey2) tableout &lt;- sum2$coefficients[,1:3] colnames(tableout) &lt;- c(&quot;Coeficiente&quot;, &quot;Error Estándar&quot;, &quot;$t$-Estadístico&quot;) TableGen1(TableData=tableout, TextTitle=&#39;Resultados de la Regresión: COSTLOG como Variable Dependiente&#39;, Align=&#39;r&#39;, ColumnSpec=1:3, Digits=3, ColWidth = &quot;3cm&quot;) # FIGURA 6.9 ri2 &lt;- rstandard(lmsurvey2) hii2 &lt;- hatvalues(lmsurvey2) # FIGURA 6.9 par(mfrow=c(1, 2), mar=c(4.1, 4.5, .2, .2)) hist(ri2, nclass=16, main=&quot;&quot;, xlab=&quot;Residuos Estandarizados&quot;, las=1, cex.lab=1.5) hist(hii2, nclass=16, main=&quot;&quot;, xlab=&quot;Apalancamientos&quot;, las=1, cex.lab=1.5) # FIGURA 6.10 lmsurvey3 &lt;- lm(COSTLOG ~ SIZELOG + INDCOST, data=survey2) ri3 &lt;- rstandard(lmsurvey3) hii3 &lt;- hatvalues(lmsurvey3) # FIGURA 6.10 par(mfrow=c(1, 2), mar=c(4.1, 4.5, .2, .2)) hist(ri3, nclass=16, main=&quot;&quot;, xlab=&quot;Residuos Estandarizados&quot;, las=1, cex.lab=1.5) hist(hii3, nclass=16, main=&quot;&quot;, xlab=&quot;Apalancamientos&quot;, las=1, cex.lab=1.5) Tabla 6.7: Resultados de la Regresión con un Término Cuadrático en INDCOST Coeficiente Error Estándar \\(t\\)-Estadístico (Intercept) 6.353 0.953 6.666 SIZELOG -0.773 0.101 -7.626 INDCOST 6.264 1.610 3.889 INDCOSTSQ -3.585 1.265 -2.833 Figura 6.11: Gráfico de dispersión de residuos versus INDCOST. La curva ajustada suave (usando lowess) sugiere un término cuadrático en INDCOST. Código R para producir la Tabla 6.7 y la Figura 6.11 # TERCERA REGRESIÓN # Tabla 6.7 survey2$INDCOSTSQ &lt;- survey2$INDCOST * survey2$INDCOST lmsurvey4 &lt;- lm(COSTLOG ~ SIZELOG + INDCOST + INDCOSTSQ, data=survey2) sum4 &lt;- summary(lmsurvey4) tableout &lt;- sum4$coefficients[,1:3] colnames(tableout) &lt;- c(&quot;Coeficiente&quot;, &quot;Error Estándar&quot;, &quot;$t$-Estadístico&quot;) TableGen1(TableData=tableout, TextTitle=&#39;Resultados de la Regresión con un Término Cuadrático en INDCOST&#39;, Align=&#39;r&#39;, ColumnSpec=1:3, Digits=3, ColWidth = &quot;3cm&quot;) par(mar=c(4.1, 2.2, 1.7, .2), cex=1.2) plot(survey2$INDCOST, lmsurvey4$residuals, xlab=&quot;INDCOST&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;RESIDUAL&quot;, side=2, las=1, at=3.3, cex=1.2, adj=.4) lines(lowess(survey2$INDCOST, lmsurvey4$residuals, f=.8)) Los gráficos de residuos versus las variables explicativas revelan algunos patrones sutiles. El gráfico de dispersión de residuos versus INDCOST, en la Figura 6.11, muestra una tendencia cuadrática leve en INDCOST. Para verificar si esta tendencia es importante, la variable INDCOST fue elevada al cuadrado y utilizada como una variable explicativa en un modelo de regresión. Los resultados de este ajuste están en la Tabla 6.6. A partir del \\(t\\)-ratio asociado con \\((INDCOST)^{2}\\), vemos que la variable parece ser importante. El signo es razonable, indicando que la tasa de incremento de COSTLOG disminuye a medida que INDCOST aumenta. Es decir, el cambio esperado en COSTLOG por unidad de cambio en INDCOST es positivo y disminuye a medida que INDCOST aumenta. Los chequeos diagnósticos adicionales del modelo no revelaron patrones adicionales. Por lo tanto, con los datos disponibles, no podemos afirmar ninguna de las cuatro hipótesis introducidas en la subsección de Introducción. Esto no significa que estas variables no sean importantes. Simplemente estamos diciendo que la variabilidad natural de los datos era lo suficientemente grande como para ocultar cualquier relación que pudiera existir. Sin embargo, hemos establecido la importancia del tamaño de la empresa y del riesgo industrial de la empresa. La Figura 6.12 resume gráficamente las relaciones estimadas entre estas variables. En particular, en el panel inferior derecho, vemos que para la mayoría de las empresas en la muestra, FIRMCOST fue relativamente estable. Sin embargo, para las empresas pequeñas, medida por SIZELOG, el riesgo industrial, medido por INDCOST, fue particularmente importante. Para las empresas pequeñas, vemos que el FIRMCOST ajustado aumenta a medida que la variable INDCOST aumenta, con la tasa de incremento estabilizándose. Aunque el modelo predice teóricamente que FIRMCOST disminuye con un INDCOST grande \\((&gt;1.2)\\), no había empresas pequeñas en esta área de la región de datos. Figura 6.12: Gráfico de cuatro modelos ajustados versus INDCOST y SIZELOG. 6.6 Lecturas Adicionales y Referencias Este capítulo concluye nuestra Parte I, una introducción a la regresión lineal. Para aprender más sobre la regresión lineal, en la Sección 1.7 proporcionamos referencias a libros de estadística alternativos que introducen el tema. También puedes estar interesado en una presentación más técnica, como el trabajo clásico de Seber (1977) o un trabajo más reciente de Abraham y Ledolter (2006). Para otros enfoques, textos como el de Wooldridge (2009) ofrecen una perspectiva econométrica donde se enfatiza la introducción de la regresión en el contexto de la teoría económica. Alternativamente, libros como el de Agresti y Finlay (2008) brindan una introducción desde una perspectiva más amplia de las ciencias sociales. Existen muchas explicaciones de la regresión para lectores con diferentes perspectivas y niveles de formación cuantitativa; esto proporciona evidencia adicional de que este es un tema importante para que los actuarios y otros gestores de riesgos financieros lo comprendan. Otra forma de obtener una comprensión más profunda de la regresión lineal es ver cómo se aplica en un contexto de series temporales en la Parte II de este libro o en extensiones a la modelización no lineal en la Parte III. Consulta Bollen (1989) para una introducción clásica a la modelización de ecuaciones estructurales. Referencias del Capítulo Abraham, Bova and Johannes Ledolter (2006). Introduction to Regression Modeling. Thomson Higher Education, Belmont, CA. Agresti, Alan and Barbara Finlay (2008). Statistical Methods for the Social Sciences, Fourth Edition. Prentice Hall, Upper Saddle, NJ. Bollen, Kenneth A. (1989). Structural Equations with Latent Variables. New York: Wiley. Box, George E. P. (1979). Robustness in the strategy of scientific model building. In R. Launer and G. Wilderson (editors), Robustness in Statistics, pages 201-236, Academic Press, New York. Faraway, Julian J. (2005). Linear Models with R. Chapman &amp; Hall/CRC, Boca Raton, Florida. Fienberg, S. E (1985). Insurance availability in Chicago. Chapter in Data: A Collection of Problems from Many Fields for the Student and Research Worker. Editors D.F. Andrews and A. M. Herzberg, Springer-Verlag, New York. Goldberger, Arthur S. (1972). Structural equation methods in the social sciences. Econometrica 40, 979-1001. Harrington, Scott E. and Greg Niehaus (1998). Race, redlining and automobile insurance prices. Journal of Business 71(3), 439-469. Heckman, J. J. (1976). The common structure of statistical models of truncation, sample selection and limited dependent variables, and a simple estimator for such models. Ann. Econ. Soc. Meas. 5, 475-492. Little, R. J. (1995). Modelling the drop-out mechanism in repeated-measures studies. Journal of the American Statistical Association 90, 1112-1121. Little, R. J. and Rubin, D. B. (1987). Statistical Analysis with Missing Data. John Wiley, New York. Roberts, Harry V. (1990). Business and economic statistics (with discussion). Statistical Science 4, 372-402. Rubin, D. R. (1976). Inference and missing data. Biometrika 63, 581-592. Schmit, Joan T. and K. Roth (1990). Cost effectiveness of risk management practices. The Journal of Risk and Insurance 57, No. 3, pages 455-470. Seber, G. A. F. (1977). Linear Regression Analysis. John Wiley &amp; Sons, New York. Wachter, K. W. and J. Trusell (1982). Estimating historical heights. Journal of the American Statistical Association 77, 279-301. Wooldridge, Jeffrey (2009). Introductory Econometrics: A Modern Approach, Fourth Edition. South-Western Publishing, Mason, Ohio. 6.7 Ejercicios 6.1 Segregación por Seguro. ¿Utilizan las compañías de seguros la raza como un factor determinante al ofrecer seguros? Fienberg (1985) recopiló datos de un informe emitido por la Comisión de Derechos Civiles de EE.UU. sobre el número de propietarios de viviendas y pólizas de seguro contra incendios residenciales emitidas en Chicago durante los meses de diciembre de 1977 a febrero de 1978. Las pólizas emitidas se clasificaron como parte del mercado voluntario estándar o del mercado involuntario subestándar. El mercado involuntario consiste en planes de “acceso justo a los seguros” (FAIR); estos son programas de seguros estatales a veces subsidiados por empresas privadas. Estos planes brindan seguros a personas que, de otro modo, serían rechazadas para asegurar su propiedad debido a problemas de alto riesgo. El objetivo principal es comprender la relación entre la actividad de seguros y la variable “raza”, el porcentaje de minorías. Los datos están disponibles para \\(n=47\\) códigos postales en el área de Chicago. Estos datos también han sido analizados por Faraway (2005). Para ayudar a controlar el tamaño de la pérdida esperada, Fienberg también recopiló datos de robos e incendios de los departamentos de policía y bomberos de Chicago. Otra variable que proporciona información sobre el tamaño de la pérdida es la antigüedad de la casa. El ingreso mediano, del Censo, proporciona información indirecta sobre el tamaño de la pérdida esperada así como sobre si el solicitante puede permitirse el seguro. La Tabla 6.8 proporciona más detalles sobre estas variables. Tabla 6.8: Disponibilidad de Seguros en Chicago Variable Descripción Promedio row.names Código postal race Composición racial en porcentaje de minorías 35 fire Incendios por 1,000 unidades habitacionales 12.3 theft Robos por 1,000 habitantes 32.4 age Porcentaje de unidades habitacionales construidas en o antes de 1939 60.3 volact Nuevas pólizas para propietarios más renovaciones, menos cancelaciones y no renovaciones por 100 unidades habitacionales 6.53 involact Nuevas pólizas del plan FAIR y renovaciones por 100 unidades habitacionales 0.615 income Ingreso familiar mediano 10696 Produce estadísticas descriptivas de todas las variables, observando los patrones de asimetría para cada variable. Crea una matriz de gráficos de dispersión de volact, involact y race. Comenta sobre las tres relaciones pares. ¿Son los patrones consistentes con una hipótesis de discriminación racial? Para entender las relaciones entre las variables, produce una tabla de correlaciones. Ajusta un modelo lineal usando volact como la variable dependiente y race, fire, theft, age e income como variables explicativas. d(i). Comenta sobre el signo y la significancia estadística del coeficiente asociado con race. d(ii). Dos códigos postales resultan tener alta influencia. Repite tu análisis después de eliminar estas dos observaciones. ¿Ha cambiado la significancia de la variable race? ¿Qué pasa con las otras variables explicativas? Repite el análisis en la parte (d) usando involact como la variable dependiente. Define proporción como involact/(volact+involact). Repite el análisis en la parte (d) usando proporción como la variable dependiente. Los mismos dos códigos postales tienen alta influencia en las partes (d), (e) y (f). ¿Por qué ocurre esto? Este análisis se realiza a nivel de código postal, no a nivel individual. Como enfatizan Harrington y Niehaus (1998), esto introduce un potencial sesgo por variables omitidas. ¿Qué variables se han omitido en el análisis que crees que podrían afectar la disponibilidad del seguro para propietarios y la raza? Fienberg señala que la proximidad de un código postal a otro puede afectar la dependencia de las observaciones. Describe cómo podrías incorporar relaciones espaciales en un análisis de regresión. 6.2 Equidad de Género en los Sueldos del Personal Académico. La Universidad de Wisconsin en Madison realizó un estudio titulado “Estudio de Equidad de Género en los Sueldos del Personal Académico”, fechado el 5 de junio de 1992. El propósito principal del estudio fue determinar si las mujeres son tratadas de manera injusta en la determinación de sueldos en una importante universidad de investigación en EE.UU. Para ello, el comité que emitió el informe estudió los sueldos de 1990 de 1,898 miembros de la facultad de la universidad. Es bien conocido que los hombres ganan más que las mujeres. De hecho, el sueldo promedio de 1990 para los 1,528 miembros masculinos de la facultad es 54,478, que es un 28% más alto que el sueldo promedio de 1990 para las miembros femeninas de la facultad, que es 43,315. Sin embargo, se argumenta que los miembros masculinos de la facultad son en general más experimentados (el promedio de años de experiencia es 18.8 años) que las miembros femeninas de la facultad (el promedio de años de experiencia es 11.9 años), y por lo tanto, merecen un sueldo más alto. Al comparar los sueldos de los profesores titulares (controlando así por años de experiencia), los miembros masculinos de la facultad ganaron aproximadamente un 13% más que sus contrapartes femeninas. Aun así, se acuerda en general que los campos en demanda deben ofrecer sueldos más altos para mantener una facultad de primer nivel. Por ejemplo, los sueldos en ingeniería son más altos que los sueldos en humanidades simplemente porque los académicos en ingeniería tienen muchas más oportunidades de empleo fuera de la academia que los académicos en humanidades. Por lo tanto, al considerar los sueldos, también se debe controlar por departamento. Para controlar estas variables, un estudio de la facultad reporta un análisis de regresión usando el logaritmo del sueldo como la variable dependiente. Las variables explicativas incluyeron información sobre raza, género, rango (ya sea profesor asistente/instructor, profesor asociado o profesor titular), varias medidas de años de experiencia, 98 categorías diferentes de departamentos y una medida de la diferencia de sueldo por departamento. Hubo 109 variables explicativas en total (incluyendo 97 variables binarias departamentales), de las cuales 12 eran variables no departamentales. La Tabla 6.9 reporta las definiciones de variables, estimaciones de parámetros y \\(t\\)-ratios para las 12 variables no departamentales. La Tabla de ANOVA 6.10 resume el ajuste de la regresión. Tabla 6.9: Variables No Departamentales y Estimaciones de Parámetros Variable Explicativa Descripción de la Variable Estimación del Parámetro \\(t\\)-Ratio INTERCEPT 10.746 261.1 GENDER = 1 si es hombre, 0 en caso contrario 0.016 1.86 RACE = 1 si es blanco, 0 en caso contrario -0.029 -2.44 FULL = 1 si es profesor titular, 0 en caso contrario 0.186 16.42 ASSISTANT = 1 si es profesor asistente, 0 en caso contrario -0.205 -15.93 ANYDOC = 1 si tiene un título terminal como un Ph.D.  0.022 1.11 COHORT1 = 1 si fue contratado antes de 1969, 0 en caso contrario -0.102 -4.84 COHORT2 = 1 si fue contratado entre 1969-1985, 0 en caso contrario -0.046 -3.48 FULLYEARS Número de años como profesor titular en UW 0.012 12.84 ASSOCYEARS Número de años como profesor asociado en UW -0.012 -8.65 ASSISYEARS Número de años como profesor asistente o instructor en UW 0.002 0.91 DIFYRS Número de años desde la obtención de un título terminal antes de llegar a UW 0.004 4.46 MRKTRATIO Logaritmo natural de una ‘razón de mercado’ definida como la razón del sueldo promedio en instituciones pares para una disciplina y rango dado 0.665 7.64 Fuente: “Estudio de Equidad de Género en los Sueldos del Personal Académico”, 5 de junio de 1992, Universidad de Wisconsin en Madison. Tabla 6.10: Tabla ANOVA de Sueldos del Personal Fuente Suma de Cuadrados \\(df\\) Cuadrado Medio \\(F\\)-Ratio Regresión 114.048 109 1.0463 62.943 Error 29.739 1789 0.0166 Total 143.788 1898 Suponga que una miembro femenina de la facultad en el departamento de química siente que su sueldo está por debajo de lo que debería ser. Describa brevemente cómo se puede utilizar este estudio como base para la evaluación del rendimiento. Basado en este estudio, ¿cree que los sueldos de las mujeres son significativamente más bajos que los de los hombres? b(i). Cite argumentos estadísticos que apoyen el hecho de que los hombres no reciben un sueldo significativamente más alto que las mujeres. b(ii). Cite argumentos estadísticos que apoyen el hecho de que los hombres reciben un sueldo significativamente más alto que las mujeres. b(iii). Suponga que decide que las mujeres ganan menos que los hombres. Basado en este estudio, ¿cuánto aumentaría los sueldos de las miembros femeninas de la facultad para igualarlos con los de sus contrapartes masculinas? 6.8 Suplementos Técnicos para el Capítulo 6 6.8.1 Efectos de la Especificación Incorrecta del Modelo Notación. Particiona la matriz de variables explicativas \\(\\mathbf{X}\\) en dos submatrices, cada una con \\(n\\) filas, de modo que \\(\\mathbf{X}=(\\mathbf{X}_{1} : \\mathbf{X}_{2})\\). Para simplificar, supongamos que \\(\\mathbf{X}_{1}\\) es una matriz de \\(n \\times p\\). De manera similar, particiona el vector de parámetros \\(\\boldsymbol \\beta =\\left( \\boldsymbol \\beta_{1}^{\\prime }, \\boldsymbol \\beta_{2}^{\\prime }\\right) ^{\\prime }\\) de manera que \\(\\mathbf{X \\boldsymbol \\beta }=\\mathbf{X}_{1} \\boldsymbol \\beta_{1}+ \\mathbf{X}_{2} \\boldsymbol \\beta_{2}\\). Comparamos el modelo completo, o “largo”, \\[ \\mathbf{y}=\\mathbf{X \\boldsymbol \\beta }+\\boldsymbol \\varepsilon = \\mathbf{X}_{1} \\boldsymbol \\beta_{1}+\\mathbf{X}_{2} \\boldsymbol \\beta_{2}+\\boldsymbol \\varepsilon \\] con el modelo reducido, o “corto”, \\[ \\mathbf{y}=\\mathbf{X}_{1} \\boldsymbol \\beta_{1}+\\boldsymbol \\varepsilon. \\] Esto simplemente generaliza la configuración anterior para permitir la omisión de varias variables. Efecto del Subajuste. Supongamos que la representación verdadera es el modelo largo pero accidentalmente ejecutamos el modelo corto. Nuestras estimaciones de parámetros al ejecutar el modelo corto están dadas por \\(\\mathbf{b}_{1}=\\mathbf{(X}_{1}^{\\prime }\\mathbf{X}_{1}\\mathbf{)}^{-1}\\mathbf{X}_{1}^{\\prime }\\mathbf{y}\\). Estas estimaciones están sesgadas porque \\[ \\begin{array}{ll} \\text{Sesgo} &amp;= \\text{E }\\mathbf{b}_{1}-\\boldsymbol \\beta_{1} = \\text{E}\\mathbf{(X}_{1}^{\\prime}\\mathbf{X}_{1}\\mathbf{)}^{-1}\\mathbf{X}_{1}^{\\prime }\\mathbf{y} -\\boldsymbol \\beta_{1} \\\\ &amp;= \\mathbf{(X}_{1}^{\\prime }\\mathbf{X}_{1}\\mathbf{)}^{-1}\\mathbf{X}_{1}^{\\prime }\\text{E }\\mathbf{y}-\\boldsymbol \\beta_{1} \\\\ &amp;= \\mathbf{(X}_{1}^{\\prime }\\mathbf{X}_{1}\\mathbf{)}^{-1}\\mathbf{X}_{1}^{\\prime }\\left( \\mathbf{X}_{1}\\boldsymbol \\beta_{1}+\\mathbf{X}_{2}\\boldsymbol \\beta_{2}\\right) - \\boldsymbol \\beta_{1} \\\\ &amp;= \\mathbf{(X}_{1}^{\\prime}\\mathbf{X}_{1}\\mathbf{)}^{-1}\\mathbf{X}_{1}^{\\prime }\\mathbf{X}_{2}\\boldsymbol \\beta_{2} = \\mathbf{A \\boldsymbol \\beta }_{2}. \\end{array} \\] Aquí, \\(\\mathbf{A}=\\mathbf{(X}_{1}^{\\prime}\\mathbf{X}_{1}\\mathbf{)}^{-1}\\mathbf{X}_{1}^{\\prime }\\mathbf{X}_{2}\\) se llama la matriz de alias, o matriz de sesgo. Al ejecutar el modelo corto, la varianza estimada es \\(s_{1}^{2}=(\\mathbf{y}^{\\prime }\\mathbf{y}-\\mathbf{b}_{1}^{\\prime }\\mathbf{X}_{1}^{\\prime }\\mathbf{y})/(n-p)\\). Se puede mostrar que \\[\\begin{equation} \\text{E }s_{1}^{2}=\\sigma ^{2}+(n-p)^{-1}\\boldsymbol \\beta_{2}^{\\prime }\\left( \\mathbf{X}_{2}^{\\prime }\\mathbf{X}_{2}-\\mathbf{X}_{2}^{\\prime }\\mathbf{X}_{1}\\mathbf{(X}_{1}^{\\prime }\\mathbf{X}_{1}\\mathbf{)}^{-1}\\mathbf{X}_{1}^{\\prime }\\mathbf{X}_{2}\\right) \\boldsymbol \\beta_{2}. \\tag{6.3} \\end{equation}\\] Así, \\(s_{1}^{2}\\) es una estimación “sobresesgada” de \\(\\sigma ^{2}\\). Sean \\(\\mathbf{x}_{1i}^{\\prime }\\) y \\(\\mathbf{x}_{2i}^{\\prime }\\) las \\(i\\)-ésimas filas de \\(\\mathbf{X}_{1}\\) y \\(\\mathbf{X}_{2}\\), respectivamente. Usando el modelo corto ajustado, el \\(i\\)-ésimo valor ajustado es \\(\\hat{y}_{1i}=\\mathbf{x}_{1i}^{\\prime }\\mathbf{b}_{1}\\). La verdadera respuesta esperada \\(i\\)-ésima es E \\(\\hat{y}_{1i}=\\mathbf{x}_{1i}^{\\prime } \\boldsymbol \\beta_{1} + \\mathbf{x}_{2i}^{\\prime } \\boldsymbol \\beta_{2}\\). Así, el sesgo del \\(i\\)-ésimo valor ajustado es \\[ \\begin{array}{ll} \\text{Sesgo}(\\hat{y}_{1i}) &amp;= \\text{E }\\hat{y}_{1i}-\\text{E }y_{i}=\\mathbf{x}_{1i}^{\\prime }\\text{E }\\mathbf{b}_{1}-\\left( \\mathbf{x}_{1i}^{\\prime }\\boldsymbol \\beta_{1}+\\mathbf{x}_{2i}^{\\prime } \\boldsymbol \\beta_{2}\\right) \\\\ &amp; =\\mathbf{x}_{1i}^{\\prime }(\\boldsymbol \\beta_{1}+\\mathbf{A \\boldsymbol \\beta }_{2})-\\left( \\mathbf{x}_{1i}^{\\prime }\\boldsymbol \\beta_{1}+\\mathbf{x}_{2i}^{\\prime }\\boldsymbol \\beta_{2}\\right) =(\\mathbf{x}_{1i}^{\\prime }\\mathbf{A}-\\mathbf{x}_{2i}^{\\prime })\\boldsymbol \\beta_{2}. \\end{array} \\] Usando esto y la ecuación (6.3), se muestra mediante álgebra directa que \\[\\begin{equation} \\text{E }s_{1}^{2}=\\sigma ^{2}+(n-p)^{-1}\\sum_{i=1}^{n}(\\text{Sesgo}(\\hat{y}_{1i}))^{2}. \\tag{6.4} \\end{equation}\\] Efecto del Sobreajuste. Supongamos ahora que la representación verdadera es el modelo corto, pero accidentalmente usamos el modelo largo. Con la matriz de alias \\(\\mathbf{A}=\\mathbf{(X}_{1}^{\\prime }\\mathbf{X}_{1}\\mathbf{)}^{-1}\\mathbf{X}_{1}^{\\prime }\\mathbf{X}_{2}\\), podemos reparametrizar el modelo largo \\[ \\begin{array}{ll} \\mathbf{y} &amp; =\\mathbf{X}_{1}\\boldsymbol \\beta_{1}+\\mathbf{X}_{2}\\boldsymbol \\beta_{2} + \\boldsymbol \\varepsilon = \\mathbf{X}_{1}\\left( \\boldsymbol \\beta_{1}+\\mathbf{A \\boldsymbol \\beta }_{2}\\right) + \\mathbf{E}_{1}\\boldsymbol \\beta_{2} + \\boldsymbol \\varepsilon \\\\ &amp;= \\mathbf{X}_{1}\\boldsymbol \\alpha_{1}+\\mathbf{E}_{1}\\boldsymbol \\beta_{2} + \\boldsymbol \\varepsilon \\end{array} \\] donde \\(\\mathbf{E}_{1}=\\mathbf{X}_{2}-\\mathbf{X}_{1}\\mathbf{A}\\) y \\(\\boldsymbol \\alpha_{1}=\\boldsymbol \\beta_{1}+\\mathbf{A \\boldsymbol \\beta}_{2}\\). La ventaja de esta nueva parametrización es que \\(\\mathbf{X}_{1}\\) es ortogonal a \\(\\mathbf{E}_{1}\\) porque \\(\\mathbf{X}_{1}^{\\prime }\\mathbf{E}_{1}=\\mathbf{X}_{1}^{\\prime }(\\mathbf{X}_{2}-\\mathbf{X}_{1}\\mathbf{A})=\\mathbf{0}\\). Con \\(\\mathbf{X}^{\\ast }=(\\mathbf{X}_{1}: \\mathbf{E}_{1})\\) y \\(\\boldsymbol \\alpha =(\\boldsymbol \\alpha_{1}^{\\prime }\\boldsymbol \\beta_{1}^{\\prime })^{\\prime }\\), el vector de estimaciones de mínimos cuadrados es \\[ \\begin{array}{ll} \\mathbf{a} &amp;= \\begin{bmatrix} \\mathbf{a}_{1} \\\\ \\mathbf{b}_{1} \\end{bmatrix} = \\left( \\mathbf{X}^{\\ast \\prime }\\mathbf{X}^{\\ast }\\right) ^{-1}\\mathbf{X}^{\\ast \\prime }\\mathbf{y} \\\\ &amp; = \\begin{bmatrix} \\mathbf{(X}_{1}^{\\prime }\\mathbf{X}_{1}\\mathbf{)}^{-1} &amp; 0 \\\\ 0 &amp; \\mathbf{(E}_{1}^{\\prime }\\mathbf{E}_{1}\\mathbf{)}^{-1} \\end{bmatrix} \\begin{bmatrix} \\mathbf{X}_{1}^{\\prime }\\mathbf{y} \\\\ \\mathbf{E}_{1}^{\\prime }\\mathbf{y} \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} \\mathbf{(X}_{1}^{\\prime }\\mathbf{X}_{1}\\mathbf{)}^{-1}\\mathbf{X}_{1}^{\\prime }\\mathbf{y} \\\\ \\mathbf{(E}_{1}^{\\prime }\\mathbf{E}_{1}\\mathbf{)}^{-1}\\mathbf{E}_{1}^{\\prime }\\mathbf{y} \\end{bmatrix}. \\end{array} \\] Desde el modelo verdadero (corto), \\(\\mathrm{E}~\\mathbf{y}=\\mathbf{X}_{1}\\boldsymbol \\beta_{1}\\), tenemos que \\[ \\mathrm{E}~\\mathbf{b}_{2} =(\\mathbf{E}_{1}^{\\prime }\\mathbf{E}_{1})^{-1}\\mathbf{E}_{1}^{\\prime }\\mathrm{E}(\\mathbf{y}) =(\\mathbf{E}_{1}^{\\prime }\\mathbf{E}_{1})^{-1}\\mathbf{E}_{1}^{\\prime }\\mathrm{E}~ (\\mathbf{X}_{1}\\mathbf{\\beta}_{1}) =\\mathbf{0}, \\] porque \\(\\mathbf{X}_{1}^{\\prime }\\mathbf{E}_{1}=\\mathbf{0}\\). La estimación de mínimos cuadrados de \\(\\boldsymbol \\beta_{1}\\) es \\(\\mathbf{b}_{1}=\\mathbf{a}_{1}-\\mathbf{Ab}_{2}\\). Debido a que \\[ \\mathrm{E}~\\mathbf{a}_{1}=\\mathbf{(X}_{1}^{\\prime }\\mathbf{X}_{1}\\mathbf{)}^{-1}\\mathbf{X}_{1}^{\\prime }\\mathrm{E}~\\mathbf{y}=\\boldsymbol \\beta_{1} \\] bajo el modelo corto, tenemos que \\(\\mathrm{E}~ \\mathbf{b}_{1} = \\mathrm{E}~ \\mathbf{a}_{1}-\\mathbf{A} \\mathrm{E}~ \\mathbf{b}_{2} = \\boldsymbol \\beta_{1}-\\mathbf{0}=\\boldsymbol \\beta_{1}\\). Así, aunque accidentalmente usemos el modelo largo, \\(\\mathbf{b}_{1}\\) sigue siendo un estimador no sesgado de \\(\\boldsymbol \\beta_{1}\\) y \\(\\mathbf{b}_{2}\\) es un estimador no sesgado de \\(\\mathbf{0}\\). Por lo tanto, no hay sesgo en el valor ajustado \\(i\\)-ésimo porque \\[ \\mathrm{E}~\\hat{y}_{i}= \\mathrm{E}~(\\mathbf{x}_{1i}^{\\prime }\\mathbf{b}_{1}+\\mathbf{x}_{2i}^{\\prime }\\mathbf{b}_{2})=\\mathbf{x}_{1i}^{\\prime }\\boldsymbol \\beta_{1}= \\mathrm{E}~y_{i} . \\] Estadístico \\(C_{p}\\). Supongamos inicialmente que la representación verdadera es el modelo largo, pero usamos erróneamente el modelo corto. El valor ajustado \\(i\\)-ésimo es \\(\\hat{y}_{1i}=\\mathbf{x}_{1i}^{\\prime }\\mathbf{b}_{1}\\), que tiene un error cuadrático medio (MSE) \\[ \\text{MSE }\\hat{y}_{1i} = \\text{E}(\\hat{y}_{1i} - \\text{E }\\hat{y}_{1i})^{2} = \\text{Var }\\hat{y}_{1i} + \\left( \\text{Bias }\\hat{y}_{1i} \\right)^{2}. \\] Para la primera parte, tenemos que \\[ \\begin{array}{ll} \\mathrm{Var}~\\hat{y}_{1i}&amp; =\\mathrm{Var}\\left( \\mathbf{x}_{1i}^{\\prime }\\mathbf{b}_{1}\\right) = \\text{Var} \\left( \\mathbf{x}_{1i}^{\\prime } \\mathbf{(X}_{1}^{\\prime }\\mathbf{X}_{1}\\mathbf{)}^{-1}\\mathbf{X}_{1}^{\\prime }\\mathbf{y}\\right) \\\\ &amp; = \\sigma^{2} \\mathbf{x}_{1i} \\mathbf{(X}_{1}^{\\prime }\\mathbf{X}_{1}\\mathbf{)}^{-1} \\mathbf{x}_{1i}^{\\prime }. \\end{array} \\] Podemos pensar en \\(\\mathbf{x}_{1i} \\mathbf{(X}_{1}^{\\prime }\\mathbf{X}_{1}\\mathbf{)}^{-1} \\mathbf{x}_{1i}^{\\prime }\\) como la palanca \\(i\\)-ésima, como en la ecuación (5.3). Así, \\(\\sum_{i=1}^{n} \\mathbf{x}_{1i} \\mathbf{(X}_{1}^{\\prime }\\mathbf{X}_{1}\\mathbf{)}^{-1} \\mathbf{x}_{1i}^{\\prime } = p\\), el número de columnas de \\(\\mathbf{X}_{1}\\). Con esto, podemos definir el error total estandarizado \\[ \\begin{array}{ll} \\frac{\\sum_{i=1}^{n} \\text{MSE }\\hat{y}_{1i}}{\\sigma^{2}} &amp; = \\frac{\\sum_{i=1}^{n} \\left( \\text{Var }\\hat{y}_{1i} + \\left( \\text{Bias }\\hat{y}_{1i} \\right)^{2} \\right)}{\\sigma^{2}} \\\\ &amp; = \\frac{\\sigma^{2} \\sum_{i=1}^{n} \\left( \\mathbf{x}_{1i} \\mathbf{(X}_{1}^{\\prime }\\mathbf{X}_{1}\\mathbf{)}^{-1} \\mathbf{x}_{1i}^{\\prime } + \\left( \\text{Bias }\\hat{y}_{1i} \\right)^{2} \\right)}{\\sigma^{2}} \\\\ &amp;= p + \\sigma^{-2} \\sum_{i=1}^{n} \\left( \\text{Bias }\\hat{y}_{1i} \\right)^{2}. \\end{array} \\] Ahora, si \\(\\sigma^{2}\\) es conocido, a partir de la ecuación (6.4), una estimación no sesgada del error total estandarizado es \\(p + \\frac{(n-p)(s_{1}^{2} - \\sigma^{2})}{\\sigma^{2}}\\). Como \\(\\sigma^{2}\\) es desconocido, debe estimarse. Si no estamos seguros de si el modelo largo o el corto es la representación apropiada, una opción conservadora es usar \\(s^{2}\\) del modelo largo, o completo. Incluso si el modelo corto es el verdadero, \\(s^{2}\\) del modelo largo sigue siendo una estimación no sesgada de \\(\\sigma^{2}\\). Así, definimos \\[ C_{p} = p + \\frac{(n-p)(s_{1}^{2} - s^{2})}{s^{2}}. \\] Si el modelo corto es correcto, entonces \\(\\mathrm{E}~s_{1}^{2} = \\mathrm{E}~s^{2} = \\sigma^{2}\\) y \\(\\mathrm{E}~C_{p} \\approx p\\). Si el modelo largo es verdadero, entonces \\(\\mathrm{E}~s_{1}^{2} &gt; \\sigma^{2}\\) y \\(\\mathrm{E}~C_{p} &gt; p\\). "],["C7Trends.html", "Capítulo 7 Modelado de Tendencias 7.1 Introducción 7.2 Ajuste de Tendencias en el Tiempo 7.3 Estacionariedad y Modelos de Paseo Aleatorio 7.4 Inferencia usando Modelos de Paseo Aleatorio 7.5 Filtrado para Lograr Estacionariedad 7.6 Evaluación de Pronósticos 7.7 Lecturas Adicionales y Referencias 7.8 Ejercicios", " Capítulo 7 Modelado de Tendencias Vista previa del capítulo. Este capítulo inicia nuestro estudio de los datos de series temporales introduciendo técnicas para identificar patrones principales, o tendencias, en datos que evolucionan con el tiempo. El enfoque está en cómo las técnicas de regresión desarrolladas en capítulos anteriores pueden ser utilizadas para modelar tendencias. Además, se presentan nuevas técnicas, como la diferenciación de datos, que nos permiten introducir de forma natural un paseo aleatorio, un modelo importante de mercados financieros eficientes. 7.1 Introducción Series Temporales y Procesos Estocásticos Las empresas no se definen por estructuras físicas como los sólidos edificios de piedra que simbolizan la seguridad financiera. Tampoco se definen por los juguetes de invasores espaciales que fabrican para niños. Las empresas están compuestas por varios procesos complejos e interrelacionados. Un proceso es una serie de acciones u operaciones que conducen a un resultado particular. Los procesos no solo son los elementos fundamentales de las empresas, sino que también forman la base de nuestras vidas cotidianas. Podemos ir al trabajo o a la escuela todos los días, practicar artes marciales o estudiar estadística. Estas son secuencias regulares de actividades que nos definen. En este texto, nos interesa modelar procesos estocásticos, definidos como colecciones ordenadas de variables aleatorias, que cuantifican un proceso de interés. Algunos procesos evolucionan con el tiempo, como los viajes diarios al trabajo o a la escuela y los ingresos trimestrales de una empresa. Usamos el término datos longitudinales para referirnos a las mediciones de un proceso que evoluciona en el tiempo. Una única medición de un proceso genera una variable en el tiempo, denotada como \\(y_1, ..., y_T,\\) y conocida como una serie temporal. En esta parte del texto, seguimos la práctica común y usamos \\(T\\) para denotar el número de observaciones disponibles (en lugar de \\(n\\)). El Capítulo 10 describirá otro tipo de datos longitudinales donde examinamos una sección transversal de entidades, como empresas, y observamos su evolución a lo largo del tiempo. Este tipo de datos también se conoce como datos de panel. Las colecciones de variables aleatorias pueden estar ordenadas de formas distintas al tiempo. Por ejemplo, los daños por huracanes se registran en el lugar donde ocurrieron y, por lo tanto, están ordenados espacialmente. Otro ejemplo es la evaluación de un proyecto de perforación petrolera, que requiere tomar muestras del suelo en varias longitudes, latitudes y profundidades. Esto genera observaciones ordenadas por las tres dimensiones del espacio, pero no por el tiempo. Un ejemplo adicional es el estudio de los agujeros en la capa de ozono, donde se toman mediciones atmosféricas. Como el interés radica en la tendencia del agotamiento del ozono, las mediciones se toman en varias longitudes, latitudes, alturas y momentos en el tiempo. Aunque solo consideramos procesos ordenados por tiempo, en otros estudios de datos longitudinales podrías encontrar ordenamientos alternativos. Los datos que no están ordenados se denominan datos de corte transversal. Series Temporales versus Modelos Causales Los métodos de regresión pueden ser utilizados para resumir muchos conjuntos de datos de series temporales. Sin embargo, usar técnicas de regresión sin establecer un contexto adecuado puede ser desastroso. Este concepto se refuerza con un ejemplo basado en el trabajo de Granger y Newbold (1974). Ejemplo: Regresión Espuria. Sea \\(\\{\\varepsilon_{x,t}\\}\\) y \\(\\{\\varepsilon_{y,t}\\}\\) dos secuencias independientes, cada una de las cuales tiene una distribución normal estándar. A partir de estas, construimos recursivamente las variables \\(x_t = 0.5 + x_{t-1} + \\varepsilon_{x,t}\\) y \\(y_t = 0.5 + y_{t-1} + \\varepsilon_{y,t}\\), utilizando las condiciones iniciales \\(x_0 = y_0 = 0\\). (En la Sección 7.3, identificaremos \\(x_t\\) y \\(y_t\\) como modelos de paseo aleatorio). La Figura 7.1 muestra una realización de {\\(x_t\\)} y {\\(y_t\\)}, generada para \\(T = 50\\) observaciones usando simulación. El panel izquierdo muestra el crecimiento de cada serie en el tiempo: el aumento se debe a la adición de 0.5 en cada punto de tiempo. El panel derecho muestra una fuerte relación entre {\\(x_t\\)} y {\\(y_t\\)}: la correlación entre estas dos series resulta ser 0.92, a pesar de que las dos series fueron generadas independientemente. Su aparente relación, llamada espuria, se debe a que ambas están relacionadas con el crecimiento a lo largo del tiempo. Figura 7.1: Regresiones Espurias. El panel izquierdo muestra dos series temporales que aumentan con el tiempo. El panel derecho muestra un diagrama de dispersión de las dos series, sugiriendo una relación positiva entre ellas. La relación es espuria en el sentido de que ambas series están impulsadas por el crecimiento en el tiempo, no por una dependencia positiva entre ellas. En un contexto longitudinal, los modelos de regresión de la forma \\[ y_t = \\beta_0 + \\beta_1 x_t + \\varepsilon_t \\] se conocen como modelos causales. Los modelos causales se emplean regularmente en econometría, donde se supone que la teoría económica proporciona la información necesaria para especificar la relación causal (\\(x\\) “causa” \\(y\\)). En contraste, los modelos estadísticos solo pueden validar relaciones empíricas (“correlación, no causalidad”). En el ejemplo de regresión espuria, ambas variables evolucionan con el tiempo, por lo que un modelo que describa cómo una variable influye en otra necesita considerar los patrones temporales de las variables del lado izquierdo y derecho de la ecuación. Especificar modelos causales para aplicaciones actuariales puede ser difícil por esta razón: los patrones de series temporales en las variables explicativas pueden enmascarar o inducir una relación significativa con la variable dependiente. En contraste, el modelado de regresión puede aplicarse fácilmente cuando las variables explicativas son simplemente funciones del tiempo, el tema de la siguiente sección. Esto se debe a que las funciones del tiempo son determinísticas y, por lo tanto, no presentan patrones de series temporales. Los modelos causales también tienen la desventaja de que sus aplicaciones están limitadas para fines de pronóstico. Esto se debe a que, para realizar un pronóstico de una realización futura de la serie, por ejemplo \\(y_{T+2}\\), se necesita tener conocimiento (o un buen pronóstico) de \\(x_{T+2}\\), el valor de la variable explicativa en el tiempo \\(T+2\\). Si \\(x\\) es una función conocida del tiempo (como en la siguiente sección), entonces esto no es un problema. Otra posibilidad es utilizar un valor rezagado de \\(x\\), como \\(y_t = \\beta_0 + \\beta_1 x_{t-1} + \\varepsilon_t\\), de modo que los predictores a un paso sean posibles (podemos usar la ecuación para predecir \\(y_{T+1}\\) porque \\(x_T\\) es conocido en el tiempo \\(T\\)). 7.2 Ajuste de Tendencias en el Tiempo Comprendiendo Patrones en el Tiempo Predecir se trata de estimar realizaciones futuras de una serie temporal. A lo largo de los años, los analistas han encontrado útil descomponer una serie en tres tipos de patrones: tendencias en el tiempo (\\(T_t\\)), estacionalidad (\\(S_t\\)) y patrones aleatorios o irregulares (\\(\\varepsilon_t\\)). Una serie puede ser pronosticada extrapolando cada uno de estos tres patrones. La tendencia es la parte de una serie que corresponde a una evolución lenta y a largo plazo. Esta es la parte más importante para los pronósticos a largo plazo. La parte estacional de la serie corresponde a aspectos que se repiten periódicamente, como cada año. Los patrones irregulares de una serie son movimientos a corto plazo que típicamente son más difíciles de anticipar. Los analistas generalmente combinan estos patrones de dos maneras: de forma aditiva, \\[\\begin{equation} y_t = T_t + S_t + \\varepsilon_t, \\tag{7.1} \\end{equation}\\] o de forma multiplicativa, \\[\\begin{equation} y_t = T_t \\times S_t + \\varepsilon_t. \\tag{7.2} \\end{equation}\\] Los modelos sin componentes estacionales pueden manejarse fácilmente usando \\(S_t=0\\) para el modelo aditivo en la ecuación (7.1) y \\(S_t=1\\) para el modelo multiplicativo en la ecuación (7.2). Si el modelo es puramente multiplicativo, como en \\(y_t = T_t \\times S_t \\times \\varepsilon_t\\), este puede convertirse en un modelo aditivo al tomar logaritmos de ambos lados. Es instructivo ver cómo estos tres componentes pueden combinarse para formar una serie de interés. Consideremos los tres componentes en la Figura 7.2. Bajo el modelo aditivo, la tendencia, la estacionalidad y la variación aleatoria se combinan para formar la serie que aparece en el panel inferior derecho. Un gráfico de \\(y_t\\) frente a \\(t\\) se llama un gráfico de serie temporal. En los gráficos de series temporales, la convención es conectar puntos adyacentes usando una línea para ayudar a detectar patrones en el tiempo. Cuando analizamos datos, el gráfico en el panel inferior derecho es el primer tipo de gráfico que examinamos. El objetivo del análisis es retroceder: queremos descomponer la serie en sus tres componentes. Cada componente puede ser pronosticado, lo que nos proporcionará pronósticos razonables y fáciles de interpretar. Figura 7.2: Gráficos de Series Temporales de los Componentes de Respuesta. El componente de tendencia lineal aparece en el panel superior izquierdo, la estacionalidad en el superior derecho y la variación aleatoria en el inferior izquierdo. La suma de los tres componentes aparece en el panel inferior derecho. Ajuste de Tendencias en el Tiempo El tipo más simple de tendencia temporal es la ausencia completa de tendencia. Suponiendo que las observaciones son idénticamente y distribuidas de manera independiente (i.i.d.), entonces podríamos usar el modelo: \\[ y_t = \\beta_0 + \\varepsilon_t. \\] Por ejemplo, si estás observando un juego de azar como las apuestas realizadas en el lanzamiento de dos dados, entonces típicamente modelamos esto como una serie i.i.d.. Ajustar funciones polinomiales del tiempo es otro tipo de tendencia que es fácil de interpretar y ajustar a los datos. Comenzamos con una línea recta para nuestra función polinómica del tiempo, lo que da como resultado el modelo de tendencia lineal en el tiempo: \\[\\begin{equation} y_t = \\beta_0 + \\beta_1 t + \\varepsilon_t. \\tag{7.3} \\end{equation}\\] De manera similar, las técnicas de regresión pueden usarse para ajustar otras funciones que representan tendencias en el tiempo. La ecuación (7.3) se puede extender fácilmente para manejar una tendencia cuadrática en el tiempo: \\[ y_t = \\beta_0 + \\beta_1 t + \\beta_2 t^2 + \\varepsilon_t, \\] o un polinomio de orden superior. Ejemplo: Tipos de Cambio en Hong Kong. Para los viajeros y empresas, los tipos de cambio son una parte importante de la economía monetaria. El tipo de cambio que consideramos es el número de dólares de Hong Kong que se pueden comprar con un dólar estadounidense. Tenemos \\(T=502\\) observaciones diarias para el período del 1 de abril de 2005 al 31 de mayo de 2007, obtenidas del informe H10 de la Reserva Federal. La Figura 7.3 muestra un gráfico de serie temporal del tipo de cambio en Hong Kong. Figura 7.3: Gráfico de Serie Temporal de los Tipos de Cambio en Hong Kong con Valores Ajustados Superpuestos. Los valores ajustados provienen de una regresión utilizando una tendencia cuadrática en el tiempo. Fuente: Tipos de Cambio Extranjeros (Reserva Federal, informe H10). R Code to Produce Figure 7.3 # Figura 73 exchange &lt;- read.csv(&quot;CSVData/HKExchange.csv&quot;, header=TRUE) #exchange &lt;- read.csv(&quot;../../CSVData/HKExchange.csv&quot;, header=TRUE) t &lt;- 1:dim(exchange)[1] trend &lt;- lm(EXHKUS ~ t+I(t^2), data=exchange) plot(t,exchange$EXHKUS,type=&quot;n&quot;, xlim=c(1,505), ylim=c(7.74,7.84), xlab=&quot;&quot;, ylab=&quot;&quot;, axes=F) lines(t,exchange$EXHKUS) lines(t,trend$fitted) xat &lt;- t[seq(1,502, by=60)] yat &lt;- seq(7.74,7.84, by=0.02) axis(1,at=xat,labels=as.character(exchange$DATE[xat]),las=1) axis(2,at=yat,labels=as.character(yat),las=1) box() La Figura 7.3 muestra una clara tendencia cuadrática en los datos. Para manejar esta tendencia, usamos \\(t=1,...,502\\) como una variable explicativa para indicar el período de tiempo. La ecuación de regresión ajustada resulta ser: \\[ \\begin{array}{cccc} \\widehat{INDEX}_t = &amp; 7.797 &amp; -3.68\\times 10^{-4}t &amp; +8.269\\times 10^{-7}t^2 \\\\ {\\small t\\text{-estadísticas}} &amp; {\\small (8,531.9)} &amp; {\\small (-44.0)} &amp; {\\small (51.2)} \\end{array} . \\] El coeficiente de determinación es un saludable \\(R^2=86.2\\%\\) y la estimación de la desviación estándar ha disminuido de \\(s_{y}=0.0183\\) a \\(s=0.0068\\) (nuestra desviación estándar residual). La Figura 7.3 muestra la relación entre los datos y los valores ajustados mediante el gráfico de serie temporal del tipo de cambio con los valores ajustados superpuestos. Para aplicar estos resultados de regresión al problema de pronóstico, supongamos que queremos predecir el tipo de cambio para el 1 de abril de 2007, o \\(t=503\\). Nuestra predicción es \\[ \\widehat{INDEX}_{503} = 7.797 - 3.68 \\times 10^{-4}(503) + 8.269 \\times 10^{-7}(503)^2 = 7.8208. \\] La conclusión general es que el modelo de regresión que utiliza un término cuadrático en el tiempo \\(t\\) como variable explicativa ajusta bien los datos. Sin embargo, una inspección más detallada de la Figura 7.3 revela patrones en los residuos, donde las respuestas en algunos lugares son consistentemente más altas y en otros consistentemente más bajas que los valores ajustados. Estos patrones sugieren que podemos mejorar la especificación del modelo. Una forma sería introducir un modelo polinómico de mayor orden en el tiempo. En la Sección 7.3, argumentaremos que el paseo aleatorio es un modelo aún mejor para estos datos. Otras funciones no lineales del tiempo también pueden ser útiles. Para ilustrar, podríamos estudiar alguna medida de las tasas de interés a lo largo del tiempo (\\(y_t\\)) y estar interesados en el efecto de un cambio en la economía (como el inicio de una guerra). Definimos \\(z_t\\) como una variable binaria que es cero antes de que ocurra el cambio y uno durante y después del cambio. Consideremos el modelo, \\[\\begin{equation} y_t = \\beta_0 + \\beta_1 z_t + \\varepsilon_t. \\tag{7.4} \\end{equation}\\] Así, usando \\[ \\mathrm{E~}y_t = \\left\\{ \\begin{array}{ll} \\beta_0 + \\beta_1 &amp; \\text{si }z_t=1 \\\\ \\beta_0 &amp; \\text{si }z_t = 0 \\end{array} \\right. , \\] el parámetro \\(\\beta_1\\) captura el cambio esperado en las tasas de interés debido al cambio en la economía. Ver Figura 7.4. Figura 7.4: Gráfico de Serie Temporal de Tasas de Interés. Hay un cambio claro en las tasas debido a un cambio en la economía. Este cambio puede medirse usando un modelo de regresión con una variable explicativa que indique el cambio. Ejemplo: Modelos de Cambio de Régimen para Retornos de Acciones a Largo Plazo. Con la suposición de normalidad, podemos escribir el modelo en la ecuación (7.4) como \\[ y_t \\sim \\left\\{ \\begin{array}{ll} N(\\mu_1, \\sigma^2) &amp; t &lt; t_0 \\\\ N(\\mu_2, \\sigma^2)&amp; t \\geq t_0 \\end{array} \\right. , \\] donde \\(\\mu_1 = \\beta_0\\), \\(\\mu_2 = \\beta_0 + \\beta_1\\) y \\(t_0\\) es el punto de cambio. Un modelo de cambio de régimen generaliza este concepto, principalmente al asumir que el punto de cambio no es conocido. En su lugar, se supone que existe un mecanismo de transición que nos permite pasar de un “régimen” a otro con una probabilidad que típicamente se estima a partir de los datos. En este modelo, hay un número finito de estados, o “regímenes.” Dentro de cada régimen, se especifica un modelo probabilístico, como la distribución normal (condicionalmente) independiente (\\(N(\\mu_2, \\sigma^2)\\)). También se podría especificar un modelo autorregresivo o autorregresivo condicional que definiremos en el Capítulo 8. Además, existe una probabilidad condicional de transición de un estado a otro (las llamadas probabilidades de transición “Markov”). Hardy (2001) introdujo los modelos de cambio de régimen en la literatura actuarial, donde la variable dependiente de interés era el retorno del mercado de valores a largo plazo medido por los retornos mensuales en (1) el Standard and Poor’s 500 y (2) el Toronto Stock Exchange 300. Hardy consideró modelos de dos y tres regímenes para datos del período de 1956 a 1999, inclusive. Hardy mostró cómo usar las estimaciones de los parámetros del modelo de cambio de régimen para calcular precios de opciones y medidas de riesgo para contratos de seguros vinculados a acciones. Ajuste de Tendencias Estacionales El comportamiento periódico regular a menudo se encuentra en datos económicos y empresariales. Debido a que esta periodicidad a menudo está ligada al clima, estas tendencias se llaman componentes estacionales. Las tendencias estacionales pueden modelarse usando las mismas técnicas que las tendencias regulares o aperiódicas. El siguiente ejemplo muestra cómo capturar el comportamiento periódico utilizando variables binarias estacionales. Ejemplo: Tendencias en el Voto. En cualquier día de elecciones, el número de votantes que realmente acuden a las urnas depende de varios factores: la publicidad que haya recibido la elección, los temas debatidos como parte de la contienda, otros problemas que enfrenten los votantes el día de las elecciones y factores no políticos, como el clima. Ahora, los posibles candidatos políticos basan sus proyecciones de financiación de campañas y probabilidades de ganar una elección en pronósticos del número de votantes que participarán en una elección. Las decisiones sobre si participar o no como candidato deben tomarse con mucha anticipación; en general, tanto tiempo antes que factores conocidos como el clima en el día de la elección no puedan usarse para generar pronósticos. Consideramos aquí el número de votantes de Wisconsin que participaron en elecciones estatales durante el período de 1920 a 1990. Aunque el interés radica en pronosticar el número real de votantes, consideramos a los votantes como un porcentaje del público votante calificado. Dividir por el público votante calificado controla el tamaño de la población de votantes; esto mejora la comparabilidad entre las partes iniciales y finales de la serie. Dado que las tendencias de mortalidad son relativamente estables, se pueden obtener fácilmente proyecciones confiables del público votante calificado. Los pronósticos del porcentaje pueden multiplicarse por las proyecciones del público votante para obtener pronósticos de la participación real de votantes. Para especificar un modelo, examinamos la Figura 7.5, un gráfico de serie temporal de la participación de votantes como porcentaje del público votante calificado. Esta figura muestra la baja participación de votantes en la parte inicial de la serie, seguida de una mayor participación en las décadas de 1950 y 1960, y una menor participación en la década de 1980. Este patrón puede modelarse usando, por ejemplo, una tendencia cuadrática en el tiempo. La figura también muestra una participación mucho mayor en años de elecciones presidenciales. Este componente periódico, o estacional, puede modelarse utilizando una variable binaria. Un modelo candidato es \\[ y_t=\\beta_0+\\beta_1t+\\beta_2t^2+\\beta_{3}z_t+\\varepsilon _t, \\] donde \\[ z_t=\\left\\{ \\begin{array}{ll} 1 &amp; \\text{si año de elección presidencial} \\\\ 0 &amp; \\text{en caso contrario} \\end{array} \\right. . \\] Aquí, \\(\\beta_{3}z_t\\) captura el componente estacional en este modelo. Se utilizó regresión para ajustar el modelo. El modelo ajustado proporcionó un buen ajuste a los datos: el coeficiente de determinación fue \\(R^2=89.6\\%.\\) La Figura 7.5 muestra una fuerte relación entre los valores ajustados y los reales. Figura 7.5: Votantes de Wisconsin como Porcentaje del Público Votante Calificado, por Año. Los círculos opacos representan los porcentajes reales de votación. Las líneas discontinuas representan la tendencia ajustada, utilizando una tendencia cuadrática en el tiempo más una variable binaria para indicar un año de elección presidencial. El ejemplo de la tendencia en el voto demuestra el uso de variables binarias para capturar componentes estacionales. De manera similar, los efectos estacionales también pueden representarse utilizando variables categóricas, como \\[ z_t=\\left\\{ \\begin{array}{ll} 1 &amp; \\text{si primavera} \\\\ 2 &amp; \\text{si verano} \\\\ 3 &amp; \\text{si otoño} \\\\ 4 &amp; \\text{si invierno}. \\end{array} \\right. \\] Otra forma de capturar efectos estacionales es mediante el uso de funciones trigonométricas. Se discute más sobre el uso de funciones trigonométricas para manejar componentes estacionales en la Sección 9.3. La eliminación de patrones estacionales se conoce como ajuste estacional. Esta estrategia es apropiada en situaciones de políticas públicas donde el interés radica en interpretar la “serie ajustada estacionalmente” resultante. Por ejemplo, las agencias gubernamentales generalmente informan los ingresos de manufactura industrial en términos de números ajustados estacionalmente, bajo el entendimiento de que los patrones relacionados con feriados y clima se consideran al informar el crecimiento. Sin embargo, para la mayoría de las aplicaciones actuariales y de gestión de riesgos, el interés suele estar en pronosticar la variación de toda la serie, no solo la parte ajustada estacionalmente. Confiabilidad de los Pronósticos de Series Temporales Los pronósticos de series temporales a veces se denominan pronósticos “ingenuos”. El adjetivo “ingenuo” es algo irónico porque muchas técnicas de pronóstico de series temporales son técnicas por naturaleza y complejas de calcular. Sin embargo, estos pronósticos se basan en la extrapolación de una única serie de observaciones. Por lo tanto, son ingenuos en el sentido de que los pronósticos ignoran otras fuentes de información que pueden estar disponibles para el pronosticador y los usuarios de los pronósticos. A pesar de ignorar esta información posiblemente importante, los pronósticos de series temporales son útiles porque proporcionan un punto de referencia objetivo con el cual comparar otros pronósticos y opiniones de expertos. Los pronósticos deben proporcionar al usuario una idea de la confiabilidad del pronóstico. Una forma de cuantificar esto es proporcionar pronósticos bajo conjuntos de supuestos “bajo-intermedio-alto”. Por ejemplo, si estamos pronosticando la deuda nacional, podríamos hacerlo bajo tres escenarios del desempeño futuro de la economía. Alternativamente, podemos calcular intervalos de predicción utilizando muchos de los modelos de pronóstico que se discuten en este texto. Los intervalos de predicción proporcionan una medida de confiabilidad que puede interpretarse en un sentido probabilístico familiar. Además, al variar el nivel de confianza deseado, los intervalos de predicción varían, permitiéndonos responder a preguntas del tipo “¿qué pasaría si…?”. Por ejemplo, en la Figura 21.10 encontrará una comparación de proyecciones “bajo-intermedio-alto” con intervalos de predicción para pronósticos de la tasa de inflación (IPC) utilizados en la proyección de fondos de la Seguridad Social. Las proyecciones “bajo-intermedio-alto” se basan en un rango de opiniones de expertos y, por lo tanto, reflejan la variabilidad de los pronosticadores. Los intervalos de predicción reflejan la incertidumbre de innovación en el modelo (asumiendo que el modelo es correcto). Ambos rangos dan al usuario una idea de la confiabilidad de los pronósticos, aunque de maneras diferentes. Los intervalos de predicción tienen la ventaja adicional de cuantificar el hecho de que los pronósticos se vuelven menos confiables cuanto más lejos proyectamos en el futuro. Incluso con datos de corte transversal, vimos que cuanto más lejos estábamos de la parte principal de los datos, menos confiados nos sentíamos en nuestras predicciones. Esto también es cierto al pronosticar datos longitudinales. Es importante comunicar esto a los consumidores de pronósticos, y los intervalos de predicción son una forma conveniente de hacerlo. En resumen, el análisis de regresión utilizando varias funciones del tiempo como variables explicativas es una herramienta simple pero poderosa para pronosticar datos longitudinales. Sin embargo, tiene desventajas. Debido a que ajustamos una curva a todo el conjunto de datos, no hay garantía de que el ajuste para la parte más reciente de los datos sea adecuado. Es decir, para el pronóstico, la principal preocupación es la parte más reciente de la serie. Sabemos que las estimaciones de análisis de regresión dan más peso a las observaciones con valores inusualmente grandes de variables explicativas. Para ilustrar, usando un modelo de tendencia lineal en el tiempo, esto significa dar más peso a las observaciones al final y al principio de la serie. Usar un modelo que dé mucho peso a las observaciones al principio de la serie se ve con sospecha por los pronosticadores. Esta desventaja del análisis de regresión nos motiva a introducir herramientas adicionales para el pronóstico. (La Sección 9.1 desarrolla este punto con mayor detalle.) 7.3 Estacionariedad y Modelos de Paseo Aleatorio Una preocupación básica con los procesos que evolucionan a lo largo del tiempo es la estabilidad del proceso. Por ejemplo: “¿Me toma más tiempo llegar al trabajo desde que instalaron el nuevo semáforo?” “¿Han mejorado las ganancias trimestrales desde que el nuevo CEO asumió el cargo?” Medimos procesos para mejorar o gestionar su desempeño y para pronosticar el futuro del proceso. Dado que la estabilidad es una preocupación fundamental, trabajaremos con un tipo especial de estabilidad llamado estacionariedad. Definición. La estacionariedad es el concepto matemático formal que corresponde a la “estabilidad” de una serie temporal de datos. Se dice que una serie es (débilmente) estacionaria si: la media \\(\\mathrm{E~}y_t\\) no depende de \\(t\\), y la covarianza entre \\(y_{s}\\) y \\(y_t\\) depende solo de la diferencia entre las unidades de tiempo, \\(|t-s|.\\) Por lo tanto, por ejemplo, bajo estacionariedad débil \\(\\mathrm{E~}y_{4}=\\mathrm{E~}y_{8}\\) porque las medias no dependen del tiempo y, por lo tanto, son iguales. Además, \\(\\mathrm{Cov}(y_{4},y_{6})=\\mathrm{Cov}(y_{6},y_{8})\\), porque \\(y_{4}\\) y \\(y_{6}\\) están separados por dos unidades de tiempo, al igual que \\(y_{6}\\) y \\(y_{8}\\). Como otra implicación de la segunda condición, note que \\(\\sigma^2 = \\mathrm{Cov}(y_t, y_t)\\) \\(= \\mathrm{Cov}(y_s, y_s) = \\sigma^2\\). Por lo tanto, una serie débilmente estacionaria tiene una media constante y una varianza constante (homocedástica). Otro tipo de estacionariedad conocido como estricta o fuerte requiere que toda la distribución de \\(y_t\\) sea constante en el tiempo, no solo la media y la varianza. Ruido Blanco El vínculo entre los modelos longitudinales y transversales puede establecerse mediante la noción de un proceso de ruido blanco. Un proceso de ruido blanco es un proceso estacionario que no muestra patrones evidentes a lo largo del tiempo. Más formalmente, un proceso de ruido blanco es simplemente una serie que es i.i.d., idéntica e independientemente distribuida. Un proceso de ruido blanco es solo un tipo de proceso estacionario: el Capítulo 8 introducirá otro tipo, un modelo autorregresivo. Una característica especial del proceso de ruido blanco es que los pronósticos no dependen de cuán lejos en el futuro deseemos pronosticar. Supongamos que una serie de observaciones, \\(y_1,...,y_T\\), ha sido identificada como un proceso de ruido blanco. Sean \\(\\overline{y}\\) y \\(s_y\\) la media muestral y la desviación estándar muestral, respectivamente. Un pronóstico de una observación en el futuro, digamos \\(y_{T+l}\\), para \\(l\\) unidades de tiempo hacia adelante, es \\(\\overline{y}\\). Además, un intervalo de pronóstico es \\[\\begin{equation} \\overline{y}\\pm \\ t_{T-1,1-\\alpha/2} ~ s_y \\sqrt{1+\\frac{1}{T}}. \\tag{7.5} \\end{equation}\\] En aplicaciones de series temporales, dado que el tamaño de la muestra \\(T\\) suele ser relativamente grande, utilizamos el intervalo de predicción aproximado del 95% \\(\\overline{y} \\pm 2 s_y\\). Este intervalo de pronóstico aproximado ignora la incertidumbre del parámetro al usar \\(\\overline{y}\\) y \\(s_y\\) para estimar la media \\(\\mathrm{E}~y\\) y la desviación estándar \\(\\sigma\\) de la serie. En cambio, enfatiza la incertidumbre en las realizaciones futuras de la serie (conocida como incertidumbre de innovación). Tenga en cuenta que este intervalo no depende de la elección de \\(l\\), el número de unidades de tiempo hacia adelante que pronosticamos. El modelo de ruido blanco es el menos y el más importante de los modelos de series temporales. Es el menos importante en el sentido de que el modelo asume que las observaciones no están relacionadas entre sí, un evento poco probable para la mayoría de las series de interés. Es el más importante porque nuestros esfuerzos de modelado están dirigidos a reducir una serie a un proceso de ruido blanco. En el análisis de series temporales, el procedimiento para reducir una serie a un proceso de ruido blanco se llama filtro. Una vez que se han filtrado todos los patrones de los datos, se dice que la incertidumbre es irreductible. Paseo Aleatorio Ahora introducimos el modelo de paseo aleatorio. Para este modelo de series temporales, mostraremos cómo filtrar los datos simplemente tomando diferencias. Para ilustrar, supongamos que juegas un juego simple basado en el lanzamiento de dos dados. Para jugar, debes pagar $7 cada vez que lanzas los dados. Recibes el número de dólares correspondiente a la suma de los dos dados, \\(c_t^{\\ast}\\). Sea \\(c_t\\) tus ganancias en cada lanzamiento, de modo que \\(c_t = c_t^{\\ast} - 7\\). Suponiendo que los lanzamientos son independientes y provienen de la misma distribución, la serie \\(\\{c_t\\}\\) es un proceso de ruido blanco. Supongamos que comienzas con un capital inicial de \\(y_0 = \\$100\\). Sea \\(y_t\\) la suma del capital después del lanzamiento \\(t\\). Note que \\(y_t\\) se determina recursivamente como \\(y_t = y_{t-1} + c_t\\). Por ejemplo, porque ganaste $3 en el primer lanzamiento, \\(t=1\\), ahora tienes un capital de \\(y_1 = y_0 + c_1\\), o 103 = 100 + 3. Tabla 7.1 muestra los resultados de los primeros cinco lanzamientos. La Figura 7.6 es un gráfico de series temporales de las sumas, \\(y_t\\), para los cincuenta lanzamientos. Tabla 7.1. Ganancias para Cinco de los 50 Lanzamientos \\[ \\begin{array}{c|ccccc} \\hline t &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\ c_t^{\\ast } &amp; 10 &amp; 9 &amp; 7 &amp; 5 &amp; 7 \\\\ c_t &amp; 3 &amp; 2 &amp; 0 &amp; -2 &amp; 0 \\\\ ~~y_t~~ &amp; ~103~ &amp; ~105~ &amp; ~105~ &amp; ~103~ &amp; 103 \\\\ \\hline \\end{array} \\] Figura 7.6: Gráfico de Series Temporales de la Suma del Capital Las sumas parciales de un proceso de ruido blanco definen un modelo de paseo aleatorio. Por ejemplo, la serie \\(\\{y_1, \\ldots ,y_{50}\\}\\) en la Figura 7.6 es una realización del modelo de paseo aleatorio. La frase suma parcial se utiliza porque cada observación, \\(y_t\\), se creó sumando las ganancias hasta el tiempo \\(t\\). En este ejemplo, las ganancias, \\(c_t\\), son un proceso de ruido blanco porque el monto retornado, \\(c_t^{\\ast}\\), es i.i.d. En nuestro ejemplo, tus ganancias de cada lanzamiento de los dados se representan mediante un proceso de ruido blanco. Si ganas en un lanzamiento de los dados no influye en el resultado del siguiente o del anterior lanzamiento. En contraste, tu cantidad de capital en cualquier lanzamiento de los dados está altamente relacionada con la cantidad de capital después del siguiente o anterior lanzamiento. Tu cantidad de capital después de cada lanzamiento de los dados se representa mediante un modelo de paseo aleatorio. 7.4 Inferencia usando Modelos de Paseo Aleatorio El paseo aleatorio es un modelo de series temporales de uso común. Para ver cómo se puede aplicar, primero discutimos algunas propiedades del modelo. Luego, utilizamos estas propiedades para pronosticar e identificar una serie como un paseo aleatorio. Finalmente, esta sección compara el paseo aleatorio con un competidor, el modelo de tendencia lineal en el tiempo. Propiedades del Modelo Para establecer las propiedades del paseo aleatorio, primero recapitulamos algunas definiciones. Sean \\(c_1,\\ldots ,c_T\\) \\(T\\) observaciones de un proceso de ruido blanco. Un paseo aleatorio puede expresarse recursivamente como \\[\\begin{equation} y_t = y_{t-1} + c_t. \\tag{7.6} \\end{equation}\\] Por sustitución repetida, tenemos \\[ y_t = c_t + y_{t-1} = c_t + \\left( c_{t-1} + y_{t-2}\\right) = \\ldots \\] Si usamos \\(y_0\\) como el nivel inicial, entonces podemos expresar el paseo aleatorio como \\[\\begin{equation} y_t = y_0 + c_1 + \\ldots + c_t. \\tag{7.7} \\end{equation}\\] La ecuación (7.7) muestra que un paseo aleatorio es la suma parcial de un proceso de ruido blanco. El paseo aleatorio no es un proceso estacionario porque la variabilidad, y posiblemente la media, depende del punto de tiempo en el que se observa la serie. Tomando la esperanza y la varianza de la ecuación (7.7), obtenemos el nivel medio y la variabilidad del proceso de paseo aleatorio: \\[ \\mathrm{E~}y_t = y_0 + t\\mu_c\\ \\ \\text{ y} \\ \\ \\mathrm{Var~} y_t = t \\sigma_c^2, \\] donde \\(\\mathrm{E~}c_t = \\mu_c\\) y \\(\\mathrm{Var~}c_t = \\sigma _c^2\\). Por lo tanto, mientras haya alguna variabilidad en el proceso de ruido blanco (\\(\\sigma_c^2 &gt; 0\\)), el paseo aleatorio no es estacionario en la varianza. Además, si \\(\\mu_c\\neq 0\\), entonces el paseo aleatorio no es estacionario en la media. Pronósticos ¿Cómo podemos pronosticar una serie de observaciones, \\(y_1,...,y_T\\), que ha sido identificada como una realización de un modelo de paseo aleatorio? La técnica que usamos es pronosticar las diferencias, o cambios, en la serie y luego sumar las diferencias pronosticadas para obtener la serie pronosticada. Esta técnica es manejable porque, por definición del modelo de paseo aleatorio, las diferencias pueden representarse utilizando un proceso de ruido blanco, un proceso que sabemos cómo pronosticar. Consideremos \\(y_{T+l}\\), el valor de la serie \\(l\\) unidades de tiempo en el futuro. Sea \\(c_t=y_t-y_{t-1}\\) que representa las diferencias en la serie, de modo que \\[\\begin{eqnarray*} y_{T+l} &amp;=&amp;y_{T+l-1}+c_{T+l} = \\left( y_{T+l-2} + c_{T+l-1}\\right) +c_{T+l} = \\ldots \\\\ &amp;=&amp;y_T+c_{T+1}+ \\ldots +c_{T+l}. \\end{eqnarray*}\\] Interpretamos \\(y_{T+l}\\) como el valor actual de la serie, \\(y_T\\), más la suma parcial de diferencias futuras. Para pronosticar \\(y_{T+l}\\), dado que en el tiempo \\(T\\) conocemos \\(y_T\\), solo necesitamos pronosticar los cambios \\(\\{c_{T+1}, \\ldots, c_{T+l}\\}\\). Debido a que un pronóstico de un valor futuro de un proceso de ruido blanco es simplemente el promedio del proceso, el pronóstico de \\(c_{T+k}\\) es \\(\\overline{c}\\) para \\(k=1,2,\\ldots,l\\). Combinando estos resultados, el pronóstico de \\(y_{T+l}\\) es \\(y_T+l\\overline{c}\\). Por ejemplo, para \\(l=1\\), interpretamos que el pronóstico del siguiente valor de la serie es el valor actual de la serie más el cambio promedio de la serie. Usando ideas similares, tenemos que un intervalo de predicción aproximado al 95% para \\(y_{T+l}\\) es \\[ y_T+l\\overline{c}\\pm 2s_c\\sqrt{l} \\] donde \\(s_c\\) es la desviación estándar calculada utilizando los cambios \\(c_2,c_{3},\\ldots,c_T\\). Note que el ancho del intervalo de predicción, \\(4 s_c \\sqrt{l}\\), crece a medida que crece el horizonte de pronóstico \\(l\\). Este aumento en el ancho simplemente refleja nuestra capacidad decreciente para predecir hacia el futuro. Como ejemplo, lanzamos los dados \\(T=50\\) veces y queremos pronosticar \\(y_{60}\\), nuestra suma de capital después de 60 lanzamientos. En el tiempo 50, resultó que nuestra suma de dinero disponible era \\(y_{50}=\\$93\\). Comenzando con \\(y_0 = \\$100\\), el cambio promedio fue \\(\\overline{c} = -7/50 = -0.14\\), con una desviación estándar \\(s_c=\\$2.703\\). Por lo tanto, el pronóstico en el tiempo 60 es \\(93+10(-.14) =91.6\\). El correspondiente intervalo de predicción al 95% es \\[ 91.6\\pm 2\\left( 2.703\\right) \\sqrt{10}=91.6\\pm 17.1=\\left( 74.5,108.7\\right). \\] Ejemplo: Tasas de Participación en la Fuerza Laboral. Los pronósticos de la tasa de participación en la fuerza laboral (\\(LFPR\\)), junto con pronósticos de la población, nos brindan una visión de la futura fuerza laboral de una nación. Esta visión proporciona información sobre el funcionamiento futuro de la economía en general, y por lo tanto, las proyecciones de \\(LFPR\\) son de interés para varias agencias gubernamentales. En los Estados Unidos, las tasas de participación en la fuerza laboral son proyectadas por la Administración del Seguro Social, la Oficina de Estadísticas Laborales, la Oficina de Presupuesto del Congreso y la Oficina de Gestión y Presupuesto. En el contexto del Seguro Social, los formuladores de políticas utilizan proyecciones de la fuerza laboral para evaluar propuestas de reforma del sistema de Seguro Social y para evaluar su futura solvencia financiera. La tasa de participación en la fuerza laboral es la fuerza laboral civil dividida por la población civil no institucionalizada. Estos datos son recopilados por la Oficina de Estadísticas Laborales. Para fines de ilustración, examinemos un grupo demográfico específico y mostremos cómo pronosticarlo; los pronósticos de otros grupos pueden encontrarse en Fullerton (1999) y Frees (2006). Específicamente, examinamos el período 1968-1998 para mujeres de 20 a 44 años, que viven en un hogar con un cónyuge presente y al menos un hijo menor de seis años. La Figura 7.7 muestra el rápido aumento en \\(LFPR\\) para este grupo a lo largo de \\(T=31\\) años. Figura 7.7: Tasas de Participación en la Fuerza Laboral para Mujeres de 20-44 años, Viviendo en un Hogar con un Cónyuge Presente y al Menos un Hijo Menor de Seis Años. El gráfico de la serie muestra un rápido aumento con el tiempo. También se muestran las diferencias, que son constantes. Código R para producir la Figura 7.7 # Figura 77 labor &lt;- read.csv(&quot;CSVData/LaborForcePR.csv&quot;, header=TRUE) #labor &lt;- read.csv(&quot;../../CSVData/LaborForcePR.csv&quot;, header=TRUE) msc &lt;- labor$MSC6U time &lt;- labor$TIME year &lt;- labor$YEAR msc1 &lt;- msc[-1] diff &lt;- msc1-msc[1:(length(msc)-1)] plot(year,msc,ylim=c(-.2,0.8),xlim=c(1965,2000), ylab=&quot;&quot;,xlab=&quot;Año&quot;,type=&quot;l&quot;) points(year,msc,pch=16) lines(year[-length(year)],diff) points(year[-length(year)],diff) legend(1967,0.72,legend=c(&quot;LFPR&quot;,&quot;Diferencias&quot;),pch=c(16,1),bty=&quot;n&quot;) Para pronosticar el \\(LFPR\\) con un paseo aleatorio, comenzamos con nuestra observación más reciente, \\(LFPR_{31}=0.6407\\). Denotamos el cambio en el \\(LFPR\\) como \\(c_t\\), de modo que \\(c_t=LFPR_t-LFPR_{t-1}\\). Resulta que el cambio promedio es \\(\\overline{c}=0.0121\\) con una desviación estándar \\(s_c=0.0101\\). Así, usando un modelo de paseo aleatorio, un intervalo de predicción aproximado al 95% para el pronóstico de \\(l\\) pasos es \\[ 0.6407+0.0121l\\pm \\ 0.0202\\sqrt{l}. \\] La Figura 7.8 ilustra los intervalos de predicción para los años 1999 a 2002, inclusive. Figura 7.8: Gráfico de Series Temporales de las Tasas de Participación en la Fuerza Laboral con Valores Pronosticados para 1999-2002. La serie del medio representa los pronósticos puntuales. Las series superior e inferior representan los límites superior e inferior de los intervalos de predicción al 95%. Los datos de 1968-1998 representan valores reales. Código R para producir la Figura 7.8 # Figura 78 #labor &lt;- read.csv(&quot;CSVData/LaborForcePR.csv&quot;, header=TRUE) #labor &lt;- read.csv(&quot;../../CSVData/LaborForcePR.csv&quot;, header=TRUE) # length(msc) = 31 l &lt;- c(1:4) pred &lt;- c(msc,msc[31]+mean(diff)*l) predl &lt;- c(msc,msc[31]+mean(diff)*l-sd(diff)*2*sqrt(l)) predu &lt;- c(msc,msc[31]+mean(diff)*l+sd(diff)*2*sqrt(l)) year1 &lt;- c(year,1999,2000,2001,2002) plot(year1,pred,ylim=c(.2,0.8),xlim=c(1965,2005), ylab=&quot;&quot;,xlab=&quot;Año&quot;,type=&quot;o&quot;,pch=16) lines(year1,predl,type=&quot;o&quot;,pch=16) lines(year1,predu,type=&quot;o&quot;,pch=16) Identificación de Estacionariedad Hemos visto cómo hacer cosas útiles, como pronósticos, con modelos de paseo aleatorio. Pero, ¿cómo identificamos que una serie es una realización de un paseo aleatorio? Sabemos que el paseo aleatorio es un tipo especial de modelo no estacionario, por lo que el primer paso es examinar una serie y decidir si es estacionaria o no. La estacionariedad cuantifica la estabilidad de un proceso. Un proceso que es estrictamente estacionario tiene la misma distribución a lo largo del tiempo, por lo que deberíamos poder tomar muestras sucesivas de tamaño modesto y mostrar que tienen aproximadamente la misma distribución. Para la estacionariedad débil, la media y la varianza son estables a lo largo del tiempo, por lo que si tomamos muestras sucesivas de tamaño modesto, esperamos que el nivel promedio y la varianza sean aproximadamente similares. Para ilustrar, al examinar gráficos de series temporales, si miras las primeras cinco observaciones, las siguientes cinco, las siguientes cinco, y así sucesivamente, deberías observar niveles aproximadamente similares de promedios y desviaciones estándar. En aplicaciones de gestión de calidad, este enfoque se cuantifica mediante el uso de gráficos de control. Un gráfico de control es una herramienta gráfica útil para detectar la falta de estacionariedad en una serie temporal. La idea básica es superponer líneas de referencia llamadas límites de control en un gráfico de series temporales de los datos. Estas líneas de referencia nos ayudan a detectar visualmente tendencias en los datos e identificar puntos inusuales. La mecánica detrás de los límites de control es sencilla. Para una serie de observaciones, calculamos la media y la desviación estándar de la serie, \\(\\overline{y}\\) y \\(s_y\\). Definimos el “límite superior de control” como \\(UCL=\\overline{y}+3s_y\\) y el “límite inferior de control” como \\(LCL=\\overline{y}-3s_y\\). Los gráficos de series temporales con estos límites de control superpuestos se conocen como gráficos de control. A veces se asocia el adjetivo retrospectivo con este tipo de gráfico de control. Este adjetivo recuerda al usuario que los promedios y las desviaciones estándar se basan en todos los datos disponibles. En contraste, cuando el gráfico de control se utiliza como una herramienta de gestión continua para detectar si un proceso industrial está “fuera de control”, un gráfico de control prospectivo puede ser más adecuado. Aquí, prospectivo simplemente significa usar solo una porción temprana del proceso, que está “bajo control”, para calcular los límites de control. Un gráfico de control que nos ayuda a examinar la estabilidad de la media es el gráfico \\(Xbar\\). Un gráfico \\(Xbar\\) se crea combinando observaciones sucesivas de tamaño modesto, calculando un promedio sobre este grupo y luego creando un gráfico de control para los promedios de los grupos. Al tomar promedios sobre grupos, la variabilidad asociada con cada punto en el gráfico es menor que la de un gráfico de control para observaciones individuales. Esto permite al analista de datos obtener una imagen más clara de cualquier patrón que pueda ser evidente en la media de la serie. Un gráfico de control que nos ayuda a examinar la estabilidad de la variabilidad es el gráfico \\(R\\). Al igual que con el gráfico \\(Xbar\\), comenzamos formando grupos sucesivos de tamaño modesto. Con el gráfico \\(R\\), para cada grupo calculamos el rango, que es la observación más grande menos la más pequeña, y luego creamos un gráfico de control para los rangos de los grupos. El rango es una medida de variabilidad simple de calcular, una ventaja importante en aplicaciones de manufactura. Identificación de Paseos Aleatorios Supongamos que sospechas que una serie no es estacionaria, ¿cómo identificas que estas son realizaciones de un modelo de paseo aleatorio? Recordemos que el valor esperado de un paseo aleatorio, \\(\\mathrm{E~}y_t=y_0+t\\mu_c\\), sugiere que dicha serie sigue una tendencia lineal en el tiempo. La varianza de un paseo aleatorio, \\(\\mathrm{Var~}y_t=t\\sigma_c^2\\), sugiere que la variabilidad de una serie aumenta a medida que el tiempo \\(t\\) crece. Primero, un gráfico de control puede ayudarnos a detectar estos patrones, ya sea una tendencia lineal en el tiempo, una variabilidad creciente, o ambos. Segundo, si los datos originales siguen un modelo de paseo aleatorio, entonces la serie diferenciada sigue un modelo de proceso de ruido blanco. Si un modelo de paseo aleatorio es un modelo candidato, deberías examinar las diferencias de la serie. En este caso, el gráfico de series temporales de las diferencias debería ser un proceso estacionario, de ruido blanco, que no muestra patrones evidentes. Los gráficos de control pueden ayudarnos a detectar esta ausencia de patrones. Tercero, compara las desviaciones estándar de la serie original y la serie diferenciada. Esperamos que la desviación estándar de la serie original sea mayor que la desviación estándar de la serie diferenciada. Por lo tanto, si la serie puede representarse mediante un paseo aleatorio, esperamos una reducción sustancial en la desviación estándar al tomar diferencias. Ejemplo: Tasas de Participación en la Fuerza Laboral - Continuación. En la Figura 7.7, la serie muestra una clara tendencia ascendente mientras que las diferencias no muestran tendencias aparentes en el tiempo. Además, al calcular las diferencias de la serie, resulta que \\[ 0.1237=SD(series)&gt;SD(differences)=0.0101. \\] Por lo tanto, parece razonable utilizar tentativamente un modelo de paseo aleatorio para la serie de tasas de participación en la fuerza laboral. En el Capítulo 8, discutiremos dos dispositivos adicionales de identificación. Estos son gráficos de dispersión de la serie frente a una versión rezagada de la serie y las estadísticas resumen correspondientes llamadas autocorrelaciones. Paseo Aleatorio versus Modelo de Tendencia Lineal en el Tiempo El ejemplo de la tasa de participación en la fuerza laboral podría representarse usando un paseo aleatorio o un modelo de tendencia lineal en el tiempo. Estos dos modelos están más relacionados entre sí de lo que parece a primera vista. Para ver esta relación, recordemos que el modelo de tendencia lineal en el tiempo puede escribirse como \\[\\begin{equation} y_t = \\beta_0 + \\beta_1 t + \\varepsilon_t, \\tag{7.8} \\end{equation}\\] donde \\(\\{\\varepsilon_t\\}\\) es un proceso de ruido blanco. Si \\(\\{y_t\\}\\) es un paseo aleatorio, entonces puede modelarse como una suma parcial como en la ecuación (7.7). También podemos descomponer el proceso de ruido blanco en una media \\(\\mu_c\\) más otro proceso de ruido blanco, es decir, \\(c_t = \\mu_c + \\varepsilon_t\\). Combinando estas dos ideas, un modelo de paseo aleatorio puede escribirse como \\[\\begin{equation} y_t = y_0 + \\mu_c t + u_t \\tag{7.9} \\end{equation}\\] donde \\(u_t = \\sum_{j=1}^{t} \\varepsilon_j\\). Comparando las ecuaciones (7.8) y (7.9), vemos que los dos modelos son similares en que la porción determinística es una función lineal del tiempo. La diferencia está en el componente de error. El componente de error para el modelo de tendencia lineal en el tiempo es un proceso estacionario de ruido blanco. El componente de error para el modelo de paseo aleatorio no es estacionario porque es la suma parcial de procesos de ruido blanco. Es decir, el componente de error también es un paseo aleatorio. Muchos tratamientos introductorios del modelo de paseo aleatorio se centran en el ejemplo del “juego justo” e ignoran el término de deriva \\(\\mu_c\\). Esto es desafortunado porque la comparación entre el modelo de paseo aleatorio y el modelo de tendencia lineal en el tiempo no es tan clara cuando el parámetro \\(\\mu_c\\) es igual a cero. 7.5 Filtrado para Lograr Estacionariedad Un filtro es un procedimiento para reducir las observaciones a ruido blanco. En regresión, logramos esto simplemente restando la función de regresión de las observaciones, es decir, \\(y_i - (\\beta_0 + \\beta_1 x_{1i} + \\ldots + \\beta_k x_{ki})=\\varepsilon_i\\). La transformación de los datos es otro dispositivo para filtrar que introdujimos en el Capítulo 1 al analizar datos de corte transversal. Encontramos otro ejemplo de un filtro en la Sección 7.3. Allí, al tomar diferencias de las observaciones, reducimos una serie de paseo aleatorio a un proceso de ruido blanco. Un tema importante de este texto es usar un enfoque iterativo para ajustar modelos a datos. En particular, en este capítulo discutimos técnicas para reducir una secuencia de observaciones a una serie estacionaria. Por definición, una serie estacionaria es estable y, por lo tanto, es mucho más fácil de pronosticar que una serie inestable. Esta etapa, a veces conocida como pre-procesamiento de los datos, generalmente da cuenta de las fuentes más importantes de tendencias en los datos. El próximo capítulo presentará modelos que explican tendencias más sutiles en los datos. Transformaciones Al analizar datos longitudinales, la transformación es una herramienta importante para filtrar un conjunto de datos. Específicamente, usar una transformación logarítmica tiende a reducir los datos “dispersos”. Esta característica nos da un método alternativo para tratar un proceso donde la variabilidad parece crecer con el tiempo. Recordemos que la primera opción discutida es proponer un modelo de paseo aleatorio y examinar las diferencias de los datos. Alternativamente, se puede tomar una transformación logarítmica que ayuda a reducir la varianza creciente en el tiempo. Además, por la discusión sobre paseos aleatorios, sabemos que si tanto la varianza de la serie como la varianza de la serie logarítmica aumentan con el tiempo, las diferencias de la transformación logarítmica pueden manejar esta variabilidad creciente. Las diferencias de logaritmos naturales son particularmente útiles porque pueden interpretarse como cambios proporcionales. Para ver esto, definimos \\(pchange_t=(y_t/y_{t-1})-1\\). Entonces, \\[ \\ln y_t-\\ln y_{t-1} = \\ln \\left( \\frac{y_t}{y_{t-1}}\\right) = \\ln \\left( 1+pchange_t\\right) \\approx pchange_t. \\] Aquí usamos la aproximación de series de Taylor \\(\\ln (1+x) \\approx x\\) que es apropiada para valores pequeños de \\(|x|\\). Ejemplo: Índice Compuesto Trimestral de Standard and Poor’s. Una tarea importante de un analista financiero es cuantificar costos asociados con flujos de efectivo futuros. Consideramos aquí fondos invertidos en una medida estándar del desempeño general del mercado, el Índice Compuesto S&amp;P 500. El objetivo es pronosticar el desempeño de la cartera para descontar flujos de efectivo. En particular, examinamos el Índice Compuesto Trimestral de S&amp;P para los años 1936 a 2007, inclusive. Según los estándares actuales, este período puede no ser el más representativo porque incluye la Gran Depresión de la década de 1930. La motivación para analizar estos datos proviene del informe del “Grupo de Trabajo sobre Garantías de Madurez” del Instituto de Actuarios (1980), que analizó la serie de 1936 a 1977, inclusive. Este informe estudió el comportamiento a largo plazo de los rendimientos de inversión desde un punto de vista actuarial. Complementamos ese trabajo mostrando cómo las técnicas gráficas pueden sugerir una transformación útil para reducir los datos a un proceso estacionario. Los datos se muestran en la Figura 7.9. A partir de los valores originales del índice en el panel superior izquierdo, vemos que el nivel promedio y la variabilidad aumentan con el tiempo. Este patrón indica claramente que la serie no es estacionaria. Por nuestras discusiones en las Secciones 7.2 y 7.3, un modelo candidato con estas propiedades es el paseo aleatorio. Sin embargo, el gráfico de series temporales de las diferencias, en el panel superior derecho de la Figura 7.9, todavía indica un patrón de variabilidad creciente en el tiempo. Las diferencias no son un proceso de ruido blanco, por lo que el paseo aleatorio no es un modelo adecuado para el Índice S&amp;P 500. Una transformación alternativa es considerar los valores logarítmicos de la serie. El gráfico de series temporales de los valores logarítmicos, presentado en el panel inferior izquierdo de la Figura 7.9, indica que el nivel promedio de la serie aumenta con el tiempo y no es constante. Por lo tanto, el índice logarítmico no es estacionario. Otra estrategia es examinar las diferencias de la serie logarítmica. Esto es especialmente deseable al analizar índices o “cestas básicas”, porque la diferencia de logaritmos puede interpretarse como cambios proporcionales. En el gráfico final de series temporales, en el panel inferior derecho de la Figura 7.9, vemos que hay menos patrones discernibles en la serie transformada, la diferencia de logaritmos. Esta serie transformada parece ser estacionaria. Es interesante notar que parece haber un mayor nivel de volatilidad al comienzo de la serie. Este tipo de volatilidad cambiante es más difícil de modelar y ha sido objeto de considerable atención en la literatura de economía financiera en años recientes (ver, por ejemplo, Hardy, 2003). Figura 7.9: Gráficos de Series Temporales del Índice S &amp; P 500. El panel superior izquierdo muestra la serie original que no es estacionaria en la media ni en la variabilidad. El panel superior derecho muestra las diferencias de la serie, que no son estacionarias en la variabilidad. El panel inferior izquierdo muestra el índice logarítmico, que no es estacionario en la media. El panel inferior derecho muestra las diferencias del índice logarítmico, que parecen ser estacionarias en la media y en la variabilidad. Código R para producir la Figura 7.9 # Figura 79 SandPQ1 &lt;- read.csv(&quot;CSVData/SP500Quarterly.csv&quot;, header=TRUE) #SandPQ1 &lt;- read.csv(&quot;../../CSVData/SP500Quarterly.csv&quot;, header=TRUE) YEAR &lt;- SandPQ1$YEAR par(mfrow=c(2, 2),mar=c(5,4,1,1)) plot(YEAR,SandPQ1$SPINDEX, type=&quot;l&quot;, ylab=&quot;Índice S&amp;P 500&quot;,las=1) par(mar=c(5,5,1,.1)) plot(YEAR,SandPQ1$DIFFINDEX, type=&quot;l&quot;, ylab=&quot;Diferencias S&amp;P 500&quot;,las=1) par(mar=c(4.2,4,1.8,1)) plot(YEAR,SandPQ1$LNSPINDEX, type=&quot;l&quot;, ylab=&quot;Índice Logarítmico S&amp;P 500&quot;,las=1) par(mar=c(4.2,5,1.8,.1)) plot(YEAR,SandPQ1$DIFFLNSP, type=&quot;l&quot;, ylab=&quot;Diferencias de Logaritmos S&amp;P 500&quot;,las=1) 7.6 Evaluación de Pronósticos Evaluar la precisión de los pronósticos es importante al modelar datos de series temporales. En esta sección, presentamos técnicas de evaluación de pronósticos que: Ayudan a detectar tendencias o patrones recientes no anticipados en los datos. Son útiles para comparar diferentes métodos de pronóstico. Proporcionan un método intuitivo y fácil de explicar para evaluar la precisión de los pronósticos. En las primeras cinco secciones del Capítulo 7, presentamos varias técnicas para detectar patrones en los residuales de un modelo ajustado. Las medidas que resumen la distribución de los residuales se denominan estadísticas de bondad de ajuste. Como vimos en nuestro estudio de modelos transversales, al ajustar varios modelos diferentes a un conjunto de datos, introducimos la posibilidad de sobreajustar los datos. Para abordar esta preocupación, utilizaremos técnicas de validación fuera de muestra, similares a las introducidas en la Sección 6.5. Para realizar una validación fuera de muestra de un modelo propuesto, idealmente se desarrollaría el modelo en un conjunto de datos y luego se corroboraría la utilidad del modelo en un segundo conjunto de datos independiente. Debido a que rara vez están disponibles dos conjuntos de datos ideales, en la práctica podemos dividir un conjunto de datos en dos submuestras: una submuestra de desarrollo del modelo y una submuestra de validación. Para datos longitudinales, la práctica consiste en utilizar la primera parte de la serie, las primeras \\(T_1\\) observaciones, para desarrollar uno o más modelos candidatos. La parte posterior de la serie, las últimas \\(T_2=T-T_1\\) observaciones, se utiliza para evaluar los pronósticos. Por ejemplo, podríamos tener diez años de datos mensuales, de modo que \\(T=120\\). Sería razonable utilizar los primeros ocho años de datos para desarrollar un modelo y los últimos dos años de datos para validación, obteniendo \\(T_1=96\\) y \\(T_2=24\\). Por lo tanto, las observaciones \\(y_1,\\ldots , y_{T_1}\\) se utilizan para desarrollar un modelo. A partir de estas \\(T_1\\) observaciones, podemos determinar los parámetros del modelo candidato. Usando el modelo ajustado, podemos determinar los valores ajustados para la submuestra de validación del modelo para \\(t = T_1 + 1,T_1+2, \\ldots, T_1+T_2\\). Al tomar la diferencia entre los valores reales y los ajustados, obtenemos los residuales de pronósticos a un paso, denotados por \\(e_t=y_t-\\widehat{y}_t\\). Estos residuales de pronóstico son las cantidades básicas que utilizaremos para evaluar y comparar técnicas de pronóstico. Para comparar modelos, utilizamos un proceso de cuatro pasos similar al descrito en la Sección 6.5, descrito a continuación. Proceso de Validación Fuera de Muestra Divide la muestra de tamaño \\(T\\) en dos submuestras: una submuestra de desarrollo del modelo (\\(t=1,\\ldots,T_1\\)) y una submuestra de validación del modelo (\\(t=T_1+1, \\ldots, T_1 + T_2\\)). Utilizando la submuestra de desarrollo del modelo, ajusta un modelo candidato al conjunto de datos \\(t=1,\\ldots,T_1\\). Usando el modelo creado en el Paso 2 y las variables dependientes hasta e incluyendo \\(t-1\\), pronostica la variable dependiente \\(\\widehat{y}_t\\), donde \\(t=T_1+1, \\ldots, T_1+T_2\\). Utiliza las observaciones reales y los valores ajustados calculados en el Paso 3 para calcular los residuales de pronóstico a un paso, \\(e_t = y_t- \\widehat{y}_t\\), para la submuestra de validación del modelo. Resume estos residuales con una o más estadísticas de comparación, descritas a continuación. Repite los Pasos 2 al 4 para cada uno de los modelos candidatos. Elige el modelo con el menor conjunto de estadísticas de comparación. La validación fuera de muestra se puede utilizar para comparar la precisión de los pronósticos de prácticamente cualquier modelo de pronóstico. Como vimos en la Sección 6.5, no estamos limitados a comparaciones donde un modelo es un subconjunto de otro, donde los modelos competidores usan las mismas unidades para la respuesta, y así sucesivamente. Hay varias estadísticas que se utilizan comúnmente para comparar pronósticos. Estadísticas Comúnmente Usadas para Comparar Pronósticos La estadística de error medio, definida por \\[ ME=\\frac{1}{T_2}\\sum_{t=T_1+1}^{T_1+T_2}e_t. \\] Esta estadística mide tendencias recientes no anticipadas por el modelo. El error porcentual medio, definido por \\[ MPE=\\frac{100}{T_2}\\sum_{t=T_1+1}^{T_1+T_2}\\frac{e_t}{y_t}. \\] Esta estadística también mide tendencias, pero examina el error relativo al valor real. El error cuadrático medio, definido por \\[ MSE=\\frac{1}{T_2}\\sum_{t=T_1+1}^{T_1+T_2}e_t^2. \\] Esta estadística puede detectar más patrones que \\(ME\\). Es la misma que la estadística transversal \\(SSPE\\), excepto por la división por \\(T_2\\). El error absoluto medio, definido por \\[ MAE=\\frac{1}{T_2}\\sum_{t=T_1+1}^{T_1+T_2}|e_t|. \\] Al igual que \\(MSE\\), esta estadística puede detectar más patrones de tendencia que \\(ME\\). Las unidades de \\(MAE\\) son las mismas que la variable dependiente. El error absoluto porcentual medio, definido por \\[ MAPE=\\frac{100}{T_2}\\sum_{t=T_1+1}^{T_1+T_2}|\\frac{e_t}{y_t}|. \\] Al igual que \\(MAE\\), esta estadística puede detectar más que patrones de tendencia. Al igual que \\(MPE\\), examina el error relativo al valor real. Ejemplo: Tasas de Participación en la Fuerza Laboral - Continuación. Podemos utilizar medidas de validación fuera de muestra para comparar dos modelos para las \\(LFPR\\)s; el modelo de tendencia lineal en el tiempo y el modelo de paseo aleatorio. Para esta ilustración, examinamos las tasas de participación laboral para los años 1968 a 1994, inclusive. Esto corresponde a \\(T_1 = 27\\) observaciones definidas en el Paso 1. Posteriormente, se recopilaron datos de tasas para los años 1995 a 1998, inclusive, correspondientes a \\(T_2 = 4\\) para la validación fuera de muestra. Para el Paso 2, ajustamos cada modelo usando \\(t=1,\\ldots,27\\), como se mostró anteriormente en este capítulo. Para el Paso 3, los pronósticos a un paso son: \\[ \\widehat{y}_t = 0.2574 + 0.0145t \\] y \\[ \\widehat{y}_t = y_{t-1} + 0.0132 \\] para los modelos de tendencia lineal en el tiempo y de paseo aleatorio, respectivamente. Para el Paso 4, Tabla 7.2 resume las estadísticas de comparación de pronósticos. Basándonos en estas estadísticas, la elección del modelo es claramente el paseo aleatorio. Tabla 7.2. Comparación de Pronósticos Fuera de Muestra \\[ \\small{ \\begin{array}{l|ccccc} \\hline &amp; ME &amp; MPE &amp; MSE &amp; MAE &amp; MAPE \\\\ \\hline \\text{Modelo de tendencia lineal en el tiempo} &amp; -0.049 &amp; -7.657 &amp; 0.003 &amp; 0.049 &amp; 7.657 \\\\ \\text{Modelo de paseo aleatorio} &amp; -0.019 &amp; -2.946 &amp; 0.001 &amp; 0.019 &amp; 3.049 \\\\ \\hline \\end{array} } \\] Código R para producir la Tabla 7.2 # Tabla 7.2 #labor &lt;- read.csv(&quot;CSVData/LaborForcePR.csv&quot;, header=TRUE) #labor &lt;- read.csv(&quot;../../CSVData/LaborForcePR.csv&quot;, header=TRUE) # msc &lt;- labor$MSC6U # time &lt;- labor$TIME # year &lt;- labor$YEAR # msc1 &lt;- msc[-1] # diff &lt;- msc1-msc[1:(length(msc)-1)] t &lt;- 1:31 # Ajustar modelo de tendencia lineal linear &lt;- lm(msc~t, subset=(year %in% c(1968:1994))) pred1 &lt;- linear$coef[1]+linear$coef[2]*t[year %in% c(1995:1998)] #linear$coef[1];linear$coef[2] # Verificar coeficientes de regresión # Ajustar modelo de paseo aleatorio diff &lt;- msc1-msc[1:(length(msc)-1)] diff1 &lt;- c(NA,diff) rw &lt;- mean(diff1[year %in% c(1969:1994)]) pred2 &lt;- msc[year==1994]+rw *c(1:4) y58 &lt;- msc[year %in% c(1995:1998)] e1 &lt;- y58-pred1 e2 &lt;- y58-pred2 me1 &lt;- mean(e1) me2 &lt;- mean(e2) mpe1 &lt;- 100*mean(e1/y58) mpe2 &lt;- 100*mean(e2/y58) mse1 &lt;- mean(e1^2) mse2 &lt;- mean(e2^2) mae1 &lt;- mean(abs(e1)) mae2 &lt;- mean(abs(e2)) mape1 &lt;- 100*mean(abs(e1/y58)) mape2 &lt;- 100*mean(abs(e2/y58)) mat &lt;- matrix(c(me1,me2,mpe1,mpe2,mse1,mse2,mae1,mae2,mape1,mape2), nrow=2,ncol=5) rownames(mat)=c(&quot;Modelo de tendencia lineal en el tiempo&quot;,&quot;Modelo de paseo aleatorio&quot;) colnames(mat)=c(&quot;ME&quot;,&quot;MPE&quot;,&quot;MSE&quot;,&quot;MAE&quot;,&quot;MAPE&quot;) knitr::kable(mat, digits = 3) ME MPE MSE MAE MAPE Modelo de tendencia lineal en el tiempo -0.049 -7.657 0.003 0.049 7.657 Modelo de paseo aleatorio -0.019 -2.946 0.001 0.019 3.049 7.7 Lecturas Adicionales y Referencias Durante muchos años, los actuarios en América del Norte fueron introducidos al análisis de series temporales a través de Miller y Wichern (1977), Abraham y Ledolter (1983) y Pindyck y Rubinfeld (1991). Una introducción más reciente es Diebold (2004), que contiene una breve introducción a los modelos de cambio de régimen. Debido a las dificultades relacionadas con su especificación y su uso limitado en pronósticos, no exploramos más los modelos causales en este texto. Para más detalles sobre los modelos causales, el lector interesado puede consultar Pindyck y Rubinfeld (1991). Referencias del Capítulo “Report of the Maturity Guarantees Working Party” (1980). Journal of the Institute of Actuaries 107, pp. 103-213. Abraham, Bovas and Johannes Ledolter (1983). Statistical Methods for Forecasting. John Wiley &amp; Sons, New York. Diebold, Francis X. (2004). Elements of Forecasting, Third Edition. Thompson South-Western, Mason, OH. Frees, Edward W. (2006). Forecasting of labor force participation rates. The Journal of Official Statistics 22(3), 453-485. Fullerton, Howard N., Jr. (1999). Labor force projections to 2008: steady growth and changing composition. Monthly Labor Review, November, pp. 19-32. Granger, Clive W. J and P. Newbold (1974). Spurious regressions in econometrics. Journal of Econometrics 2, 111-120. Hardy, Mary (2001). A regime-switching model of long-term stock returns. North American Actuarial Journal 5(2), 41-53. Hardy, Mary (2003). Investment Guarantees: Modeling and Risk Management for Equity-Linked Life Insurance. John Wiley &amp; Sons, New York. Miller, Robert B. and Dean W. Wichern (1977). Intermediate Business Statistics: Analysis of Variance, Regression and Time Series. Holt, Rinehart and Winston, New York. Pindyck, R.S. and D.L. Rubinfeld (1991). Econometric Models and Economic Forecasts, Third Edition, McGraw-Hill, New York. 7.8 Ejercicios 7.1. Considera un paseo aleatorio \\(\\{y_t \\}\\) como la suma parcial de un proceso de ruido blanco \\(\\{ c_t \\}\\) con media \\(\\mathrm{E}~c_t= \\mu_c\\) y varianza \\(\\mathrm{Var}~c_t = \\sigma_c^2\\). Usa la ecuación (7.7) para demostrar: \\(\\mathrm{E}~y_t= y_0 + t \\mu_c\\), donde \\(y_0\\) es el valor inicial y \\(\\mathrm{Var}~y_t= t \\sigma_c^2\\). 7.2. Considera un paseo aleatorio \\(\\{y_t \\}\\) como la suma parcial de un proceso de ruido blanco \\(\\{ c_t \\}\\). Demuestra que el error de pronóstico a \\(l\\) pasos es \\(y_{T+l}-\\widehat{y_{T+l}} = \\sum_{j=1}^l (c_{T+j} - \\bar{c} ).\\) Demuestra que la varianza aproximada del error de pronóstico a \\(l\\) pasos es \\(l \\sigma_c^2.\\) 7.3. Tasas de Cambio del Euro. La tasa de cambio que consideramos es la cantidad de euros que se puede adquirir con un dólar estadounidense. Tenemos \\(T=699\\) observaciones diarias del periodo del 1 de abril de 2005 al 8 de enero de 2008. Estos datos fueron obtenidos de la Reserva Federal (informe H10). Fuente: Banco de la Reserva Federal de Nueva York. Nota: Los datos se basan en tasas de compra al mediodía en Nueva York de una muestra de participantes del mercado y representan tasas establecidas para transferencias cablegráficas pagaderas en las monedas indicadas. Estas son también las tasas de cambio requeridas por la Comisión de Valores de Estados Unidos para el sistema de divulgación integrado para emisores privados extranjeros. Figura 7.10: Gráfico de series temporales de la tasa de cambio del euro. La Figura 7.10 es un gráfico de series temporales de la tasa de cambio del euro. a(i). Define el concepto de una serie temporal estacionaria. a(ii). ¿Es la serie EURO estacionaria? Usa tu definición en la parte a(i) para justificar tu respuesta. Basándote en la inspección de la Figura 7.10 en la parte (a), decides ajustar un modelo de tendencia cuadrática a los datos. La Figura 7.11 superpone los valores ajustados en un gráfico de la serie. b(i). Menciona varias estadísticas básicas de regresión que resumen la calidad del ajuste. b(ii). Describe brevemente cualquier patrón de residuales que observes en la Figura 7.11. b(iii). Aquí, el TIEMPO varía de \\(1, 2, \\ldots, 699\\). Usando este modelo, calcula el pronóstico a tres pasos correspondiente a TIEMPO = 702. Figura 7.11: Curva ajustada cuadrática superpuesta en la tasa de cambio del euro. Para investigar un enfoque diferente, DIFFEURO, calcula la diferencia de EURO. Decides modelar DIFFEURO como un proceso de ruido blanco. c(i). ¿Cuál es el nombre del modelo correspondiente para EURO? c(ii). El valor más reciente de EURO es \\(EURO_{699} = 0.6795\\). Usando el modelo identificado en la parte c(i), proporciona un pronóstico a tres pasos correspondiente a TIEMPO = 702. c(iii). Usando el modelo identificado en la parte c(i) y el pronóstico puntual en la parte c(ii), proporciona el intervalo de predicción al 95% correspondiente para \\(EURO_{702}\\). "],["C8AR.html", "Capítulo 8 Autocorrelaciones y Modelos Autorregresivos 8.1 Autocorrelaciones 8.2 Modelos Autorregresivos de Orden Uno 8.3 Estimación y Verificación de Diagnóstico 8.4 Suavización y Predicción 8.5 Modelado y Pronóstico de Box-Jenkins 8.6 Aplicación: Tasas de Cambio de Hong Kong 8.7 Lecturas Adicionales y Referencias", " Capítulo 8 Autocorrelaciones y Modelos Autorregresivos Vista previa del capítulo. Este capítulo continúa nuestro estudio de los datos de series temporales. El Capítulo 7 introdujo técnicas para determinar patrones principales que proporcionan un buen primer paso para los pronósticos. El Capítulo 8 presenta técnicas para detectar tendencias sutiles en el tiempo y modelos para acomodar estas tendencias. Estas técnicas detectan y modelan relaciones entre los valores actuales y pasados de una serie utilizando conceptos de regresión. 8.1 Autocorrelaciones Aplicación: Retornos de Bonos con Inflación Para motivar los métodos introducidos en este capítulo, trabajamos en el contexto de la serie de retornos de bonos con inflación. A partir de enero de 2003, el Departamento del Tesoro de los Estados Unidos estableció un índice de bonos con inflación que resume los retornos de bonos a largo plazo ofrecidos por el Departamento del Tesoro que están indexados por inflación. Para un bono protegido contra la inflación del Tesoro (TIPS), el principal del bono está indexado al valor (con un rezago de tres meses) del índice de precios al consumidor (no ajustado estacionalmente). Luego, el bono paga un cupón semestral a una tasa determinada en la subasta cuando se emite el bono. El índice que examinamos es el promedio no ponderado de las ofertas para todos los TIPS con plazos restantes de vencimiento de 10 o más años. Se consideran valores mensuales del índice desde enero de 2003 hasta marzo de 2007, para un total de \\(T=51\\) retornos. Un gráfico de series temporales de los datos se presenta en la Figura 8.1. Este gráfico sugiere que la serie es estacionaria, por lo que es útil examinar la distribución de la serie a través de estadísticas resumidas que aparecen en Tabla 8.1. Figura 8.1: Gráfico de Series Temporales del Índice de Bonos con Inflación. Valores mensuales de enero de 2003 a marzo de 2007, inclusive. Tabla 8.1. Estadísticas Resumidas del Índice de Bonos con Inflación \\[ \\small{ \\begin{array}{lccccc} \\hline &amp; &amp; &amp; \\text{Desviación} &amp; &amp; \\\\ \\text{Variable} &amp; \\text{Media} &amp; \\text{Mediana} &amp; \\text{Estándar} &amp; \\text{Mínimo} &amp; \\text{Máximo} \\\\ \\hline \\text{ÍNDICE} &amp; 2.245 &amp; 2.26 &amp; 0.259 &amp; 1.77 &amp; 2.80 \\\\ \\hline ~~~Fuente: \\text{Tesoro de los EE.UU.} \\end{array} } \\] Código R para producir la Figura 8.1 y la Tabla 8.1 # Figura 81 inflation &lt;- read.csv(&quot;CSVData/InflationBond.csv&quot;, header=TRUE) #inflation &lt;- read.csv(&quot;../../CSVData/InflationBond.csv&quot;, header=TRUE) index &lt;- inflation$INFBOND foo &lt;- ts(index, freq = 12, start = c(2003,1), end=c(2007,3)) ts.plot(foo, ylab=&quot;Índice&quot;, xlab=&quot;Año&quot;, type=&quot;o&quot;, ylim=c(1.5,3.0), gpars=list(pch=16)) Tab81 &lt;- cbind(Mean=mean(index), Median=median(index), Std=sd(index), Minimum=min(index), Maximum=max(index)) knitr::kable(Tab81, digits = 4) Mean Median Std Minimum Maximum 2.2449 2.26 0.2589 1.77 2.8 Nuestro objetivo es detectar patrones en los datos y proporcionar modelos para representar estos patrones. Aunque la Figura 8.1 muestra una serie estacionaria sin tendencias importantes, algunos patrones sutiles son evidentes. A partir de mediados de 2003 y luego a principios de 2004, vemos grandes incrementos seguidos por una serie de disminuciones en el índice. A partir de 2005, parece estar ocurriendo un patrón de incremento con algún comportamiento cíclico. Aunque no está claro qué fenómeno económico representan estos patrones, no es lo que esperaríamos ver en un proceso de ruido blanco. Para un proceso de ruido blanco, una serie puede aumentar o disminuir al azar de un período al siguiente, produciendo una serie no suave, “irregular” a lo largo del tiempo. Para ayudar a entender estos patrones, la Figura 8.2 presenta un diagrama de dispersión de la serie (\\(y_t\\)) frente a su valor rezagado (\\(y_{t-1}\\)). Dado que este es un paso crucial para entender este capítulo, la Tabla 8.2 presenta un pequeño subconjunto de los datos para que puedas ver exactamente qué representa cada punto en el diagrama de dispersión. La Figura 8.2 muestra una fuerte relación entre \\(y_t\\) y \\(y_{t-1}\\); modelaremos esta relación en la siguiente sección. Figura 8.2: Índice de Bonos con Inflación frente a Valor Rezagado. Este diagrama de dispersión revela una relación lineal entre el índice y su valor rezagado. Tabla 8.2. Índice y Valor Rezagado para los Primeros Cinco de \\(T=51\\) Valores \\[ \\small{ \\begin{array}{l|crrrr} \\hline t &amp; 1~~ &amp; 2~~ &amp; 3~~ &amp; 4~~ &amp; 5~~ \\\\ \\hline \\text{Índice }(y_t) &amp; 2.72 &amp; 2.50 &amp; 2.52 &amp; 2.72 &amp; 2.40 \\\\ \\text{Índice Rezagado } (y_{t-1}) &amp; * &amp; 2.72 &amp; 2.50 &amp; 2.52 &amp; 2.72 \\\\ \\hline \\end{array} } \\] Código R para producir la Figura 8.2 # Figura 82 #inflation &lt;- read.csv(&quot;CSVData/InflationBond.csv&quot;, header=TRUE) #inflation &lt;- read.csv(&quot;../../CSVData/InflationBond.csv&quot;, header=TRUE) #index &lt;- inflation$INFBOND n &lt;- length(index) plot(index[-1], index[-n], xlim=c(1.5,3.0), ylim=c(1.5,3.0), xlab=&quot;Índice Rezagado&quot;, ylab=&quot;Índice de Bonos con Inflación&quot;) Autocorrelaciones Los diagramas de dispersión son útiles porque muestran gráficamente relaciones no lineales, así como lineales, entre dos variables. Como establecimos en el Capítulo 2, las correlaciones pueden usarse para medir la relación lineal entre dos variables. Recuerda que al tratar con datos transversales, resumimos relaciones entre {\\(y_t\\)} y {\\(x_t\\)} usando la estadística de correlación: \\[ r = \\frac{1}{(T-1)s_{x}s_y} \\sum_{t=1}^{T} \\left( x_t - \\overline{x}\\right) \\left( y_t-\\overline{y} \\right) . \\] Ahora imitamos esta estadística usando la serie {\\(y_{t-1}\\)} en lugar de {\\(x_t\\)}. Con este reemplazo, usa \\(\\overline{y}\\) en lugar de \\(\\overline{x}\\) y, para el denominador, usa \\(s_y\\) en lugar de \\(s_x\\). Con esta última sustitución, tenemos \\((T-1) s_y^2 = \\sum_{t=1}^{T}(y_t-\\overline{y})^2\\). Nuestra estadística de correlación resultante es: \\[ r_1 = \\frac{\\sum_{t=2}^{T} \\left( y_{t-1}-\\overline{y}\\right) \\left( y_t- \\overline{y}\\right) }{\\sum_{t=1}^{T} (y_t-\\overline{y})^2}. \\] Esta estadística se conoce como autocorrelación, es decir, una correlación de la serie consigo misma. Esta estadística resume la relación lineal entre {\\(y_t\\)} y {\\(y_{t-1}\\)}, es decir, observaciones que están separadas por una unidad de tiempo. También será útil resumir la relación lineal entre observaciones que están separadas por \\(k\\) unidades de tiempo, {\\(y_t\\)} y {\\(y_{t-k}\\)}, de la siguiente manera. Definición. La estadística de autocorrelación rezagada en k es: \\[ r_k = \\frac{\\sum_{t=k+1}^{T}\\left( y_{t-k}-\\overline{y}\\right) \\left( y_t- \\overline{y}\\right) }{\\sum_{t=1}^{T}(y_t-\\overline{y})^2}, ~~~~k=1,2, \\ldots \\] Las propiedades de las autocorrelaciones son similares a las correlaciones. Al igual que con la estadística de correlación usual \\(r\\), el denominador, \\(\\sum_{t=1}^{T}(y_t - \\overline{y})^2\\), siempre es no negativo y, por lo tanto, no cambia el signo del numerador. Usamos este dispositivo de reescalado para que \\(r_k\\) siempre esté dentro del intervalo [-1, 1]. Así, al interpretar \\(r_k\\), un valor cercano a -1, 0 y 1 significa, respectivamente, una relación negativa fuerte, casi nula o positiva fuerte entre \\(y_t\\) y \\(y_{t-k}\\). Si existe una relación positiva entre  \\(y_t\\) y \\(y_{t-1}\\), entonces \\(r_1 &gt; 0\\) y el proceso se dice positivamente autocorrelacionado. Por ejemplo, en la Tabla 8.3 se muestran las primeras cinco autocorrelaciones de la serie de bonos con inflación. Estas autocorrelaciones indican que existe una relación positiva entre observaciones adyacentes. Tabla 8.3. Autocorrelaciones para la Serie de Bonos con Inflación \\[ \\small{ \\begin{array}{l|ccccc} \\hline \\text{Rezago } k &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\ \\hline \\text{Autocorrelación } r_k &amp; 0.814 &amp; 0.632 &amp; 0.561 &amp; 0.447 &amp; 0.367 \\\\ \\hline \\end{array} } \\] Código R para producir la Tabla 8.3 temp &lt;- acf(index, plot = FALSE) Tab83 &lt;- temp$acf[1:6] knitr::kable(Tab83, digits = 3) x 1.000 0.814 0.632 0.561 0.447 0.367 8.2 Modelos Autorregresivos de Orden Uno Definición y Propiedades del Modelo En la Figura 8.2 notamos la fuerte relación entre los valores pasados inmediatos y los valores actuales del índice de bonos con inflación. Esto sugiere usar \\(y_{t-1}\\) para explicar \\(y_t\\) en un modelo de regresión. Usar valores previos de una serie para predecir valores actuales de la misma serie se denomina, no sorprendentemente, una autorregresión. Cuando solo se usa el pasado inmediato como predictor, utilizamos el siguiente modelo. Definición. El modelo autorregresivo de orden uno, denotado como \\(AR(1)\\), se escribe como \\[\\begin{equation} y_t = \\beta_0 + \\beta_1 y_{t-1} + \\varepsilon_t, ~~~ t=2,\\ldots,T, \\tag{8.1} \\end{equation}\\] donde {\\(\\varepsilon_t\\)} es un proceso de ruido blanco tal que \\(\\mathrm{Cov}(\\varepsilon_{t+k}, y_t)=0\\) para \\(k&gt;0\\) y \\(\\beta_0\\) y \\(\\beta_1\\) son parámetros desconocidos. En el modelo \\(AR\\)(1), el parámetro \\(\\beta_0\\) puede ser cualquier constante fija. Sin embargo, el parámetro \\(\\beta_1\\) está restringido a estar entre -1 y 1. Al hacer esta restricción, se puede establecer que la serie \\(AR\\)(1) {\\(y_t\\)} es estacionaria. Nota que si \\(\\beta_1 = 1\\), entonces el modelo es un paseo aleatorio y, por lo tanto, no es estacionario. Esto se debe a que, si \\(\\beta_1 = 1\\), entonces la ecuación (8.1) puede reescribirse como: \\[ y_t - y_{t-1} = \\beta_0 + \\varepsilon_t. \\] Si la diferencia de una serie forma un proceso de ruido blanco, entonces la serie misma debe ser un paseo aleatorio. La ecuación (8.1) es útil en la discusión de propiedades del modelo. Podemos ver un modelo \\(AR\\)(1) como una generalización tanto de un proceso de ruido blanco como de un modelo de paseo aleatorio. Si \\(\\beta_1=0\\), entonces la ecuación (8.1) se reduce a un proceso de ruido blanco. Si \\(\\beta_1 = 1\\), entonces la ecuación (8.1) es un paseo aleatorio. Un proceso estacionario donde hay una relación lineal entre \\(y_{t-2}\\) y \\(y_t\\) se dice autorregresivo de orden 2, y de manera similar para procesos de mayor orden. La discusión de procesos de mayor orden está en la Sección 8.5. Selección del Modelo Al examinar los datos, ¿cómo reconocer que un modelo autorregresivo puede ser un modelo adecuado? Primero, un modelo autorregresivo es estacionario, por lo que un gráfico de control es un buen dispositivo para examinar gráficamente los datos y buscar estabilidad. Segundo, las realizaciones adyacentes de un modelo \\(AR\\)(1) deberían estar relacionadas; esto puede detectarse visualmente con un diagrama de dispersión de valores actuales frente a valores pasados inmediatos de la serie. Tercero, podemos reconocer un modelo \\(AR\\)(1) a través de su estructura de autocorrelación, como sigue. Una propiedad útil del modelo \\(AR\\)(1) es que la correlación entre puntos separados por \\(k\\) unidades de tiempo resulta ser \\(\\beta_1^{k}\\). En otras palabras, \\[\\begin{equation} \\rho_k = \\mathrm{Corr}(y_t,y_{t-k}) = \\frac{\\mathrm{Cov}(y_t,y_{t-k})}{\\sqrt{\\mathrm{Var}(y_t)\\mathrm{Var}(y_{t-k})}} = \\frac{\\mathrm{Cov}(y_t,y_{t-k})}{\\sigma_y^2} = \\beta_1^k. \\tag{8.2} \\end{equation}\\] Las primeras dos igualdades son definiciones y la tercera se debe a la estacionariedad. Al lector se le pide verificar la cuarta igualdad en los ejercicios. Por lo tanto, los valores absolutos de las autocorrelaciones de un proceso \\(AR\\)(1) se vuelven más pequeños a medida que aumenta el rezago \\(k\\). De hecho, disminuyen a un ritmo geométrico. Observamos que para un proceso de ruido blanco, tenemos \\(\\beta_1 = 0\\), y así \\(\\rho_k\\) debería ser igual a cero para todos los rezagos \\(k\\). Para ayudar en la identificación del modelo, usamos la idea de igualar las autocorrelaciones observadas \\(r_k\\) con las cantidades que esperamos de la teoría, \\(\\rho_k\\). Para ruido blanco, el coeficiente de autocorrelación muestral debería ser aproximadamente cero para cada rezago \\(k\\). Aunque \\(r_k\\) está algebraicamente acotado entre -1 y 1, surge la pregunta, ¿qué tan grande necesita ser \\(r_k\\), en valor absoluto, para considerarse significativamente diferente de cero? La respuesta a este tipo de pregunta se da en términos del error estándar de la estadística. Bajo la hipótesis de no autocorrelación, una buena aproximación para el error estándar de la estadística de autocorrelación en el rezago \\(k\\) es: \\[ se(r_k) = \\frac{1}{\\sqrt{T}}. \\] Nuestra regla práctica es que si \\(r_k\\) supera \\(2 \\times se(r_k)\\) en valor absoluto, puede considerarse significativamente diferente de cero. Esta regla se basa en un nivel de significancia del 5%. Ejemplo: Índice de Bonos con Inflación - Continuación. ¿Es un modelo de proceso de ruido blanco un buen candidato para representar esta serie? Las autocorrelaciones se muestran en la Tabla 8.2. Para un modelo de proceso de ruido blanco, esperamos que cada autocorrelación \\(r_k\\) sea cercana a cero, pero notamos que, por ejemplo, \\(r_1=0.814\\). Dado que hay \\(T=51\\) retornos disponibles, el error estándar aproximado de cada autocorrelación es: \\[ se(r_k) = \\frac{1}{\\sqrt{51}} = 0.140. \\] Así, \\(r_1\\) es 0.814 / 0.140 = 5.81 errores estándar por encima de cero. Usando la distribución normal como base de referencia, esta diferencia es significativa, lo que implica que un proceso de ruido blanco no es un modelo candidato adecuado. ¿Es el modelo autorregresivo de orden uno una elección adecuada? Bueno, porque \\(\\rho_k\\) = \\(\\beta_1^{k}\\), una buena estimación de \\(\\beta_1\\)=\\(\\rho_1\\) es \\(r_1=0.814\\). Si este es el caso, entonces bajo el modelo \\(AR\\)(1), otra estimación de \\(\\rho_k\\) es \\((0.814)^{k}\\). Así, tenemos dos estimaciones de \\(\\rho_k\\): (i) \\(r_k\\), una estimación empírica que no depende de un modelo paramétrico y (ii) \\((r_1)^{k}\\), que depende del modelo \\(AR\\)(1). Para ilustrar, véase la Tabla 8.4. Tabla 8.4. Comparación de Autocorrelaciones Empíricas con las Estimadas bajo el Modelo \\(AR\\)(1) \\[ \\small{ \\begin{array}{l|ccccc} \\hline \\text{Rezago } k &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\ \\hline \\text{Estimado } \\rho_k \\text{ bajo el modelo } AR(1) &amp; 0.814 &amp; (0.814)^2 &amp; (0.814)^{3} &amp; (0.814)^{4} &amp; (0.814)^{5} \\\\ &amp; &amp; =.66 &amp; =.54 &amp; =.44 &amp; =.36 \\\\ \\text{Autocorrelación } r_k &amp; 0.814 &amp; 0.632 &amp; 0.561 &amp; 0.447 &amp; 0.367 \\\\ \\hline \\end{array} } \\] Dado que el error estándar aproximado es \\(se(r_k) = 0.14\\), parece haber una buena coincidencia entre los dos conjuntos de autocorrelaciones. Debido a esta coincidencia, en la Sección 8.3 discutiremos cómo ajustar el modelo \\(AR\\)(1) a este conjunto de datos. Proceso de Meandro Muchos procesos muestran el patrón de que puntos adyacentes están relacionados entre sí. Pensando en un proceso que evoluciona como un río, Roberts (1991) describe gráficamente tales procesos como meandros. Para complementar esta noción intuitiva, decimos que un proceso está en meandro si la autocorrelación de rezago uno de la serie es positiva. Por ejemplo, a partir de los gráficos en las Figuras 8.1 y 8.2, parece claro que el Índice de Bonos con Inflación es un buen ejemplo de una serie en meandro. De hecho, un modelo \\(AR\\)(1) con un coeficiente de pendiente positivo es un proceso de meandro. ¿Qué sucede en el caso en que el coeficiente de pendiente se aproxima a uno, resultando en un paseo aleatorio? Considere el ejemplo de Tasas de Cambio de Hong Kong dado en el Capítulo 7. Aunque se introdujo como un modelo de tendencia cuadrática en el tiempo, un ejercicio muestra que la serie puede modelarse de manera más apropiada como un paseo aleatorio. Parece claro que cualquier punto en el proceso está altamente relacionado con cada punto adyacente en el proceso. Para enfatizar este punto, la Figura 8.3 muestra una fuerte relación lineal entre el valor actual y el valor pasado inmediato de las tasas de cambio. Debido a la fuerte relación lineal en la Figura 8.3, utilizaremos el término “proceso de meandro” para un conjunto de datos que puede modelarse utilizando un paseo aleatorio. Figura 8.3: Tasas de Cambio Diarias de Hong Kong frente a Valores de Rezago. Código R para Generar la Figura 8.3 # Figura 83 exchange &lt;- read.csv(&quot;CSVData/HKExchange.csv&quot;, header=TRUE) #exchange &lt;- read.csv(&quot;../../CSVData/HKExchange.csv&quot;, header=TRUE) EXHKUS &lt;- exchange$EXHKUS plot(EXHKUS[-length(EXHKUS)], EXHKUS[-1], xlab=&quot;Tasa de Cambio Rezagada&quot;, ylab=&quot;Tasa de Cambio&quot;, xlim=c(7.75,7.82),ylim=c(7.75,7.82)) 8.3 Estimación y Verificación de Diagnóstico Habiendo identificado un modelo tentativo, la tarea ahora es estimar los valores de \\(\\beta_0\\) y \\(\\beta_1\\). En esta sección, utilizamos el método de mínimos cuadrados condicionales para determinar las estimaciones, denotadas como \\(b_0\\) y \\(b_1\\), respectivamente. Este enfoque se basa en el método de mínimos cuadrados introducido en la Sección 2.1. Específicamente, ahora utilizamos los mínimos cuadrados para encontrar estimaciones que se ajusten mejor a una observación condicional a la observación previa. Las fórmulas para las estimaciones de mínimos cuadrados condicionales se determinan a partir de los procedimientos habituales de mínimos cuadrados, utilizando el valor rezagado de \\(y\\) como la variable explicativa. Es fácil ver que las estimaciones de mínimos cuadrados condicionales se aproximan de cerca a: \\[ b_1 \\approx r_1 \\ \\ \\ \\ \\ \\ \\text{y} \\ \\ \\ \\ \\ \\ b_0 \\approx \\overline{y}(1-r_1). \\] Las diferencias entre estas aproximaciones y las estimaciones de mínimos cuadrados condicionales surgen porque no tenemos una variable explicativa para \\(y_1\\), la primera observación. Estas diferencias son típicamente pequeñas en la mayoría de las series y disminuyen a medida que aumenta la longitud de la serie. Los residuos de un modelo \\(AR\\)(1) se definen como: \\[ e_t = y_t - \\left( b_0 + b_1 y_{t-1} \\right). \\] Como hemos visto, los patrones en los residuos pueden revelar formas de mejorar la especificación del modelo. Se puede usar un gráfico de control de los residuos para evaluar la estacionariedad y calcular la función de autocorrelación de los residuos para verificar la ausencia de patrones leves a lo largo del tiempo. Los residuos también juegan un papel importante en la estimación de errores estándar asociados con las estimaciones de los parámetros del modelo. A partir de la ecuación (8.1), vemos que los errores no observados impulsan la actualización de las nuevas observaciones. Por lo tanto, tiene sentido centrarse en la varianza de los errores y, como en los datos transversales, definimos \\(\\sigma^2=\\sigma_{\\varepsilon }^2=\\) \\(\\mathrm{Var}~\\varepsilon_t.\\) En la regresión transversal, debido a que las variables predictoras no eran estocásticas, la varianza de la respuesta (\\(\\sigma_y^2\\)) es igual a la varianza de los errores (\\(\\sigma^2\\)). Esto no es generalmente cierto en modelos de series temporales que usan predictores estocásticos. Para el modelo \\(AR\\)(1), al tomar varianzas de ambos lados de la ecuación (8.1), se establece: \\[ \\sigma_y^2 (1-\\beta_1^2) = \\sigma^2 , \\] por lo que \\(\\sigma_y^2 &gt; \\sigma^2\\). Para estimar \\(\\sigma^2\\), definimos: \\[\\begin{equation} s^2 = \\frac{1}{T-3}\\sum_{t=2}^{T} \\left( e_t - \\overline{e}\\right)^2. \\tag{8.3} \\end{equation}\\] En la ecuación (8.3), el primer residuo, \\(e_1\\), no está disponible porque \\(y_{t-1}\\) no está disponible cuando \\(t=1\\) y, por lo tanto, el número de residuos es \\(T-1\\). Sin el primer residuo, el promedio de los residuos ya no es automáticamente cero y, por lo tanto, se incluye en la suma de cuadrados. Además, el denominador en el lado derecho de la ecuación (8.3) sigue siendo el número de observaciones menos el número de parámetros, teniendo en cuenta las condiciones de que el “número de observaciones” es \\(T-1\\) y el “número de parámetros” es dos. Como en el contexto de regresión transversal, nos referimos a \\(s^2\\) como el error cuadrático medio (MSE). Ejemplo: Índice de Bonos de Inflación - Continuación. El índice de inflación fue ajustado utilizando un modelo \\(AR\\)(1). La ecuación estimada resulta ser: \\[ \\begin{array}{llllrl} \\widehat{INDEX}_t &amp; = &amp; 0.40849 &amp; + &amp; 0.81384 &amp; INDEX_{t-1} \\\\ {\\small errores~estándar} &amp; &amp; {\\small (0.16916)} &amp; &amp; {\\small (0.07486)} &amp; \\end{array} \\] con \\(s = 0.14\\). Esto es menor que la desviación estándar de la serie original (0.259 de Tabla 8.1), indicando un mejor ajuste a los datos que un modelo de ruido blanco. Los errores estándar, dados entre paréntesis, fueron calculados utilizando el método de mínimos cuadrados condicionales. Por ejemplo, el \\(t\\)-ratio para \\(\\beta_1\\) es \\(0.8727/0.0736=14.9\\), indicando que la respuesta inmediata pasada es un predictor importante de la respuesta actual. Los residuos se calcularon como \\(e_t = INDEX_t - (0.2923+0.8727INDEX_{t-1})\\). El gráfico de control de los residuos en la Figura 8.4 no revela patrones aparentes. Varios autocorrelaciones de residuos se presentan en Tabla 8.5. Con \\(T=51\\) observaciones, el error estándar aproximado es \\(se(r_k) = 1/ \\sqrt{51} = 0.14\\). La autocorrelación del segundo retardo es aproximadamente -2.3 errores estándar lejos de cero, y las demás son más pequeñas, en valor absoluto. Estos valores son menores que los de Tabla 8.3, indicando que hemos eliminado algunos de los patrones temporales con la especificación \\(AR\\)(1). La autocorrelación estadísticamente significativa en el retardo 2 indica que todavía hay potencial para mejorar el modelo. Figura 8.4: Gráfico de Control de Residuos de un Ajuste \\(AR\\)(1) de la Serie del Índice de Inflación. Las líneas discontinuas marcan los límites de control superior e inferior, que son la media más o menos tres desviaciones estándar. Table 8.5. Autocorrelaciones de Residuos del Modelo \\(AR\\)(1) \\[ \\small{ \\begin{array}{l|ccccc} \\hline \\text{Retardo }k &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\ \\hline \\text{Autocorrelación de Residuos }r_k &amp; 0.157 &amp;-0.289 &amp; 0.059 &amp; 0.073&amp; -0.124 \\\\ \\hline \\end{array} } \\] Código R para producir la Figura 8.4 y la Tabla 8.5 # Figura 84 #inflation &lt;- read.csv(&quot;CSVData/InflationBond.csv&quot;, header=TRUE) #index&lt;- inflation$INFBOND lagind &lt;- index[-length(index)] AR1 &lt;- lm(index[-1] ~ lagind ) e &lt;- index[-1] - as.numeric(AR1$fitted) s &lt;- sqrt(sum((e-mean(e))^2)/(length(index[-1])-2)) plot(c(1:50), e, xlim=c(0,50), ylim=c(-0.6,0.6), xlab=&quot;TIEMPO&quot;, ylab=&quot;&quot;, type=&quot;o&quot;, pch=16,axes=F) abline(h=0, lty=1) abline(h=c(-3*s,3*s), lty=2) xat &lt;- seq(0,50,by=10) yat &lt;- seq(-0.6,0.6,by=0.3) axis(1,at=xat,labels=as.character(xat),las=1) axis(2,at=yat,labels=as.character(yat),las=1) box() # Tabla 8.5 temp &lt;- acf(e, plot = FALSE, demean = FALSE, lag.max = 6) Tab85 &lt;- temp$acf[1:6] knitr::kable(Tab85, digits = 3) x 1.000 0.157 -0.289 0.059 0.073 -0.124 8.4 Suavización y Predicción Habiendo identificado, ajustado y verificado la identificación del modelo, ahora procedemos a la inferencia básica. Recordemos que por inferencia nos referimos al proceso de utilizar el conjunto de datos para hacer afirmaciones sobre la naturaleza del mundo. Para hacer declaraciones sobre la serie, los analistas a menudo examinan los valores ajustados bajo el modelo, llamados la serie suavizada. La serie suavizada es el valor esperado estimado de la serie dado el pasado. Para el modelo \\(AR\\)(1), la serie suavizada es \\[ \\widehat{y}_t=b_0+b_1y_{t-1}. \\] En la Figura 8.5, un círculo abierto representa el Índice de Bonos de Inflación real y un círculo opaco representa la correspondiente serie suavizada. Debido a que la serie suavizada es la serie real con el componente de ruido estimado eliminado, a veces se interpreta como el valor “real” de la serie. Figura 8.5: Índice de Bonos de Inflación con una Serie Suavizada Superpuesta. El índice está representado por los símbolos abiertos, la serie suavizada está representada por los símbolos opacos. Código R para producir la Figura 8.5 # Figura 85 #inflation &lt;- read.csv(&quot;CSVData/InflationBond.csv&quot;, header=TRUE) #index&lt;- inflation$INFBOND #foo &lt;- ts(index, freq = 12, start = c(2003,1),end=c(2007,3)) #AR1 &lt;- lm(index[-1] ~ lagind ) ts.plot(foo,ylab=&quot;Índice&quot;, xlab=&quot;AÑO&quot;,ylim=c(1.5,3.0), gpars=list(pch=16,type=&quot;o&quot;)) pred &lt;- c(NA,as.numeric(AR1$fitted)) par(new=TRUE) foopred &lt;- ts(pred, freq = 12, start = c(2003,2),end=c(2007,3)) ts.plot(foopred,gpars=list(pch=1,type=&quot;o&quot;,axes=F),ylim=c(1.5,3.0), xlab=&quot;&quot;,ylab=&quot;&quot;) legend(2006,2.9,legend=c(&quot;INFBOND&quot;,&quot;SUAVIZADA&quot;),pch=c(16,1),bty=&quot;n&quot;) Por lo general, la aplicación más importante del modelado de series de tiempo es la predicción de valores futuros de la serie. A partir de la ecuación (8.1), el valor inmediato futuro de la serie es \\(y_{T+1} = \\beta_0 + \\beta_1 y_T + \\varepsilon_{T+1}\\). Dado que la serie {\\(\\varepsilon_t\\)} es aleatoria, una predicción natural de \\(\\varepsilon_{T+1}\\) es su media, cero. Por lo tanto, si las estimaciones \\(b_0\\) y \\(b_1\\) son cercanas a los verdaderos parámetros \\(\\beta_0\\) y \\(\\beta_1\\), una estimación deseable de la serie en el tiempo \\(T+1\\) es \\(\\widehat{y}_{T+1} = b_0 + b_1 y_T\\). De manera similar, se puede calcular recursivamente una estimación para la serie \\(k\\) puntos en el futuro, \\(y_{T+k}\\). Definición. El pronóstico a \\(k\\) pasos adelante de \\(y_{T+k}\\) para un modelo \\(AR\\)(1) se determina recursivamente como: \\[\\begin{equation} \\widehat{y}_{T+k} = b_0 + b_1 \\widehat{y}_{T+k-1}. \\tag{8.4} \\end{equation}\\] Esto a veces se conoce como la regla de la cadena de predicción. Para estimar el error al usar \\(\\widehat{y}_{T+1}\\) para predecir \\(y_{T+1}\\), supongamos momentáneamente que el error al usar \\(b_0\\) y \\(b_1\\) para estimar \\(\\beta_0\\) y \\(\\beta_1\\) es despreciable. Bajo esta suposición, el error de pronóstico es: \\[ y_{T+1}-\\widehat{y}_{T+1} = \\beta_0 + \\beta_1 y_t + \\varepsilon_{T+1} - \\left( b_0 + b_1 y_t\\right) \\approx \\varepsilon_{T+1}. \\] Por lo tanto, la varianza de este error de pronóstico es aproximadamente \\(\\sigma^2\\). De manera similar, se puede demostrar que la varianza aproximada del error de pronóstico \\(y_{T+k}-\\widehat{y}_{T+k}\\) es \\(\\sigma^2(1 + \\beta_1^2 + \\ldots + \\beta_1^{2(k-1)})\\). A partir de este cálculo de varianza y la normalidad aproximada, tenemos el siguiente intervalo de predicción. Definición. El intervalo de predicción a \\(k\\) pasos adelante de \\(y_{T+k}\\) para un modelo \\(AR\\)(1) es: \\[ \\widehat{y}_{T+k} \\pm (t\\text{-valor})~s \\sqrt{1 + b_1^2+ \\ldots + b_1^{2(k-1)}}. \\] Aquí, el \\(t\\text{-valor}\\) es un percentil de la curva \\(t\\) usando \\(df=T-3\\) grados de libertad. El percentil es \\(1 - (\\text{nivel de predicción})/2\\). Por ejemplo, para intervalos de predicción al 95%, tendríamos \\(t\\text{-valor} \\approx 2\\). Así, los intervalos de predicción de uno y dos pasos al 95% son: \\[ \\begin{array}{lc} \\text{Un paso:} &amp; \\widehat{y}_{T+1}\\pm \\ 2s \\\\ \\text{Dos pasos:} &amp; \\widehat{y}_{T+2}\\pm \\ 2s(1+b_1^2)^{1/2}. \\end{array} \\] La Figura 8.6 ilustra los pronósticos del índice de bonos de inflación. Los intervalos de predicción se amplían a medida que aumenta el número de pasos en el futuro; esto refleja nuestra creciente incertidumbre al pronosticar más lejos en el tiempo. Figura 8.6: Intervalos de Predicción para la Serie de Bonos de Inflación. Código R para producir la Figura 8.6 # Figura 86 #inflation &lt;- read.csv(&quot;CSVData/InflationBond.csv&quot;, header=TRUE) #index&lt;- inflation$INFBOND #AR1 &lt;- lm(index[-1] ~ lagind ) fore &lt;- index b0 &lt;- as.numeric(AR1$coef[1]) b1 &lt;- as.numeric(AR1$coef[2]) for(i in 52:57) {fore &lt;- c(fore,fore[length(fore)]*b1 + b0)} crit &lt;- qt(0.025,48,lower.tail=F) temp &lt;- b1^((c(1:6)-1)*2) wid &lt;- NULL for(k in 1:6) {wid[k] &lt;- sqrt(sum(temp[1:k]))*s*crit} lower &lt;- c(index,fore[52:57]-wid) upper &lt;- c(index,fore[52:57]+wid) foofore &lt;- ts(fore, freq = 12, start = c(2003,2),end=c(2007,9)) ts.plot(foofore,gpars=list(type=&quot;o&quot;,pch=16),ylim=c(1.5,3.0), xlab=&quot;Año&quot;,ylab=&quot;Índice de Bonos&quot;,xlim=c(2003,2008)) par(new=TRUE) foolower &lt;- ts(lower, freq = 12, start = c(2003,2),end=c(2007,9)) ts.plot(foolower,gpars=list(type=&quot;l&quot;,lty=1),ylim=c(1.5,3.0), xlab=&quot;&quot;,ylab=&quot;&quot;,axes=F,xlim=c(2003,2008)) par(new=TRUE) fooupper &lt;- ts(upper, freq = 12, start = c(2003,2),end=c(2007,9)) ts.plot(fooupper,gpars=list(type=&quot;l&quot;,lty=3),ylim=c(1.5,3.0), xlab=&quot;&quot;,ylab=&quot;&quot;,axes=F,xlim=c(2003,2008)) 8.5 Modelado y Pronóstico de Box-Jenkins Las Secciones 8.1 a 8.4 introdujeron el modelo \\(AR(1)\\), incluyendo sus propiedades, métodos de identificación y pronóstico. Ahora introducimos una clase más amplia de modelos conocida como modelos autorregresivos integrados de promedio móvil (ARIMA), desarrollados por George Box y Gwilym Jenkins, ver Box, Jenkins y Reinsel (1994). 8.5.1 Modelos Modelos \\(AR(p)\\) El modelo autorregresivo de orden uno nos permite relacionar el comportamiento actual de una observación directamente con su valor inmediatamente anterior. Sin embargo, en algunas aplicaciones, también existen efectos importantes de observaciones que están más distantes en el pasado que simplemente la observación inmediata anterior. Para cuantificar esto, ya hemos introducido la autocorrelación de retardo \\(k\\), \\(\\rho_k\\), que captura la relación lineal entre \\(y_t\\) y \\(y_{t-k}\\). Para incorporar esta característica en un marco de pronóstico, tenemos el modelo autorregresivo de orden p, denotado como \\(AR(p)\\). La ecuación del modelo es: \\[\\begin{equation} y_t = \\beta_0 + \\beta_1 y_{t-1} + \\ldots + \\beta_p y_{t-p} + \\varepsilon_t, ~~~~ t=p+1,\\ldots ,T, \\tag{8.5} \\end{equation}\\] donde {\\(\\varepsilon_t\\)} es un proceso de ruido blanco tal que \\(\\mathrm{Cov}(\\varepsilon_{t+k}, y_t)=0\\) para \\(k&gt;0\\) y \\(\\beta_0\\), \\(\\beta_1,\\ldots,\\beta_p\\) son parámetros desconocidos. Por convención, cuando los analistas de datos especifican un modelo \\(AR(p)\\), incluyen no solo \\(y_{t-p}\\) como variable predictora, sino también los retardos intermedios \\(y_{t-1}, \\ldots, y_{t-p+1}\\). Las excepciones a esta convención son los modelos autorregresivos estacionales, que se introducirán en la Sección 9.4. También por convención, el \\(AR(p)\\) es un modelo de un proceso estacionario y estocástico. Por lo tanto, se necesitan ciertas restricciones en los parámetros \\(\\beta_1, \\ldots, \\beta_p\\) para garantizar la estacionariedad (débil). Estas restricciones se desarrollan en la siguiente subsección. Notación de Desplazamiento hacia Atrás El operador de desplazamiento hacia atrás \\(\\mathrm{B}\\) se define como \\(\\mathrm{B}y_t\\) = \\(y_{t-1}\\). La notación \\(\\mathrm{B}^{k}\\) significa aplicar el operador \\(k\\) veces, es decir, \\[ \\mathrm{B}^{k}~y_t = \\mathrm{BB \\cdots B~} y_t = \\mathrm{B} ^{k-1}~y_{t-1} = \\cdots = y_{t-k}. \\] Este operador es lineal en el sentido de que \\(\\mathrm{B} (a_1 y_t + a_2 y_{t-1}) = a_1 y_{t-1} + a_2 y_{t-2}\\), donde \\(a_1\\) y \\(a_2\\) son constantes. Por lo tanto, podemos expresar el modelo \\(AR(p)\\) como: \\[\\begin{eqnarray*} \\beta_0 + \\varepsilon_t &amp;=&amp; y_t - \\left( \\beta_1 y_{t-1} + \\ldots + \\beta_p y_{t-p}\\right) \\\\ &amp;=&amp; \\left(1-\\beta_1 \\mathrm{B} - \\ldots - \\beta_p \\mathrm{B}^{p}\\right) y_t = \\Phi \\left( \\mathrm{B}\\right) y_t. \\end{eqnarray*}\\] Si \\(x\\) es un escalar, entonces \\(\\Phi \\left( x\\right) = 1 - \\beta_1 x - \\ldots - \\beta_p x^p\\) es un polinomio de orden \\(p\\) en \\(x\\). Por lo tanto, existen \\(p\\) raíces de la ecuación \\(\\Phi \\left( x\\right) =0\\). Estas raíces, \\(g_1,..,g_p\\), pueden o no ser números complejos. Se puede demostrar, ver Box, Jenkins y Reinsel (1994), que para la estacionariedad, todas las raíces deben estar estrictamente fuera del círculo unitario. Para ilustrar, para \\(p=1\\), tenemos \\(\\Phi \\left( x\\right) = 1 - \\beta_1 x\\). La raíz de esta ecuación es \\(g_1 = \\beta_1^{-1}\\). Por lo tanto, requerimos \\(|g_1|&gt;1\\), o \\(|\\beta_1|&lt;1\\), para la estacionariedad. Modelos \\(MA(q)\\) Una interpretación del modelo \\(y_t=\\beta_0+\\varepsilon_t\\) es que la perturbación \\(\\varepsilon_t\\) afecta la medida del valor “verdadero” esperado de \\(y_t\\). De manera similar, podemos considerar el modelo \\(y_t=\\beta_0 + \\varepsilon_t-\\theta_1\\varepsilon_{t-1}\\), donde \\(\\theta_1 \\varepsilon_{t-1}\\) es la perturbación del período de tiempo anterior. Extendiéndolo, introducimos el modelo de promedio móvil de orden q, denotado como \\(MA(q)\\). La ecuación del modelo es: \\[\\begin{equation} y_t = \\beta_0 + \\varepsilon_t - \\theta_1 \\varepsilon_{t-1} - \\ldots - \\theta_q \\varepsilon_{t-q}, \\tag{8.6} \\end{equation}\\] donde el proceso {\\(\\varepsilon_t\\)} es un proceso de ruido blanco tal que \\(\\mathrm{Cov}(\\varepsilon_{t+k}, y_t)=0\\) para \\(k&gt;0\\) y \\(\\beta_0\\), \\(\\theta_1, \\ldots, \\theta_q\\) son parámetros desconocidos. Con la ecuación (8.6) es fácil ver que \\(\\mathrm{Cov}(y_{t+k},y_t)=0\\) para \\(k&gt;q\\). Por lo tanto, \\(\\rho_k =0\\) para \\(k&gt;q\\). A diferencia del modelo \\(AR(p)\\), el proceso \\(MA(q)\\) es estacionario para cualquier valor finito de los parámetros \\(\\beta_0\\), \\(\\theta_1, \\ldots, \\theta_q\\). Es conveniente escribir el \\(MA(q)\\) usando notación de desplazamiento hacia atrás, de la siguiente manera: \\[ y_t - \\beta_0 = \\left( 1-\\theta_1\\mathrm{B} - \\ldots - \\theta_q \\mathrm{B}^q\\right) \\varepsilon_t = \\Theta \\left( \\mathrm{B}\\right) \\varepsilon_t. \\] Al igual que con \\(\\Phi \\left( x\\right)\\), si \\(x\\) es un escalar, entonces \\(\\Theta \\left( x\\right) = 1 - \\theta_1 x - \\ldots - \\theta_q x^q\\) es un polinomio de orden \\(q\\) en \\(x\\). Es desafortunado que la frase “promedio móvil” se utilice tanto para el modelo definido por la ecuación (8.6) como para el estimador definido en la Sección 9.2. Intentaremos aclarar su uso según surja. Modelos \\(ARMA\\) y \\(ARIMA\\) La combinación de los modelos \\(AR(p)\\) y \\(MA(q)\\) da lugar al modelo autorregresivo de promedio móvil de orden \\(p\\) y \\(q\\), o \\(ARMA(p,q)\\), \\[\\begin{equation} y_t - \\beta_1 y_{t-1} - \\ldots - \\beta_p y_{t-p} = \\beta_0 + \\varepsilon _t - \\theta_1 \\varepsilon_{t-1} - \\ldots - \\theta_q \\varepsilon_{t-q}, \\tag{8.7} \\end{equation}\\] que se puede representar como \\[\\begin{equation} \\Phi \\left( \\mathrm{B}\\right) y_t = \\beta_0 + \\Theta \\left( \\mathrm{B} \\right) \\varepsilon_t. \\tag{8.8} \\end{equation}\\] En muchas aplicaciones, los datos requieren diferenciarse para mostrar estacionariedad. Asumimos que los datos se diferencian \\(d\\) veces para obtener \\[\\begin{equation} w_t = \\left( 1-\\mathrm{B}\\right)^d y_t = \\left( 1-\\mathrm{B}\\right) ^{d-1}\\left( y_t-y_{t-1}\\right) = \\left( 1-\\mathrm{B}\\right) ^{d-2}\\left( y_t-y_{t-1}-\\left( y_{t-1}-y_{t-2}\\right) \\right) = \\ldots \\tag{8.9} \\end{equation}\\] En la práctica, \\(d\\) suele ser cero, uno o dos. Con esto, el modelo autorregresivo integrado de promedio móvil de orden \\((p,d,q)\\), denotado como \\(ARIMA(p,d,q)\\), es \\[\\begin{equation} \\Phi \\left( \\mathrm{B}\\right) w_t = \\beta_0+\\Theta \\left( \\mathrm{B} \\right) \\varepsilon_t. \\tag{8.10} \\end{equation}\\] A menudo, \\(\\beta_0\\) es cero para \\(d&gt;0\\). Existen varios procedimientos para estimar los parámetros del modelo, incluidos la estimación de máxima verosimilitud, los mínimos cuadrados condicionales y no condicionales. En la mayoría de los casos, estos procedimientos requieren ajustes iterativos. Consulte Abraham y Ledolter (1983) para obtener más información. Ejemplo: Pronóstico de Tasas de Mortalidad. Para cuantificar valores en seguros de vida y rentas vitalicias, los actuarios necesitan pronósticos de tasas de mortalidad específicas por edad. Desde su publicación, el método propuesto por Lee y Carter (1992) ha demostrado ser un método popular para pronosticar mortalidad. Por ejemplo, Li y Chan (2007) utilizaron estos métodos para producir pronósticos de tasas poblacionales canadienses (1921-2000) y estadounidenses (1900-2000). Mostraron cómo modificar la metodología básica para incorporar eventos atípicos, incluidas guerras y pandemias como la gripe y la neumonía. El método de Lee-Carter suele basarse en tasas de mortalidad centrales a la edad \\(x\\) en el tiempo \\(t\\), denotadas por \\(m_{x,t}\\). La ecuación del modelo es \\[\\begin{equation} m_{x,t} = \\alpha_x + \\beta_x \\kappa_t + \\varepsilon_{x,t} . \\tag{8.11} \\end{equation}\\] Aquí, el intercepto (\\(\\alpha_x\\)) y la pendiente (\\(\\beta_x\\)) dependen únicamente de la edad \\(x\\), no del tiempo \\(t\\). El parámetro \\(\\kappa_t\\) captura los efectos importantes del tiempo (excepto los del término de perturbación \\(\\varepsilon_{x,t}\\)). Aunque el modelo de Lee-Carter parece a primera vista una regresión lineal con una variable explicativa, el término \\(\\kappa_t\\) no es observado y se requieren técnicas diferentes para la estimación del modelo. Existen diferentes algoritmos, incluidos la descomposición en valores singulares propuesta por Lee y Carter, el enfoque de componentes principales y un modelo de regresión de Poisson; ver Li y Chan (2007) para referencias. El término \\(\\kappa_t\\) variable en el tiempo se representa típicamente usando un modelo \\(ARIMA\\). Li y Chan encontraron que un paseo aleatorio (con ajustes para eventos inusuales) era un modelo adecuado para las tasas de Canadá y EE.UU. (con coeficientes diferentes), reforzando los hallazgos de Lee y Carter. 8.5.2 Pronóstico Pronósticos Puntuales Óptimos De manera similar a los pronósticos introducidos en la Sección 8.4, es común proporcionar pronósticos que son estimaciones de expectativas condicionales de la distribución predictiva. Específicamente, asumimos que tenemos disponible una realización de {\\(y_1, y_2, \\ldots, y_T\\)} y deseamos pronosticar \\(y_{T+l}\\), el valor de la serie “\\(l\\)” pasos hacia adelante en el futuro. Si los parámetros del proceso fueran conocidos, entonces usaríamos \\(\\mathrm{E}(y_{T+l}|y_T,y_{T-1},y_{T-2},\\ldots)\\), es decir, la expectativa condicional de \\(y_{T+l}\\) dado el valor de la serie hasta el tiempo \\(T\\). Usamos la notación \\(\\mathrm{E}_T\\) para esta expectativa condicional. Para ilustrar, tomando \\(t=T+l\\) y aplicando \\(\\mathrm{E}_T\\) a ambos lados de la ecuación (8.7), obtenemos \\[\\begin{equation} y_T(l) - \\beta_1 y_T(l-1) - \\ldots - \\beta_p y_T(l-p) = \\beta_0 + \\mathrm{E}_T\\left( \\varepsilon_{T+l} - \\theta_1 \\varepsilon_{T+l-1} - \\ldots - \\theta _q \\varepsilon_{T+l-q}\\right) , \\tag{8.12} \\end{equation}\\] usando la notación \\(y_T(k) = \\mathrm{E}_T\\left( y_{T+k}\\right)\\). Para \\(k \\leq 0\\), \\(\\mathrm{E}_T\\left( y_{T+k}\\right) =y_{T+k}\\), ya que el valor de \\(y_{T+k}\\) es conocido en el tiempo \\(T\\). Además, \\(\\mathrm{E}_T\\left( \\varepsilon_{T+k}\\right) =0\\) para \\(k&gt;0\\) ya que los términos de perturbación en el futuro se asumen no correlacionados con los valores actuales y pasados de la serie. Por lo tanto, la ecuación (8.12) proporciona la base de la regla en cadena de pronóstico, donde proporcionamos recursivamente pronósticos en el tiempo \\(l\\) basados en pronósticos y realizaciones previas de la serie. Para implementar la ecuación (8.12), sustituimos estimaciones de parámetros y residuales por los términos de perturbación. Caso Especial - Modelo MA(1). Ya hemos visto la regla en cadena de pronóstico para el modelo \\(AR(1)\\) en la Sección 8.4. Para el modelo \\(MA(1)\\), note que para \\(l\\geq 2\\), tenemos \\(y_T(l)=\\mathrm{E}_T\\left( y_{T+l}\\right)\\) \\(=\\mathrm{E}_T\\left( \\beta_0+\\varepsilon_{T+l}-\\theta_1\\varepsilon_{T+l-1}\\right) =\\beta_0\\), porque \\(\\varepsilon_{T+l}\\) y \\(\\varepsilon _{T+l-1}\\) están en el futuro en el tiempo \\(T\\). Para \\(l=1\\), tenemos \\(y_T(1)= \\mathrm{E}_T\\left( \\beta_0+\\varepsilon_{T+1}-\\theta _1\\varepsilon_T\\right)\\) \\(=\\beta_0-\\theta_1\\mathrm{E}_T\\left( \\varepsilon_T\\right)\\). Típicamente, uno estimaría el término \\(\\mathrm{E}_T\\left( \\varepsilon_T\\right)\\) usando el residual en el tiempo \\(T\\). Representación del Coeficiente \\(\\psi\\) Cualquier modelo \\(ARIMA(p,d,q)\\) puede expresarse como \\[ y_t=\\beta_0^{\\ast }+\\varepsilon_t+\\psi_1 \\varepsilon_{t-1}+\\psi _2\\varepsilon_{t-2}+\\ldots=\\beta_0^{\\ast }+\\sum_{k=0}^{\\infty }\\psi _{k}\\varepsilon_{t-k}, \\] llamado la representación del coeficiente \\(\\psi\\). Es decir, el valor actual de un proceso puede expresarse como una constante más una combinación lineal de las perturbaciones actuales y pasadas. Los valores de {\\(\\psi_{k}\\)} dependen de los parámetros lineales del proceso \\(ARIMA\\) y pueden determinarse mediante una sustitución recursiva sencilla. Para ilustrar, para el modelo \\(AR(1)\\), tenemos \\[\\begin{eqnarray*} y_t &amp;=&amp;\\beta_0+\\varepsilon_t+\\beta_1y_{t-1}=\\beta_0+\\varepsilon _t+\\beta_1\\left( \\beta_0+\\varepsilon _{t-1}+\\beta_1y_{t-2}\\right) =\\ldots \\\\ &amp;=&amp;\\frac{\\beta_0}{1-\\beta_1}+\\varepsilon_t+\\beta_1\\varepsilon _{t-1}+\\beta_1^2\\varepsilon_{t-2}+\\ldots=\\frac{\\beta_0}{1-\\beta_1} +\\sum_{k=0}^{\\infty }\\beta_1^{k}\\varepsilon_{t-k}. \\end{eqnarray*}\\] Es decir, \\(\\psi_{k}=\\beta_1^{k}\\). Intervalo de Pronóstico Usando la representación del coeficiente \\(\\psi\\), podemos expresar la expectativa condicional de \\(y_{T+l}\\) como \\[ \\mathrm{E}_T\\left( y_{T+l}\\right) =\\beta_0^{\\ast }+\\sum_{k=0}^{\\infty }\\psi_{k}\\mathrm{E}_T\\left( \\varepsilon _{T+l-k}\\right) =\\beta_0^{\\ast }+\\sum_{k=l}^{\\infty }\\psi _{k}\\mathrm{E}_T\\left( \\varepsilon_{T+l-k}\\right) . \\] Esto se debe a que, en el tiempo \\(T\\), los errores \\(\\varepsilon_T,\\varepsilon _{T-1},\\ldots\\), han sido determinados por la realización del proceso. Sin embargo, los errores \\(\\varepsilon_{T+1},\\ldots,\\varepsilon_{T+l}\\) no se han realizado y, por lo tanto, tienen una expectativa condicional igual a cero. Así, el error de pronóstico de \\(l\\) pasos es \\[ y_{T+l}-\\mathrm{E}_T\\left( y_{T+l}\\right) =\\beta_0^{\\ast }+\\sum_{k=0}^{\\infty }\\psi_{k}\\varepsilon_{T+l-k}-\\left( \\beta_0^{\\ast }+\\sum_{k=l}^{\\infty }\\psi_{k}\\mathrm{E}_T\\left( \\varepsilon _{T+l-k}\\right) \\right) =\\sum_{k=0}^{l-1}\\psi_{k}\\varepsilon _{T+l-k}. \\] Nos centramos en la variabilidad de los errores de los pronósticos. Es decir, cálculos sencillos arrojan \\(\\mathrm{Var}\\left( y_{T+l}-\\mathrm{E}_T \\left( y_{T+l}\\right) \\right) =\\sigma^2\\sum_{k=1}^{l-1}\\psi_{k}^2\\). Así, asumiendo normalidad en los errores, un intervalo de pronóstico de \\(100(1-\\alpha)\\) para \\(y_{T+l}\\) es \\[ \\widehat{y}_{T+l} \\pm (t-value) s \\sqrt{\\sum_{k=0}^{l-1} \\widehat{\\psi}_k^2} . \\] donde \\(t\\)-value es el percentil \\((1-\\alpha /2)\\) de una \\(t\\)-distribución con \\(df=T-(número~de~parámetros~lineales)\\). Si \\(y_t\\) es un proceso \\(ARIMA(p,d,q)\\), entonces \\(\\psi_{k}\\) es una función de \\(\\beta_1,\\ldots,\\beta_p,\\theta_1,\\ldots,\\theta_q\\) y el número de parámetros lineales es \\(1+p+q\\). 8.6 Aplicación: Tasas de Cambio de Hong Kong La Sección 7.2 presentó la serie de Tasas de Cambio de Hong Kong, basada en \\(T=502\\) observaciones diarias para el período del 1 de abril de 2005 al 31 de mayo de 2007. Se ajustó una tendencia cuadrática al modelo que produjo un \\(R^2=86.2\\) con una desviación estándar residual de \\(s=0.0068\\). Ahora mostramos cómo mejorar este ajuste utilizando modelos \\(ARIMA\\). Para comenzar, la Figura 8.7 muestra un gráfico de series de tiempo de los residuos del modelo de tendencia cuadrática en el tiempo. Este gráfico muestra un patrón serpenteante, lo que sugiere que hay información en los residuos que puede ser explotada. Figura 8.7: Residuos de un Modelo de Tendencia Cuadrática en el Tiempo de las Tasas de Cambio de Hong Kong. R Code to Produce Figure 8.7 # Figura 87 # exchange &lt;- read.csv(&quot;CSVData/HKExchange.csv&quot;, header=TRUE) # #exchange &lt;- read.csv(&quot;../../CSVData/HKExchange.csv&quot;, header=TRUE) # EXHKUS &lt;- exchange$EXHKUS t &lt;- 1:length(EXHKUS) qtrend &lt;- lm(EXHKUS~t+I(t^2)) #summary(qtrend) plot( (1:length(EXHKUS)), qtrend$resid, type=&quot;o&quot;, xlab=&quot;Índice&quot;, ylab=&quot;Residuos&quot;) Más evidencia de estos patrones se encuentra en la tabla de autocorrelaciones en Tabla 8.6. Aquí observamos grandes autocorrelaciones residuales que no disminuyen rápidamente a medida que el retraso \\(k\\) aumenta. Un patrón similar también es evidente en la serie original, EXHKUS. Esto confirma la no estacionariedad que observamos en la Sección 7.2. Como una transformación alternativa, se diferenciaron las series, produciendo DIFFHKUS. Esta serie diferenciada tiene una desviación estándar de \\(s_{DIFF}=0.0020\\), lo que sugiere que es más estable que la serie original o los residuos del modelo de tendencia cuadrática en el tiempo. Tabla 8.6 presenta las autocorrelaciones de la serie diferenciada, indicando patrones leves. Sin embargo, estas autocorrelaciones todavía son significativamente diferentes de cero. Para \\(T=501\\) diferencias, podemos usar como un error estándar aproximado para las autocorrelaciones \\(1/\\sqrt{501}\\approx 0.0447.\\) Con esto, vemos que la autocorrelación en el retraso 2 es \\(0.151/0.0447\\approx 3.38\\) errores estándar por debajo de cero, lo cual es estadísticamente significativo. Esto sugiere introducir otro modelo para aprovechar la información en los patrones de series de tiempo. Tabla 8.6. Autocorrelaciones de las Tasas de Cambio de Hong Kong \\[ \\small{ \\begin{array}{l|cccccccccc} \\hline \\text{Retraso }&amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 \\\\ \\hline \\text{Residuos del} &amp; 0.958 &amp; 0.910 &amp; 0.876 &amp; 0.847 &amp; 0.819 &amp; 0.783 &amp; 0.748 &amp; 0.711 &amp; 0.677 &amp; 0.636 \\\\ \\ \\ \\text{Modelo Cuadrático} &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ \\text{EXHKUS} &amp; 0.988 &amp; 0.975 &amp; 0.963 &amp; 0.952 &amp; 0.942 &amp; 0.930 &amp; 0.919 &amp; 0.907 &amp; 0.895 &amp; 0.882 \\\\ \\ \\ \\text{(Serie Original)} &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ \\text{DIFFHKUS} &amp; 0.078 &amp; -0.151 &amp; -0.038 &amp; -0.001 &amp; 0.095 &amp; -0.005 &amp; 0.051 &amp; -0.012 &amp; 0.084 &amp; -0.001 \\\\ \\hline \\end{array} } \\] R Code to Produce Table 8.6 # Tabla 8.6 # exchange &lt;- read.csv(&quot;CSVData/HKExchange.csv&quot;, header=TRUE) # #exchange &lt;- read.csv(&quot;../../CSVData/HKExchange.csv&quot;, header=TRUE) # EXHKUS &lt;- exchange$EXHKUS # t &lt;- 1:length(EXHKUS) # qtrend &lt;- lm(EXHKUS~t+I(t^2)) diffrate &lt;- EXHKUS[-1]-EXHKUS[-length(EXHKUS)] #sd(diffrate) auto.qtrend &lt;- as.numeric(acf(qtrend$resid,lag.max=10,plot=F)$acf) auto.orig &lt;- as.numeric(acf(EXHKUS, lag.max=10,plot=F)$acf) auto.diff &lt;- as.numeric(acf(diffrate, lag.max=10,plot=F)$acf) Tab86 &lt;- rbind(auto.qtrend, auto.orig, auto.diff) knitr::kable(Tab86, digits = 3) auto.qtrend 1 0.958 0.910 0.876 0.847 0.819 0.783 0.748 0.711 0.677 0.636 auto.orig 1 0.988 0.975 0.963 0.952 0.942 0.930 0.919 0.907 0.895 0.882 auto.diff 1 0.078 -0.151 -0.038 -0.001 0.095 -0.005 0.051 -0.012 0.084 -0.001 Selección de Modelo y Autocorrelaciones Parciales Para todos los modelos autorregresivos estacionarios, se puede demostrar que los valores absolutos de las autocorrelaciones disminuyen a medida que el retraso \\(k\\) aumenta. En el caso de que las autocorrelaciones disminuyan aproximadamente como una serie geométrica, se puede identificar un modelo \\(AR(1)\\). Desafortunadamente, para otros tipos de series autorregresivas, las reglas prácticas para identificar la serie a partir de las autocorrelaciones se vuelven más confusas. Un dispositivo útil para identificar el orden de una serie autorregresiva es la función de autocorrelación parcial. Al igual que las autocorrelaciones, ahora definimos una autocorrelación parcial en un retraso específico \\(k\\). Consideremos la ecuación del modelo \\[ y_t=\\beta_{0,k}+\\beta_{1,k} y_{t-1}+\\ldots+\\beta_{k,k} y_{t-k} + \\varepsilon_t. \\] Aquí, {\\(\\varepsilon_t\\)} es un error estacionario que puede o no ser un proceso de ruido blanco. El segundo subíndice en los \\(\\beta\\), “\\(,k\\)”, está allí para recordarnos que el valor de cada \\(\\beta\\) puede cambiar cuando cambia el orden del modelo, \\(k\\). Con esta especificación del modelo, podemos interpretar \\(\\beta_{k,k}\\) como la correlación entre \\(y_t\\) y \\(y_{t-k}\\) después de haber eliminado los efectos de las variables intermedias, \\(y_{t-1},\\ldots,y_{t-k+1}\\). Esta es la misma idea que el coeficiente de correlación parcial, introducido en la Sección 4.4. Las estimaciones de los coeficientes de correlación parcial, \\(b_{k,k}\\), pueden calcularse utilizando mínimos cuadrados condicionales u otras técnicas. Al igual que con otras correlaciones, podemos usar \\(1/\\sqrt{T}\\) como un error estándar aproximado para detectar diferencias significativas con respecto a cero. Las autocorrelaciones parciales se utilizan en la identificación de modelos de la siguiente manera. Primero calcule las primeras estimaciones, \\(b_{1,1},b_{2,2},b_{3,3}\\), y así sucesivamente. Luego, elija el orden del modelo autorregresivo como el mayor \\(k\\) de modo que la estimación \\(b_{k,k}\\) sea significativamente diferente de cero. Para ver cómo se aplica esto en el ejemplo de la Tasa de Cambio de Hong Kong, recordemos que el error estándar aproximado para las correlaciones es \\(1/\\sqrt{501}\\approx 0.0447\\). Tabla 8.7 proporciona las primeras diez autocorrelaciones parciales para las tasas y para sus diferencias. Usando el doble del error estándar como nuestra regla de corte, vemos que la segunda autocorrelación parcial de las diferencias excede \\(2\\times 0.0447=0.0894\\) en valor absoluto. Esto sugeriría usar un \\(AR(2)\\) como una primera elección de modelo tentativa. Alternativamente, el lector puede argumentar que, dado que las autocorrelaciones parciales quinta y novena también son estadísticamente significativas, un \\(AR(5)\\) o \\(AR(9)\\) más complejo podría ser más apropiado. La filosofía es “usar el modelo más simple posible, pero no más simple”. Preferimos emplear modelos más simples y ajustarlos primero, luego probar si capturan o no los aspectos importantes de los datos. Finalmente, puede estar interesado en ver qué sucede con las autocorrelaciones parciales calculadas en una serie no estacionaria. Tabla 8.7 proporciona autocorrelaciones parciales para la serie original (EXHKUS). Observe cuán grande es la primera autocorrelación parcial. Es decir, otra forma de identificar una serie como no estacionaria es examinar la función de autocorrelación parcial y buscar una gran autocorrelación parcial en el primer retraso. Tabla 8.7. Autocorrelaciones Parciales de EXHKUS y DIFFHKUS \\[ \\small{ \\begin{array}{l|cccccccccc} \\hline \\text{Retraso} &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 \\\\ \\hline \\text{EXHKUS} &amp; 0.988 &amp; -0.034 &amp; 0.051 &amp; 0.019 &amp; -0.001 &amp; -0.023 &amp; 0.010 &amp; -0.047 &amp; -0.013 &amp; -0.049 \\\\ \\text{DIFFHKUS} &amp; 0.078 &amp; -0.158 &amp; -0.013 &amp; -0.021 &amp; 0.092 &amp; -0.026 &amp; 0.085 &amp; -0.027 &amp; 0.117 &amp; -0.036 \\\\ \\hline \\end{array} } \\] R Code to Produce Table 8.7 # Tabla 8.7 # exchange &lt;- read.csv(&quot;CSVData/HKExchange.csv&quot;, header=TRUE) # #exchange &lt;- read.csv(&quot;../../CSVData/HKExchange.csv&quot;, header=TRUE) # EXHKUS &lt;- exchange$EXHKUS #diffrate &lt;- EXHKUS[-1]-EXHKUS[-length(EXHKUS)] pauto.orig &lt;- as.numeric(pacf(EXHKUS,lag.max=10,plot=F)$acf) pauto.diff &lt;- as.numeric(pacf(diffrate,lag.max=10,plot=F)$acf) Tab87 &lt;- rbind(pauto.orig, pauto.diff) knitr::kable(Tab87, digits = 3) pauto.orig 0.988 -0.034 0.051 0.019 -0.001 -0.023 0.010 -0.047 -0.013 -0.049 pauto.diff 0.078 -0.158 -0.013 -0.021 0.092 -0.026 0.085 -0.027 0.117 -0.036 Verificación de Residuos Después de identificar y ajustar un modelo, la verificación de residuos sigue siendo una parte importante para determinar la validez del modelo. Para el modelo \\(ARMA(p,q)\\), calculamos los valores ajustados como \\[\\begin{equation} \\widehat{y}_t = b_0 + b_1 y_{t-1} + \\ldots + b_p y_{t-p} - \\widehat{\\theta}_1 e_{t-1}- \\ldots - \\widehat{\\theta }_q e_{t-q}. \\tag{8.13} \\end{equation}\\] Aquí, \\(\\widehat{\\theta}_1, \\ldots, \\widehat{\\theta}_q\\) son estimaciones de \\(\\theta_1,\\ldots, \\theta_q\\). Los residuos pueden calcularse de la manera habitual, es decir, como \\(e_t=y_t-\\widehat{y}_t\\). Sin más aproximaciones, observe que los residuos iniciales faltan porque los valores ajustados antes del tiempo \\(t=\\max (p,q)\\) no pueden calcularse usando la ecuación (8.13). Para verificar patrones, use los dispositivos descritos en la Sección 8.3, como el gráfico de control para verificar la estacionariedad y la función de autocorrelación para verificar las relaciones entre variables con retrasos. Autocorrelación de Residuos Los residuos del modelo ajustado deberían parecerse a un ruido blanco y, por lo tanto, mostrar pocos patrones discernibles. En particular, esperamos que \\(r_k(e)\\), la autocorrelación de los residuos en el retraso \\(k\\), sea aproximadamente cero. Para evaluar esto, tenemos que \\(se\\left( r_k(e) \\right) \\approx 1/\\sqrt{T}\\). Más precisamente, MacLeod (1977, 1978) ha dado aproximaciones para una amplia clase de modelos \\(ARMA\\). Resulta que el \\(1/\\sqrt{T}\\) puede mejorarse para valores pequeños de \\(k\\). (Estos valores mejorados pueden verse en la salida de la mayoría de los paquetes estadísticos). La mejora depende del modelo que se esté ajustando. Para ilustrar, supongamos que un modelo \\(AR(1)\\) con parámetro autorregresivo \\(\\beta_1\\) se ajusta a los datos. Entonces, el error estándar aproximado de la autocorrelación residual en el retraso uno es \\(|\\beta_1|/\\sqrt{T}\\). Este error estándar puede ser mucho menor que \\(1/\\sqrt{T}\\), dependiendo del valor de \\(\\beta_1\\). Prueba de Varios Retrasos Para probar si hay autocorrelación residual significativa en un retraso específico \\(k\\), usamos \\(r_k(e) /se\\left( r_k(e) \\right)\\). Además, para verificar si los residuos se parecen a un proceso de ruido blanco, podemos probar si \\(r_k(e)\\) está cerca de cero para varios valores de \\(k\\). Para probar si las primeras \\(K\\) autocorrelaciones residuales son cero, se utiliza la estadística chi-cuadrado de Box y Pierce (1970) \\[ Q_{BP} = T \\sum_{k=1}^{K} r_k \\left( e \\right)^2. \\] Aquí, \\(K\\) es un número entero especificado por el usuario. Si no hay autocorrelación real, esperamos que \\(Q_{BP}\\) sea pequeño; más precisamente, Box y Pierce demostraron que \\(Q_{BP}\\) sigue una distribución aproximada \\(\\chi^2\\) con \\(df=K-(número~de~parámetros~lineales)\\). Para un modelo \\(ARMA(p,q)\\), el número de parámetros lineales es \\(1+p+q\\). Otra estadística ampliamente utilizada es \\[ Q_{LB}=T(T+2)\\sum_{k=1}^{K}\\frac{r_k \\left( e\\right)^2}{T-k}. \\] debido a Ljung y Box (1978). Esta estadística tiene un mejor rendimiento en muestras pequeñas que la estadística \\(BP\\). Bajo la hipótesis de no autocorrelación residual, \\(Q_{LB}\\) sigue la misma distribución \\(\\chi^2\\) que \\(Q_{BP}\\). Así, para cada estadística, rechazamos \\(H_{0}\\): No hay autocorrelación residual si la estadística excede el valor \\(\\chi\\), un percentil \\(1-\\alpha\\) de una distribución \\(\\chi^2\\). Una regla práctica conveniente es usar \\(\\chi\\)-valor = 1.5 \\(df\\). Ejemplo: Tasa de Cambio de Hong Kong - Continuación. Se ajustaron dos modelos, el \\(ARIMA(2,1,0)\\) y el \\(ARIMA(0,1,2)\\); estos son los modelos \\(AR(2)\\) y \\(MA(2)\\) después de tomar diferencias. Usando {\\(y_t\\)} para las diferencias, el modelo \\(AR(2)\\) estimado es: \\[ \\begin{array}{clllrllll} \\widehat{y}_t &amp; = &amp; 0.0000317 &amp; + &amp; 0.0900 &amp; y_{t-1} &amp; - &amp; 0.158 &amp; y_{t-2} \\\\ {\\small t-estadística} &amp; &amp; {\\small [0.37]} &amp; &amp; {\\small [2.03]} &amp; &amp; &amp; {\\small [-3.57]} &amp; \\end{array} \\] con un error estándar residual de \\(s=0.00193.\\) El modelo \\(MA(2)\\) estimado es: \\[ \\begin{array}{clllrllll} \\widehat{y}_t &amp; = &amp; 0.0000297 &amp; - &amp; 0.0920 &amp; e_{t-1} &amp; + &amp; 0.162 &amp; e_{t-2} \\\\ {\\small t-estadística} &amp; &amp; {\\small [0.37]} &amp; &amp; {\\small [-2.08]} &amp; &amp; &amp; {\\small [3.66]} &amp; \\end{array} \\] con el mismo error estándar residual de \\(s=0.00193.\\) Estas estadísticas indican que los modelos son aproximadamente comparables. La estadística de Ljung-Box en Tabla 8.8 también indica una gran similitud entre los modelos. Tabla 8.8. Estadísticas de Ljung-Box \\(Q_{LB}\\) para los Modelos de Tasa de Cambio de Hong Kong \\[ \\small{ \\begin{array}{l|ccccc} \\hline &amp;&amp;&amp; \\text{Retraso }K \\\\ \\text{Modelo} &amp; 2 &amp; 4 &amp; 6 &amp; 8 &amp; 10 \\\\ \\hline AR(2) &amp; 0.0065 &amp; 0.5674 &amp; 6.3496 &amp; 10.4539 &amp; 16.3258 \\\\ MA(2) &amp; 0.0110 &amp; 0.2872 &amp; 6.6557 &amp; 11.3361 &amp; 17.6882 \\\\ \\hline \\end{array} } \\] R Code to Produce Table 8.8 # Tabla 8.8 # exchange &lt;- read.csv(&quot;CSVData/HKExchange.csv&quot;, header=TRUE) # #exchange &lt;- read.csv(&quot;../../CSVData/HKExchange.csv&quot;, header=TRUE) # EXHKUS &lt;- exchange$EXHKUS #diffrate &lt;- EXHKUS[-1]-EXHKUS[-length(EXHKUS)] # Ajustar modelos ARIMA AR2 &lt;- arima(diffrate, order = c(2,0,0)) #AR2$coef/sqrt(diag(AR2$var.coef)) # Comentario: diferentes valores de intercepto #sqrt(AR2$sigma2) MA2 &lt;- arima(diffrate, order = c(0,0,2)) #MA2$coef/sqrt(diag(MA2$var.coef)) #sqrt(MA2$sigma2) LB1 &lt;- LB2 &lt;- NULL for(k in seq(2,10, by=2)){ LB1 &lt;- c(LB1,as.numeric(Box.test(AR2$residuals,lag=k, type=&quot;Ljung&quot;,fitdf = 3)$statistic)) LB2 &lt;- c(LB2,as.numeric(Box.test(MA2$residuals,lag=k, type=&quot;Ljung&quot;,fitdf = 3)$statistic)) } Tab88 &lt;- rbind(LB1,LB2) rownames(Tab88)=c(&quot;AR(2)&quot;,&quot;MA(2)&quot;) colnames(Tab88)=paste(&quot;Retraso&quot;,c(2,4,6,8,10)) knitr::kable(Tab88, digits = 4) Retraso 2 Retraso 4 Retraso 6 Retraso 8 Retraso 10 AR(2) 0.0065 0.5674 6.3496 10.4539 16.3258 MA(2) 0.0110 0.2872 6.6557 11.3361 17.6882 Los modelos ajustados \\(MA\\)(2) y \\(AR\\)(2) son aproximadamente similares. Presentamos el modelo \\(AR\\)(2) solo para fines de predicción porque los modelos autorregresivos suelen ser más fáciles de interpretar. La Figura 8.8 resume las predicciones, calculadas para diez días. Note los intervalos de predicción que se amplían, algo típico en pronósticos para series no estacionarias. Figura 8.8: Pronósticos a Diez Días e Intervalos de Predicción de las Tasas de Cambio de Hong Kong. Los pronósticos se basan en el modelo \\(ARIMA(2,1,0)\\). R Code to Produce Figure 8.8 # Figura 88 # exchange &lt;- read.csv(&quot;CSVData/HKExchange.csv&quot;, header=TRUE) # #exchange &lt;- read.csv(&quot;../../CSVData/HKExchange.csv&quot;, header=TRUE) # EXHKUS &lt;- exchange$EXHKUS date &lt;- exchange$DATE modelAR &lt;- arima(EXHKUS,order=c(2,1,0)) pred10 &lt;- predict(modelAR,10) pred &lt;- c(EXHKUS, pred10$pred) lower &lt;- c(EXHKUS, pred10$pred-2*pred10$se) upper &lt;- c(EXHKUS, pred10$pred+2*pred10$se) plot(c(1:length(pred)), pred, xlim=c(0,510),ylim=c(7.74,7.832), xlab=&quot;Tiempo&quot;, ylab=&quot;EXHKUS&quot;,type=&quot;o&quot;) lines(c(1:length(pred)),lower) lines(c(1:length(pred)),upper) 8.7 Lecturas Adicionales y Referencias El libro clásico introductorio a las series temporales de Box-Jenkins es Box, Jenkins y Reinsel (1994). Referencias del Capítulo Abraham, Bovas and Johannes Ledolter (1983). Statistical Methods for Forecasting. John Wiley &amp; Sons, New York. Box, George E. P., Gwilym M. Jenkins and Gregory C. Reinsel (1994). Time Series Analysis: Forecasting and Control, Third Edition, Prentice-Hall, Englewood Cliffs, New Jersey. Box, George E. P., and D. A. Pierce (1970). Distribution of residual autocorrelations in autoregressive moving average time series models. Journal of the American Statistical Association 65, 1509-1526. Chan, Wai-Sum and Siu-Hang Li (2007). The Lee-Carter model for forecasting mortality, revisited. North American Actuarial Journal 11(1), 68-89. Lee, Ronald D. and Lawrence R. Carter (1992). Modelling and forecasting U.S. mortality. Journal of the American Statistical Association 87, 659-671. Ljung, G. M. and George E. P. Box (1978). On a measure of lack of fit in time series models. Biometrika 65, 297-303. MacLeod, A. I. (1977). Improved Box-Jenkins estimators. Biometrika 64, 531-534. MacLeod, A. I. (1978). On the distribution of residual autocorrelations in Box-Jenkins models. Journal of the Royal Statistical Society B 40, 296-302. Miller, Robert B. and Dean W. Wichern (1977). Intermediate Business Statistics: Analysis of Variance, Regression and Time Series. Holt, Rinehart and Winston, New York. Roberts, Harry V. (1991). Data Analysis for Managers with MINITAB. Scientific Press, South San Francisco, CA. 8.1. Un fondo mutuo ha proporcionado tasas de rendimiento de inversión durante cinco años consecutivos como sigue: \\[ \\begin{array}{l|ccccc} \\hline \\text{Año} &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\ \\hline \\text{Rendimiento} &amp; 0.09 &amp; 0.08 &amp; 0.09 &amp; 0.12 &amp; -0.03 \\\\ \\hline \\end{array} \\] Determine \\(r_1\\) y \\(r_2\\), los coeficientes de autocorrelación para retrasos 1 y 2. 8.2. La estadística Durbin-Watson está diseñada para detectar autocorrelación y se define como: \\[ DW = \\frac {\\sum_{t=2}^T (y_t - y_{t-1})^2} {\\sum_{t=1}^T (y_t - \\bar{y})^2}. \\] Derive la relación aproximada entre \\(DW\\) y el coeficiente de autocorrelación de retraso 1, \\(r_1\\). Suponga que \\(r_1 = 0.4\\). ¿Cuál es el valor aproximado de \\(DW\\)? 8.3. Considere las fórmulas del modelo de regresión lineal del Capítulo 2 con \\(y_{t-1}\\) en lugar de \\(x_t\\), para \\(t=2, \\ldots, T\\). Proporcione una expresión exacta para \\(b_1\\). Proporcione una expresión exacta para \\(b_0\\). Demuestre que \\(b_0 \\approx \\bar{y} (1-r_1)\\). 8.4. Comience con el modelo \\(AR\\)(1) como en la ecuación (8.1). Tome las varianzas de cada lado de la ecuación (8.1) para mostrar que \\(\\sigma_y^2(1-\\beta_1^2) = \\sigma^2,\\) donde \\(\\sigma_y^2 = \\mathrm{Var}~y_t\\) y \\(\\sigma^2 = \\mathrm{Var}~\\varepsilon_t\\). Demuestre que \\(\\mathrm{Cov}(y_t,y_{t-1}) = \\beta_1 \\sigma_y^2.\\) Demuestre que \\(\\mathrm{Cov}(y_t,y_{t-k}) = \\beta_1^k \\sigma_y^2.\\) Use la parte (c) para establecer la ecuación (8.2). 8.5. Considere pronósticos con el modelo \\(AR\\)(1). Use la regla de encadenamiento de pronósticos en la ecuación (8.4) para mostrar \\[ y_{T+k}-\\widehat{y}_{T+k} \\approx \\varepsilon_{T+k} + \\beta_1 \\varepsilon_{T+k-1} + \\cdots + \\beta_1^{k-1} \\varepsilon_{T+1}. \\] A partir de la parte (a), demuestre que la varianza aproximada del error de pronóstico es \\(\\sigma^2 \\sum_{l=0}^{k-1} \\beta_1^{2l}.\\) 8.6. Estos datos consisten en los 503 rendimientos diarios de los años calendario 2005 y 2006 del índice ponderado por valor del Standard and Poor’s (S&amp;P). (El archivo de datos contiene años adicionales; este ejercicio usa solo los datos de 2005 y 2006). Cada año, hay alrededor de 250 días en los que el mercado está abierto y se intercambian acciones; los fines de semana y festivos está cerrado. Hay varios índices para medir el rendimiento general del mercado. El índice ponderado por valor se crea suponiendo que la cantidad invertida en cada acción es proporcional a su capitalización de mercado. Aquí, la capitalización de mercado es simplemente el precio inicial por acción multiplicado por el número de acciones en circulación. Una alternativa es el índice ponderado por igual, creado tomando un promedio simple del precio de cierre o último precio de las acciones que forman parte del S&amp;P en ese día de negociación. La teoría económica financiera establece que si el mercado fuera predecible, muchos inversionistas intentarían aprovechar estas predicciones, forzando así la imprevisibilidad. Por ejemplo, supongamos que un modelo estadístico predice de manera confiable que el fondo mutuo A se duplicará en los próximos 18 meses. Entonces, el principio de no arbitraje en economía financiera establece que varios inversionistas alerta, armados con la información del modelo estadístico, ofertarían para comprar el fondo mutuo A, lo que provocaría un aumento en el precio debido al aumento de la demanda. Estos inversionistas alerta continuarían comprando hasta que el precio del fondo mutuo A aumentara al punto donde el rendimiento fuera equivalente a otras oportunidades de inversión en la misma clase de riesgo. Así, cualquier ventaja producida por el modelo estadístico desaparecería rápidamente, eliminando esta ventaja. Por lo tanto, la teoría económica financiera establece que para mercados líquidos como las acciones representadas en el índice S&amp;P no deberían detectarse patrones, resultando en un proceso de ruido blanco. En la práctica, se ha encontrado que los costos de compra y venta de acciones (llamados costos de transacción) son lo suficientemente altos como para evitar que aprovechemos estas ligeras tendencias en las fluctuaciones del mercado. Esto ilustra un punto conocido como estadísticamente significativo pero no prácticamente importante. Esto no sugiere que la estadística no sea práctica (¡ni pensarlo!). En cambio, la estadística en sí misma no reconoce explícitamente factores, como los económicos, psicológicos y otros, que pueden ser extremadamente importantes en una situación dada. Corresponde al analista interpretar el análisis estadístico a la luz de estos factores. Figura 8.9: Gráfico de Serie Temporal del Rendimiento Diario del Mercado S &amp; P, 2005-2006. R Code to Produce Figure 8.9 # Figura 89 spdaily &lt;- read.csv(&quot;CSVData/SP500Daily.csv&quot;, header=TRUE) #spdaily &lt;- read.csv(&quot;../../CSVData/SP500Daily.csv&quot;, header=TRUE) sub56 &lt;- spdaily[c(1257:1759),] plot(c(1:dim(sub56)[1]), sub56$vwretd, type=&quot;o&quot;, pch=16, xlab=&quot;Índice&quot;, ylab=&quot;Rendimiento Ponderado por Valor&quot;, xlim=c(0,500)) El gráfico de la serie temporal en la Figura 8.9 da una idea preliminar de las características de la secuencia. Comente sobre la estacionariedad de la secuencia. Calcule estadísticas resumidas de la secuencia. Suponga que asume un modelo de ruido blanco para la secuencia. Calcule pronósticos de 1, 2 y 3 pasos para los rendimientos diarios de los tres primeros días de negociación de 2007. Calcule las autocorrelaciones para los retrasos 1 a 10. ¿Detecta alguna autocorrelación que sea estadísticamente significativa y diferente de cero? "],["C9Forecast.html", "Capítulo 9 Pronósticos y Modelos de Series Temporales 9.1 Suavización con Promedios Móviles 9.2 Suavización Exponencial 9.3 Modelos de Series Temporales Estacionales 9.4 Pruebas de Raíces Unitarias 9.5 Modelos ARCH/GARCH 9.6 Lecturas y Referencias Adicionales", " Capítulo 9 Pronósticos y Modelos de Series Temporales Vista Previa del Capítulo. Este capítulo introduce dos técnicas populares de suavización, promedios móviles (corridos) y suavización exponencial, para realizar pronósticos. Estas técnicas son simples de explicar y fácilmente interpretables. También pueden expresarse como modelos de regresión, donde se utiliza la técnica de mínimos cuadrados ponderados (WLS) para calcular las estimaciones de los parámetros. Luego se presenta la estacionalidad, seguida de una discusión sobre dos temas más avanzados de series temporales, pruebas de raíces unitarias y modelos de volatilidad (ARCH/GARCH). 9.1 Suavización con Promedios Móviles Suavizar una serie temporal con un promedio móvil, o corrido, es un procedimiento probado por el tiempo. Esta técnica sigue siendo utilizada por muchos analistas de datos debido a su facilidad de cálculo y posterior facilidad de interpretación. Como discutimos a continuación, este estimador también puede motivarse como un estimador de mínimos cuadrados ponderados (WLS). Por lo tanto, el estimador disfruta de ciertas propiedades teóricas. El estimador de promedio móvil o corrido básico se define como: \\[\\begin{equation} \\widehat{s}_t = \\frac{y_t + y_{t-1} + \\ldots + y_{t-k+1}}{k}, \\tag{9.1} \\end{equation}\\] donde \\(k\\) es la longitud del promedio móvil. La elección de \\(k\\) depende de la cantidad de suavización deseada. Cuanto mayor sea el valor de \\(k\\), más suave será la estimación \\(\\widehat{s}_t\\) porque se realiza un mayor promedio. La elección \\(k=1\\) corresponde a no realizar suavización. Aplicación: Componente Médico del IPC El índice de precios al consumidor (IPC) es una canasta de bienes y servicios cuyo precio mide en los Estados Unidos la Oficina de Estadísticas Laborales. Al medir esta canasta periódicamente, los consumidores obtienen una idea del aumento constante de precios a lo largo del tiempo, lo que, entre otras cosas, sirve como un proxy para la inflación. El IPC está compuesto por muchos componentes, que reflejan la importancia relativa de cada componente para la economía en general. Aquí, estudiamos el componente médico del IPC, la parte de la canasta general que ha crecido más rápido desde 1967. Los datos que consideramos son valores trimestrales del componente médico del IPC (MCPI) durante un período de sesenta años, desde 1947 hasta el primer trimestre de 2007, inclusive. Durante este período, el índice aumentó de 13.3 a 346.0. Esto representa un aumento de 26 veces en los sesenta años, lo que se traduce en un aumento trimestral de 1.36%. La Figura 9.1 es un gráfico de serie temporal de los cambios porcentuales trimestrales en el MCPI. Tenga en cuenta que ya hemos cambiado del índice no estacionario a cambios porcentuales. (El índice es no estacionario porque muestra un crecimiento tremendo durante el período considerado). Para ilustrar el efecto de la elección de \\(k\\), considere los dos paneles de la Figura 9.1. En el panel superior de la Figura 9.1, la serie suavizada con \\(k=4\\) se superpone a la serie real. El panel inferior es el gráfico correspondiente con \\(k=8\\). Los valores ajustados en el panel inferior son menos irregulares que los del panel superior. Esto nos ayuda a identificar gráficamente las tendencias reales en la serie. El peligro de elegir un valor demasiado alto de \\(k\\) es que podemos “sobre-suavizar” los datos y perder de vista las tendencias reales. Figura 9.1: Cambios Porcentuales Trimestrales en el Componente Médico del Índice de Precios al Consumidor. En ambos paneles, la línea discontinua es el índice. En el panel superior, la línea sólida es la versión suavizada con \\(k\\)=4. En el panel inferior, la línea sólida es la versión suavizada con \\(k\\)=8. Fuente: Oficina de Estadísticas Laborales Para pronosticar la serie, reexpresamos la ecuación (9.1) recursivamente para obtener \\[\\begin{equation} \\widehat{s}_t = \\frac{y_t + y_{t-1} + \\ldots + y_{t-k+1}}{k} = \\frac{y_t + k \\widehat{s}_{t-1} - y_{t-k}}{k} = \\widehat{s}_{t-1} + \\frac{y_t-y_{t-k}}{k}. \\tag{9.2} \\end{equation}\\] Si no hay tendencias en los datos, entonces el segundo término en el lado derecho, \\((y_t-y_{t-k})/k\\), puede ignorarse en la práctica. Esto da como resultado la ecuación de pronóstico \\(\\widehat{y}_{T+l} = \\widehat{s}_T\\) para pronósticos \\(l\\) unidades de tiempo hacia el futuro. Existen varias variantes de promedios móviles en la literatura. Por ejemplo, supongamos que una serie puede expresarse como \\(y_t = \\beta_0 + \\beta_1 t + \\varepsilon_t\\), un modelo de tendencia lineal en el tiempo. Esto puede manejarse a través del siguiente procedimiento de suavización doble: Crear una serie suavizada utilizando la ecuación (9.1), es decir, \\(\\widehat{s}_t^{(1)}=(y_t+\\ldots+y_{t-k+1})/k.\\) Crear una serie suavizada doble utilizando la ecuación (9.1) y tratando la serie suavizada creada en el paso (i) como entrada. Es decir, \\(\\widehat{s}_t^{(2)} = (\\widehat{s}_t^{(1)} + \\ldots + \\widehat{s}_{t-k+1}^{(1)})/k.\\) Es fácil verificar que este procedimiento suaviza el efecto de una tendencia lineal en el tiempo. La estimación de la tendencia es \\(b_{1,T}=2\\left( \\widehat{s}_T^{(1)}-\\widehat{s}_T^{(2)}\\right) /(k-1)\\). Los pronósticos resultantes son \\(\\widehat{y}_{T+l} = \\widehat{s}_T + b_{1,T}~l\\) para pronósticos \\(l\\) unidades de tiempo hacia el futuro. Mínimos Cuadrados Ponderados Una característica importante de los promedios móviles es que pueden expresarse como estimaciones de mínimos cuadrados ponderados (WLS). La estimación WLS se introdujo en la Sección 5.7.3. Encontrará una discusión adicional en la Sección 15.1.1. Recuerde que las estimaciones WLS minimizan una suma ponderada de cuadrados. El procedimiento WLS encuentra los valores de \\(b_0^{\\ast}, \\ldots, b_{k}^{\\ast}\\) que minimizan \\[\\begin{equation} WSS_T\\left( b_0^{\\ast },\\ldots, b_k^{\\ast}\\right) = \\sum_{t=1}^{T} w_t \\left( y_t-\\left( b_0^{\\ast} + b_1^{\\ast} x_{t1}, \\ldots, b_{k}^{\\ast} x_{tk} \\right) \\right)^2. \\tag{9.3} \\end{equation}\\] Aquí, \\(WSS_T\\) es la suma ponderada de cuadrados en el tiempo \\(T\\). Para llegar a la estimación de promedio móvil, usamos el modelo \\(y_t = \\beta_0 + \\varepsilon_t\\) con la elección de pesos \\(w_t=1\\) para \\(t = T-k+1, \\ldots, T\\) y \\(w_t=0\\) para \\(t&lt;T-k+1\\). Así, el problema de minimizar \\(WSS_T\\) en la ecuación (9.3) se reduce a encontrar \\(b_0^{\\ast}\\) que minimice \\(\\sum_{t=T-k+1}^{T}\\left( y_t - b_0^{\\ast} \\right)^2.\\) El valor de \\(b_0^{\\ast}\\) en esta expresión es \\(b_0 = \\widehat{s}_T\\), que es el promedio móvil de longitud \\(k\\). Este modelo, junto con esta elección de pesos, se llama un modelo de media constante local. Bajo un modelo de media constante global, se utilizan pesos iguales y la estimación de mínimos cuadrados de \\(\\beta_0\\) es el promedio total, \\(\\overline{y}\\). Bajo el modelo de media constante local, damos igual peso a las observaciones dentro de \\(k\\) unidades de tiempo del tiempo de evaluación \\(T\\) y peso cero a otras observaciones. Aunque es intuitivamente atractivo dar más peso a las observaciones más recientes, la idea de un corte abrupto en un \\(k\\) algo arbitrario no es atractiva. Esta crítica se aborda utilizando la suavización exponencial, introducida en la siguiente sección. 9.2 Suavización Exponencial Las estimaciones de suavización exponencial son promedios ponderados de valores pasados de una serie, donde los pesos se obtienen mediante una serie que disminuye exponencialmente. Para ilustrar, piense en \\(w\\) como un número de peso entre cero y uno, y considere el promedio ponderado \\[ \\frac{y_t + w y_{t-1} + w^2 y_{t-2} + w^3 y_{t-3} + \\ldots}{1/(1-w)}. \\] Este es un promedio ponderado porque los pesos \\(w^k (1-w)\\) suman uno, es decir, una expansión de serie geométrica da como resultado \\(\\sum_{k=0}^{\\infty }w^k = 1/(1-w)\\). Debido a que no hay observaciones disponibles en el pasado infinito, usamos la versión truncada \\[\\begin{equation} \\widehat{s}_t = \\frac{y_t + w y_{t-1} + \\ldots + w^{t-1} y_1 + \\ldots + w^t y_0}{1/(1-w) } \\tag{9.4} \\end{equation}\\] para definir la estimación suavizada exponencialmente de la serie. Aquí, \\(y_0\\) es el valor inicial de la serie y a menudo se elige como cero, \\(y_1\\), o el valor promedio de la serie, \\(\\overline{y}\\). Al igual que las estimaciones de promedio móvil, las estimaciones suavizadas en la ecuación (9.4) proporcionan mayores pesos a observaciones más recientes en comparación con observaciones lejanas respecto al tiempo \\(t\\). A diferencia de los promedios móviles, la función de peso es suave. La definición de estimaciones de suavización exponencial en la ecuación (9.4) parece compleja. Sin embargo, al igual que con los promedios móviles en la ecuación (9.2), podemos reexpresar la ecuación (9.4) recursivamente para obtener \\[\\begin{equation} \\widehat{s}_t = \\widehat{s}_{t-1} + (1-w)(y_t-\\widehat{s}_{t-1}) = (1-w) y_t + w \\widehat{s}_{t-1}. \\tag{9.5} \\end{equation}\\] La expresión de las estimaciones suavizadas en la ecuación (9.5) es más fácil de calcular que la definición en la ecuación (9.4). La ecuación (9.5) también proporciona ideas sobre el rol de \\(w\\) como el parámetro de suavización. Por ejemplo, por un lado, cuando \\(w\\) se acerca a cero, \\(\\widehat{s}_t\\) se acerca a \\(y_t\\). Esto indica que se ha realizado poca suavización. Por otro lado, cuando \\(w\\) se acerca a uno, el efecto de \\(y_t\\) en \\(\\widehat{s}_t\\) es mínimo. Esto indica que se ha realizado una gran cantidad de suavización porque el valor ajustado actual está compuesto casi en su totalidad por observaciones pasadas. Ejemplo: Componente Médico del IPC - Continuación. Para ilustrar el efecto de la elección del parámetro de suavización, considere los dos paneles de la Figura 9.2. Estos son gráficos de series temporales del índice trimestral del componente médico del IPC. En el panel superior, la serie suavizada con \\(w=0.2\\) se superpone a la serie real. El panel inferior es el gráfico correspondiente con \\(w=0.8\\). En estas figuras, podemos observar que cuanto mayor es \\(w\\), más suaves son los valores ajustados. Figura 9.2: Componente Médico del Índice de Precios al Consumidor con Suavización. En ambos paneles, la línea discontinua es el índice. En el panel superior, la línea continua es la versión suavizada con \\(w\\)=0.2. En el panel inferior, la línea continua es la versión suavizada con \\(w\\)=0.8. La ecuación (9.5) también sugiere usar la relación \\(\\widehat{y}_{T+l} = \\widehat{s}_T\\) para nuestro pronóstico de \\(y_{T+l}\\), es decir, la serie en \\(l\\) unidades de tiempo hacia el futuro. Los pronósticos no solo proporcionan una forma de predecir el futuro, sino también de evaluar el ajuste. En el tiempo \\(t-1\\), nuestro “pronóstico” de \\(y_t\\) es \\(\\widehat{s}_{t-1}\\). La diferencia se llama el error de predicción de un paso. Para evaluar el grado de ajuste, usamos la suma de los cuadrados de los errores de predicción de un paso \\[\\begin{equation} SS\\left( w\\right) = \\sum_{t=1}^T \\left( y_t - \\widehat{s}_{t-1} \\right)^2. \\tag{9.6} \\end{equation}\\] Es importante notar que esta suma de cuadrados es una función del parámetro de suavización, \\(w\\). Esto proporciona un criterio para elegir el parámetro de suavización: elegir el \\(w\\) que minimice \\(SS\\left( w\\right)\\). Tradicionalmente, los analistas han recomendado que \\(w\\) se encuentre dentro del intervalo (.70, .95), sin proporcionar un criterio objetivo para la elección. Aunque minimizar \\(SS\\left( w\\right)\\) proporciona un criterio objetivo, también es intensivo computacionalmente. En ausencia de una rutina numérica sofisticada, esta minimización generalmente se realiza calculando \\(SS\\left( w\\right)\\) para varios valores de \\(w\\) y eligiendo el \\(w\\) que proporcione el valor más pequeño de \\(SS\\left( w\\right)\\). Para ilustrar la elección del parámetro de suavización exponencial \\(w\\), volvemos al ejemplo del IPC médico. La Figura 9.3 resume el cálculo de \\(SS\\left( w\\right)\\) para varios valores de \\(w\\). Para este conjunto de datos, parece que una elección de \\(w \\approx 0.50\\) minimiza \\(SS(w)\\). Figura 9.3: Suma de los Cuadrados de los Errores de Predicción de un Paso. Gráfico de la suma de los cuadrados de los errores de predicción \\(SS(w)\\) como una función del parámetro de suavización exponencial \\(w\\). Al igual que con los promedios móviles, la presencia de una tendencia lineal en el tiempo, \\(T_t = \\beta_0 + \\beta_1 t\\), puede manejarse mediante el siguiente procedimiento de suavización doble: Crear una serie suavizada utilizando la ecuación (9.5), es decir, \\(\\widehat{s}_t^{(1)} = (1-w) y_t + w \\widehat{s}_{t-1}^{(1)}.\\) Crear una serie suavizada doble utilizando la ecuación (9.5) y tratando la serie suavizada creada en el paso (i) como entrada. Es decir, \\(\\widehat{s}_t^{(2)} = (1-w) \\widehat{s}_t^{(1)} + w\\widehat{s}_{t-1}^{(2)}\\). La estimación de la tendencia es \\(b_{1,T} = ((1-w)/w)(\\widehat{s}_T^{(1)}- \\widehat{s}_T^{(2)})\\). Los pronósticos se obtienen mediante \\(\\widehat{y}_{T+l}= b_{0,T}+b_{1,T}~l\\), donde la estimación del intercepto es \\(b_{0,T} = 2\\widehat{s}_T^{(1)} - \\widehat{s}_T^{(2)}\\). También mostraremos cómo usar la suavización exponencial para datos con patrones estacionales en la Sección 9.3. Mínimos Cuadrados Ponderados Al igual que con los promedios móviles, una característica importante de las estimaciones suavizadas exponencialmente es que pueden expresarse como estimaciones de mínimos cuadrados ponderados (WLS, por sus siglas en inglés). Para ver esto, para el modelo \\(y_t = \\beta_0 + \\varepsilon_t,\\) la suma ponderada de cuadrados en la ecuación (9.3) se reduce a \\[ WSS_T\\left( b_0^{\\ast}\\right) = \\sum_{t=1}^T w_t \\left( y_t - b_0^{\\ast} \\right)^2. \\] El valor de \\(b_0^{\\ast}\\) que minimiza \\(WSS_T\\left( b_0^{\\ast} \\right)\\) es \\(b_0 = \\left( \\sum_{t=1}^T w_t y_t \\right) / \\left( \\sum_{t=1}^T w_t \\right)\\). Con la elección \\(w_t = w^{T-t}\\), tenemos \\(b_0 \\approx \\widehat{s}_T\\), donde hay igualdad excepto por el pequeño detalle del valor inicial. Por lo tanto, las estimaciones de suavización exponencial son estimaciones de \\(WLS\\). Además, debido a la forma de los pesos elegidos, las estimaciones de suavización exponencial también se llaman estimaciones de mínimos cuadrados con descuento. Aquí, \\(w_t=w^{T-t}\\) es una función de descuento que podría usarse al considerar el valor temporal del dinero. 9.3 Modelos de Series Temporales Estacionales Los patrones estacionales aparecen en muchas series temporales que surgen en el estudio de los negocios y la economía. Los modelos de estacionalidad se utilizan principalmente para abordar patrones que surgen como resultado de un fenómeno físico identificable. Por ejemplo, los patrones estacionales del clima afectan la salud de las personas y, a su vez, la demanda de medicamentos recetados. Estos mismos modelos estacionales pueden utilizarse para modelar comportamientos cíclicos más prolongados. Existen diversas técnicas disponibles para manejar patrones estacionales, incluidas los efectos estacionales fijos, los modelos autorregresivos estacionales y los métodos de suavización exponencial estacional. A continuación, abordamos cada una de estas técnicas. Efectos Estacionales Fijos Recuerde que, en las ecuaciones (7.1) y (7.2), usamos \\(S_t\\) para representar los efectos estacionales bajo los modelos de descomposición aditiva y multiplicativa, respectivamente. Un modelo de efectos estacionales fijos representa \\(S_t\\) como una función del tiempo \\(t\\). Los dos ejemplos más importantes son las funciones estacionales binarias y las funciones trigonométricas. La Sección 7.2, Ejemplo de Tendencias en la Votación, mostró cómo usar una variable estacional binaria y el Ejemplo del Costo de los Medicamentos Recetados a continuación demostrará el uso de funciones trigonométricas. El calificativo “efectos fijos” significa que las relaciones son constantes en el tiempo. En contraste, tanto las técnicas de suavización exponencial como las autorregresivas nos proporcionan métodos que se adaptan a eventos recientes y permiten tendencias que cambian con el tiempo. Una amplia clase de patrones estacionales puede representarse utilizando funciones trigonométricas. Considere la función \\[ \\mathrm{g}(t)=a\\sin (ft+b) \\] donde \\(a\\) es la amplitud (el valor más alto de la curva), \\(f\\) es la frecuencia (el número de ciclos que ocurren en el intervalo \\((0,2\\pi )\\)) y \\(b\\) es el desplazamiento de fase. Debido a una identidad básica, \\(\\sin (x+y) = \\sin x \\cos y + \\sin y \\cos x\\), podemos escribir \\[ \\mathrm{g}(t) = \\beta_1 \\sin (ft) + \\beta_2 \\cos (ft) \\] donde \\(\\beta_1 = a \\cos b\\) y \\(\\beta_2 = a \\sin b\\). Para una serie temporal con base estacional SB, podemos representar una amplia variedad de patrones estacionales utilizando \\[\\begin{equation} S_t = \\sum_{i=1}^m a_i \\sin (f_i t + b_i) = \\sum_{i=1}^m \\left\\{ \\beta_{1i} \\sin (f_i t) + \\beta_{2i} \\cos (f_i t) \\right\\} \\tag{9.7} \\end{equation}\\] con \\(f_i=2\\pi i/SB\\). Para ilustrar, la función compleja que se muestra en la Figura 9.5 fue construida como la suma de las \\((m=)\\) 2 funciones trigonométricas más simples que se muestran en la Figura 9.4. Figura 9.4: Gráfico de Dos Funciones Trigonométricas. Aquí, g\\(_1(t)\\) tiene amplitud \\(a_1=5\\), frecuencia \\(f_1=2 \\pi /12\\) y desplazamiento de fase \\(b_1=0\\). Además, g\\(_2(t)\\) tiene amplitud \\(a_2=2\\), frecuencia \\(f_2=4 \\pi/12\\) y desplazamiento de fase \\(b_2=\\pi/4\\). Figura 9.5: Gráfico de la Suma de Dos Funciones Trigonométricas en la Figura 9.4. Considere el modelo \\(y_t=\\beta_0+S_t+\\varepsilon_t\\), donde \\(S_t\\) está especificado en la ecuación (9.7). Debido a que \\(\\sin (f_it)\\) y \\(\\cos (f_it)\\) son funciones del tiempo, pueden tratarse como variables explicativas conocidas. Por lo tanto, el modelo \\[ y_t = \\beta_0 + \\sum_{i=1}^{m}\\left\\{ \\beta_{1i}\\sin (f_i t) + \\beta_{2i} \\cos (f_i t)\\right\\} + \\varepsilon_t \\] es un modelo de regresión lineal múltiple con \\(k=2m\\) variables explicativas. Este modelo puede estimarse utilizando software estadístico estándar de regresión. Además, podemos usar nuestras técnicas de selección de variables para elegir \\(m\\), el número de funciones trigonométricas. Notamos que \\(m\\) es como máximo \\(SB/2\\), para \\(SB\\) par. De lo contrario, tendríamos colinealidad perfecta debido a la periodicidad de la función seno. El siguiente ejemplo demuestra cómo elegir \\(m\\). Ejemplo: Costo de Medicamentos Recetados. Consideramos una serie del Programa de Medicamentos Recetados del Estado de Nueva Jersey, el costo por receta. Esta serie mensual está disponible para el período de agosto de 1986 hasta marzo de 1992, inclusive. La Figura 9.6 muestra que la serie es claramente no estacionaria, ya que el costo por receta está aumentando con el tiempo. Hay varias maneras de manejar esta tendencia. Se puede comenzar con una tendencia lineal en el tiempo e incluir reclamos de retraso para manejar autocorrelaciones. Para esta serie, un buen enfoque es considerar los cambios porcentuales en la serie de costo por receta. La Figura 9.7 es un gráfico de series temporales de los cambios porcentuales. En esta figura, vemos que muchas de las tendencias evidentes en la Figura 9.6 han sido filtradas. Figura 9.6: Gráfico de Series Temporales del Costo por Reclamo de Medicamentos Recetados en el Estado de Nueva Jersey. Figura 9.7: Cambios Porcentuales Mensuales del Costo por Reclamo de Medicamentos Recetados. La Figura 9.7 muestra algunos patrones estacionales leves en los datos. Una inspección detallada de los datos revela mayores incrementos porcentuales en primavera y menores en los meses de otoño. Se ajustó una función trigonométrica usando \\(m=1\\); el modelo ajustado es \\[ \\begin{array}{cccc} \\widehat{y}_t= &amp; 1.2217 &amp; -1.6956\\sin (2\\pi t/12) &amp; +0.6536\\cos (2\\pi t/12) \\\\ {\\small \\text{errores estándar}} &amp; {\\small (0.2325)} &amp; {\\small (0.3269)} &amp; {\\small (0.3298)} \\\\ {\\small t-\\text{estadísticos}} &amp; {\\small [5.25]} &amp; {\\small [-5.08]} &amp; {\\small [1.98]} \\end{array} \\] con \\(s=1.897\\) y \\(R^2=31.5\\) por ciento. Este modelo revela algunos patrones estacionales importantes. Las variables explicativas son estadísticamente significativas y una prueba \\(F\\) establece la significancia del modelo. La Figura 9.8 muestra los datos con los valores ajustados del modelo superpuestos. Estos valores ajustados superpuestos ayudan a detectar visualmente los patrones estacionales. Figura 9.8: Cambios Porcentuales Mensuales del Costo por Reclamo de Medicamentos Recetados. Se han superpuesto los valores ajustados del modelo trigonométrico estacional. El examen de los residuales de este modelo ajustado reveló pocos patrones adicionales. Además, se ajustó el modelo usando \\(m=2\\) a los datos, resultando en \\(R^2 = 33.6\\) por ciento. Podemos decidir si usar \\(m=1\\) o \\(2\\) considerando el modelo \\[ y_t = \\beta_0 + \\sum_{i=1}^2 \\left\\{ \\beta_{1i} \\sin (f_i t) + \\beta_{2i} \\cos (f_i t)\\right\\} + \\varepsilon_t \\] y probando \\(H_0:\\beta_{12} = \\beta_{22}=0\\). Usando la prueba parcial \\(F\\), con \\(n=67, k=p=2\\), tenemos \\[ F-ratio=\\frac{(0.336-0.315)/2}{(1.000-0.336)/62} = 0.98. \\] Con \\(df_1=p=2\\) y \\(df_2=n-(k+p+1)=62\\), el percentil 95\\(th\\) de la distribución \\(F\\) es \\(F\\)-value = 3.15. Debido a que \\(F-ratio&lt;F-value\\), no podemos rechazar \\(H_0\\) y concluimos que \\(m=1\\) es la opción preferida. Finalmente, también es interesante ver cómo nuestro modelo de los datos transformados funciona con nuestros datos originales, en unidades de costo por reclamo. Los valores ajustados de los incrementos porcentuales se convirtieron de nuevo en valores ajustados de costo por reclamo. La Figura 9.9 muestra los datos originales con los valores ajustados superpuestos. Esta figura establece la fuerte relación entre las series reales y ajustadas. Figura 9.9: Cambios Porcentuales Mensuales del Costo por Reclamo de Medicamentos Recetados. Se han superpuesto los valores ajustados del modelo trigonométrico estacional. Modelos Autorregresivos Estacionales En el Capítulo 8 examinamos patrones a lo largo del tiempo utilizando las autocorrelaciones de la forma \\(\\rho_{k}\\), la correlación entre \\(y_t\\) y \\(y_{t-k}\\). Construimos representaciones de estos patrones temporales utilizando modelos autorregresivos, modelos de regresión con respuestas rezagadas como variables explicativas. Los patrones estacionales en el tiempo se pueden manejar de manera similar. Definimos el modelo autorregresivo estacional de orden P, SAR(P), como \\[\\begin{equation} y_t=\\beta_0+\\beta_1y_{t-SB}+\\beta_2y_{t-2SB}+\\ldots+\\beta _{P}y_{t-PSB}+\\varepsilon_t, \\tag{9.8} \\end{equation}\\] donde \\(SB\\) es la base estacional considerada. Por ejemplo, usando \\(SB=12\\), un modelo estacional de orden uno, \\(SAR(1)\\), es \\[ y_t=\\beta_0+\\beta_1y_{t-12}+\\varepsilon_t. \\] A diferencia del modelo \\(AR(12)\\) definido en el Capítulo 9, para el modelo \\(SAR(1)\\) omitimos \\(y_{t-1},y_{t-2},\\ldots,y_{t-11}\\) como variables explicativas, aunque conservamos \\(y_{t-12}\\). Al igual que en el Capítulo 8, la elección del orden del modelo se realiza examinando la estructura de autocorrelación y utilizando una estrategia iterativa de ajuste del modelo. De manera similar, la elección de la estacionalidad \\(SB\\) se basa en un examen de los datos. Remitimos al lector interesado a Abraham y Ledolter (1983). Ejemplo: Costo de Medicamentos Recetados - Continuación. Tabla 9.1 presenta las autocorrelaciones para el incremento porcentual en el costo por reclamo de medicamentos recetados. Hay \\(T=67\\) observaciones en este conjunto de datos, lo que resulta en un error estándar aproximado de \\(se(r_{k})=1/\\sqrt{67}\\approx 0.122\\). Así, las autocorrelaciones en y alrededor de los rezagos 6, 12 y 18 parecen ser significativamente diferentes de cero. Esto sugiere usar \\(SB=6\\). Un examen más detallado de los datos sugirió un modelo \\(SAR(2)\\). Table 9.1. Autocorrelaciones del Costo por Reclamos de Medicamentos Recetados \\[ \\small{ \\begin{array}{c|ccccccccc} \\hline k &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 \\\\ r_{k} &amp; 0.08 &amp; 0.10 &amp; -0.12 &amp; -0.11 &amp; -0.32 &amp; -0.33 &amp; -0.29 &amp; 0.07 &amp; 0.08 \\\\ \\hline k &amp; 10 &amp; 11 &amp; 12 &amp; 13 &amp; 14 &amp; 15 &amp; 16 &amp; 17 &amp; 18 \\\\ r_{k} &amp; 0.25 &amp; 0.24 &amp; 0.31 &amp; -0.01 &amp; 0.14 &amp; -0.10 &amp; -0.08 &amp; -0.25 &amp; -0.18 \\\\ \\hline \\end{array} } \\] El modelo ajustado resultante es: \\[ \\begin{array}{cccc} \\widehat{y}_t= &amp; 1.2191 &amp; -0.2867y_{t-6} &amp;+0.3120y_{t-12} \\\\ {\\small \\text{errores estándar}} &amp; {\\small (0.4064)} &amp; {\\small (0.1502)} &amp; {\\small (0.1489)} \\\\ {\\small t-\\text{estadísticos}} &amp; {\\small [3.00]} &amp; {\\small [-1.91]} &amp; {\\small [2.09]} \\end{array} \\] \\(s=2.156\\). Este modelo fue ajustado utilizando mínimos cuadrados condicionales. Note que, debido a que estamos usando \\(y_{t-12}\\) como una variable explicativa, el primer residual que se puede estimar es el 13. Es decir, perdemos doce observaciones al realizar el rezago por doce al usar estimaciones de mínimos cuadrados. Suavizamiento Exponencial Estacional Un método de suavizamiento exponencial que ha gozado de considerable popularidad entre los pronosticadores es el modelo estacional aditivo de Holt-Winter. Aunque es difícil expresar los pronósticos de este modelo como estimaciones de mínimos cuadrados ponderados, el modelo parece funcionar bien en la práctica. Holt (1957) introdujo la siguiente generalización del método de suavizamiento exponencial doble. Sea \\(w_1\\) y \\(w_2\\) parámetros de suavizamiento y calcule recursivamente las estimaciones de parámetros: \\[\\begin{eqnarray*} b_{0,t} &amp;=&amp;(1-w_1)y_t+w_1(b_{0,t-1}+b_{1,t-1}) \\\\ b_{1,t} &amp;=&amp;(1-w_2)(b_{0,t}-b_{0,t-1})+w_2b_{1,t-1} . \\end{eqnarray*}\\] Estas estimaciones se pueden utilizar para pronosticar el modelo de tendencia lineal, \\(y_t = \\beta_0 + \\beta_1 t + \\varepsilon_t\\). Los pronósticos son \\(\\widehat{y}_{T+l} = b_{0,T} + b_{1,T}~l\\). Con la elección \\(w_1=w_2=2w/(1+w)\\), se puede demostrar que el procedimiento de Holt produce las mismas estimaciones que las descritas en la Sección 9.2. Winters (1960) extendió el procedimiento de Holt para acomodar tendencias estacionales. Específicamente, el modelo estacional aditivo de Holt-Winter es \\[ y_t = \\beta_0 + \\beta_1 t + S_t + \\varepsilon_t \\] donde \\(S_t=S_{t-SB}, S_1+S_2+\\ldots+S_{SB}=0\\), y \\(SB\\) es la base estacional. Ahora empleamos tres parámetros de suavizamiento: uno para el nivel, \\(w_1\\), uno para la tendencia, \\(w_2\\), y uno para la estacionalidad, \\(w_{3}\\). Las estimaciones de los parámetros para este modelo se determinan recursivamente usando: \\[\\begin{eqnarray*} b_{0,t} &amp;=&amp;(1-w_1)\\left( y_t-\\widehat{S}_{t-SB}\\right) +w_1(b_{0,t-1}+b_{1,t-1}) \\\\ b_{1,t} &amp;=&amp;(1-w_2)(b_{0,t}-b_{0,t-1})+w_2b_{1,t-1} \\\\ \\widehat{S}_t &amp;=&amp;(1-w_{3})\\left( y_t-b_{0,t}\\right) +w_{3}\\widehat{S} _{t-SB}. \\end{eqnarray*}\\] Con estas estimaciones de parámetros, los pronósticos se determinan usando: \\[ \\widehat{y}_{T+l}=b_{0,T}+b_{1,T}~l+\\widehat{S}_T(l) \\] donde \\(\\widehat{S}_T(l)=\\widehat{S}_{T+l}\\) para \\(l=1,2,\\ldots,SB\\), \\(\\widehat{S}_T(l)=\\widehat{S}_{T+l-SB}\\) para \\(l=SB+1,\\ldots,2SB\\), y así sucesivamente. Para calcular las estimaciones recursivas, debemos decidir (i) valores iniciales y (ii) una elección de parámetros de suavizamiento. Para determinar valores iniciales, recomendamos ajustar una ecuación de regresión a la primera parte de los datos. La ecuación de regresión incluirá una tendencia lineal en el tiempo, \\(\\beta_0 + \\beta_1 t\\), y \\(SB-1\\) variables binarias para la variación estacional. Por lo tanto, solo se requieren \\(SB+1\\) observaciones para determinar las estimaciones iniciales \\(b_{0,0}, b_{1,0}, y_{1-SB}, y_{2-SB},\\ldots, y_0\\). Elegir los tres parámetros de suavizamiento es más difícil. Los analistas han encontrado complicado elegir tres parámetros utilizando un criterio objetivo, como la minimización de la suma de los errores de predicción de un paso, como en la Sección 9.2. Parte de la dificultad radica en la no linealidad de la minimización, lo que resulta en un tiempo computacional prohibitivo. Otra parte de la dificultad es que funciones como la suma de los errores de predicción de un paso a menudo resultan ser relativamente insensibles a la elección de parámetros. En cambio, los analistas han confiado en reglas prácticas para guiar la elección de los parámetros de suavizamiento. En particular, dado que los efectos estacionales pueden tardar varios años en desarrollarse, se recomienda un valor más bajo de \\(w_{3}\\) (lo que resulta en un mayor suavizamiento). Cryer y Miller (1994) recomiendan \\(w_1=w_2=0.9\\) y \\(w_{3}=0.6\\). 9.4 Pruebas de Raíces Unitarias Ahora hemos visto dos modelos en competencia que manejan la no estacionariedad con una tendencia en la media: el modelo de tendencia lineal en el tiempo y el modelo de caminata aleatoria. La Sección 7.6 ilustró cómo podemos elegir entre estos dos modelos utilizando datos fuera de la muestra. Para un procedimiento de selección basado en datos dentro de la muestra, considere el modelo \\[ y_t = \\mu_0 + \\phi (y_{t-1} - \\mu_0) + \\mu_1 \\left( \\phi + (1-\\phi) t \\right) + \\varepsilon_t. \\] Cuando \\(\\phi =1\\), esto se reduce a un modelo de caminata aleatoria con \\(y_t=\\mu_1+y_{t-1}+\\varepsilon_t.\\) Cuando \\(\\phi &lt;1\\) y \\(\\mu_1=0\\), esto se reduce a un modelo \\(AR(1)\\), \\(y_t=\\beta_0+\\phi y_{t-1}+\\varepsilon_t\\), con \\(\\beta_0=\\mu_0\\left( 1-\\phi \\right)\\). Cuando \\(\\phi =0\\), esto se reduce a un modelo de tendencia lineal en el tiempo con \\(y_t = \\mu_0 + \\mu_1 t + \\varepsilon_t.\\) Ejecutar un modelo donde la variable dependiente potencialmente sea una caminata aleatoria es problemático. Por lo tanto, es habitual usar mínimos cuadrados en el modelo \\[\\begin{equation} y_t-y_{t-1}=\\beta_0+\\left( \\phi -1\\right) y_{t-1}+\\beta _1t+\\varepsilon_t \\tag{9.9} \\end{equation}\\] donde interpretamos \\(\\beta_0=\\mu_0\\left( 1-\\phi \\right) + \\phi \\mu_1\\) y \\(\\beta_1=\\mu_1\\left( 1-\\phi \\right).\\) A partir de esta regresión, sea \\(t_{DF}\\) el estadístico \\(t\\) asociado con la variable \\(y_{t-1}\\). Deseamos usar el estadístico \\(t\\) para probar la hipótesis nula \\(H_0:\\phi =1\\) frente a la alternativa unilateral \\(H_{a}:\\phi &lt;1\\). Dado que \\(\\{y_{t-1}\\}\\) es un proceso de caminata aleatoria bajo la hipótesis nula, la distribución de \\(t_{DF}\\) no sigue la distribución \\(t\\) habitual, sino una distribución especial, debida a Dickey y Fuller (1979). Esta distribución ha sido tabulada y programada en varios paquetes estadísticos, Fuller (1996). Ejemplo: Tasas de Participación Laboral - Continuación. Ilustramos el rendimiento de las pruebas de Dickey-Fuller en las tasas de participación laboral introducidas en el Capítulo 7. Allí, establecimos que la serie era claramente no estacionaria y que los pronósticos fuera de la muestra mostraron que la caminata aleatoria era preferida en comparación con el modelo de tendencia lineal en el tiempo. Tabla 9.2 resume la prueba. Tanto sin (\\(\\mu_1 = 0\\)) como con (\\(\\mu_1 \\neq 0\\)) la línea de tendencia, el estadístico \\(t\\) (\\(t_{DF}\\)) no es estadísticamente significativo (en comparación con el valor crítico del 10%). Esto proporciona evidencia de que la caminata aleatoria es la elección de modelo preferida. Table 9.2. Estadísticos de Prueba de Dickey-Fuller con Valores Críticos \\[ \\small{ \\begin{array}{c|cccc} \\hline &amp; \\text{Sin Tendencia} &amp; &amp;\\text{Con Tendencia} \\\\ &amp; &amp; 10\\% ~ \\text{Crítico} &amp; &amp; 10\\%~ \\text{Crítico} \\\\ \\text{Rezago} (p)&amp; t_{DF} &amp; \\text{Valor} &amp; t_{DF} &amp; \\text{Valor} \\\\ \\hline &amp; -1.614 &amp; -2.624 &amp; -0.266 &amp; -3.228 \\\\ 1 &amp; -1.816 &amp; -2.625 &amp; -0.037 &amp; -3.230 \\\\ 2 &amp; -1.736 &amp; -2.626 &amp; 0.421 &amp; -3.233 \\\\ \\hline \\end{array} } \\] Una crítica a la prueba de Dickey-Fuller es que se presume que el término de perturbación en la ecuación (9.9) no tiene correlación serial. Para protegernos contra esto, una alternativa comúnmente utilizada es el estadístico de prueba de Dickey-Fuller aumentada. Este es el estadístico \\(t\\) asociado con la variable \\(y_{t-1}\\) utilizando mínimos cuadrados ordinarios en la siguiente ecuación: \\[\\begin{equation} y_t-y_{t-1}=\\beta_0+\\left( \\phi -1\\right) y_{t-1}+\\beta _1t+\\sum_{j=1}^{p}\\phi_{j}\\left( y_{t-j}-y_{t-j-1}\\right) +\\varepsilon_t. \\tag{9.10} \\end{equation}\\] En esta ecuación, hemos aumentado el término de perturbación con términos autorregresivos en las diferencias {\\(y_{t-j}-y_{t-j-1}\\)}. La idea es que estos términos sirven para capturar la correlación serial en el término de perturbación. La investigación no ha llegado a un consenso sobre cómo elegir el número de rezagos (\\(p\\)); en la mayoría de las aplicaciones, los analistas proporcionan los resultados del estadístico de prueba para varias opciones de rezagos y esperan que las conclusiones alcanzadas sean cualitativamente similares. Este es ciertamente el caso para las tasas de participación laboral, como se demuestra en la Tabla 9.2. Aquí, vemos que para cada elección de rezago, no se puede rechazar la hipótesis nula de la caminata aleatoria. 9.5 Modelos ARCH/GARCH Hasta este punto, nos hemos centrado en pronosticar el nivel de la serie, es decir, la media condicional. Sin embargo, hay aplicaciones importantes, notablemente en el estudio de las finanzas, donde es importante pronosticar la variabilidad. Para ilustrarlo, la varianza desempeña un papel clave en la valoración de opciones, como al usar la fórmula de Black-Scholes. Muchas series temporales financieras exhiben agrupamiento de volatilidad, es decir, períodos de alta volatilidad (grandes cambios en la serie) seguidos por períodos de baja volatilidad. Para ilustrarlo, considere lo siguiente. Ejemplo: Rendimientos diarios del S &amp; P 500. La Figura 9.10 proporciona un gráfico de series temporales de los rendimientos diarios del Standard &amp; Poor’s 500 durante el período 2000-2006, inclusive. Aquí, vemos que la primera parte de la serie, antes de enero de 2003, es más volátil en comparación con la última parte de la serie. Excepto por la volatilidad cambiante, la serie parece ser estacionaria, sin aumentos o disminuciones dramáticas. Figura 9.10: Gráfico de Series Temporales de los Rendimientos Diarios del S&amp;P, 2000-2006, inclusive. El concepto de variabilidad cambiante a lo largo del tiempo parece estar en conflicto con nuestras nociones de estacionariedad. Esto se debe a que una condición para la estacionariedad débil es que la serie tenga una varianza constante. Lo sorprendente es que podemos permitir varianzas cambiantes condicionando al pasado y aún así mantener un modelo débilmente estacionario. Para verlo matemáticamente, usamos la notación \\(\\Omega_t\\) para denotar el conjunto de información, la colección de conocimientos sobre el proceso hasta e incluyendo el tiempo \\(t\\). Para una serie débilmente estacionaria, podemos denotar esto como \\(\\Omega _t=\\{\\varepsilon_t,\\varepsilon_{t-1},\\ldots\\}\\). Permitimos que la varianza dependa del tiempo \\(t\\) condicionando al pasado, \\[ \\sigma_t^2=\\mathrm{Var}_{t-1}\\left( \\varepsilon_t\\right) =\\mathrm{E} \\left( \\left[ \\varepsilon_t-\\mathrm{E}\\left( \\varepsilon_t|\\Omega _{t-1}\\right) \\right] ^2|\\Omega_{t-1}\\right) . \\] Ahora presentamos varios modelos paramétricos de \\(\\sigma_t^2\\) que nos permiten cuantificar y pronosticar esta volatilidad cambiante. Modelo ARCH El modelo de heterocedasticidad cambiante autorregresiva de orden p, \\(ARCH(p),\\) es debido a Engle (1982). Ahora asumimos que la distribución de \\(\\varepsilon_t\\) dado \\(\\Omega_{t-1}\\) es normal con media cero y varianza \\(\\sigma_t^2\\). Además, asumimos que la varianza condicional se determina recursivamente por \\[ \\sigma_t^2=w+\\gamma_1\\varepsilon_{t-1}^2+\\ldots+\\gamma _{p}\\varepsilon_{t-p}^2=w+\\gamma (B)\\varepsilon_t^2, \\] donde \\(\\gamma (x)=\\gamma_1x+\\ldots+\\gamma_{p}x^{p}.\\) Aquí, \\(w&gt;0\\) es el parámetro de volatilidad “a largo plazo” y $ _1,,_p$ son coeficientes tales que \\(\\gamma _j \\geq 0\\) y \\(\\gamma (1)=\\sum_{j=1}^{p} \\gamma_j &lt; 1\\). En el caso de que \\(p=1\\), podemos ver que un cambio grande en la serie \\(\\varepsilon_{t-1}^2\\) puede inducir una gran varianza condicional \\(\\sigma_t^2\\). Órdenes más altos de \\(p\\) ayudan a capturar efectos a más largo plazo. Por lo tanto, este modelo es intuitivamente atractivo para los analistas. Curiosamente, Engle proporcionó condiciones adicionales leves para asegurar que \\(\\{\\varepsilon_t\\}\\) sea débilmente estacionaria. Así, a pesar de tener una varianza condicional cambiante, la varianza incondicional permanece constante a lo largo del tiempo. Modelo GARCH El modelo GARCH generalizado de orden p, \\(GARCH(p,q),\\) complementa al modelo \\(ARCH\\) de la misma manera que el promedio móvil complementa al modelo autorregresivo. Al igual que el modelo \\(ARCH\\), asumimos que la distribución de \\(\\varepsilon_t\\) dado \\(\\Omega_{t-1}\\) sigue una distribución normal con media cero y varianza \\(\\sigma_t^2\\). La varianza condicional se determina recursivamente mediante \\[ \\sigma_t^2-\\delta_1\\sigma_{t-1}^2+-\\ldots-\\delta_{q}\\sigma _{t-q}^2=w+\\gamma_1\\varepsilon_{t-1}^2+\\ldots+\\gamma_{p}\\varepsilon _{t-p}^2, \\] o \\(\\sigma_t^2=w+\\gamma (B)\\varepsilon_t^2+\\delta (B)\\sigma_t^2,\\) donde \\(\\delta (x)=\\delta_1x+\\ldots+\\delta_{q}x^{q}.\\) Además de los requisitos del modelo \\(ARCH(p)\\), también necesitamos que \\(\\delta_{j}\\geq 0\\) y \\(\\gamma (1)+\\delta \\left( 1\\right) &lt;1\\). Resulta que el modelo \\(GARCH(p,q)\\) también es débilmente estacionario, con media cero y varianza (incondicional) \\(\\mathrm{Var~}\\varepsilon_t=w/(1-\\gamma (1)-\\delta \\left( 1\\right) ).\\) Ejemplo: Rendimientos Diarios del S &amp; P 500 - Continuado. Tras un examen de los datos (detalles no presentados aquí), se ajustó un modelo \\(MA(2)\\) a la serie con errores \\(GARCH(1,1)\\). Específicamente, si \\(y_t\\) denota el rendimiento diario del S &amp; P, para \\(t=1,\\ldots,1759\\), se ajustó el modelo \\[ y_t = \\beta_0 + \\varepsilon_t - \\theta_1 \\varepsilon_{t-1} - \\theta_2 \\varepsilon_{t-2}, \\] donde la varianza condicional se determina recursivamente mediante \\[ \\sigma_t^2 - \\delta_1 \\sigma_{t-1}^2 = w + \\gamma_1 \\varepsilon_{t-1}^2. \\] El modelo ajustado aparece en la Tabla 9.3. Aquí, el paquete estadístico que utilizamos emplea máxima verosimilitud para determinar los parámetros estimados, así como los errores estándar necesarios para los \\(t\\)-estadísticos. Los \\(t\\)-estadísticos muestran que todas las estimaciones de los parámetros, excepto \\(\\theta_1\\), son estadísticamente significativas. Como se discutió en el Capítulo 8, la convención es retener los coeficientes de menor orden, como \\(\\theta_1\\), si los coeficientes de mayor orden, como \\(\\theta_2\\), son significativos. Observe en la Tabla 9.3 que la suma del coeficiente \\(ARCH\\) (\\(\\delta_1\\)) y el coeficiente \\(GARCH\\) (\\(\\gamma_1\\)) es casi uno, con el coeficiente \\(GARCH\\) sustancialmente mayor que el coeficiente \\(ARCH\\). Este fenómeno también se reporta en Diebold (2004, página 400), quien afirma que se encuentra comúnmente en estudios de rendimientos de activos financieros. Tabla 9.3. Modelo Ajustado para Rendimientos Diarios del S &amp; P 500 \\[ \\small{ \\begin{array}{crr} \\hline \\text{Parámetro} &amp; \\text{Estimación} &amp; t-\\text{estadístico} \\\\ \\hline \\beta_0 &amp; 0.0004616 &amp; 2.51 \\\\ \\theta_1 &amp; -0.0391526 &amp; -1.49 \\\\ \\theta_2 &amp; -0.0612666 &amp; -2.51 \\\\ \\delta_1 &amp; 0.0667424 &amp; 6.97 \\\\ \\gamma_1 &amp; 0.9288311 &amp; 93.55 \\\\ w &amp; 5.61\\times 10^{-7} &amp; 2.30 \\\\ \\text{Log-verosimilitud} &amp; 5,658.852 &amp; \\\\ \\hline \\end{array} } \\] 9.6 Lecturas y Referencias Adicionales Para otras variaciones del método de promedio móvil y suavización exponencial, consulte Abraham y Ledolter (1983). Para un tratamiento más detallado de las pruebas de raíz unitaria, remitimos al lector a Diebold (2004) o Fuller (1996) para un tratamiento más avanzado. Referencias del Capítulo Abraham, Bovas and Ledolter, Johannes (1983). Statistical Methods for Forecasting. John Wiley &amp; Sons, New York. Cryer, Jon D. and Robert B. Miller (1994). Statistics for Business: Data Analysis and Modelling. PWS-Kent, Boston. Dickey, D. A. and Wayne A. Fuller (1979). Distribution of the estimators for autoregressive time series with a unit root. Journal of the American Statistical Association 74, 427-431. Diebold, Francis X. (2004). Elements of Forecasting , Third Edition. Thomson, South-Western, Mason Ohio. Engle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of UK inflation. Econometrica 50, 987-1007. Fuller, Wayne A. (1996). Introduction to Statistical Time Series, Second Edition. John Wiley &amp; Sons, New York. Holt, C. C. (1957). Forecasting trends and seasonals by exponenetially weighted moving averages. O.N.R. Memorandum, No. 52, Carnegie Institute of Technology. Winters, P. R. (1960). Forecasting sales by exponentially weighted moving averages. Management Science 6, 324-342. "],["C10Panel.html", "Capítulo 10 Modelos de Datos Longitudinales y de Panel 10.1 ¿Qué son los Datos Longitudinales y de Panel? 10.2 Visualización de Datos Longitudinales y de Panel 10.3 Modelos Básicos de Efectos Fijos 10.4 Modelos Extendidos de Efectos Fijos 10.5 Modelos de Efectos Aleatorios 10.6 Lecturas Adicionales y Referencias", " Capítulo 10 Modelos de Datos Longitudinales y de Panel Vista Previa del Capítulo. Los datos longitudinales, también conocidos como datos de panel, están compuestos por una sección transversal de sujetos que observamos repetidamente a lo largo del tiempo. Los datos longitudinales nos permiten estudiar patrones transversales y dinámicos simultáneamente; este capítulo describe varias técnicas para visualizar datos longitudinales. Se introducen dos tipos de modelos: modelos de efectos fijos y modelos de efectos aleatorios. Este capítulo muestra cómo estimar modelos de efectos fijos usando variables explicativas categóricas. La estimación de modelos de efectos aleatorios se pospone a un capítulo posterior; este capítulo describe cuándo y cómo usar estos modelos. 10.1 ¿Qué son los Datos Longitudinales y de Panel? En los capítulos 1 al 6 estudiamos técnicas de regresión transversal que nos permitieron predecir una variable dependiente \\(y\\) utilizando variables explicativas \\(x\\). Para muchos problemas, el mejor predictor es un valor del período anterior; los métodos de series temporales que estudiamos en los capítulos 7 al 9 utilizan la historia de una variable dependiente para la predicción. Por ejemplo, un actuario que busca predecir las reclamaciones de seguros de una pequeña empresa a menudo encontrará que las reclamaciones del año pasado son el mejor predictor. Sin embargo, una limitación de los métodos de series temporales es que se basan en disponer de muchas observaciones a lo largo del tiempo (típicamente 30 o más). Al estudiar reclamaciones anuales de una empresa, rara vez se dispone de una serie temporal larga; ya sea porque las empresas no tienen los datos o, si los tienen, no es razonable usar el mismo modelo estocástico para las reclamaciones de hoy que para las de hace 30 años. Nos gustaría tener un modelo que nos permita utilizar información sobre las características de la empresa, variables explicativas como industria, número de empleados, composición por edad y género, y así sucesivamente, así como el historial reciente de reclamaciones. Es decir, necesitamos un modelo que combine variables explicativas de regresión transversal con variables dependientes rezagadas de series temporales como predictores. El análisis de datos longitudinales representa una fusión entre la regresión y el análisis de series temporales. Los datos longitudinales están compuestos por una sección transversal de sujetos que observamos repetidamente a lo largo del tiempo. A diferencia de los datos de regresión, con los datos longitudinales observamos a los sujetos a lo largo del tiempo. Al observar repetidamente una sección transversal, los analistas pueden realizar mejores evaluaciones de las relaciones de regresión con un diseño de datos longitudinales en comparación con un diseño de regresión. A diferencia de los datos de series temporales, con los datos longitudinales observamos a muchos sujetos. Al observar el comportamiento de series temporales en muchos sujetos, podemos realizar evaluaciones informadas de los patrones temporales incluso cuando solo está disponible una serie (temporal) corta. Los patrones temporales también se conocen como dinámicos. Con los datos longitudinales, podemos estudiar simultáneamente patrones transversales y dinámicos. El término “datos de panel” proviene de encuestas a individuos. En este contexto, un “panel” es un grupo de individuos encuestados repetidamente a lo largo del tiempo. Usamos los términos “datos longitudinales” y “datos de panel” de manera intercambiable, aunque, por simplicidad, a menudo usamos solo el primer término. Como hemos visto en nuestra discusión del Capítulo 6 sobre variables omitidas, cualquier nueva variable puede alterar nuestras impresiones y modelos de la relación entre \\(y\\) y una \\(x\\). Esto también es cierto para las variables dependientes rezagadas. El siguiente ejemplo demuestra que la introducción de una variable dependiente rezagada puede impactar drásticamente una relación de regresión transversal. Ejemplo: Tasas de Divorcio. La Figura 10.1 muestra las tasas de divorcio de 1965 versus los pagos de AFDC (Ayuda para Familias con Hijos Dependientes) para los cincuenta estados. En este ejemplo, cada estado representa una unidad de observación, la tasa de divorcio es la variable dependiente de interés y el nivel de pago de AFDC representa una variable que puede contribuir a nuestra comprensión de las tasas de divorcio. Código R para Generar Figuras 10.1 y 10.2 # Figuras 101 y 102 # Datos de divorcio disponibles en el libro &quot;Longitudinal and Panel Data&quot; # de Edward Frees. Ver el enlace: # https://instruction.bus.wisc.edu/jfrees/jfreesbooks/Longitudinal%20and%20Panel%20Data/Book/PDataBook.htm # &quot;\\t&quot; INDICA SEPARADO POR TABULACIONES ; divorce &lt;- read.table(&quot;https://instruction.bus.wisc.edu/jfrees/jfreesbooks/Longitudinal%20and%20Panel%20Data/Book/Data/TXTData/Divorce.txt&quot;, sep =&quot;\\t&quot;, quote = &quot;&quot;, header=TRUE) # FIGURA 10.1. GRÁFICA DE DATOS DE 1965 ; par(mfrow=c(1, 2), cex=1 ) cexlabel &lt;- 0.9 plot(DIVORCE ~ AFDC, subset=TIME %in% c(1), data = divorce, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;,ylab=&quot;&quot;,xlab=&quot;&quot;) axis(2, at=seq(0, 6, by=1), las=1, tck=0.01) axis(2, at=seq(0, 6, by=0.1), lab=F, tck=0.005) axis(1, at=seq(20,220, by=40), tck=0.01) axis(1, at=seq(20,220, by=2), lab=F, tck=0.005) mtext(&quot;DIVORCE&quot;, side=2, line=-1, at=6, las=1, cex=cexlabel ) mtext(&quot;AFDC&quot;, side=1, line=2, at=120, cex=cexlabel ) # Agregar leyenda debajo de la primera gráfica mtext(&quot;Fig 10.1. Gráfica de Divorcios de 1965 versus \\n Pagos de AFDC.&quot;, side = 1, line = 4, cex = 0.9, adj = .5) # FIGURA 10.2. GRÁFICA DE 1965 Y 1975, CONECTANDO LOS YEARS ; plot(DIVORCE ~ AFDC, data = subset(divorce, TIME %in% c(1, 2)), xaxt=&quot;n&quot;, yaxt=&quot;n&quot;,ylab=&quot;&quot;,xlab=&quot;&quot;) for (i in divorce$STATE) { lines(DIVORCE ~ AFDC, data = subset(divorce, TIME %in% c(1, 2) &amp; STATE == i)) } axis(2, at=seq(0, 10, by=1), las=1, tck=0.01) axis(2, at=seq(0, 10, by=0.1), lab=F, tck=0.005) axis(1, at=seq(0,400, by=100), tck=0.01) axis(1, at=seq(0,400, by=10), lab=F, tck=0.005) mtext(&quot;DIVORCE&quot;, side=2, line=-1, at=8.9, cex=cexlabel , las=1) mtext(&quot;AFDC&quot;, side=1, line=2, at=200, cex=cexlabel ) # Agregar leyenda debajo de la segunda gráfica mtext(&quot;Fig 10.2. Gráfica de Divorcios versus Pagos \\n de AFDC - 1965 y 1975&quot;, side = 1, line = 4, cex = 0.9, adj = 0.5) La Figura 10.1 muestra una relación negativa; el coeficiente de correlación correspondiente es -0.37. Algunos argumentan que esta relación negativa es contraintuitiva, ya que se esperaría una relación positiva entre los pagos de asistencia social (AFDC) y las tasas de divorcio; los estados con climas culturales deseables disfrutan tanto de bajas tasas de divorcio como de bajos pagos de asistencia social. Otros argumentan que esta relación negativa es intuitivamente plausible; los estados ricos pueden permitirse pagos altos de asistencia social y producen climas económicos y culturales propicios para bajas tasas de divorcio. Debido a que los datos son observacionales, no es apropiado argumentar una relación causal entre los pagos de asistencia social y las tasas de divorcio sin teoría económica o sociológica adicional. Otra gráfica, no mostrada aquí, presenta una relación negativa similar para 1975; la correlación correspondiente es -0.425. La Figura 10.2 muestra los datos de 1965 y 1975; una línea conecta las dos observaciones dentro de cada estado. La línea representa un cambio a lo largo del tiempo (dinámico), no una relación transversal. Cada línea muestra una relación positiva, es decir, a medida que aumentan los pagos de asistencia social, también lo hacen las tasas de divorcio para cada estado. Nuevamente, no inferimos direcciones de causalidad a partir de esta visualización. El punto es que la relación dinámica entre divorcio y pagos de asistencia social dentro de un estado difiere drásticamente de la relación transversal entre estados. Los modelos de datos longitudinales a veces se diferencian de la regresión y las series temporales a través de sus “subíndices dobles”. Usamos el subíndice \\(i\\) para denotar la unidad de observación, o sujeto, y \\(t\\) para denotar el tiempo. Con este propósito, definimos \\(y_{it}\\) como la variable dependiente para el \\(i\\)-ésimo sujeto durante el \\(t\\)-ésimo período de tiempo. Un conjunto de datos longitudinales consiste en observaciones del \\(i\\)-ésimo sujeto a lo largo de \\(t=1, \\ldots, T_i\\) períodos de tiempo, para cada uno de \\(i=1, \\ldots, n\\) sujetos. Por lo tanto, observamos: \\[ \\begin{array}{cc} \\textrm{primer sujeto} &amp; \\{y_{11}, \\ldots, y_{1T_1} \\} \\\\ \\textrm{segundo sujeto} &amp; \\{y_{21}, \\ldots, y_{2T_2} \\} \\\\ {\\vdots} &amp; {\\vdots} \\\\ n\\textrm{-ésimo sujeto} &amp; \\{y_{n1}, \\ldots, y_{nT_n} \\} \\\\ \\end{array} \\] En el ejemplo de divorcio, la mayoría de los estados tienen \\(T_i=2\\) observaciones y se representan gráficamente en la Figura 10.2 mediante una línea que conecta las dos observaciones. Algunos estados tienen solo \\(T_i=1\\) observación y se representan gráficamente mediante un círculo abierto. Para muchos conjuntos de datos, es útil permitir que el número de observaciones dependa del sujeto; \\(T_i\\) denota el número de observaciones para el \\(i\\)-ésimo sujeto. Esta situación se conoce como el caso de datos desbalanceados. En otros conjuntos de datos, cada sujeto tiene el mismo número de observaciones; esto se conoce como el caso de datos balanceados. Las aplicaciones que consideramos están basadas en muchas unidades transversales y solo unas pocas réplicas de series temporales. Es decir, consideramos aplicaciones donde \\(n\\) es grande en relación a \\(T = \\max (T_1, \\ldots, T_n)\\), el número máximo de períodos de tiempo. Los lectores ciertamente encontrarán aplicaciones importantes donde ocurre lo contrario, \\(T &gt; n\\), o donde \\(n \\approx T\\). 10.2 Visualización de Datos Longitudinales y de Panel Para explorar algunas formas de visualizar datos longitudinales, consideremos el siguiente ejemplo. Ejemplo: Costos Hospitalarios de Medicare. Consideramos \\(T=6\\) años, de 1990 a 1995, de datos sobre costos hospitalarios para pacientes internados cubiertos por el programa Medicare. Los datos fueron obtenidos de la Administración de Financiamiento de Cuidados de Salud. Por ejemplo, en 1995, los costos cubiertos totales fueron de $157.8 mil millones para doce millones de altas hospitalarias. Para este análisis, usamos el estado como sujeto o clase de riesgo. Aquí, consideramos \\(n=54\\) estados que incluyen los 50 estados de la Unión, el Distrito de Columbia, las Islas Vírgenes, Puerto Rico y una categoría no especificada como “otros”. La variable dependiente de interés es el componente de severidad, costos cubiertos por alta, que etiquetamos como CCPD. La variable CCPD es de interés para los actuarios porque el programa Medicare reembolsa a los hospitales por estancia. Asimismo, muchos planes de atención administrada reembolsan a los hospitales por estancia. Dado que CCPD varía según el estado y el tiempo, tanto el estado como el tiempo (YEAR=1, , 6) son variables explicativas potencialmente importantes. No asumimos a priori que la frecuencia es independiente de la severidad. Por lo tanto, el número de altas, NUM_DSCHG, es otra variable explicativa potencial. También investigamos la importancia de otro componente de utilización hospitalaria, AVE_DAYS, definido como la estancia hospitalaria promedio por alta en días. plot(1,2) Figura 10.1: silly #knitr::kable(2, caption = &quot;Silly. Create a table just to update the counter...&quot;) plot(1,2) Figura 10.2: silly #knitr::kable(2, caption = &quot;Silly. Create a table just to update the counter...&quot;) La Figura 10.3 ilustra la gráfica de series temporales múltiples. Aquí, vemos que no solo los costos cubiertos totales aumentan, sino que también aumentan para cada estado. También son evidentes diferentes niveles de costos hospitalarios entre estados; llamamos a esta característica heterogeneidad. La Figura 10.3 indica que hay mayor variabilidad entre estados que a lo largo del tiempo. Figura 10.3: Gráfica de Series Temporales Múltiples de CCPD. Los costos cubiertos por alta (CCPD) están graficados a lo largo de \\(T=6\\) años, de 1990 a 1995. Los segmentos de línea conectan estados; así, vemos que el CCPD aumenta para casi todos los estados con el tiempo. Código R para Generar la Figura 10.3 # Figura 103 # Datos de Medicare disponibles en el libro &quot;Longitudinal and Panel Data&quot; # de Edward Frees. Ver el enlace: # https://instruction.bus.wisc.edu/jfrees/jfreesbooks/Longitudinal%20and%20Panel%20Data/Book/PDataBook.htm # &quot;\\t&quot; INDICA SEPARADO POR TABULACIONES ; Medicare &lt;- read.table(&quot;https://instruction.bus.wisc.edu/jfrees/jfreesbooks/Longitudinal%20and%20Panel%20Data/Book/Data/TXTData/Medicare.txt&quot;, sep =&quot;\\t&quot;, quote = &quot;&quot;, header=TRUE) # CREAR OTRAS VARIABLES; Medicare$AVE_DAYS= Medicare$TOT_D/Medicare$NUM_DCHG Medicare$CCPD=Medicare$COV_CHG/Medicare$NUM_DCHG Medicare$NUM_DCHG=Medicare$NUM_DCHG/1000 Medicare$YEAR1 = 1989+Medicare$YEAR # FIGURA 10.3: CCPD vs YEAR; GRÁFICA DE SERIES TEMPORALES MÚLTIPLES; par(cex=0.9) plot(CCPD ~ YEAR1, data = Medicare, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab=&quot;&quot;) for (i in Medicare$STATE) { lines(CCPD ~ YEAR1, data = subset(Medicare, STATE == i)) } axis(2, at=seq(0, 22000, by=4000), las=1, tck=0.01) axis(1, at=seq(1990,1995, by=1), tck=0.01) mtext(&quot;CCPD&quot;, side=2, line=0, at=23500, cex=1, las=1) mtext(&quot;YEAR&quot;, side=1, line=3, at=3.5, cex=1) La Figura 10.4 es una variación de una gráfica de dispersión con símbolos. Esta es una gráfica de CCPD contra el número de altas. En lugar de usar diferentes símbolos para cada estado, conectamos observaciones dentro de un estado a lo largo del tiempo. Esta gráfica muestra una relación positiva general entre CCPD y el número de altas. Al igual que CCPD, vemos una variación sustancial entre estados en los diferentes números de altas. También como CCPD, el número de altas aumenta con el tiempo, de modo que, para cada estado, hay una relación positiva entre CCPD y el número de altas. La pendiente es mayor para aquellos estados con un menor número de altas. Esta gráfica también sugiere que el número de altas con un rezago de un año es un predictor importante de CCPD. Figura 10.4: Gráfica de Dispersión de CCPD contra Número de Altas. Los segmentos de línea conectan observaciones dentro de un estado durante 1990-1995. Vemos una variación sustancial entre estados en el número de altas. Existe una relación positiva entre CCPD y el número de altas para cada estado. Las pendientes son mayores para aquellos estados con un menor número de altas. Código R para Generar la Figura 10.4 # Figura 104 plot(CCPD ~ NUM_DCHG, data = Medicare, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab=&quot;&quot;) for (i in Medicare$STATE) { lines(CCPD ~ NUM_DCHG, data = subset(Medicare, STATE == i)) } axis(2, at=seq(0, 22000, by=4000), las=1, font=10, cex=0.005, tck=0.01) axis(2, at=seq(0, 22000, by=200), lab=F, tck=0.005) axis(1, at=seq(0,1200, by=200), font=10, cex=0.005, tck=0.01) axis(1, at=seq(0,1200, by=20), lab=F, tck=0.005) mtext(&quot;CCPD&quot;, side=2, line=0, at=23000, cex=1, las=1) mtext(&quot;Número de Altas en Miles&quot;, side=1, line=3, at=500, cex=1) Gráfica Trellis Una técnica para la visualización gráfica que ha ganado popularidad recientemente en la literatura estadística es la gráfica Trellis. Esta técnica gráfica toma su nombre de un “trellis”, que es una estructura de celosía abierta. Al observar una casa o jardín, generalmente se piensa en un trellis como un soporte para plantas trepadoras como enredaderas. Usamos esta estructura de celosía y nos referimos a una gráfica Trellis como un conjunto de uno o más paneles organizados en una matriz rectangular. Las gráficas que contienen múltiples versiones de una forma gráfica básica, cada versión representando una variación del tema básico, promueven comparaciones y evaluaciones de cambio. Al repetir una forma gráfica básica, promovemos el proceso de comunicación. Tufte (1997) afirma que el uso de pequeños múltiples en las representaciones gráficas logra los mismos efectos deseables que el uso de estructuras paralelas en la escritura. La estructura paralela en la escritura es exitosa porque permite a los lectores identificar una relación de oración una sola vez y luego enfocarse en el significado de cada elemento individual de la oración, como una palabra, frase o cláusula. La estructura paralela ayuda a lograr economía de expresión y a reunir ideas relacionadas para comparación y contraste. De manera similar, los pequeños múltiples en las gráficas nos permiten visualizar relaciones complejas entre diferentes grupos y a lo largo del tiempo. Consulte la Guía Cinco en la Sección 21.3 para una discusión más detallada. La Figura 10.5 ilustra el uso de pequeños múltiples. En cada panel, la gráfica representada es idéntica excepto que se basa en un estado diferente; este uso de estructura paralela nos permite demostrar el aumento en los costos cubiertos por alta (CCPD) para cada estado. Además, al organizar los estados por CCPD promedio, podemos observar el nivel general de CCPD para cada estado, así como las variaciones en la pendiente (tasa de aumento). Figura 10.5: Gráfica Trellis de CCPD versus Año. Cada uno de los 54 paneles representa una gráfica de CCPD versus YEAR, 1990-1995 (el eje horizontal está suprimido). El aumento para Nueva Jersey (NJ) es inusualmente grande. Código R para Generar la Figura 10.5 # Figura 105 # # &quot;\\t&quot; INDICA SEPARADO POR TABULACIONES ; # Medicare &lt;- read.table(&quot;https://instruction.bus.wisc.edu/jfrees/jfreesbooks/Longitudinal%20and%20Panel%20Data/Book/Data/TXTData/Medicare.txt&quot;, sep =&quot;\\t&quot;, quote = &quot;&quot;, header=TRUE) # CREAR UN NUEVO CONJUNTO DE DATOS, ELIMINANDO EL VALOR ATÍPICO # EXCLUYENDO LA 2DA OBSERVACIÓN DEL 54TO ESTADO; Medicare2 &lt;- subset(Medicare, STATE != 54 | YEAR != 2) # FIGURA 10.5: GRÁFICA TRELLIS library(nlme) #trellis.device(color=F) # indicando al dispositivo trellis que imite &quot;blanco y negro&quot; GrpMedicare = groupedData(CCPD ~ YEAR1| NMSTATE, data=Medicare2) plot(GrpMedicare, xlab=&quot;YEAR&quot;, ylab=&quot;CCPD&quot;, scale = list(x=list(draw=FALSE)), layout=c(18,3), par.strip.text = list(cex=0.75), grid=FALSE) 10.3 Modelos Básicos de Efectos Fijos Datos Como se describe en la Sección 10.1, denotamos \\(y_{it}\\) como la variable dependiente del \\(i\\)-ésimo sujeto en el \\(t\\)-ésimo punto temporal. Asociado con cada variable dependiente hay un conjunto de variables explicativas. Para el ejemplo de costos hospitalarios por estado, estas variables explicativas incluyen el número de pacientes dados de alta y la estancia hospitalaria promedio por alta. En general, asumimos que hay \\(k\\) variables explicativas \\(x_{it,1}, x_{it,2}, \\ldots, x_{it,k}\\) que pueden variar según el sujeto \\(i\\) y el tiempo \\(t\\). Logramos una forma notacional más compacta expresando las \\(k\\) variables explicativas como un vector columna \\(k \\times 1\\): \\[ \\mathbf{x}_{it} = \\left(\\begin{array}{c} x_{it,1} \\\\ x_{it,2} \\\\ \\vdots \\\\ x_{it,k} \\end{array}\\right) . \\] Con esta notación, los datos para el \\(i\\)-ésimo sujeto consisten en: \\[ \\left(\\begin{array}{c} x_{i1,1}, x_{i1,2}, \\ldots, x_{i1,k}, y_{i1} \\\\ \\vdots \\\\ x_{iT_i,1}, x_{iT_i,2}, \\ldots, x_{iT_i,k}, y_{iT_i} \\\\ \\end{array}\\right) = \\left(\\begin{array}{c} \\mathbf{x}_{i1}^{\\prime}, y_{i1} \\\\ \\vdots \\\\ \\mathbf{x}_{iT_i}^{\\prime}, y_{iT_i} \\\\ \\end{array}\\right) . \\] Modelo Un modelo básico (y muy útil) de datos longitudinales es un caso especial del modelo de regresión lineal múltiple introducido en la Sección 3.2. Utilizamos las suposiciones de modelado de la Sección 3.2.3 con la función de regresión \\[\\begin{eqnarray} \\mathrm{E}~y_{it} &amp; = &amp; \\alpha_i + \\beta_1 x_{it,1} + \\beta_2 x_{it,2} + \\cdots + \\beta_k x_{it,k} \\nonumber\\\\ &amp; = &amp; \\alpha_i + \\mathbf{x}_{it}^{\\prime} \\boldsymbol \\beta,~~~~~~ t=1, \\ldots, T_i,~~ i=1, \\ldots, n . \\tag{10.1} \\end{eqnarray}\\] Este es el modelo básico de efectos fijos. Los parámetros \\(\\{\\beta_j\\}\\) son comunes a cada sujeto y se denominan parámetros globales, o de población. Los parámetros \\(\\{\\alpha_i\\}\\) varían por sujeto y se conocen como parámetros individuales, o específicos del sujeto. En muchas aplicaciones, los parámetros de población capturan relaciones generales de interés y, por lo tanto, son los parámetros de interés. Los parámetros específicos del sujeto explican las características diferentes de los sujetos, no los patrones generales de la población. Por lo tanto, a menudo son de interés secundario y se denominan parámetros de nuisance. En la Sección 10.5, discutiremos el caso donde \\(\\{\\alpha_i\\}\\) son variables aleatorias. Para distinguir este caso, esta sección trata a \\(\\{\\alpha_i\\}\\) como parámetros no estocásticos denominados “efectos fijos”. Los parámetros específicos del sujeto ayudan a controlar las diferencias, o “heterogeneidad” entre los sujetos. Los estimadores de estos parámetros utilizan información en las mediciones repetidas sobre un sujeto. Por el contrario, los parámetros \\(\\{\\alpha_i\\}\\) no son estimables en modelos de regresión transversal sin observaciones repetidas. Es decir, con \\(T_i = 1\\), el modelo \\(y_{it} = \\alpha_i + \\beta_1 x_{it,1}\\) \\(+ \\beta_2 x_{it,2} + \\cdots + \\beta_k x_{it,k}\\) \\(+ \\varepsilon_{it}\\) tiene más parámetros (\\(n+k\\)) que observaciones (\\(n\\)), y, por lo tanto, no podemos identificar todos los parámetros. Normalmente, el término de perturbación \\(\\varepsilon_{it}\\) incluye la información en \\(\\alpha_i\\) en los modelos de regresión transversal. Una ventaja importante de los modelos de datos longitudinales en comparación con los modelos de regresión transversal es la capacidad de separar los efectos de \\(\\{\\alpha_i\\}\\) de los términos de perturbación \\(\\{\\varepsilon_{it}\\}\\). Al separar los efectos específicos del sujeto, nuestras estimaciones de la variabilidad se vuelven más precisas y logramos inferencias más exactas. Estimación La estimación del modelo básico de efectos fijos se deriva directamente de los métodos de mínimos cuadrados. La idea clave es que los parámetros de heterogeneidad \\(\\{\\alpha_i\\}\\) simplemente representan un factor, es decir, una variable categórica que describe la unidad de observación. Con esto, la estimación por mínimos cuadrados sigue directamente con los detalles proporcionados en la Sección 4.4 y los apéndices de apoyo. Como se describe en el Capítulo 4, se pueden reemplazar las variables categóricas con un conjunto apropiado de variables binarias. Por esta razón, los estimadores de datos de panel a veces se conocen como estimadores del modelo de mínimos cuadrados con variables ficticias. Sin embargo, como hemos visto en el Capítulo 4, se debe tener cuidado con las rutinas estadísticas. Para algunas aplicaciones, el número de sujetos puede fácilmente llegar a miles. Crear tantas variables binarias es computacionalmente engorroso. Cuando identifica una variable como categórica, los paquetes estadísticos generalmente utilizan procedimientos recursivos más eficientes computacionalmente (descritos en la Sección 4.7.2). El factor de heterogeneidad \\(\\{\\alpha_i\\}\\) no depende del tiempo. Debido a esto, es fácil establecer que los coeficientes de regresión asociados con variables constantes en el tiempo no se pueden estimar utilizando el modelo básico de efectos fijos. En otras palabras, las variables constantes en el tiempo son perfectamente colineales con el factor de heterogeneidad. Debido a esta limitación, los analistas a menudo prefieren diseñar sus estudios para utilizar el modelo alternativo de efectos aleatorios que describiremos en la Sección 10.5. Ejemplo: Costos Hospitalarios de Medicare - Continuación. Comparamos el ajuste del modelo básico de efectos fijos con los modelos de regresión ordinaria. El Modelo 1 de Tabla 10.1 muestra el ajuste de un modelo de regresión ordinaria utilizando el número de altas (NUM_DCHG), YEAR y la estancia hospitalaria promedio (AVE_DAYS). Según los grandes valores \\(t\\)-estadísticos, cada variable es estadísticamente significativa. El término de intersección no se imprime. La Figura 10.5 sugiere que New Jersey tiene un aumento inusualmente grande. Por lo tanto, se creó un término de interacción, YEARNJ, que equivale a YEAR si la observación es de New Jersey y cero en caso contrario. Esta variable se incorpora en el Modelo 2, donde no parece ser significativa. Tabla 10.1 también muestra el ajuste de un modelo básico de efectos fijos con estas variables explicativas. En la tabla, no se informan los 54 coeficientes específicos de sujeto. En este modelo, cada variable es estadísticamente significativa, incluido el término de interacción. Lo más notable es la mejora en el ajuste general. La desviación estándar residual (\\(s\\)) disminuyó de 2,731 a 530 y el coeficiente de determinación (\\(R^2\\)) aumentó del 29% al 99.8%. Tabla 10.1. Coeficientes y Estadísticas Resumen de Tres Modelos \\[ \\small{ \\begin{array} {lrrrrrr} \\hline &amp;\\text{Regresión}&amp;&amp;\\text{Regresión}&amp;&amp;\\text{Modelo Básico de} \\\\ &amp;\\text{Modelo 1} &amp;&amp;\\text{Modelo 2} &amp; &amp;\\text{Efectos Fijos} \\\\ &amp; \\text{Coeficiente} &amp; t\\text{-estad} &amp;\\text{Coeficiente} &amp; t\\text{-estad} &amp;\\text{Coeficiente} &amp; t\\text{-estad} \\\\ \\hline \\text{NUM_DCHG} &amp; 4.70 &amp; 6.49 &amp; 4.66 &amp; 6.44 &amp; 10.75 &amp; 4.18 \\\\ \\text{YEAR} &amp; 744.15 &amp; 7.96 &amp; 733.27 &amp; 7.79 &amp; 710.88 &amp; 26.51 \\\\ \\text{AVE_DAYS} &amp; 325.16 &amp; 3.85 &amp; 308.47 &amp; 3.58 &amp; 361.29 &amp; 6.23 \\\\ \\text{YEARNJ} &amp; &amp; &amp; 299.93 &amp; 1.01 &amp; 1,262.46 &amp; 9.82 \\\\ \\hline s &amp; {2,731.90} &amp; &amp; {2,731.78} &amp; &amp; {529.45} \\\\ R^2 \\text{ (en porcentaje)} &amp; {28.6} &amp; &amp;{28.8} &amp;&amp; {99.8} \\\\ R_a^2 \\text{ (en porcentaje)} &amp; {27.9} &amp;&amp; {27.9} &amp;&amp; {99.8} \\\\ \\hline \\end{array} } \\] R Code to Produce Table 10.1 # Tabla 10.1 # &quot;\\t&quot; INDICA SEPARADO POR TABLAS ; #Medicare &lt;- read.table(&quot;https://instruction.bus.wisc.edu/jfrees/jfreesbooks/Longitudinal%20and%20Panel%20Data/Book/Data/TXTData/Medicare.txt&quot;, sep =&quot;\\t&quot;, quote = &quot;&quot;, header=TRUE) # CREAR OTRAS VARIABLES; Medicare$AVE_DAYS &lt;- Medicare$TOT_D/Medicare$NUM_DCHG Medicare$CCPD &lt;- Medicare$COV_CHG/Medicare$NUM_DCHG Medicare$NUM_DCHG1 &lt;- Medicare$NUM_DCHG/1000 Medicare$YEAR1 &lt;- 1989 + Medicare$YEAR # CREAR UN NUEVO CONJUNTO DE DATOS, REMOVIENDO EL ATÍPICO # EXCLUYENDO LA 2DA OBSERVACIÓN DEL 54TO ESTADO; Medicare2 &lt;- subset(Medicare, STATE != 54 | YEAR != 2) # CREAR UNA NUEVA VARIABLE; # LA NUEVA VARIABLE YR31 SE UTILIZARÁ EN EL MODELO FINAL PARA DAR AL ESTADO 31 UNA PENDIENTE ESPECÍFICA; Medicare2$FSTATE &lt;- factor(Medicare2$NMSTATE) Medicare2$YEARNJ &lt;- (Medicare2$STATE==31)*Medicare2$YEAR # Modelo de Regresión 1; Model.1 &lt;- lm(CCPD ~ NUM_DCHG1 + YEAR + AVE_DAYS , data=Medicare2) Model.1.sum &lt;- summary(Model.1) # Modelo de Regresión 2; Model.2 &lt;- lm(CCPD ~ NUM_DCHG1 + YEAR + AVE_DAYS + YEARNJ , data=Medicare2) Model.2.sum &lt;- summary(Model.2) # Modelo Básico de Efectos Fijos Model.3 &lt;- lm(CCPD ~ NUM_DCHG1 + YEAR + AVE_DAYS + YEARNJ + FSTATE - 1, data=Medicare2) Model.3.sum &lt;- summary(Model.3) #anova(Model.2, Model.3) # SECCIÓN 10.3 beta1 &lt;- Model.1$coefficients tstats1 &lt;- beta1/(Model.1.sum$sigma*sqrt(diag(Model.1.sum$cov.unscaled))) beta2 &lt;- Model.2$coefficients tstats2 &lt;- beta2/(Model.2.sum$sigma*sqrt(diag(Model.2.sum$cov.unscaled))) beta3 &lt;- Model.3$coefficients tstats3 &lt;- beta3/(Model.3.sum$sigma*sqrt(diag(Model.3.sum$cov.unscaled))) # Tabla 10.1 - Estadísticas resumen de tres modelos Tab101.A &lt;- cbind(c(beta1[2:4], NA), c(tstats1[2:4], NA), beta2[2:5], tstats2[2:5], beta3[1:4], tstats3[1:4]) knitr::kable(Tab101.A, digits = 2) NUM_DCHG1 4696265.11 6.49 4660581.38 6.44 10754717.33 4.18 YEAR 744152.54 7.96 733274.95 7.79 710884.20 26.51 AVE_DAYS 325.16 3.85 308.47 3.58 361.29 6.23 NA NA 299927.00 1.01 1262456.08 9.82 sigma &lt;- cbind(Model.1.sum$sigma, Model.2.sum$sigma,Model.3.sum$sigma) r.squared &lt;- 100*cbind(Model.1.sum$r.squared, Model.2.sum$r.squared, Model.3.sum$r.squared) adj.r.squared &lt;- 100*cbind(Model.1.sum$adj.r.squared, Model.2.sum$adj.r.squared, Model.3.sum$adj.r.squared) Tab101.B &lt;- rbind(sigma, r.squared, adj.r.squared) knitr::kable(Tab101.B, digits = 2) 2731900.53 2731781.24 529451.05 28.56 28.79 99.81 27.89 27.90 99.77 10.4 Modelos Extendidos de Efectos Fijos Modelos de Análisis de Covarianza En el modelo básico de efectos fijos, no se asumen relaciones especiales entre sujetos y períodos de tiempo. Intercambiando los roles de “\\(i\\)” y “\\(t\\)”, podemos considerar la función de regresión \\[ \\mathrm{E}~y_{it} = \\lambda_t + \\mathbf{x}_{it}^{\\prime} \\boldsymbol \\beta. \\] Tanto esta función de regresión como la de la ecuación (10.1) se basan en los modelos tradicionales de análisis de covarianza de una vía introducidos en la Sección 4.4. Por esta razón, el modelo básico de efectos fijos también se llama el modelo de efectos fijos de una vía. Usando variables binarias (dummy) para la dimensión temporal, podemos incorporar parámetros específicos del tiempo en los parámetros de población. De esta forma, es sencillo considerar la función de regresión \\[ \\mathrm{E}~y_{it} = \\alpha_i + \\lambda_t + \\mathbf{x}_{it}^{\\prime} \\boldsymbol \\beta , \\] conocida como el modelo de efectos fijos de dos vías. Ejemplo: Salarios Urbanos. Glaeser y Maré (2001) investigaron los efectos de los determinantes sobre los salarios, con el objetivo de comprender por qué los trabajadores en ciudades ganan más que sus contrapartes no urbanas. Examinaron modelos de efectos fijos de dos vías utilizando datos de la Encuesta Nacional Longitudinal de la Juventud (NLSY); también usaron datos del Estudio Panel de Dinámica de Ingresos (PSID) para evaluar la robustez de sus resultados en otra muestra. Para los datos de la NLSY, examinaron \\(n = 5,405\\) hombres jefes de familia durante los años 1983-1993, consistiendo en un total de \\(N = 40,194\\) observaciones. La variable dependiente fue el salario horario logarítmico. La principal variable explicativa de interés fue una variable categórica de tres niveles que mide el tamaño de la ciudad en la que residen los trabajadores. Para capturar esta variable, se usaron dos variables binarias (dummy): (1) una variable para indicar si el trabajador reside en una gran ciudad (con más de medio millón de residentes), un “área metropolitana densa”, y (2) una variable para indicar si el trabajador reside en un área metropolitana que no contiene una gran ciudad, un “área metropolitana no densa”. El nivel de referencia es un área no metropolitana. Se incluyeron varias otras variables de control para capturar los efectos de la experiencia, ocupación, educación y raza del trabajador. Al incluir variables dummy temporales, hubo \\(k = 30\\) variables explicativas en las regresiones reportadas. Modelos de Coeficientes Variables En el ejemplo de costos hospitalarios de Medicare, introdujimos una variable de interacción para representar los aumentos inusualmente altos en los costos de Nueva Jersey. Sin embargo, un examen de la Figura 10.5 sugiere que muchos otros estados también son “inusuales”. Siguiendo esta línea de pensamiento, podríamos desear permitir que cada estado tenga su propia tasa de incremento, correspondiente a los aumentos en los costos hospitalarios para ese estado. Podríamos considerar una función de regresión de la forma \\[\\begin{equation} \\small{ \\mathrm{E}~CCPD_{it} = \\alpha_i + \\beta_1 (NUM\\_DCHG)_{it} + \\beta_{2i} (YEAR)_{t} + \\beta_3 (AVE\\_DAYS)_{it} , } \\tag{10.2} \\end{equation}\\] donde la pendiente asociada con YEAR se permite que varíe con el estado “\\(i\\)”. Siguiendo esta línea de pensamiento, escribimos la función de regresión para un modelo de coeficientes variables de efectos fijos como \\[ \\mathrm{E}~y_{it} = \\mathbf{x}_{it}^{\\prime} \\boldsymbol \\beta_i. \\] Con esta notación, podemos permitir que cualquiera o todas las variables estén asociadas con coeficientes específicos del sujeto. Para simplicidad, el intercepto específico del sujeto ahora se incluye en el vector de coeficientes de regresión \\(\\boldsymbol \\beta_i\\). Ejemplo: Costos Hospitalarios de Medicare - Continuación. La función de regresión en la ecuación (10.2) se ajustó a los datos. No es sorprendente que haya resultado en un excelente ajuste en el sentido de que el coeficiente de determinación sea \\(R^2 = 99.915 \\%\\) y la versión ajustada sea \\(R_a^2 = 99.987 \\%\\). Sin embargo, comparado con el modelo básico de efectos fijos, hay 52 parámetros adicionales, una pendiente para cada estado (54 estados inicialmente, menos uno para el término de ‘población’ y menos uno para Nueva Jersey ya incluido). ¿Son útiles los términos adicionales? Una forma de analizar esto es mediante la prueba de hipótesis lineal general introducida en la Sección 4.2.2. En este contexto, el modelo de coeficientes variables representa la ecuación “completa” y el modelo básico de efectos fijos es nuestra ecuación “reducida”. De la ecuación (4.4), el estadístico de prueba es \\[ F-\\textrm{ratio} = \\frac {(0.99915 - 0.99809)/52}{(1-0.99915)/213} = 5.11 . \\] Comparando esto con la distribución \\(F\\) con \\(df_1 = 52\\) y \\(df_2 = 213\\), vemos que el \\(p\\)-valor asociado es menor a 0.0001, indicando una fuerte significancia estadística. Por lo tanto, esta es una indicación de que el modelo de pendiente variable es preferido en comparación con el modelo básico de efectos fijos. Modelos con Correlación Serial En datos longitudinales, los sujetos se miden repetidamente a lo largo del tiempo. Para algunas aplicaciones, las tendencias temporales representan una porción menor de la variación general. En estos casos, se puede ajustar su presencia calculando errores estándar de los coeficientes de regresión de manera robusta, similar a la discusión en la Sección 5.7.2. Sin embargo, para otras aplicaciones, entender bien las tendencias temporales es vital. Una de estas aplicaciones, importante en la ciencia actuarial, es la predicción; por ejemplo, recuerde la discusión en la Sección 10.1 sobre un actuario que predice reclamaciones de seguros para una pequeña empresa. Hemos visto en los Capítulos 7–9 algunas formas básicas de incorporar tendencias temporales, a través de tendencias lineales en el tiempo (como el término YEAR en el ejemplo de costos hospitalarios de Medicare) o utilizando variables dummy en el tiempo (otro tipo de modelo de efectos fijos de una vía). Otra posibilidad es usar una variable dependiente rezagada como predictor. Sin embargo, se sabe que esto tiene consecuencias negativas inesperadas para el modelo básico de efectos fijos (véase, por ejemplo, la discusión en Hsiao, 2003, Sección 4.2, o Frees, 2004, Sección 6.3). En su lugar, es habitual examinar la estructura de correlación serial del término de perturbación \\(\\varepsilon_{it} = y_{it} - \\mathrm{E}~y_{it}\\). Por ejemplo, una especificación común es usar una autocorrelación de orden uno, estructura \\(AR(1)\\), como \\[ \\varepsilon_{it} = \\rho_{\\varepsilon} \\varepsilon_{i,t-1} + \\eta_{it}, \\] donde \\(\\{ \\eta_{it} \\}\\) es un conjunto de variables aleatorias de perturbación y \\(\\rho_{\\varepsilon}\\) es el parámetro de autocorrelación. En muchos conjuntos de datos longitudinales, el pequeño número de mediciones en el tiempo (\\(T\\)) dificultaría el cálculo del coeficiente de correlación \\(\\rho_{\\varepsilon}\\) utilizando métodos tradicionales como los introducidos en el Capítulo 8. Sin embargo, con datos longitudinales, tenemos muchas replicaciones (\\(n\\)) de estas series de tiempo cortas; intuitivamente, estas replicaciones proporcionan la información necesaria para estimar de manera confiable el parámetro autorregresivo. 10.5 Modelos de Efectos Aleatorios Supongamos que se está interesado en estudiar el comportamiento de sujetos seleccionados aleatoriamente de una población. Por ejemplo, podría desear predecir reclamaciones de seguros para una pequeña empresa utilizando características de la empresa, así como el historial de reclamaciones pasado. Aquí, el conjunto de pequeñas empresas puede seleccionarse aleatoriamente de una base de datos más grande. En contraste, el ejemplo de Medicare en la Sección 10.3 trató con un conjunto fijo de sujetos. Es decir, es difícil pensar en los 54 estados como un subconjunto de alguna “superpoblación” de estados. Para ambas situaciones, es natural usar parámetros específicos del sujeto, \\(\\{\\alpha_i \\}\\), para representar la heterogeneidad entre los sujetos. A diferencia de la Sección 10.3, ahora discutimos situaciones en las que es más razonable representar \\(\\{\\alpha_i \\}\\) como variables aleatorias en lugar de parámetros fijos pero desconocidos. Al argumentar que \\(\\{\\alpha_i \\}\\) son muestras de una distribución, tendremos la capacidad de hacer inferencias sobre sujetos en una población que no están incluidos en la muestra. Modelo Básico de Efectos Aleatorios La ecuación del modelo básico de efectos aleatorios es \\[\\begin{equation} y_{it} = \\alpha_i + \\mathbf{x}_{it}^{\\prime} \\boldsymbol \\beta + \\varepsilon_{it},~~~~~~ t=1, \\ldots, T_i,~~ i=1, \\ldots, n . \\tag{10.3} \\end{equation}\\] Esta notación es similar al modelo básico de efectos fijos. Sin embargo, ahora el término \\(\\alpha_i\\) se asume como una variable aleatoria, no como un parámetro fijo desconocido. El término \\(\\alpha_i\\) se conoce como un efecto aleatorio. Los modelos de efectos mixtos son aquellos que incluyen efectos aleatorios y fijos. Debido a que la ecuación (10.3) incluye efectos aleatorios (\\(\\alpha_i\\)) y efectos fijos (\\(\\mathbf{x}_{it}\\)), el modelo básico de efectos aleatorios es un caso especial del modelo lineal mixto. El modelo lineal mixto general se introduce en la Sección 15.1. Para completar la especificación, asumimos que \\(\\{\\alpha_i \\}\\) son idéntica e independientemente distribuidos con media cero y varianza \\(\\sigma_{\\alpha}^2\\). Además, asumimos que \\(\\{\\alpha_i \\}\\) son independientes de las variables aleatorias de perturbación, \\(\\varepsilon_{it}\\). Note que debido a que \\(\\mathrm{E} ~\\alpha_i = 0\\), es habitual incluir una constante dentro del vector \\(\\mathbf{x}_{it}\\). Esto no era cierto en los modelos de efectos fijos en la Sección 10.3 donde no centramos los términos específicos del sujeto alrededor de 0. Las combinaciones lineales de la forma \\(\\mathbf{x}_{it}^{\\prime} \\boldsymbol \\beta\\) cuantifican el efecto de variables conocidas que pueden afectar la variable dependiente. Las variables adicionales, que son poco importantes o inobservables, forman parte del “término de error”. En la ecuación (10.3), podemos pensar en un modelo de regresión \\(y_{it} = \\mathbf{x}_{it}^{\\prime} \\boldsymbol \\beta + \\eta_{it},\\) donde el término de error \\(\\eta_{it}\\) se descompone en dos componentes para que \\(\\eta_{it}= \\alpha_i + \\varepsilon_{it}\\). El término \\(\\alpha_i\\) representa la porción constante en el tiempo, mientras que \\(\\varepsilon_{it}\\) representa la porción restante. Para identificar los parámetros del modelo, asumimos que los dos términos son independientes. En la literatura econométrica, esto se conoce como el modelo de componentes del error; en las ciencias biológicas, se conoce como el modelo de interceptos aleatorios. Estimación La estimación del modelo de efectos aleatorios no sigue directamente los métodos de mínimos cuadrados como en los modelos de efectos fijos. Esto se debe a que las observaciones ya no son independientes debido a los términos de efectos aleatorios. En su lugar, se utiliza una extensión de los mínimos cuadrados conocida como mínimos cuadrados generalizados para tener en cuenta esta dependencia. Los mínimos cuadrados generalizados, a menudo denominados por el acrónimo GLS, son un tipo de mínimos cuadrados ponderados. Dado que los modelos de efectos aleatorios son casos especiales de los modelos lineales mixtos, introduciremos la estimación GLS en este marco más amplio en la Sección 15.1. Para ver la dependencia entre observaciones, consideremos la covarianza entre las dos primeras observaciones del \\(i\\)-ésimo sujeto. Los cálculos básicos muestran: \\[\\begin{eqnarray*} \\mathrm{Cov}(y_{i1}, y_{i2}) &amp;= &amp;\\mathrm{Cov}(\\alpha_i + \\mathbf{x}_{i1}^{\\prime} \\boldsymbol \\beta + \\varepsilon_{i1}, \\alpha_i + \\mathbf{x}_{i2}^{\\prime} \\boldsymbol \\beta + \\varepsilon_{i2}) \\\\ &amp;= &amp;\\mathrm{Cov}(\\alpha_i +\\varepsilon_{i1}, \\alpha_i + \\varepsilon_{i2}) \\\\ &amp;= &amp;\\mathrm{Cov}(\\alpha_i , \\alpha_i)+\\mathrm{Cov}(\\alpha_i, \\varepsilon_{i2})+\\mathrm{Cov}(\\varepsilon_{i1}, \\alpha_i)+\\mathrm{Cov}(\\varepsilon_{i1}, \\varepsilon_{i2}) \\\\ &amp;= &amp; \\mathrm{Cov}(\\alpha_i , \\alpha_i) = \\sigma^2_{\\alpha} . \\end{eqnarray*}\\] Los términos sistemáticos \\(\\mathbf{x}^{\\prime} \\boldsymbol \\beta\\) desaparecen del cálculo de covarianza porque no son aleatorios. Además, los términos de covarianza que involucran \\(\\varepsilon\\) son cero debido a la independencia asumida. Este cálculo muestra que la covarianza entre dos observaciones del mismo sujeto es \\(\\sigma^2_{\\alpha}\\). Cálculos similares muestran que la varianza de una observación es \\(\\sigma^2_{\\alpha} +\\sigma^2_{\\varepsilon}.\\) Por lo tanto, la correlación entre observaciones dentro de un sujeto es \\(\\sigma^2_{\\alpha} / (\\sigma^2_{\\alpha} +\\sigma^2_{\\varepsilon})\\). Esta cantidad se conoce como la correlación intraclase, una medida comúnmente reportada de dependencia en estudios de efectos aleatorios. Ejemplo: Seguro de Vida Colectivo. Frees, Young y Luo (2001) analizaron datos de reclamaciones proporcionados por un asegurador de cooperativas de crédito. Los datos contienen información sobre reclamaciones y exposición de 88 cooperativas de crédito en Florida para los años 1993-1996. Estas son reclamaciones de “ahorros de vida” de un contrato entre la cooperativa de crédito y sus miembros que proporciona un beneficio por fallecimiento basado en los ahorros depositados en la cooperativa de crédito. Los actuarios típicamente calculan el precio del seguro de vida utilizando la edad y género del asegurado, así como otras variables explicativas como la ocupación. Sin embargo, para estos datos de grupos pequeños, a menudo solo está disponible una cantidad mínima de información para comprender el comportamiento de las reclamaciones. De las \\(88 \\times 4=352\\) observaciones potenciales, 27 no estuvieron disponibles porque estas cooperativas de crédito no tuvieron cobertura en ese año (y, por lo tanto, fueron excluidas). Así, estos datos estaban desbalanceados. La variable dependiente es el total anual de reclamaciones del contrato de ahorros de vida, en unidades logarítmicas. Las variables explicativas fueron la cobertura anual, en unidades logarítmicas, y YEAR, una tendencia temporal. Un ajuste del modelo básico de efectos aleatorios mostró que tanto YEAR como la cobertura anual tenían coeficientes positivos y fuertemente significativos desde el punto de vista estadístico. Es decir, la cantidad típica de reclamaciones aumentó durante el período estudiado y las reclamaciones aumentaron a medida que aumentaba la cobertura, manteniendo todo lo demás constante. También hubo fuertes efectos de las cooperativas de crédito. Por ejemplo, la correlación intraclase estimada fue de 0.703, lo que también sugiere una fuerte dependencia entre las observaciones. Modelos Extendidos de Efectos Aleatorios Al igual que con los efectos fijos, los modelos de efectos aleatorios pueden extenderse fácilmente para incorporar coeficientes variables y correlaciones seriales. Por ejemplo, Frees et al. (2001) consideraron la ecuación del modelo \\[\\begin{equation} y_{it} = \\alpha_{1i} + \\alpha_{2i} \\mathrm{LNCoverage}_{it}+ \\beta_1 + \\beta_2 \\mathrm{YEAR}_t+ \\beta_3 \\mathrm{LNCoverage}_{it}+ \\varepsilon_{it}, \\tag{10.4} \\end{equation}\\] donde \\(\\mathrm{LNCoverage}_{it}\\) es la cobertura de ahorros de vida en unidades logarítmicas. Como en el modelo básico, es habitual usar una media cero para los efectos aleatorios. Así, el intercepto general es \\(\\beta_1\\) y \\(\\alpha_{1i}\\) representa las desviaciones de las cooperativas de crédito. Además, la pendiente general o global asociada con \\(\\mathrm{LNCoverage}\\) es \\(\\beta_3\\) y \\(\\alpha_{2i}\\) representa las desviaciones de las cooperativas de crédito. Dicho de otra manera, la pendiente correspondiente a \\(\\mathrm{LNCoverage}\\) para la \\(i\\)-ésima cooperativa de crédito es \\(\\beta_3 + \\alpha_{2i}\\). Más generalmente, la ecuación del modelo de efectos aleatorios con coeficientes variables puede escribirse como \\[\\begin{equation} y_{it} = \\mathbf{x}_{it}^{\\prime} \\boldsymbol \\beta + \\mathbf{z}_{it}^{\\prime} \\boldsymbol \\alpha _i + \\varepsilon_{it}. \\tag{10.5} \\end{equation}\\] Al igual que en el modelo de efectos fijos con coeficientes variables, podemos permitir que cualquiera o todas las variables estén asociadas con coeficientes específicos para cada sujeto. La convención utilizada en la literatura es especificar los efectos fijos a través del componente sistemático \\(\\mathbf{x}_{it}^{\\prime} \\boldsymbol \\beta\\) y los efectos aleatorios a través del componente \\(\\mathbf{z}_{it}^{\\prime} \\boldsymbol \\alpha _i\\). Aquí, el vector \\(\\mathbf{z}_{it}\\) típicamente es igual o un subconjunto de \\(\\mathbf{x}_{it}\\), aunque no necesariamente debe ser así. Con esta notación, ahora tenemos un vector de efectos aleatorios \\(\\boldsymbol \\alpha _i\\) que son específicos de cada sujeto. Para reducir al modelo básico, solo se necesita elegir \\(\\boldsymbol \\alpha _i\\) como un escalar (un vector \\(1 \\times 1\\)) y \\(\\mathbf{z}_{it}\\equiv 1\\). El ejemplo en la ecuación (10.4) resulta de elegir \\(\\boldsymbol \\alpha _i = (\\alpha_{1i}, \\alpha_{2i})^{\\prime}\\) y \\(\\mathbf{z}_{it} = (1, \\mathrm{LNCoverage}_{it})^{\\prime}\\). Al igual que en los modelos de efectos fijos, se pueden incorporar fácilmente modelos de correlación serial en los modelos de efectos aleatorios especificando una estructura de correlación para \\(\\varepsilon_{i1}, \\ldots, \\varepsilon_{iT}\\). Esta característica está disponible en los paquetes estadísticos y se describe completamente en las referencias de la Sección 10.6. 10.6 Lecturas Adicionales y Referencias Los modelos de datos longitudinales y de panel se utilizan ampliamente. Por ejemplo, un índice de revistas de negocios y economía, ABI/INFORM, enumera 685 artículos en 2004 y 2005 que utilizan métodos de datos de panel. Otro índice de revistas científicas, el ISI Web of Science, enumera 1,137 artículos en 2004 y 2005 que utilizan métodos de datos longitudinales. Una introducción completa a los datos longitudinales y de panel que enfatiza aplicaciones en negocios y ciencias sociales es Frees (2004). Diggle et al. (2002) proporciona una introducción desde una perspectiva biomédica. Hsiao (2003) ofrece una introducción clásica desde una perspectiva econométrica. Los actuarios están particularmente interesados en las predicciones resultantes de datos longitudinales. Estas predicciones pueden formar la base para actualizar los precios de seguros. Este tema se discute en el Capítulo 18 sobre credibilidad y factores bonus-malus. Referencias del Capítulo Diggle, Peter J., Patrick Heagarty, Kung-Yee Liang and Scott L. Zeger (2002). Analysis of Longitudinal Data, Second Edition. Oxford University Press, London. Frees, Edward W. (2004). Longitudinal and Panel Data: Analysis and Applications in the Social Sciences. Cambridge University Press, New York. Frees, Edward W., Virginia R. Young and Yu Luo (2001). Case studies using panel data models. North American Actuarial Journal 5 (4), 24-42. Glaeser, E. L. and D. C. Maré (2001). Cities and skills. Journal of Labor Economics 19, 316-342. Hsiao, Cheng (2003). * Analysis of Panel Data, Second Edition*. Cambridge University Press, New York. Tufte, Edward R. (1997). Visual Explanations. Cheshire, Conn.: Graphics Press. "],["C11Binary.html", "Capítulo 11 Variables Dependientes Categóricas 11.1 Variables Dependientes Binarias 11.2 Modelos de Regresión Logística y Probit 11.3 Inferencia para Modelos de Regresión Logística y Probit 11.4 Aplicación: Gastos Médicos 11.5 Variables Dependientes Nominales 11.6 Variables Dependientes Ordinales 11.7 Lecturas Adicionales y Referencias 11.8 Ejercicios 11.9 Suplementos Técnicos - Inferencia Basada en Verosimilitud", " Capítulo 11 Variables Dependientes Categóricas Vista Previa del Capítulo. Un modelo con una variable dependiente categórica permite predecir si una observación pertenece a un grupo o categoría distinta. Las variables binarias representan un caso especial importante; pueden indicar si un evento de interés ha ocurrido o no. En aplicaciones actuariales y financieras, el evento puede ser si ocurre un siniestro, si una persona compra un seguro, si una persona se jubila o si una empresa se vuelve insolvente. Este capítulo introduce los modelos de regresión logística y probit para variables dependientes binarias. Las variables categóricas también pueden representar más de dos grupos, conocidos como resultados multicategoría. Las variables multicategoría pueden estar desordenadas o ordenadas, dependiendo de si tiene sentido clasificar los resultados de la variable. Para los resultados esordenados, conocidos como variables nominales, el capítulo introduce los modelos de logits generalizados y logit multinomial. Para los resultados ordenados, conocidos como variables ordinales, el capítulo introduce los modelos de logit acumulativo y probit. 11.1 Variables Dependientes Binarias Ya hemos introducido las variables binarias como un tipo especial de variable discreta que puede utilizarse para indicar si un sujeto tiene una característica de interés, como el sexo de una persona o la propiedad de una empresa de seguros cautiva por parte de una firma. Las variables binarias también describen si ha ocurrido o no un evento de interés, como un accidente. Un modelo con una variable dependiente binaria permite predecir si ha ocurrido un evento o si un sujeto tiene una característica de interés. Ejemplo: Gastos MEPS. La Sección 11.4 describirá una extensa base de datos de la Encuesta del Panel de Gastos Médicos (MEPS, por sus siglas en inglés) sobre la utilización y los gastos de hospitalización. Para estos datos, consideraremos \\[ y_i = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{la i-ésima persona fue hospitalizada durante el período de la muestra} \\\\ 0 &amp; \\text{de lo contrario} \\end{array} \\right. . \\] Hay \\(n=2,000\\) personas en esta muestra, distribuidas de la siguiente manera: Tabla 11.1: Hospitalización por Sexo Hombres Mujeres No hospitalizado \\(y=0\\) 902 (95.3%) 941 (89.3%) Hospitalizado \\(y=1\\) 44 (4.7%) 113 (10.7%) Total 946 1,054 Código R para Generar la Tabla 11.1 Hexpend &lt;- read.csv(&quot;CSVData/HealthExpend.csv&quot;, header=TRUE) # Tabla 11.1 POSEXP = 1*(Hexpend$EXPENDIP&gt;0) #table(POSEXP) #table(Hexpend$GENDER) #Hmisc::summarize(POSEXP, Hexpend$GENDER, mean) row1 &lt;- c(&quot;No hospitalizado&quot; , &quot;$y=0$&quot; , &quot;902 (95.3%)&quot; , &quot;941 (89.3%)&quot; ) row2 &lt;- c(&quot;Hospitalizado&quot; , &quot;$y=1$ &quot;, &quot;44 (4.7%)&quot; , &quot;113 (10.7%)&quot; ) row3 &lt;- c(&quot;Total&quot; , &quot;&quot;, &quot;946&quot; , &quot;1,054&quot; ) tableout &lt;- rbind(row1, row2, row3) row.names(tableout) &lt;- NULL colnames(tableout) &lt;- c(&quot;&quot;, &quot;&quot;, &quot;Hombres&quot;, &quot;Mujeres&quot;) TableGen1(TableData=tableout, TextTitle=&#39;Hospitalización por Sexo&#39;, Align=&#39;lccc&#39;, ColumnSpec=1:3, ColWidth = &quot;3cm&quot;) %&gt;% kableExtra::column_spec(1, width = &quot;4cm&quot;) La Tabla 11.1 sugiere que el sexo tiene una influencia importante sobre si una persona se hospitaliza. Al igual que con las técnicas de regresión lineal introducidas en capítulos anteriores, estamos interesados en usar las características de una persona, como su edad, sexo, educación, ingresos y estado de salud previo, para ayudar a explicar la variable dependiente \\(y\\). A diferencia de los capítulos anteriores, ahora la variable dependiente es discreta y no sigue una distribución normal, ni siquiera de manera aproximada. En circunstancias limitadas, la regresión lineal puede usarse con variables dependientes binarias; esta aplicación se conoce como un modelo de probabilidad lineal. Modelos de Probabilidad Lineal Para introducir algunas de las complejidades que se encuentran con variables dependientes binarias, denotemos la probabilidad de que la respuesta sea igual a 1 como \\(\\pi_i= \\mathrm{Pr}(y_i=1)\\). Una variable aleatoria binaria tiene una distribución de Bernoulli. Por lo tanto, podemos interpretar la respuesta media como la probabilidad de que la respuesta sea uno, es decir, \\(\\mathrm{E~}y_i\\) \\(=0\\times \\mathrm{Pr}(y_i=0) + 1 \\times \\mathrm{Pr}(y_i=1)\\) \\(= \\pi_i\\). Además, la varianza está relacionada con la media a través de la expresión \\(\\mathrm{Var}~y_i = \\pi_i(1-\\pi_i)\\). Comenzamos considerando un modelo lineal de la forma \\[ y_i = \\mathbf{x}_i^{\\mathbf{\\prime}} \\boldsymbol \\beta + \\varepsilon_i, \\] conocido como modelo de probabilidad lineal. Asumiendo \\(\\mathrm{E~}\\varepsilon_i=0\\), tenemos que \\(\\mathrm{E~}y_i=\\mathbf{x}_i^{\\mathbf{\\prime }} \\boldsymbol \\beta =\\pi_i\\). Dado que \\(y_i\\) tiene una distribución de Bernoulli, \\(\\mathrm{Var}~y_i=\\mathbf{x}_i^{\\mathbf{\\prime}} \\boldsymbol \\beta(1-\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta)\\). Los modelos de probabilidad lineal se utilizan debido a la facilidad de interpretación de los parámetros. Para conjuntos de datos grandes, la simplicidad computacional de los estimadores de mínimos cuadrados ordinarios es atractiva en comparación con algunos modelos no lineales más complejos que se introducen más adelante en este capítulo. Como se describió en el Capítulo 3, los estimadores de mínimos cuadrados ordinarios para \\(\\boldsymbol \\beta\\) tienen propiedades deseables. Es sencillo comprobar que los estimadores son consistentes y asintóticamente normales bajo condiciones moderadas sobre las variables explicativas {\\(\\mathbf{x}_i\\)}. Sin embargo, los modelos de probabilidad lineal tienen varios inconvenientes que son graves en muchas aplicaciones. Desventajas del Modelo de Probabilidad Lineal Los valores ajustados pueden ser inadecuados. La respuesta esperada es una probabilidad y, por lo tanto, debe variar entre 0 y 1. Sin embargo, la combinación lineal, \\(\\mathbf{x}_i^{\\mathbf{\\prime}} \\boldsymbol \\beta\\), puede variar entre infinito negativo y positivo. Esta desproporción implica, por ejemplo, que los valores ajustados pueden ser poco razonables. Heterocedasticidad. Los modelos lineales suponen homocedasticidad (varianza constante), pero la varianza de la respuesta depende de la media, que varía entre las observaciones. El problema de la variabilidad cambiante se conoce como heterocedasticidad. El análisis de los residuos no tiene sentido. La respuesta debe ser 0 o 1, aunque los modelos de regresión generalmente consideran la distribución del término de error como continua. Esta desproporción implica, por ejemplo, que el análisis usual de los residuos en los modelos de regresión no tiene sentido. Para manejar el problema de la heterocedasticidad, es posible un procedimiento de mínimos cuadrados ponderados (en dos etapas). En la primera etapa, se usan mínimos cuadrados ordinarios para calcular estimaciones de \\(\\boldsymbol \\beta\\). Con esta estimación, se puede calcular una varianza estimada para cada sujeto utilizando la relación \\(\\mathrm{Var}~y_i=\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta (1-\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta)\\). En la segunda etapa, se realiza un ajuste de mínimos cuadrados ponderados utilizando el inverso de las varianzas estimadas como ponderaciones para obtener nuevas estimaciones de \\(\\boldsymbol \\beta\\). Es posible iterar este procedimiento, aunque estudios han demostrado que tiene pocas ventajas hacerlo (ver Carroll y Ruppert, 1988). Alternativamente, se pueden usar estimadores de mínimos cuadrados ordinarios de \\(\\boldsymbol \\beta\\) con errores estándar robustos a la heterocedasticidad (ver Sección 5.7.2). 11.2 Modelos de Regresión Logística y Probit 11.2.1 Uso de Funciones No Lineales de Variables Explicativas Para evitar las desventajas de los modelos de probabilidad lineal, consideramos modelos alternativos en los que expresamos la expectativa de la respuesta como una función de las variables explicativas, \\(\\pi_i=\\mathrm{\\pi }(\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta)\\) \\(=\\Pr (y_i=1|\\mathbf{x}_i)\\). Nos centramos en dos casos especiales de la función \\(\\mathrm{\\pi }(\\cdot)\\): \\(\\mathrm{\\pi }(z)=\\frac{1}{1+\\exp (-z)}=\\frac{e^{z}}{1+e^{z}}\\), el caso logit, y \\(\\mathrm{\\pi }(z)=\\mathrm{\\Phi }(z)\\), el caso probit. Aquí, \\(\\mathrm{\\Phi }(\\cdot)\\) es la función de distribución normal estándar. La elección de la función identidad (un tipo especial de función lineal), \\(\\mathrm{\\pi }(z)=z\\), da lugar al modelo de probabilidad lineal. En cambio, \\(\\mathrm{\\pi}\\) es no lineal tanto en el caso logit como en el probit. Estas dos funciones son similares en el sentido de que están casi linealmente relacionadas en el intervalo \\(0.1 \\le p \\le 0.9\\). Así que, en gran medida, la elección de la función depende de las preferencias del analista. La Figura 11.1 compara las funciones logit y probit, mostrando que será difícil distinguir entre las dos especificaciones en la mayoría de los conjuntos de datos. La inversa de la función, \\(\\mathrm{\\pi }^{-1}\\), especifica la forma de la probabilidad que es lineal en las variables explicativas, es decir, \\(\\mathrm{\\pi }^{-1}(\\pi_i)= \\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta\\). En el Capítulo 13, nos referimos a esta inversa como la función de enlace. Figura 11.1: Comparación de la Distribución Logit y Probit (Normal Estándar) Ejemplo: Puntuación de Crédito. Los bancos, las agencias de crédito y otras instituciones financieras desarrollan “puntuaciones de crédito” para individuos que se utilizan para predecir la probabilidad de que el prestatario reembolse sus deudas actuales y futuras. Se dice que las personas que no cumplen con los plazos de pago estipulados en un acuerdo de préstamo están en “incumplimiento”. Una puntuación de crédito es entonces una probabilidad predicha de estar en incumplimiento, utilizando la información de la solicitud de crédito como variables explicativas para desarrollar la puntuación. La elección de las variables explicativas depende del propósito de la solicitud; la puntuación de crédito se usa tanto para emitir tarjetas de crédito para pequeñas compras de consumo como para solicitudes de hipotecas de casas multimillonarias. En la Tabla 11.2, Hand y Henley (1997) proporcionan una lista de características típicas que se utilizan en la puntuación de crédito. Tabla 11.2. Características Utilizadas en Algunos Procedimientos de Puntuación de Crédito \\[ \\small{ \\begin{array}{ll} \\hline \\textbf{Características} &amp; \\textbf{Valores Potenciales} \\\\ \\hline \\text{Tiempo en la dirección actual} &amp; \\text{0-1, 1-2, 3-4, 5+ años}\\\\ \\text{Estado de la vivienda} &amp; \\text{Propietario, inquilino, otro }\\\\ \\text{Código postal} &amp; \\text{Rango A, B, C, D, E} \\\\ \\text{Teléfono} &amp; \\text{Sí, no} \\\\ \\text{Ingresos anuales del solicitante} &amp; \\text{£ (0-10000),} \\text{£ (10,000-20,000)} \\text{£ (20,000+)} \\\\ \\text{Tarjeta de crédito} &amp; \\text{Sí, no} \\\\ \\text{Tipo de cuenta bancaria} &amp; \\text{Corriente y/o ahorro, ninguna} \\\\ \\text{Edad }&amp; \\text{18-25, 26-40, 41-55, 55+ años} \\\\ \\text{Juicios del Tribunal del Condado} &amp; \\text{Número} \\\\ \\text{Tipo de ocupación} &amp; \\text{Codificado} \\\\ \\text{Propósito del préstamo} &amp; \\text{Codificado} \\\\ \\text{Estado civil} &amp; \\text{Casado, divorciado, soltero, viudo, otro} \\\\ \\text{Tiempo con el banco} &amp; \\text{Años} \\\\ \\text{Tiempo con el empleador} &amp; \\text{Años }\\\\ \\hline \\textit{Fuente}: \\text{Hand y Henley (1997)} \\\\ \\end{array} } \\] Con la información de la solicitud de crédito y la experiencia de incumplimiento, se puede utilizar un modelo de regresión logística para ajustar la probabilidad de incumplimiento con puntuaciones de crédito derivadas de los valores ajustados. Wiginton (1980) proporciona una de las primeras aplicaciones de la regresión logística a la puntuación de crédito de consumidores. En ese momento, otros métodos estadísticos conocidos como análisis discriminante estaban a la vanguardia de las metodologías de puntuación cuantitativa. En su artículo de revisión, Hand y Henley (1997) discuten otros competidores de la regresión logística, incluidos los sistemas de aprendizaje automático y las redes neuronales. Como señalan Hand y Henley, no existe un “mejor” método de manera uniforme. Las técnicas de regresión son importantes por sí mismas debido a su uso generalizado y porque pueden proporcionar una plataforma para aprender sobre métodos más nuevos. Las puntuaciones de crédito estiman la probabilidad de incumplimiento en préstamos, pero los emisores de crédito también están interesados en la cantidad y el momento del pago de la deuda. Por ejemplo, un “buen” riesgo puede pagar un saldo de crédito tan rápidamente que el prestamista obtiene pocas ganancias. Además, un mal riesgo hipotecario puede incumplir un préstamo tan tarde en el contrato que el prestamista ya ha ganado suficiente ganancia. Véase Gourieroux y Jasiak (2007) para una discusión amplia sobre cómo el modelado de crédito puede usarse para evaluar el riesgo y la rentabilidad de los préstamos. 11.2.2 Interpretación del Umbral Tanto el caso logit como el probit se pueden interpretar de la siguiente manera. Supongamos que existe un modelo lineal subyacente, \\(y_i^{\\ast} = \\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta + \\varepsilon_i^{\\ast}\\). Aquí, no observamos la respuesta \\(y_i^{\\ast}\\), pero la interpretamos como la “propensión” a poseer una característica. Por ejemplo, podríamos pensar en la fortaleza financiera de una empresa de seguros como una medida de su propensión a volverse insolvente (incapaz de cumplir con sus obligaciones financieras). Bajo la interpretación del umbral, no observamos la propensión, pero sí observamos cuando la propensión cruza un umbral. Es habitual asumir que este umbral es 0, por simplicidad. Así, observamos \\[ y_i=\\left\\{ \\begin{array}{ll} 0 &amp; y_i^{\\ast} \\le 0 \\\\ 1 &amp; y_i^{\\ast}&gt;0 \\end{array} \\right. . \\] Para ver cómo se deriva el caso logit a partir del modelo del umbral, supongamos una función de distribución logística para las perturbaciones, de modo que \\[ \\mathrm{\\Pr }(\\varepsilon_i^{\\ast} \\le a)=\\frac{1}{1+\\exp (-a)}. \\] Al igual que la distribución normal, se puede verificar calculando la densidad que la distribución logística es simétrica alrededor de cero. Así, \\(-\\varepsilon_i^{\\ast}\\) tiene la misma distribución que \\(\\varepsilon_i^{\\ast}\\) y entonces \\[ \\pi_i =\\Pr (y_i=1|\\mathbf{x}_i)=\\mathrm{\\Pr }(y_i^{\\ast}&gt;0) =\\mathrm{ \\Pr }(\\varepsilon_i^{\\ast} \\le \\mathbf{x}_i^{\\mathbf{\\prime}}\\mathbf{\\beta }) =\\frac{1}{1+\\exp (-\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta)} =\\mathrm{\\pi }(\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta). \\] Esto establece la interpretación del umbral para el caso logit. El desarrollo para el caso probit es similar y se omite. 11.2.3 Interpretación de Utilidad Aleatoria Tanto el caso logit como el probit también se justifican apelando a la siguiente interpretación de “utilidad aleatoria” del modelo. En algunas aplicaciones económicas, los individuos seleccionan una de dos opciones. Aquí, las preferencias entre opciones están indexadas por una función de utilidad no observada; los individuos seleccionan la opción que les brinda mayor utilidad. Para el sujeto \\(i\\), utilizamos la notación \\(u_i\\) para esta función de utilidad. Modelamos la utilidad (\\(U\\)) como una función de un valor subyacente (\\(V\\)) más un ruido aleatorio (\\(\\varepsilon\\)), es decir, \\(U_{ij}=u_i(V_{ij}+\\varepsilon_{ij})\\), donde \\(j\\) puede ser 1 o 2, correspondiente a la elección. Para ilustrar, asumimos que el individuo elige la categoría correspondiente a \\(j=1\\) si \\(U_{i1}&gt;U_{i2}\\) y denotamos esta elección como \\(y_i=1\\). Asumiendo que \\(u_i\\) es una función estrictamente creciente, tenemos \\[\\begin{eqnarray*} \\Pr (y_i &amp;=&amp;1)=\\mathrm{\\Pr }(U_{i2}&lt;U_{i1})=\\mathrm{\\Pr }\\left( u_i(V_{i2}+\\varepsilon_{i2})&lt;u_i(V_{i1}+\\varepsilon_{i1})\\right) \\\\ &amp;=&amp;\\mathrm{\\Pr }(\\varepsilon_{i2}-\\varepsilon_{i1}&lt;V_{i1}-V_{i2}). \\end{eqnarray*}\\] Para parametrizar el problema, asumimos que el valor \\(V\\) es una combinación lineal desconocida de las variables explicativas. Específicamente, tomamos \\(V_{i2}=0\\) y \\(V_{i1}=\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta\\). Podemos asumir que la diferencia en los errores, \\(\\varepsilon_{i2}-\\varepsilon_{i1}\\), sigue una distribución normal o logística, correspondiente a los casos probit y logit, respectivamente. La distribución logística se cumple si asumimos que los errores tienen una distribución de valor extremo, o Gumbel (ver, por ejemplo, Amemiya, 1985). 11.2.4 Regresión Logística Una ventaja del caso logit es que permite expresiones en forma cerrada, a diferencia de la función de distribución normal. Regresión logística es otra frase utilizada para describir el caso logit. Usando \\(p=\\mathrm{\\pi }(z)= \\left( 1+ \\mathrm{e}^{-z}\\right)^{-1}\\), la inversa de \\(\\mathrm{\\pi }\\) se calcula como \\(z=\\mathrm{\\pi }^{-1}(p)=\\ln(p/(1-p))\\). Para simplificar presentaciones futuras, definimos \\[ \\mathrm{logit}(p)=\\ln \\left( \\frac{p}{1-p}\\right) \\] como la función logit. Con un modelo de regresión logística, representamos la combinación lineal de variables explicativas como el logit de la probabilidad de éxito, es decir, \\(\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta=\\mathrm{logit}(\\pi_i)\\). Interpretación de los Odds Cuando la respuesta \\(y\\) es binaria, conocer solo \\(p=\\Pr(y=1)\\) resume toda la distribución. En algunas aplicaciones, una simple transformación de \\(p\\) tiene una interpretación importante. El ejemplo más común de esto son los odds, dados por \\(p/(1-p)\\). Por ejemplo, supongamos que \\(y\\) indica si un caballo gana una carrera y \\(p\\) es la probabilidad de que el caballo gane. Si \\(p=0.25\\), entonces los odds de que el caballo gane son \\(0.25/(1.00-0.25)=0.3333\\). Podríamos decir que los odds de ganar son 0.3333 a 1, o de uno a tres. De manera equivalente, decimos que la probabilidad de no ganar es \\(1-p=0.75\\), de modo que los odds de que el caballo no gane son \\(0.75/(1-0.75)=3\\) y los odds en contra del caballo son tres a uno. Los odds tienen una interpretación útil desde el punto de vista de las apuestas. Supongamos que estamos jugando un juego justo y que hacemos una apuesta de 1 con odds de uno a tres. Si el caballo gana, entonces recuperamos nuestro 1 más una ganancia de 3. Si el caballo pierde, perdemos nuestra apuesta de 1. Es un juego justo en el sentido de que el valor esperado del juego es cero porque ganamos 3 con probabilidad \\(p=0.25\\) y perdemos 1 con probabilidad \\(1-p=0.75\\). Desde el punto de vista económico, los odds proporcionan los números importantes (apuesta de 1 y ganancia de 3), no las probabilidades. Por supuesto, si conocemos \\(p\\), siempre podemos calcular los odds. Del mismo modo, si conocemos los odds, siempre podemos calcular la probabilidad \\(p\\). El logit es la función logarítmica de los odds, también conocida como log-odds. Interpretación de la Razón de Momios Para interpretar los coeficientes de regresión en el modelo de regresión logística, \\(\\boldsymbol \\beta=(\\beta_0,\\ldots ,\\beta_{k})^{\\prime}\\), comenzamos asumiendo que la \\(j\\)-ésima variable explicativa, \\(x_{ij}\\), es 0 o 1. Entonces, con la notación \\(\\mathbf{x}_i=(x_{i0},...,x_{ij},\\ldots ,x_{ik})^{\\prime}\\), podemos interpretar \\[\\begin{eqnarray*} \\beta_j &amp;=&amp;(x_{i0},...,1,\\ldots ,x_{ik})^{\\prime}\\boldsymbol \\beta -(x_{i0},...,0,\\ldots ,x_{ik})^{\\prime}\\boldsymbol \\beta \\\\ &amp;=&amp;\\ln \\left( \\frac{\\Pr (y_i=1|x_{ij}=1)}{1-\\Pr (y_i=1|x_{ij}=1)}\\right) -\\ln \\left( \\frac{\\Pr (y_i=1|x_{ij}=0)}{1-\\Pr (y_i=1|x_{ij}=0)}\\right) \\end{eqnarray*}\\] Así, \\[ e^{\\beta_j}=\\frac{\\Pr (y_i=1|x_{ij}=1)/\\left( 1-\\Pr (y_i=1|x_{ij}=1)\\right) }{\\Pr (y_i=1|x_{ij}=0)/\\left( 1-\\Pr (y_i=1|x_{ij}=0)\\right) }. \\] Esto muestra que \\(e^{\\beta_j}\\) puede expresarse como la razón de dos momios, conocida como la razón de momios. Es decir, el numerador de esta expresión es el momio cuando \\(x_{ij}=1\\), mientras que el denominador es el momio cuando \\(x_{ij}=0\\). Así, podemos decir que el momio cuando \\(x_{ij}=1\\) es \\(\\exp (\\beta_j)\\) veces mayor que el momio cuando \\(x_{ij}=0\\). Para ilustrar, supongamos que \\(\\beta_j=0.693\\), de modo que \\(\\exp (\\beta _j)=2\\). A partir de esto, decimos que los momios (para \\(y=1\\)) son el doble para \\(x_{ij}=1\\) que para \\(x_{ij}=0\\). De manera similar, suponiendo que la \\(j\\)-ésima variable explicativa es continua (diferenciable), tenemos \\[\\begin{eqnarray} \\beta_j &amp;=&amp;\\frac{\\partial }{\\partial x_{ij}}\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta =\\frac{\\partial }{\\partial x_{ij}}\\ln \\left( \\frac{\\Pr (y_i=1|x_{ij})}{1-\\Pr (y_i=1|x_{ij})}\\right) \\nonumber \\\\ &amp;=&amp;\\frac{\\frac{\\partial }{\\partial x_{ij}}\\Pr (y_i=1|x_{ij})/\\left( 1-\\Pr (y_i=1|x_{ij})\\right) }{\\Pr (y_i=1|x_{ij})/\\left( 1-\\Pr (y_i=1|x_{ij})\\right) }. \\tag{11.1} \\end{eqnarray}\\] Así, podemos interpretar \\(\\beta_j\\) como el cambio proporcional en la razón de momios, conocido como elasticidad en economía. Ejemplo: Gastos MEPS - Continuación. La Tabla 11.1 muestra que el porcentaje de mujeres que fueron hospitalizadas es del \\(10.7\\%\\); alternativamente, el momio de que una mujer sea hospitalizada es \\(0.107/(1-0.107)=0.120\\). Para los hombres, el porcentaje es del \\(4.7\\%\\), de modo que el momio es \\(0.0493\\). La razón de momios es \\(0.120/0.0493=2.434\\); las mujeres tienen más del doble de probabilidad de ser hospitalizadas que los hombres. A partir de un ajuste de regresión logística (descrito en la Sección 11.4), el coeficiente asociado al sexo es \\(0.733\\). Con base en este modelo, decimos que las mujeres tienen \\(\\exp (0.733)=2.081\\) veces más probabilidad que los hombres de ser hospitalizadas. La estimación de regresión de la razón de momios controla por variables adicionales (como edad y educación) en comparación con el cálculo básico basado en frecuencias crudas. 11.3 Inferencia para Modelos de Regresión Logística y Probit 11.3.1 Estimación de Parámetros El método habitual de estimación para modelos logísticos y probit es el máxima verosimilitud, descrito con más detalle en la Sección 11.9. Para proporcionar una intuición, describimos las ideas en el contexto de los modelos de regresión con variables dependientes binarias. La verosimilitud es el valor observado de la función de probabilidad. Para una sola observación, la verosimilitud es \\[ \\left\\{ \\begin{array}{ll} 1-\\pi_i &amp; \\mathrm{si}\\ y_i=0 \\\\ \\pi_i &amp; \\mathrm{si}\\ y_i=1 \\end{array} \\right. . \\] El objetivo de la estimación de máxima verosimilitud es encontrar los valores de los parámetros que producen la mayor verosimilitud. Encontrar el máximo de la función logarítmica produce la misma solución que encontrar el máximo de la función correspondiente. Debido a que generalmente es más sencillo computacionalmente, consideramos la verosimilitud logarítmica (o log-verosimilitud), escrita como \\[\\begin{equation} \\left\\{ \\begin{array}{ll} \\ln \\left( 1-\\pi_i\\right) &amp; \\mathrm{si}\\ y_i=0 \\\\ \\ln \\pi_i &amp; \\mathrm{si}\\ y_i=1 \\end{array} \\right. . \\tag{11.2} \\end{equation}\\] De manera más compacta, la log-verosimilitud de una sola observación es \\[ y_i\\ln \\mathrm{\\pi }(\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta) + (1-y_i) \\ln \\left( 1-\\mathrm{\\pi }(\\mathbf{x}_i^{\\mathbf{\\prime}} \\boldsymbol \\beta)\\right) , \\] donde \\(\\pi_i=\\mathrm{\\pi }(\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta)\\). Asumiendo independencia entre las observaciones, la verosimilitud del conjunto de datos es un producto de las verosimilitudes de cada observación. Tomando logaritmos, la log-verosimilitud del conjunto de datos es la suma de las log-verosimilitudes de cada observación. La log-verosimilitud del conjunto de datos es \\[\\begin{equation} L(\\boldsymbol \\beta)=\\sum\\limits_{i=1}^{n}\\left\\{ y_i\\ln \\mathrm{\\pi }( \\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta) + (1-y_i) \\ln \\left( 1- \\mathrm{\\pi }(\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta)\\right) \\right\\} . \\tag{11.3} \\end{equation}\\] La log-verosimilitud se considera como una función de los parámetros, con los datos fijos. En contraste, la función de probabilidad conjunta se considera como una función de los datos realizados, con los parámetros fijos. El método de máxima verosimilitud consiste en encontrar los valores de \\(\\boldsymbol \\beta\\) que maximizan la log-verosimilitud. El método habitual para encontrar el máximo es tomar las derivadas parciales con respecto a los parámetros de interés y encontrar las raíces de las ecuaciones resultantes. En este caso, al tomar las derivadas parciales con respecto a \\(\\boldsymbol \\beta\\) se obtienen las ecuaciones de puntaje: \\[\\begin{equation} \\frac{\\partial }{\\partial \\boldsymbol \\beta}L(\\boldsymbol \\beta )=\\sum\\limits_{i=1}^{n}\\mathbf{x}_i\\left( y_i-\\mathrm{\\pi }(\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta)\\right) \\frac{\\mathrm{\\pi }^{\\prime}( \\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta)}{\\mathrm{\\pi }(\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta)(1-\\mathrm{\\pi }(\\mathbf{x}_i^{ \\mathbf{\\prime}}\\boldsymbol \\beta))}=\\mathbf{0}, \\tag{11.4} \\end{equation}\\] donde \\(\\pi^{\\prime}\\) es la derivada de \\(\\pi\\). La solución de estas ecuaciones, denotada como \\(\\mathbf{b}_{MLE}\\), es el estimador de máxima verosimilitud. Para la función logit, las ecuaciones de puntaje se reducen a \\[\\begin{equation} \\frac{\\partial }{\\partial \\boldsymbol \\beta}L(\\boldsymbol \\beta )=\\sum\\limits_{i=1}^{n}\\mathbf{x}_i\\left( y_i-\\mathrm{\\pi }(\\mathbf{x} _i^{\\mathbf{\\prime}}\\boldsymbol \\beta)\\right) =\\mathbf{0}, \\tag{11.5} \\end{equation}\\] donde \\(\\mathrm{\\pi }(z)=1/(1+\\exp (-z))\\). 11.3.2 Inferencia Adicional Un estimador de la varianza muestral grande de \\(\\boldsymbol \\beta\\) puede calcularse tomando las derivadas parciales de las ecuaciones de puntaje. Específicamente, el término \\[ \\mathbf{I}(\\boldsymbol \\beta) = - \\mathrm{E} \\left( \\frac{\\partial^2} {\\partial \\boldsymbol \\beta ~ \\partial \\boldsymbol \\beta ^{\\prime}}L(\\boldsymbol \\beta) \\right) \\] es la matriz de información. Como caso especial, usando la función logit y la ecuación (11.5), cálculos sencillos muestran que la matriz de información es \\[ \\mathbf{I}(\\boldsymbol \\beta) = \\sum\\limits_{i=1}^{n} \\sigma_i^2 \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\] donde \\(\\sigma_i^2 = \\mathrm{\\pi} (\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta) (1 - \\mathrm{\\pi}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta))\\). La raíz cuadrada del \\((j+1)\\)-ésimo elemento diagonal de esta matriz, evaluado en \\(\\boldsymbol \\beta = \\mathbf{b}_{MLE}\\), proporciona el error estándar de \\(b_{j,MLE}\\), denotado como \\(se(b_{j,MLE})\\). Para evaluar el ajuste general del modelo, es habitual citar estadísticos de prueba de razón de verosimilitud en modelos de regresión no lineales. Para probar la adecuación general del modelo \\(H_0:\\boldsymbol \\beta=\\mathbf{0}\\), utilizamos el estadístico \\[ LRT=2\\times (L(\\mathbf{b}_{MLE})-L_0), \\] donde \\(L_0\\) es la log-verosimilitud maximizada solo con el término de intercepto. Bajo la hipótesis nula \\(H_0\\), este estadístico tiene una distribución chi-cuadrado con \\(k\\) grados de libertad. La Sección 11.9.3 describe los estadísticos de prueba de razón de verosimilitud con mayor detalle técnico. Como se describe en la Sección 11.9, las medidas de bondad de ajuste pueden ser difíciles de interpretar en modelos no lineales. Una medida es el llamado \\(max-scaled~R^2\\), definido como \\(R_{ms}^2=R^2/R_{max}^2\\), donde \\[ R^2=1-\\left( \\frac{\\exp (L_0/n)}{\\exp (L(\\mathbf{b}_{MLE})/n)}\\right) \\] y \\(R_{max }^2 = 1 - \\exp(L_0/n)^2\\). Aquí, \\(L_0/n\\) representa el valor promedio de esta log-verosimilitud. Otra medida es un “pseudo-\\(R^2\\)” \\[ \\frac{L( \\mathbf{b}_{MLE}) - L_0}{L_{max}-L_0}, \\] donde \\(L_0\\) y \\(L_{max }\\) son la log-verosimilitud basada solo en un intercepto y en el máximo alcanzable, respectivamente. Al igual que el coeficiente de determinación, el pseudo-\\(R^2\\) toma valores entre cero y uno, con valores mayores que indican un mejor ajuste a los datos. Otras versiones del pseudo-\\(R^2\\) están disponibles en la literatura, véase, por ejemplo, Cameron y Trivedi (1998). Una ventaja de esta medida de pseudo-\\(R^2\\) es su relación con las pruebas de hipótesis de los coeficientes de regresión. Ejemplo: Seguridad Laboral. Valletta (1999) estudió la disminución de la seguridad laboral utilizando la base de datos de la Encuesta Panel de Dinámicas de Ingresos (PSID). Aquí consideramos uno de los modelos de regresión presentados por Valletta, basado en una muestra de jefes de hogar masculinos que consta de \\(n=24,168\\) observaciones durante los años 1976-1992, inclusive. La encuesta PSID registra las razones por las cuales los hombres dejaron su empleo más reciente, incluyendo cierres de planta, “renuncia” y cambios de trabajo por otras razones. Sin embargo, Valletta se centró en los despidos (“lay off” o “despedido”) porque las separaciones involuntarias están asociadas con la inseguridad laboral. Tabla 11.3 presenta un modelo de regresión probit realizado por Valletta (1999), utilizando los despidos como variable dependiente. Además de las variables explicativas enumeradas en Tabla 11.3, otras variables controladas incluyeron educación, estado civil, número de hijos, raza, años de experiencia laboral a tiempo completo y su cuadrado, afiliación sindical, empleo en el gobierno, salario logarítmico, la tasa de empleo en EE.UU. y ubicación según el área estadística metropolitana de residencia. En la Tabla 11.3, la antigüedad representa los años trabajados en la empresa actual. Además, el empleo en el sector fue medido examinando el empleo según la Encuesta de Precios al Consumidor en 387 sectores de la economía, basada en 43 categorías industriales y nueve regiones del país. Por un lado, el coeficiente de antigüedad revela que los trabajadores con más experiencia tienen menos probabilidades de ser despedidos. Por otro lado, el coeficiente asociado a la interacción entre antigüedad y la tendencia temporal revela una tasa creciente de despidos para los trabajadores con más experiencia. La interpretación de los coeficientes de empleo por sector también es de interés. Con una antigüedad promedio de aproximadamente 7.8 años en la muestra, vemos que los hombres con poca antigüedad no se ven muy afectados por los cambios en el empleo por sector. Sin embargo, para los hombres con más experiencia, hay una probabilidad creciente de despido asociada con los sectores de la economía donde el crecimiento disminuye. Tabla 11.3. Estimaciones de Regresión Probit para Despidos \\[ \\small{ \\begin{array}{lrr} \\hline \\textbf{Variable} &amp; \\textbf{Estimación del} &amp; \\textbf{Error} \\\\ &amp; \\textbf{Parámetro} &amp; \\textbf{Estándar} \\\\ \\hline \\text{Antigüedad} &amp; -0.084 &amp; 0.010 \\\\ \\text{Tendencia Temporal} &amp; -0.002 &amp; 0.005 \\\\ \\text{Antigüedad*(Tendencia Temporal)} &amp; 0.003 &amp; 0.001 \\\\ \\text{Cambio en el Empleo Logarítmico por Sector} &amp; 0.094 &amp; 0.057 \\\\ \\text{Antigüedad*(Cambio en el Empleo Logarítmico por Sector)} &amp; -0.020 &amp; 0.009 \\\\ \\hline \\text{-2 Log Verosimilitud} &amp; 7,027.8 &amp; \\\\ \\text{Pseudo}-R^2 &amp; 0.097 &amp; \\\\ \\hline \\end{array} } \\] 11.4 Aplicación: Gastos Médicos Esta sección considera datos de la Encuesta del Panel de Gastos Médicos (MEPS), realizada por la Agencia de Investigación y Calidad de la Salud de EE. UU. MEPS es una encuesta probabilística que proporciona estimaciones representativas a nivel nacional sobre el uso de atención médica, los gastos, las fuentes de pago y la cobertura de seguros para la población civil de EE. UU. Esta encuesta recoge información detallada sobre las personas y cada episodio de atención médica, por tipo de servicios, incluyendo visitas al consultorio del médico, visitas a la sala de emergencias del hospital, visitas ambulatorias al hospital, estancias hospitalarias, visitas a otros proveedores médicos y uso de medicamentos recetados. Esta información detallada permite desarrollar modelos de utilización de atención médica para predecir gastos futuros. Consideramos datos de MEPS del primer panel de 2003 y tomamos una muestra aleatoria de \\(n=2,000\\) individuos entre 18 y 65 años. Variable Dependiente Nuestra variable dependiente es un indicador de gastos positivos por admisiones hospitalarias. En MEPS, las admisiones hospitalarias incluyen personas que fueron admitidas en un hospital y pasaron la noche. En contraste, los eventos ambulatorios incluyen visitas al departamento ambulatorio del hospital, visitas a proveedores en consultorios y visitas a la sala de emergencias, excluyendo servicios dentales. (Los servicios dentales, en comparación con otros tipos de atención médica, son más predecibles y ocurren de manera más regular). Las estancias hospitalarias con la misma fecha de ingreso y alta, conocidas como “estancias de cero noches”, se incluyeron en los recuentos y gastos ambulatorios. Los pagos asociados con visitas a la sala de emergencias que precedieron inmediatamente a una estancia hospitalaria se incluyeron en los gastos hospitalarios. Los medicamentos recetados vinculados a admisiones hospitalarias se incluyeron en los gastos hospitalarios (no en la utilización ambulatoria). Variables Explicativas Las variables explicativas que pueden ayudar a explicar la utilización de atención médica se categorizan en factores demográficos, geográficos, de salud, educación y económicos. Los factores demográficos incluyen edad, sexo y etnia. A medida que las personas envejecen, la tasa a la que su salud se deteriora aumenta con la edad; como resultado, la edad tiene un impacto creciente en la demanda de atención médica. El sexo y la etnia pueden tratarse como aproximaciones de la herencia de salud y los hábitos sociales en el mantenimiento de la salud. Para un factor geográfico, usamos la región como una aproximación de la accesibilidad a los servicios de salud y del impacto económico o regional general en el comportamiento de atención médica de los residentes. Se piensa que la demanda de servicios médicos está influenciada por el estado de salud y la educación de las personas. En MEPS, la salud física y mental autoevaluada, y cualquier limitación funcional o relacionada con la actividad durante el período de la muestra, se utilizan como aproximaciones del estado de salud. La educación tiende a tener un impacto ambiguo en la demanda de servicios de atención médica. Una teoría es que las personas más educadas son más conscientes de los riesgos para la salud, por lo tanto, son más activas en el mantenimiento de su salud; como resultado, las personas educadas pueden ser menos propensas a enfermedades graves que conduzcan a hospitalizaciones. Otra teoría es que las personas con menos educación tienen mayor exposición a riesgos para la salud y, a través de esta exposición, desarrollan una mayor tolerancia a ciertos tipos de riesgos. En MEPS, la educación se aproxima por los títulos recibidos y se clasifica en tres niveles diferentes: menos que secundaria, secundaria, y universidad o superior. Las covariables económicas incluyen ingresos y cobertura de seguro. Una medida de ingresos en MEPS es el ingreso relativo al nivel de pobreza. Este enfoque es adecuado porque resume los efectos de los diferentes niveles de ingresos en la utilización de atención médica en dólares constantes. La cobertura de seguro también es una variable importante para explicar la utilización de atención médica. Un problema con la cobertura de seguro de salud es que reduce los precios pagados por los asegurados, lo que induce un riesgo moral. La investigación asociada con el Experimento de Seguro de Salud de Rand sugirió empíricamente que los efectos del costo compartido derivados de la cobertura de seguro afectarán principalmente el número de contactos médicos más que la intensidad de cada contacto. Esto motivó nuestra introducción de una variable binaria que toma el valor de 1 si una persona tuvo algún seguro público o privado durante al menos un mes, y 0 de lo contrario. Estadísticas Descriptivas Tabla 11.4 describe estas variables explicativas y proporciona estadísticas descriptivas que sugieren sus efectos sobre la probabilidad de gastos hospitalarios positivos. Por ejemplo, vemos que las mujeres tuvieron una mayor utilización general que los hombres. Específicamente, el 10.7% de las mujeres tuvo un gasto positivo durante el año en comparación con solo el 4.7% de los hombres. De manera similar, la utilización varía según otras covariables, lo que sugiere su importancia como predictores de los gastos. Tabla 11.4. Porcentaje de Gastos Positivos por Variable Explicativa \\[ \\scriptsize{ \\begin{array}{lllrr} \\hline &amp; &amp; &amp; \\textbf{Por-} &amp; \\textbf{Porcentaje} \\\\ \\textbf{Categoría} &amp; \\textbf{Variable} &amp; \\textbf{Descripción} &amp; \\textbf{centaje} &amp; \\textbf{Gastos} \\\\ &amp; &amp; &amp; \\textbf{de datos} &amp; \\textbf{Positivos} \\\\ \\hline \\text{Demografía} &amp; AGE &amp; \\text{Edad en años entre} \\\\ &amp; &amp; \\text{ 18 a 65 (media: 39.0)} \\\\ &amp; GENDER &amp; 1 \\text{si mujer} &amp; 52.7 &amp; 10.7 \\\\ &amp; &amp; \\text{0 si hombre} &amp; 47.3 &amp; 4.7\\\\ \\text{Etnia} &amp; ASIAN &amp; \\text{1 si asiático} &amp; 4.3 &amp; 4.7 \\\\ &amp; BLACK &amp; \\text{1 si negro} &amp; 14.8 &amp; 10.5 \\\\ &amp; NATIVE &amp; \\text{1 si nativo} &amp; 1.1 &amp; 13.6 \\\\ &amp; WHITE &amp; \\text{Nivel de referencia} &amp; 79.9 &amp; 7.5 \\\\ \\text{Región} &amp; NORTHEAST &amp; 1 \\text{si noreste} &amp; 14.3 &amp; 10.1 \\\\ &amp; MIDWEST &amp; 1 \\text{si medio oeste} &amp; 19.7 &amp; 8.7 \\\\ &amp; SOUTH &amp; 1 \\text{si sur} &amp; 38.2 &amp; 8.4 \\\\ &amp; WEST &amp; \\text{Nivel de referencia} &amp;27.9 &amp; 5.4 \\\\ \\hline \\text{Educación} &amp; \\text{COLLEGE} &amp; 1 \\text{si universidad o grado superior} &amp; 27.2 &amp; 6.8 \\\\ &amp; HIGHSCHOOL &amp; 1 \\text{si grado de secundaria} &amp; 43.3 &amp; 7.9\\\\ &amp; \\text{Nivel de referencia } &amp; &amp; 29.5 &amp; 8.8\\\\ &amp; \\text{ es menor que grado} &amp; &amp; \\\\ &amp; \\text{ de secundaria} &amp; &amp; \\\\ \\hline \\text{Salud } &amp; POOR &amp; \\text{1 si pobre} &amp; 3.8 &amp; 36.0 \\\\ \\ \\ \\text{autoevaluada} &amp; FAIR &amp; \\text{1 si regular} &amp; 9.9 &amp; 8.1 \\\\ \\ \\ \\text{física} &amp; GOOD &amp; \\text{1 si buena} &amp; 29.9 &amp; 8.2 \\\\ \\ \\ \\text{salud}&amp; VGOOD &amp; \\text{1 si muy buena} &amp; 31.1 &amp; 6.3 \\\\ &amp; \\text{Nivel de referencia} &amp; &amp; 25.4 &amp; 5.1 \\\\ &amp; ~~~\\text{ es excelente salud} &amp; &amp; \\\\ \\text{Salud mental} &amp; MNHPOOR &amp; \\text{1 si pobre o regular} &amp; 7.5 &amp; 16.8 \\\\ \\ \\ \\text{autoevaluada} &amp; &amp; \\text{0 si buena a excelente salud mental} &amp; 92.6 &amp; 7.1 \\\\ \\text{Cualquier} &amp; ANYLIMIT &amp; \\text{1 si cualquier limitación}&amp; 22.3 &amp; 14.6 \\\\ \\ \\ \\ \\text{ limitación} &amp; &amp; \\ \\ \\ \\text{funcional/actividad}&amp; &amp; \\\\ \\ \\ \\text{en la actividad} &amp; &amp; \\text{0 si no hay limitación} &amp; 77.7 &amp; 5.9 \\\\ \\hline \\text{Ingresos} &amp; HINCOME &amp; \\text{1 si ingresos altos} &amp; 31.6 &amp; 5.4 \\\\ \\ \\ \\text{comparado con} &amp; MINCOME &amp; \\text{1 si ingresos medios} &amp; 29.9 &amp; 7.0 \\\\ \\ \\ \\text{nivel de pobreza} &amp; LINCOME &amp; \\text{1 si ingresos bajos} &amp; 15.8 &amp; 8.3 \\\\ &amp; NPOOR &amp; \\text{1 si casi pobre} &amp; 5.8 &amp; 9.5 \\\\ &amp; \\text{Nivel de referencia} &amp;&amp; 17.0 &amp; 13.0 \\\\ &amp; ~~~\\text{ es pobre/negativo} &amp; &amp; \\\\ \\hline \\text{Seguro} &amp; INSURE &amp; \\text{1 si cubierto por seguro } &amp; 77.8 &amp; 9.2 \\\\ \\ \\ \\text{de salud} &amp; &amp; \\ \\ \\text{público/privado en } &amp; &amp; \\\\ &amp; &amp; \\ \\ \\text{cualquier mes de 2003} &amp; &amp; \\\\ &amp; &amp; \\text{0 si no tiene seguro de salud en 2003} &amp; 22.3 &amp; 3.1 \\\\ \\hline Total &amp; &amp; &amp; 100.0 &amp; 7.9 \\\\ \\hline \\end{array} } \\] Código R para Generar la Tabla 11.4 # CREAR UNA FUNCIÓN CORTA PARA AHORRAR TRABAJO fun1 &lt;- function(y){ options(digits=3) temp0 &lt;- cbind(table(y)/length(y),summarize(POSEXP, y, mean)) temp &lt;- temp0[c(2,1),] temp[,c(2,4)] &lt;- round(temp[,c(2,4)], digits=3) return(cbind(temp[,3],temp[,2],temp[,4]))} var1 &lt;- fun1(Hexpend$GENDER) var2 &lt;- fun1(Hexpend$RACE) var3 &lt;- fun1(Hexpend$REGION) var4 &lt;- fun1(Hexpend$EDUC) var5 &lt;- fun1(Hexpend$PHSTAT) #var6 &lt;- fun1(Hexpend$MPOOR) var7 &lt;- fun1(Hexpend$ANYLIMIT) var8 &lt;- fun1(Hexpend$INCOME) var9 &lt;- fun1(Hexpend$insure) tableout &lt;- rbind(var1, var2, var3, var4, var5, var7, var8, var9) #tableout La Tabla 11.5 resume el ajuste de varios modelos de regresión binaria. Los ajustes se informan en la columna “Modelo Completo” para todas las variables utilizando la función logit. Los \\(t\\)-ratios para muchas de las variables explicativas superan dos en valor absoluto, lo que sugiere que son predictores útiles. A partir de una inspección de estos \\(t\\)-ratios, uno podría considerar un modelo más parsimonioso eliminando las variables estadísticamente insignificantes. La Tabla 11.5 muestra un “Modelo Reducido,” en el cual las variables de edad y estado de salud mental han sido eliminadas. Para evaluar su significancia conjunta, podemos calcular un estadístico de prueba de razón de verosimilitud como el doble del cambio en la log-verosimilitud. Esto resulta ser solo \\(2\\times \\left( -488.78-(-488.69)\\right) =0.36.\\) Comparando esto con una distribución chi-cuadrado con \\(df=2\\) grados de libertad, obtenemos un valor-\\(p=0.835\\), lo que indica que los parámetros adicionales para la edad y el estado de salud mental no son estadísticamente significativos. La Tabla 11.5 también proporciona los ajustes del modelo probit. Aquí, vemos que los resultados son similares a los del modelo logit, según el signo de los coeficientes y su significancia, lo que sugiere que para esta aplicación hay poca diferencia entre las dos especificaciones. knitr::kable(2, caption = &quot;Tonto. Crear una tabla solo para actualizar el contador...&quot;) Tabla 11.2: Tonto. Crear una tabla solo para actualizar el contador… x 2 knitr::kable(2, caption = &quot;Tonto.&quot;) Tabla 11.3: Tonto. x 2 knitr::kable(2, caption = &quot;Tonto. &quot;) Tabla 11.4: Tonto. x 2 Tabla 11.5. Comparación de Modelos de Regresión Binaria \\[ \\scriptsize{ \\begin{array}{l|rr|rr|rr} \\hline &amp; \\text{Logistic} &amp; &amp; \\text{Logistic} &amp;&amp;\\text{Probit} \\\\ \\hline &amp; \\text{Full Model} &amp; &amp;\\text{Reduced Model} &amp;&amp; \\text{Reduced Model} \\\\ &amp; \\text{Parameter} &amp; &amp; \\text{Parameter} &amp; &amp; \\text{Parameter} &amp; \\\\ \\text{Effect} &amp; \\text{Estimate} &amp; t\\text{-ratio}o &amp; \\text{Estimate} &amp; t\\text{-rati}o &amp; \\text{Estimate} &amp; t\\text{-ratio} \\\\ \\hline Intercept &amp; -4.239 &amp; -8.982 &amp; -4.278 &amp; -10.094 &amp; -2.281 &amp; -11.432 \\\\ AGE &amp; -0.001 &amp; -0.180 &amp; &amp; &amp; &amp; \\\\ GENDER &amp; 0.733 &amp; 3.812 &amp; 0.732 &amp; 3.806 &amp; 0.395 &amp; 4.178 \\\\ ASIAN &amp; -0.219 &amp; -0.411 &amp; -0.219 &amp; -0.412 &amp; -0.108 &amp; -0.427 \\\\ BLACK &amp; -0.001 &amp; -0.003 &amp; 0.004 &amp; 0.019 &amp; 0.009 &amp; 0.073 \\\\ NATIVE &amp; 0.610 &amp; 0.926 &amp; 0.612 &amp; 0.930 &amp; 0.285 &amp; 0.780 \\\\ NORTHEAST &amp; 0.609 &amp; 2.112 &amp; 0.604 &amp; 2.098 &amp; 0.281 &amp; 1.950 \\\\ MIDWEST &amp; 0.524 &amp; 1.904 &amp; 0.517 &amp; 1.883 &amp; 0.237 &amp; 1.754 \\\\ SOUTH &amp; 0.339 &amp; 1.376 &amp; 0.328 &amp; 1.342 &amp; 0.130 &amp; 1.085 \\\\ \\hline COLLEGE &amp; 0.068 &amp; 0.255 &amp; 0.070 &amp; 0.263 &amp; 0.049 &amp; 0.362 \\\\ HIGHSCHOOL &amp; 0.004 &amp; 0.017 &amp; 0.009 &amp; 0.041 &amp; 0.003 &amp; 0.030 \\\\ \\hline POOR &amp; 1.712 &amp; 4.385 &amp; 1.652 &amp; 4.575 &amp; 0.939 &amp; 4.805 \\\\ FAIR &amp; 0.136 &amp; 0.375 &amp; 0.109 &amp; 0.306 &amp; 0.079 &amp; 0.450 \\\\ GOOD &amp; 0.376 &amp; 1.429 &amp; 0.368 &amp; 1.405 &amp; 0.182 &amp; 1.412 \\\\ VGOOD &amp; 0.178 &amp; 0.667 &amp; 0.174 &amp; 0.655 &amp; 0.094 &amp; 0.728 \\\\ MNHPOOR &amp; -0.113 &amp; -0.369 &amp; &amp; &amp; &amp; \\\\ ANYLIMIT &amp; 0.564 &amp; 2.680 &amp; 0.545 &amp; 2.704 &amp; 0.311 &amp; 3.022 \\\\ \\hline HINCOME &amp; -0.921 &amp; -3.101 &amp; -0.919 &amp; -3.162 &amp; -0.470 &amp; -3.224 \\\\ MINCOME &amp; -0.609 &amp; -2.315 &amp; -0.604 &amp; -2.317 &amp; -0.314 &amp; -2.345 \\\\ LINCOME &amp; -0.411 &amp; -1.453 &amp; -0.408 &amp; -1.449 &amp; -0.241 &amp; -1.633 \\\\ NPOOR &amp; -0.201 &amp; -0.528 &amp; -0.204 &amp; -0.534 &amp; -0.146 &amp; -0.721 \\\\ INSURE &amp; 1.234 &amp; 4.047 &amp; 1.227 &amp; 4.031 &amp; 0.579 &amp; 4.147 \\\\ \\hline Log-Likelihood &amp; -488.69 &amp;&amp; -488.78 &amp;&amp; -486.98 \\\\ \\textit{AIC} &amp; 1,021.38 &amp;&amp; 1,017.56 &amp;&amp; 1,013.96 \\\\ \\hline \\end{array} } \\] Código R para Generar la Tabla 11.5 #Hexpend &lt;- read.csv(&quot;CSVData/HealthExpend.csv&quot;, header=TRUE) # Tabla 11.5 Hexpend$POSEXP = 1*(Hexpend$EXPENDIP&gt;0) # CAMBIAR NIVELES DE REFERENCIA PARA COINCIDIR CON EL LIBRO (HECHO EN SAS) attach(Hexpend) RACE = relevel(factor(RACE),ref=&quot;WHITE&quot;) REGION = relevel(factor(REGION),ref=&quot;WEST&quot;) EDUC = relevel(factor(EDUC),ref=&quot;LHIGHSC&quot;) PHSTAT = relevel(factor(PHSTAT),ref=&quot;EXCE&quot;) INCOME = relevel(factor(INCOME),ref=&quot;POOR&quot;) # MODELO LOGIT COMPLETO; PosExpglmFull = glm(POSEXP~AGE+GENDER +factor(RACE)+ factor(REGION)+factor(EDUC) +factor(PHSTAT)+ANYLIMIT+factor(INCOME) +insure, family=binomial(link=logit)) #summary(PosExpglmFull) Table1 &lt;- summary(PosExpglmFull) Table1Red &lt;- round(Table1$coefficients[,c(1,3)], digits=3) # MODELO LOGIT REDUCIDO; PosExpglmRed = glm(POSEXP~GENDER + factor(RACE) + +factor(REGION)+factor(EDUC) +factor(PHSTAT)+ANYLIMIT+factor(INCOME) +insure, family=binomial(link=logit)) #summary(PosExpglmRed) Table2 &lt;- summary(PosExpglmRed) Table2Red &lt;- round(Table2$coefficients[,c(1,3)], digits = 3) #anova(PosExpglmRed,PosExpglmFull, test=&quot;Chisq&quot;) # MODELO PROBIT REDUCIDO; PosExpglmRedProbit = glm(POSEXP~GENDER+ factor(RACE)+ +factor(REGION)+factor(EDUC) +factor(PHSTAT)+factor(ANYLIMIT)+factor(INCOME) +factor(insure), binomial(link=probit)) #summary(PosExpglmRedProbit) Table3 &lt;- summary(PosExpglmRedProbit) Table3Red &lt;- round(Table3$coefficients[,c(1,3)], digits = 3) detach(Hexpend) row.names(Table2Red) &lt;- NULL row.names(Table3Red) &lt;- NULL Table23Join &lt;- cbind(Table2Red,Table3Red) Table23JoinA &lt;- rbind(Table23Join[1,], c(&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;), Table23Join[2:14,], c(&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;), Table23Join[15:20,]) Table123 &lt;- cbind(Table1Red, Table23JoinA) rowLogLik &lt;- round(c(logLik(PosExpglmFull) , logLik(PosExpglmRed) , logLik(PosExpglmRedProbit) ), digits = 2) rowAIC &lt;- round(c(AIC(PosExpglmFull) , AIC(PosExpglmRed) , AIC(PosExpglmRedProbit) ), digits = 2) rowSumStats &lt;- rbind(c(rowLogLik[1], &quot;&quot;,rowLogLik[2], &quot;&quot;,rowLogLik[3], &quot;&quot;), c(rowAIC[1], &quot;&quot;,rowAIC[2], &quot;&quot;,rowAIC[3], &quot;&quot;) ) row.names(rowSumStats) &lt;- c(&quot;Log-Verosimilitud&quot;, &quot;$AIC$&quot;) Table123A &lt;- rbind(Table123, rowSumStats) colnames(Table123A) &lt;- rep(c(&quot;Estimaciones de Parámetro&quot;, &quot;$t$-Ratio&quot;),3) TableGen1(TableData=Table123A, TextTitle=&#39;Comparación de Modelos de Regresión Binaria&#39;, Align=&#39;r&#39;, ColumnSpec=1:6, ColWidth = ColWidth6) %&gt;% add_header_above(c(&quot; &quot;=1, &quot;Modelo Logístico Completo&quot; = 2, &quot;Modelo Logístico Reducido&quot; = 2, &quot;Modelo Probit Reducido&quot;=2)) Tabla 11.5: Comparación de Modelos de Regresión Binaria Modelo Logístico Completo Modelo Logístico Reducido Modelo Probit Reducido Estimaciones de Parámetro \\(t\\)-Ratio Estimaciones de Parámetro \\(t\\)-Ratio Estimaciones de Parámetro \\(t\\)-Ratio (Intercept) -4.239 -8.981 -4.274 -10.082 -2.279 -11.346 AGE -0.001 -0.17 GENDER 0.734 3.815 0.735 3.817 0.395 4.197 factor(RACE)ASIAN -0.222 -0.417 -0.223 -0.418 -0.108 -0.432 factor(RACE)BLACK -0.004 -0.017 -0.002 -0.008 0.008 0.062 factor(RACE)NATIV 0.602 0.913 0.607 0.92 0.283 0.781 factor(RACE)OTHER -0.215 -0.32 -0.214 -0.319 -0.05 -0.148 factor(REGION)MIDWEST 0.518 1.887 0.517 1.882 0.236 1.747 factor(REGION)NORTHEAST 0.605 2.099 0.603 2.092 0.28 1.941 factor(REGION)SOUTH 0.332 1.355 0.33 1.348 0.13 1.081 factor(EDUC)COLLEGE 0.068 0.255 0.068 0.253 0.048 0.357 factor(EDUC)HIGHSCH 0.005 0.025 0.005 0.024 0.003 0.024 factor(PHSTAT)FAIR 0.115 0.321 0.108 0.303 0.078 0.445 factor(PHSTAT)GOOD 0.371 1.409 0.366 1.399 0.181 1.409 factor(PHSTAT)POOR 1.668 4.524 1.656 4.583 0.939 4.739 factor(PHSTAT)VGOO 0.175 0.654 ANYLIMIT 0.554 2.651 0.171 0.644 0.093 0.723 factor(INCOME)HINCOME -0.911 -3.082 0.545 2.702 0.311 3.008 factor(INCOME)LINCOME -0.404 -1.431 -0.92 -3.165 -0.471 -3.187 factor(INCOME)MINCOME -0.6 -2.288 -0.408 -1.448 -0.241 -1.635 factor(INCOME)NPOOR -0.199 -0.522 -0.604 -2.317 -0.315 -2.326 insure 1.232 4.041 -0.199 -0.522 -0.146 -0.722 Log-Verosimilitud -488.71 -488.72 -486.97 \\(AIC\\) 1021.42 1019.45 1015.94 11.5 Variables Dependientes Nominales Ahora consideramos una respuesta que es una variable categórica no ordenada, también conocida como variable dependiente nominal. Suponemos que la variable dependiente \\(y\\) puede tomar los valores \\(1, 2, \\ldots , c,\\) correspondientes a \\(c\\) categorías. Cuando \\(c&gt;2\\), nos referimos a los datos como “multicategoría”, también conocidos como policótomos o policitomos. En muchas aplicaciones, las categorías de respuesta corresponden a un atributo poseído o a elecciones hechas por individuos, hogares o empresas. Algunas aplicaciones incluyen: Elección de empleo, como en Valletta (1999) Modo de transporte, como en el trabajo clásico de McFadden (1978) Tipo de seguro de salud, como en Browne y Frees (2007). Para una observación del sujeto \\(i\\), denotamos la probabilidad de elegir la categoría \\(j\\) como \\(\\pi_{ij}= \\mathrm{Pr}(y_i = j)\\), de modo que \\(\\pi_{i1}+\\cdots+\\pi_{ic}=1\\). En general, modelaremos estas probabilidades como una función (conocida) de parámetros y utilizaremos la estimación de máxima verosimilitud para la inferencia estadística. Sea \\(y_{ij}\\) una variable binaria que es 1 si \\(y_i=j\\). Extendiendo la ecuación (11.2) a \\(c\\) categorías, la verosimilitud para el sujeto \\(i\\) es: \\[ \\prod_{j=1}^c \\left( \\pi_{i,j} \\right)^{y_{i,j}} =\\left\\{ \\begin{array}{cc} \\pi_{i,1} &amp; \\mathrm {if}~ y_i = 1 \\\\ \\pi_{i,2} &amp; \\mathrm {if}~ y_i = 2 \\\\ \\vdots &amp; \\vdots \\\\ \\pi_{i,c} &amp; \\mathrm {if}~ y_i = c \\\\ \\end{array} \\right. . \\] Por lo tanto, suponiendo independencia entre las observaciones, la verosimilitud total es \\[ L = \\sum_{i=1}^n \\sum_{j=1}^c y_{i,j}~ \\mathrm{ln}~ \\pi_{i,j} . \\] Con este marco, la estimación estándar de máxima verosimilitud está disponible (Sección 11.9). Por lo tanto, nuestra tarea principal es especificar una forma apropiada para \\(\\pi\\). 11.5.1 Logit Generalizado Al igual que la regresión lineal estándar, los modelos de logit generalizado emplean combinaciones lineales de variables explicativas de la forma: \\[\\begin{equation} V_{i,j} = \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta_j . \\tag{11.6} \\end{equation}\\] Debido a que las variables dependientes no son numéricas, no podemos modelar la respuesta \\(y\\) como una combinación lineal de variables explicativas más un error. En su lugar, utilizamos las probabilidades \\[\\begin{equation} \\mathrm{Pr} \\left(y_i = j \\right) = \\pi_{i,j} = \\frac {\\exp (V_{i,j})}{\\sum_{k=1}^c \\exp(V_{i,k})} . \\tag{11.7} \\end{equation}\\] Tenga en cuenta aquí que \\(\\boldsymbol \\beta_j\\) es el vector correspondiente de parámetros que puede depender de la alternativa \\(j\\), mientras que las variables explicativas \\(\\mathbf{x}_i\\) no. Para que las probabilidades sumen uno, una normalización conveniente para este modelo es \\(\\boldsymbol \\beta_c =\\mathbf{0}\\). Con esta normalización y el caso especial de \\(c = 2\\), el logit generalizado se reduce al modelo de logit introducido en la Sección 11.2. Interpretaciones de Parámetros Ahora describimos una interpretación de los coeficientes en los modelos de logit generalizado, similar al modelo logístico. De las ecuaciones (11.6) y (11.7), tenemos \\[ \\mathrm{ln}~ \\frac{\\mathrm{Pr} \\left(y_i = j \\right)} {\\mathrm{Pr} \\left(y_i = c \\right)} = V_{i,j} - V_{i,c} =\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta_j . \\] El lado izquierdo de esta ecuación se interpreta como el logaritmo de las probabilidades de elegir la opción \\(j\\) en comparación con la opción \\(c\\). Por lo tanto, podemos interpretar \\(\\boldsymbol \\beta_j\\) como el cambio proporcional en la razón de probabilidad. Los logits generalizados tienen una interesante estructura anidada que exploraremos brevemente en la Sección 11.5.3. Es decir, es fácil comprobar que, condicional a no elegir la primera categoría, la forma de Pr(\\(y_i = j| y_i \\neq 1\\)) tiene una forma de logit generalizado en la ecuación (11.7). Además, si \\(j\\) y \\(h\\) son alternativas diferentes, observamos que \\[\\begin{eqnarray*} \\mathrm{Pr}(y_i = j| y_i=j ~\\mathrm{or}~ y_i=h) &amp;=&amp;\\frac{\\mathrm{Pr}(y_i = j)}{\\mathrm{Pr}(y_i = j)+\\mathrm{Pr}(y_i = h)} =\\frac{\\mathrm{exp}(V_{i,j})}{\\mathrm{exp}(V_{i,j})+\\mathrm{exp}(V_{i,h})} \\\\ &amp;=&amp;\\frac{1}{1+\\mathrm{exp}(\\mathbf{x}_i^{\\prime}(\\boldsymbol \\beta _h - \\boldsymbol \\beta_j))} . \\end{eqnarray*}\\] Esto tiene una forma de logit que se introdujo en la Sección 11.2. Caso Especial - Modelo con solo intercepto. Para desarrollar intuición, ahora consideramos el modelo con solo interceptos. Así, sea \\(\\mathbf{x}_i = 1\\) y \\(\\boldsymbol \\beta_j = \\beta_{0,j} = \\alpha_j\\). Con la convención de que \\(\\alpha_c=0\\), tenemos \\[ \\mathrm{Pr} \\left(y_i = j \\right) = \\pi_{i,j} = \\frac {e^{\\alpha_j}}{e^{\\alpha_1}+e^{\\alpha_2}+\\cdots+e^{\\alpha_{c-1}}+1} \\] y \\[ \\mathrm{ln}~ \\frac{\\mathrm{Pr} \\left(y_i = j \\right)} {\\mathrm{Pr} \\left(y_i = c \\right)} = \\alpha_j. \\] A partir de la segunda relación, podemos interpretar el \\(j\\)-ésimo intercepto \\(\\alpha_j\\) como las probabilidades logarítmicas de elegir la alternativa \\(j\\) en comparación con la alternativa \\(c\\). Ejemplo: Seguridad Laboral - Continuación. Esta es una continuación del ejemplo de la Sección 11.2 sobre los determinantes de la rotación laboral, basado en el trabajo de Valletta (1999). El primer análisis de estos datos consideró solo la variable dependiente binaria de despido, ya que este resultado es la principal fuente de inseguridad laboral. Valletta (1999) también presentó resultados de un modelo logit generalizado, su principal motivación fue que la teoría económica que describe la rotación laboral implica que otras razones para dejar un trabajo pueden afectar las probabilidades de despido. Para el modelo logit generalizado, la variable de respuesta tiene \\(c = 5\\) categorías: despido, dejó el trabajo debido al cierre de plantas, “renunció”, cambió de trabajo por otras razones y no hubo cambio en el empleo. La categoría de “no hubo cambio en el empleo” es la omitida en Tabla 11.6. Las variables explicativas del logit generalizado son las mismas que en la regresión probit; las estimaciones resumidas en la Tabla 11.1 se reproducen aquí para mayor conveniencia. Tabla 11.6 muestra que la rotación disminuye a medida que aumenta la antigüedad. Para ilustrar, considere a un hombre típico en la muestra de 1992 donde tenemos tiempo = 16 y nos enfocamos en las probabilidades de despido. Para este valor de tiempo, el coeficiente asociado con la antigüedad para el despido es -0.221 + 16 (0.008) = -0.093 (debido al término de interacción). A partir de esto, interpretamos que un año adicional de antigüedad implica que la probabilidad de despido es exp(-0.093) = 91% de lo que sería de otra manera, lo que representa una disminución del 9%. Tabla 11.6 también muestra que los coeficientes generalizados asociados con el despido son similares a los ajustes del modelo probit. Los errores estándar también son cualitativamente similares, aunque más altos para los logit generalizados en comparación con el modelo probit. En particular, nuevamente vemos que el coeficiente asociado con la interacción entre la antigüedad y la tendencia del tiempo revela una tasa de despido en aumento para los trabajadores con experiencia. Lo mismo ocurre con la tasa de renuncia. Tabla 11.6. Estimaciones de Regresión Logit Generalizado y Probit para la Rotación Laboral \\[ \\small{ \\begin{array}{lcrrrr} \\hline &amp; \\textbf{Probit} &amp; &amp;\\textbf{Logit} &amp;\\textbf{Generalizado} &amp;\\textbf{Modelo} \\\\ &amp; \\text{Regresión} &amp; &amp; \\text{Cierre de} &amp; \\text{Otras} &amp; \\\\ \\text{Variable} &amp; \\text{Modelo} &amp; \\text{Despido} &amp; \\text{Plantas} &amp; \\text{razones} &amp; \\text{Renuncia} \\\\ &amp; \\text{(Despido)}\\\\ \\hline \\text{Antigüedad} &amp; -0.084 &amp; -0.221 &amp; -0.086 &amp; -0.068 &amp; -0.127 \\\\ &amp; (0.010) &amp; (0.025) &amp; (0.019) &amp; (0.020) &amp; (0.012) \\\\ \\text{Tendencia Temporal} &amp; -0.002 &amp; -0.008 &amp; -0.024 &amp; 0.011 &amp; -0.022 \\\\ &amp; (0.005) &amp; (0.011) &amp; (0.016) &amp; (0.013) &amp; (0.007)\\\\ \\text{Antigüedad (Tendencia Temporal)} &amp; 0.003 &amp; 0.008 &amp; 0.004 &amp; -0.005 &amp; 0.006 \\\\ &amp; (0.001) &amp; (0.002) &amp; (0.001) &amp; (0.002) &amp; (0.001) \\\\ \\text{Cambio en Logaritmo} &amp; 0.094 &amp; 0.286 &amp; 0.459 &amp; -0.022 &amp; 0.333 \\\\ ~~\\text{de Empleo en el Sector} &amp; (0.057) &amp; (0.123) &amp; (0.189) &amp; (0.158) &amp; (0.082) \\\\ \\text{Antigüedad (Cambio en Logaritmo} &amp; -0.020 &amp; -0.061 &amp; -0.053 &amp; -0.005 &amp; -0.027 \\\\ ~~\\text{de Empleo en el Sector)} &amp; (0.009) &amp; (0.023) &amp; (0.025) &amp; (0.025) &amp; (0.012) \\\\ \\hline \\end{array} } \\] Notas: Errores estándar en paréntesis. La categoría omitida es la de no cambio en el empleo para el logit generalizado. Otras variables controladas incluyen educación, estado civil, número de hijos, raza, años de experiencia laboral a tiempo completo y su cuadrado, membresía sindical, empleo en el gobierno, salario logarítmico, la tasa de empleo en EE.UU. y ubicación. 11.5.2 Logit Multinomial Similar al ecuación (11.6), una combinación lineal alternativa de las variables explicativas es \\[\\begin{equation} V_{i,j} = \\mathbf{x}_{i,j}^{\\prime} \\boldsymbol \\beta, \\tag{11.8} \\end{equation}\\] donde \\(\\mathbf{x}_{i,j}\\) es un vector de variables explicativas que depende de la \\(j\\)-ésima alternativa mientras que los parámetros \\(\\boldsymbol \\beta\\) no. Utilizando las expresiones en las ecuaciones (11.7) y (11.8) se forma la base del modelo logit multinomial, también conocido como el modelo logit condicional (McFadden, 1974). Con esta especificación, la log-verosimilitud total es \\[ L = \\sum_{i=1}^n \\sum_{j=1}^c y_{i,j}~ \\mathrm{ln}~ \\pi_{i,j} = \\sum_{i=1}^n \\left[ \\sum_{j=1}^c y_{i,j} \\mathbf{x}_{i,j}^{\\prime} \\boldsymbol \\beta \\ - \\mathrm{ln} \\left(\\sum_{k=1}^c \\mathrm{exp}(\\mathbf{x}_{i,k}^{\\prime} \\boldsymbol \\beta) \\right) \\right]. \\] Esta expresión directa para la verosimilitud permite realizar fácilmente la inferencia por máxima verosimilitud. El modelo logit generalizado es un caso especial del modelo logit multinomial. Para ver esto, considere las variables explicativas \\(\\mathbf{x}_i\\) y los parámetros \\(\\boldsymbol \\beta_j\\), cada uno de dimensión \\(k\\times 1\\). Defina \\[ \\mathbf{x}_{i,j} = \\left( \\begin{array}{c} \\mathbf{0} \\\\ \\vdots \\\\ \\mathbf{0} \\\\ \\mathbf{x}_i \\\\ \\mathbf{0} \\\\ \\vdots \\\\ \\mathbf{0} \\\\ \\end{array}\\right) ~~~ \\mathrm{y}~~~ \\boldsymbol \\beta = \\left( \\begin{array}{c} \\boldsymbol \\beta_1 \\\\ \\boldsymbol \\beta_2 \\\\ \\vdots \\\\ \\boldsymbol \\beta_c \\\\ \\end{array} \\right). \\] Específicamente, \\(\\mathbf{x}_{i,j}\\) se define como \\(j-1\\) vectores nulos (cada uno de dimensión \\(k\\times 1\\)), seguido por \\(\\mathbf{x}_i\\) y luego seguido por \\(c-j\\) vectores nulos. Con esta especificación, tenemos \\(\\mathbf{x}_{i,j}^{\\prime} \\boldsymbol \\beta =\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta_j\\). Así, un paquete estadístico que realiza estimaciones logit multinomiales también puede realizar estimaciones logit generalizadas mediante la codificación apropiada de variables explicativas y parámetros. Otra consecuencia de esta conexión es que algunos autores usan el término logit multinomial cuando se refieren al modelo logit generalizado. Además, mediante esquemas de codificación similares, los modelos logit multinomiales también pueden manejar combinaciones lineales de la forma: \\[ V_i = \\mathbf{x}_{i,1,j}^{\\prime} \\boldsymbol \\beta + \\mathbf{x}_{i,2}^{\\prime} \\boldsymbol \\beta_j . \\] Aquí, \\(\\mathbf{x}_{i,1,j}\\) son variables explicativas que dependen de la alternativa, mientras que \\(\\mathbf{x}_{i,2}\\) no. De manera similar, \\(\\boldsymbol \\beta_j\\) son parámetros que dependen de la alternativa, mientras que \\(\\boldsymbol \\beta\\) no. Este tipo de combinación lineal es la base de un modelo logit mixto. Al igual que con los logits condicionales, es común elegir un conjunto de parámetros como base y especificar \\(\\boldsymbol \\beta_c = \\mathbf{0}\\) para evitar redundancias. Para interpretar los parámetros del modelo logit multinomial, podemos comparar las alternativas \\(h\\) y \\(k\\) usando las ecuaciones (11.7) y (11.8), obteniendo \\[ \\mathrm{ln}~ \\frac{\\mathrm{Pr} \\left(y_i = h \\right)} {\\mathrm{Pr} \\left(y_i = k \\right)} = (\\mathbf{x}_{i,h}-\\mathbf{x}_{i,k}) ^{\\prime} \\boldsymbol \\beta . \\] Así, podemos interpretar \\(\\beta_j\\) como el cambio proporcional en la razón de probabilidades, donde el cambio es el valor de la \\(j\\)-ésima variable explicativa, pasando de la alternativa \\(k\\) a la \\(h\\). Con la ecuación (11.7), note que \\(\\pi_{i,1} / \\pi_{i,2} = \\mathrm{exp}(V_{i,1}) /\\mathrm{exp}(V_{i,2})\\). Esta razón no depende de los valores subyacentes de las otras alternativas, \\(V_{i,j}\\), para \\(j=3, \\ldots, c\\). Esta característica, llamada independencia de alternativas irrelevantes, puede ser una desventaja del modelo logit multinomial para algunas aplicaciones. Ejemplo: Elección de Seguro de Salud. Para ilustrar, Browne y Frees (2007) examinaron \\(c=4\\) opciones de seguro de salud, que consistían en: \\(y=1\\) - un individuo cubierto por seguro grupal, \\(y=2\\) - un individuo cubierto por seguro privado, no grupal, \\(y=3\\) - un individuo cubierto por seguro gubernamental, pero no privado, o \\(y=4\\) - un individuo no cubierto por seguro de salud. Sus datos sobre cobertura de seguro de salud provinieron del suplemento de marzo de la Encuesta de Población Actual (CPS, por sus siglas en inglés), realizada por la Oficina de Estadísticas Laborales. Browne y Frees (2007) analizaron aproximadamente 10,800 hogares de personas solteras por año, cubriendo de 1988 a 1995, lo que dio un total de \\(n=86,475\\) observaciones. Examinaron si las restricciones de suscripción, leyes aprobadas para prohibir que los aseguradores discriminen, facilitan o desalientan el consumo de seguros de salud. Se centraron en las leyes de discapacidad que prohibían a los aseguradores utilizar la discapacidad física como criterio de suscripción. La Tabla 11.7 sugiere que las leyes de discapacidad tienen poco efecto en el comportamiento promedio de compra de seguro de salud. Para ilustrar, para los individuos encuestados con las leyes de discapacidad en vigor, el 57.6% compró seguro de salud grupal en comparación con el 59.3% de aquellos donde las restricciones no estaban en vigor. De manera similar, el 19.9% no tenía seguro cuando las restricciones de discapacidad estaban en vigor en comparación con el 20.1% cuando no lo estaban. En términos de probabilidades, cuando las restricciones de discapacidad estaban en vigor, las probabilidades de comprar un seguro de salud grupal en comparación con quedar sin seguro son 57.6/19.9 = 2.895. Cuando las restricciones de discapacidad no estaban en vigor, las probabilidades son 2.946. La razón de probabilidades, 2.895/2.946 = 0.983, indica que hay poco cambio en las probabilidades cuando se compara si las restricciones de discapacidad estaban en vigor o no. knitr::kable(2, caption = &quot;Silly. Create a table just to update the counter...&quot;) Tabla 11.6: Silly. Create a table just to update the counter… x 2 Tabla 11.7: Porcentajes de Cobertura de Salud por Variable de Ley Ley de Discapacidad en Vigor Número Sin Seguro No Grupal Gubernamental Grupal Prob. Comparativa Grupal vs. Sin Seguro Razón de Probabilidades No 82246 20.1 12.2 8.4 59.3 2.946 Sí 4229 19.9 10.1 12.5 57.6 2.895 0.983 Total 86475 20.1 12.1 8.6 59.2 En contraste, la Tabla 11.8 sugiere que las leyes de discapacidad pueden tener efectos importantes en el comportamiento promedio de compra de seguro de salud de subgrupos seleccionados de la muestra. La Tabla 11.8 muestra el porcentaje de personas sin seguro y las probabilidades de adquirir un seguro grupal (comparado con estar sin seguro) para subgrupos seleccionados. Para ilustrar, para personas con discapacidad, las probabilidades de adquirir un seguro grupal son 1.329 veces más altas cuando las restricciones de discapacidad están en vigor. La Tabla 11.7 sugiere que las restricciones de discapacidad no tienen efecto; esto puede ser cierto al observar toda la muestra. Sin embargo, al examinar subgrupos, la Tabla 11.8 muestra que podemos ver efectos importantes asociados con las restricciones legales de suscripción que no son evidentes al observar los promedios en toda la muestra. Tabla 11.8: Probabilidades de Cobertura de Salud por Ley y Discapacidad Física Subgrupos Seleccionados Ley de Discapacidad en Vigor Número Porcentaje Grupal Porcentaje Sin Seguro Prob. Comparativa Grupal vs. Sin Seguro Razón de Probabilidades Sin Discapacidad No 72150 64.2 20.5 3.134 Sin Discapacidad Sí 3649 63.4 21.2 2.985 0.952 Con Discapacidad No 10096 24.5 17.6 1.391 Con Discapacidad Sí 580 21 11.4 1.848 1.329 Hay muchas formas de seleccionar subgrupos de interés. Con un gran conjunto de datos de \\(n=86,475\\) observaciones, probablemente se podrían elegir subgrupos para confirmar casi cualquier hipótesis. Además, existe la preocupación de que los datos de la CPS pueden no proporcionar una muestra representativa de las poblaciones estatales. Por lo tanto, es habitual utilizar técnicas de regresión para “controlar” las variables explicativas, como la discapacidad física. La Tabla 11.9 presenta los principales resultados de un modelo logit multinomial con muchas variables de control incluidas. Se incluyó una variable dummy para cada uno de los 50 estados (el Distrito de Columbia es un “estado” en este conjunto de datos, por lo que necesitamos \\(51-1=50\\) variables dummy). Estas variables fueron sugeridas en la literatura y son descritas con más detalle en Browne y Frees (2007). Incluyen el género del individuo, estado civil, raza, nivel educativo, si es trabajador independiente o no y si el individuo trabajó a tiempo completo, a tiempo parcial o no trabajó. En la Tabla 11.9, “Ley” se refiere a la variable binaria que es 1 si una restricción legal estaba en vigor y “Discapacidad” es una variable binaria que es 1 si un individuo tiene una discapacidad física. Por lo tanto, la interacción “Ley*Discapacidad” informa sobre el efecto de una restricción legal en una persona con discapacidad física. La interpretación es similar a la de la Tabla 11.8. Específicamente, interpretamos el coeficiente 1.419 como que las personas con discapacidad son un 41.9% más propensas a adquirir un seguro de salud grupal comparado con no adquirir ningún seguro, cuando la restricción de suscripción por discapacidad está en vigor. De manera similar, las personas sin discapacidad son un 21.2% (\\(=1/0.825 - 1\\)) menos propensas a adquirir un seguro de salud grupal comparado con no adquirir ningún seguro, cuando la restricción de suscripción por discapacidad está en vigor. Este resultado sugiere que las personas sin discapacidad son más propensas a quedar sin seguro como resultado de las prohibiciones sobre el uso del estado de discapacidad como criterio de suscripción. En general, los resultados son estadísticamente significativos, confirmando que esta restricción legal tiene un impacto en el consumo de seguros de salud. Tabla 11.9: Razones de Probabilidades del Modelo de Regresión Logit Multinomial Grupal vs. Sin Seguro No Grupal vs. Sin Seguro Gubernamental vs. Sin Seguro Grupal vs. No Grupal Grupal vs. Gubernamental No Grupal vs. Gubernamental Ley \\(\\times\\) Sin Discapacidad 0.825 1.053 1.010 0.784 0.818 1.043 \\(p\\)-Valor 0.001 0.452 0.900 0.001 0.023 0.677 Ley \\(\\times\\) Con Discapacidad 1.419 0.953 1.664 1.490 0.854 0.573 \\(p\\)-Valor 0.062 0.789 0.001 0.079 0.441 0.001 Notas: La regresión incluye 150 (\\(=50 \\times 3\\)) efectos específicos por estado, varias variables continuas (edad, educación e ingresos, así como términos de orden superior) y variables categóricas (como raza y año). 11.5.3 Logit Anidado Para mitigar el problema de la independencia de alternativas irrelevantes en los logits multinomiales, ahora introducimos un tipo de modelo jerárquico conocido como un logit anidado. Para interpretar el modelo logit anidado, en la primera etapa se elige una alternativa (digamos la primera alternativa) con probabilidad \\[\\begin{equation} \\pi_{i,1} = \\mathrm{Pr}(y_i = 1) = \\frac{\\mathrm{exp}(V_{i,1})}{\\mathrm{exp}(V_{i,1})+ \\left[ \\sum_{k=2}^c \\mathrm{exp}(V_{i,k}/ \\rho) \\right]^{\\rho}} . \\tag{11.9} \\end{equation}\\] Luego, condicionado a no elegir la primera alternativa, la probabilidad de elegir cualquiera de las otras alternativas sigue un modelo logit multinomial con probabilidades \\[\\begin{equation} \\frac{\\pi_{i,j}}{1-\\pi_{i,1}} = \\mathrm{Pr}(y_i = j | y_i \\neq 1) = \\frac{\\mathrm{exp}(V_{i,j}/ \\rho)}{\\sum_{k=2}^c \\mathrm{exp}(V_{i,k}/ \\rho) }, ~~~j=2, \\ldots, c . \\tag{11.10} \\end{equation}\\] En las ecuaciones (11.9) y (11.10), el parámetro \\(\\rho\\) mide la asociación entre las elecciones \\(j = 2, \\ldots, c\\). El valor de \\(\\rho=1\\) se reduce al modelo logit multinomial, lo que interpretamos como independencia de alternativas irrelevantes. También interpretamos Prob(\\(y_i = 1\\)) como un promedio ponderado de los valores de la primera elección y las otras. Condicionado a no elegir la primera categoría, la forma de \\(\\mathrm{Pr}(y_i = j| y_i \\neq 1)\\) en la ecuación (11.10) tiene la misma forma que el logit multinomial. La ventaja del logit anidado es que generaliza el modelo logit multinomial de manera que ya no tenemos el problema de la independencia de alternativas irrelevantes. Una desventaja, señalada por McFadden (1981), es que solo se observa una elección; por lo tanto, no sabemos qué categoría pertenece a la primera etapa de la anidación sin una teoría adicional sobre el comportamiento de elección. No obstante, el logit anidado generaliza el logit multinomial al permitir estructuras de “dependencia” alternativas. Es decir, se puede ver el logit anidado como una alternativa robusta al logit multinomial y examinar cada una de las categorías en la primera etapa de la anidación. 11.6 Variables Dependientes Ordinales Ahora consideramos una respuesta que es una variable categórica ordenada, también conocida como una variable dependiente ordinal. Para ilustrar, cualquier tipo de respuesta de encuesta en la que se califique la impresión en una escala de siete puntos que va desde “muy insatisfecho” hasta “muy satisfecho” es un ejemplo de una variable ordinal. Ejemplo: Elección de Plan de Salud. Pauly y Herring (2007) examinaron \\(c=4\\) opciones de tipos de planes de salud, que consistían en: \\(y=1\\) - una organización para el mantenimiento de la salud (HMO), \\(y=2\\) - un plan de punto de servicio (POS), \\(y=3\\) - una organización de proveedores preferidos (PPO) o \\(y=4\\) - un plan de pago por servicio (FFS). Un plan FFS es el menos restrictivo, permitiendo a los afiliados ver a los proveedores de atención médica (como médicos de atención primaria) por una tarifa que refleja el costo de los servicios prestados. El plan PPO es el siguiente menos restrictivo; este plan generalmente utiliza pagos FFS pero los afiliados generalmente deben elegir de una lista de “proveedores preferidos”. Pauly y Herring (2007) consideraron los planes POS y HMO como el tercer y cuarto menos restrictivo, respectivamente. Un HMO a menudo utiliza la capitación (una tarifa fija por persona) para reembolsar a los proveedores, restringiendo a los afiliados a una red de proveedores. En contraste, un plan POS da a los afiliados la opción de ver a proveedores fuera de la red de HMO (por una tarifa adicional). 11.6.1 Logit Acumulativo Los modelos de variables dependientes ordinales se basan en probabilidades acumulativas de la forma \\[ \\mathrm{Pr} ( y \\le j ) = \\pi_1 + \\cdots + \\pi_j, ~ ~ j=1, \\ldots, c . \\] En esta sección, usamos logits acumulativos \\[\\begin{equation} \\mathrm{logit}\\left(\\mathrm{Pr} ( y \\le j ) \\right) = \\mathrm{ln} \\left(\\frac{\\Pr ( y \\le j )}{1-\\Pr ( y \\le j )} \\right) = \\mathrm{ln} \\left(\\frac{\\pi_1 + \\cdots + \\pi_j}{\\pi_{j+1} + \\cdots + \\pi_c} \\right) . \\tag{11.11} \\end{equation}\\] El modelo de logit acumulativo más simple es \\[ \\mathrm{logit}\\left(\\Pr ( y \\le j ) \\right) = \\alpha_j \\] que no utiliza ninguna variable explicativa. Los parámetros de “punto de corte” \\(\\alpha_j\\) son no decrecientes, de modo que \\(\\alpha_1 \\le \\alpha_2 \\le \\ldots \\le \\alpha_c,\\) reflejando la naturaleza acumulativa de la función de distribución \\(\\mathrm{Pr} ( y \\le j )\\). El modelo de razones proporcionales incorpora variables explicativas. Con este modelo, los logits acumulativos se expresan como \\[\\begin{equation} \\mathrm{logit}\\left(\\Pr ( y \\le j ) \\right) = \\alpha_j + \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta . \\tag{11.12} \\end{equation}\\] Este modelo proporciona interpretaciones de los parámetros similares a las descritas para la regresión logística en la Sección 11.4. Por ejemplo, si la variable \\(x_1\\) es continua, entonces como en la ecuación (11.1) tenemos \\[ \\beta_1 = \\frac{\\partial }{\\partial x_{i1}}\\left( \\alpha_j + \\mathbf{x}_i^{\\prime}\\boldsymbol \\beta \\right) = \\frac{\\frac{\\partial }{\\partial x_{i1}}\\Pr (y_i \\le j|\\mathbf{x}_i)/\\left( 1-\\Pr (y_ile j|\\mathbf{x}_i)\\right) }{\\Pr (y_ile j|\\mathbf{x}_i)/\\left( 1-\\Pr (y_ile j|\\mathbf{x}_i)\\right) }. \\] Por lo tanto, podemos interpretar \\(\\beta_1\\) como el cambio proporcional en la razón de probabilidades acumulativas. Ejemplo: Elección de Plan de Salud - Continuación. Pauly y Herring utilizaron datos de las Encuestas del Estudio de Seguimiento Comunitario de Hogares (CTS-HS) de 1996-1997 y 1998-1999 para estudiar la demanda de seguros de salud. Esta es una encuesta representativa a nivel nacional que contiene más de 60,000 individuos por período. Como una medida de la demanda, Pauly y Herring examinaron la elección de plan de salud, razonando que los individuos que eligieron (a través de empleo o membresía en una asociación) planes menos restrictivos buscaban una mayor protección para la atención médica. (También analizaron otras medidas, incluyendo el número de restricciones impuestas a los planes y la cantidad de compartición de costos.) Tabla 11.10 proporciona los determinantes de la elección de plan de salud basado en \\(n=34,486\\) individuos que tenían seguro de salud grupal, de entre 18 y 64 años sin seguro público. Pauly y Herring también compararon estos resultados con aquellos que tenían seguro de salud individual para entender las diferencias en los determinantes entre estos dos mercados. Tabla 11.10. Modelo de Logit Acumulativo para la Elección de Plan de Salud \\[ \\small{ \\begin{array}{llll} \\hline \\textbf{Variable} &amp; \\textbf{Razón de } &amp; \\textbf{Variable} &amp; \\textbf{Razón de} \\\\ &amp; \\textbf{ Probabilidades} &amp; &amp; \\textbf{Probabilidades} \\\\ \\hline \\text{Edad} &amp; 0.992^{***} &amp; \\text{Hispano} &amp; 1.735^{***} \\\\ \\text{Mujer} &amp; 1.064^{***} &amp; \\text{Tomador de riesgos} &amp; 0.967 \\\\ \\text{Tamaño de la familia} &amp; 0.985 &amp; \\text{Fumador} &amp; 1.055^{***} \\\\ \\text{Ingresos familiares} &amp; 0.963^{***} &amp; \\text{Salud regular/mala} &amp; 1.056 \\\\ \\text{Educación} &amp; 1.006 &amp; \\alpha_1 &amp; 0.769^{***} \\\\ \\text{Asiático} &amp; 1.180^{***} &amp; \\alpha_2 &amp; 1.406^{***} \\\\ \\text{Afroamericano} &amp; 1.643^{***} &amp; \\alpha_3 &amp; 12.089^{***} \\\\ \\text{R^2 Máximo Rescalado} &amp; 0.102 &amp; \\\\ \\hline \\end{array} } \\] Notas: Fuente: Pauly y Herring (2007). \\(^{***}\\) indica que los \\(p\\)-valores asociados son menores a 0.01. Para la raza, Caucásico es la variable omitida. Para interpretar las razones de probabilidades en Tabla 11.10, primero notamos que las estimaciones de los puntos de corte, correspondientes a \\(\\alpha_1,\\) \\(\\alpha_2\\) y \\(\\alpha_3\\), aumentan a medida que las opciones se vuelven menos restrictivas, como se anticipaba. Para el género, vemos que las probabilidades estimadas para las mujeres son 1.064 veces las de los hombres en la dirección de elegir un plan de salud menos restrictivo. Controlando por otras variables, las mujeres tienen más probabilidades de elegir planes menos restrictivos que los hombres. De manera similar, los más jóvenes, menos adinerados, no caucásicos y fumadores tienen más probabilidades de elegir planes menos restrictivos. Los coeficientes asociados con el tamaño de la familia, educación, toma de riesgos y salud auto-reportada no fueron estadísticamente significativos en este modelo ajustado. 11.6.2 Probit Acumulativo Como en la Sección 11.2.2 para la regresión logística, los modelos de logit acumulativo tienen una interpretación de umbral. Específicamente, sea \\(y_i^{\\ast}\\) una variable latente, no observada, aleatoria sobre la cual basamos la variable dependiente observada como \\[ y_i=\\left\\{ \\begin{array}{cc} 1 &amp; y_i^{\\ast} \\le \\alpha_1 \\\\ 2 &amp; \\alpha_1 &lt; y_i^{\\ast} \\le \\alpha_2 \\\\ \\vdots &amp; \\vdots \\\\ c-1 &amp; \\alpha_{c-2} &lt; y_i^{\\ast} \\le \\alpha_{c-1} \\\\ c &amp; \\alpha_{c-1} &lt; y_i^{\\ast}\\\\ \\end{array} \\right. . \\] Si \\(y_i^{\\ast} - \\mathbf{x}_i^{\\prime}\\boldsymbol \\beta\\) tiene una distribución logística, entonces \\[ \\Pr(y_i^{\\ast} - \\mathbf{x}_i^{\\prime}\\boldsymbol \\beta \\le a)=\\frac{1}{1+\\exp (-a)} \\] y por lo tanto \\[ \\Pr(y_i \\le j ) = \\Pr(y_i^{\\ast} \\le \\alpha_j) =\\frac{1}{1+\\exp \\left( -(\\alpha_j - \\mathbf{x}_i^{\\prime}\\boldsymbol \\beta) \\right)}. \\] Aplicando la transformación logit a ambos lados se obtiene la ecuación (11.12). Alternativamente, asumamos que \\(y_i^{\\ast} - \\mathbf{x}_i^{prime}\\boldsymbol \\beta\\) tiene una distribución normal estándar. Entonces, \\[ \\Pr(y_i \\le j ) = \\Pr(y_i^{\\ast} \\le \\alpha_j) =\\Phi \\left( \\alpha_j - \\mathbf{x}_i^{\\prime}\\boldsymbol \\beta \\right). \\] Este es el modelo de probit acumulativo. Al igual que en los modelos de variables binarias, el probit acumulativo da resultados que son similares al modelo logit acumulativo. 11.7 Lecturas Adicionales y Referencias Los modelos de regresión de variables binarias se utilizan ampliamente. Para introducciones más detalladas, consulte Hosmer y Lemshow (1989) o Agresti (1996). También puede examinar tratamientos más rigurosos como los de Agresti (1990) y Cameron y Trivedi (1998). El trabajo de Agresti (1990, 1996) discute variables dependientes multicategoría, al igual que el tratamiento avanzado de econometría en Amemiya (1985). Referencias del Capítulo Agresti, Alan (1990). Categorical Data Analysis. Wiley, New York. Agresti, Alan (1996). An Introduction to Categorical Data Analysis. Wiley, New York. Amemiya, Takeshi (1985). Advanced Econometrics. Harvard University Press, Cambridge, Massachusetts. Browne, Mark J. and Edward W. Frees (2007). Prohibitions on health insurance underwriting. Working paper. Cameron, A. Colin and Pravin K. Trivedi (1998). Regression Analysis of Count Data. Cambridge University Press, Cambridge. Carroll, Raymond J. and David Ruppert (1988). Transformation and Weighting in Regression. Chapman-Hall. Gourieroux, Christian and Joann Jasiak (2007). The Econometrics of Individual Risk. Princeton University Press, Princeton. Hand, D.J. and W. E. Henley (1997). Statistical classification methods in consumer credit scoring: A review. Journal of the Royal Statistical Society A, 160(3), 523-541. Hosmer, David W. and Stanley Lemeshow (1989). Applied Logistic Regression. Wiley, New York. Pauly, Mark V. and Bradley Herring (2007). The demand for health insurance in the group setting: Can you always get what you want? Journal of Risk and Insurance 74, 115-140. Smith, Richard M. and Phyllis Schumacher (2006). Academic attributes of college freshmen that lead to success in actuarial studies in a business college. Journal of Education for Business 81(5), 256-260. Valletta, R. G. (1999). Declining job security. Journal of Labor Economics 17, S170-S197. Wiginton, John C. (1980). A note on the comparison of logit and discriminant models of consumer credit behavior. Journal of Financial and Quantitative Analysis 15(3), 757-770. 11.8 Ejercicios 11.1 Similitud entre Logit y Probit. Suponga que la variable aleatoria \\(y^{\\ast}\\) tiene una función de distribución logit, \\(\\Pr(y^{\\ast} le y) = \\mathrm{F}(y) = e^y/(1+e^y).\\) Calcule la correspondiente función de densidad de probabilidad. Utilice la función de densidad de probabilidad para calcular la media (\\(\\mu_y\\)). Calcule la desviación estándar correspondiente (\\(\\sigma_y\\)). Defina la variable aleatoria reescalada \\(y^{\\ast \\ast} =\\frac{y^{\\ast}-\\mu_y}{\\sigma_y}.\\) Determine la función de densidad de probabilidad para \\(y^{\\ast \\ast}\\). Trace la función de densidad de probabilidad en la parte (d). Superponga este gráfico con uno de una función de densidad de probabilidad normal estándar. (Esto proporciona una versión de la función de densidad de las gráficas de la función de distribución en la Figura 11.1.) 11.2 Interpretación de umbral del modelo de regresión probit. Considere un modelo lineal subyacente, \\(y_i^{\\ast }=\\mathbf{x}_i^{\\mathbf{ \\prime }}\\boldsymbol \\beta+\\epsilon_i^{\\ast }\\), donde \\(\\epsilon_i^{\\ast }\\) está distribuido normalmente con media cero y varianza \\(\\sigma ^{2}\\). Defina \\(y_i=\\mathrm{I}(y_i^{\\ast }&gt;0),\\) donde I(\\(\\cdot\\)) es la función indicadora. Muestre que \\(\\pi_i=\\Pr (y_i=1|\\mathbf{x}_i)\\) \\(=\\mathrm{\\Phi }(\\mathbf{x}_i^{\\mathbf{\\prime }}\\mathbf{\\beta /\\sigma })\\), donde \\(\\mathrm{\\Phi }(\\cdot)\\) es la función de distribución normal estándar. 11.3 Interpretación de utilidad aleatoria del modelo de regresión logística. Bajo la interpretación de utilidad aleatoria, un individuo con utilidad $ U_{ij}=u_i(V_{ij}+_{ij})$, donde \\(j\\) puede ser 1 o 2, selecciona la categoría correspondiente a \\(j=1\\) con probabilidad \\[\\begin{eqnarray*} \\pi_i &amp;=&amp; \\Pr (y_i =1)=\\mathrm{\\Pr }(U_{i2}&lt;U_{i1}) \\\\ &amp;=&amp;\\mathrm{\\Pr }(\\epsilon _{i2}-\\epsilon _{i1}&lt;V_{i1}-V_{i2}). \\end{eqnarray*}\\] Como en la Sección 11.2.3, tomamos \\(V_{i2}=0\\) y \\(V_{i1}=\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta\\). Supongamos además que los errores provienen de una distribución de valor extremo de la forma \\[ \\Pr (\\epsilon_{ij}&lt;a)=\\exp (-e^{-a}). \\] Muestre que la probabilidad de elección \\(\\pi_i\\) tiene una forma logit. Es decir, muestre que \\[ \\pi_i=\\frac{1}{1+\\exp (-\\mathbf{x}_i^{\\mathbf{\\prime }}\\boldsymbol \\beta)}. \\] 11.4 Dos Poblaciones. Comience con una población y suponga que \\(y_1, \\ldots, y_n\\) es una muestra i.i.d. de una distribución Bernoulli con media \\(\\pi\\). Muestre que el estimador de máxima verosimilitud de \\(\\pi\\) es \\(\\overline{y}\\). Ahora considere dos poblaciones. Suponga que \\(y_1, \\ldots, y_{n_1}\\) es una muestra i.i.d. de una distribución Bernoulli con media \\(\\pi_1\\) y que \\(y_{n_1+1}, \\ldots, y_{n_1+n_2}\\) es una muestra i.i.d. de una distribución Bernoulli con media \\(\\pi_2\\), donde las muestras son independientes entre sí. b(i). Muestre que el estimador de máxima verosimilitud de \\(\\pi_2 - \\pi_1\\) es \\(\\overline{y}_2 - \\overline{y}_1\\). b(ii). Determine la varianza del estimador en la parte b(i). Ahora exprese el problema de las dos poblaciones en un contexto de regresión usando una variable explicativa. Específicamente, suponga que \\(x_i\\) solo toma los valores 0 y 1. De las \\(n\\) observaciones, \\(n_1\\) toman el valor \\(x=0\\). Estas \\(n_1\\) observaciones tienen un valor promedio de \\(y\\) de \\(\\overline{y}_1\\). Las restantes \\(n_2 =n-n_1\\) observaciones tienen el valor \\(x=1\\) y un valor promedio de \\(y\\) de \\(\\overline{y}_2\\). Usando el caso logit, sea \\(b_{0,MLE}\\) y \\(b_{1,MLE}\\) representen los estimadores de máxima verosimilitud de \\(\\beta_0\\) y \\(\\beta_1\\), respectivamente. c(i). Muestre que los estimadores de máxima verosimilitud satisfacen las ecuaciones \\[ \\overline{y}_1 = \\mathrm{\\pi}\\left(b_{0,MLE}\\right) \\] y \\[ \\overline{y}_2 = \\mathrm{\\pi}\\left(b_{0,MLE}+b_{1,MLE}\\right). \\] c(ii). Use la parte c(i) para mostrar que el estimador de máxima verosimilitud para \\(\\beta_1\\) es \\(\\mathrm{\\pi}^{-1}(\\overline{y}_2)-\\mathrm{\\pi}^{-1}(\\overline{y}_1)\\). c(iii). Con la notación \\(\\pi_1 = \\mathrm{\\pi}(\\beta_0)\\) y \\(\\pi_2 = \\mathrm{\\pi}(\\beta_0 +\\beta_1)\\), confirme que la matriz de información se puede expresar como \\[ \\mathbf{I}(\\beta_0, \\beta_1) = n_1 \\pi_1 (1-\\pi_1) \\left( \\begin{array}{cc} 1 &amp; 0 \\\\ 0 &amp; 0 \\\\ \\end{array} \\right) + n_2 \\pi_2 (1-\\pi_2) \\left( \\begin{array}{cc} 1 &amp; 1 \\\\ 1 &amp; 1 \\\\ \\end{array} \\right). \\] c(iv). Utilice la matriz de información para determinar la varianza en muestras grandes del estimador de máxima verosimilitud para \\(\\beta_1\\). 11.5 Valores Ajustados. Sea \\(\\widehat{y}_i = \\mathrm{\\pi }\\left( \\mathbf{x}_i^{\\prime} \\mathbf{b}_{MLE})\\right)\\) el \\(i\\)-ésimo valor ajustado para la función logit. Suponga que se utiliza un intercepto en el modelo, de modo que una de las variables explicativas \\(x\\) es una constante igual a uno. Muestre que la respuesta promedio es igual al valor ajustado promedio, es decir, muestre que \\(\\overline{y} = n^{-1} \\sum_{i=1}^n \\widehat{y}_i\\). 11.6 Comenzando con las ecuaciones de puntaje (11.4), verifique la expresión para el caso logit en la ecuación (11.5). 11.7 Matriz de Información Comenzando con la función de puntaje para el caso logit en la ecuación (11.5), muestre que la matriz de información puede ser expresada como \\[ \\mathbf{I}(\\boldsymbol \\beta) = \\sum\\limits_{i=1}^{n} \\sigma_i^2 \\mathbf{x}_i\\mathbf{x}_i^{\\mathbf{\\prime }}, \\] donde \\(\\sigma_i^2 = \\mathrm{\\pi}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta)(1-\\mathrm{\\pi}(\\mathbf{x}_i^{\\prime}\\boldsymbol \\beta))\\). Comenzando con la función de puntaje general en la ecuación (11.4), determine la matriz de información. 11.8 Reclamaciones de seguro por lesiones en automóviles. Refiérase a la descripción en el Ejercicio 1.5. Consideramos \\(n=1,340\\) reclamaciones por responsabilidad civil por lesiones corporales de un solo estado utilizando una encuesta de 2002 realizada por el Consejo de Investigación de Seguros (IRC). El IRC es una división del Instituto Americano de Chartered Property Casualty Underwriters y el Instituto de Seguros de América. La encuesta solicitó a las empresas participantes que informaran sobre reclamaciones cerradas con pago durante un período de dos semanas designado. En esta asignación, nos interesa entender las características de los demandantes que eligen ser representados por un abogado al resolver su reclamo. Las descripciones de las variables se dan en la Tabla 11.11. knitr::kable(2, caption = &quot;Silly. Create a table just to update the counter...&quot;) Tabla 11.10: Silly. Create a table just to update the counter… x 2 Tabla 11.11: Reclamaciones por Lesiones Corporales Variable \\(\\textbf{Descripción}\\) ATTORNEY si el reclamante está representado por un abogado (=1 si sí y =2 si no) CLMAGE edad del reclamante CLMSEX género del reclamante (=1 si masculino y =2 si femenino) MARITAL estado civil del reclamante (=1 si casado, =2 si soltero, =3 si viudo, y =4 si divorciado/separado) SEATBELT si el reclamante estaba usando un cinturón de seguridad/restricción infantil (=1 si sí, =2 si no, y =3 si no aplicable) CLMINSUR si el conductor del vehículo del reclamante estaba sin seguro (=1 si sí, =2 si no, y =3 si no aplicable) LOSS pérdida económica total del reclamante (en miles). Estadísticas Resumidas. Calcule histogramas y estadísticas resumidas de las variables explicativas continuas CLMAGE y LOSS. Basado en estos resultados, cree una versión logarítmica de LOSS, llamada lnLOSS. Examine las medias de CLMAGE, LOSS y lnLOSS por nivel de ATTORNEY. ¿Sugieren estas estadísticas que las variables continuas difieren según ATTORNEY? Cree tablas de conteo (o porcentajes) de ATTORNEY por nivel de CLMSEX, MARITAL, SEATBELT y CLMINSUR. ¿Sugieren estas estadísticas que las variables categóricas difieren según ATTORNEY? Identifique el número de valores faltantes para cada variable explicativa. Modelos de Regresión Logística. Ejecute un modelo de regresión logística utilizando solo la variable explicativa CLMSEX. ¿Es un factor importante para determinar el uso de un abogado? Proporcione una interpretación en términos de las probabilidades de usar un abogado. Ejecute un modelo de regresión logística utilizando las variables explicativas CLMAGE, CLMSEX, MARITAL, SEATBELT y CLMINSUR. ¿Qué variables parecen ser estadísticamente significativas? Para el modelo en la parte (ii), ¿quién usa más abogados, hombres o mujeres? Proporcione una interpretación en términos de las probabilidades de usar un abogado para la variable CLMSEX. Ejecute un modelo de regresión logística utilizando las variables explicativas CLMAGE, CLMSEX, MARITAL, SEATBELT, CLMINSUR, LOSS y lnLOSS. Decida cuál de las dos medidas de pérdida es más importante y vuelva a ejecutar el modelo utilizando solo una de estas variables. En este modelo, ¿es la medida de las pérdidas una variable estadísticamente significativa? Ejecute su modelo en la parte (iv) pero omitiendo la variable CLMAGE. Describa las diferencias entre este ajuste de modelo y el de la parte (iv), enfocándose en las variables estadísticamente significativas y en el número de observaciones utilizadas en el ajuste del modelo. Considere un reclamante masculino soltero de 32 años de edad. Suponga que el reclamante estaba usando un cinturón de seguridad, que el conductor estaba asegurado y la pérdida económica total es de $5,000. Para el modelo en la parte (iv), ¿cuál es la estimación de la probabilidad de usar un abogado? Regresión Probit. Repita la parte b(v) utilizando modelos de regresión probit, pero interprete solo el signo de los coeficientes de regresión. 11.9 Carreras de Caballos en Hong Kong. El hipódromo es un ejemplo fascinante de la dinámica de los mercados financieros en acción. Vamos a la pista y hagamos una apuesta. Supongamos que, de un campo de 10 caballos, simplemente queremos elegir un ganador. En el contexto de la regresión, dejaremos que \\(y\\) sea la variable de respuesta que indica si un caballo gana (\\(y\\) = 1) o no (\\(y\\) = 0). De los formularios de carreras, periódicos, y otras fuentes, hay muchas variables explicativas que están disponibles públicamente y que podrían ayudarnos a predecir el resultado para \\(y\\). Algunas variables candidatas pueden incluir la edad del caballo, el rendimiento reciente del caballo y el jinete, el pedigrí del caballo, y así sucesivamente. Estas variables son evaluadas por los inversores presentes en la carrera, la multitud apostadora. Al igual que en muchos mercados financieros, resulta que una de las variables explicativas más útiles es la evaluación general de la multitud sobre las habilidades del caballo. Estas evaluaciones no se hacen basadas en una encuesta de la multitud, sino más bien en función de las apuestas realizadas. La información sobre las apuestas de la multitud está disponible en un gran cartel en la carrera llamado el tote board (tablero de apuestas). El tote board muestra las probabilidades de que cada caballo gane una carrera. Tabla 11.12 es un tote board hipotético para una carrera de 10 caballos. Tabla 11.12. Tote Board Hipotético \\[ \\scriptsize{ \\begin{array}{l|cccccccccc} \\hline \\text{Caballo} &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 \\\\ \\text{Probabilidades Publicadas} &amp; 1-1 &amp; 79-1 &amp; 7-1 &amp; 3-1 &amp; 15-1 &amp; 7-1 &amp; 49-1 &amp; 49-1 &amp; 19-1 &amp; 79-1 \\\\ \\hline \\end{array} } \\] Las probabilidades que aparecen en el tote board han sido ajustadas para proporcionar una “tasa de la pista.” Es decir, por cada dólar apostado, $\\(T\\) va a la pista por patrocinar la carrera y $(1-\\(T\\)) va a los apostadores ganadores. Las tasas típicas de la pista están en el orden del veinte por ciento, o \\(T\\)=0.20. Podemos convertir fácilmente las probabilidades en el tote board en la evaluación de la multitud de las probabilidades de ganar. Para ilustrar esto, Tabla 11.13 muestra apuestas hipotéticas para ganar que resultaron en la información mostrada en el tote board hipotético en la Tabla 11.12. Tabla 11.13. Apuestas Hipotéticas \\[ \\scriptsize{ \\begin{array}{l|cccccccccc} \\hline \\text{Caballo} &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 &amp; Total\\\\ \\hline \\text{Apuestas para Ganar} &amp; 8,000 &amp; 200 &amp; 2,000 &amp; 4,000 &amp; 1,000 &amp; 3,000 &amp; 400 &amp; 400 &amp; 800 &amp; 200 &amp; 20,000 \\\\ \\text{Probabilidad} &amp; 0.40 &amp; 0.01 &amp; 0.10 &amp; 0.20 &amp; 0.05 &amp; 0.15 &amp; 0.02 &amp; 0.02 &amp; 0.04 &amp; 0.02 &amp; 1.000 \\\\ \\text{Probabilidades Publicadas} &amp; 1-1 &amp; 79-1 &amp; 7-1 &amp; 3-1 &amp; 15-1 &amp; 7-1 &amp; 49-1 &amp; 49-1 &amp; 19-1 &amp; 79-1 \\\\ \\hline \\end{array} } \\] Para esta carrera hipotética, se apostaron 20,000 para ganar. Debido a que 8,000 de estos 20,000 se apostaron en el primer caballo, interprete la relación 8000/20000 = 0.40 como la evaluación de la multitud de la probabilidad de ganar. Las probabilidades teóricas se calculan como 0.4/(1-0.4) = 2/3, o una apuesta de 0.67 gana 1. Sin embargo, las probabilidades teóricas asumen un juego justo sin tasa de la pista. Para ajustar al hecho de que solo (1-\\(T\\)) están disponibles para el ganador, las probabilidades publicadas para este caballo serían 0.4/(1-\\(T\\)-0.4) = 1, si \\(T\\)=0.20. En este caso, ahora se requiere una apuesta de 1 para ganar 1. Entonces tenemos la relación \\(probabilidades~ajustadas\\) \\(= x/(1-T-x)\\), donde \\(x\\) es la evaluación de la multitud de la probabilidad de ganar. Antes del inicio de la carrera, el tote board nos proporciona probabilidades ajustadas que pueden convertirse fácilmente en \\(x\\), la evaluación de la multitud sobre ganar. Usamos esta medida para ayudarnos a predecir \\(y\\), el evento de que el caballo realmente gane la carrera. Consideramos datos de 925 carreras realizadas en Hong Kong desde septiembre de 1981 hasta septiembre de 1989. En cada carrera, había diez caballos, uno de los cuales fue seleccionado aleatoriamente para estar en la muestra. En los datos, use FINISH = \\(y\\) como el indicador de que un caballo gane una carrera y WIN = \\(x\\) como la evaluación a priori de la multitud de la probabilidad de que un caballo gane una carrera. Un colega estadísticamente ingenuo quisiera duplicar el tamaño de la muestra eligiendo dos caballos de cada carrera en lugar de seleccionar aleatoriamente un caballo de un campo de 10. Describa la relación entre las variables dependientes de los dos caballos seleccionados. Explique cómo esto viola las suposiciones del modelo de regresión. Calcule la media de FINISH y las estadísticas resumidas de WIN. Note que la desviación estándar de FINISH es mayor que la de WIN, aunque las medias de las muestras sean aproximadamente las mismas. Para la variable FINISH, ¿cuál es la relación entre la media muestral y la desviación estándar? Calcule las estadísticas resumidas de WIN por nivel de FINISH. Note que la media muestral es mayor para los caballos que ganaron (FINISH = 1) que para aquellos que perdieron (FINISH = 0). Interprete este resultado. Estime un modelo de probabilidad lineal, utilizando WIN para predecir FINISH. ¿Es WIN un predictor estadísticamente significativo de FINISH? ¿Qué tan bien se ajusta este modelo a los datos utilizando la estadística de bondad de ajuste habitual? Para este modelo estimado, ¿es posible que los valores ajustados se encuentren fuera del intervalo [0, 1]? Note que, por definición, la variable x WIN debe estar dentro del intervalo [0, 1]. Estime un modelo de regresión logística, utilizando WIN para predecir FINISH. ¿Es WIN un predictor estadísticamente significativo de FINISH? Compare los valores ajustados de los modelos en las partes (d) y (e) Para cada modelo, proporcione valores ajustados en WIN = 0, 0.01, 0.05, 0.10 y 1.0. Trace un gráfico de los valores ajustados del modelo de probabilidad lineal versus los valores ajustados del modelo de regresión logística. Interprete WIN como la evaluación a priori de la multitud de la probabilidad de que un caballo gane una carrera. Los valores ajustados, FINISH, es su nueva estimación de la probabilidad de que un caballo gane una carrera, basada en la evaluación de la multitud. Trace el gráfico de la diferencia FINISH - WIN versus WIN. Discuta una estrategia de apuestas que podría emplear basada en la diferencia, FINISH - WIN. 11.10 Demanda de Seguro de Vida a Término. Continuamos nuestro estudio de la Demanda de Seguro de Vida a Término de los Capítulos 3 y 4. Específicamente, examinamos la Encuesta de Finanzas del Consumidor (SCF) de 2004, una muestra representativa a nivel nacional que contiene información extensa sobre activos, pasivos, ingresos y características demográficas de los encuestados (potenciales clientes de EE. UU.). Ahora volvemos a la muestra original de \\(n=500\\) familias con ingresos positivos y estudiamos si una familia compra o no seguro de vida a término. De nuestra muestra, resulta que 225 no compraron (FACEPOS=0), mientras que 275 sí compraron seguro de vida a término (FACEPOS=1). Estadísticas Resumidas. Proporcione una tabla de medias de las variables explicativas por nivel de la variable dependiente FACEPOS. Interprete lo que aprendemos de esta tabla. Modelo de Probabilidad Lineal. Ajuste un modelo de probabilidad lineal usando FACEPOS como la variable dependiente y LINCOME, EDUCATION, AGE y GENDER como variables explicativas continuas, junto con el factor MARSTAT. b(i). Defina brevemente un modelo de probabilidad lineal. b(ii). Comente sobre la calidad del modelo ajustado. b(iii). ¿Cuáles son los tres principales inconvenientes del modelo de probabilidad lineal? Modelo de Regresión Logística. Ajuste un modelo de regresión logística utilizando el mismo conjunto de variables explicativas. c(i). Identifique qué variables parecen ser estadísticamente significativas. En su identificación, describa la base para sus conclusiones. c(ii). ¿Qué medida resume la bondad de ajuste? Modelo de Regresión Logística Reducido. Defina MARSTAT1 como una variable binaria que indica MARSTAT=1. Ajuste un segundo modelo de regresión logística utilizando LINCOME, EDUCATION y MARSTAT1. d(i). Compare estos dos modelos, utilizando una prueba de razón de verosimilitud. Establezca sus hipótesis nula y alternativa, criterio de decisión y su regla de decisión. d(ii). ¿Quién es más probable que compre un seguro de vida a término, casados o no casados? Proporcione una interpretación en términos de las probabilidades de comprar un seguro de vida a término para la variable MARSTAT1. d(iii). Considere a un hombre casado que tiene 54 años. Suponga que esta persona tiene 13 años de educación, un salario anual de $70,000 y vive en un hogar compuesto por cuatro personas. Para este modelo, ¿cuál es la estimación de la probabilidad de comprar un seguro de vida a término? 11.11 Éxito en los Estudios Actuariales. Al igual que en los campos médicos y legales, los miembros de la profesión actuarial enfrentan problemas interesantes y generalmente son bien remunerados por sus esfuerzos en resolver estos problemas. También, como en las profesiones médicas y legales, las barreras educativas para convertirse en actuario son desafiantes, limitando la entrada en el campo. Para asesorar a los estudiantes sobre si tienen el potencial para cumplir con las demandas de este campo intelectualmente desafiante, Smith y Schumacher (2006) estudiaron los atributos de los estudiantes en una facultad de negocios. Específicamente, examinaron a \\(n=185\\) estudiantes de primer año en la Universidad de Bryant en Rhode Island que habían comenzado sus carreras universitarias entre 1995 y 2001. La variable dependiente de interés era si se graduaron con una concentración en actuaría, para estos estudiantes el primer paso para convertirse en actuario profesional. De estos, 77 se graduaron con una concentración en actuaría y los otros 108 abandonaron la concentración (en Bryant, la mayoría se transfirió a otras concentraciones, aunque algunos dejaron la universidad). Smith y Schumacher (2006) informaron sobre los efectos de cuatro mecanismos de evaluación temprana, así como GENDER, una variable de control. Los mecanismos de evaluación fueron: PLACE%, rendimiento en un examen de ubicación matemática administrado justo antes del primer año, MSAT y VSAT, las porciones de matemáticas (M) y verbal (V) del Scholastic Aptitude Test (SAT), y RANK, el rango en la escuela secundaria dado como una proporción (siendo más cercano a uno mejor). La Tabla 11.14 muestra que los estudiantes que eventualmente se graduaron con una concentración en actuaría obtuvieron mejores resultados en estos mecanismos de evaluación temprana que los que abandonaron la concentración actuarial. Se ajustó una regresión logística a los datos, con los resultados reportados en la Tabla 11.14. Para tener una idea de qué variables son estadísticamente significativas, calcule los \\(t\\)-ratios para cada variable. Para cada variable, indique si es o no estadísticamente significativa. Para tener una idea del impacto relativo de los mecanismos de evaluación, use los coeficientes en la Tabla 11.14 para calcular las probabilidades de éxito estimadas para las siguientes combinaciones de variables. En sus cálculos, asuma que GENDER=1. b(i). Asuma PLACE% =0.80, MSAT = 680, VSAT=570 y RANK=0.90. b(ii). Asuma PLACE% =0.60, MSAT = 680, VSAT=570 y RANK=0.90. b(iii). Asuma PLACE% =0.80, MSAT = 620, VSAT=570 y RANK=0.90. b(iv). Asuma PLACE% =0.80, MSAT = 680, VSAT=540 y RANK=0.90. b(v). Asuma PLACE% =0.80, MSAT = 680, VSAT=570 y RANK=0.70. knitr::kable(2, caption = &quot;Silly. Create a table just to update the counter...&quot;) Tabla 11.12: Silly. Create a table just to update the counter… x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Tabla 11.13: Silly. x 2 Tabla 11.14: Estadísticas Resumidas y Ajustes de Regresión Logística para Predecir Graduación Actuarial Promedio para Actuariales Regresión Logística Variable Graduados Desertores Estimación Error Estándar Intercepto -12.094 2.575 GENDER 0.256 0.407 PLACE% 0.83 0.64 4.336 1.657 MSAT 679.25 624.25 0.008 0.004 VSAT 572.2 544.25 -0.002 0.003 RANK 0.88 0.76 4.442 1.836 11.12 Caso-Control. Considere el siguiente método de selección de muestra “caso-control” para variables dependientes binarias. Intuitivamente, si estamos trabajando con un problema donde el evento de interés es raro, queremos asegurarnos de que muestreamos una cantidad suficiente de eventos para que nuestros procedimientos de estimación sean confiables. Suponga que tenemos una gran base de datos que consiste en \\(\\{y_i, \\mathbf{x}_i\\}\\), con \\(i=1,\\ldots, N\\) observaciones. (Para los registros de una compañía de seguros, \\(N\\) podría fácilmente ser de diez millones o más). Queremos asegurarnos de obtener una cantidad suficiente de \\(y_i = 1\\) (correspondiente a reclamaciones o “casos”) en nuestra muestra, más una muestra de \\(y_i = 0\\) (correspondiente a no reclamaciones o “controles”). Por lo tanto, dividimos el conjunto de datos en dos subconjuntos. Para el primer subconjunto que consiste en observaciones con \\(y_i = 1\\), tomamos una muestra aleatoria con probabilidad \\(\\tau_1\\). De manera similar, para el segundo subconjunto que consiste en observaciones con \\(y_i = 0\\), tomamos una muestra aleatoria con probabilidad \\(\\tau_0\\). Por ejemplo, en la práctica podríamos usar \\(\\tau_1=1\\) y \\(\\tau_0 = 0.10\\), lo que correspondería a tomar todas las reclamaciones y una muestra del 10% de no reclamaciones - por lo tanto, se considera que \\(\\tau_1\\) y \\(\\tau_0\\) son conocidos por el analista. Sea \\(\\{r_i = 1\\}\\) el evento que indica que la observación es seleccionada para ser parte del análisis. Determine \\(\\Pr(y_i = 1, r_i = 1)\\), \\(\\Pr(y_i = 0, r_i = 1)\\) y \\(\\Pr(r_i = 1)\\) en términos de \\(\\tau_0\\), \\(\\tau_1\\) y \\(\\pi_i = \\Pr(y_i=1)\\). Usando los cálculos en la parte (a), determine la probabilidad condicional \\(\\Pr(y_i=1 | r_i=1)\\). Ahora suponga que \\(\\pi_i\\) tiene una forma logística (\\(\\pi(z) = \\exp(z)/(1+\\exp(z))\\) y \\(\\pi_i= \\pi(\\mathbf{x}_i^{\\prime}\\boldsymbol \\beta ))\\). Reescriba su respuesta a la parte (b) usando esta forma logística. Escriba la verosimilitud de los \\(y_i\\) observados (condicional en \\(r_i = 1, i=1, \\ldots, n\\)). Muestre cómo podemos interpretar esto como la verosimilitud de una regresión logística usual con la excepción de que el intercepto ha cambiado. Especifique el nuevo intercepto en términos del intercepto original, \\(\\tau_0\\) y \\(\\tau_1\\). 11.9 Suplementos Técnicos - Inferencia Basada en Verosimilitud Comencemos con variables aleatorias \\(\\left( y_1, \\ldots, y_n \\right) ^{\\prime} = \\mathbf y\\) cuya distribución conjunta es conocida hasta un vector de parámetros \\(\\boldsymbol \\theta\\). En aplicaciones de regresión, \\(\\boldsymbol \\theta\\) consiste en los coeficientes de regresión, \\(\\boldsymbol \\beta\\), y posiblemente un parámetro de escala \\(\\sigma^2\\) así como parámetros adicionales. Esta función de densidad de probabilidad conjunta se denota como \\(\\mathrm{f}(\\mathbf{y};\\boldsymbol \\theta)\\). La función también puede ser una función de masa de probabilidad para variables aleatorias discretas o una distribución mixta para variables aleatorias que tienen componentes discretos y continuos. En cada caso, podemos usar la misma notación, \\(\\mathrm{f}(\\mathbf{y};\\boldsymbol \\theta),\\) y llamarla la función de verosimilitud. La verosimilitud es una función de los parámetros con los datos (\\(\\mathbf{y}\\)) fijos en lugar de una función de los datos con los parámetros (\\(\\boldsymbol \\theta\\)) fijos. Es habitual trabajar con la versión logarítmica de la función de verosimilitud y así definimos la función de log-verosimilitud como \\[ L(\\boldsymbol \\theta) = L(\\mathbf{y};\\boldsymbol \\theta ) = \\ln \\mathrm{f}(\\mathbf{y};\\boldsymbol \\theta), \\] evaluada en una realización de \\(\\mathbf{y}\\). En parte, esto se debe a que a menudo trabajamos con el caso especial importante donde las variables aleatorias \\(y_1, \\ldots, y_n\\) son independientes. En este caso, la función de densidad conjunta se puede expresar como un producto de las funciones de densidad marginales y, al tomar logaritmos, podemos trabajar con sumas. Incluso cuando no se trata de variables aleatorias independientes, como con datos de series temporales, a menudo es más conveniente desde el punto de vista computacional trabajar con log-verosimilitudes en lugar de la función de verosimilitud original. 11.9.1 Propiedades de las Funciones de Verosimilitud Dos propiedades básicas de las funciones de verosimilitud son: \\[\\begin{equation} \\mathrm{E} \\left( \\frac{ \\partial}{\\partial \\boldsymbol \\theta} L(\\boldsymbol \\theta) \\right) = \\mathbf 0 \\tag{11.13} \\end{equation}\\] y \\[\\begin{equation} \\mathrm{E} \\left( \\frac{ \\partial^2}{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^{\\prime}} L(\\boldsymbol \\theta) \\right) + \\mathrm{E} \\left( \\frac{ \\partial L(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta} \\frac{ \\partial L(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta^{\\prime}} \\right) = \\mathbf 0. \\tag{11.14} \\end{equation}\\] La derivada de la función de log-verosimilitud, \\(\\partial L(\\boldsymbol \\theta)/\\partial \\boldsymbol \\theta\\), se llama función de puntaje. La ecuación (11.13) muestra que la función de puntaje tiene media cero. Para ver esto, bajo condiciones de regularidad adecuadas, tenemos \\[\\begin{eqnarray*} \\mathrm{E} \\left( \\frac{ \\partial}{\\partial \\boldsymbol \\theta} L(\\boldsymbol \\theta) \\right) &amp;=&amp; \\mathrm{E} \\left( \\frac{ \\frac{\\partial}{\\partial \\boldsymbol \\theta} \\mathrm{f}(\\mathbf{y};\\boldsymbol \\theta )}{\\mathrm{f}(\\mathbf{y};\\boldsymbol \\theta )} \\right) = \\int \\frac{\\partial}{\\partial \\boldsymbol \\theta} \\mathrm{f}(\\mathbf{y};\\boldsymbol \\theta ) d \\mathbf y = \\frac{\\partial}{\\partial \\boldsymbol \\theta} \\int \\mathrm{f}(\\mathbf{y};\\boldsymbol \\theta ) d \\mathbf y \\\\ &amp;=&amp; \\frac{\\partial}{\\partial \\boldsymbol \\theta} 1 = \\mathbf 0. \\end{eqnarray*}\\] Por conveniencia, esta demostración asume una densidad para f(\\(\\cdot\\)); las extensiones a distribuciones de masa y mixtas son sencillas. La demostración de la ecuación (11.14) es similar y se omite. Para establecer la ecuación (11.13), implícitamente utilizamos “condiciones de regularidad adecuadas” para permitir el intercambio de la derivada y el signo de integral. Para ser más precisos, un analista que trabaje con un tipo específico de distribución puede utilizar esta información para verificar que el intercambio de la derivada y el signo de integral es válido. Usando la ecuación (11.14), podemos definir la matriz de información \\[\\begin{equation} \\mathbf{I}(\\boldsymbol \\theta) = \\mathrm{E} \\left( \\frac{ \\partial L(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta} \\frac{ \\partial L(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta^{\\prime}} \\right) = -\\mathrm{E} \\left( \\frac{ \\partial^2}{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^{\\prime}} L(\\boldsymbol \\theta) \\right). \\tag{11.15} \\end{equation}\\] Esta cantidad se utiliza ampliamente en el estudio de las propiedades de muestras grandes de las funciones de verosimilitud. La matriz de información aparece en la distribución de muestras grandes de la función de puntaje. Específicamente, bajo condiciones amplias, tenemos que \\(\\partial L(\\boldsymbol \\theta)/\\partial \\boldsymbol \\theta\\) tiene una distribución normal en muestras grandes con media 0 y varianza \\(\\mathbf{I}(\\boldsymbol \\theta)\\). Para ilustrar, supongamos que las variables aleatorias son independientes, de modo que la función de puntaje se puede escribir como \\[ \\frac{ \\partial}{\\partial \\boldsymbol \\theta} L(\\boldsymbol \\theta) =\\frac{ \\partial}{\\partial \\boldsymbol \\theta} \\ln \\prod_{i=1}^n \\mathrm{f}(y_i;\\boldsymbol \\theta ) =\\sum_{i=1}^n \\frac{ \\partial}{\\partial \\boldsymbol \\theta} \\ln \\mathrm{f}(y_i;\\boldsymbol \\theta ). \\] La función de puntaje es la suma de variables aleatorias con media cero debido a la ecuación (11.13); los teoremas del límite central están ampliamente disponibles para garantizar que las sumas de variables aleatorias independientes tengan distribuciones normales en muestras grandes (ver Sección 1.4 para un ejemplo). Además, si las variables aleatorias son idénticas, entonces a partir de la ecuación (11.15) podemos ver que el segundo momento de \\(\\partial \\ln \\mathrm{f}(y_i;\\boldsymbol \\theta ) /\\partial \\boldsymbol \\theta\\) es la matriz de información, obteniendo el resultado. 11.9.2 Estimadores de Máxima Verosimilitud Los estimadores de máxima verosimilitud son valores de los parámetros \\(\\boldsymbol \\theta\\) que son “más probables” de haber sido producidos por los datos. El valor de \\(\\boldsymbol \\theta\\), denotado como \\(\\boldsymbol \\theta_{MLE}\\), que maximiza \\(\\mathrm{f}(\\mathbf{y};\\boldsymbol \\theta)\\) se llama el estimador de máxima verosimilitud. Debido a que \\(\\ln(\\cdot)\\) es una función uno a uno, también podemos determinar \\(\\boldsymbol \\theta_{MLE}\\) maximizando la función de log-verosimilitud, \\(L(\\boldsymbol \\theta)\\). Bajo condiciones amplias, tenemos que \\(\\boldsymbol \\theta_{MLE}\\) tiene una distribución normal en muestras grandes con media \\(\\boldsymbol \\theta\\) y varianza \\(\\left( \\mathbf{I}(\\boldsymbol \\theta) \\right)^{-1}\\). Este es un resultado crítico sobre el cual se basa gran parte de la estimación y la prueba de hipótesis. Para subrayar este resultado, examinamos el caso especial de la regresión “basada en normales”. Caso Especial. Regresión con distribuciones normales. Supongamos que \\(y_1, \\ldots, y_n\\) son independientes y están distribuidos normalmente, con media \\(\\mathrm{E~}y_i = \\mu_i = \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta\\) y varianza \\(\\sigma^2\\). Los parámetros se pueden resumir como \\(\\boldsymbol \\theta = \\left( \\boldsymbol \\beta^{\\prime}, \\sigma^2 \\right)^{\\prime}.\\) Recordemos de la ecuación (1.1) que la función de densidad de probabilidad normal es \\[ \\mathrm{f}(y; \\mu_i, \\sigma^2)=\\frac{1}{\\sigma \\sqrt{2\\pi }}\\exp \\left( -\\frac{1}{2\\sigma^2}\\left( y-\\mu_i \\right)^2\\right) . \\] Con esto, los dos componentes de la función de puntaje son \\[\\begin{eqnarray*} \\frac{ \\partial}{\\partial \\boldsymbol \\beta} L(\\boldsymbol \\theta) &amp;=&amp; \\sum_{i=1}^n \\frac{ \\partial}{\\partial \\boldsymbol \\beta} \\ln \\mathrm{f}(y_i; \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta, \\sigma^2) =-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\frac{ \\partial}{\\partial \\boldsymbol \\beta} \\left(y_i-\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta \\right)^2 \\\\ &amp;=&amp; -\\frac{(-2)}{2 \\sigma^2} \\sum_{i=1}^n \\left(y_i-\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta \\right) \\mathbf{x}_i \\end{eqnarray*}\\] y \\[\\begin{eqnarray*} \\frac{ \\partial}{\\partial \\sigma^2} L(\\boldsymbol \\theta) &amp;=&amp; \\sum_{i=1}^n \\frac{ \\partial}{\\partial \\sigma^2} \\ln \\mathrm{f}(y_i; \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta, \\sigma^2) = -\\frac{n}{2 \\sigma^2} + \\frac {1}{2 \\sigma ^4}\\sum_{i=1}^n \\left(y_i-\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta \\right)^2 . \\end{eqnarray*}\\] Igualando estas ecuaciones a cero y resolviendo se obtienen los estimadores de máxima verosimilitud \\[ \\boldsymbol \\beta_{MLE} = \\left(\\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^{\\prime}\\right)^{-1} \\sum_{i=1}^n \\mathbf{x}_i y_i = \\mathbf{b} \\] y \\[ \\sigma^2_{MLE} = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\mathbf{x}_i^{\\prime} \\mathbf{b} \\right)^2 = \\frac{n-(k+1)}{n} s^2. \\] Así, el estimador de máxima verosimilitud de \\(\\boldsymbol \\beta\\) es igual al estimador habitual de mínimos cuadrados. El estimador de máxima verosimilitud de \\(\\sigma^2\\) es un múltiplo escalar del estimador habitual de mínimos cuadrados. El estimador de mínimos cuadrados \\(s^2\\) es insesgado, mientras que \\(\\sigma^2_{MLE}\\) es solo aproximadamente insesgado en muestras grandes. La matriz de información es \\[ \\mathbf{I}(\\boldsymbol \\theta) = -\\mathrm{E~} \\left( \\begin{array}{cc} \\frac{ \\partial^2}{\\partial \\boldsymbol \\beta ~\\partial \\boldsymbol \\beta^{\\prime}} L(\\boldsymbol \\theta) &amp; \\frac{ \\partial^2}{\\partial \\boldsymbol \\beta ~\\partial \\sigma^2} L(\\boldsymbol \\theta) \\\\ \\frac{ \\partial^2}{\\partial \\sigma^2 \\partial \\boldsymbol \\beta^{\\prime} } L(\\boldsymbol \\theta) &amp; \\frac{ \\partial^2}{\\partial \\sigma^2 \\partial \\sigma^2} L(\\boldsymbol \\theta)\\\\ \\end{array} \\right)= \\left( \\begin{array}{cc} \\frac{ 1}{\\sigma^2} \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^{\\prime} &amp; 0 \\\\ 0 &amp; \\frac{n}{2 \\sigma^4}\\\\ \\end{array} \\right). \\] Así, \\(\\boldsymbol \\beta_{MLE} = \\mathbf{b}\\) tiene una distribución normal en muestras grandes con media \\(\\boldsymbol \\beta\\) y matriz de varianza-covarianza \\(\\sigma^2 \\left(\\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\right)^{-1}\\), como se vio anteriormente. Además, \\(\\sigma^2_{MLE}\\) tiene una distribución normal en muestras grandes con media \\(\\sigma^2\\) y varianza \\(2 \\sigma^4 /n.\\) La máxima verosimilitud es una técnica de estimación general que se puede aplicar en muchos contextos estadísticos, no solo en aplicaciones de regresión y series temporales. Se puede aplicar ampliamente y disfruta de ciertas propiedades de optimalidad. Ya hemos mencionado el resultado de que los estimadores de máxima verosimilitud suelen tener una distribución normal en muestras grandes. Además, los estimadores de máxima verosimilitud son los más eficientes en el siguiente sentido. Supongamos que \\(\\widehat{\\boldsymbol \\theta}\\) es un estimador alternativo insesgado. El teorema de Cramer-Rao establece, bajo condiciones de regularidad leves, para todos los vectores \\(\\mathbf c\\), que \\(\\mathrm{Var~} \\mathbf c^{\\prime} \\boldsymbol \\theta_{MLE} \\le \\mathrm{Var~} \\mathbf c^{\\prime} \\widehat{\\boldsymbol \\theta}\\), para \\(n\\) suficientemente grande. También notamos que \\(2 \\left( L(\\boldsymbol \\theta_{MLE}) - L(\\boldsymbol \\theta) \\right)\\) tiene una distribución chi-cuadrado con grados de libertad igual a la dimensión de \\(\\boldsymbol \\theta\\). En algunas aplicaciones, como el caso de regresión con una distribución normal, los estimadores de máxima verosimilitud se pueden calcular analíticamente como una expresión de forma cerrada. Típicamente, esto se puede hacer encontrando raíces de la primera derivada de la función. Sin embargo, en general, los estimadores de máxima verosimilitud no se pueden calcular con expresiones de forma cerrada y se determinan iterativamente. Se utilizan ampliamente dos procedimientos generales: 1. Newton-Raphson utiliza el algoritmo iterativo \\[\\begin{equation} \\boldsymbol \\theta_{NEW} = \\boldsymbol \\theta_{OLD} - \\left. \\left\\{ \\left( \\frac{ \\partial^2 L}{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^{\\prime}} \\right)^{-1} \\frac{\\partial L}{\\partial \\boldsymbol \\theta } \\right\\} \\right|_ {\\boldsymbol \\theta = \\boldsymbol \\theta_{OLD}} . \\tag{11.16} \\end{equation}\\] 2. Puntuación de Fisher utiliza el algoritmo iterativo \\[\\begin{equation} \\boldsymbol \\theta_{NEW} = \\boldsymbol \\theta_{OLD} + \\mathbf{I}(\\boldsymbol \\theta_{OLD})^{-1} \\left. \\left\\{ \\frac{\\partial L}{\\partial \\boldsymbol \\theta } \\right\\} \\right|_ {\\boldsymbol \\theta = \\boldsymbol \\theta_{OLD}} . \\tag{11.17} \\end{equation}\\] donde \\(\\mathbf{I}(\\boldsymbol \\theta)\\) es la matriz de información. 11.9.3 Pruebas de Hipótesis Consideramos probar la hipótesis nula \\(H_0: h(\\boldsymbol \\theta) = \\mathbf{d}\\), donde \\(\\mathbf{d}\\) es un vector conocido de dimensión \\(r \\times 1\\) y h(\\(\\cdot\\)) es conocido y diferenciable. Este marco de pruebas abarca como un caso especial la hipótesis lineal general introducida en el Capítulo 4. Existen tres enfoques generales para probar hipótesis, denominados razón de verosimilitud, Wald y Rao. El enfoque de Wald evalúa una función de la verosimilitud en \\(\\boldsymbol \\theta_{MLE}\\). El enfoque de la razón de verosimilitud utiliza \\(\\boldsymbol \\theta_{MLE}\\) y \\(\\boldsymbol \\theta_{Reduced}\\). Aquí, \\(\\boldsymbol \\theta_{Reduced}\\) es el valor de \\(\\boldsymbol \\theta\\) que maximiza \\(L(\\boldsymbol \\theta_{Reduced})\\) bajo la restricción de que \\(h(\\boldsymbol \\theta) = \\mathbf{d}\\). El enfoque de Rao también utiliza \\(\\boldsymbol \\theta_{Reduced}\\) pero lo determina maximizando \\(L(\\boldsymbol \\theta) - \\boldsymbol \\lambda^{\\prime}(h(\\boldsymbol \\theta) -\\mathbf{d})\\), donde \\(\\boldsymbol \\lambda\\) es un vector de multiplicadores de Lagrange. Por lo tanto, la prueba de Rao también se llama la prueba del multiplicador de Lagrange. Las estadísticas de prueba asociadas con los tres enfoques son: \\(LRT = 2 \\times \\left\\{L(\\boldsymbol \\theta_{MLE})-L(\\boldsymbol \\theta_{Reduced}) \\right\\}\\) Wald: \\(TS_W(\\boldsymbol \\theta_{MLE})\\), donde \\[ TS_W(\\boldsymbol \\theta)=(h(\\boldsymbol \\theta) -\\mathbf{d})^{\\prime} \\left\\{ \\frac{\\partial}{\\partial \\boldsymbol \\theta} h(\\boldsymbol \\theta)^{\\prime} \\left(-\\mathbf{I}(\\boldsymbol \\theta) \\right)^{-1} \\frac{\\partial}{\\partial \\boldsymbol \\theta} h(\\boldsymbol \\theta) \\right\\}^{-1} (h(\\boldsymbol \\theta) -\\mathbf{d}), \\] y Rao: \\(TS_R(\\boldsymbol \\theta_{Reduced})\\), donde \\[ TS_R(\\boldsymbol \\theta) = \\frac{\\partial}{\\partial \\boldsymbol \\theta} L(\\boldsymbol \\theta) \\left(-\\mathbf{I}(\\boldsymbol \\theta) \\right)^{-1} \\frac{\\partial}{\\partial \\boldsymbol \\theta} L(\\boldsymbol \\theta)^{\\prime}. \\] Bajo condiciones amplias, las tres estadísticas de prueba tienen distribuciones chi-cuadrado en muestras grandes con \\(r\\) grados de libertad bajo \\(H_0\\). Los tres métodos funcionan bien cuando el número de parámetros es de dimensión finita y la hipótesis nula especifica que \\(\\boldsymbol \\theta\\) está en el interior del espacio de parámetros. La principal ventaja de la estadística de Wald es que solo requiere el cálculo de \\(\\boldsymbol \\theta_{MLE}\\) y no de \\(\\boldsymbol \\theta_{Reduced}\\). En contraste, la principal ventaja de la estadística de Rao es que solo requiere el cálculo de \\(\\boldsymbol \\theta_{Reduced}\\) y no de \\(\\boldsymbol \\theta_{MLE}\\). En muchas aplicaciones, el cálculo de \\(\\boldsymbol \\theta_{MLE}\\) es laborioso. La prueba de razón de verosimilitud es una extensión directa de la prueba \\(F\\) parcial introducida en el Capítulo 4: permite comparar directamente modelos anidados, una técnica útil en aplicaciones. 11.9.4 Criterios de Información Las pruebas de razón de verosimilitud son útiles para elegir entre dos modelos que son anidados, es decir, donde un modelo es un subconjunto del otro. ¿Cómo comparamos modelos cuando no están anidados? Una forma es utilizar los siguientes criterios de información. La distancia entre dos distribuciones de probabilidad dadas por funciones de densidad de probabilidad \\(g\\) y \\(f_{\\boldsymbol \\theta}\\) se puede resumir como \\[ \\mathrm{KL}(g,f_{\\boldsymbol \\theta}) = \\mathrm{E}_g \\ln \\frac{g(y)}{f_{\\boldsymbol \\theta}(y)} . \\] Esta es la distancia de Kullback-Leibler. Aquí, hemos indexado \\(f\\) con un vector de parámetros \\(\\boldsymbol \\theta\\). Si dejamos que la función de densidad \\(g\\) sea fija en un valor hipotético, digamos \\(f_{{\\boldsymbol \\theta}_0}\\), entonces minimizar \\(\\mathrm{KL}(f_{{\\boldsymbol \\theta}_0},f_{\\boldsymbol \\theta})\\) es equivalente a maximizar el log-verosimilitud. Sin embargo, maximizar la verosimilitud no impone una estructura suficiente en el problema porque sabemos que siempre podemos aumentar la verosimilitud introduciendo parámetros adicionales. Así, Akaike en 1974 mostró que una alternativa razonable es minimizar \\[ AIC = -2 \\times L(\\boldsymbol \\theta_{MLE}) + 2 \\times (número~de~parámetros), \\] conocido como Criterio de Información de Akaike. Aquí, el término adicional \\(2 \\times (número~de~parámetros)\\) es una penalización por la complejidad del modelo. Con esta penalización, no se puede mejorar el ajuste simplemente introduciendo parámetros adicionales. Esta estadística se puede utilizar al comparar varios modelos alternativos que no necesariamente están anidados. Se elige el modelo que minimiza \\(AIC\\). Si los modelos bajo consideración tienen el mismo número de parámetros, esto es equivalente a elegir el modelo que maximiza el log-verosimilitud. Observamos que esta definición no es adoptada uniformemente en la literatura. Por ejemplo, en análisis de series temporales, el \\(AIC\\) se reescala por el número de parámetros. Otras versiones que proporcionan correcciones para muestras finitas también están disponibles en la literatura. Schwarz en 1978 derivó un criterio alternativo utilizando métodos bayesianos. Su medida se conoce como el Criterio de Información Bayesiano, definido como \\[ BIC = -2 \\times L(\\boldsymbol \\theta_{MLE}) + (número~de~parámetros) \\times \\ln (número~de~observaciones), \\] Esta medida da un mayor peso al número de parámetros. Es decir, todo lo demás siendo igual, el \\(BIC\\) sugerirá un modelo más parsimonioso que el \\(AIC\\). Al igual que el coeficiente de determinación ajustado \\(R^2_a\\) que hemos introducido en la literatura de regresión, tanto el \\(AIC\\) como el \\(BIC\\) proporcionan medidas de ajuste con una penalización por la complejidad del modelo. En los modelos de regresión lineal normal, la Sección 5.6 señaló que minimizar el \\(AIC\\) es equivalente a minimizar \\(n \\ln s^2 + k\\). Otra estadística de regresión lineal que equilibra la bondad de ajuste y la complejidad del modelo es la estadística \\(C_p\\) de Mallows. Para \\(p\\) variables candidatas en el modelo, esto se define como \\(C_p = (Error~SS)_p/s^2 - (n-2p).\\) Véase, por ejemplo, Cameron y Trivedi (1998) para referencias y más discusión sobre criterios de información. "],["C12Count.html", "Capítulo 12 Variables Dependientes de Conteo 12.1 Regresión de Poisson 12.2 Aplicación: Seguro de Automóviles en Singapur 12.3 Sobre dispersión y Modelos Binomiales Negativos 12.4 Otros Modelos de Conteo 12.5 Lecturas Adicionales y Referencias 12.6 Ejercicios", " Capítulo 12 Variables Dependientes de Conteo Vista Previa del Capítulo. En este capítulo, la variable dependiente \\(y\\) es un conteo, tomando valores 0, 1, 2 y así sucesivamente, que describe el número de eventos. Las variables dependientes de conteo forman la base de los modelos actuariales de frecuencia de reclamaciones. En otras aplicaciones, una variable dependiente de conteo puede ser el número de accidentes, el número de personas que se jubilan o el número de empresas que se vuelven insolventes. El capítulo introduce la regresión de Poisson, un modelo que incluye variables explicativas con una distribución de Poisson para conteos. Este modelo fundamental maneja muchos conjuntos de datos de interés para los actuarios. Sin embargo, con la distribución de Poisson, la media es igual a la varianza, una limitación que sugiere la necesidad de distribuciones más generales como la binomial negativa. Incluso la binomial negativa de dos parámetros puede no capturar algunas características importantes, lo que motiva la necesidad de modelos más complejos como los modelos “inflados de ceros” y los modelos de variables latentes introducidos en este capítulo. 12.1 Regresión de Poisson 12.1.1 Distribución de Poisson Una variable aleatoria de conteo \\(y\\) es aquella que tiene resultados en los enteros no negativos, \\(j=0,1,2,...\\) La Poisson es una distribución fundamental utilizada para conteos que tiene una función de masa de probabilidad \\[\\begin{equation} \\Pr \\left( y=j\\right) =\\frac{\\mu^j}{j!}e^{-\\mu },~~~j=0,1,2,... \\tag{12.1} \\end{equation}\\] Se puede demostrar que \\(\\mathrm{E~} y =\\sum\\nolimits_{j=0}^{\\infty}j\\Pr \\left( y=j\\right) =\\mu\\), por lo que podemos interpretar el parámetro \\(\\mu\\) como la media de la distribución. De manera similar, se puede demostrar que \\(\\mathrm{Var~}y =\\mu\\), por lo que la media es igual a la varianza en esta distribución. Una de las primeras aplicaciones (Bortkiewicz, 1898) se basó en usar la distribución de Poisson para representar el número anual de muertes en el ejército prusiano debido a “patadas de mulas”. La distribución todavía se usa ampliamente como un modelo del número de accidentes, como lesiones en un entorno industrial (para cobertura de compensación de trabajadores) y daños a la propiedad en seguros de automóviles. Ejemplo: Datos de Automóviles de Singapur. Estos datos provienen de una cartera de 1993 de \\(n=7,483\\) pólizas de seguro de automóviles de una importante compañía de seguros en Singapur. Los datos se describirán más a fondo en la Sección 12.2. La Tabla 12.1 proporciona la distribución del número de accidentes. La variable dependiente es el número de accidentes automovilísticos por asegurado. Para este conjunto de datos, resulta que el número máximo de accidentes en un año fue tres. En promedio, hubo \\(\\overline{y}=0.06989\\) accidentes por persona. Tabla 12.1: Comparación de Conteos Observados y Ajustados Basados en Datos de Automóviles de Singapur Conteo (\\(j\\)) Observado (\\(n_j\\)) Conteos Ajustados usando la Distribución de Poisson \\((n\\widehat{p}_j)\\) 0 6996 6977.86 1 455 487.69 2 28 17.04 3 4 0.4 4 0 0.01 Total 7483 7483 Código R para Producir la Tabla 12.1 # Tabla 12.1 # ESTE CÓDIGO USA EL CONJUNTO DE DATOS SingaporeAuto.csv #Singapore &lt;- read.csv(&quot;../../CSVData/SingaporeAuto.csv&quot;, header=TRUE) Singapore &lt;- read.csv(&quot;CSVData/SingaporeAuto.csv&quot;, header=TRUE) # TABLA DE CONTEOS OBSERVADOS Y AJUSTADOS POISSON muhat &lt;- mean(Singapore$Clm_Count) col2 &lt;- c(table(Singapore$Clm_Count),0) col1 &lt;- ynum &lt;- c(0,1,2,3,4) col3 &lt;- length(Singapore$Clm_Count)*dpois(ynum, muhat) tableoutA &lt;- cbind(col1, col2, col3) tableoutA[,3] &lt;- round(tableoutA[,3],digits = 2) row5 &lt;- c(&quot;Total&quot;, sum(col2), round(sum(col3),digits = 2)) tableout &lt;- rbind(tableoutA, row5) row.names(tableout) &lt;- NULL colnames(tableout) &lt;- c(&quot;Conteo ($j$)&quot;, &quot;Observado ($n_j$)&quot;, &quot;Conteos Ajustados usando la Distribución de Poisson $(n\\\\widehat{p}_j)$&quot;) TableGen1(TableData=tableout, TextTitle=&#39;Comparación de Conteos Observados y Ajustados Basados en Datos de Automóviles de Singapur&#39;, Align=&#39;c&#39;, Digits = 2, ColumnSpec=1:2, BorderRight=1, ColWidth = &quot;3cm&quot;) # OTRA MANERA #CountPoisson1 &lt;- glm(Clm_Count ~ 1,poisson(link=log), data = Singapore) #summary(CountPoisson1) #exp(CountPoisson1$coefficients) La Tabla 12.1 también proporciona los conteos ajustados que se calcularon utilizando el estimador de máxima verosimilitud de \\(\\mu\\). Específicamente, a partir de la ecuación (12.1) podemos escribir la función de masa como \\(\\mathrm{f}(y,\\mu) = \\mu^y e^{-\\mu} /y!,\\) y así la log-verosimilitud es \\[\\begin{equation} L(\\mu) = \\sum_{i=1}^{n} \\ln \\mathrm{f}(y_i,\\mu) = \\sum_{i=1}^{n}\\left( -\\mu +y_i\\ln \\mu -\\ln y_i!\\right) . \\tag{12.2} \\end{equation}\\] Es sencillo demostrar que la log-verosimilitud tiene un máximo en \\(\\widehat{\\mu }=\\overline{y}\\), el número promedio de reclamaciones. Las probabilidades estimadas, utilizando la ecuación (12.1) y \\(\\widehat{\\mu }= \\overline{y}\\), se denotan como \\(\\widehat{p}_j\\). Usamos estas probabilidades estimadas en la Tabla 12.1 al calcular los conteos ajustados con \\(n=7,483\\). Para comparar los conteos observados y ajustados, una estadística de bondad de ajuste ampliamente utilizada es la estadística chi-cuadrado de Pearson, dada por \\[\\begin{equation} \\sum_j\\frac{\\left( n_j-n\\widehat{p}_j\\right)^2}{n\\widehat{p}_j}. \\tag{12.3} \\end{equation}\\] Bajo la hipótesis nula de que la distribución de Poisson es un modelo correcto, esta estadística tiene una distribución chi-cuadrado en muestras grandes, donde los grados de libertad son el número de celdas menos uno menos el número de parámetros estimados. Para los datos de Singapur en la Tabla 12.1, esto es \\(df=5-1-1=3\\). Resulta que la estadística es 41.98, lo que indica que este modelo básico de Poisson es inadecuado. Código R para producir la estadística de bondad de ajuste de Pearson CountPoisson1 &lt;- glm(Clm_Count ~ 1,poisson(link=log), data = Singapore) temp &lt;- summary(CountPoisson1) str(temp) (temp$aic - 2*(1+1))/2 # COMPARACIÓN DE MODELOS DE FRECUENCIA AJUSTADOS options(digits=8) table1p = cbind(sum(dpois(0,CountPoisson1$fitted.values)), sum(dpois(1,CountPoisson1$fitted.values)), sum(dpois(2,CountPoisson1$fitted.values)), sum(dpois(3,CountPoisson1$fitted.values)), sum(dpois(4,CountPoisson1$fitted.values))) # ESTADÍSTICA CHI-CUADRADO DE PEARSON actual = data.frame(table(Singapore$Clm_Count))[,2]; actual[5] = 0 diff = actual-table1p (PearsonG = sum(diff*diff/table1p)) 12.1.2 Modelo de Regresión Para extender el modelo básico de Poisson, primero permitimos que la media varíe por una cantidad conocida llamada exposición \\(E_i\\) , de modo que \\[ \\mathrm{E~}y_i=E_i\\times \\mu . \\] Para motivar esta especificación, recordemos que las sumas de variables aleatorias de Poisson independientes también tienen una distribución de Poisson, por lo que es razonable pensar en las exposiciones como grandes números positivos. Así, es común modelar el número de accidentes por mil vehículos o el número de homicidios por millón de habitantes. Además, consideramos también los casos en los que las unidades de exposición pueden ser fracciones. Para ilustrar, en nuestros datos de Singapur, \\(E_i\\) representará la fracción del año en la que un asegurado tuvo cobertura de seguro. La lógica detrás de esto es que el número esperado de accidentes es directamente proporcional a la duración de la cobertura. (Esto también puede motivarse desde un marco probabilístico basado en colecciones de variables aleatorias distribuidas según Poisson, conocidas como procesos de Poisson, véase, por ejemplo, Klugman et al., 2008). Más generalmente, deseamos permitir que la media varíe de acuerdo con la información contenida en otras variables explicativas. Para el Poisson, es común especificar \\[ \\mathrm{E~}y_i = \\mu_i = \\exp \\left( \\mathbf{x}_i^{\\prime}\\boldsymbol \\beta \\right) . \\] Usar la función exponencial para mapear el componente sistemático \\(\\mathbf{x}_i^{\\prime }\\boldsymbol \\beta\\) en la media asegura que \\(\\mathrm{E~}y_i\\) permanecerá positiva. Suponiendo la linealidad de los coeficientes de regresión, se permite una fácil interpretación. Específicamente, dado que \\[ \\frac{\\partial \\mathrm{E~}y_i}{\\partial x_{ij}} \\times \\frac{1}{\\mathrm{E~}y_i} =\\beta_j, \\] podemos interpretar \\(\\beta_j\\) como el cambio proporcional en la media por unidad de cambio en \\(x_{ij}\\). La función que conecta la media con el componente sistemático se conoce como la función de enlace logarítmico, es decir, \\(\\ln \\mu_i=\\mathbf{x}_i^{\\prime }\\boldsymbol \\beta\\). Para incorporar las exposiciones, siempre se puede especificar que una de las variables explicativas sea \\(\\ln E_i\\) y restringir el coeficiente de regresión correspondiente a ser 1. Este término se conoce como un desplazamiento. Con esta convención, la función de enlace es \\[\\begin{equation} \\ln \\mu_i=\\ln E_i+\\mathbf{x}_i^{\\prime }\\boldsymbol \\beta. \\tag{12.4} \\end{equation}\\] Ejemplo: Accidentes de Automóviles en California. Weber (1971) proporcionó la primera aplicación de la regresión de Poisson a frecuencias de accidentes automovilísticos en su estudio de los registros de conducción en California. En uno de sus modelos, Weber examinó el número de accidentes automovilísticos durante 1963 de casi 87,000 conductores hombres. Sus variables explicativas consistían en: \\(x_1\\) = el logaritmo natural del índice de densidad de tráfico del condado donde reside el conductor, \\(x_2 =5/(edad-13)\\) \\(x_3\\) = el número de condenas contables incurridas durante los años 1961-62 \\(x_4\\) = el número de involucramientos en accidentes durante los años 1961-62 \\(x_5\\) = el número de condenas no contables incurridas durante los años 1961-62. Curiosamente, en esta aplicación temprana, Weber logró un ajuste satisfactorio representando la media como una combinación lineal de variables explicativas (\\(\\mathrm{E }~y_i=\\mathbf{x}_i^{\\prime }\\boldsymbol \\beta\\)), no en la versión exponenciada como en la ecuación (12.4) que ahora es comúnmente utilizada. 12.1.3 Estimación La estimación de máxima verosimilitud es la técnica usual para los modelos de regresión de Poisson. Utilizando la función de enlace logarítmico en la ecuación (12.4), la log-verosimilitud se expresa como: \\[\\begin{eqnarray*} L(\\boldsymbol \\beta) &amp;=&amp;\\sum_{i=1}^{n}\\left( -\\mu_i+y_i\\ln \\mu _i-\\ln y_i!\\right) \\\\ &amp;=&amp;\\sum_{i=1}^{n}\\left( -E_i\\exp \\left( \\mathbf{x}_i^{\\prime }\\boldsymbol \\beta \\right) +y_i\\left( \\ln E_i+\\mathbf{x}_i^{\\prime }\\boldsymbol \\beta \\right) -\\ln y_i!\\right) . \\end{eqnarray*}\\] Al igualar la función de puntuación a cero se obtiene \\[\\begin{equation} \\left. \\frac{\\partial }{\\partial \\boldsymbol \\beta}\\mathrm{L}(\\boldsymbol \\beta )\\right\\vert_{\\mathbf{\\beta =b}}=\\sum_{i=1}^{n}\\left( y_i-E_i\\exp \\left( \\mathbf{x}_i^{\\prime }\\mathbf{b}\\right) \\right) \\mathbf{x} _i=\\sum_{i=1}^{n}\\left( y_i-\\widehat{\\mu }_i\\right) \\mathbf{x}_i= \\mathbf{0}, \\tag{12.5} \\end{equation}\\] donde \\(\\widehat{\\mu }_i = E_i\\exp \\left( \\mathbf{x}_i^{\\prime }\\mathbf{b} \\right)\\). Resolver esta ecuación (numéricamente) nos da \\(\\mathbf{b}\\), el estimador de máxima verosimilitud de \\(\\boldsymbol \\beta\\). A partir de la ecuación (12.5), vemos que si una fila de \\(\\mathbf{x}_i\\) es constante (correspondiendo a un término de intercepto constante en la regresión), entonces la suma de los residuos \\(y_i - \\widehat{\\mu}_i\\) es cero. Tomar las segundas derivadas nos da la matriz de información, \\[ \\mathbf{I}(\\boldsymbol \\beta) = - \\mathrm{E} \\frac{\\partial ^2}{\\partial \\boldsymbol \\beta\\partial \\boldsymbol \\beta^{\\prime }}\\mathrm{L}(\\boldsymbol \\beta)=\\sum_{i=1}^{n}E_i\\exp \\left( \\mathbf{x}_i^{\\prime }\\boldsymbol \\beta\\right) \\mathbf{x}_i\\mathbf{x}_i^{\\prime }=\\sum_{i=1}^{n}\\mu_i\\mathbf{x}_i\\mathbf{x}_i^{\\prime }. \\] La teoría estándar de estimación por máxima verosimilitud (Sección 11.9.2) muestra que la matriz de varianza-covarianza asintótica de \\(\\mathbf{b}\\) es \\[ \\widehat{\\mathrm{Var~}\\mathbf{b}}=\\left( \\sum\\limits_{i=1}^{n}\\widehat{\\mu } _i\\mathbf{x}_i\\mathbf{x}_i^{\\prime }\\right)^{-1}. \\] La raíz cuadrada del \\(j\\)-ésimo elemento diagonal de \\(\\widehat{\\mathrm{Var~} \\mathbf{b}}\\) nos da el error estándar para \\(b_j\\), que denotamos como \\(se(b_j)\\). Ejemplo: Seguro de Mala Praxis Médica. Los médicos cometen errores y pueden ser demandados por las partes perjudicadas por estos errores. Al igual que muchos profesionales, es común que los médicos contraten cobertura de seguro para mitigar las consecuencias financieras de las demandas por “mala praxis”. Dado que las aseguradoras desean fijar precios precisos para este tipo de cobertura, parece natural preguntar qué tipo de médicos es más probable que presenten reclamaciones de mala praxis. Fournier y McInnes (2001) examinaron una muestra de \\(n=9,059\\) médicos de Florida utilizando datos del archivo de reclamaciones de seguros de responsabilidad profesional médica de Florida. Los autores examinaron reclamaciones cerradas entre 1985 y 1989 para médicos que obtuvieron su licencia antes de 1981, omitiendo así las reclamaciones de médicos recién licenciados. Las reclamaciones por mala praxis médica pueden tardar mucho en resolverse (“liquidarse”); en su estudio, Fournier y McInnes encontraron que el 2 por ciento de las reclamaciones aún no se habían resuelto después de 5 años del evento de mala praxis. Por lo tanto, eligieron un período temprano (1985-1989) para permitir que la experiencia madurara. Los autores también ignoraron las reclamaciones menores al considerar solo aquellas que excedieron los $100. Tabla 12.2 proporciona los coeficientes de la regresión de Poisson ajustada, junto con los errores estándar que aparecen en Fournier y McInnes (2001). La tabla muestra que el área de especialización del médico, la región, el tamaño de la práctica y las características personales del médico (experiencia y género) son determinantes importantes del número de demandas por mala praxis médica. Por ejemplo, podemos interpretar el coeficiente asociado al género como que se espera que los hombres tengan \\(\\exp (0.432)= 1.540\\) veces más reclamaciones que las mujeres. Tabla 12.2. Coeficientes de Regresión del Modelo de Regresión Poisson para Mala Praxis Médica \\[ \\small{ \\begin{array}{lcc|lcc} \\hline &amp; &amp; \\text{Error} &amp; &amp; &amp; \\text{Error} \\\\ \\text{Variables Explicativas} &amp; \\text{Coeficiente} &amp; \\text{Estándar} &amp; \\text{Variables Explicativas} &amp; \\text{Coeficiente} &amp; \\text{Estándar} \\\\ \\hline \\text{Intercepto} &amp; -1.634 &amp; 0.254 &amp; \\text{MSA: Miami Dade-Broward} &amp; 0.377 &amp; 0.094 \\\\ \\text{Log Años con Licencia} &amp; -0.392 &amp; 0.054 &amp; \\text{MSA: Otros} &amp; 0.012 &amp; 0.084 \\\\ \\text{Femenino} &amp; -0.432 &amp; 0.082 &amp; \\ \\ \\ \\ \\textit{Especialidad} \\\\ \\text{Volumen de Pacientes} &amp; 0.643 &amp; 0.045 &amp; \\text{Anestesiología} &amp; 0.944 &amp; 0.099 \\\\ \\text{(Volumen de Pacientes)}^2 &amp; -0.066 &amp; 0.008 &amp; \\text{Medicina de Emergencia} &amp; 0.583 &amp; 0.105 \\\\ \\text{Educación Per Cápita} &amp; -0.015 &amp; 0.006 &amp; \\text{Medicina Interna} &amp; 0.428 &amp; 0.066 \\\\ \\text{Ingreso Per Cápita} &amp; 0.047 &amp; 0.011 &amp; \\text{Obstetricia-Ginecología} &amp; 1.226 &amp; 0.070 \\\\ \\ \\ \\ \\ \\textit{Variables Regionales} &amp; &amp; &amp; \\text{Otorrinolaringología} &amp; 1.063 &amp; 0.109 \\\\ \\text{Segundo Circuito} &amp; 0.066 &amp; 0.072 &amp; \\text{Pediatría} &amp; 0.385 &amp; 0.089 \\\\ \\text{Tercer Circuito} &amp; 0.103 &amp; 0.088 &amp; \\text{Radiología} &amp; 0.478 &amp; 0.099 \\\\ \\text{Cuarto Circuito} &amp; 0.214 &amp; 0.098 &amp; \\text{Cirugía} &amp; 1.410 &amp; 0.061 \\\\ \\text{Quinto Circuito} &amp; 0.287 &amp; 0.069 &amp; \\text{Otras Especialidades} &amp; 0.011 &amp; 0.076 \\\\ \\hline \\end{array} } \\] 12.1.4 Inferencia Adicional En los modelos de regresión de Poisson, anticipamos que las variables dependientes sean heterocedásticas debido a la relación \\(\\mathrm{Var~}y_i=\\mu_i\\). Esta característica significa que los residuales ordinarios \\(y_i-\\widehat{\\mu}_i\\) son menos útiles, por lo que es más común examinar los residuales de Pearson definidos como \\[ r_i=\\frac{y_i-\\widehat{\\mu}_i}{\\sqrt{\\widehat{\\mu}_i}}. \\] Por construcción, los residuales de Pearson son aproximadamente homocedásticos. Gráficos de los residuales de Pearson pueden usarse para identificar observaciones inusuales o para detectar si variables adicionales de interés pueden mejorar la especificación del modelo. Los residuales de Pearson también pueden usarse para calcular una estadística de bondad de ajuste de Pearson, \\[\\begin{equation} \\sum\\limits_{i=1}^{n}r_i^2=\\sum\\limits_{i=1}^{n}\\frac{\\left( y_i-\\widehat{\\mu}_i\\right)^2}{\\widehat{\\mu}_i}. \\tag{12.6} \\end{equation}\\] Esta estadística es una medida general de qué tan bien se ajusta el modelo a los datos. Si el modelo está correctamente especificado, esta estadística debería ser aproximadamente \\(n-(k+1)\\). En general, las estadísticas de bondad de ajuste de Pearson toman la forma \\(\\sum \\left( O-E\\right)^2/E\\), donde \\(O\\) es una cantidad observada y \\(E\\) es el valor estimado (esperado) correspondiente basado en un modelo. La estadística en la ecuación (12.6) se calcula a nivel de observación, mientras que la estadística en la ecuación (12.3) se calculó resumiendo información sobre celdas. En la regresión lineal, el coeficiente de determinación \\(R^2\\) es una medida de bondad de ajuste ampliamente aceptada. En la regresión no lineal, como para variables dependientes binarias y de conteo, esto no es así. Las estadísticas de información, como el Criterio de Información de Akaike, \\[ AIC=-2 L(\\mathbf{b}) +2(k+1), \\] representan un tipo de estadística útil para la bondad de ajuste, que se define de manera amplia en un rango amplio de modelos. Los modelos con valores más pequeños de \\(AIC\\) ajustan mejor y son preferidos. Como se mencionó en la Sección 12.1.3, los estadísticos \\(t\\) se utilizan regularmente para probar la significancia de los coeficientes de regresión individuales. Para probar colecciones de coeficientes de regresión, es común utilizar la prueba de razón de verosimilitud. La prueba de razón de verosimilitud es un procedimiento bien conocido para probar la hipótesis nula \\(H_0:\\mathrm{h}(\\boldsymbol \\beta) = \\mathbf{d}\\), donde \\(\\mathbf{d}\\) es un vector conocido de dimensión \\(r\\times 1\\) y \\(\\mathrm{h}(\\mathbf{.})\\) es una función conocida y diferenciable. Este enfoque utiliza \\(\\mathbf{b}\\) y \\(\\mathbf{b}_{\\mathrm{Reducido}}\\), donde \\(\\mathbf{b}_{\\mathrm{Reducido}}\\) es el valor de \\(\\boldsymbol \\beta\\) que maximiza \\(L(\\boldsymbol \\beta)\\) bajo la restricción de que \\(\\mathrm{h}(\\boldsymbol \\beta)=\\mathbf{d}\\). Se calcula la estadística de prueba \\[\\begin{equation} LRT = 2 \\left( L(\\mathbf{b}) - L(\\mathbf{b}_{\\mathrm{Reducido}}) \\right) . \\tag{12.7} \\end{equation}\\] Bajo la hipótesis nula \\(H_0\\), la estadística de prueba \\(LRT\\) tiene una distribución asintótica chi-cuadrado con \\(r\\) grados de libertad. Por lo tanto, valores grandes de \\(LRT\\) sugieren que la hipótesis nula no es válida. 12.2 Aplicación: Seguro de Automóviles en Singapur Frees y Valdez (2008) investigan modelos jerárquicos de la experiencia de conducción en Singapur. Aquí examinamos en detalle un subconjunto de sus datos, centrándonos en los conteos de accidentes automovilísticos de 1993. El propósito del análisis es comprender el impacto de las características del vehículo y del conductor en la experiencia de accidentes. Estas relaciones proporcionan una base para un actuario que trabaja en la tarificación, es decir, establecer el precio de las coberturas de seguros. Los datos provienen de la Asociación de Seguros Generales de Singapur, una organización que agrupa a aseguradoras generales (propiedad y accidentes) en Singapur (ver el sitio web de la organización: www.gia.org.sg). A partir de esta base de datos, se disponía de varias características para explicar la frecuencia de accidentes automovilísticos. Estas características incluyen variables del vehículo, como el tipo y la edad, así como variables a nivel de persona, como la edad, el género y la experiencia previa de conducción. La Tabla 12.3 resume estas características. knitr::kable(2, caption = &quot;Silly. Create a table just to update the counter...&quot;) Tabla 12.2: Silly. Create a table just to update the counter… x 2 Tabla 12.3: Descripción de las Covariables Covariable Descripción Tipo de Vehículo El tipo de vehículo asegurado, ya sea automóvil (A) u otro (O). Edad del Vehículo La edad del vehículo, en años, agrupada en seis categorías. Género El género del titular de la póliza, ya sea masculino o femenino Edad La edad del titular de la póliza, en años, agrupada en siete categorías. NCD Descuento por No Reclamos. Esto se basa en el historial de accidentes previo del titular de la póliza. Cuanto mayor sea el descuento, mejor será el historial de accidentes anterior. Tabla 12.4 muestra los efectos de las características del vehículo sobre el número de reclamaciones. La categoría “Automóvil” tiene una experiencia de reclamaciones general más baja. La categoría “Otro” consiste principalmente en vehículos de mercancías (comerciales), así como coches de fin de semana y de alquiler. La edad del vehículo muestra efectos no lineales sobre la frecuencia de accidentes. Aquí, observamos pocas reclamaciones para autos nuevos, con un aumento inicial en la frecuencia de accidentes con el tiempo. Sin embargo, para vehículos en funcionamiento durante largos periodos, las frecuencias de accidentes son relativamente bajas. También hay algunos efectos de interacción importantes entre el tipo de vehículo y la edad que no se muestran aquí. No obstante, Tabla 12.4 sugiere claramente la importancia de estas dos variables en las frecuencias de reclamaciones. Tabla 12.4. Efecto de las Características del Vehículo sobre las Reclamaciones \\[ \\small{ \\begin{array}{crrrr|r} \\hline &amp; \\text{Conteo=0} &amp; \\text{Conteo=1} &amp; \\text{Conteo=2} &amp; \\text{Conteo=3} &amp; \\text{Totales} \\\\ \\hline \\text{Tipo de Vehículo} \\\\ \\text{Otro} &amp; 3,441 &amp; 184 &amp; 13 &amp; 3 &amp; 3,641 \\\\ &amp; (94.5) &amp; (5.1) &amp; (0.4) &amp; (0.1) &amp; (48.7) \\\\ \\text{Automóvil} &amp; 3,555 &amp; 271 &amp; 15 &amp; 1 &amp; 3,842 \\\\ &amp; (92.5) &amp; (7.1) &amp; (0.4) &amp; (0.0) &amp; (51.3) \\\\ \\hline \\text{Edad del Vehículo (en años)} \\\\ 0\\text{ a } 2 &amp; 4,069 &amp; 313 &amp; 20 &amp; 4 &amp; 4,406 \\\\ &amp; (92.4) &amp; (7.1) &amp; (0.5) &amp; (0.1) &amp; (50.8) \\\\ 3 \\text{ a } 5 &amp; 708 &amp; 59 &amp; 4 &amp; &amp; 771 \\\\ &amp; (91.8) &amp; (7.7) &amp; (0.5) &amp; &amp; (10.3) \\\\ 6 \\text{ a } 10 &amp; 872 &amp; 49 &amp; 3 &amp; &amp; 924 \\\\ &amp; (94.4) &amp; (5.3) &amp; (0.3) &amp; &amp; (12.3) \\\\ 11 \\text{ a } 15 &amp; 1,133 &amp; 30 &amp; 1 &amp; &amp; 1,164 \\\\ &amp; (97.3) &amp; (2.6)&amp; (0.1) &amp; &amp; (15.6) \\\\ \\text{16 años y más} &amp; 214 &amp; 4 &amp; &amp; &amp; 218 \\\\ &amp; (98.2) &amp; (1.8) &amp; &amp; &amp; (2.9) \\\\ \\hline \\text{Totales} &amp; 6,996 &amp; 455 &amp; 28 &amp; 4 &amp; 7,483 \\\\ \\hline \\end{array} } \\] Nota: Los números entre paréntesis son porcentajes. Código R para generar la Tabla 12.4 # Tabla 12.4 # EFECTO DE LAS CARACTERÍSTICAS DEL VEHÍCULO Auto &lt;- 1*(Singapore$VehicleType==&quot;A&quot;) TAuto &lt;- table(Auto, Singapore$Clm_Count) TAuto1 &lt;- cbind(TAuto, rowSums(TAuto)) TVehA &lt;- table(Singapore$VAgecat1, Singapore$Clm_Count) TVehA1 &lt;- cbind(TVehA, rowSums(TVehA)) TComb &lt;- rbind(TAuto1, TVehA1) AddRow &lt;- function(row){ rowper &lt;- 100*row/row[5] rowper[5] &lt;- 100*row[5]/length(Singapore$Clm_Count) return(rbind(row, rowper)) } TCombBig &lt;- rbind( AddRow(TComb[1,]), AddRow(TComb[2,]), AddRow(TComb[3,]), AddRow(TComb[4,]), AddRow(TComb[5,]), AddRow(TComb[6,]), AddRow(TComb[7,]) ) TCombBig[c(1,3,5,7,9, 11, 13),] &lt;- round(TCombBig[c(1,3,5,7,9, 11, 13),],digits=0) TCombBig &lt;- round(TCombBig,digits=1) row.names(TCombBig) &lt;- c(&quot;Otro&quot;, &quot;&quot;, &quot;Automóvil&quot;, &quot;&quot;, &quot;0-2&quot;, &quot;&quot;, &quot;3-5&quot;, &quot;&quot;, &quot;6-10&quot;, &quot;&quot;, &quot;11-15&quot;,&quot;&quot;, &quot;16 años y más&quot;, &quot;&quot;) colnames(TCombBig) &lt;- c(&quot;Conteo=0&quot;,&quot;Conteo=1&quot;, &quot;Conteo=2&quot;,&quot;Conteo=3&quot;,&quot;Totales&quot;) TableGen1(TableData=TCombBig, TextTitle=&#39;Efecto de las Características del Vehículo sobre las Reclamaciones&#39;, Align=&#39;r&#39;, ColumnSpec=1:4, Digits = 1, BorderRight = c(1,5), ColWidth = ColWidth5) Tabla 12.4: Efecto de las Características del Vehículo sobre las Reclamaciones Conteo=0 Conteo=1 Conteo=2 Conteo=3 Totales Otro 3441.0 184.0 13.0 3.0 3641.0 94.5 5.1 0.4 0.1 48.7 Automóvil 3555.0 271.0 15.0 1.0 3842.0 92.5 7.1 0.4 0.0 51.3 0-2 4069.0 313.0 20.0 4.0 4406.0 92.4 7.1 0.5 0.1 58.9 3-5 708.0 59.0 4.0 0.0 771.0 91.8 7.7 0.5 0.0 10.3 6-10 872.0 49.0 3.0 0.0 924.0 94.4 5.3 0.3 0.0 12.3 11-15 1133.0 30.0 1.0 0.0 1164.0 97.3 2.6 0.1 0.0 15.6 16 años y más 214.0 4.0 0.0 0.0 218.0 98.2 1.8 0.0 0.0 2.9 La Tabla 12.5 muestra los efectos de las características a nivel personal, como género, edad y descuento por no reclamación (NCD), sobre la distribución de frecuencia. Las características a nivel personal no estaban disponibles en su mayoría para los vehículos de uso comercial, por lo que la Tabla 12.5 presenta estadísticas resumidas solo para aquellas observaciones que tenían cobertura de automóviles con la información de género y edad necesaria. Cuando restringimos la consideración a automóviles de uso privado, relativamente pocas pólizas no contenían información de género y edad. La Tabla 12.5 sugiere que la experiencia de conducción era bastante similar entre hombres y mujeres. Esta compañía aseguraba muy pocos conductores jóvenes, por lo que la categoría de conductores jóvenes masculinos, que normalmente tiene tasas de accidentes extremadamente altas en la mayoría de los estudios sobre automóviles, es menos relevante para estos datos. No obstante, la Tabla 12.5 sugiere fuertes efectos de la edad, con conductores mayores mostrando una mejor experiencia de conducción. La Tabla 12.5 también demuestra la importancia de los descuentos por no reclamación (NCD). Como se anticipaba, los conductores con mejores historiales de conducción, que disfrutan de un mayor NCD, tienen menos accidentes. Tabla 12.5: Efecto de las Características Personales sobre las Reclamaciones. Basado en la muestra con Auto = 1. Conteo = 0 Número Porcentaje Total Género Femenino 654 93.4 700 Masculino 2901 92.3 3142 Categoría de Edad 22-25 131 92.9 141 26-35 1354 91.7 1476 36-45 1412 93.2 1515 46-55 503 93.8 536 56-65 140 89.2 157 66 y más 15 88.2 17 Descuento por No Reclamaciones 0 889 89.6 992 10 433 91.2 475 20 361 92.8 389 30 344 93.5 368 40 291 94.8 307 50 1237 94.4 1311 Código R para generar la Tabla 12.5 # Tabla 12.5 Auto &lt;- 1*(Singapore$VehicleType==&quot;A&quot;) # EFECTO DE LAS CARACTERÍSTICAS PERSONALES SingaporeA &lt;- subset(Singapore, (Auto==1)) Count0 &lt;- 1*(SingaporeA$Clm_Count==0) rowGender &lt;- table(Count0,SingaporeA$SexInsured) rowAge&lt;- table(Count0,SingaporeA$AgeCat) rowNCD &lt;- table(Count0,SingaporeA$NCD) TabPersonal &lt;- t(cbind(rowGender, rowAge, rowNCD)) TabPersonal[,1] &lt;- TabPersonal[,1]+TabPersonal[,2] TabPersonalA &lt;- cbind(TabPersonal[,2],100*TabPersonal[,2]/TabPersonal[,1], TabPersonal[,1]) TabPersonalA[,2] &lt;- round(TabPersonalA[,2], digits = 1) row.names(TabPersonalA) &lt;- c(&quot;Femenino&quot;, &quot;Masculino&quot;, &quot;22-25&quot;, &quot;26-35&quot;, &quot;36-45&quot;, &quot;46-55&quot;, &quot;56-65&quot;, &quot;66 y más&quot;, 0, 10, 20, 30, 40, 50) `*Género*` &lt;- c(&quot;&quot;, &quot;&quot;, &quot;&quot;) -&gt; `*Categoría de Edad*` -&gt; `*Descuento por No Reclamaciones*` TabPersonalB &lt;- rbind(`*Género*`, TabPersonalA[1:2,], `*Categoría de Edad*`, TabPersonalA[3:8,], `*Descuento por No Reclamaciones*`, TabPersonalA[9:14,]) colnames(TabPersonalB) &lt;- c(&quot;Número&quot;, &quot;Porcentaje&quot;, &quot;Total&quot;) TableGen1(TableData=TabPersonalB, TextTitle=&#39;Efecto de las Características Personales sobre las Reclamaciones. Basado en la muestra con Auto = 1.&#39;, Align=&#39;ccc&#39;, ColumnSpec=1:2, Digits = 1, BorderRight = c(1,3), ColWidth = ColWidth4) %&gt;% add_header_above(c(&quot; &quot;=1, &quot;Conteo = 0&quot; = 2, &quot; &quot;=1)) %&gt;% kableExtra::column_spec(1, width = &quot;4cm&quot;) Como parte del proceso de examen, investigamos términos de interacción entre las covariables y especificaciones no lineales. Sin embargo, Tabla 12.6 resume un modelo Poisson ajustado más simple con solo efectos aditivos. Tabla 12.6 muestra que tanto la edad del vehículo como el descuento por no reclamaciones son categorías importantes, ya que los \\(t\\)-ratios de muchos de los coeficientes son estadísticamente significativos. La verosimilitud logarítmica total para este modelo es \\(L( \\mathbf{b}) =-1,776.730\\). Los niveles de referencia omitidos se indican en la nota al pie de Tabla 12.6 para ayudar a interpretar los parámetros. Por ejemplo, para \\(NCD=0\\), esperamos que un conductor con mal historial con \\(NCD=0\\) tenga \\(\\exp (0.729)=2.07\\) veces más accidentes que un conductor excelente comparable con \\(NCD=50\\). Del mismo modo, esperamos que un conductor con mal historial con \\(NCD=0\\) tenga \\(\\exp (0.729-0.293)=1.55\\) veces más accidentes que un conductor promedio comparable con \\(NCD=20\\). Tabla 12.6. Estimaciones de Parámetros de un Modelo Poisson Ajustado \\[ \\small{ \\begin{array}{rrr|rrr} \\hline &amp; \\text{Parámetro} &amp; &amp; &amp; \\text{Parámetro} &amp; \\\\ \\text{Variable} &amp; \\text{Estimación} &amp; t\\text{-ratio} &amp; \\text{Variable} &amp; \\text{Estimación} &amp; t\\text{-ratio} \\\\ \\hline &amp; &amp; &amp; (Auto=1)\\times \\text{Descuento} \\\\ &amp; &amp; &amp; \\text{por No Reclamaciones*} \\\\ \\text{Intercepto} &amp; -3.306 &amp; -6.602 &amp; 0 &amp; 0.729 &amp; 4.704 \\\\ \\text{Auto} &amp; -0.667 &amp; -1.869 &amp; 10 &amp; 0.528 &amp; 2.732 \\\\ \\text{Femenino} &amp; -0.173 &amp; -1.115 &amp; 20 &amp; 0.293 &amp; 1.326 \\\\ &amp; &amp; &amp; 30 &amp; 0.260 &amp; 1.152 \\\\ (Auto=1)\\times &amp; &amp; &amp;40 &amp; -0.095 &amp; -0.342 \\\\ \\text{Categoría de Edad*} &amp; &amp; &amp;\\text{Edad del Vehículo} \\\\ 22-25 &amp; 0.747 &amp; 0.961 &amp;\\ \\ \\ \\text{en años)*} \\\\ 26-35 &amp; 0.489 &amp; 1.251 &amp; 0-2 &amp; 1.674 &amp; 3.276 \\\\ 36-45 &amp; -0.057 &amp; -0.161 &amp; 3-5 &amp;1.504 &amp; 2.917 \\\\ 46-55 &amp; 0.124 &amp; 0.385 &amp; 6-10 &amp; 1.081&amp; 2.084 \\\\ 56-65 &amp; 0.165 &amp; 0.523 &amp; 11-15 &amp; 0.362 &amp; 0.682 \\\\ \\hline \\end{array} } \\] *Los niveles de referencia omitidos son: “66 y más” para la Categoría de Edad, “50” para el Descuento por No Reclamaciones y “16 años y más” para la Edad del Vehículo. Código R para generar la Tabla 12.6 Tabla 12.6: Estimaciones de Parámetros de un Modelo Poisson Ajustado Estimación del Parámetro \\(t\\)-Ratio Intercepto -3.306 -6.602 Femenino -0.173 -1.115 Auto 0.079 0.11 (Auto = 1) × Categoría de Edad * 22-25 -0.747 -0.961 26-35 -0.582 -0.81 36-45 -0.623 -0.87 46-55 -0.803 -1.099 56-65 -0.257 -0.343 (Auto = 1) × Descuento por No Reclamaciones * 0 0.729 4.704 10 0.528 2.732 20 0.293 1.326 30 0.26 1.152 40 -0.095 -0.342 Edad del Vehículo (en años) * 0-2 1.674 3.276 3-5 1.504 2.917 6-10 1.081 2.084 11-15 0.362 0.682 Nota: Los niveles de referencia omitidos son “66 y más” para la edad, “50” para el descuento por no reclamaciones, y “16 y más” para la edad del vehículo. Para un modelo más parsimonioso, se podría considerar eliminar las variables de automóvil, género y edad. Al eliminar estas siete variables, el modelo resultante tiene una verosimilitud logarítmica de \\(L \\left( \\mathbf{b}_{\\mathrm{Reduced}}\\right) =-1,779.420\\). Para entender si esto representa una reducción significativa, podemos calcular el estadístico de razón de verosimilitud usando la ecuación (12.7), \\[ LRT=2\\times \\left( -1,776.730 - (-1,779.420) \\right) =5.379. \\] Comparando este valor con una distribución chi-cuadrado con \\(df=7\\) grados de libertad, el valor \\(p\\) del estadístico \\(=\\Pr \\left( \\chi _{7}^2&gt;5.379\\right) =0.618\\) indica que estas variables no son estadísticamente significativas. No obstante, para propósitos de desarrollo adicional del modelo, hemos retenido las variables de automóvil, género y edad, ya que es común incluir estas variables en los modelos de tarificación. Como se describió en la Sección 12.1.4, existen varias formas de evaluar el ajuste general de un modelo. Tabla 12.7 compara varios modelos ajustados, proporcionando valores ajustados para cada nivel de respuesta y resumiendo el ajuste general con estadísticas de bondad de ajuste chi-cuadrado de Pearson. La parte izquierda de la tabla repite la información básica que apareció en la Tabla 12.1, para mayor comodidad. Para empezar, primero note que incluso sin covariables, la inclusión del término de ajuste, exposiciones, mejora dramáticamente el ajuste del modelo. Esto es intuitivamente atractivo; a medida que un conductor tiene más cobertura de seguro durante un año, es más probable que esté involucrado en un accidente cubierto por el contrato de seguro. Tabla 12.7 también muestra la mejora en el ajuste general al incluir el modelo ajustado resumido en Tabla 12.6. Al compararlo con una distribución chi-cuadrado, el valor \\(p\\) del estadístico \\(=\\Pr \\left( \\chi_{4}^2&gt;8.77\\right) =0.067\\) sugiere una concordancia entre los datos y el valor ajustado. Sin embargo, esta especificación del modelo puede mejorarse: la siguiente sección introduce un modelo binomial negativo que resulta ser un mejor ajuste para este conjunto de datos. Tabla 12.7. Comparación de Modelos de Frecuencia Ajustados \\[ \\small{ \\begin{array}{cr|rrrr} \\hline &amp; &amp; \\text{Sin} &amp; \\text{Con} &amp; \\text{Exposiciones}\\\\ \\text{Conteo} &amp; \\text{Observado} &amp; \\text{Exposiciones/} &amp; \\text{Sin} &amp; \\text{Poisson} &amp; \\text{Binomial} \\\\ &amp; &amp; \\text{Sin Covariables} &amp; \\text{Covariables} &amp; &amp; \\text{Negativo} \\\\ \\hline 0 &amp; 6,996 &amp; 6,977.86 &amp; 6,983.05 &amp; 6,986.94 &amp; 6,996.04 \\\\ 1 &amp; 455 &amp; 487.70 &amp; 477.67 &amp; 470.30 &amp; 453.40 \\\\ 2 &amp; 28 &amp; 17.04 &amp; 21.52 &amp; 24.63 &amp; 31.09 \\\\ 3 &amp; 4 &amp; 0.40 &amp; 0.73 &amp; 1.09 &amp; 2.28 \\\\ 4 &amp; 0 &amp; 0.01 &amp; 0.02 &amp; 0.04 &amp; 0.18 \\\\ \\hline \\text{Bondad de ajuste}&amp; \\text{de Pearson} &amp; 41.98 &amp; 17.62 &amp; 8.77 &amp; 1.79\\\\ \\hline \\end{array} } \\] Código en R para generar la Tabla 12.7 Tabla 12.7: Comparación de Modelos de Frecuencia Ajustados Con Exposiciones Conteo Observado Sin Exposiciones/Sin Covariables Sin Covariables Poisson Binomial Negativa 0 6996 6977.86 6983.05 6986.94 6997.01 1 455 487.69 477.67 470.3 451.64 2 28 17.04 21.52 24.63 31.72 3 4 0.4 0.73 1.09 2.42 4 0 0.01 0.02 0.04 0.2 Pearson Bondad de Ajuste 41.98 17.62 8.77 1.69 12.3 Sobre dispersión y Modelos Binomiales Negativos Aunque la simplicidad es una virtud del modelo de regresión Poisson, su forma también puede ser demasiado restrictiva. En particular, la exigencia de que la media sea igual a la varianza, conocida como equidispersión, no se cumple para muchos conjuntos de datos de interés. Si la varianza excede la media, se dice que los datos están sobredispersos. Un caso menos común ocurre cuando la varianza es menor que la media, lo que se conoce como subdispersión. Ajustando los Errores Estándar para Datos no Equidispersos Para mitigar esta preocupación, una especificación común es asumir que \\[\\begin{equation} \\mathrm{Var~}y_i=\\phi \\mu_i, \\tag{12.8} \\end{equation}\\] donde \\(\\phi &gt;0\\) es un parámetro que acomoda la posible sobre- o sub-dispersión. Como se sugiere en la ecuación (12.5), la estimación consistente de \\(\\boldsymbol \\beta\\) requiere únicamente que la función de la media esté correctamente especificada, sin necesidad de que se cumplan las suposiciones de equidispersión o de la distribución de Poisson. Esta característica también se aplica a la regresión lineal. Debido a esto, el estimador \\(\\mathbf{b}\\) a veces se denomina estimador de cuasi-verosimilitud. Con este estimador, podemos calcular las medias estimadas \\(\\widehat{\\mu}_i\\) y luego estimar \\(\\phi\\) como \\[\\begin{equation} \\widehat{\\phi }=\\frac{1}{n-(k+1)}\\sum\\limits_{i=1}^{n}\\frac{\\left( y_i-\\widehat{\\mu }_i\\right)^2}{\\widehat{\\mu }_i}. \\tag{12.9} \\end{equation}\\] Los errores estándar se basan entonces en \\[ \\widehat{\\mathrm{Var~}\\mathbf{b}}=\\left( \\widehat{\\phi }\\sum\\limits_{i=1}^{n}\\widehat{\\mu }_i\\mathbf{x}_i\\mathbf{x}_i^{\\prime }\\right)^{-1}. \\] Un inconveniente de la ecuación (12.8) es que se asume que la varianza de cada observación es un múltiplo constante de su media. Para conjuntos de datos donde esta suposición es dudosa, es común utilizar un error estándar robusto, que se calcula como la raíz cuadrada del elemento diagonal de \\[ \\mathrm{Var~}\\mathbf{b}=\\left( \\sum\\limits_{i=1}^{n}\\mu_i\\mathbf{x}_i \\mathbf{x}_i^{\\prime }\\right)^{-1}\\left( \\sum\\limits_{i=1}^{n}\\left( y_i-\\mu_i\\right)^2\\mathbf{x}_i\\mathbf{x}_i^{\\prime }\\right) \\left( \\sum\\limits_{i=1}^{n}\\mu_i\\mathbf{x}_i\\mathbf{x}_i^{\\prime }\\right)^{-1}, \\] evaluado en \\(\\widehat{\\mu }_i\\). Aquí, la idea es que \\(\\left( y_i-\\mu_i\\right)^2\\) es un estimador insesgado de \\(\\mathrm{Var~}y_i\\), independientemente de la forma. Aunque \\(\\left( y_i-\\mu_i\\right)^2\\) es un estimador deficiente de \\(\\mathrm{Var~}y_i\\) para cada observación \\(i\\), la suma ponderada \\(\\sum\\nolimits_i\\left( y_i-\\mu_i\\right)^2\\mathbf{x}_i\\mathbf{x}_i^{\\prime }\\) es un estimador fiable de \\(\\sum\\nolimits_i\\left( \\mathrm{Var~}y_i\\right) \\mathbf{x}_i\\mathbf{x}_i^{\\prime }\\). Para el estimador de cuasi-verosimilitud, la estrategia de estimación asume solo una especificación correcta de la media y utiliza una especificación más robusta de la varianza que la implicada por la distribución de Poisson. La ventaja y desventaja de este estimador es que no está vinculado a una distribución completa. Esta suposición lo hace difícil, por ejemplo, si el interés está en estimar la probabilidad de conteos de ceros. Un enfoque alternativo es suponer un modelo paramétrico más flexible que permita una mayor dispersión. Binomial Negativa Un modelo ampliamente utilizado para conteos es la binomial negativa, con función de masa de probabilidad \\[\\begin{equation} \\mathrm{Pr}(y=j)=\\left( \\begin{array}{c} j+r-1 \\\\ r-1 \\end{array} \\right) p^{r}\\left( 1-p\\right)^j, \\tag{12.10} \\end{equation}\\] donde \\(r\\) y \\(p\\) son parámetros del modelo. Para ayudar a interpretar los parámetros del modelo, cálculos sencillos muestran que \\(\\mathrm{E~}y=r(1-p)/p\\) y \\(\\mathrm{Var~}y = r(1-p)/p^2\\). La binomial negativa tiene varias ventajas importantes en comparación con la distribución de Poisson. Primero, debido a que hay dos parámetros que describen la distribución binomial negativa, tiene mayor flexibilidad para ajustar los datos. Segundo, se puede demostrar que la distribución de Poisson es un caso límite de la binomial negativa (permitiendo que \\(p\\rightarrow 1\\) y \\(r \\rightarrow \\infty\\) de modo que \\(r(1-p) \\rightarrow \\lambda\\)). En este sentido, la distribución de Poisson está anidada dentro de la binomial negativa. Tercero, se puede demostrar que la distribución binomial negativa surge de una mezcla de variables de Poisson. Por ejemplo, piense en el conjunto de datos de Singapur con cada conductor teniendo su propio valor de \\(\\lambda\\). Condicional en \\(\\lambda\\), suponga que la distribución de accidentes del conductor sigue una distribución de Poisson con parámetro \\(\\lambda\\). Además, suponga que la distribución de \\(\\lambda\\) puede describirse como una distribución gamma. Entonces, se puede demostrar que los conteos totales de accidentes tienen una distribución binomial negativa. Véase, por ejemplo, Klugman et al. (2008). Estas interpretaciones de “mezcla” son útiles para explicar resultados a los consumidores de análisis actuariales. Para la modelización de regresión, el parámetro “\\(p\\)” varía según el sujeto \\(i\\). Es habitual reparametrizar el modelo y usar una función de enlace logarítmico de modo que \\(\\sigma =1/r\\) y que \\(p_i\\) esté relacionado con la media a través de \\(\\mu_i =r(1-p_i)/p_i = \\exp (\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta)\\). Dado que la binomial negativa es una distribución de frecuencias de probabilidad, no hay dificultad en estimar características de esta distribución, como la probabilidad de conteos de ceros, después de un ajuste de regresión. Esto contrasta con la estimación de cuasi-verosimilitud de un modelo de Poisson con una especificación ad hoc de la varianza, resumida en la ecuación (12.9). Ejemplo: Datos de Automóviles en Singapur - Continuación. La distribución binomial negativa se ajustó a los datos de Singapur en la Sección 12.2 utilizando el conjunto de covariables resumido en Tabla 12.6. La verosimilitud resultante fue \\(\\mathrm{L}_{NegBin}(\\mathbf{b})=-1,774.494;\\) esto es mayor que la verosimilitud del ajuste Poisson \\(\\mathrm{L}_{Poisson}\\left( \\mathbf{b} \\right) =-1,776.730\\) debido a un parámetro adicional. La prueba de razón de verosimilitud habitual no es formalmente apropiada porque los modelos solo están anidados en un sentido límite. Es más útil comparar las estadísticas de bondad de ajuste presentadas en Tabla 12.7. Aquí, vemos que la binomial negativa es un mejor ajuste que el modelo de Poisson (con los mismos componentes sistemáticos). Una prueba chi-cuadrado sobre si la binomial negativa con covariables es adecuada arroja un valor \\(p\\) \\(=\\Pr \\left( \\chi_{4}^2&gt;1.79\\right) =0.774\\), lo que sugiere un fuerte acuerdo entre los datos observados y los valores ajustados. Interpretamos los resultados de la Tabla 12.7 como que la distribución binomial negativa captura bien la heterogeneidad en la distribución de frecuencia de accidentes. Código R para la Distribución Binomial Negativa con Datos de Singapur library(MASS) #Singapore &lt;- read.csv(&quot;CSVData/SingaporeAuto.csv&quot;, header=TRUE) Singapore$Female &lt;- 1*(Singapore$SexInsured == &quot;F&quot; ) Singapore$Auto &lt;- 1*(Singapore$VehicleType==&quot;A&quot;) NCD1F &lt;- relevel(factor(Singapore$NCD), ref=&quot;50&quot;) AgeCatF &lt;- relevel(factor(Singapore$AgeCat), ref=&quot;7&quot;) VAgecat1F &lt;-relevel(factor(Singapore$VAgecat1), ref=&quot;6&quot;) # AJUSTAR UN MODELO BINOMIAL NEGATIVO; CountNB1 &lt;- MASS::glm.nb(Clm_Count ~ Female + Auto + Auto:AgeCatF + Auto:NCD1F + VAgecat1F + offset(LNWEIGHT), link=log, data = Singapore) temp &lt;- summary(CountNB1) summary(temp) temp$twologlik/2 # ELIMINAR UN FACTOR DEL MODELO BINOMIAL NEGATIVO; CountNB2 &lt;- MASS::glm.nb(Clm_Count ~ Female + Auto + Auto:NCD1F + VAgecat1F + offset(LNWEIGHT), link=log, data = Singapore) summary(CountNB2) 12.4 Otros Modelos de Conteo Los actuarios están familiarizados con una variedad de modelos de frecuencia; ver, por ejemplo, Klugman et al. (2008). En principio, cada modelo de frecuencia podría usarse en un contexto de regresión simplemente incorporando un componente sistemático, \\(\\mathbf{x}^{\\prime}\\boldsymbol \\beta\\), en uno o más parámetros del modelo. Sin embargo, los analistas han encontrado que cuatro variaciones de los modelos básicos funcionan bien para ajustar modelos a datos y proporcionan una plataforma intuitiva para interpretar los resultados del modelo. 12.4.1 Modelos Inflados en Ceros Para muchos conjuntos de datos, un aspecto problemático es el número “excesivo” de ceros, en comparación con un modelo especificado. Por ejemplo, esto podría ocurrir en los datos de reclamos de automóviles porque los asegurados son reacios a reportar reclamos, temiendo que un reclamo reportado resulte en primas de seguro más altas en el futuro. Por lo tanto, tenemos un número mayor al anticipado de ceros debido a la no presentación de reclamos. Un modelo inflado en ceros representa el número de reclamos \\(y_i\\) como una mezcla de una masa puntual en cero y otra distribución de frecuencia de reclamos, digamos \\(g_i(j)\\) (que típicamente es Poisson o binomial negativa). (Podríamos interpretar la masa puntual como la tendencia a no reportar.) La probabilidad de obtener la masa puntual se modelaría mediante un modelo de conteo binario como, por ejemplo, el modelo logit \\[ \\pi_i=\\frac{\\exp \\left( \\mathbf{x}_i^{\\prime}\\boldsymbol \\beta _{1}\\right) }{1+\\exp \\left( \\mathbf{x}_i^{\\prime}\\boldsymbol \\beta _{1}\\right) }. \\] Como consecuencia de la suposición de mezcla, la distribución de conteo inflada en ceros puede escribirse como \\[\\begin{equation} \\Pr \\left( y_i=j\\right) =\\left\\{ \\begin{array}{ll} \\pi_i+(1-\\pi_i)g_i(0) &amp; j=0 \\\\ (1-\\pi_i)g_i(j) &amp; j=1,2,... \\end{array} \\right. . \\tag{12.11} \\end{equation}\\] A partir de la ecuación (12.11), vemos que los ceros podrían surgir tanto de la masa puntual como de la otra distribución de frecuencia de reclamos. Para ver los efectos de un modelo inflado en ceros, supongamos que \\(g_i\\) sigue una distribución Poisson con media \\(\\mu_i\\). Entonces, cálculos simples muestran que \\[ \\mathrm{E~} y_i = (1 - \\pi_i) \\mu_i \\] y \\[ \\mathrm{Var~} y_i = \\pi_i \\mu_i + \\pi_i\\mu_i^2(1-\\pi_i). \\] Así, para el modelo Poisson inflado en ceros, la varianza siempre excede a la media, lo que acomoda la sobredispersión en comparación con el modelo Poisson. Ejemplo: Seguro de Automóviles. Yip y Yau (2005) examinan una cartera de \\(n=2,812\\) pólizas de automóviles disponibles en SAS Institute, Inc. Las variables explicativas incluyen edad, género, estado civil, ingreso anual, categoría laboral y nivel educativo del asegurado. Para este conjunto de datos, encontraron que varios modelos de conteo inflados en ceros se adaptaban bien a la presencia de ceros adicionales. 12.4.2 Modelos Hurdle Un “modelo hurdle” proporciona otro mecanismo para modificar distribuciones básicas de conteo para representar situaciones con un exceso de ceros. Los modelos hurdle pueden ser motivados por procesos de toma de decisiones secuenciales enfrentados por los individuos. Por ejemplo, en la elección de atención médica, podemos pensar en la decisión de un individuo de buscar atención médica como un proceso inicial. Condicional a haber buscado atención \\(\\{y \\geq 1\\}\\), la cantidad de atención médica es una decisión tomada por un proveedor de atención médica (como un médico u hospital), lo que representa un proceso diferente. Uno necesita superar el primer “hurdle” (la decisión de buscar atención médica) para abordar el segundo (la cantidad de atención médica). Un atractivo del modelo hurdle es su conexión con el modelo “principal-agente”, donde el proveedor (agente) decide la cantidad después de que el asegurado (principal) haya establecido contacto. Como otro ejemplo, en el seguro de propiedad y accidentes, el proceso de decisión que utiliza el asegurado para reportar el primer reclamo puede diferir del que utiliza para reportar reclamos subsiguientes. Para representar los modelos hurdle, sea \\(\\pi_i\\) la probabilidad de que \\(\\{y_i=0\\}\\) utilizada para la primera decisión, y supongamos que \\(g_i\\) representa la distribución de conteo que se utilizará para la segunda decisión. Definimos la función de masa de probabilidad como \\[\\begin{equation} \\Pr \\left( y_i=j\\right) =\\left\\{ \\begin{array}{ll} \\pi_i &amp; j=0 \\\\ k_i g_i(j) &amp; j=1,2,... \\end{array} \\right. . \\tag{12.12} \\end{equation}\\] donde \\(k_i = (1-\\pi_i)/(1-g_i(0))\\). Al igual que con los modelos inflados en ceros, un modelo logit podría ser adecuado para representar \\(\\pi_i\\). Para ver los efectos de un modelo hurdle, supongamos que \\(g_i\\) sigue una distribución Poisson con media \\(\\mu_i\\). Entonces, cálculos simples muestran que \\[ \\mathrm{E~} y_i = k_i \\mu_i \\] y \\[ \\mathrm{Var~} y_i = k_i \\mu_i + k_i \\mu_i^2(1-k_i). \\] Dado que \\(k_i\\) puede ser mayor o menor que 1, este modelo permite tanto subdispersión como sobredispersión en comparación con el modelo Poisson. El modelo hurdle es un caso especial del “modelo de dos partes” descrito en el Capítulo 16. Allí, veremos que para los modelos de dos partes, la cantidad de atención médica utilizada puede ser una variable continua, además de una variable de conteo. Un atractivo de los modelos de dos partes es que los parámetros de cada hurdle/parte pueden analizarse por separado. Específicamente, la log-verosimilitud para el \\(i\\)-ésimo sujeto puede escribirse como \\[ \\ln \\left[ \\Pr \\left( y_i=j\\right) \\right] =\\left[ \\mathrm{I}(j=0)\\ln \\pi_i+\\mathrm{I}(j\\geq 1)\\ln (1-\\pi_i)\\right] +\\mathrm{I}(j\\geq 1)\\ln \\frac{g_i(j)}{(1-g_i(0))}. \\] Los términos en los corchetes en el lado derecho corresponden a la verosimilitud para un modelo binario de conteo. Los términos posteriores corresponden a un modelo de conteo con los ceros eliminados (conocido como un modelo truncado). Si los parámetros para las dos piezas son diferentes (“separables”), entonces la maximización puede hacerse por separado para cada parte. 12.4.3 Modelos de Heterogeneidad En un modelo de heterogeneidad, se permite que uno o más parámetros del modelo varíen de manera aleatoria. La motivación es que estos parámetros aleatorios capturan características no observadas de un sujeto. Por ejemplo, supongamos que \\(\\alpha_i\\) representa un parámetro aleatorio y que \\(y_i\\), dado \\(\\alpha_i\\), tiene una media condicional \\(\\exp \\left( \\alpha_i + \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta \\right)\\). Interpretamos \\(\\alpha_i\\), llamado componente de heterogeneidad, como una representación de características no observadas del sujeto que contribuyen de manera lineal al componente sistemático \\(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta\\). Para ver los efectos del componente de heterogeneidad en la distribución de conteo, cálculos básicos muestran que \\[ \\mathrm{E~} y_i = \\exp \\left( \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta \\right) = \\mu_i \\] y \\[ \\mathrm{Var~} y_i = \\mu_i + \\mu_i^2 \\mathrm{Var}\\left( e^{\\alpha_i} \\right), \\] donde típicamente asumimos que \\(\\mathrm{E}\\left( e^{\\alpha_i} \\right) = 1\\) para la identificación de parámetros. Así, los modelos de heterogeneidad acomodan fácilmente la sobredispersión en los conjuntos de datos. Es común suponer que la distribución de conteo es Poisson, condicional a \\(\\alpha_i\\). Hay varias opciones para la distribución de \\(\\alpha_i\\), siendo las dos más comunes la log-gamma y la log-normal. Para la primera, se asume que \\(\\exp \\left( \\alpha_i \\right)\\) sigue una distribución gamma, lo que implica que \\(\\exp \\left( \\alpha_i + \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta \\right)\\) también sigue una distribución gamma. Recordemos que ya hemos señalado en la Sección 12.3 que usar una distribución de mezcla gamma para conteos Poisson da como resultado una distribución binomial negativa. Por lo tanto, esta elección proporciona otra motivación para la popularidad de la binomial negativa como la distribución de conteo preferida. Para la segunda, es bastante común en análisis de datos aplicados asumir que una cantidad observada como \\(\\exp \\left( \\alpha_i \\right)\\) tiene una distribución normal. Aunque no hay expresiones analíticas en forma cerrada para la distribución marginal de conteo resultante, hay varios paquetes de software que facilitan las dificultades computacionales. El componente de heterogeneidad es particularmente útil en muestras repetidas, donde puede usarse para modelar la agrupación de observaciones. Las observaciones de diferentes grupos tienden a ser disímiles en comparación con las observaciones dentro de un grupo, una característica conocida como heterogeneidad. La similitud de las observaciones dentro de un grupo puede capturarse mediante un término común \\(\\alpha_i\\). Diferentes términos de heterogeneidad para observaciones de diferentes grupos pueden capturar la heterogeneidad. Para una introducción a la modelización en muestreo repetido, véase el Capítulo 10. Ejemplo: Seguro de Responsabilidad Civil de Automóviles en España. Boucher et al. (2006) analizaron una cartera de \\(n=548,830\\) contratos de automóviles de una importante compañía de seguros que opera en España. Los reclamos eran por responsabilidad civil de automóviles, por lo que, en caso de un accidente automovilístico, el monto que el asegurado debe pagar por daños no materiales a otras partes está cubierto bajo el contrato de seguro. Para estos datos, la frecuencia media de reclamos fue aproximadamente del 6.9%. Las variables explicativas incluyen edad, género, ubicación de conducción, experiencia de conducción, tamaño del motor y tipo de póliza. El artículo considera una amplia variedad de modelos inflados en ceros, hurdle y de heterogeneidad, mostrando que cada uno de ellos mejoró sustancialmente el modelo Poisson básico. 12.4.4 Modelos de Clases Latentes En la mayoría de los conjuntos de datos, es fácil pensar en clasificaciones de sujetos que el analista quisiera hacer para promover la homogeneidad entre las observaciones. Algunos ejemplos incluyen: “personas saludables” y “personas enfermas” al examinar los gastos en atención médica, conductores de automóviles que tienen más probabilidades de presentar un reclamo en caso de accidente en comparación con aquellos que son reacios a hacerlo, y médicos que son “bajos” riesgos en comparación con “altos” riesgos al examinar la cobertura de seguros por negligencia médica. Para muchos conjuntos de datos de interés, dicha información de clasificación obvia no está disponible y se dice que es no observada o latente. Un modelo de “clases latentes” todavía emplea esta idea de clasificación, pero la trata como una variable aleatoria discreta desconocida. Así, al igual que en las Secciones 12.4.1-12.4.3, utilizamos modelos de mezcla para modificar las distribuciones básicas de conteo, pero ahora asumimos que la mezcla es una variable aleatoria discreta que interpretamos como la clase latente. Para ser específicos, supongamos que tenemos dos clases, “bajo riesgo” y “alto riesgo”, con probabilidad \\(\\pi_L\\) de que un sujeto pertenezca a la clase de bajo riesgo. Entonces, podemos escribir la función de masa de probabilidad como \\[\\begin{equation} \\Pr \\left( y_i=j\\right) =\\pi_L \\Pr \\left( y_i=j;L\\right) + \\left( 1-\\pi_L \\right) \\Pr \\left( y_i=j;H \\right), \\tag{12.13} \\end{equation}\\] donde \\(\\Pr \\left( y_i=j;L \\right)\\) y \\(\\Pr \\left( y_i=j;H \\right)\\) son las funciones de masa de probabilidad para los riesgos bajos y altos, respectivamente. Este modelo es intuitivamente agradable, ya que corresponde a la percepción de un analista sobre el comportamiento del mundo. Es flexible en el sentido de que el modelo acomoda fácilmente la subdispersión y sobredispersión, distribuciones con colas largas y distribuciones bimodales. Sin embargo, esta flexibilidad también conlleva dificultades con respecto a los problemas computacionales. Existe la posibilidad de múltiples máximos locales al estimar mediante máxima verosimilitud. La convergencia puede ser más lenta en comparación con otros métodos descritos en las Secciones 12.4.1-12.4.3. No obstante, los modelos de clases latentes han demostrado ser fructíferos en aplicaciones de interés para los actuarios. Ejemplo: Experimento de Seguro de Salud Rand. Deb y Trivedi (2002) encontraron una fuerte evidencia de que un modelo de clases latentes funciona bien en comparación con el modelo hurdle. Ellos examinaron conteos de utilización de gastos en atención médica para el Experimento de Seguro de Salud Rand, un conjunto de datos que ha sido ampliamente analizado en la literatura de economía de la salud. Interpretaron \\(\\Pr \\left( y_i=j;L\\right)\\) como una distribución de usuarios de atención médica poco frecuentes y \\(\\Pr \\left( y_i=j;H\\right)\\) como una distribución de usuarios frecuentes de atención médica. Cada distribución se basó en una distribución binomial negativa, con diferentes parámetros para cada clase. Encontraron diferencias estadísticamente significativas para sus cuatro variables de seguro, dos variables de coseguro, una variable que indicaba si había un deducible individual y una variable que describía el límite máximo reembolsado. Debido a que los sujetos fueron asignados aleatoriamente a planes de seguro (algo muy inusual), los efectos de las variables de seguro sobre la utilización de atención médica son particularmente interesantes desde un punto de vista político, al igual que las diferencias entre sujetos de bajo y alto uso. Para sus datos, estimaron que aproximadamente el 20% estaban en la clase de alto uso. 12.5 Lecturas Adicionales y Referencias La distribución de Poisson fue derivada por Poisson (1837) como un caso límite de la distribución binomial. Greenwood y Yule (1920) derivaron la distribución binomial negativa como una mezcla de una Poisson con una distribución gamma. Curiosamente, un ejemplo del artículo de 1920 fue utilizar la distribución de Poisson como un modelo de accidentes, con la media como una variable aleatoria gamma, reflejando la variación de los trabajadores en una población. Greenwood y Yule se refirieron a esto como individuos sujetos a “accidentes repetidos”, lo que otros autores han denominado “propensión a los accidentes.” La primera aplicación de la regresión Poisson se debe a Cochran (1940) en el contexto del modelado ANOVA y a Jorgensen (1961) en el contexto de la regresión lineal múltiple. Como se describe en la Sección 12.2, Weber (1971) presenta la primera aplicación a los accidentes de automóviles. Este capítulo se centra en las aplicaciones de modelos de conteo en seguros y gestión de riesgos. Para aquellos interesados en automóviles, existe una literatura relacionada sobre estudios del proceso de accidentes de vehículos de motor, véase, por ejemplo, Lord et al. (2005). Para aplicaciones en otras áreas de las ciencias sociales y desarrollo adicional de modelos, nos remitimos a Cameron y Trivedi (1998). Referencias Bortkiewicz, L. von (1898). Das Gesetz de Kleinen Zahlen. Leipzig, Teubner. Boucher, Jean-Philippe, Michel Denuit and Montserratt Guill'{e}n (2006). Risk classification for claim counts: A comparative analysis of various zero-inflated mixed Poisson and hurdle models. Working paper. Cameron, A. Colin and Pravin K. Trivedi. (1998) Regression Analysis of Count Data. Cambridge University Press, Cambridge. Cochran, W. G. (1940). The analysis of variance when experimental errors follow the Poisson or binomial law. Annals of Mathematical Statistics 11, 335-347. Deb, Partha and Pravin K. Trivedi (2002). The structure of demand for health care: latent class versus two-part models. Journal of Health Economics 21, 601-625. Fournier, Gary M. and Melayne Morgan McInnes (2001). The case of experience rating in medical malpractice insurance: An empirical evaluation. The Journal of Risk and Insurance 68, 255-276. Frees, Edward W. and Emiliano Valdez (2008). Hierarchical insurance claims modeling. Journal of the American Statistical Association 103, 1457-1469. Greenwood, M. and G. U. Yule (1920). An inquiry into the nature of frequency distributions representative of multiple happenings with particular reference to the occurrence of multiple attacks of disease or of repeated accidents. Journal of the Royal Statistical Society 83, 255-279. Jones, Andrew M. (2000). Health econometrics. Chapter 6 of the Handbook of Health Economics, Volume 1. Edited by Antonio.J. Culyer, and Joseph.P. Newhouse, Elsevier, Amersterdam. 265-344. Jorgensen, Dale W. (1961). Multiple regression analysis of a Poisson process. Journal of the American Statistical Association 56, 235-245. Lord, Dominique, Simon P. Washington and John N. Ivan (2005). Poisson, Poisson-gamma and zero-inflated regression models of motor vehicle crashes: Balancing statistical theory and fit. Accident Analysis and Prevention 37, 35-46. Klugman, Stuart A, Harry H. Panjer and Gordon E. Willmot (2008). Loss Models: From Data to Decisions. John Wiley &amp; Sons, Hoboken, New Jersey. Purcaru, Oana and Michel Denuit (2003). Dependence in dynamic claim frequency credibility models. ASTIN Bulletin 33(1), 23-40. Weber, Donald C. (1971). Accident rate potential: An application of multiple regression analysis of a Poisson process. Journal of the American Statistical Association 66, 285-288. Yip, Karen C. H. and Kelvin K.W. Yau (2005). On modeling claim frequency data in general insurance with extra zeros. Insurance: Mathematics and Economics 36(2) 153-163. 12.6 Ejercicios 12.1 Muestra que la log-verosimilitud en la ecuación (12.2) tiene un máximo en \\(\\widehat{\\mu }=\\overline{y}\\). 12.2 Para los datos de la Tabla 12.1, confirma que el estadístico de Pearson en la ecuación (12.3) es 41.98. 12.3 Residuos de Poisson. Considera una regresión de Poisson. Sea \\(e_i = y_i - \\widehat{\\mu}_i\\) el residuo ordinario \\(i\\)-ésimo. Supón que se utiliza una intersección en el modelo de modo que una de las variables explicativas \\(x\\) es una constante igual a uno. Demuestra que el promedio de los residuos ordinarios es 0. Demuestra que la correlación entre los residuos ordinarios y cada variable explicativa es cero. 12.4 Distribución Binomial Negativa. Supón que \\(y_1, \\ldots, y_n\\) son i.i.d. con una distribución binomial negativa con parámetros \\(r\\) y \\(p\\). Determina los estimadores de máxima verosimilitud. Utiliza el mecanismo de muestreo en la parte (a) pero con los parámetros \\(\\sigma =1/r\\) y \\(\\mu\\) donde \\(\\mu =r(1-p)/p\\). Determina los estimadores de máxima verosimilitud de \\(\\sigma\\) y \\(\\mu\\). Supón que \\(y_1, \\ldots, y_n\\) son independientes con \\(y_i\\) teniendo una distribución binomial negativa con parámetros \\(r\\) y \\(p_i\\), donde \\(\\sigma =1/r\\) y \\(p_i\\) satisface \\(r(1-p_i)/p_i=\\exp (\\mathbf{x}_i^{\\prime }\\boldsymbol \\beta) (= \\mu_i)\\). Determina la función de puntaje en términos de \\(\\sigma\\) y \\(\\boldsymbol \\beta\\). 12.5 Datos de Gastos Médicos. Este ejercicio considera datos de la Encuesta del Panel de Gastos Médicos (MEPS) descritos en el Ejercicio 1.1 y la Sección 11.4. Nuestra variable dependiente consiste en el número de visitas ambulatorias (COUNTOP). Para MEPS, los eventos ambulatorios incluyen visitas a departamentos ambulatorios de hospitales, consultas con proveedores en consultorios y visitas a salas de emergencia, excluyendo los servicios dentales. (Los servicios dentales, en comparación con otros tipos de servicios de salud, son más predecibles y ocurren de manera más regular). Las estancias hospitalarias con la misma fecha de admisión y alta, conocidas como “estancias de cero noches”, también se incluyeron en los conteos y gastos ambulatorios. (Los pagos asociados con visitas a la sala de emergencias que preceden inmediatamente a una estancia hospitalaria se incluyeron en los gastos de hospitalización. Los medicamentos recetados que pueden vincularse con hospitalizaciones se incluyeron en los gastos de hospitalización, no en la utilización ambulatoria). Considera las variables explicativas descritas en la Sección 11.4. Proporciona una tabla de conteos, un histograma y estadísticas resumidas de COUNTOP. Nota la forma de la distribución y la relación entre la media muestral y la varianza muestral. Crea tablas de medias de COUNTOP por nivel de GÉNERO, etnicidad, región, educación, salud física autoevaluada, salud mental autoevaluada, limitación de actividad, ingresos y seguro. ¿Sugerirían estas tablas que estas variables explicativas tienen un impacto en COUNTOP? Como línea base, estima un modelo de Poisson sin ninguna variable explicativa y calcula un estadístico chi-cuadrado de Pearson para bondad de ajuste (a nivel individual). Estima un modelo de Poisson utilizando las variables explicativas en la parte (b). d(i). Comenta brevemente sobre la significancia estadística de cada variable. d(ii). Proporciona una interpretación del coeficiente de GÉNERO. d(iii). Calcula un estadístico chi-cuadrado de Pearson (a nivel individual) para la bondad de ajuste. Compáralo con el de la parte (b). Basado en este estadístico y en la significancia estadística de los coeficientes discutidos en la parte d(i), ¿qué modelo prefieres? d(iv). Reestima el modelo utilizando el estimador de cuasi-verosimilitud del parámetro de dispersión. ¿Cómo han cambiado tus comentarios en la parte d(i)? Estima un modelo binomial negativo utilizando las variables explicativas de la parte (d). e(i). Comenta brevemente sobre la significancia estadística de cada variable. e(ii). Calcula un estadístico chi-cuadrado de Pearson (a nivel individual) para la bondad de ajuste. Compáralo con los de las partes (b) y (d). ¿Qué modelo prefieres? Cita también el estadístico \\(AIC\\) en tu comparación. e(iii). Reestima el modelo, eliminando el factor ingreso. Utiliza la prueba de razón de verosimilitud para determinar si el ingreso es un factor estadísticamente significativo. Como verificación de robustez, estima un modelo de regresión logística utilizando las variables explicativas de la parte (d). ¿Los signos y la significancia de los coeficientes de este ajuste de modelo proporcionan la misma interpretación que en el modelo binomial negativo en la parte (e)? 12.6 Dos Poisson de Poblaciones. Podemos expresar el problema de dos poblaciones en un contexto de regresión utilizando una variable explicativa. Específicamente, supón que \\(x_i\\) solo toma los valores 0 y 1. De las \\(n\\) observaciones, \\(n_0\\) toman el valor \\(x=0\\). Estas \\(n_0\\) observaciones tienen un valor promedio de \\(y\\) de \\(\\overline{y}_0\\). Las \\(n_1 =n-n_0\\) observaciones restantes tienen valor \\(x=1\\) y un valor promedio de \\(y\\) de \\(\\overline{y}_1\\). Utiliza el modelo de Poisson con la función de enlace logarítmica y el componente sistemático \\(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta = \\beta_0 +\\beta_1 x_i\\). Determina los estimadores de máxima verosimilitud de \\(\\beta_0\\) y \\(\\beta_1\\), respectivamente. Supón que \\(n_0 = 10\\), \\(n_1= 90\\), \\(\\overline{y}_0 = 0.20\\) y \\(\\overline{y}_1= 0.05\\). Utilizando tus resultados en la parte a(i), calcula los estimadores de máxima verosimilitud de \\(\\beta_0\\) y \\(\\beta_1\\), respectivamente. Determina la matriz de información. "],["C13GLM.html", "Capítulo 13 Modelos Lineales Generalizados 13.1 Introducción 13.2 Modelo GLM 13.3 Estimación 13.4 Aplicación: Gastos Médicos 13.5 Residuales 13.6 Distribución de Tweedie 13.7 Lecturas adicionales y referencias 13.8 Ejercicios 13.9 Suplementos Técnicos - Familia Exponencial", " Capítulo 13 Modelos Lineales Generalizados Vista Previa del Capítulo. Este capítulo describe un marco unificador para el modelo lineal de la Parte I y los modelos binarios y de conteo en los Capítulos 11 y 12. Los modelos lineales generalizados, conocidos comúnmente por el acrónimo GLM, representan una clase importante de modelos de regresión no lineal que han encontrado un uso extensivo en la práctica actuarial. Este marco unificador no solo abarca muchos de los modelos que hemos visto, sino que también proporciona una plataforma para nuevos modelos, incluyendo las regresiones gamma para datos con colas gruesas y las distribuciones de “Tweedie” para datos de dos partes. 13.1 Introducción Existen muchas maneras de extender o generalizar el modelo de regresión lineal. Este capítulo introduce una extensión que es tan utilizada que se conoce como el “modelo lineal generalizado”, o por el acrónimo GLM. Los modelos lineales generalizados incluyen regresiones lineales, logísticas y de Poisson, todas como casos especiales. Una característica común de estos modelos es que en cada caso podemos expresar la media de la respuesta como una función de combinaciones lineales de variables explicativas. En el contexto de los GLM, es habitual usar \\(\\mu_i = \\mathrm{E}~y_i\\) para la media de la respuesta y llamar \\(\\eta_i = \\mathbf{x}_i^{\\mathbf{\\prime}} \\boldsymbol \\beta\\) el componente sistemático del modelo. Hemos visto que podemos expresar el componente sistemático como: \\(\\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta = \\mu_i\\), para la regresión lineal (normal), \\(\\mathbf{x}_i^{\\mathbf{\\prime}} \\boldsymbol \\beta = \\exp(\\mu_i)/(1+\\exp(\\mu_i))\\), para la regresión logística y \\(\\mathbf{x}_i^{\\mathbf{\\prime}} \\boldsymbol \\beta = \\ln (\\mu_i)\\), para la regresión de Poisson. Para los GLM, el componente sistemático está relacionado con la media a través de la expresión \\[\\begin{equation} \\eta _i = \\mathbf{x}_i^{\\mathbf{\\prime}} \\boldsymbol \\beta = \\mathrm{g}\\left( \\mu _i\\right). \\tag{13.1} \\end{equation}\\] Aquí, g(.) es conocida y se llama función de enlace. La inversa de la función de enlace, \\(\\mu _i = \\mathrm{g}^{-1}( \\mathbf{x}_i^{\\mathbf{\\prime}} \\boldsymbol \\beta)\\), es la función de la media. La segunda característica común implica la distribución de las variables dependientes. En la Sección 13.2, introduciremos la familia exponencial lineal de distribuciones, una extensión de la distribución exponencial. Esta familia incluye la normal, Bernoulli y Poisson como casos especiales. La tercera característica común de los modelos GLM es la robustez de la inferencia con respecto a la elección de distribuciones. Aunque la regresión lineal está motivada por la teoría de la distribución normal, hemos visto que las respuestas no necesitan estar distribuidas normalmente para que los procedimientos de inferencia estadística sean efectivos. Las suposiciones de muestreo de la Sección 3.2 se centran en: la forma de la función de la media (suposición F1), variables explicativas no estocásticas o exógenas (F2), varianza constante (F3) y independencia entre observaciones (F4). Los modelos GLM mantienen las suposiciones F2 y F4 y generalizan F1 a través de la función de enlace. La elección de diferentes distribuciones nos permite relajar F3 especificando la varianza como una función de la media, escrita como \\(\\mathrm{Var~}y_i = \\phi v(\\mu_i)\\). Tabla 13.1 muestra cómo la varianza depende de la media para diferentes distribuciones. Como veremos al considerar la estimación (Sección 13.3), es la elección de la función de varianza la que impulsa las propiedades más importantes de la inferencia, no la elección de la distribución. Tabla 13.1. Funciones de Varianza para Distribuciones Seleccionadas \\[ \\small{ \\begin{array}{lc} \\hline \\text{Distribución} &amp; \\text{Función de Varianza }v(\\mu) \\\\ \\hline \\text{Normal} &amp; 1 \\\\ \\text{Bernoulli} &amp; \\mu ( 1- \\mu ) \\\\ \\text{Poisson} &amp; \\mu \\\\ \\text{Gamma} &amp; \\mu ^2 \\\\ \\text{Gaussiana Inversa} &amp; \\mu ^3 \\\\ \\hline \\end{array} } \\] Al considerar la regresión en el contexto de los GLM, podremos manejar variables dependientes que sean aproximadamente normales, binarias o que representen conteos, todo dentro de un único marco. Esto facilitará nuestra comprensión de la regresión al permitirnos ver el “panorama general” y no preocuparnos tanto por los detalles. Además, la generalidad de los GLM nos permitirá introducir nuevas aplicaciones, como las regresiones gamma, que son útiles para distribuciones con colas gruesas, y las llamadas distribuciones “Tweedie” para datos de dos partes. Los datos de dos partes son un tema que se aborda en el Capítulo 16, donde hay una masa en cero y un componente continuo. En el caso de los datos de reclamaciones de seguros, el cero representa la ausencia de una reclamación y el componente continuo representa el monto de una reclamación. Este capítulo describe los procedimientos de estimación para calibrar los modelos GLM, pruebas de significancia y estadísticas de bondad de ajuste para documentar la utilidad del modelo, y residuos para evaluar la robustez del ajuste del modelo. Veremos que el trabajo que realizamos anteriormente en modelos de regresión lineal, binaria y de conteo proporciona las bases para las herramientas necesarias para el modelo GLM. De hecho, muchas de estas herramientas y conceptos son ligeras variaciones de lo que ya hemos desarrollado y podremos construir sobre estas bases. 13.2 Modelo GLM Para especificar un GLM, el analista elige una distribución subyacente de la respuesta, tema de la Sección 13.2.1, y una función que vincula la media de la respuesta con las covariables, tema de la Sección 13.2.2. 13.2.1 Familia Exponencial Lineal de Distribuciones Definición. La distribución de la familia exponencial lineal es \\[\\begin{equation} \\mathrm{f}(y; \\theta, \\phi) = \\exp \\left( \\frac{y\\theta - b(\\theta)}{\\phi} + S(y, \\phi) \\right). \\tag{13.2} \\end{equation}\\] Aquí, \\(y\\) es una variable dependiente y \\(\\theta\\) es el parámetro de interés. La cantidad \\(\\phi\\) es un parámetro de escala. El término \\(b(\\theta)\\) depende solo del parámetro \\(\\theta\\), no de la variable dependiente. El estadístico \\(S(y, \\phi)\\) es una función de la variable dependiente y el parámetro de escala, pero no del parámetro \\(\\theta\\). La variable dependiente \\(y\\) puede ser discreta, continua o una mezcla. Así, \\(\\mathrm{f}(.)\\) puede interpretarse como una función de densidad o de masa, dependiendo de la aplicación. Tabla 13.8 proporciona varios ejemplos, incluyendo las distribuciones normal, binomial y de Poisson. Para ilustrar, consideremos una distribución normal con una función de densidad de probabilidad de la forma: \\[\\begin{eqnarray*} \\mathrm{f}(y; \\mu, \\sigma^2) &amp;=&amp; \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(- \\frac{(y-\\mu)^2}{2\\sigma^2}\\right) \\\\ &amp;=&amp; \\exp\\left(\\frac{(y\\mu - \\mu^2/2)}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2} \\ln(2\\pi \\sigma^2)\\right). \\end{eqnarray*}\\] Con las elecciones \\(\\theta = \\mu\\), \\(\\phi = \\sigma^2\\), \\(b(\\theta) = \\theta^2/2\\) y \\(S(y, \\phi) = -y^2/(2\\phi) - \\ln(2\\pi\\phi)/2\\), vemos que la función de densidad normal puede expresarse como en la ecuación (13.2). Para la distribución en la ecuación (13.2), algunos cálculos sencillos muestran que: \\(\\mathrm{E~}y = b^{\\prime}(\\theta)\\) y \\(\\mathrm{Var~}y = \\phi b^{\\prime\\prime}(\\theta)\\). Para referencia, estos cálculos aparecen en la Sección 13.9.2. Para ilustrar, en el contexto del ejemplo de la distribución normal mencionado arriba, es fácil verificar que \\(\\mathrm{E~}y = b^{\\prime}(\\theta) = \\theta = \\mu\\) y \\(\\mathrm{Var~} y = \\sigma^2 b^{\\prime\\prime}(\\theta) = \\sigma^2\\), como se esperaba. En situaciones de modelado de regresión, la distribución de \\(y_i\\) varía por observación a través del subíndice “\\(i\\)”. Es habitual dejar que la familia de distribuciones permanezca constante, pero permitir que los parámetros varíen por observación utilizando la notación \\(\\theta_i\\) y \\(\\phi_i\\). Para nuestras aplicaciones, la variación del parámetro de escala se debe a factores de peso conocidos. Específicamente, cuando el parámetro de escala varía por observación, se sigue \\(\\phi_i = \\phi / w_i\\), es decir, una constante dividida por un peso conocido \\(w_i\\). Con la relación \\(\\mathrm{Var~}y_i = \\phi_i b^{\\prime\\prime}(\\theta_i) = \\phi b^{\\prime\\prime}(\\theta_i) / w_i\\), tenemos que un peso mayor implica una varianza menor, todo lo demás constante. 13.2.2 Funciones de Enlace En situaciones de regresión, deseamos entender el impacto de \\(\\eta_i = \\mathbf{x}_i^{\\mathbf{\\prime}} \\boldsymbol \\beta\\), el componente sistemático. Como vimos en la subsección anterior, podemos expresar la media de \\(y_i\\) como \\(\\mathrm{E~}y_i = \\mu_i = b^{\\prime}(\\theta_i)\\). La ecuación (13.1) sirve para “enlazar” el componente sistemático con \\(\\mu_i\\) y, por lo tanto, con el parámetro \\(\\theta_i\\). Es posible usar la función identidad para g(.) de modo que \\(\\mu_i = b^{\\prime}(\\theta_i)\\). De hecho, este es el caso habitual en la regresión lineal. Sin embargo, las combinaciones lineales de variables explicativas, \\(\\mathbf{x}_i^{\\mathbf{\\prime}} \\boldsymbol \\beta\\), pueden variar entre negativo e infinito positivo, mientras que las medias a menudo están restringidas a un rango más pequeño. Por ejemplo, las medias de Poisson varían entre cero e infinito. La función de enlace sirve para mapear el dominio de la función de la media en toda la recta real. Caso Especial: Enlaces para la distribución Bernoulli. Las medias de Bernoulli son probabilidades y, por lo tanto, varían entre cero y uno. Para este caso, es útil elegir una función de enlace que mapee el intervalo unitario (0,1) en toda la recta real. A continuación se presentan tres ejemplos importantes de funciones de enlace para la distribución Bernoulli: Logit: \\(g(\\mu )=\\mathrm{logit}(\\mu )=\\ln (\\mu /(1-\\mu ))\\) . Probit: \\(g(\\mu )=\\Phi ^{-1}(\\mu )\\), donde \\(\\Phi ^{-1}\\) es la inversa de la función de distribución normal estándar. Complementario log-log: \\(g(\\mu )=\\ln \\left( -\\ln (1-\\mu )\\right)\\). Esta ilustración demuestra que puede haber varias funciones de enlace que sean adecuadas para una distribución particular. Para ayudar en la selección, un caso intuitivamente atractivo ocurre cuando el componente sistemático es igual al parámetro de interés ($=$). Para ver esto, recordemos primero que \\(\\eta =g(\\mu )\\) y \\(\\mu =b^{\\prime }(\\theta )\\), omitiendo los subíndices “\\(i\\)” por el momento. Entonces, es fácil ver que si \\(g^{-1}=b^{\\prime }\\), entonces \\(\\eta =g(b^{\\prime }(\\theta ))=\\theta\\). La elección de \\(g\\) que es la inversa de \\(b^{\\prime }(\\theta )\\) se llama el enlace canónico. Tabla 13.2 muestra la función de la media y el enlace canónico correspondiente para varias distribuciones importantes. Tabla 13.2. Funciones de Media y Enlaces Canónicos para Distribuciones Seleccionadas \\[ \\small{ \\begin{array}{lcc} \\hline \\text{Distribución} &amp; \\text{Función de media } b^{\\prime }(\\theta ) &amp; \\text{Enlace canónico }g(\\mu ) \\\\ \\hline \\text{Normal} &amp; \\theta &amp; \\mu \\\\ \\text{Bernoulli} &amp; e^{\\theta}/(1+e^{\\theta} ) &amp; \\mathrm{logit}(\\mu ) \\\\ \\text{Poisson} &amp; e^{\\theta } &amp; \\ln \\mu \\\\ \\text{Gamma} &amp; -1/\\theta &amp; -1/\\mu \\\\ \\text{Gaussiana Inversa} &amp; (-2 \\theta )^{-1/2} &amp; -1 /(2 \\mu^2) \\\\ \\hline \\end{array} } \\] Las funciones de enlace relacionan la media con el componente sistemático y con los parámetros de la regresión. Dado que los parámetros de la regresión son desconocidos, es común especificar los enlaces solo hasta la escala. Por ejemplo, es común especificar el enlace canónico para la gaussiana inversa como \\(1 /\\mu^2\\) (en lugar de $-1 /(2 ^2) $). Si es necesario, siempre se puede recuperar la escala al estimar los coeficientes de regresión desconocidos. Ejemplo: Clasificación para la Fijación de Tarifas. El proceso de agrupar riesgos con características similares se conoce como clasificación de riesgos. La fijación de tarifas es el arte de establecer primas, o tarifas, basadas en la experiencia de pérdidas y las exposiciones de las clases de riesgo. Por ejemplo, Mildenhall (1999) consideró 8,942 pérdidas por colisión de pólizas de seguros de automóviles de uso privado en el Reino Unido (UK). Los datos fueron obtenidos de Nelder y McCullagh (1989, Sección 8.4.1) pero se originaron en Baxter et al. (1980). Un plan de tarifas típico para automóviles personales se basa en las características del conductor y del vehículo. Las características del conductor pueden incluir la edad, género, estado civil, historial (accidentes e infracciones) y descuento por buen estudiante. Las características del vehículo pueden incluir el tipo y año del modelo del vehículo, propósito (negocios/escuela o placer), área de estacionamiento, entre otras. Podemos representar el componente sistemático como: \\[ \\eta_{ij} = \\beta_0 + \\alpha_i + \\tau_j, \\] donde \\(\\alpha_i\\) representa el efecto de la \\(i\\)-ésima categoría de clasificación del conductor y \\(\\tau_j\\) el efecto del \\(j\\)-ésimo tipo de vehículo. Tabla 13.3 muestra los datos de Mildenhall para ocho tipos de conductores (grupos de edad) y cuatro clases de vehículos (uso del vehículo). La severidad promedio está en libras esterlinas ajustadas por inflación. En la terminología de GLM, un plan tarifario aditivo se basa en la función de enlace identidad, mientras que un plan multiplicativo se basa en una función de enlace logarítmica. Específicamente, si usamos \\(\\eta_{ij} = \\ln (\\mu_{ij})\\), entonces podemos escribir la media como: \\[\\begin{equation} \\mu_{ij} = \\exp(\\beta_0 + \\alpha_i + \\tau_j) = B \\times A_i \\times T_j, \\tag{13.3} \\end{equation}\\] donde \\(B=\\exp(\\beta_0)\\) es una constante de escala, \\(A_i=\\exp(\\alpha_i)\\) representa los efectos del conductor y \\(T_j=\\exp(\\tau_j)\\) representa los efectos del vehículo. Tabla 13.3. Datos de Colisiones en Automóviles Privados en el Reino Unido \\[ \\scriptsize{ \\begin{array}{lcrr|llcrr} \\hline \\text{Edad} &amp; \\text{Uso del} &amp; \\text{Severidad} &amp; \\text{Número de} &amp;~~~ &amp; \\text{Edad} &amp; \\text{Uso del} &amp; \\text{Severidad} &amp; \\text{Número de } \\\\ &amp; \\text{ Vehículo} &amp; \\text{Promedio} &amp; \\text{Reclamaciones} &amp;~~~ &amp; \\text{Edad} &amp; \\text{Vehículo} &amp; \\text{Promedio} &amp; \\text{Reclamaciones} \\\\ \\hline 17-20 &amp; \\text{Recreativo} &amp; 250.48 &amp; 21 &amp; &amp; 35-39 &amp; \\text{Recreativo} &amp; 153.62 &amp; 151 \\\\ 17-20 &amp; \\text{ConducirCorto} &amp; 274.78 &amp; 40 &amp; &amp; 35-39 &amp; \\text{ConducirCorto} &amp; 201.67 &amp; 479 \\\\ 17-20 &amp; \\text{ConducirLargo} &amp; 244.52 &amp; 23 &amp; &amp; 35-39 &amp; \\text{ConducirLargo} &amp; 238.21 &amp; 381 \\\\ 17-20 &amp; \\text{Negocios} &amp; 797.80 &amp; 5 &amp; &amp; 35-39 &amp; \\text{Negocios} &amp; 256.21 &amp; 166 \\\\ \\hline 21-24 &amp; \\text{Recreativo} &amp; 213.71 &amp; 63 &amp; &amp; 40-49 &amp; \\text{Recreativo} &amp; 208.59 &amp; 245 \\\\ 21-24 &amp; \\text{ConducirCorto} &amp; 298.60 &amp; 171 &amp; &amp; 40-49 &amp; \\text{ConducirCorto} &amp; 202.80 &amp; 970 \\\\ 21-24 &amp; \\text{ConducirLargo} &amp; 298.13 &amp; 92 &amp; &amp; 40-49 &amp; \\text{ConducirLargo} &amp; 236.06 &amp; 719 \\\\ 21-24 &amp; \\text{Negocios} &amp; 362.23 &amp; 44 &amp; &amp; 40-49 &amp; \\text{Negocios} &amp; 352.49 &amp; 304 \\\\ \\hline 25-29 &amp; \\text{Recreativo} &amp; 250.57 &amp; 140 &amp; &amp; 50-59 &amp; \\text{Recreativo} &amp; 207.57 &amp; 266 \\\\ 25-29 &amp; \\text{ConducirCorto} &amp; 248.56 &amp; 343 &amp; &amp; 50-59 &amp; \\text{ConducirCorto} &amp; 202.67 &amp; 859 \\\\ 25-29 &amp; \\text{ConducirLargo} &amp; 297.90 &amp; 318 &amp; &amp; 50-59 &amp; \\text{ConducirLargo} &amp; 253.63 &amp; 504 \\\\ 25-29 &amp; \\text{Negocios} &amp; 342.31 &amp; 129 &amp; &amp; 50-59 &amp; \\text{Negocios} &amp; 340.56 &amp; 162 \\\\ \\hline 30-34 &amp; \\text{Recreativo} &amp; 229.09 &amp; 123 &amp; &amp; 60+ &amp; \\text{Recreativo} &amp; 192.00 &amp; 260 \\\\ 30-34 &amp; \\text{ConducirCorto} &amp; 228.48 &amp; 448 &amp; &amp; 60+ &amp; \\text{ConducirCorto} &amp; 196.33 &amp; 578 \\\\ 30-34 &amp; \\text{ConducirLargo} &amp; 293.87 &amp; 361 &amp; &amp; 60+ &amp; \\text{ConducirLargo} &amp; 259.79 &amp; 312 \\\\ 30-34 &amp; \\text{Negocios} &amp; 367.46 &amp; 169 &amp; &amp; 60+ &amp; \\text{Negocios} &amp; 342.58 &amp; 96 \\\\ \\hline \\end{array} } \\] Fuente: Mildenhall (1999). “ConducirCorto” significa conducir al trabajo menos de 10 millas. “ConducirLargo” significa conducir al trabajo más de 10 millas. Código en R para Generar la Tabla 13.3 AutoDat &lt;- read.csv(&quot;../../CSVData/AutoCollision1.csv&quot;, quote=&quot;&quot;, header=TRUE) AutoDat1 &lt;- AutoDat AutoDat1[,1] &lt;- gsub(&quot;\\226&quot;, &quot;-&quot;, AutoDat1[,1] , fixed = TRUE) tableout &lt;- cbind(AutoDat1[1:16,],AutoDat1[17:32,]) TableGen1(TableData=tableout, TextTitle=&#39;Datos de Colisiones en Automóviles Privados en el Reino Unido&#39;, Align=&#39;c&#39;, ColumnSpec=1:7, BorderRight = 4, ColWidth = ColWidth8 ) 13.3 Estimación Esta sección presenta la estimación por máxima verosimilitud, la forma habitual de estimación. Para proporcionar una intuición, nos enfocamos en el caso más simple de enlaces canónicos. Los resultados para enlaces más generales aparecen en la Sección 13.9.3. 13.3.1 Estimación de Máxima Verosimilitud para Enlaces Canónicos A partir de la ecuación (13.2) y la independencia entre las observaciones, la función de log-verosimilitud es: \\[\\begin{equation} \\ln \\mathrm{f}( \\mathbf{y}) =\\sum_{i=1}^n \\left\\{ \\frac{y_i\\theta_i-b(\\theta_i)}{\\phi _i}+S( y_i,\\phi _i ) \\right\\} . \\tag{13.4} \\end{equation}\\] Recuerda que para enlaces canónicos, tenemos la igualdad entre el parámetro de la distribución y el componente sistemático, de modo que \\(\\theta_i=\\eta _i=\\mathbf{x}_i^{\\mathbf{\\prime }}\\boldsymbol \\beta\\). Así, con \\(\\phi _i=\\phi /w_i\\), la función de log-verosimilitud es: \\[\\begin{equation} L (\\boldsymbol \\beta, \\phi ) = \\ln \\mathrm{f}( \\mathbf{y}) =\\sum_{i=1}^n \\left\\{ \\frac{y_i\\mathbf{x}_i^{\\mathbf{\\prime }}\\boldsymbol \\beta-b(\\mathbf{x}_i^{\\mathbf{\\prime }} \\boldsymbol \\beta)}{\\phi / w_i}+S( y_i,\\phi / w_i) \\right\\} . \\tag{13.5} \\end{equation}\\] Al tomar la derivada parcial con respecto a \\(\\boldsymbol \\beta\\), obtenemos la función de puntuación: \\[\\begin{equation} \\frac{\\partial }{\\partial \\boldsymbol \\beta} L( \\boldsymbol \\beta, \\phi ) = \\frac{1}{\\phi} \\sum_{i=1}^n \\left( y_i-b^{\\prime }(\\mathbf{x}_i^{\\mathbf{\\prime }}\\boldsymbol \\beta) \\right) w_i \\mathbf{x}_i . \\tag{13.6} \\end{equation}\\] Dado que \\(\\mu _i=b^{\\prime }(\\theta_i)=b^{\\prime }(\\mathbf{x}_i^{\\mathbf{\\prime }}\\boldsymbol \\beta)\\), podemos resolver para los estimadores de máxima verosimilitud de \\(\\boldsymbol \\beta\\), \\(\\mathbf{b}_{MLE}\\), a través de las “ecuaciones normales”: \\[\\begin{equation} \\mathbf{0}=\\sum_{i=1}^n w_i \\left( y_i-\\mu _i\\right) \\mathbf{x}_i. \\tag{13.7} \\end{equation}\\] Estas ecuaciones representan un sistema con \\(k+1\\) ecuaciones y \\(k+1\\) incógnitas. Normalmente, la solución es única y usamos la notación \\(\\mathbf{b}_{MLE}\\) para denotar la solución. Una de las razones del uso generalizado de los métodos GLM es que los estimadores de máxima verosimilitud pueden calcularse rápidamente mediante una técnica conocida como mínimos cuadrados iterados ponderados, descrita en la Sección 13.9.4. Ten en cuenta que, al igual que en las ecuaciones normales de la regresión lineal ordinaria, no necesitamos considerar la estimación del parámetro de escala de la varianza \\(\\phi\\) en esta etapa. Es decir, primero podemos calcular \\(\\mathbf{b}_{MLE}\\) y, cuando sea necesario, estimar \\(\\phi\\). (Para ciertas distribuciones como la binomial y la de Poisson, \\(\\phi\\) es conocido y no requiere estimación). Como se describe en la Sección 11.9, los estimadores de máxima verosimilitud son consistentes y tienen distribuciones normales para muestras grandes bajo condiciones generales. La inferencia de máxima verosimilitud proporciona un mecanismo para calcular esta distribución. A partir de las ecuaciones (13.6) y del suplemento técnico sobre la verosimilitud (Sección 11.9, ecuación 11.14), la matriz de información correspondiente es: \\[\\begin{equation} \\mathbf{I} \\left( \\mathbf{b}_{MLE} \\right) = \\frac{1}{\\phi}\\sum_{i=1}^n w_i ~b^{\\prime \\prime }(\\mathbf{x}_i^{\\mathbf{\\prime }} \\mathbf{b}_{MLE} ) ~ \\mathbf{x}_i \\mathbf{x}_i^{\\mathbf{\\prime }} . \\tag{13.8} \\end{equation}\\] La inversa de la matriz de información es la matriz de varianza-covarianza para muestras grandes de \\(\\mathbf{b}_{MLE}\\). Específicamente, la raíz cuadrada del \\((j+1)\\)-ésimo elemento diagonal de la inversa de esta matriz proporciona el error estándar para \\(b_{j,MLE}\\), el cual denotamos como \\(se(b_{j,MLE})\\). Las extensiones a enlaces generales son similares. La inferencia para \\(\\mathbf{b}_{MLE}\\) es robusta en cuanto a la elección de distribuciones en el siguiente sentido: la solución de los estimadores de máxima verosimilitud en la ecuación (13.7) depende solo de la función de media; se puede demostrar que la consistencia de los estimadores depende únicamente de la correcta elección de esta función. Además, el comportamiento de muestra grande de \\(\\mathbf{b}_{MLE}\\) esencialmente solo requiere que la media y la varianza sean especificadas correctamente, no la elección de la distribución. (Se requieren algunas condiciones adicionales de regularidad, pero estas son requisitos técnicos leves). Por ejemplo, supongamos que un analista elige una distribución de Poisson con un enlace logarítmico. Si el enlace logarítmico es apropiado, solo se necesita la igualdad entre la media y la varianza, ver Tabla 13.1. A diferencia del dominio usual de la distribución de Poisson, las variables dependientes podrían ser no enteras o incluso negativas. La inferencia para \\(\\mathbf{b}_{MLE}\\) en muestras grandes solo requiere que se elijan correctamente las funciones de media y varianza. Ejemplo: Clasificación de Tarifas - Continuación. Utilizando los datos en la Tabla 13.3, se ajustó una función de enlace logarítmico con una distribución gamma utilizando el número de reclamaciones como pesos (\\(w_i\\)). La Tabla 13.4 muestra las estimaciones de la severidad esperada utilizando la ecuación (13.3). Los promedios sugieren que los conductores jóvenes (edades 17-20 y 21-24) tienen las reclamaciones más altas. En cuanto al uso del vehículo, aquellos que conducen por placer tuvieron las reclamaciones más bajas, mientras que aquellos que conducen por negocios tuvieron las reclamaciones más altas. Tabla 13.4. Estimación de la Severidad Esperada para un Plan Tarifario Multiplicativo \\[ \\small{ \\begin{array}{l|rrrr|r} \\hline \\text{Grupo de Edad } &amp; \\text{ConducirCorto }&amp; \\text{ConducirLargo} &amp; \\text{Placer} &amp; \\text{Negocios} &amp; \\text{Promedio} \\\\ \\hline 17-20 &amp; 322.17 &amp; 265.56 &amp; 254.90 &amp; 419.07 &amp; 315.42 \\\\ 21-24 &amp; 320.66 &amp; 264.31 &amp; 253.70 &amp; 417.10 &amp; 313.94 \\\\ 25-29 &amp; 297.26 &amp; 245.02 &amp; 235.19 &amp; 386.66 &amp; 291.03 \\\\ 30-34 &amp; 284.85 &amp; 234.80 &amp; 225.37 &amp; 370.53 &amp; 278.89 \\\\ 35-39 &amp; 229.37 &amp; 189.06 &amp; 181.47 &amp; 298.35 &amp; 224.56 \\\\ 40-49 &amp; 248.15 &amp; 204.54 &amp; 196.33 &amp; 322.78 &amp; 242.95 \\\\ 50-59 &amp; 251.95 &amp; 207.67 &amp; 199.34 &amp; 327.72 &amp; 246.67 \\\\ 60+ &amp; 246.47 &amp; 203.16 &amp; 195.00 &amp; 320.60 &amp; 241.31 \\\\ \\hline Promedio &amp; 275.11 &amp; 226.77 &amp; 217.66 &amp; 357.85 &amp; 269.35 \\\\ \\hline \\end{array} } \\] Código en R para Generar la Tabla 13.4 # AutoDat &lt;- read.csv(&quot;CSVData/AutoCollision.csv&quot;, quote=&quot;&quot;, header=TRUE) AutoDat &lt;- read.table(&quot;CSVData/AutoCollision1.txt&quot;, header=TRUE) # MODELO CON DOS FACTORES model1 &lt;- glm(Severity~factor(Age)+factor(Vehicle_Use), control = glm.control(maxit = 50), weights=Claim_Count, family=Gamma(link=&quot;log&quot;), data = AutoDat) #summary(model1) #model1$coefficients Agecoeff &lt;- model1$coefficients[1:8] Agecoeff[1]&lt;- 0 EAgecoeff &lt;- exp(Agecoeff) #EAgecoeff Vehcoeff &lt;- model1$coefficients[9:11]; Vehcoeff &lt;- c(Vehcoeff,0) EVehcoeff &lt;- exp(Vehcoeff) #EVehcoeff Premiums &lt;- exp(model1$coefficients[1])*EAgecoeff %*%t(EVehcoeff) Premiums1 &lt;- round(Premiums, digits = 2) AgeGroup &lt;- dimnames( table(AutoDat$Age) ) AgeGroup1 &lt;- AgeGroup # AgeGroup1 &lt;- gsub(&quot;\\226&quot;, &quot;-&quot;, AgeGroup1[[1]] , fixed = TRUE) tableout &lt;- cbind(AgeGroup1, Premiums1) colnames(tableout) &lt;- c(&quot;Grupo de Edad&quot;, &quot;ConducirCorto&quot;, &quot;ConducirLargo&quot;, &quot;Placer&quot;, &quot;Negocios&quot;) TableGen1(TableData=tableout, TextTitle=&#39;Estimación de la Severidad Esperada para un Plan Tarifario Multiplicativo&#39;, Align=&#39;c&#39;, ColumnSpec=1:3, Digits = 2, BorderRight = 1, ColWidth = ColWidth4 ) Tabla 13.1: Estimación de la Severidad Esperada para un Plan Tarifario Multiplicativo Grupo de Edad ConducirCorto ConducirLargo Placer Negocios c(“17?20”, “21?24”, “25?29”, “30?34”, “35?39”, “40?49”, “50?59”, “60+”) 322.17 265.56 254.9 419.07 c(“17?20”, “21?24”, “25?29”, “30?34”, “35?39”, “40?49”, “50?59”, “60+”) 320.66 264.31 253.7 417.1 c(“17?20”, “21?24”, “25?29”, “30?34”, “35?39”, “40?49”, “50?59”, “60+”) 297.26 245.02 235.19 386.66 c(“17?20”, “21?24”, “25?29”, “30?34”, “35?39”, “40?49”, “50?59”, “60+”) 284.85 234.8 225.37 370.53 c(“17?20”, “21?24”, “25?29”, “30?34”, “35?39”, “40?49”, “50?59”, “60+”) 229.37 189.06 181.47 298.35 c(“17?20”, “21?24”, “25?29”, “30?34”, “35?39”, “40?49”, “50?59”, “60+”) 248.15 204.54 196.33 322.78 c(“17?20”, “21?24”, “25?29”, “30?34”, “35?39”, “40?49”, “50?59”, “60+”) 251.95 207.67 199.34 327.72 c(“17?20”, “21?24”, “25?29”, “30?34”, “35?39”, “40?49”, “50?59”, “60+”) 246.47 203.16 195 320.6 13.3.2 Sobredispersión Para algunos miembros de la familia exponencial lineal, como las distribuciones Bernoulli y Poisson, la varianza está determinada por la media. En contraste, la distribución normal tiene un parámetro de escala separado. Al ajustar modelos a datos con variables dependientes binarias o de conteo, es común observar que la varianza excede la anticipada por el ajuste de los parámetros de la media. Este fenómeno se conoce como sobredispersión. Existen varios modelos probabilísticos alternativos disponibles para explicar este fenómeno, dependiendo de la aplicación en cuestión. Ver la Sección 12.3 para un ejemplo y McCullagh y Nelder (1989) para un inventario más detallado. Aunque llegar a un modelo probabilístico satisfactorio es el camino más deseable, en muchas situaciones los analistas se conforman con postular un modelo aproximado mediante la relación: \\[ \\mathrm{Var~}y_i=\\sigma ^{2}\\phi ~b^{\\prime \\prime }(\\theta_i)/w_i. \\] El parámetro \\(\\phi\\) se especifica a través de la elección de la distribución, mientras que el parámetro de escala \\(\\sigma ^{2}\\) permite una variabilidad extra. Por ejemplo, la Tabla 13.8 muestra que al especificar ya sea la distribución Bernoulli o Poisson, tenemos \\(\\phi =1\\). Aunque el parámetro de escala \\(\\sigma ^{2}\\) permite una variabilidad adicional, también puede acomodar situaciones en las que la variabilidad es menor de la especificada por la forma de la distribución (aunque esta situación es menos común). Finalmente, cabe destacar que para algunas distribuciones, como la normal, el término adicional ya está incorporado en el parámetro \\(\\phi\\), por lo que no tiene un propósito útil. Cuando se incluye el parámetro de escala adicional \\(\\sigma ^{2}\\), es común estimarlo mediante el estadístico chi-cuadrado de Pearson dividido por los grados de libertad del error. Es decir, \\[ \\widehat{\\sigma }^{2}=\\frac{1}{N-k} \\sum_{i=1}^n w_i\\frac{\\left( y_i-b^{\\prime }(\\mathbf{x}_i^{\\mathbf{\\prime }}\\mathbf{b}_{MLE})\\right) ^{2}}{\\phi b^{\\prime \\prime }(\\mathbf{x}_i^{\\mathbf{\\prime }}\\mathbf{b}_{MLE})}. \\] Al igual que con la distribución Poisson en la Sección 12.3, otra forma de manejar patrones inusuales de varianza es a través de errores estándar robustos o empíricos. La Sección 13.9.3 proporciona más detalles. 13.3.3 Estadísticas de Bondad de Ajuste En los modelos de regresión lineal, la estadística de bondad de ajuste más ampliamente citada es la medida \\(R^2\\), que se basa en la descomposición \\[ \\sum_i \\left(y_i - \\overline{y} \\right)^2 = \\sum_i \\left(y_i - \\widehat{y}_i \\right)^2 + \\sum_i \\left(\\widehat{y}_i - \\overline{y}\\right)^2 + 2 \\times \\sum_i \\left(y_i - \\widehat{y}_i \\right)\\left(\\widehat{y}_i - \\overline{y}\\right). \\] En el lenguaje de la Sección 2.3, esta descomposición es: \\[ \\textit{Total SS = Error SS + Regresión SS + 2} \\times \\textit{Suma de Productos Cruzados}. \\] La dificultad con los modelos no lineales es que el término de Suma de Productos Cruzados rara vez es igual a cero. Por lo tanto, se obtienen estadísticas diferentes al definir \\(R^2\\) como (Regresión SS/Total SS) en comparación con (1-Error SS/Total SS). La Sección 11.3.2 describió algunas medidas alternativas de \\(R^2\\) que a veces se citan en configuraciones GLM. Una medida de bondad de ajuste ampliamente citada es el estadístico chi-cuadrado de Pearson, que fue introducido en la Sección 12.1.4. En el contexto GLM, suponemos que \\(\\mathrm{E~}y_i = \\mu_i\\), \\(\\mathrm{Var~}y_i = \\phi v(\\mu_i)\\) es la función de varianza (como en los ejemplos de la Tabla 13.1) y que \\(\\widehat{\\mu}_i\\) es un estimador de \\(\\mu_i\\). Luego, el estadístico chi-cuadrado de Pearson se define como \\(\\sum_i \\left( y_i - \\widehat{\\mu}_i \\right)^2/\\ ( \\phi v(\\widehat{\\mu}_i))\\). Como hemos visto en los modelos de Poisson para datos de conteo, esta formulación se reduce a la forma \\(\\sum_i \\left( y_i - \\widehat{\\mu}_i \\right)^2/\\widehat{\\mu}_i\\). Los criterios de información generales, incluyendo \\(AIC\\) y \\(BIC\\), que fueron definidos en la Sección 11.9 también se citan regularmente en estudios de GLM. Una medida de bondad de ajuste específica para los modelos GLM es la estadística de devianza. Para definir esta estadística, trabajamos con la noción de un modelo saturado donde hay tantos parámetros como observaciones, \\(\\theta_i, i=1, \\ldots, n\\). Un modelo saturado proporciona el mejor ajuste posible. Con un parámetro para cada observación, maximizamos la verosimilitud en una base de observación por observación. Por lo tanto, al derivar la verosimilitud logarítmica de la ecuación (13.2) se obtiene \\[ \\frac{\\partial}{\\partial \\theta_i} \\ln \\mathrm{f}( y_i; \\theta_i ,\\phi ) = \\frac{y_i-b^{\\prime}(\\theta_i)}{\\phi}. \\] Al igualar esto a cero, se obtiene la estimación del parámetro, digamos \\(\\theta_{i,SAT}\\), como la solución de \\(y_i=b^{\\prime}(\\theta_{i,Sat})\\). Denotando \\(\\boldsymbol \\theta _{SAT}\\) como el vector de parámetros, la verosimilitud \\(L(\\boldsymbol \\theta _{SAT})\\) es el valor máximo posible de la verosimilitud logarítmica. Luego, para un estimador genérico \\(\\widehat{\\boldsymbol \\theta}\\), la estadística de devianza escalada se define como \\[ \\mathbf{\\mathrm{D}}^{\\ast}(\\widehat{\\boldsymbol \\theta}) = 2 \\times \\left( L(\\boldsymbol \\theta _{SAT}) - L(\\widehat{\\boldsymbol \\theta}) \\right). \\] En las familias exponenciales lineales, se multiplica por el factor de escala \\(\\phi\\) para definir la estadística de devianza, \\(\\mathbf{\\mathrm{D}}(\\widehat{\\boldsymbol \\theta}) = \\phi \\mathbf{\\mathrm{D}}^{\\ast}(\\widehat{\\boldsymbol \\theta})\\). Esta multiplicación realmente elimina el factor de escala de la varianza de la definición de la estadística. Es fácil verificar que la estadística de devianza se reduce a las siguientes formas para tres casos especiales: Normal: \\(\\mathbf{\\mathrm{D}}(\\widehat{\\boldsymbol \\mu})= \\sum_i \\left( y_i - \\widehat \\mu _i \\right) ^2\\), Bernoulli: \\(\\mathbf{\\mathrm{D}}(\\widehat{\\boldsymbol \\pi})= \\sum_i \\left\\{ y_i \\ln \\frac {y_i}{\\widehat \\pi_i} + (1-y_i) \\ln \\frac {1-y_i}{1-\\widehat \\pi_i} \\right\\}\\), y Poisson: \\(\\mathbf{\\mathrm{D}}(\\widehat{\\boldsymbol \\mu})= \\sum_i \\left\\{ y_i \\ln \\frac {y_i}{\\widehat \\mu_i} + (y_i - \\widehat \\mu _i) \\right\\}\\). Aquí usamos la convención de que \\(y \\ln y = 0\\) cuando \\(y = 0\\). 13.4 Aplicación: Gastos Médicos Ahora volvemos a los datos de la Encuesta del Panel de Gastos Médicos (MEPS) introducidos en la Sección 11.4. En esa sección, buscamos desarrollar un modelo para entender el evento de una admisión hospitalaria como paciente interno. En esta sección, ahora deseamos modelar la cantidad del gasto. En terminología actuarial, la Sección 11.4 consideraba la “frecuencia”, mientras que esta sección involucra la “severidad”. De las 2,000 observaciones seleccionadas aleatoriamente del año 2003 consideradas en la Sección 11.4, solo \\(n=157\\) fueron admitidos al hospital durante el año. La Tabla 13.5 resume los datos utilizando las mismas variables explicativas que en la Tabla 11.4. Por ejemplo, la Tabla 13.5 muestra que la muestra es 72% femenina, casi 76% blanca y más del 91% tiene seguro. La tabla también muestra relativamente pocos gastos por parte de asiáticos, nativos americanos y personas sin seguro en nuestra muestra. La Tabla 13.5 también proporciona los gastos medianos por variable categórica. Esta tabla sugiere que el género, una mala autoevaluación de la salud física y los ingresos bajos o negativos pueden ser determinantes importantes de la cantidad de los gastos médicos. Tabla 13.5. Gastos Medianos por Variable Explicativa Basado en una Muestra de \\(n=157\\) con Gastos Positivos \\[ \\scriptsize{ \\begin{array}{lllrr} \\hline \\text{Categoría} &amp; \\text{Variable} &amp; \\text{Descripción} &amp; \\text{Porcentaje} &amp; \\text{Mediana} \\\\ &amp; &amp; &amp; \\text{de datos} &amp; \\text{Gasto} \\\\ \\hline &amp; COUNTIP &amp; \\text{Número de gastos (mediana: 1.0)} \\\\ \\text{Demografía} &amp; AGE &amp; \\text{Edad en años entre } \\\\ &amp; &amp; \\ \\ \\ \\text{ 18 a 65 (mediana: 41.0)} \\\\ &amp; GENDER &amp; \\text{1 si es mujer} &amp; 72.0 &amp; 5,546 \\\\ &amp; &amp; \\text{0 si es hombre} &amp; 28.0 &amp; 7,313 \\\\ \\text{Etnicidad} &amp; ASIAN &amp; \\text{1 si es asiático} &amp; 2.6 &amp; 4,003 \\\\ &amp; BLACK &amp; \\text{1 si es afroamericano} &amp; 19.8 &amp; 6,100 \\\\ &amp; NATIVE &amp; \\text{1 si es nativo} &amp; 1.9 &amp; 2,310 \\\\ &amp; WHITE &amp; \\textit{Nivel de referencia} &amp; 75.6 &amp; 5,695 \\\\ \\text{Región} &amp; NORTHEAST &amp; \\text{1 si es del noreste} &amp; 18.5 &amp; 5,833 \\\\ &amp; MIDWEST &amp; \\text{1 si es del medio oeste} &amp; 21.7 &amp; 7,999 \\\\ &amp; SOUTH &amp; \\text{1 si es del sur} &amp; 40.8 &amp; 5,595 \\\\ &amp; WEST &amp; \\textit{Nivel de referencia} &amp; 19.1 &amp; 4,297 \\\\ \\hline \\text{Educación} &amp; COLLEGE &amp; \\text{1 si tiene título universitario o superior} &amp; 23.6 &amp; 5,611 \\\\ &amp; \\text{HIGHSCHOOL} &amp; \\text{1 si tiene título de secundaria} &amp; 43.3 &amp; 5,907 \\\\ &amp;&amp; \\text{Nivel de referencia es menor} &amp; 33.1 &amp; 5,338 \\\\ &amp;&amp; \\ \\ \\ \\text{ que título de secundaria} &amp; \\\\ \\hline \\text{Salud física} &amp; POOR &amp; \\text{1 si es mala} &amp; 17.2 &amp; 10,447 \\\\ \\ \\ \\text{autocalificada} &amp; FAIR &amp; \\text{1 si es regular} &amp; 10.2 &amp; 5,228 \\\\ &amp; GOOD &amp; \\text{1 si es buena } &amp; 31.2 &amp; 5,032 \\\\ &amp; VGOOD &amp; \\text{1 si es muy buena} &amp; 24.8 &amp; 5,546 \\\\ &amp;&amp; \\text{Nivel de referencia es excelente salud} &amp; 16.6 &amp; 5,277 \\\\ \\text{Salud mental} &amp; MPOOR &amp; \\text{1 si es mala o regular} &amp; 15.9 &amp; 6,583 \\\\ \\ \\ \\text{autocalificada} &amp; &amp; \\text{0 si es buena a excelente} &amp; 84.1 &amp; 5,599 \\\\ \\text{Cualquier} &amp; ANYLIMIT &amp; \\text{1 si tiene alguna limitación funcional/de actividad} &amp; 41.4 &amp; 7,826 \\\\ \\ \\ \\text{limitación} &amp; &amp; \\text{0 en caso contrario} &amp; 58.6 &amp; 4,746 \\\\ \\hline \\text{Ingreso} &amp;&amp; \\text{Nivel de referencia es ingreso alto} &amp; 21.7 &amp; 7,271 \\\\ \\text{comparado con} &amp; MINCOME &amp; \\text{1 si es ingreso medio} &amp; 26.8 &amp; 5,851 \\\\ \\text{línea de pobreza} &amp; LINCOME &amp; \\text{1 si es ingreso bajo} &amp; 16.6 &amp; 6,909 \\\\ &amp; NPOOR &amp; \\text{1 si es casi pobre} &amp; 7.0 &amp; 5,546 \\\\ &amp; POORNEG &amp; \\text{si es pobre/ingreso negativo} &amp; 28.0 &amp; 4,097 \\\\ \\hline \\text{Seguro} &amp; INSURE &amp; \\text{1 si tiene cobertura de salud pública/privada} &amp; 91.1 &amp; 5,943 \\\\ \\ \\ \\text{médico} &amp; &amp; \\ \\ \\text{en cualquier mes de 2003} &amp; &amp; \\\\ &amp; &amp; \\text{0 si no tiene seguro médico en 2003} &amp; 8.9 &amp; 2,668 \\\\ \\hline Total &amp; &amp; &amp; 100.0 &amp; 5,695 \\\\ \\hline \\end{array} } \\] knitr::kable(2, caption = &quot;Silly. Create a table just to update the counter...&quot;) Tabla 13.2: Silly. Create a table just to update the counter… x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Tabla 13.3: Silly. x 2 knitr::kable(2, caption = &quot;Silly. &quot;) Tabla 13.4: Silly. x 2 Código R para generar la Tabla 13.5 Como se señaló en el prefacio, la mayor parte del código para el libro, escrito alrededor de 2008, estaba en SAS. El código ilustrativo proporcionado aquí, en R, presenta algunas ligeras discrepancias con el original. Hexpend &lt;- read.csv(&quot;CSVData/HealthExpend.csv&quot;, header=TRUE) # Tabla 13.5 #Hexpend$POSEXP &lt;- 1*(Hexpend$EXPENDIP&gt;0) HexpendPos &lt;- subset(Hexpend, EXPENDIP &gt; 0) # CREAR UNA FUNCIÓN CORTA PARA AHORRAR TRABAJO fun2 &lt;- function(group_var, summary_vars){ # Convertir group_var y summary_vars a caracteres si no lo son group_var &lt;- as.character(substitute(group_var)) summary_vars &lt;- as.character(substitute(summary_vars)) temp &lt;- doBy::summaryBy( formula = as.formula(paste(summary_vars, &quot;~&quot;, group_var)), data = HexpendPos, FUN = function(x) { c(m = median(x), num = length(x)) } ) temp1 &lt;- temp[,c(1,3,2)] temp1[,2] &lt;- round(100*temp1[,2]/length(HexpendPos$EXPENDIP), digits=1) colnames(temp1) &lt;- NULL temp1[,1] &lt;- as.character(temp1[,1]) return(as.matrix(temp1) ) } var1 &lt;- fun2(GENDER,EXPENDIP) var2 &lt;- fun2(RACE,EXPENDIP) var3 &lt;- fun2(REGION,EXPENDIP) var4 &lt;- fun2(EDUC,EXPENDIP) var5 &lt;- fun2(PHSTAT,EXPENDIP) #var6 &lt;- fun2(MPOOR,EXPENDIP) var7 &lt;- fun2(ANYLIMIT,EXPENDIP) var8 &lt;- fun2(INCOME,EXPENDIP) var9 &lt;- fun2(insure,EXPENDIP) tableout &lt;- rbind(var1, var2, var3, var4, var5, var7, var8, var9) TableGen1(TableData=tableout, TextTitle=&#39;Gastos Medianos por Variable Explicativa Basado en una Muestra de $n=157$ con Gastos Positivos&#39;, Align=&#39;crr&#39;, ColumnSpec=1:3, ColWidth = ColWidth4 ) Tabla 13.5: Gastos Medianos por Variable Explicativa Basado en una Muestra de \\(n=157\\) con Gastos Positivos 1 0 28 7312.695 2 1 72 5546.470 1 ASIAN 2.5 4002.735 2 BLACK 19.7 6100.200 3 NATIV 1.9 2310.320 4 OTHER 1.9 4050.650 5 WHITE 73.9 5725.325 1 MIDWEST 21.7 7998.660 2 NORTHEAST 18.5 5832.860 3 SOUTH 40.8 5595.295 4 WEST 19.1 4296.540 1 COLLEGE 23.6 5610.76 2 HIGHSCH 43.3 5907.33 3 LHIGHSC 33.1 5338.04 1 EXCE 16.6 5277.160 2 FAIR 10.2 5228.465 3 GOOD 31.2 5031.960 4 POOR 17.2 10447.130 5 VGOO 24.8 5546.470 1 0 58.6 4745.89 2 1 41.4 7825.50 1 HINCOME 21.7 7270.780 2 LINCOME 16.6 6908.720 3 MINCOME 26.8 5851.160 4 NPOOR 7.0 5546.470 5 POOR 28.0 4096.545 1 0 8.9 2668.37 2 1 91.1 5943.25 Tabla 13.5 utiliza medianas en lugar de medias debido a que la distribución de los gastos está sesgada hacia la derecha. Esto es evidente en la Figura 13.1, que proporciona un histograma suave (conocido como “estimación de densidad por kernel”, ver Sección 15.2) para los gastos hospitalarios. Para distribuciones sesgadas, la mediana a menudo brinda una mejor idea del centro de la distribución que la media. La distribución está aún más sesgada de lo que sugiere esta figura, ya que el gasto más grande (que es $607,800) se omite en la representación gráfica. Figura 13.1: Histograma Empírico Suave de Gastos Hospitalarios Positivos. Se omite el gasto más grande. Código R para generar la Figura 13.1 # ELIMINAR TEMPORALMENTE EL OBS # 58 Hexpend &lt;- read.csv(&quot;CSVData/HealthExpend.csv&quot;, header=TRUE) Hexpend58 &lt;- subset(Hexpend, !(EXPENDIP&gt;600000)) plot(density(Hexpend58$EXPENDIP), main=&quot;&quot;, xlab=&quot;Gastos&quot;) #hist(Hexpend58$EXPENDIP, main=&quot;&quot;, xlab=&quot;Gastos&quot;) Un modelo de regresión gamma con un enlace logarítmico se ajustó a los gastos hospitalarios utilizando todas las variables explicativas. Los resultados de este ajuste de modelo aparecen en la Tabla 13.6. Aquí vemos que muchos de los posibles determinantes importantes de los gastos médicos no son estadísticamente significativos. Esto es común en el análisis de gastos, donde las variables ayudan a predecir la frecuencia, aunque no son tan útiles para explicar la severidad. Debido a la colinealidad, hemos visto en modelos lineales que tener demasiadas variables en un modelo ajustado puede llevar a la insignificancia estadística de variables importantes e incluso causar que los signos se inviertan. Para un modelo más simple, eliminamos las variables de Asian, Native American y uninsured, ya que representan un pequeño subconjunto de nuestra muestra. También utilizamos solo la variable POOR para el estado de salud autoinformado y solo POORNEG para los ingresos, esencialmente reduciendo estas variables categóricas a variables binarias. La Tabla 13.6, bajo el encabezado “Modelo Reducido”, muestra los resultados de este ajuste del modelo. Este modelo tiene casi el mismo estadístico de bondad de ajuste, AIC, lo que sugiere que es una alternativa razonable. Bajo el ajuste del modelo reducido, las variables COUNTIP (conteo de pacientes hospitalizados), AGE, COLLEGE y POORNEG son variables estadísticamente significativas. Para estas variables significativas, los signos son intuitivamente razonables. Por ejemplo, el coeficiente positivo asociado con COUNTIP significa que a medida que aumenta el número de visitas hospitalarias, los gastos totales aumentan, como era de esperar. Para otra especificación alternativa del modelo, la Tabla 13.6 también muestra un ajuste de un modelo inverso gaussiano con un enlace logarítmico. A partir del estadístico AIC, vemos que este modelo no se ajusta tan bien como el modelo de regresión gamma. Todas las variables son estadísticamente insignificantes, lo que dificulta la interpretación de este modelo. Tabla 13.6. Comparación de Modelos de Regresión Gamma e Inversa Gaussiana \\[ \\scriptsize{ \\begin{array}{l|rr|rr|rr} \\hline &amp; \\text{Gamma} &amp; &amp;\\text{Gamma}&amp;&amp;\\text{Inversa}&amp;\\text{Gaussiana} \\\\ \\hline &amp; \\text{Modelo} &amp;\\text{Modelo}&amp; \\text{Modelo} &amp; \\text{Modelo}&amp;\\text{Modelo}&amp;\\text{Modelo} \\\\ &amp; \\text{Completo} &amp;\\text{Reducido}&amp; \\text{Completo} &amp; \\text{Reducido}&amp;\\text{Reducido}&amp;\\text{Reducido} \\\\ \\hline &amp; \\scriptsize{Parámetro} &amp; &amp; \\scriptsize{Parámetro} &amp; &amp; \\scriptsize{Parámetro} &amp; \\\\ \\text{Efecto} &amp; \\scriptsize{Estimación} &amp; t\\text{-valor} &amp; \\scriptsize{Estimación} &amp; t\\text{-valor} &amp; \\scriptsize{Estimación} &amp; t\\text{-valor} \\\\ \\hline Intercepto &amp; 6.891 &amp; 13.080 &amp; 7.859 &amp; 17.951 &amp; 6.544 &amp; 3.024 \\\\ COUNTIP &amp; 0.681 &amp; 6.155 &amp; 0.672 &amp; 5.965 &amp; 1.263 &amp; 0.989 \\\\ AGE &amp; 0.021 &amp; 3.024 &amp; 0.015 &amp; 2.439 &amp; 0.018 &amp; 0.727 \\\\ GENDER &amp; -0.228 &amp; -1.263 &amp; -0.118 &amp; -0.648 &amp; 0.363 &amp; 0.482 \\\\ ASIAN &amp; -0.506 &amp; -1.029 &amp; &amp; &amp; &amp; \\\\ BLACK &amp; -0.331 &amp; -1.656 &amp; -0.258 &amp; -1.287 &amp; -0.321 &amp; -0.577 \\\\ NATIVE &amp; -1.220 &amp; -2.217 &amp; &amp; &amp; &amp; \\\\ NORTHEAST &amp; -0.372 &amp; -1.548 &amp; -0.214 &amp; -0.890 &amp; 0.109 &amp; 0.165 \\\\ MIDWEST &amp; 0.255 &amp; 1.062 &amp; 0.448 &amp; 1.888 &amp; 0.399 &amp; 0.654 \\\\ SOUTH &amp; 0.010 &amp; 0.047 &amp; 0.108 &amp; 0.516 &amp; 0.164 &amp; 0.319 \\\\ \\hline COLLEGE &amp; -0.413 &amp; -1.723 &amp; -0.469 &amp; -2.108 &amp; -0.367 &amp; -0.606 \\\\ HIGHSCHOOL &amp; -0.155 &amp; -0.827 &amp; -0.210 &amp; -1.138 &amp; -0.039 &amp; -0.078 \\\\ \\hline POOR &amp; -0.003 &amp; -0.010 &amp; 0.167 &amp; 0.706 &amp; 0.167 &amp; 0.258 \\\\ FAIR &amp; -0.194 &amp; -0.641 &amp; &amp; &amp; &amp; \\\\ GOOD &amp; 0.041 &amp; 0.183 &amp; &amp; &amp; &amp; \\\\ VGOOD &amp; 0.000 &amp; 0.000 &amp; &amp; &amp; &amp; \\\\ MNHPOOR &amp; -0.396 &amp; -1.634 &amp; -0.314 &amp; -1.337 &amp; -0.378 &amp; -0.642 \\\\ ANYLIMIT &amp; 0.010 &amp; 0.053 &amp; 0.052 &amp; 0.266 &amp; 0.218 &amp; 0.287 \\\\ \\hline MINCOME &amp; 0.114 &amp; 0.522 &amp; &amp; &amp; &amp; \\\\ LINCOME &amp; 0.536 &amp; 2.148 &amp; &amp; &amp; &amp; \\\\ NPOOR &amp; 0.453 &amp; 1.243 &amp; &amp; &amp; &amp; \\\\ POORNEG &amp; -0.078 &amp; -0.308 &amp; -0.406 &amp; -2.129 &amp; -0.356 &amp; -0.595 \\\\ INSURE &amp; 0.794 &amp; 3.068 &amp; &amp; &amp; &amp; \\\\ \\hline Escala &amp; 1.409 &amp; 9.779 &amp; 1.280 &amp; 9.854 &amp; 0.026 &amp; 17.720 \\\\ \\hline \\scriptsize{Log-Verosimilitud} &amp; -1,558.67 &amp;&amp; -1,567.93 &amp;&amp; -1,669.02 \\\\ AIC &amp; 3,163.34 &amp;&amp; 3,163.86 &amp;&amp; 3,366.04 \\\\ \\hline \\end{array} } \\] Código R para generar la Tabla 13.6 Como se señaló en el prefacio, la mayor parte del código para el libro, escrito alrededor de 2008, estaba en SAS. El código ilustrativo proporcionado aquí, en R, presenta algunas ligeras discrepancias con el original. Hexpend &lt;- read.csv(&quot;CSVData/HealthExpend.csv&quot;, header=TRUE) # Tabla 13.6 #POSEXP = 1*(Hexpend$EXPENDIP&gt;0) HexpendPos &lt;- subset(Hexpend, EXPENDIP &gt; 0) # MODELO CON TODAS LAS VARIABLES # SE TUVO QUE AUMENTAR EL NÚMERO POR DEFECTO DE ITERACIONES PARA LA CONVERGENCIA model1 = glm(EXPENDIP~COUNTIP+AGE+GENDER +factor(RACE)+factor(REGION)+factor(EDUC)+factor(PHSTAT) +MNHPOOR+ANYLIMIT+factor(INCOME)+insure, control = glm.control(maxit = 50), data=HexpendPos,family=Gamma(link=&quot;log&quot;)) output1 &lt;- summary(model1 ) tidy_model &lt;- broom::tidy(model1) kable(tidy_model, format = &quot;html&quot;, digits = 3, table.attr = &quot;class=&#39;table table-striped&#39;&quot;, caption = &quot;Resumen del Modelo de Regresión Gamma&quot;) Tabla 13.6: Resumen del Modelo de Regresión Gamma term estimate std.error statistic p.value (Intercept) 6.211 0.664 9.352 0.000 COUNTIP 0.682 0.094 7.287 0.000 AGE 0.021 0.007 2.989 0.003 GENDER -0.238 0.172 -1.382 0.169 factor(RACE)BLACK 0.182 0.512 0.356 0.722 factor(RACE)NATIV -0.697 0.732 -0.953 0.343 factor(RACE)OTHER 0.726 0.707 1.027 0.306 factor(RACE)WHITE 0.513 0.481 1.066 0.289 factor(REGION)NORTHEAST -0.628 0.238 -2.642 0.009 factor(REGION)SOUTH -0.256 0.199 -1.288 0.200 factor(REGION)WEST -0.261 0.233 -1.118 0.266 factor(EDUC)HIGHSCH 0.261 0.193 1.356 0.177 factor(EDUC)LHIGHSC 0.411 0.225 1.828 0.070 factor(PHSTAT)FAIR -0.165 0.314 -0.524 0.601 factor(PHSTAT)GOOD 0.063 0.227 0.276 0.783 factor(PHSTAT)POOR 0.026 0.298 0.088 0.930 factor(PHSTAT)VGOO 0.018 0.237 0.076 0.939 MNHPOOR -0.397 0.238 -1.664 0.098 ANYLIMIT -0.006 0.189 -0.031 0.976 factor(INCOME)LINCOME 0.551 0.257 2.147 0.034 factor(INCOME)MINCOME 0.123 0.211 0.583 0.561 factor(INCOME)NPOOR 0.463 0.354 1.308 0.193 factor(INCOME)POOR -0.068 0.251 -0.273 0.785 insure 0.789 0.257 3.068 0.003 # MODELO REDUCIDO HexpendPos$BLACK = (HexpendPos$RACE == &quot;BLACK&quot;) HexpendPos$POOR = (HexpendPos$PHSTAT == &quot;POOR&quot;) HexpendPos$POORNEG = (HexpendPos$INCOME == &quot;POOR&quot;) model2 &lt;- glm(EXPENDIP ~ COUNTIP + AGE + GENDER + factor(BLACK) + factor(REGION) + factor(EDUC) + factor(POOR) + MNHPOOR + ANYLIMIT + factor(POORNEG), data=HexpendPos, family=Gamma(link=&quot;log&quot;) ) #summary(model2) anova(model1, model2) Analysis of Deviance Table Model 1: EXPENDIP ~ COUNTIP + AGE + GENDER + factor(RACE) + factor(REGION) + factor(EDUC) + factor(PHSTAT) + MNHPOOR + ANYLIMIT + factor(INCOME) + insure Model 2: EXPENDIP ~ COUNTIP + AGE + GENDER + factor(BLACK) + factor(REGION) + factor(EDUC) + factor(POOR) + MNHPOOR + ANYLIMIT + factor(POORNEG) Resid. Df Resid. Dev Df Deviance 1 133 123.954 2 143 137.876 -10 -13.9218 Hexpend58 &lt;- subset(HexpendPos, !(EXPENDIP&gt;600000)) # MODELO REDUCIDO - INVERSA GAUSSIANA # ¡NO QUIERE CONVERGER! # model3 &lt;- glm(EXPENDIP ~ COUNTIP + AGE + GENDER + # factor(BLACK) + factor(REGION) + factor(EDUC) + factor(POOR) + # MNHPOOR + ANYLIMIT + factor(POORNEG), # data=Hexpend58, family=inverse.gaussian(link = &quot;log&quot;) ) # summary(model3) # TAMBIÉN PROBLEMAS CON family=inverse.gaussian(link = &quot;1/mu^2&quot;) 13.5 Residuales Una forma de seleccionar un modelo adecuado es ajustar una especificación tentativa y analizar formas de mejorarlo. Este método diagnóstico se basa en los residuales. En el modelo lineal, definimos los residuales como la respuesta menos los valores ajustados correspondientes. En un contexto de GLM, nos referimos a estos como “residuales crudos”, denotados como \\(y_i - \\widehat{\\mu}_i\\). Como hemos visto, los residuales son útiles para: descubrir nuevas covariables o los efectos de patrones no lineales en covariables existentes, identificar observaciones con mal ajuste, cuantificar los efectos de las observaciones individuales en los parámetros del modelo, y revelar otros patrones del modelo, como heterocedasticidad o tendencias temporales. Como se enfatiza en esta sección, los residuales también son los componentes básicos de muchas estadísticas de bondad de ajuste. Como vimos en la Sección 11.1 sobre variables dependientes binarias, el análisis de los residuales crudos puede ser irrelevante en algunos contextos no lineales. Cox y Snell (1968, 1971) introdujeron una noción general de residuales que es más útil en modelos no lineales que los simples residuales crudos. Para nuestras aplicaciones, asumimos que \\(y_i\\) tiene una función de distribución F(\\(\\cdot\\)) que está indexada por las variables explicativas \\(\\mathbf{x}_i\\) y un vector de parámetros \\(\\boldsymbol \\theta\\) denotado como F(\\(\\mathbf{x}_i,\\boldsymbol \\theta\\)). Aquí, la función de distribución es común a todas las observaciones, pero la distribución varía a través de las variables explicativas. El vector de parámetros \\(\\boldsymbol \\theta\\) incluye los parámetros de regresión \\(\\boldsymbol \\beta\\) así como los parámetros de escala. Con conocimiento de F(\\(\\cdot\\)), ahora asumimos que se puede calcular una nueva función R(\\(\\cdot\\)) tal que \\(\\varepsilon_i = \\mathrm{R}(y_i; \\mathbf{x}_i,\\boldsymbol \\theta)\\), donde \\(\\varepsilon_i\\) son distribuidos idénticamente e independientemente. Aquí, la función R(\\(\\cdot\\)) depende de las variables explicativas \\(\\mathbf{x}_i\\) y los parámetros \\(\\boldsymbol \\theta\\). Los residuales de Cox-Snell son entonces \\(e_i = \\mathrm{R}(y_i; \\mathbf{x}_i,\\widehat{\\boldsymbol \\theta})\\). Si el modelo está bien especificado y las estimaciones de los parámetros \\(\\widehat{\\boldsymbol \\theta}\\) están cerca de los parámetros del modelo \\(\\boldsymbol \\theta\\), entonces los residuales \\(e_i\\) deberían estar cerca de ser i.i.d. Para ilustrar, en el modelo lineal utilizamos la función de residuales \\(\\mathrm{R}(y_i; \\mathbf{x}_i,\\boldsymbol \\theta)\\) \\(= y_i - \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta\\) \\(= y_i - \\mu_i = \\varepsilon_i\\), resultando en residuales crudos (ordinarios). Otra opción es reescalar mediante la desviación estándar \\[ \\mathrm{R}(y_i; \\mathbf{x}_i,\\boldsymbol \\theta) = \\frac{y_i - \\mu_i}{\\sqrt{\\mathrm{Var~} y_i}} , \\] que produce residuales de Pearson. Otro tipo de residual se basa en transformar primero la respuesta con una función conocida h(\\(\\cdot\\)), \\[ \\mathrm{R}(y_i; \\mathbf{x}_i,\\boldsymbol \\theta) = \\frac{\\mathrm{h}(y_i) - \\mathrm{E~} \\mathrm{h}(y_i)}{\\sqrt{\\mathrm{Var~}\\mathrm{h}(y_i)}}. \\] Elegir la función de modo que h(\\(y_i\\)) sea aproximadamente distribuida normalmente produce residuales de Anscombe. Otra elección popular es seleccionar la transformación de modo que la varianza sea estabilizada. Tabla 13.7 presenta transformaciones para las distribuciones binomial, Poisson y gamma. Tabla 13.7. Transformaciones Aproximadas a la Normalidad y de Estabilización de Varianza \\[ \\small{ \\begin{array}{lcc} \\hline \\text{Distribución} &amp; \\text{Transformación Aproximada } &amp; \\text{Transformación de Estabilización } \\\\ &amp; \\text{a la Normalidad Transformación} &amp; \\text{de Varianza Transformación} \\\\ \\hline \\text{Binomial}(m,p) &amp; \\frac{\\mathrm{h}_B(y/m)-[\\mathrm{h}_B(p)+(p(1-p))^{-1/3}(2p-1)/(6m)]}{(p(1-p)^{1/6}/\\sqrt{m})} &amp; \\frac{\\sin^{-1}(\\sqrt{y/m})-\\sin^{-1}(\\sqrt{p})}{a/(2\\sqrt{m})} \\\\ \\text{Poisson}(\\lambda) &amp; 1.5 \\lambda^{-1/6} \\left[ y^{2/3}-(\\lambda^{2/3}-\\lambda^{-1/3}/9) \\right] &amp; 2(\\sqrt{y}-\\sqrt{\\lambda})\\\\ \\text{Gamma}(\\alpha, \\gamma) &amp; 3 \\alpha^{1/6} \\left[ (y/\\gamma)^{1/3}-(\\alpha^{1/3}-\\alpha^{-2/3}/9) \\right] &amp; \\text{No considerado}\\\\ \\hline \\\\ \\end{array} } \\] Nota: \\(\\mathrm{h}_B(u) = \\int_0^u s^{-1/3} (1-s)^{-1/3}ds\\) es la función beta incompleta evaluada en \\((2/3, 2/3)\\), hasta una constante escalar. Fuente: Pierce y Schafer (1986). Otra opción ampliamente utilizada en estudios de GLM son los residuales de devianza, definidos como \\[ \\mathrm{sign} \\left(y_i - \\widehat{\\mu}_i \\right) \\sqrt{2 \\left( \\ln \\mathrm{f}(y_i;\\theta_{i,SAT}) - \\ln \\mathrm{f}(y_i;\\widehat{\\theta}_{i}) \\right)}. \\] La Sección 13.3.3 introdujo las estimaciones de los parámetros del modelo saturado, \\(\\theta_{i,SAT}\\). Como mencionan Pierce y Schafer (1986), los residuales de devianza son muy cercanos a los residuales de Anscombe en muchos casos; podemos verificar los residuales de devianza para la normalidad aproximada. Además, los residuales de devianza pueden definirse fácilmente para cualquier modelo GLM y son fáciles de calcular. 13.6 Distribución de Tweedie Hemos visto que la familia exponencial natural incluye distribuciones continuas, como la normal y gamma, así como distribuciones discretas, como la binomial y Poisson. También incluye distribuciones que son mezclas de componentes discretos y continuos. En la modelización de reclamaciones de seguros, la mezcla más utilizada es la distribución de Tweedie (1984). Esta tiene una masa positiva en cero, representando la ausencia de reclamaciones, y un componente continuo para valores positivos que representan el monto de una reclamación. La distribución de Tweedie se define como una suma de Poisson de variables aleatorias gamma, conocida como una pérdida agregada en ciencias actuariales. Específicamente, supongamos que \\(N\\) tiene una distribución de Poisson con media \\(\\lambda\\), representando el número de reclamaciones. Sea {\\(y_j\\)} una secuencia i.i.d., independiente de \\(N\\), con cada \\(y_j\\) siguiendo una distribución gamma con parámetros \\(\\alpha\\) y \\(\\gamma\\), representando el monto de una reclamación. Entonces, \\(S_N = y_1 + \\ldots + y_N\\) es una suma de Poisson de gammas. La Sección 16.5 discutirá con más detalle los modelos de pérdidas agregadas. Esta sección se enfoca en el caso especial importante de la distribución de Tweedie. Para entender el aspecto de mezcla de la distribución de Tweedie, primero observa que es sencillo calcular la probabilidad de tener cero reclamaciones como \\[ \\Pr ( S_N=0) = \\Pr (N=0) = e^{-\\lambda}. \\] La función de distribución puede calcularse usando expectativas condicionales, \\[ \\Pr ( S_N \\le y) = e^{-\\lambda}+\\sum_{n=1}^{\\infty} \\Pr(N=n) \\Pr(S_n \\le y), ~~~~~y \\geq 0. \\] Dado que la suma de gammas i.i.d. es una gamma, \\(S_n\\) (no \\(S_N\\)) tiene una distribución gamma con parámetros \\(n\\alpha\\) y \\(\\gamma\\). Así, para \\(y&gt;0\\), la densidad de la distribución de Tweedie es \\[\\begin{equation} \\mathrm{f}_{S}(y) = \\sum_{n=1}^{\\infty} e^{-\\lambda} \\frac{\\lambda ^n}{n!} \\frac{\\gamma^{n \\alpha}}{\\Gamma(n \\alpha)} y^{n \\alpha -1} e^{-y \\gamma}. \\tag{13.9} \\end{equation}\\] A primera vista, esta densidad no parece pertenecer a la familia exponencial lineal dada en la ecuación (13.2). Para ver la relación, primero calculamos los momentos usando expectativas iteradas como \\[\\begin{equation} \\mathrm{E~}S_N = \\lambda \\frac{\\alpha}{\\gamma}~~~~~~\\mathrm{y}~~~~~ \\mathrm{Var~}S_N = \\frac{\\lambda \\alpha}{\\gamma^2} (1+\\alpha). \\tag{13.10} \\end{equation}\\] Ahora, definimos tres parámetros \\(\\mu, \\phi, p\\) mediante las relaciones \\[ \\lambda = \\frac{\\mu^{2-p}}{\\phi (2-p)},~~~~~~\\alpha = \\frac{2-p}{p-1}~~~~~~\\mathrm{y}~~~~~ \\frac{1}{\\gamma} = \\phi(p-1)\\mu^{p-1}. \\] Al insertar estos nuevos parámetros en la ecuación (13.9) obtenemos \\[\\begin{equation} \\mathrm{f}_{S}(y) = \\exp \\left[ \\frac{-1}{\\phi} \\left( \\frac{\\mu^{2-p}}{2-p} + \\frac{y}{(p-1)\\mu^{p-1}} \\right) + S(y,\\phi) \\right]. \\tag{13.11} \\end{equation}\\] Dejamos el cálculo de \\(S(y,\\phi)\\) como un ejercicio. Así, la distribución de Tweedie pertenece a la familia exponencial lineal. Cálculos sencillos muestran que \\[\\begin{equation} \\mathrm{E~}S_N = \\mu~~~~~~\\mathrm{y}~~~~~ \\mathrm{Var~}S_N = \\phi \\mu^p, \\tag{13.12} \\end{equation}\\] donde \\(1&lt;p&lt;2.\\) Al examinar nuestra función de varianza Tabla 13.1, la distribución de Tweedie también puede verse como una elección que es intermedia entre las distribuciones Poisson y gamma. 13.7 Lecturas adicionales y referencias Un tratamiento más extenso de los GLM puede encontrarse en el trabajo clásico de McCullagh y Nelder (1989) o la introducción más sencilla de Dobson (2002). de Jong y Heller (2008) proporcionan una reciente introducción a los GLM con un énfasis especial en seguros. Los GLM han recibido una considerable atención de parte de los actuarios de seguros de propiedad y accidentes (generales) en los últimos tiempos. Véase Anderson et al. (2004) y Clark y Thayer (2004) para introducciones. Mildenhall (1999) proporciona un análisis sistemático de la conexión entre los modelos GLM y los algoritmos para evaluar diferentes planes de tarificación. Fu y Wu (2008) proporcionan una versión actualizada, introduciendo una clase más amplia de algoritmos iterativos que se conectan bien con los modelos GLM. Para más información sobre la distribución de Tweedie, consulte J{}rgensen y de Souza (1994) y Smyth y J{}rgensen (2002). Referencias del capítulo Anderson, Duncan, Sholom Feldblum, Claudine Modlin, Doris Schirmacher, Ernesto Schirmacher and Neeza Thandi (2004). A practitioner’s guide to generalized linear models. Casualty Actuarial Society 2004 Discussion Papers 1-116, Casualty Actuarial Society. Baxter, L. A., S. M. Coutts and G. A. F. Ross (1980). Applications of linear models in motor insurance. Proceedings of the 21st International Congress of Actuaries. Zurich. pp. 11-29. Clark, David R. and Charles A. Thayer (2004). A primer on the exponential family of distributions. Casualty Actuarial Society 2004 Discussion Papers 117-148, Casualty Actuarial Society. Cox, David R. and E. J. Snell (1968). A general definition of residuals. Journal of the Royal Statistical Society, Series B, 30, 248-275. Cox, David R. and E. J. Snell (1971). On test statistics computed from residuals. Biometrika 71, 589-594. Dobson, Annette J. (2002). An Introduction to Generalized Linear Models, Second Edition. Chapman &amp; Hall, London. Fu, Luyang and Cheng-sheng Peter Wu (2008). General iteration algorithm for classification ratemaking. Variance 1(2), 193-213, Casualty Actuarial Society. de Jong, Piet and Gillian Z. Heller (2008). Generalized Linear Models for Insurance Data. Cambridge University Press, Cambridge, UK. J{}rgensen, Bent and Marta C. Paes de Souza (1994). Fitting Tweedie’s compound Poisson model to insurance claims data. Scandinavian Actuarial Journal 1994;1, 69-93. McCullagh, P. and J.A. Nelder (1989). Generalized Linear Models, Second Edition. Chapman &amp; Hall, London. Mildenhall, Stephen J. (1999). A systematic relationship between minimum bias and generalized linear models. Casualty Actuarial Society Proceedings 86, Casualty Actuarial Society. Pierce, Donald A. and Daniel W. Schafer (1986). Residuals in generalized linear models. Journal of the American Statistical Association 81, 977-986. Smyth, Gordon K. and Bent J{}rgensen (2002). Fitting Tweedie’s compound Poisson model to insurance claims data: Dispersion modelling. Astin Bulletin 32(1), 143-157. Tweedie, M. C. K. (1984). An index which distinguishes between some important exponential families. In Statistics: Applications and New Directions. Proceedings of the Indian Statistical Golden Jubilee International Conference* (Editors J. K. Ghosh and J. Roy), pp. 579-604. Indian Statistical Institute, Calcutta. 13.8 Ejercicios 13.1 Verifica las entradas en Tabla 13.8 para la distribución gamma. Específicamente: Demuestra que la distribución gamma es un miembro de la familia exponencial lineal de distribuciones. Describe los componentes de la familia exponencial lineal (\\(\\theta, \\phi, b(\\theta), S(y,\\phi)\\)) en términos de los parámetros de la distribución gamma. Demuestra que la media y la varianza de la distribución gamma y de las distribuciones de la familia exponencial lineal coinciden. 13.2 Clasificación de Tarificación. Este ejercicio considera los datos descritos en el ejemplo de clasificación de tarificación de la Sección 13.2.2, utilizando los datos de la Tabla 13.3. Ajusta un modelo de regresión gamma usando una función de enlace logarítmica con los conteos de reclamaciones como ponderaciones (\\(w_i\\)). Utiliza las variables explicativas categóricas de grupo de edad y uso del vehículo para estimar la severidad media esperada. Con base en tus parámetros estimados en la parte (a), verifica las severidades esperadas estimadas en la Tabla 13.4. 13.3 Verifica que la distribución de Tweedie es un miembro de la familia exponencial lineal de distribuciones comprobando la ecuación (13.9). En particular, proporciona una expresión para \\(S(y,\\phi)\\) (nota que \\(S(y,\\phi)\\) también depende de \\(p\\) pero no de \\(\\mu\\)). Puedes consultar a Clark y Thayer (2004) para verificar tu trabajo. Además, verifica los momentos en la ecuación (13.10). 13.9 Suplementos Técnicos - Familia Exponencial 13.9.1 Familia Exponencial Lineal de Distribuciones La distribución de la variable aleatoria \\(y\\) puede ser discreta, continua o una mezcla. Así, f(.) en la ecuación (13.2) puede interpretarse como una función de densidad o una función de masa, dependiendo de la aplicación. Tabla 13.8 proporciona varios ejemplos, incluyendo las distribuciones normal, binomial y Poisson. Tabla 13.8. Distribuciones Seleccionadas de la Familia Exponencial de Un Parámetro \\[ \\scriptsize{ \\begin{array}{l|ccccc} \\hline &amp; &amp; \\text{Densidad o} &amp; &amp; &amp; \\\\ \\text{Distribución} &amp; \\text{Parámetros} &amp; \\text{Función de Masa} &amp; \\text{Componentes} &amp; \\mathrm{E}~y &amp; \\mathrm{Var}~y \\\\ \\hline \\text{General} &amp; \\theta,~ \\phi &amp; \\exp \\left( \\frac{y\\theta -b(\\theta )}{\\phi } +S\\left( y,\\phi \\right) \\right) &amp; \\theta,~ \\phi, b(\\theta), S(y, \\phi) &amp; b^{\\prime}(\\theta) &amp; b^{\\prime \\prime}(\\theta) \\phi \\\\ \\text{Normal} &amp; \\mu, \\sigma^2 &amp; \\frac{1}{\\sigma \\sqrt{2\\pi }}\\exp \\left( -\\frac{(y-\\mu )^{2}}{2\\sigma ^{2}}\\right) &amp; \\mu, \\sigma^2, \\frac{\\theta^2}{2}, - \\left(\\frac{y^2}{2\\phi} + \\frac{\\ln(2 \\pi \\phi)}{2} \\right) &amp; \\theta=\\mu &amp; \\phi= \\sigma^2 \\\\ \\text{Binomial} &amp; \\pi &amp; {n \\choose y} \\pi ^y (1-\\pi)^{n-y} &amp; \\ln \\left(\\frac{\\pi}{1-\\pi} \\right), 1, n \\ln(1+e^{\\theta} ), &amp; n \\frac {e^{\\theta}}{1+e^{\\theta}} &amp; n \\frac {e^{\\theta}}{(1+e^{\\theta})^2} \\\\ &amp; &amp; &amp; \\ln {n \\choose y} &amp; = n \\pi &amp; = n \\pi (1-\\pi) \\\\ \\text{Poisson} &amp; \\lambda &amp; \\frac{\\lambda^y}{y!} \\exp(-\\lambda) &amp; \\ln \\lambda, 1, e^{\\theta}, - \\ln (y!) &amp; e^{\\theta} = \\lambda &amp; e^{\\theta} = \\lambda \\\\ \\text{Negativa} &amp; r,p &amp; \\frac{\\Gamma(y+r)}{y! \\Gamma(r)} p^r ( 1-p)^y &amp; \\ln(1-p), 1, -r \\ln(1-e^{\\theta}), &amp; \\frac{r(1-p)}{p} &amp; \\frac{r(1-p)}{p^2} \\\\ \\ \\ \\ \\text{Binomial}^{\\ast} &amp; &amp; &amp; ~~~\\ln \\left[ \\frac{\\Gamma(y+r)}{y! \\Gamma(r)} \\right] &amp; = \\mu &amp; = \\mu+\\mu^2/r \\\\ \\text{Gamma} &amp; \\alpha, \\gamma &amp; \\frac{\\gamma ^ \\alpha}{\\Gamma (\\alpha)} y^{\\alpha -1 }\\exp(-y \\gamma) &amp; - \\frac{\\gamma}{\\alpha}, \\frac{1}{\\alpha}, - \\ln ( - \\theta), -\\phi^{-1} \\ln \\phi &amp; - \\frac{1}{\\theta} = \\frac{\\alpha}{\\gamma} &amp; \\frac{\\phi}{\\theta ^2} = \\frac{\\alpha}{\\gamma ^2} \\\\ &amp; &amp; &amp; - \\ln \\left( \\Gamma(\\phi ^{-1}) \\right) + (\\phi^{-1} - 1) \\ln y &amp; &amp; \\\\ \\text{Inversa} &amp; \\mu, \\lambda &amp; \\sqrt{\\frac{\\lambda}{2 \\pi y^3} } \\exp \\left(- \\frac{\\lambda (y-\\mu)^2}{2 \\mu^2 y} \\right) &amp; -1/( 2\\mu^2), 1/\\lambda, -\\sqrt{-2 \\theta}, &amp; \\left(-2 \\theta \\right)^{-1/2} &amp; \\phi (-2 \\theta)^{-3/2} \\\\ \\ \\ \\ \\text{Gaussiana} &amp; &amp; &amp; \\theta/(\\phi y) - 0.5 \\ln (\\phi 2 \\pi y^3 ) &amp; = \\mu &amp; = \\frac{\\mu^3}{\\lambda} \\\\ \\text{Tweedie} &amp; &amp; \\textit{Ver Sección 13.6}\\\\ \\hline \\\\ \\end{array} } \\] \\(^{\\ast}\\) Esto supone que el parámetro \\(r\\) es fijo, pero no tiene que ser un número entero. 13.9.2 Momentos Para evaluar los momentos de las familias exponenciales, es conveniente trabajar con la función generadora de momentos. Para simplificar, asumimos que la variable aleatoria \\(y\\) es continua. Definimos la función generadora de momentos \\[\\begin{eqnarray*} M(s) &amp;=&amp; \\mathrm{E}~ e^{sy} = \\int \\exp \\left( sy+ \\frac{y \\theta - b(\\theta)}{\\phi} + S(y, \\phi) \\right) dy \\\\ &amp;=&amp; \\exp \\left(\\frac{b(\\theta + s \\phi) - b(\\theta)}{\\phi} \\right) \\int \\exp \\left( \\frac{y(\\theta + s\\phi) - b(\\theta+s\\phi)}{\\phi} + S(y, \\phi) \\right) dy \\\\ &amp;=&amp; \\exp \\left(\\frac{b(\\theta + s \\phi) - b(\\theta)}{\\phi} \\right), \\end{eqnarray*}\\] porque \\(\\int \\exp \\left( \\frac{1}{\\phi} \\left[ y(\\theta + s\\phi) - b(\\theta+s\\phi) \\right] + S(y, \\phi) \\right) dy=1\\). Con esta expresión, podemos generar los momentos. Así, para la media, tenemos \\[\\begin{eqnarray*} \\mathrm{E}~ y &amp;=&amp; M^{\\prime}(0) = \\left . \\frac{\\partial}{\\partial s} \\exp \\left(\\frac{b(\\theta + s \\phi) - b(\\theta)}{\\phi} \\right) \\right | _{s=0} \\\\ &amp;=&amp; \\left[ b^{\\prime}(\\theta + s \\phi ) \\exp \\left( \\frac{b(\\theta + s \\phi) - b(\\theta)}{\\phi} \\right) \\right] _{s=0} = b^{\\prime}(\\theta). \\end{eqnarray*}\\] De manera similar, para el segundo momento, tenemos \\[\\begin{eqnarray*} M^{\\prime \\prime}(s) &amp;=&amp; \\frac{\\partial}{\\partial s} \\left[ b^{\\prime}(\\theta + s \\phi) \\exp \\left(\\frac{b(\\theta + s \\phi) - b(\\theta)}{\\phi} \\right) \\right] \\\\ &amp;=&amp; \\phi b^{\\prime \\prime}(\\theta + s \\phi) \\exp \\left(\\frac{b(\\theta + s \\phi) - b(\\theta)}{\\phi} \\right) + (b^{\\prime}(\\theta + s \\phi))^2 \\left(\\frac{b(\\theta + s \\phi) - b(\\theta)}{\\phi} \\right). \\end{eqnarray*}\\] Esto produce \\(\\mathrm{E}y^2 = M^{\\prime \\prime}(0)\\) \\(= \\phi b^{\\prime \\prime}(\\theta) + (b^{\\prime}(\\theta))^2\\) y \\(\\mathrm{Var}~y =\\phi b^{\\prime \\prime}(\\theta)\\). 13.9.3 Estimación de Máxima Verosimilitud para Enlaces Generales Para enlaces generales, ya no asumimos la relación \\(\\theta_i=\\mathbf{x}_i^{\\mathbf{\\prime }}\\boldsymbol \\beta\\), sino que asumimos que \\(\\boldsymbol \\beta\\) está relacionado con \\(\\theta_i\\) a través de las relaciones \\(\\mu _i=b^{\\prime }(\\theta_i)\\) y \\(\\mathbf{x}_i^{\\mathbf{\\prime }}\\boldsymbol \\beta = g\\left( \\mu _i\\right)\\). Seguimos asumiendo que el parámetro de escala varía según la observación, de modo que \\(\\phi_i = \\phi/w_i\\), donde \\(w_i\\) es una función de ponderación conocida. Usando la ecuación (13.4), tenemos que el \\(j\\)-ésimo elemento de la función de puntaje es \\[ \\frac{\\partial }{\\partial \\beta _{j}}\\ln \\mathrm{f}\\left( \\mathbf{y}\\right) =\\sum_{i=1}^n \\frac{\\partial \\theta_i}{\\partial \\beta _{j}} \\frac{y_i-\\mu _i}{\\phi _i}, \\] ya que \\(b^{\\prime }(\\theta_i)=\\mu _i\\). Ahora, usamos la regla de la cadena y la relación \\(\\mathrm{Var~}y_i=\\phi _ib^{\\prime \\prime }(\\theta_i)\\) para obtener \\[ \\frac{\\partial \\mu _i}{\\partial \\beta _{j}}=\\frac{\\partial b^{\\prime }(\\theta_i)}{\\partial \\beta _{j}}=b^{\\prime \\prime }(\\theta_i)\\frac{ \\partial \\theta_i}{\\partial \\beta _{j}}=\\frac{\\mathrm{Var~}y_i}{\\phi _i}\\frac{\\partial \\theta_i}{\\partial \\beta _{j}}. \\] Así, tenemos \\[ \\frac{\\partial \\theta_i}{\\partial \\beta _{j}}\\frac{1}{\\phi _i}=\\frac{\\partial \\mu _i}{\\partial \\beta _{j}}\\frac{1}{\\mathrm{Var~}y_i}. \\] Esto produce \\[ \\frac{\\partial }{\\partial \\beta _{j}}\\ln \\mathrm{f}\\left( \\mathbf{y}\\right) =\\sum_{i=1}^n \\frac{\\partial \\mu _i}{\\partial \\beta _{j}}\\left( \\mathrm{Var~}y_i\\right) ^{-1}\\left( y_i-\\mu _i\\right), \\] que resumimos como \\[\\begin{equation} \\frac{\\partial L(\\boldsymbol \\beta) }{\\partial \\boldsymbol \\beta } = \\frac{\\partial }{\\partial \\boldsymbol \\beta }\\ln \\mathrm{f}\\left( \\mathbf{y}\\right) =\\sum_{i=1}^n \\frac{\\partial \\mu _i}{\\partial \\boldsymbol \\beta}\\left( \\mathrm{Var~}y_i\\right) ^{-1}\\left( y_i-\\mu _i\\right), \\tag{13.13} \\end{equation}\\] que se conoce como la forma de las ecuaciones de estimación generalizadas. Resolver las raíces de la ecuación de puntaje \\(L(\\boldsymbol \\beta) = \\mathbf{0}\\) proporciona estimaciones de máxima verosimilitud, \\(\\mathbf{b}_{MLE}\\). En general, esto requiere métodos numéricos iterativos. Una excepción es el siguiente caso especial. Caso especial. Sin covariables. Supongamos que \\(k=1\\) y que \\(x_i =1\\). Entonces, \\(\\beta = \\eta = g(\\mu)\\), y \\(\\mu\\) no depende de \\(i\\). A partir de la relación \\(\\mathrm{Var~}y_i = \\phi b^{\\prime \\prime}(\\theta)/w_i\\) y la ecuación (13.13), la función de puntaje es \\[ \\frac{\\partial L( \\beta) }{\\partial \\beta } = \\frac{\\partial \\mu}{\\partial \\beta} ~ \\frac{1}{\\phi b^{\\prime \\prime}(\\theta)} \\sum_{i=1}^n w_i \\left( y_i-\\mu \\right). \\] Igualando esto a cero se obtiene \\(\\overline{y}_w = \\sum_i w_i y_i / \\sum_i w_i = \\widehat{\\mu}_{MLE} = g^{-1}(b_{MLE})\\), o \\(b_{MLE}=g(\\overline{y}_w)\\), donde \\(\\overline{y}_w\\) es un promedio ponderado de los \\(y\\). Para la matriz de información, usamos la independencia entre los sujetos y la ecuación (13.13) para obtener \\[\\begin{eqnarray} \\mathbf{I}(\\boldsymbol \\beta) &amp;=&amp; \\mathrm{E~} \\left( \\frac{\\partial L(\\boldsymbol \\beta) }{\\partial \\boldsymbol \\beta } \\frac{\\partial L(\\boldsymbol \\beta) }{\\partial \\boldsymbol \\beta ^{\\prime}} \\right) \\nonumber \\\\ &amp;=&amp; \\sum_{i=1}^n \\frac{\\partial \\mu _i}{\\partial \\boldsymbol \\beta}~\\mathrm{E} \\left( \\left( \\mathrm{Var~}y_i\\right) ^{-1}\\left( y_i-\\mu _i\\right)^2 \\left( \\mathrm{Var~}y_i\\right) ^{-1}\\right) \\frac{\\partial \\mu _i}{\\partial \\boldsymbol \\beta ^{\\prime}} \\nonumber \\\\ &amp;=&amp; \\sum_{i=1}^n \\frac{\\partial \\mu _i}{\\partial \\boldsymbol \\beta} \\left( \\mathrm{Var~}y_i\\right) ^{-1} \\frac{\\partial \\mu _i}{\\partial \\boldsymbol \\beta ^{\\prime}} . \\tag{13.14} \\end{eqnarray}\\] Tomar la raíz cuadrada de los elementos diagonales de la inversa de \\(\\mathbf{I}(\\boldsymbol \\beta)\\), después de insertar las estimaciones de los parámetros en las funciones de media y varianza, proporciona errores estándar basados en el modelo. Para datos en los que se duda de la correcta especificación del componente de varianza, se reemplaza Var \\(y_i\\) por una estimación insesgada \\((y_i - \\mu_i)^2\\). El estimador resultante \\[ se(b_j) = \\sqrt{ j\\text{-ésimo elemento diagonal de } [\\sum_{i=1}^n \\frac{\\partial \\mu _i}{\\partial \\boldsymbol \\beta} (y_i - \\mu_i)^{-2} \\frac{\\partial \\mu _i}{\\partial \\boldsymbol \\beta ^{\\prime}}]|_{\\boldsymbol \\beta = \\mathbf{b}_{MLE}}^{-1}} \\] se conoce como el error estándar empírico o robusto. 13.9.4 Mínimos Cuadrados Reponderados Iterativos Para ver cómo funcionan los métodos iterativos generales, usamos el enlace canónico de modo que \\(\\eta_i = \\theta_i\\). Luego, la función de puntaje está dada en la ecuación (13.6) y el Hessiano en la ecuación (13.8). Usando la ecuación de Newton-Raphson (11.15) en la Sección 11.9 obtenemos \\[\\begin{equation} \\boldsymbol \\beta_{NEW} = \\boldsymbol \\beta_{OLD} - \\left( \\sum_{i=1}^n w_i b^{\\prime \\prime}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta _{OLD}) \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\right)^{-1} \\left( \\sum_{i=1}^n w_i (y_i - b^{\\prime}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta_{OLD})) \\mathbf{x}_i \\right). \\tag{13.15} \\end{equation}\\] Observa que, dado que la matriz de segundas derivadas no es estocástica, Newton-Raphson es equivalente al algoritmo de puntuación de Fisher. Como se describe en la Sección 13.3.1, la estimación de \\(\\boldsymbol \\beta\\) no requiere el conocimiento de \\(\\phi\\). Seguimos usando el enlace canónico y definimos una “variable dependiente ajustada” \\[ y_i^{\\ast}(\\boldsymbol \\beta) = \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta + \\frac{y_i - b^{\\prime}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta)}{b^{\\prime \\prime}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta)}. \\] Esto tiene varianza \\[ \\mathrm{Var~}y_i^{\\ast}(\\boldsymbol \\beta) = \\frac{\\mathrm{Var~}y_i}{(b^{\\prime \\prime}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta))^2} = \\frac{\\phi_i b^{\\prime \\prime}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta)}{(b^{\\prime \\prime}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta))^2} = \\frac{\\phi/ w_i }{b^{\\prime \\prime}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta)}. \\] Usamos el nuevo peso como el recíproco de la varianza, \\(w_i(\\boldsymbol \\beta)=w_i b^{\\prime \\prime}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta) / \\phi.\\) Luego, con la expresión \\[ w_i \\left(y_i - b^{\\prime}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta)\\right) = w_i b^{\\prime \\prime}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta) (y_i^{\\ast}(\\boldsymbol \\beta) - \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta) = \\phi w_i(\\boldsymbol \\beta) (y_i^{\\ast}(\\boldsymbol \\beta) - \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta), \\] a partir de la iteración de Newton-Raphson en la ecuación (13.15), tenemos \\(\\boldsymbol \\beta_{NEW}\\) \\[\\begin{eqnarray*} ~ &amp;=&amp; \\boldsymbol \\beta_{OLD} - \\left( \\sum_{i=1}^n w_i b^{\\prime \\prime}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta _{OLD}) \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\right)^{-1} \\left( \\sum_{i=1}^n w_i (y_i - b^{\\prime}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta_{OLD})) \\mathbf{x}_i \\right) \\\\ &amp;=&amp; \\boldsymbol \\beta_{OLD} - \\left( \\sum_{i=1}^n \\phi w_i(\\boldsymbol \\beta _{OLD}) \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\right)^{-1} \\left( \\sum_{i=1}^n \\phi w_i(\\boldsymbol \\beta _{OLD}) (y_i^{\\ast}(\\boldsymbol \\beta_{OLD}) - \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta_{OLD})\\mathbf{x}_i \\right) \\\\ &amp;=&amp; \\boldsymbol \\beta_{OLD} - \\left( \\sum_{i=1}^n w_i(\\boldsymbol \\beta _{OLD}) \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\right)^{-1} \\left( \\sum_{i=1}^n w_i(\\boldsymbol \\beta _{OLD})\\mathbf{x}_i y_i^{\\ast}(\\boldsymbol \\beta_{OLD}) - \\sum_{i=1}^n w_i(\\boldsymbol \\beta _{OLD})\\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta_{OLD}) \\right) \\\\ &amp;=&amp; \\left( \\sum_{i=1}^n w_i(\\boldsymbol \\beta _{OLD}) \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\right)^{-1} \\left( \\sum_{i=1}^n w_i(\\boldsymbol \\beta _{OLD})\\mathbf{x}_i y_i^{\\ast}(\\boldsymbol \\beta_{OLD}) \\right). \\end{eqnarray*}\\] Así, esto proporciona un método para la iteración usando mínimos cuadrados ponderados. Los mínimos cuadrados reponderados iterativos también están disponibles para enlaces generales utilizando la puntuación de Fisher. Ver McCullagh y Nelder (1989) para más detalles. "],["C14Survival.html", "Capítulo 14 Modelos de Supervivencia 14.1 Introducción 14.2 Censura y Truncamiento 14.3 Modelo de Tiempo de Fallo Acelerado 14.4 Modelo de Riesgos Proporcionales 14.5 Eventos Recurrentes 14.6 Lecturas Adicionales y Referencias", " Capítulo 14 Modelos de Supervivencia Vista Previa del Capítulo. Este capítulo introduce la regresión donde la variable dependiente es el tiempo hasta un evento, como el tiempo hasta la muerte, el inicio de una enfermedad o el incumplimiento de un préstamo. Los tiempos de eventos a menudo están limitados por procedimientos de muestreo, por lo que en este capítulo se resumen ideas de censura y truncamiento de datos. Los tiempos de eventos son no negativos y sus distribuciones se describen en términos de funciones de supervivencia y de riesgo. Se consideran dos tipos de regresión basada en el riesgo: un modelo completamente paramétrico de tiempo de falla acelerado y un modelo semiparamétrico de riesgos proporcionales. 14.1 Introducción En los modelos de supervivencia, la variable dependiente es el tiempo hasta un evento de interés. El ejemplo clásico de un evento es el tiempo hasta la muerte (el complemento de la muerte es la supervivencia). Los modelos de supervivencia ahora se aplican ampliamente en muchas disciplinas científicas; otros ejemplos de eventos de interés incluyen el inicio de la enfermedad de Alzheimer (biomedicina), el tiempo hasta la bancarrota (economía) y el tiempo hasta el divorcio (sociología). Ejemplo: Tiempo hasta la Bancarrota. Shumway (2001) examinó el tiempo hasta la bancarrota para 3,182 empresas listadas en el archivo industrial de Compustat y el archivo de rendimientos diarios de acciones de CRSP para la Bolsa de Nueva York durante el período 1962-1992. Se examinaron varias variables financieras explicativas, incluyendo capital de trabajo sobre activos totales, ganancias retenidas sobre activos totales, ganancias antes de intereses e impuestos sobre activos totales, capital de mercado sobre pasivos totales, ventas sobre activos totales, ingresos netos sobre activos totales, pasivos totales sobre activos totales y activos corrientes sobre pasivos corrientes. El conjunto de datos incluyó 300 bancarrotas de 39,745 años-firma. Véase también Kim et al. (1995) para un estudio similar sobre insolvencias en seguros. Una característica distintiva del modelado de supervivencia es que es común que la variable dependiente solo se observe de manera limitada. Algunos eventos de interés, como la bancarrota o el divorcio, nunca ocurren para sujetos específicos (y por lo tanto pueden pensarse como que toman un tiempo infinito). Para otros sujetos, incluso cuando el tiempo del evento es finito, puede ocurrir después del período de estudio, de modo que los datos están censurados (por la derecha). Es decir, la información completa sobre los tiempos de eventos puede no estar disponible debido al diseño del estudio. Además, las empresas pueden fusionarse o ser adquiridas por otras empresas, y las personas pueden mudarse de un área geográfica, dejando el estudio. Por lo tanto, los datos pueden estar limitados por eventos que son ajenos a la pregunta de investigación en consideración, representados como censura aleatoria. La censura es una característica regular de los datos de supervivencia; los valores grandes de una variable dependiente requieren más tiempo para desarrollarse, por lo que pueden ser más difíciles de observar que los valores pequeños, considerando otras condiciones iguales. También pueden ocurrir otros tipos de limitaciones; los sujetos cuya observación depende de la experiencia del evento de interés se denominan truncados. Para ilustrar, en una investigación de mortalidad en la vejez, solo aquellos que sobreviven hasta los 85 años son reclutados para ser parte del estudio. La Sección 14.2 describe la censura y el truncamiento con más detalle. Otra característica distintiva del modelado de supervivencia es que la variable dependiente tiene valores positivos. Por lo tanto, las aproximaciones de curva normal utilizadas en la regresión lineal son menos útiles en el análisis de supervivencia; este capítulo introduce modelos de regresión alternativos. Además, es habitual interpretar los modelos de supervivencia utilizando la función de riesgo, definida como \\[ \\mathrm{h}(t)= \\frac{\\textit{función de densidad de probabilidad}} {\\textit{función de supervivencia}} =\\frac{\\mathrm{f}(t)}{\\mathrm{S}(t)}, \\] la probabilidad “instantánea” de un evento, condicionada a la supervivencia hasta el tiempo \\(t\\). La función de riesgo recibe muchos otros nombres: se conoce como la fuerza de mortalidad en la ciencia actuarial, la tasa de falla en ingeniería y la función de intensidad en procesos estocásticos. En economía, las funciones de riesgo se utilizan para describir la dependencia de duración, la relación entre la probabilidad instantánea de un evento (la densidad) y el tiempo transcurrido en un estado dado. La dependencia de duración negativa está asociada con tasas de riesgo decrecientes. Por ejemplo, cuanto mayor sea el tiempo hasta que un reclamante solicite un pago por una lesión asegurada, menor será la probabilidad de hacer una solicitud. La dependencia de duración positiva está asociada con tasas de riesgo crecientes. Por ejemplo, la mortalidad humana en la vejez generalmente muestra una tasa de riesgo creciente. Cuanto mayor sea la edad de una persona, mayor será la probabilidad de muerte en el corto plazo. Una cantidad relacionada de interés es la función acumulada de riesgo, \\(H(t)= \\int_0^t h(s)ds\\). Esta cantidad también puede expresarse como el logaritmo negativo de la función de supervivencia, y de manera inversa, \\(\\Pr(y&gt;t)=\\mathrm{S}(t) = \\exp (-H(t))\\). Los dos modelos de regresión más utilizados en el análisis de supervivencia están basados en funciones de riesgo. La Sección 14.3 introduce el modelo de tiempo de falla acelerado, donde se asume un modelo lineal para el logaritmo del tiempo hasta la falla, pero con una distribución de error que no necesita ser aproximadamente normal. La Sección 14.4 introduce el modelo de riesgos proporcionales debido a Cox (1972), donde se asume que la función de riesgo puede escribirse como el producto de un riesgo “base” y una función de una combinación lineal de variables explicativas. Con los datos de supervivencia, observamos una muestra transversal de sujetos donde el tiempo es la variable dependiente de interés. Al igual que en el Capítulo 10 sobre datos longitudinales y de panel, también existen aplicaciones en las que estamos interesados en observaciones repetidas para cada sujeto. Por ejemplo, si sufres una lesión cubierta por un seguro, los pagos que surgen de esta reclamación pueden ocurrir repetidamente a lo largo del tiempo, dependiendo del tiempo de recuperación. La Sección 14.5 introduce la noción de tiempos de eventos repetidos, llamados eventos recurrentes. 14.2 Censura y Truncamiento 14.2.1 Definiciones y Ejemplos Dos tipos de limitaciones que se encuentran en los datos de supervivencia son la censura y el truncamiento. La censura y el truncamiento también son características comunes en otras aplicaciones actuariales, incluidas las del Capítulo 16 sobre modelos de dos partes y el Capítulo 17 sobre modelos de colas largas. Por lo tanto, esta sección describe estos conceptos en detalle. Para la censura, la forma más común es la censura por la derecha, en la cual observamos el menor valor entre la “verdadera” variable dependiente y una variable de tiempo de censura. Por ejemplo, supongamos que deseamos estudiar el tiempo hasta que un nuevo empleado deje la empresa y que tenemos cinco años de datos para realizar nuestro análisis. Entonces, observamos el menor valor entre cinco años y el tiempo que el empleado estuvo con la empresa. También observamos si el empleado dejó la empresa dentro de esos cinco años. Usando notación, sea \\(y\\) el tiempo hasta el evento, como la cantidad de tiempo que el empleado trabajó en la empresa. Sea \\(C_U\\) el tiempo de censura, como \\(C_U=5\\). Entonces, observamos la variable \\(y_U^{\\ast}= \\min(y, C_U)\\). También observamos si ocurrió o no la censura. Sea \\(\\delta_U= \\mathrm{I}(y \\geq C_U)\\) una variable binaria que es 1 si ocurre censura, \\(y \\geq C_U\\), y 0 en caso contrario. Por ejemplo, en la Figura 14.1, los valores de \\(y\\) que son mayores que el límite superior de censura \\(C_U\\) no se observan; por lo tanto, esto se llama comúnmente censura por la derecha. Figura 14.1: Figura que Ilustra Censura por la Izquierda y por la Derecha Otras formas comunes de censura son la censura por la izquierda y la censura por intervalo. Con la censura por la izquierda, observamos \\(y_L^{\\ast}= \\max(y, C_L)\\) y \\(\\delta_L= \\mathrm{I}(y \\leq C_L)\\) donde \\(C_L\\) es el tiempo de censura. Por ejemplo, si estás realizando un estudio y entrevistas a una persona sobre un evento pasado, el sujeto puede recordar que el evento ocurrió antes de \\(C_L\\) pero no la fecha exacta. Con la censura por intervalo, hay un intervalo de tiempo, como \\((C_L, C_U)\\), en el que se sabe que \\(y\\) ocurre pero no se observa el valor exacto. Por ejemplo, podrías estar mirando dos años sucesivos de registros anuales de empleados. Las personas empleadas en el primer año pero no en el segundo dejaron el trabajo en algún momento durante el año. Con una fecha exacta de salida, podrías calcular el tiempo que estuvieron en la empresa. Sin la fecha de salida, solo sabes que salieron en algún momento durante un intervalo de un año. Los tiempos de censura como \\(C_L\\) y \\(C_U\\) pueden o no ser estocásticos. Si los tiempos de censura representan variables conocidas por el analista, como el período de observación, se dice que son tiempos de censura fija. Aquí, el adjetivo “fijo” significa que los tiempos son conocidos de antemano. Los tiempos de censura pueden variar según el individuo pero seguir siendo fijos. Sin embargo, la censura también puede ser el resultado de fenómenos imprevistos, como la fusión de una empresa o el cambio de domicilio de un sujeto fuera de un área geográfica. En este caso, la censura \\(C\\) se representa mediante una variable aleatoria y se dice que es censura aleatoria. Si la censura es fija o si el tiempo de censura aleatorio es independiente del evento de interés, entonces se dice que la censura es no informativa. Si los tiempos de censura son independientes del evento de interés, entonces esencialmente podemos tratar la censura como fija. Cuando la censura y el tiempo hasta el evento son dependientes (conocido como censura informativa), entonces se requieren modelos especiales. Por ejemplo, si el evento de interés es el tiempo hasta la quiebra de una empresa y el mecanismo de censura implica la presentación de estados financieros adecuados, entonces existe una relación potencial. Las empresas financieramente débiles tienen más probabilidades de quebrar y menos probabilidades de realizar el esfuerzo contable para presentar estados financieros adecuados. Las observaciones censuradas están disponibles para estudio, aunque de forma limitada. En contraste, las respuestas truncadas son un tipo de datos faltantes. Para ilustrarlo, con datos truncados por la izquierda, si \\(y\\) es menor que un umbral (digamos, \\(C_L\\)), entonces no se observa. Para datos truncados por la derecha, si \\(y\\) excede un umbral (digamos, \\(C_U\\)), entonces no se observa. Para comparar observaciones truncadas y censuradas, considera el siguiente ejemplo. Figura 14.2: Línea de tiempo para varios sujetos en un estudio de mortalidad. Ejemplo: Estudio de Mortalidad. Supongamos que estás realizando un estudio de dos años sobre la mortalidad de sujetos de alto riesgo, comenzando el 1 de enero de 2010 y finalizando el 1 de enero de 2012. La Figura 14.2 representa gráficamente los seis tipos de sujetos reclutados. Para cada sujeto, el inicio de la flecha representa el momento en que el sujeto fue reclutado y el final de la flecha representa el tiempo del evento. Por lo tanto, la flecha representa el tiempo de exposición. Tipo A - censura por la derecha. Este sujeto está vivo al inicio y al final del estudio. Como no se conoce el tiempo de muerte al final del estudio, está censurado por la derecha. La mayoría de los sujetos son de tipo A. Tipo B. Se dispone de información completa para un sujeto de tipo B. El sujeto está vivo al inicio del estudio y la muerte ocurre dentro del período de observación. Tipo C - censura por la derecha y truncamiento por la izquierda. Un sujeto de tipo C está censurado por la derecha, ya que la muerte ocurre después del período de observación. Sin embargo, el sujeto ingresó después del inicio del estudio y se dice que tiene un tiempo de entrada retrasado. Como el sujeto no habría sido observado si la muerte hubiera ocurrido antes de la entrada, está truncado por la izquierda. Tipo D - truncamiento por la izquierda. Un sujeto de tipo D también tiene una entrada retrasada. Como la muerte ocurre dentro del período de observación, este sujeto no está censurado por la derecha. Tipo E - truncamiento por la izquierda. Un sujeto de tipo E no está incluido en el estudio porque la muerte ocurre antes del período de observación. Tipo F - truncamiento por la derecha. De manera similar, un sujeto de tipo F no está incluido porque el tiempo de entrada ocurre después del período de observación. 14.2.2 Inferencia por Verosimilitud Muchas técnicas de inferencia para el modelado de supervivencia implican la estimación por verosimilitud, por lo que es útil entender las implicaciones de la censura y el truncamiento al especificar funciones de verosimilitud. Para simplificar, asumimos tiempos de censura fijos y un tiempo hasta el evento continuo \\(y\\). Para comenzar, considere el caso de datos censurados por la derecha, donde observamos \\(y_U^{\\ast}= \\min(y, C_U)\\) y \\(\\delta_U= \\mathrm{I}(y \\geq C_U)\\). Si ocurre censura de modo que \\(\\delta_U=1\\), entonces \\(y \\geq C_U\\) y la verosimilitud es \\(\\Pr(y \\geq C_U) = \\mathrm{S}(C_U)\\). Si no ocurre censura de modo que \\(\\delta_U=0\\), entonces \\(y &lt; C_U\\) y la verosimilitud es \\(\\mathrm{f}(y)\\). Resumiendo, tenemos: \\[\\begin{eqnarray*} \\text{Verosimilitud} &amp;=&amp; \\left\\{ \\begin{array}{cl} \\mathrm{f}(y) &amp; \\textrm{si}~\\delta=0 \\\\ \\mathrm{S}(C_U) &amp; \\textrm{si}~\\delta=1 \\end{array} \\right. \\\\ &amp;=&amp; \\left( \\mathrm{f}(y)\\right)^{1-\\delta} \\left( \\mathrm{S}(C_U)\\right)^{\\delta} . \\end{eqnarray*}\\] La expresión de la derecha nos permite presentar la verosimilitud de manera más compacta. Ahora, para una muestra independiente de tamaño \\(n\\), \\(\\{ (y_{U1}, \\delta_1), \\ldots,(y_{Un}, \\delta_n) \\}\\), la verosimilitud es: \\[ \\prod_{i=1}^n \\left( \\mathrm{f}(y_i)\\right)^{1-\\delta_i} \\left( \\mathrm{S}(C_{Ui})\\right)^{\\delta_i} = \\prod_{\\delta_i=0} \\mathrm{f}(y_i) \\prod_{\\delta_i=1} \\mathrm{S}(C_{Ui}), \\] con posibles tiempos de censura \\(\\{ C_{U1}, \\ldots,C_{Un} \\}\\). Aquí, la notación “\\(\\prod_{\\delta_i=0}\\)” significa tomar el producto sobre observaciones no censuradas, y de manera similar para “\\(\\prod_{\\delta_i=1}\\)”. Los datos truncados se manejan en la inferencia por verosimilitud mediante probabilidades condicionales. Específicamente, ajustamos la contribución a la verosimilitud dividiendo por la probabilidad de que la variable haya sido observada. Resumiendo, tenemos las siguientes contribuciones a la verosimilitud para seis tipos de resultados: \\[ \\small{ \\begin{array}{lc} \\hline \\text{Resultado } &amp; \\text{Contribución a la Verosimilitud} \\\\\\hline \\text{Valor exacto} &amp; f(y) \\\\ \\text{Censura por la derecha} &amp; S(C_U) \\\\ \\text{Censura por la izquierda} &amp; 1-S(C_L) \\\\ \\text{Truncamiento por la derecha} &amp; f(y)/(1-S(C_U)) \\\\ \\text{Truncamiento por la izquierda} &amp; f(y)/S(C_L) \\\\ \\text{Censura por intervalo} &amp; S(C_L)-S(C_U) \\\\ \\hline \\end{array} } \\] Para tiempos de evento conocidos y datos censurados, la verosimilitud es: \\[ \\prod_{E} \\mathrm{f}(y_i) \\prod_{R} \\mathrm{S}(C_{Ui}) \\prod_{L} (1-\\mathrm{S}(C_{Li})) \\prod_{I} (\\mathrm{S}(C_{Li})-\\mathrm{S}(C_{Ui})), \\] donde “\\(\\prod_{E}\\)” es el producto sobre observaciones con valores exactos (Exact), y de manera similar para Right (derecha), Left (izquierda) y Interval (intervalo). Para datos censurados por la derecha y truncados por la izquierda, la verosimilitud es: \\[ \\prod_{E} \\frac{\\mathrm{f}(y_i)}{\\mathrm{S}(C_{Li})} \\prod_{R} \\frac{\\mathrm{S}(C_{Ui})}{\\mathrm{S}(C_{Li})} , \\] y de manera similar para otras combinaciones. Para obtener más información, considere lo siguiente. Caso Especial: Distribución Exponencial. Considere datos censurados por la derecha y truncados por la izquierda, con variables dependientes \\(y_i\\) distribuidas exponencialmente con media \\(\\mu\\). Con estas especificaciones, recuerde que \\(\\mathrm{f}(y) = \\mu^{-1} \\exp(-y/\\mu)\\) y \\(\\mathrm{S}(y) = \\exp(-y/\\mu)\\). Para este caso especial, la verosimilitud logarítmica es: \\[ \\begin{array}{ll} \\ln \\text{Verosimilitud} &amp;= \\sum_{E} \\left( \\ln \\mathrm{f}(y_i) - \\ln \\mathrm{S}(C_{Li}) \\right) +\\sum_{R}\\left( \\ln \\mathrm{S}(C_{Ui})- \\ln \\mathrm{S}(C_{Li}) \\right) \\\\ &amp;= \\sum_{E} (-\\ln \\mu -(y_i-C_{Li})/\\mu ) -\\sum_{R} (C_{Ui}-C_{Li})/\\mu . \\end{array} \\] Para simplificar la notación, definimos \\(\\delta_i = \\mathrm{I}(y_i \\geq C_{Ui})\\) como una variable binaria que indica censura por la derecha. Sea \\(y_i^{\\ast \\ast} = \\min(y_i, C_{Ui}) - C_{Li}\\) la cantidad en que la variable observada excede el límite inferior de truncamiento. Con esto, la verosimilitud logarítmica es: \\[\\begin{equation} \\ln \\text{Verosimilitud} = - \\sum_{i=1}^n ((1-\\delta_i) \\ln \\mu + \\frac{y_i^{\\ast \\ast}}{\\mu} ). \\tag{14.1} \\end{equation}\\] Derivando con respecto al parámetro \\(\\mu\\) e igualando a cero se obtiene el estimador de máxima verosimilitud: \\[\\begin{eqnarray*} \\widehat{\\mu} &amp;=&amp; \\frac{1}{n_u} \\sum_{i=1}^n y_i^{\\ast \\ast}, \\end{eqnarray*}\\] donde \\(n_u = \\sum_i (1-\\delta_i)\\) es el número de observaciones no censuradas. 14.2.3 Estimador Producto-Límite Puede ser útil calibrar los métodos de verosimilitud con métodos no paramétricos que no dependen de una forma paramétrica de la distribución. El estimador producto-límite debido a Kaplan y Meier (1958) es un estimador bien conocido de la distribución en presencia de censura. Para introducir este estimador, consideremos el caso de datos censurados por la derecha. Sean \\(t_1 &lt; \\cdots &lt; t_c\\) los puntos de tiempo distintos en los que ocurre un evento de interés y sea \\(d_j\\) el número de eventos en el punto de tiempo \\(t_j\\). Además, definimos \\(R_j\\) como el “conjunto de riesgo” correspondiente, es decir, el número de observaciones que están activas en un instante justo antes de \\(t_j\\). Usando notación, el conjunto de riesgo es \\(R_j = \\sum_{i=1}^n I\\left( y_i \\geq t_j \\right)\\). Con esta notación, el estimador producto-límite de la función de supervivencia es: \\[\\begin{equation} \\widehat{S}(t) = \\left\\{ \\begin{array}{ll} 1 &amp; t&lt;t_1 \\\\ \\prod_{t_j \\leq t} \\left(1 - \\frac{d_j}{R_j} \\right) &amp; t \\geq t_1 \\end{array} \\right. . \\tag{14.2} \\end{equation}\\] Para interpretar el estimador producto-límite, evaluémoslo en los tiempos de eventos. En el primer tiempo de evento \\(t_1\\), el estimador es \\(\\widehat{S}(t_1)= 1- d_1/R_1=(R_1 - d_1)/R_1\\), la proporción de no eventos en el conjunto de riesgo \\(R_1\\). En el segundo tiempo de evento \\(t_2\\), la probabilidad de supervivencia condicional a la supervivencia hasta el tiempo \\(t_1\\) es \\(\\widehat{S}(t_2)/ \\widehat{S}(t_1)\\) \\(= 1 - d_2/R_2\\) \\(=(R_2 - d_2)/R_2\\), la proporción de no eventos en el conjunto de riesgo \\(R_2\\). De manera similar, en el \\(j\\)-ésimo tiempo de evento: \\[ \\frac{\\widehat{S}(t_j)}{\\widehat{S}(t_{j-1})}= 1 - \\frac{d_j}{R_j}=\\frac{R_j - d_j}{R_j} . \\] A partir de estas probabilidades condicionales, se puede construir el estimador de supervivencia como: \\[ \\widehat{S}(t_j) = \\frac{\\widehat{S}(t_j)}{\\widehat{S}(t_{j-1})} \\times \\cdots \\times \\frac{\\widehat{S}(t_2)}{\\widehat{S}(t_{1})} \\times \\widehat{S}(t_{1}), \\] lo que resulta en la ecuación (14.2). En este sentido, el estimador es un “producto,” hasta el “límite” del tiempo. Para tiempos entre los tiempos de evento, la estimación de supervivencia se considera constante. Para ver cómo usar el estimador producto-límite, consideremos un pequeño conjunto de datos de \\(n=23\\) observaciones, donde 18 son tiempos de evento y 5 han sido censurados por la derecha. Este ejemplo es de Miller (1997), donde los eventos corresponden a la supervivencia de pacientes con leucemia mieloide aguda. Después de recibir quimioterapia para lograr la remisión completa, se asignaron aleatoriamente a uno de dos grupos, aquellos que recibieron quimioterapia de mantenimiento y aquellos que no (el grupo control). El evento fue el tiempo en semanas hasta la recaída desde el estado de remisión completa. Para el grupo de mantenimiento, los tiempos son: 9, 13, 13+, 18, 23, 28+, 31, 34, 45+, 48, 161+. Para el grupo de control, los tiempos son: 5, 5, 8, 8, 12, 16+, 23, 27, 30, 33, 43, 45. Aquí, el signo más (+) indica censura por la derecha de una observación. Las Figuras 14.3 y 14.4 muestran las estimaciones de la función de supervivencia producto-límite para estos datos. Note la naturaleza escalonada de la función, donde las caídas (desde la izquierda, o saltos desde la derecha) corresponden a tiempos de eventos. Cuando no hay censura, el estimador producto-límite se reduce al estimador empírico usual de la función de supervivencia. En las figuras, las observaciones censuradas se representan con un símbolo de trazado de más (+). Cuando la última observación ha sido censurada, como en la Figura 14.3, existen diferentes métodos para definir la curva de supervivencia para tiempos que exceden esta observación. Los analistas que realizan estimaciones en estos tiempos deben ser conscientes de la opción que utiliza su paquete estadístico. Estas opciones se describen en libros estándar de análisis de supervivencia, como Klein y Moschberger (1997). Figura 14.3: Estimación Producto-Límite de las Funciones de Supervivencia para Dos Grupos. Este gráfico muestra que aquellos con tratamiento de quimioterapia de mantenimiento tienen mayores probabilidades de supervivencia estimadas que aquellos en el grupo control. Figura 14.4: Estimación Producto-Límite de la Función de Supervivencia para el Grupo Control. Los límites superior e inferior provienen de la fórmula de Greenwood para la estimación de la varianza. Figure 14.4 también muestra una estimación del error estándar. Estos se calculan utilizando la fórmula de Greenwood (1926) para una estimación de la varianza, dada por: \\[ \\widehat{\\mathrm{Var}(\\widehat{S}(t))} = (\\widehat{S}(t))^2 \\sum_{t_j \\leq t} \\frac{d_j}{R_j(R_j-d_j)} . \\] Código R para generar las Figuras 14.3 y 14.4 # Figura 14.3 library(survival) #fix(aml) aml &lt;- read.csv(&quot;Chapters/Chapter14Survival/aml.csv&quot;, header=TRUE) fit &lt;- survfit(Surv(time, status) ~ x, data = aml) plot(fit, mark.time = TRUE) text(50,0.7,&quot;Mantenimiento&quot;,adj=0, cex=1.2) text(1,0.05,&quot;Control&quot;,adj=0, cex=1.2) # Figura 14.4 library(survival) aml2 &lt;- subset(aml, x==&quot;Nonmaintained&quot; ) fit &lt;- survfit(Surv(time, status) ~ 1, data=aml2) plot(fit, mark.time = TRUE) 14.3 Modelo de Tiempo de Fallo Acelerado Un modelo de tiempo de fallo acelerado (\\(AFT\\)) puede expresarse usando una ecuación de regresión lineal \\(\\ln y_i = \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta + \\varepsilon_i\\), donde \\(y_i\\) es el tiempo de evento. A diferencia del modelo de regresión lineal usual, el modelo AFT hace una suposición paramétrica sobre el término de perturbación, como una distribución normal, de valor extremo o logística. Como hemos visto, la distribución normal es la base para el modelo de regresión lineal usual. Así, muchos analistas comienzan tomando logaritmos de los datos de eventos y utilizando rutinas de regresión lineal habituales para explorar sus datos. Distribuciones de Localización y Escala Para entender el nombre “tiempo de fallo acelerado,” también conocido como riesgos acelerados, es útil repasar la idea estadística de familias de localización y escala. Una distribución de localización y escala paramétrica es aquella donde la función de densidad tiene la forma: \\[ \\mathrm{f}(t) =\\frac{1}{\\sigma }\\mathrm{f}_0 \\left( \\frac{t-\\mu }{\\sigma }\\right) , \\] donde \\(\\mu\\) es el parámetro de localización y \\(\\sigma &gt;0\\) es el parámetro de escala. El caso \\(\\mu =0\\) y \\(\\sigma =1\\) corresponde a la forma estándar de la distribución. Dentro de una distribución de localización y escala, los cambios aditivos, como pasar de grados Kelvin a grados Centígrados, \\(^{\\circ }K=\\) \\(^{\\circ }C+273.15\\), y los múltiplos escalares, como pasar de dólares a miles de dólares, permanecen en la misma familia de distribución. Como veremos en el Capítulo 17, un parámetro de localización es un lugar natural para introducir covariables de regresión. Una variable aleatoria \\(y\\) se dice que tiene una distribución logarítmica de localización y escala si \\(\\ln y\\) tiene una distribución de localización y escala. Supongamos que una variable aleatoria \\(y_0\\) tiene una distribución de localización y escala estándar con función de supervivencia \\(\\mathrm{S}_0\\left( z\\right)\\). Entonces, la función de supervivencia de \\(y\\) definida por \\(\\ln y = \\mu + \\sigma y_0\\) puede expresarse como: \\[ \\mathrm{S}\\left( t\\right) =\\Pr \\left( y&gt;t\\right) =\\Pr \\left( y_0&gt;\\frac{\\ln t-\\mu }{\\sigma }\\right) =\\mathrm{S}_0\\left( \\frac{\\ln t-\\mu }{\\sigma } \\right) =\\mathrm{S}_0^{\\ast }\\left( \\left( \\frac{t}{e^{\\mu }}\\right) ^{1/\\sigma }\\right) , \\] donde \\(\\mathrm{S}_0^{\\ast }(t) =\\mathrm{S}_0( \\ln t)\\) es la función de supervivencia de la forma estándar de \\(\\ln y\\). En el contexto del modelado de supervivencia donde \\(t\\) representa tiempo, el efecto de reescalar dividiendo por \\(e^{\\mu }\\) puede interpretarse como “acelerar el tiempo” (cuando \\(e^{\\mu }&lt;1\\)). Esta es la motivación para el nombre “modelos de tiempo de fallo acelerado.” La Tabla 14.1 proporciona algunos casos especiales de distribuciones de localización y escala ampliamente utilizadas y sus contrapartes logarítmicas. Tabla 14.1: Distribuciones de Localización y Escala Distribución de Supervivencia Estándar Distribución de Localización y Escala Distribución Logarítmica de Localización y Escala \\(\\mathrm{S}_0(t) = \\exp(- e^t)\\) Distribución de valor extremo Weibull \\(\\mathrm{S}_0(t) =1 - \\Phi(t)\\) Normal Lognormal \\(\\mathrm{S}_0(t) = (1 + e^t)^{-1}\\) Logística Log-logística Inferencia para Modelos AFT Para tener una idea de las complejidades en la estimación de un modelo \\(AFT\\), volvemos a la distribución exponencial. Este es un caso especial de regresión Weibull con el parámetro de escala \\(\\sigma =1\\). Caso Especial: Distribución Exponencial - Continuación. Para introducir covariables de regresión, dejamos que \\(\\mu_i = \\exp (\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta)\\). Usando el mismo razonamiento que con la ecuación (14.1), la verosimilitud logarítmica es: \\[ \\begin{array}{ll} \\ln \\text{Verosimilitud} &amp;= - \\sum_{i=1}^n ((1-\\delta_i) \\ln \\mu_i +\\frac{y_i^{\\ast \\ast}}{\\mu_i} )\\\\ &amp;= - \\sum_{i=1}^n ((1-\\delta_i) \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta+ y_i^{\\ast \\ast}\\exp (-\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta)) , \\end{array} \\] donde \\(\\delta_i = I(y_i \\geq C_{Ui})\\) y \\(y_i^{\\ast \\ast} =\\min(y_i, C_{Ui}) - C_{Li}\\). Al derivar con respecto a \\(\\boldsymbol \\beta\\), obtenemos la función de puntuación: \\[ \\begin{array}{ll} \\frac{\\partial \\ln \\text{Verosimilitud}}{\\partial \\boldsymbol \\beta} &amp;= - \\sum_{i=1}^n ((1-\\delta_i) \\mathbf{x}_i - \\mathbf{x}_i y_i^{\\ast \\ast}\\exp (-\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta)) \\\\ &amp;= \\sum_{i=1}^n \\mathbf{x}_i \\frac{y_i^{\\ast \\ast}-\\mu_i (1-\\delta_i)} {\\mu_i } . \\end{array} \\] Esto tiene la forma de una ecuación de estimación generalizada, introducida en el Capítulo 13. Aunque las soluciones en forma cerrada rara vez existen, puede resolverse fácilmente con paquetes estadísticos modernos. Como ilustra este caso especial, la estimación de modelos de regresión \\(AFT\\) puede abordarse fácilmente a través de máxima verosimilitud. Además, las propiedades de las estimaciones de máxima verosimilitud son bien entendidas, por lo que contamos con herramientas generales de inferencia, como estimación y prueba de hipótesis. Los paquetes estadísticos estándar proporcionan resultados que apoyan esta inferencia. 14.4 Modelo de Riesgos Proporcionales La suposición de riesgos proporcionales se define en la Sección 14.4.1 y las técnicas de inferencia se discuten en la Sección 14.4.2. 14.4.1 Riesgos Proporcionales En el modelo de riesgos proporcionales (\\(PH\\)) debido a Cox (1972), se asume que la función de riesgo puede escribirse como el producto de un riesgo “base” y una función de una combinación lineal de variables explicativas. Para ilustrar, usamos: \\[\\begin{equation} h_i(t) = h_0(t) \\exp( \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta ). \\tag{14.3} \\end{equation}\\] donde \\(h_0(t)\\) es el riesgo base. Este se conoce como un modelo de “riesgos proporcionales” porque, si se toma la razón de funciones de riesgo para dos conjuntos de covariables, digamos \\(\\mathbf{x}_1\\) y \\(\\mathbf{x}_2\\), se obtiene: \\[ \\frac{h_1(t|\\mathbf{x}_1)}{h_2(t|\\mathbf{x}_2)} = \\frac {h_0(t) \\exp( \\mathbf{x}_1^{\\prime} \\boldsymbol \\beta )} {h_0(t) \\exp( \\mathbf{x}_2^{\\prime} \\boldsymbol \\beta )} = \\exp( (\\mathbf{x}_1-\\mathbf{x}_2)^{\\prime}\\boldsymbol \\beta ) . \\] Nótese que la razón no depende del tiempo \\(t\\). Como hemos visto en muchas aplicaciones de regresión, los usuarios están interesados en los efectos de las variables y no siempre en otros aspectos del modelo. La razón por la que el modelo \\(PH\\) en la ecuación (14.3) ha resultado tan popular es que especifica los efectos de las variables explicativas como una función simple de la combinación lineal mientras permite un componente base flexible, \\(h_0\\). Aunque este componente base es común a todos los sujetos, no necesita especificarse paramétricamente como en los modelos \\(AFT\\). Además, no es necesario usar la función “exp” para las variables explicativas; sin embargo, esta es la especificación común ya que asegura que la función de riesgo seguirá siendo no negativa. Los riesgos proporcionales pueden motivarse como una extensión de la regresión exponencial. Considere una variable aleatoria \\(y^{\\ast}\\) que tiene una distribución exponencial con media \\(\\mu = \\exp (\\mathbf{x}^{\\prime} \\boldsymbol \\beta)\\). Supongamos que observamos \\(y = \\mathrm{g}(y^{\\ast})\\), donde \\(\\mathrm{g}(\\cdot)\\) es desconocida excepto que es monótonamente creciente. Muchas distribuciones de supervivencia son transformaciones de la distribución exponencial. Por ejemplo, si \\(y = \\mathrm{g}(y^{\\ast})=(y^{\\ast})^{\\sigma}\\), entonces es fácil verificar que \\(y\\) tiene una distribución Weibull como se da en la Tabla 14.1. La distribución Weibull es el único modelo \\(AFT\\) que tiene riesgos proporcionales (ver Lawless, 2003, ejercicio 6.1). Cálculos simples muestran que la función de riesgo de \\(y\\) puede expresarse como: \\[ \\mathrm{h}_y(t) = \\mathrm{g}^{\\prime}(t) / \\mu = \\mathrm{g}^{\\prime}(t) \\exp (- \\mathbf{x}^{\\prime} \\boldsymbol \\beta ), \\] ver, por ejemplo, Zhou (2001). Esto tiene una estructura de riesgos proporcionales, como en la ecuación (14.3), donde \\(\\mathrm{g}^{\\prime}(t)\\) sirve como el riesgo base. En el modelo \\(PH\\), se asume que el riesgo base es desconocido. En contraste, para la Weibull y otros modelos de regresión \\(AFT\\), se asume que la función de riesgo base es conocida hasta uno o dos parámetros que pueden estimarse a partir de los datos. 14.4.2 Inferencia Debido a la especificación flexible del componente base, las técnicas usuales de estimación de máxima verosimilitud no están disponibles para estimar el modelo \\(PH\\). Para esbozar el procedimiento de estimación, dejamos que \\((y_1, \\delta_1), \\ldots, (y_n, \\delta_n)\\) sean independientes y asumimos que \\(y_i\\) sigue la ecuación (14.3) con regresores \\(\\mathbf{x}_i\\). Es decir, ahora dejamos de usar la notación con asterisco (\\(\\ast\\)) y dejamos que \\(y_i\\) denote valores observados (exactos o censurados) y usamos \\(\\delta_i\\) como la variable binaria que indica censura (derecha). Además, dejamos que \\(H_0\\) sea la función de riesgo acumulada asociada con la función base \\(h_0\\). Recordando la relación general \\(\\mathrm{S}(t) = \\exp (-H(t))\\), con la ecuación (14.3) tenemos \\(\\mathrm{S}(t) = \\exp \\left(-H_0(t)\\exp( \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta )\\right).\\) Desde la perspectiva usual de verosimilitud, de la Sección 14.2.2 la verosimilitud es: \\[\\begin{eqnarray*} L(\\boldsymbol \\beta , h_0)&amp;= &amp; \\prod_{i=1}^n \\mathrm{f}(y_i)^{1-\\delta_i} \\mathrm{S}(y_i)^{\\delta_i} = \\prod_{i=1}^n h(y_i)^{1-\\delta_i} \\exp(-H(y_i)) \\\\&amp;= &amp; \\prod_{i=1}^n \\left( h_0(t) \\exp( \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta ) \\right)^{1-\\delta_i} \\exp\\left(-H_0(y_i)\\exp( \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta ) \\right) . \\end{eqnarray*}\\] Ahora, las estimaciones de parámetros que maximizan \\(L(\\boldsymbol \\beta , h_0)\\) no siguen las propiedades usuales de la estimación de máxima verosimilitud porque el riesgo base \\(h_0\\) no se especifica paramétricamente. Una forma de abordar este problema es mediante Breslow (1974), quien mostró que un estimador de máxima verosimilitud no paramétrico de \\(h_0\\) podría usarse en \\(L(\\boldsymbol \\beta , h_0)\\). Esto resulta en lo que Cox llamó una verosimilitud parcial: \\[\\begin{equation} L_P(\\boldsymbol \\beta) = \\prod_{i=1}^n \\left( \\frac{\\exp( \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta )} {\\sum_{j \\in R(y_i)} \\exp( \\mathbf{x}_j^{\\prime} \\boldsymbol \\beta )} \\right)^{1-\\delta_i}, \\tag{14.4} \\end{equation}\\] donde \\(R(t)\\) es el conjunto de riesgo en el tiempo \\(t\\). Específicamente, este es el conjunto de todos \\(\\{y_1, \\ldots, y_n \\}\\) tales que \\(y_i \\geq t\\), es decir, el conjunto de todos los sujetos que aún están bajo estudio en el tiempo \\(t\\). La ecuación (14.4) es solo una verosimilitud “parcial” en el sentido de que no utiliza toda la información en \\((y_1, \\delta_1), \\ldots, (y_n, \\delta_n)\\). Por ejemplo, de la ecuación (14.4), vemos que la inferencia para los coeficientes de regresión depende únicamente de los rangos de las variables dependientes \\(\\{y_1, \\ldots, y_n \\}\\), no de sus valores reales. No obstante, la ecuación (14.4) sugiere (y es cierto) que la teoría de distribución muestral en grandes muestras tiene propiedades similares a la teoría paramétrica (totalmente) deseable. Desde la perspectiva del usuario, esta verosimilitud parcial puede tratarse como una verosimilitud usual. Es decir, los parámetros de regresión que maximizan la ecuación (14.4) son consistentes y tienen una distribución normal en grandes muestras con las estimaciones de varianza habituales (ver Sección 11.9 para una revisión). Esto es ligeramente sorprendente porque el modelo de riesgos proporcionales es semi-paramétrico; en la ecuación (14.3), la función de riesgo tiene un componente completamente paramétrico, \\(\\exp( \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta )\\), pero también contiene un riesgo base no paramétrico, \\(h_0(t)\\). En general, los modelos no paramétricos son más flexibles que sus contrapartes paramétricas para el ajuste de modelos, pero resultan en propiedades muestrales menos deseables (específicamente, tasas de convergencia más lentas a una distribución asintótica). Ejemplo: Puntajes de Crédito. Stepanova y Thomas (2002) usaron modelos de riesgos proporcionales para crear puntajes de crédito que los bancos podrían usar para evaluar la calidad de préstamos personales. Sus puntajes dependían de los propósitos del préstamo y las características de los solicitantes. Los clientes potenciales solicitan préstamos a los bancos por muchos propósitos, incluidos la financiación de la compra de una casa, automóvil, barco o instrumento musical, para mejoras en el hogar y reparaciones de automóviles, o para financiar una boda y lunas de miel; Stepanova y Thomas enumeraron 22 propósitos de préstamo. En la solicitud de préstamo, las personas proporcionaron su edad, monto solicitado, años con el empleador actual y otras características personales; Stepanova y Thomas enumeraron 22 características de la solicitud utilizadas en su análisis. Sus datos provienen de una importante institución financiera del Reino Unido. Consistieron en información de solicitud de 50,000 préstamos personales, con el estado de reembolso para cada uno de los primeros 36 meses del préstamo. Así, cada préstamo estaba (fijamente) censurado a la derecha por el menor entre la duración del período de estudio, 36 meses, y el plazo de reembolso del préstamo, que variaba entre 6 y 60 meses. Para crear los puntajes, los autores examinaron dos variables dependientes que tienen consecuencias financieras negativas para el prestamista, el tiempo hasta el incumplimiento del préstamo y el tiempo hasta el reembolso anticipado. Para este estudio, la definición de incumplimiento es 3 o más meses de morosidad. (En principio, se podrían analizar ambos tiempos simultáneamente en un marco de “riesgos competitivos”). Cuando Stepanova y Thomas usaron un modelo de riesgos proporcionales con el tiempo hasta el incumplimiento como variable dependiente, el tiempo hasta el reembolso anticipado era una variable de censura aleatoria a la derecha. A la inversa, cuando el tiempo hasta el reembolso anticipado era la variable dependiente, el tiempo hasta el incumplimiento era una variable de censura aleatoria a la derecha. Stepanova y Thomas utilizaron estimaciones del modelo de las funciones de supervivencia a los 12 meses y a los 24 meses como sus puntajes de crédito para ambas variables dependientes. Compararon estos puntajes con los de un modelo de regresión logística donde, por ejemplo, la variable dependiente era el incumplimiento del préstamo dentro de los 12 meses. Ningún enfoque dominó al otro; encontraron situaciones en las que cada enfoque de modelado proporcionaba mejores predictores del riesgo crediticio de un individuo. Una característica importante del modelo de riesgos proporcionales es que puede extenderse fácilmente para manejar covariables que varían en el tiempo de la forma \\(\\mathbf{x}_i(t)\\). En este caso, se puede escribir la verosimilitud parcial como \\[ L_P(\\boldsymbol \\beta) = \\prod_{i=1}^n \\left( \\frac{\\exp( \\mathbf{x}_i^{\\prime}(y_i) \\boldsymbol \\beta )} {\\sum_{j \\in R(y_i)} \\exp( \\mathbf{x}_j^{\\prime}(y_j) \\boldsymbol \\beta ) }\\right)^{1-\\delta_i} . \\] Usar esta verosimilitud es complejo, aunque la maximización puede realizarse fácilmente con software estadístico moderno. La inferencia es compleja porque puede ser difícil separar las covariables observadas que varían en el tiempo \\(\\mathbf{x}_i(t)\\) del riesgo base no observado que varía en el tiempo \\(\\mathrm{h}_0(t)\\). Consulte las referencias en la Sección 14.6 para más detalles. 14.5 Eventos Recurrentes Los eventos recurrentes son tiempos de eventos que pueden ocurrir repetidamente a lo largo del tiempo para cada sujeto. Para el \\(i\\)-ésimo sujeto, usamos la notación \\(y_{i1} &lt; y_{i2} &lt; \\cdots &lt; y_{im_i}\\) para denotar los \\(m_i\\) tiempos de eventos. Entre otras aplicaciones, los eventos recurrentes pueden usarse para modelar: Reclamos de garantía, Pagos de reclamos que han sido reportados a una compañía de seguros y Reclamos de eventos que han ocurrido pero aún no han sido reportados a una compañía de seguros. Ejemplo: Reclamos de Garantía de Automóviles. Cook y Lawless (2007) consideran una muestra de 15,775 automóviles que fueron vendidos y bajo garantía durante 365 días. Las garantías son garantías de confiabilidad del producto emitidas por el fabricante. Los datos de garantía corresponden a un sistema del vehículo (como frenos o tren motriz) y cubren un año con un límite de 12,000 millas. La Tabla 14.2 resume la distribución de los 2,620 reclamos de esta muestra. Tabla 14.2: Distribución de Frecuencia de Reclamos de Garantía Número de Reclamos 0 1 2 3 4 5+ Total Número de Autos 13,987 1,243 379 103 34 29 15,775 Fuente: Cook y Lawless (2007, Tabla 1.3). La Tabla 14.2 muestra que hay 1,788 (=15,775-13,987) automóviles con al menos un (\\(m_i &gt; 0\\)) reclamo. La Figura 14.5 representa la estructura de los tiempos de eventos. Figura 14.5: Figura que Ilustra Reclamos de Garantía (Potencialmente) Repetidos. Presentamos modelos de eventos recurrentes usando procesos de conteo, específicamente empleando procesos de Poisson. Aunque existen enfoques alternativos, para los análisis estadísticos el concepto de procesos de conteo parece el más fructífero (Cook y Lawless, 2007). Además, existe una fuerte conexión histórica de los procesos de Poisson con la teoría actuarial de ruina (Klugman, Panjer y Willmot, 2008, Capítulo 11), por lo que muchos actuarios están familiarizados con los procesos de Poisson. Sea \\(N_i(t)\\) el número de eventos que han ocurrido hasta el tiempo \\(t\\). Usando álgebra, podemos escribir esto como \\(N_i(t) = \\sum_{j \\geq 1} \\mathrm{I}(y_{ij} \\leq t).\\) Debido a que \\(N_i(t)\\) varía con la experiencia de un sujeto, es una variable aleatoria para cada \\(t\\) fijo. Además, al observar toda la evolución de los reclamos, \\(\\{N_i(t), t \\geq 0 \\}\\), se conoce como un proceso estocástico. Un proceso de conteo es un tipo especial de proceso estocástico que describe conteos que aumentan de manera monótona con el tiempo. Un proceso de Poisson es un tipo especial de proceso de conteo. Si \\(\\{N_i(t), t \\geq 0 \\}\\) es un proceso de Poisson, entonces \\(N_i(t)\\) tiene una distribución de Poisson con media, digamos, \\(\\mathrm{h}_i(t)\\) para cada \\(t\\) fijo. En la literatura de procesos de conteo, es común referirse a \\(\\mathrm{h}_i(t)\\) como la función de intensidad. La inferencia estadística para eventos recurrentes puede llevarse a cabo utilizando procesos de Poisson y técnicas paramétricas de máxima verosimilitud. Como se discute en Cook y Lawless (2007), la verosimilitud para el \\(i\\)-ésimo sujeto se basa en la densidad de probabilidad condicional de los resultados observados “\\(m_i\\) eventos, en los tiempos \\(y_{i1} &lt; \\cdots &lt; y_{im_i}\\)”. Esto produce la verosimilitud \\[\\begin{equation} \\text{Verosimilitud}_i = \\prod_{j=1}^{m_i} \\{\\mathrm{h}_i(y_{ij}) \\} \\exp \\left(- \\int_0^{\\infty} y_i(s) \\mathrm{h}_i(s) ds \\right) \\tag{14.5} \\end{equation}\\] donde \\(y_i(s)\\) es una variable binaria que indica si el \\(i\\)-ésimo sujeto es observado hasta el tiempo \\(s\\). Para parametrizar la función de intensidad \\(\\mathrm{h}_i(t)\\), asumimos que tenemos disponibles variables explicativas. Por ejemplo, al examinar reclamos de garantía de automóviles, podríamos disponer de información como la marca y el modelo del vehículo, o características del conductor como género y edad al momento de la compra. También podríamos tener características que sean funciones del tiempo del reclamo, como el número de millas recorridas. Por lo tanto, usamos la notación \\(\\mathbf{x}_i(t)\\) para denotar la posible dependencia de las variables explicativas en el tiempo del evento. Similar a la ecuación (14.3), es habitual escribir la función de intensidad como \\[ h_i(t) = h_0(t) \\exp( \\mathbf{x}_i^{\\prime}(t) \\boldsymbol \\beta ). \\] donde \\(h_0(t)\\) es la intensidad base. Para una especificación totalmente paramétrica, se especificaría la intensidad base en términos de varios parámetros. Luego, se usaría la función de intensidad \\(h_i(t)\\) en la ecuación de verosimilitud (14.5), que serviría como base para la inferencia por verosimilitud. Como se discute en Cook y Lawless (2007), también son posibles enfoques semiparamétricos donde la base no está completamente parametrizada (como en el modelo \\(PH\\)). 14.6 Lecturas Adicionales y Referencias Como se describe en Collett (1994), el estimador de límite de producto ha sido utilizado desde principios del siglo XX. Greenwood (1926) estableció la fórmula para una varianza aproximada. El trabajo de Kaplan y Meier (1958) suele asociarse con el estimador de límite de producto; demostraron que es un estimador de máxima verosimilitud no paramétrico de la función de supervivencia. Existen varias fuentes excelentes para el estudio adicional del análisis de supervivencia, particularmente en las ciencias biomédicas; Collett (1994) es una de estas fuentes. El texto de Klein y Moeschberger (1997) fue utilizado durante varios años como lectura obligatoria para el temario de la Sociedad de Actuarios de Norteamérica. Lawless (2003) proporciona una introducción desde una perspectiva de ingeniería. Lancaster (1990) discute temas econométricos. Hougaard (2000) ofrece un tratamiento avanzado. Recomendamos Cook y Lawless (2007) para un tratamiento extenso de eventos recurrentes. Referencias del Capítulo Breslow, Norman (1974). Covariance analysis of censored survival data. Biometrics 30, 89-99. Collett, D. (1994). Modelling Survival Data in Medical Research. Chapman &amp; Hall, London. Cook, Richard J. and Jerald F. Lawless (2007). The Statistical Analysis of Recurrent Events. Springer-Verlag, New York. Cox, David R. (1972). Regression models and life-tables. Journal of the Royal Statistical Society, Series B 34, 187-202. Gourieroux, Christian and Joann Jasiak (2007). The Econometrics of Individual Risk. Princeton University Press, Princeton, New Jersey. Greenwood, M. (1926). The errors of sampling of the survivorship tables. Reports on Public Health and Statistical Subjects, number 33, Appendix, HMSO, London. Kaplan, E. L. and Meier, P. (1958). Nonparametric estimation from incomplete observations. Journal of the American Statistical Association 53, 457-481. Kim, Yong-Duck, Dan R. Anderson, Terry L. Amburgey and James C. Hickman (1995). The use of event history analysis to examine insurance insolvencies. Journal of Risk and Insurance 62, 94-110. Klein, John P. and Melvin L. Moeschberger (1997). Survival Analysis: Techniques for Censored and Truncated Data. Springer-Verlag, New York. Klugman, Stuart A, Harry H. Panjer and Gordon E. Willmot (2008). Loss Models: From Data to Decisions. John Wiley &amp; Sons, Hoboken, New Jersey. Lancaster, Tony (1990). The Econometric Analysis of Transition Data. Cambridge University Press, New York. Lawless, Jerald F. (2003). Statistical Models and Methods for Lifetime Data, Second Edition. John Wiley &amp; Sons, New York. Hougaard, Philip (2000). Analysis of Multivariate Survival Data. Springer-Verlag, New York. Miller, Rupert G. (1997). Survival Analysis. John Wiley &amp; Sons, New York. Shumway, Tyler (2001). Forecasting bankruptcy more accurately: A simple hazard model. Journal of Business 74, 101-124. Stepanova, Maria and Lyn Thomas (2002). Survival analysis methods for personal loan data. Operations Research 50(2), 277-290. Zhou, Mai (2001). Understanding the Cox regression models with time-changing covariates. American Statistician 55(2), 153-155. "],["C15Misc.html", "Capítulo 15 Temas Misceláneos de Regresión 15.1 Modelos Lineales Mixtos 15.2 Regresión Bayesiana 15.3 Estimación de Densidad y Suavizado de Diagramas de Dispersión 15.4 Modelos Aditivos Generalizados 15.5 Bootstrapping 15.6 Lecturas Adicionales y Referencias", " Capítulo 15 Temas Misceláneos de Regresión Vista Previa del Capítulo. Este capítulo proporciona un recorrido rápido por varios temas de regresión que un analista probablemente encontrará en diferentes contextos de regresión. El objetivo de este capítulo es introducir estos temas, proporcionar definiciones e ilustrar contextos en los que estos temas pueden aplicarse. 15.1 Modelos Lineales Mixtos Aunque los modelos lineales mixtos son una parte establecida de la metodología estadística, su uso no es tan generalizado como la regresión en aplicaciones actuariales y financieras. Por lo tanto, esta sección introduce este marco de modelado, comenzando con un caso especial ampliamente utilizado. Después de introducir el marco de modelado, esta sección describe la estimación de coeficientes de regresión y componentes de varianza. Comenzamos con el modelo de efectos aleatorios unidireccional, con la ecuación del modelo \\[\\begin{equation} y_{it} = \\mu + \\alpha_i + \\varepsilon_{it}, ~~~~~ t=1, \\ldots, T_i, ~~ i=1,\\ldots, n. \\tag{15.1} \\end{equation}\\] Podemos usar este modelo para representar observaciones repetidas de un sujeto o grupo \\(i\\). El subíndice \\(t\\) se utiliza para denotar replicaciones que pueden ser en el tiempo o en múltiples membresías de grupo (como varios empleados en una empresa). Las observaciones repetidas en el tiempo fueron el enfoque del Capítulo 10. Cuando hay solo una observación por grupo, de modo que \\(T_i=1\\), el término de perturbación representa la información no observable sobre la variable dependiente. Con observaciones repetidas, tenemos la oportunidad de capturar características no observables del grupo a través del término \\(\\alpha_i\\). Aquí, se asume que \\(\\alpha_i\\) es una variable aleatoria y se conoce como un efecto aleatorio. Otro enfoque, introducido en la Sección 4.3, representaba \\(\\alpha_i\\) como un parámetro a estimar utilizando una variable explicativa categórica. Para este modelo, \\(\\mu\\) representa una media general, \\(\\alpha_i\\) la desviación de la media debido a características no observadas del grupo y \\(\\varepsilon_{it}\\) la variación de respuesta individual. Asumimos que \\(\\{\\alpha_i\\}\\) son i.i.d. con media cero y varianza \\(\\sigma^2_{\\alpha}\\). Además, asumimos que \\(\\{\\varepsilon_{it}\\}\\) son i.i.d. con media cero y varianza \\(\\sigma^2\\) y son independientes de \\(\\alpha_i\\). Una extensión de la ecuación (15.1) es el modelo básico de efectos aleatorios descrito en la Sección 10.5, basado en la ecuación del modelo \\[\\begin{equation}\\label{E15:BasicRE} y_{it} =\\alpha_i + \\mathbf{x}_{it}^{\\prime} \\boldsymbol \\beta + \\varepsilon_{it}. \\tag{15.2} \\end{equation}\\] En esta extensión, la media general \\(\\mu\\) se reemplaza por la función de regresión \\(\\mathbf{x}_{it}^{\\prime} \\boldsymbol \\beta\\). Este modelo incluye efectos aleatorios (\\(\\alpha_i\\)) así como efectos fijos (\\(\\mathbf{x}_{it}\\)). Los modelos mixtos son aquellos que incluyen efectos aleatorios y fijos. Apilando las ecuaciones del modelo de manera apropiada, se obtiene una expresión para el modelo lineal mixto: \\[\\begin{equation} \\mathbf{y} = \\mathbf{Z} \\boldsymbol \\alpha + \\mathbf{X} \\boldsymbol \\beta +\\boldsymbol \\varepsilon . \\tag{15.3} \\end{equation}\\] Aquí, \\(\\mathbf{y}\\) es un vector \\(N \\times 1\\) de variables dependientes, \\(\\boldsymbol \\varepsilon\\) es un vector \\(N \\times 1\\) de errores, Z y X son matrices conocidas \\(N \\times q\\) y \\(N \\times k\\) de variables explicativas, respectivamente, y \\(\\boldsymbol \\alpha\\) y \\(\\boldsymbol \\beta\\) son vectores desconocidos \\(q \\times 1\\) y \\(k \\times 1\\) de parámetros. En el modelo lineal mixto, los parámetros \\(\\boldsymbol \\beta\\) son fijos (no estocásticos) y los parámetros \\(\\boldsymbol \\alpha\\) son aleatorios (estocásticos). Para la estructura de la media, asumimos E(\\(\\mathbf{y}|\\boldsymbol \\alpha) = \\mathbf{Z} \\boldsymbol \\alpha + \\mathbf{X} \\boldsymbol \\beta\\) y E \\(\\boldsymbol \\alpha = \\mathbf{0}\\), de modo que \\(\\mathrm{E}~\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta\\). Para la estructura de covarianza, asumimos Var(\\(\\mathbf{y}|\\boldsymbol \\alpha) = \\mathbf{R}\\), Var(\\(\\boldsymbol \\alpha)= \\mathbf{D}\\) y Cov(\\(\\boldsymbol \\alpha,\\boldsymbol \\varepsilon ^{\\prime} )= \\mathbf{0}\\). Esto da como resultado Var \\(\\mathbf{y} = \\mathbf{Z D Z}^{\\prime} + \\mathbf{R = V}\\). En aplicaciones longitudinales, la matriz \\(\\mathbf{R}\\) se utiliza para modelar la correlación serial intra-sujeto. El modelo lineal mixto es bastante general e incluye muchos modelos como casos especiales. Para un tratamiento en profundidad de los modelos lineales mixtos, ver Pinheiro y Bates (2000). Para ilustrar, volvemos al modelo básico de efectos aleatorios en la ecuación (15.2). Apilando las replicaciones del grupo \\(i\\)-ésimo, podemos escribir \\[ \\mathbf{y}_i = \\alpha_i \\mathbf{1}_i + \\mathbf{X}_i \\boldsymbol \\beta +\\boldsymbol \\varepsilon_i, \\] donde \\(\\mathbf{y}_i = (y_{i1} , \\ldots, y_{iT_i})^{\\prime}\\) es el vector de variables dependientes, \\(\\boldsymbol \\varepsilon_i = ( \\varepsilon_{i1} , \\ldots, \\varepsilon_{iT_i})^{\\prime}\\) es el vector correspondiente de términos de perturbación, \\(\\mathbf{X}_i = (\\mathbf{x}_{i1} , \\ldots, \\mathbf{x}_{iT_i})^{\\prime}\\) es la matriz \\(T_i \\times k\\) de variables explicativas y \\(\\mathbf{1}_i\\) es un vector \\(T_i \\times 1\\) de unos. Apilando los grupos \\(i=1, \\ldots, n\\), se obtiene la ecuación (15.3) con \\(\\mathbf{y} = (\\mathbf{y}_1^{\\prime} , \\ldots, \\mathbf{y}_n^{\\prime})^{\\prime}\\), \\(\\boldsymbol \\varepsilon = (\\boldsymbol \\varepsilon_1 ^{\\prime}, \\ldots, \\boldsymbol \\varepsilon_n^{\\prime})^{\\prime}\\), \\(\\boldsymbol \\alpha = ( \\alpha _1 , \\ldots, \\alpha _n)^{\\prime}\\), \\[ \\mathbf{X}= \\left( \\begin{array}{c} \\mathbf{X}_1 \\\\ \\vdots \\\\ \\mathbf{X}_n \\\\ \\end{array} \\right) ~~~~\\mathrm{y}~~~~ \\mathbf{Z}= \\left( \\begin{array}{cccc} \\mathbf{1}_1 &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0}\\\\ \\mathbf{0} &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0} &amp; \\mathbf{0}&amp; \\cdots &amp; \\mathbf{1}_n\\\\ \\end{array} \\right) . \\] La estimación del modelo lineal mixto se realiza en dos etapas. En la primera etapa, estimamos los coeficientes de regresión \\(\\boldsymbol \\beta\\), asumiendo conocimiento de la matriz de varianza-covarianza \\(\\mathbf V\\). Luego, en la segunda etapa, se estiman los componentes de la matriz de varianza-covarianza \\(\\mathbf V\\). 15.1.1 Mínimos Cuadrados Ponderados En la Sección 5.7.3, introdujimos la noción de estimaciones de mínimos cuadrados ponderados de los coeficientes de regresión de la forma \\[\\begin{equation} \\mathbf{b}_{WLS} = \\left(\\mathbf{X}^{\\prime} \\mathbf{W}\\mathbf{X} \\right)^{-1}\\mathbf{X}^{\\prime} \\mathbf{W}\\mathbf{y} . \\tag{15.4} \\end{equation}\\] La matriz \\(n \\times n\\) \\(\\mathbf{W}\\) se eligió de la forma \\(\\mathbf{W} = diag(w_i)\\), de modo que el elemento diagonal \\(i\\)-ésimo de \\(\\mathbf{W}\\) es un peso \\(w_i\\). Como se introdujo en la Sección 5.7.3, esto nos permitió ajustar modelos de regresión heterocedásticos. Más generalmente, podemos permitir que \\(\\mathbf{W}\\) sea cualquier matriz (simétrica) (tal que \\(\\mathbf{X}^{\\prime} \\mathbf{W}\\mathbf{X}\\) sea invertible). Esta extensión nos permite acomodar otros tipos de dependencias que aparecen, por ejemplo, en modelos lineales mixtos. Asumiendo únicamente que E \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta\\) y Var \\(\\mathbf{y} = \\mathbf{V}\\), es fácil establecer \\[\\begin{equation} \\mathrm{E}~\\mathbf{b}_{WLS} = \\boldsymbol \\beta \\tag{15.5} \\end{equation}\\] y \\[\\begin{equation} \\mathrm{Var}~\\mathbf{b}_{WLS} = \\left(\\mathbf{X}^{\\prime} \\mathbf{W}\\mathbf{X} \\right)^{-1} \\left(\\mathbf{X}^{\\prime} \\mathbf{W}\\mathbf{V}\\mathbf{W}\\mathbf{X} \\right) \\left(\\mathbf{X}^{\\prime} \\mathbf{W}\\mathbf{X} \\right)^{-1} . \\tag{15.6} \\end{equation}\\] La ecuación (15.5) indica que \\(\\mathbf{b}_{WLS}\\) es un estimador insesgado de \\(\\boldsymbol \\beta\\). La ecuación (15.6) es un resultado básico que se utiliza para la inferencia estadística, incluyendo la evaluación de errores estándar. La mejor elección de la matriz de pesos es el inverso de la matriz de varianza-covarianza, de modo que \\(\\mathbf{W}=\\mathbf{V}^{-1}\\). Esta elección resulta en el estimador de mínimos cuadrados generalizados, comúnmente denotado por el acrónimo \\(GLS\\). La varianza es \\[\\begin{equation} \\mathrm{Var}~\\mathbf{b}_{GLS} = \\left(\\mathbf{X}^{\\prime} \\mathbf{V}^{-1}\\mathbf{X} \\right)^{-1} . \\tag{15.7} \\end{equation}\\] Esto es óptimo en el sentido de que se puede demostrar que \\(\\mathbf{b}_{GLS} = \\left(\\mathbf{X}^{\\prime} \\mathbf{V}^{-1}\\mathbf{X} \\right)^{-1}\\mathbf{X}^{\\prime} \\mathbf{V}^{-1}\\mathbf{y}\\) tiene varianza mínima entre la clase de todos los estimadores insesgados del vector de parámetros \\(\\boldsymbol \\beta\\). Esta propiedad se conoce como el \\(Teorema~de~Gauss-Markov\\), una extensión para matrices de varianza-covarianza generales \\(\\mathbf{V}\\) de la propiedad introducida en la Sección 3.2.3. 15.1.2 Estimación de Componentes de Varianza La estimación por mínimos cuadrados generalizados asume que \\(\\mathbf{V}\\) es conocida, al menos hasta una constante escalar. Por supuesto, es poco probable que una matriz general \\(n \\times n\\) \\(\\mathbf{V}\\) pueda ser estimada a partir de \\(n\\) observaciones. Sin embargo, es posible y rutinario estimar casos especiales de \\(\\mathbf{V}\\). Sea \\(\\boldsymbol \\tau\\) el vector de parámetros que indexan \\(\\mathbf{V}\\); una vez que \\(\\boldsymbol \\tau\\) es conocido, la matriz \\(\\mathbf{V}\\) está completamente especificada. Llamamos componentes de varianza a los elementos de \\(\\boldsymbol \\tau\\). Por ejemplo, en nuestro caso básico de regresión, tenemos \\(\\mathbf{V} = \\sigma^2 \\mathbf{I}\\), por lo que \\(\\boldsymbol \\tau = \\sigma^2\\). Como otro ejemplo, en el modelo básico de efectos aleatorios unidireccionales, la estructura de varianza está descrita por los componentes de varianza \\(\\boldsymbol \\tau = (\\sigma^2, \\sigma^2_{\\alpha})^{\\prime}\\). Existen varios métodos para estimar componentes de varianza, algunos basados en verosimilitud y otros que usan el método de momentos. Estos métodos están disponibles en software estadístico. Para dar a los lectores una idea de los cálculos involucrados, esbozamos brevemente el procedimiento basado en máxima verosimilitud utilizando distribuciones normales. Para observaciones \\(\\mathbf{y}\\) distribuidas normalmente con media E \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta\\) y Var \\(\\mathbf{y} = \\mathbf{V} = \\mathbf{V (\\boldsymbol \\tau)}\\), la verosimilitud logarítmica está dada por \\[\\begin{equation} L(\\boldsymbol \\beta, \\boldsymbol \\tau ) = - \\frac{1}{2} \\left[ N \\ln (2 \\pi) + \\ln \\det (\\mathbf{V (\\boldsymbol \\tau)}) + (\\mathbf{y} - \\mathbf{X} \\boldsymbol \\beta)^{\\prime} \\mathbf{V (\\boldsymbol \\tau)}^{-1} (\\mathbf{y} - \\mathbf{X} \\boldsymbol \\beta) \\right]. \\tag{15.8} \\end{equation}\\] Esta verosimilitud logarítmica debe maximizarse en términos de los parámetros \\(\\boldsymbol \\beta\\) y \\(\\boldsymbol \\tau\\). En la primera etapa, mantenemos \\(\\boldsymbol \\tau\\) fijo y maximizamos la ecuación (15.8) sobre \\(\\boldsymbol \\beta\\). Cálculos agradables muestran que \\(\\mathbf{b}_{GLS}\\) es, de hecho, el estimador de máxima verosimilitud de \\(\\boldsymbol \\beta\\). Insertando esto en la ecuación (15.8), obtenemos la verosimilitud “perfilada” \\[\\begin{equation} \\small{ L_P(\\boldsymbol \\tau ) = L(\\mathbf{b}_{GLS}, \\boldsymbol \\tau ) \\propto - \\frac{1}{2}\\left[ \\ln \\det (\\mathbf{V (\\boldsymbol \\tau)}) + (\\mathbf{y} - \\mathbf{X} \\mathbf{b}_{GLS})^{\\prime} \\mathbf{V (\\boldsymbol \\tau)}^{-1} (\\mathbf{y} - \\mathbf{X} \\mathbf{b}_{GLS}) \\right], } \\tag{15.9} \\end{equation}\\] donde hemos eliminado las constantes que no dependen de \\(\\boldsymbol \\tau\\). (El símbolo \\(\\propto\\) significa “es proporcional a.”) Para implementar este procedimiento de dos etapas, el software informático generalmente utiliza estimaciones de mínimos cuadrados ordinarios (OLS) b como valores iniciales. Luego, en la segunda etapa, se determinan las estimaciones de \\(\\boldsymbol \\tau\\) mediante métodos iterativos (optimización numérica) buscando los valores de \\(\\boldsymbol \\tau\\) que maximizan \\(L(\\mathbf{b},\\boldsymbol \\tau)\\). Estas estimaciones se utilizan para actualizar las estimaciones de los coeficientes de regresión utilizando mínimos cuadrados ponderados. Este proceso continúa hasta alcanzar la convergencia. Hay dos ventajas en este procedimiento de dos etapas. Primero, al desacoplar la regresión de la estimación de los parámetros de los componentes de varianza, podemos aplicar cualquier método que deseemos a los componentes de varianza y luego “insertar” estas estimaciones en el componente de regresión (estimación de mínimos cuadrados generalizados estimados). Segundo, tenemos una expresión en forma cerrada para las estimaciones de la regresión. Esto es más rápido computacionalmente que los métodos iterativos requeridos por las rutinas generales de optimización. 15.1.3 Mejor Predicción Lineal Insesgada Esta sección desarrolla las mejores predicciones lineales insesgadas (BLUPs, por sus siglas en inglés) en el contexto de modelos lineales mixtos. Introducimos BLUPs como el predictor de error cuadrático medio mínimo de una variable aleatoria, w. Este desarrollo es originalmente debido a Goldberger (1962), quien acuñó el término “mejor predictor lineal insesgado”. El acrónimo BLUP fue utilizado por primera vez por Henderson (1973). El objetivo genérico es predecir una variable aleatoria w, tal que \\(\\mathrm{E}~ w = \\boldsymbol \\lambda ^{\\prime} \\boldsymbol \\beta\\) y \\(\\mathrm{Var}~ w = \\sigma^2_w\\). Denotamos la covarianza entre \\(w\\) y \\(\\mathbf{y}\\) como el vector \\(1 \\times N\\) \\(\\mathrm{Cov}(w,\\mathbf{y}) = \\mathrm{E}\\{(w-\\mathrm{E}w)(\\mathbf{y}-\\mathrm{E}\\mathbf{y})^{\\prime} \\}\\). La elección de \\(w\\), y por ende \\(\\boldsymbol \\lambda\\) y \\(\\sigma^2_w\\), dependerá de la aplicación. Bajo estas suposiciones, se puede demostrar que el \\(BLUP\\) de \\(w\\) es \\[\\begin{equation} w_{BLUP} = \\boldsymbol \\lambda ^{\\prime} \\mathbf{b}_{GLS} + \\mathrm{Cov}(w,\\mathbf{y})\\mathbf{V}^{-1}(\\mathbf{y}-\\mathbf{X} \\mathbf{b}_{GLS}). \\tag{15.10} \\end{equation}\\] Los predictores \\(BLUP\\) son óptimos, asumiendo que los componentes de varianza implícitos en \\(\\mathbf{V}\\) y \\(\\mathrm{Cov}(w,\\mathbf{y})\\) son conocidos. Las aplicaciones de BLUP típicamente requieren que los componentes de varianza sean estimados, como se describe en la Sección 15.1.2. Los BLUPs con componentes de varianza estimados se conocen como BLUPs empíricos o EBLUPs. Hay tres tipos importantes de elección para \\(w\\): \\(w=\\varepsilon\\), lo que resulta en los denominados “residuos \\(BLUP\\)”, efectos aleatorios, como \\(\\boldsymbol \\alpha\\), y observaciones futuras, resultando en pronósticos óptimos. Para la primera elección, encontrará que los residuos \\(BLUP\\) están regularmente codificados en los paquetes de software estadístico que ajustan modelos lineales mixtos. Para la segunda elección, al dejar que \\(w\\) sea una combinación lineal arbitraria de efectos aleatorios, se puede demostrar que el predictor \\(BLUP\\) de \\(\\boldsymbol \\alpha\\) es \\[\\begin{equation} \\mathbf{a}_{BLUP} = \\mathbf{D Z}^{\\prime} \\mathbf{V}^{-1} (\\mathbf{y} - \\mathbf{X b}_{GLS}). \\tag{15.11} \\end{equation}\\] Para ejemplos de la tercera elección, pronósticos con modelos lineales mixtos, nos referimos a Frees (2004, Capítulos 4 y 8). Para considerar una aplicación de la ecuación (15.11), considere lo siguiente. Caso especial: Modelo de Efectos Aleatorios Unidireccionales. Considere el modelo basado en la ecuación (15.1) y suponga que deseamos estimar la media condicional del grupo \\(i\\), \\(w=\\mu + \\alpha_i.\\) Luego, cálculos directos (ver Frees, 2004, Capítulo 4) basados en la ecuación (15.11) muestran que el \\(BLUP\\) es \\[\\begin{equation} \\zeta_i \\bar{y}_i + (1-\\zeta_i ) m_{\\alpha,GLS} , \\tag{15.12} \\end{equation}\\] con peso \\(\\zeta_i = T_i /(T_i + \\sigma^2/\\sigma^2_{\\alpha})\\) y la estimación \\(GLS\\) de \\(\\mu\\), \\(m_{\\alpha,GLS} = \\sum_i \\zeta_i \\bar{y}_i / \\sum_i \\zeta_i\\). En el Capítulo 18, interpretaremos \\(\\zeta_i\\) como un factor de credibilidad. 15.2 Regresión Bayesiana En los modelos estadísticos bayesianos, se considera que tanto los parámetros del modelo como los datos son variables aleatorias. En esta sección, usamos un tipo específico de modelo bayesiano, el modelo jerárquico lineal normal discutido, por ejemplo, por Gelman et al. (2004). Como en el esquema de muestreo en dos etapas descrito en la Sección 3.3.1, el modelo jerárquico lineal se especifica en etapas. Específicamente, consideramos la siguiente jerarquía de dos niveles: Dado los parámetros \\(\\boldsymbol \\alpha\\) y \\(\\boldsymbol \\beta\\), el modelo de respuesta es \\(\\mathbf{y} = \\mathbf{Z}\\boldsymbol \\alpha + \\mathbf{X}\\boldsymbol \\beta + \\boldsymbol \\varepsilon\\). Este nivel es un modelo lineal ordinario (fijo) que se introdujo en los Capítulos 3 y 4. Específicamente, asumimos que el vector de respuestas \\(\\mathbf{y}\\) condicionado a \\(\\boldsymbol \\alpha\\) y \\(\\boldsymbol \\beta\\) está normalmente distribuido y que E (\\(\\mathbf{y} | \\boldsymbol \\alpha, \\boldsymbol \\beta ) = \\mathbf{Z}\\boldsymbol \\alpha + \\mathbf{X}\\boldsymbol \\beta\\) y Var (\\(\\mathbf{y} | \\boldsymbol \\alpha, \\boldsymbol \\beta) = \\mathbf{R}\\). Asumimos que \\(\\boldsymbol \\alpha\\) está distribuido normalmente con media \\(\\boldsymbol{\\mu _{\\alpha}}\\) y varianza \\(\\mathbf{D}\\) y que \\(\\boldsymbol \\beta\\) está distribuido normalmente con media \\(\\boldsymbol{\\mu _{\\beta}}\\) y varianza \\(\\boldsymbol{\\Sigma _{\\beta}}\\), cada uno independiente del otro. Las diferencias técnicas entre el modelo lineal mixto y el modelo jerárquico lineal normal son: En el modelo lineal mixto, \\(\\boldsymbol \\beta\\) es un parámetro fijo desconocido, mientras que en el modelo jerárquico lineal normal, \\(\\boldsymbol \\beta\\) es un vector aleatorio, y el modelo lineal mixto no hace suposiciones de distribución, mientras que en el modelo jerárquico lineal normal se hacen suposiciones de distribución en cada etapa. Además, hay diferencias importantes en la interpretación. Para ilustrar, suponga que \\(\\boldsymbol \\beta= \\mathbf{0}\\) con probabilidad uno. En la interpretación clásica no bayesiana, también conocida como frecuentista, pensamos en la distribución de \\(\\{\\boldsymbol \\alpha\\}\\) como representativa de la probabilidad de obtener una realización de \\(\\boldsymbol \\alpha _i\\). Esta interpretación es adecuada cuando tenemos una población de empresas o personas, y cada realización es un muestreo de esa población. En contraste, en el caso bayesiano, se interpreta la distribución de \\(\\{\\boldsymbol \\alpha\\}\\) como representativa del conocimiento que se tiene de este parámetro. Esta distribución puede ser subjetiva y permite al analista inyectar formalmente sus evaluaciones en el modelo. En este sentido, la interpretación frecuentista puede considerarse un caso especial del marco bayesiano. La distribución conjunta de \\((\\boldsymbol \\alpha^{\\prime}, \\boldsymbol \\beta^{\\prime})^{\\prime}\\) se conoce como la distribución a priori. Para resumir, la distribución conjunta de \\((\\boldsymbol \\alpha^{\\prime}, \\boldsymbol \\beta^{\\prime}, \\mathbf{y}^{\\prime})^{\\prime}\\) es \\[\\begin{equation} \\left( \\begin{array}{c} \\boldsymbol \\alpha \\\\ \\boldsymbol \\beta \\\\ \\mathbf{y} \\\\ \\end{array} \\right) \\sim N \\left( \\left( \\begin{array}{c} \\boldsymbol {\\mu_{\\alpha}} \\\\ \\boldsymbol {\\mu_{\\beta}} \\\\ \\mathbf{Z}\\boldsymbol {\\mu_{\\alpha}} + \\mathbf{X}\\boldsymbol {\\mu_{\\beta}} \\\\ \\end{array} \\right) , \\left(\\begin{array}{ccc} \\mathbf{D} &amp; \\mathbf{0} &amp; \\mathbf{DZ}^{\\prime} \\\\ \\mathbf{0} &amp; \\boldsymbol {\\Sigma_{\\beta}} &amp; \\boldsymbol {\\Sigma_{\\beta}}\\mathbf{X}^{\\prime} \\\\ \\mathbf{ZD} &amp; \\mathbf{X}\\boldsymbol {\\Sigma_{\\beta}} &amp; \\mathbf{V} + \\mathbf{X}\\boldsymbol {\\Sigma_{\\beta}} \\mathbf{X}^{\\prime}\\\\ \\end{array} \\right) \\right) , \\tag{15.13} \\end{equation}\\] donde \\(\\mathbf{V = R + Z D }\\mathbf{Z}^{\\prime}\\). La distribución de los parámetros dados los datos se conoce como la distribución posterior. Para calcular esta distribución condicional, utilizamos resultados estándar del análisis multivariado. Específicamente, la distribución posterior de \\((\\boldsymbol \\alpha^{\\prime}, \\boldsymbol \\beta^{\\prime})^{\\prime}\\) dado \\(\\mathbf{y}\\) es normal. No es difícil verificar que la media condicional es \\[\\begin{equation} \\mathrm{E}~ \\left( \\begin{array}{c} \\boldsymbol \\alpha \\\\ \\boldsymbol \\beta \\\\ \\end{array} \\right) | \\mathbf{y} = \\left( \\begin{array}{c} \\boldsymbol {\\mu_{\\alpha}} + \\mathbf{DZ}^{\\prime} (\\mathbf{V} + \\mathbf{X}\\boldsymbol {\\Sigma_{\\beta}} \\mathbf{X}^{\\prime})^{-1} (\\mathbf{y} -\\mathbf{Z}\\boldsymbol {\\mu_{\\alpha}} - \\mathbf{X}\\boldsymbol {\\mu_{\\beta}}) \\\\ \\boldsymbol {\\mu_{\\beta}} + \\boldsymbol {\\Sigma_{\\beta}}\\mathbf{X}^{\\prime} (\\mathbf{V} + \\mathbf{X}\\boldsymbol {\\Sigma_{\\beta}} \\mathbf{X}^{\\prime})^{-1} (\\mathbf{y} -\\mathbf{Z}\\boldsymbol {\\mu_{\\alpha}} - \\mathbf{X}\\boldsymbol {\\mu_{\\beta}}) \\\\ \\end{array} \\right) . \\tag{15.14} \\end{equation}\\] Hasta este punto, el tratamiento de los parámetros \\(\\boldsymbol \\alpha\\) y \\(\\boldsymbol \\beta\\) ha sido simétrico. En algunas aplicaciones, como con datos longitudinales, generalmente se tiene más información sobre los parámetros globales \\(\\boldsymbol \\beta\\) que sobre los parámetros específicos del sujeto \\(\\boldsymbol \\alpha\\). Para ver cómo cambia la distribución posterior dependiendo de la cantidad de información disponible, consideramos dos casos extremos. Primero, considere el caso \\(\\boldsymbol{\\Sigma _{\\beta}}= \\mathbf{0}\\), de modo que \\(\\boldsymbol \\beta=\\boldsymbol{\\mu _{\\beta}}\\) con probabilidad uno. Intuitivamente, esto significa que \\(\\boldsymbol \\beta\\) es conocido con precisión, generalmente a partir de información colateral. Luego, a partir de la ecuación (15.14), tenemos \\[\\begin{equation} \\mathrm{E}~ ( \\boldsymbol \\alpha | \\mathbf{y}) = \\boldsymbol {\\mu_{\\alpha}} + \\mathbf{DZ}^{\\prime} \\mathbf{V}^{-1} (\\mathbf{y} -\\mathbf{Z}\\boldsymbol {\\mu_{\\alpha}} - \\mathbf{X}\\boldsymbol \\beta) . \\tag{15.15} \\end{equation}\\] Asumiendo que \\(\\boldsymbol{\\mu _{\\alpha}}= \\mathbf{0}\\), el mejor estimador lineal insesgado de E (\\(\\boldsymbol \\alpha | \\mathbf{y}\\)) es \\[ \\mathbf{a}_{BLUP} = \\mathbf{D Z}^{\\prime} \\mathbf{V}^{-1} (\\mathbf{y} - \\mathbf{X b}_{GLS}). \\] Recuerde de la ecuación (15.11) que \\(\\mathbf{a}_{BLUP}\\) es también el mejor predictor lineal insesgado en el marco de modelo frecuentista (no bayesiano). En segundo lugar, considere el caso en que \\(\\boldsymbol{\\Sigma _{\\beta}}^{-1}= \\mathbf{0}\\). En este caso, la información previa sobre el parámetro \\(\\boldsymbol \\beta\\) es vaga; esto se conoce como usar una priori difusa. En este caso, se puede verificar que \\[ \\mathrm{E}~ (\\boldsymbol \\alpha | \\mathbf{y}) \\rightarrow \\mathbf{a}_{BLUP}, \\] a medida que \\(\\boldsymbol{\\Sigma _{\\beta}}^{-1}\\rightarrow \\mathbf{0}\\). (Ver, por ejemplo, Frees, 2004, Sección 4.6.) Así, es interesante que en ambos casos extremos, llegamos a la estadística \\(\\mathbf{a}_{BLUP}\\) como un predictor de \\(\\boldsymbol \\alpha\\). Este análisis asume que \\(\\mathbf{D}\\) y \\(\\mathbf{R}\\) son matrices de parámetros fijos. También es posible asumir distribuciones para estos parámetros; típicamente, se utilizan distribuciones de Wishart independientes para \\(\\mathbf{D}^{-1}\\) y \\(\\mathbf{R}^{-1}\\), ya que son prioris conjugadas. Alternativamente, se pueden estimar \\(\\mathbf{D}\\) y \\(\\mathbf{R}\\) usando los métodos descritos en la Sección 15.1. La estrategia general de sustituir estimaciones puntuales por ciertos parámetros en una distribución posterior se llama estimación bayesiana empírica. Para examinar casos intermedios, consideramos el siguiente caso especial. Generalizaciones pueden encontrarse en Luo, Young y Frees (2001). Caso Especial: Modelo de Efectos Aleatorios Unidireccionales. Retomamos el modelo considerado en la ecuación (15.2) y, para simplificar, asumimos datos balanceados de modo que \\(T_i = T\\). El objetivo es determinar las distribuciones posteriores de los parámetros. Con fines ilustrativos, nos enfocamos en las medias posteriores. Así, reescribiendo la ecuación (15.2), el modelo es \\[ y_{it} = \\beta + \\alpha_i + \\varepsilon_{it}, \\] donde usamos \\(\\beta \\sim N(\\mu_{\\beta}, \\sigma^2_{\\beta})\\) en lugar de la media fija \\(\\mu\\). La distribución previa de \\(\\alpha_i\\) es independiente con \\(\\alpha_i \\sim N(0, \\sigma^2_{\\alpha})\\). Usando la ecuación (15.14), obtenemos la media posterior de \\(\\beta\\), \\[\\begin{equation} \\hat{\\beta} = \\mathrm{E}~(\\beta| \\mathbf{y}) = \\left( \\frac{1}{\\sigma^2_{\\beta}}+ \\frac{nT}{\\sigma^2_{\\varepsilon}+T\\sigma^2_{\\alpha}} \\right)^{-1} \\left( \\frac{nT}{\\sigma^2_{\\varepsilon}+T\\sigma^2_{\\alpha}} \\bar{y} + \\frac{\\mu}{\\sigma^2_{\\beta}} \\right) , \\tag{15.16} \\end{equation}\\] después de algo de álgebra. Así, \\(\\hat{\\beta}\\) es un promedio ponderado de la media muestral, \\(\\bar{y}\\), y la media previa, \\(\\mu_{\\beta}\\). Es fácil ver que \\(\\hat{\\beta}\\) se aproxima a la media muestral a medida que \\(\\sigma^2_{\\beta} \\rightarrow \\infty\\), es decir, cuando la información previa sobre \\(\\beta\\) se vuelve “vaga”. Por el contrario, \\(\\hat{\\beta}\\) se aproxima a la media previa \\(\\mu_{\\beta}\\) a medida que \\(\\sigma^2_{\\beta} \\rightarrow 0\\), es decir, cuando la información previa se vuelve “precisa”. De manera similar, utilizando la ecuación (15.14), la media posterior de \\(\\alpha_i\\) es \\[ \\hat{\\alpha_i} = \\mathrm{E}~(\\alpha_i | \\mathbf{y}) = \\zeta \\left[ ( \\bar{y}_i - \\mu_{\\beta} ) - \\zeta_{\\beta} (\\bar{y} - \\mu_{\\beta} ) \\right], \\] donde tenemos que \\[ \\zeta = \\frac{T \\sigma^2_{\\alpha}}{\\sigma^2_{\\varepsilon}+T \\sigma^2_{\\alpha}} \\] y definimos \\[ \\zeta_{\\beta} = \\frac{nT \\sigma^2_{\\beta}}{\\sigma^2_{\\varepsilon}+T \\sigma^2_{\\alpha}+nT \\sigma^2_{\\beta}}. \\] Nótese que \\(\\zeta_{\\beta}\\) mide la precisión del conocimiento sobre \\(\\beta\\). Específicamente, vemos que \\(\\zeta_{\\beta}\\) se aproxima a uno cuando \\(\\sigma^2_{\\beta} \\rightarrow \\infty\\), y se aproxima a cero cuando \\(\\sigma^2_{\\beta} \\rightarrow 0\\). Combinando estos dos resultados, tenemos que \\[ \\hat{\\alpha_i} +\\hat{\\beta} = (1-\\zeta_{\\beta}) \\left[ (1-\\zeta) \\mu_{\\beta} + \\zeta \\bar{y}_i \\right] + \\zeta_{\\beta} \\left[ (1-\\zeta)\\bar{y} + \\zeta\\bar{y}_i \\right]. \\] Así, si nuestro conocimiento de la distribución de \\(\\beta\\) es vago, entonces \\(\\zeta_{\\beta} =1\\) y el predictor se reduce a la expresión en la ecuación (15.12) (para datos balanceados). Por el contrario, si nuestro conocimiento de la distribución de \\(\\beta\\) es preciso, entonces \\(\\zeta_{\\beta} =0\\) y el predictor se reduce a la expresión dada en el Capítulo 18. Con la formulación bayesiana, podemos considerar situaciones donde el conocimiento está disponible, aunque sea impreciso. En resumen, hay varias ventajas del enfoque bayesiano. Primero, se puede describir toda la distribución de los parámetros condicionales a los datos. Esto permite, por ejemplo, proporcionar declaraciones de probabilidad respecto a la verosimilitud de los parámetros. Segundo, este enfoque permite a los analistas combinar información conocida de otras fuentes con los datos de manera coherente. En nuestro desarrollo, asumimos que la información puede conocerse a través del vector de parámetros \\(\\boldsymbol \\beta\\), con su confiabilidad controlada mediante la matriz de dispersión \\(\\boldsymbol{\\Sigma_{\\beta}}\\). Valores de \\(\\boldsymbol{\\Sigma_{\\beta}}=\\mathbf{0}\\) indican completa confianza en los valores de \\(\\boldsymbol{\\mu_{\\beta}}\\), mientras que valores de \\(\\boldsymbol{\\Sigma_{\\beta}}^{-1}=\\mathbf{0}\\) indican completa dependencia de los datos en lugar de conocimiento previo. Tercero, el enfoque bayesiano proporciona un enfoque unificado para estimar \\((\\boldsymbol \\alpha, \\boldsymbol \\beta)\\). La Sección 15.1 sobre métodos no bayesianos requería una subsección separada sobre la estimación de componentes de varianza. En contraste, en métodos bayesianos, todos los parámetros pueden tratarse de manera similar. Esto es conveniente para explicar resultados a los consumidores del análisis de datos. Cuarto, el análisis bayesiano es particularmente útil para pronosticar respuestas futuras. 15.3 Estimación de Densidad y Suavizado de Diagramas de Dispersión Al explorar una variable o relaciones entre dos variables, a menudo se desea obtener una idea general de los patrones sin imponer relaciones funcionales estrictas. Generalmente, los procedimientos gráficos funcionan bien porque podemos comprender relaciones potencialmente no lineales más fácilmente de manera visual que con resúmenes numéricos. Esta sección introduce la (estimación de densidad con núcleo) para visualizar la distribución de una variable y el suavizado de diagramas de dispersión para visualizar la relación entre dos variables. Para obtener una impresión rápida de la distribución de una variable, un histograma es fácil de calcular e interpretar. Sin embargo, como se sugirió en el Capítulo 1, cambiar la ubicación y el tamaño de los rectángulos que comprenden el histograma puede dar diferentes impresiones de la distribución a los espectadores. Para introducir una alternativa, supongamos que tenemos una muestra aleatoria \\(y_1, \\ldots, y_n\\) de una función de densidad de probabilidad \\(f(.)\\). Definimos el estimador de densidad con núcleo como \\[ \\hat{\\mathrm{f}}(y) = \\frac{1}{n b_n} \\sum_{i=1}^n \\mathrm{k}\\left(\\frac{y-y_i}{b_n}\\right), \\] donde \\(b_n\\) es un número pequeño llamado ancho de banda y k(.) es una función de densidad de probabilidad llamada núcleo. Para desarrollar la intuición, primero consideramos el caso donde el núcleo k(.) es una función de densidad de probabilidad para una distribución uniforme en (-1,1). Para el núcleo uniforme, el estimador de densidad con núcleo cuenta el número de observaciones \\(y_i\\) que están dentro de \\(b_n\\) unidades de \\(y\\), y luego expresa la estimación de la densidad como el conteo dividido por el tamaño de la muestra multiplicado por el ancho del rectángulo (es decir, el conteo dividido por \\(n \\times 2 b_n\\)). De esta manera, puede considerarse un estimador de histograma “local” en el sentido de que el centro del histograma depende del argumento \\(y\\). Existen varias posibilidades para el núcleo. Algunas opciones ampliamente utilizadas son: el núcleo uniforme, \\(\\mathrm{k}(u) = \\frac{1}{2}\\) para \\(-1 \\leq u \\leq 1\\) y 0 en otro caso, el núcleo “Epanechikov”, \\(\\mathrm{k}(u) = \\frac{3}{4}(1-u^2)\\) para \\(-1 \\leq u \\leq 1\\) y 0 en otro caso, y el núcleo gaussiano, \\(\\mathrm{k}(u) = \\phi(u)\\) para \\(-\\infty &lt; u &lt; \\infty\\), la función de densidad normal estándar. El núcleo Epanechnikov es una versión más suave que utiliza un polinomio cuadrático para que no se usen rectángulos discontinuos. El núcleo gaussiano es aún más continuo en el sentido de que el dominio ya no es más o menos \\(b_n\\) sino toda la recta real. El ancho de banda \\(b_n\\) controla la cantidad de promedio. Para ver los efectos de diferentes elecciones de ancho de banda, consideremos un conjunto de datos sobre la utilización de hogares de ancianos que se introducirá en la Sección 17.3.2. Aquí, consideramos las tasas de ocupación, una medida de la utilización de hogares de ancianos. Un valor de 100 significa ocupación completa, aunque debido a la forma en que se construye esta medida, es posible que los valores superen 100. Específicamente, hay \\(n=349\\) tasas de ocupación que se muestran en la Figura 15.1. Ambas figuras utilizan un núcleo gaussiano. El panel izquierdo está basado en un ancho de banda de 0.1. Este panel parece muy irregular; el ancho de banda relativamente pequeño significa que se realiza poco promedio. Para los puntos atípicos, cada pico representa una sola observación. En contraste, el panel derecho está basado en un ancho de banda de 1.374. En comparación con el panel izquierdo, esta imagen muestra un panorama más suave, permitiendo al analista buscar patrones sin distraerse con bordes irregulares. Desde este panel, podemos ver fácilmente que la mayor parte de la masa está por debajo del 100 por ciento. Además, la distribución tiene sesgo hacia la izquierda, con valores entre 100 y 120 siendo raros. El ancho de banda 1.374 se seleccionó utilizando un procedimiento automático incorporado en el software. Estos procedimientos automáticos eligen el ancho de banda para encontrar el mejor equilibrio entre la precisión y la suavidad de las estimaciones. (Para esta figura, utilizamos el software estadístico “R” que tiene incorporado el procedimiento de Silverman). Figura 15.1: Estimaciones de Densidad con Núcleo de las Tasas de Ocupación de Hogares de Ancianos con Diferentes Anchos de Banda. El panel izquierdo está basado en un ancho de banda = 0.1, el panel derecho está basado en un ancho de banda = 1.374. Las estimaciones de densidad con núcleo también dependen de la elección del núcleo, aunque esto es típicamente mucho menos importante en las aplicaciones que la elección del ancho de banda. Para mostrar los efectos de diferentes núcleos, mostramos solo las \\(n=3\\) tasas de ocupación que excedieron 110 en la Figura 15.2. El panel izquierdo muestra la superposición de histogramas rectangulares basados en el núcleo uniforme. Los núcleos más suaves de Epanechnikov y gaussiano en los paneles del medio y derecho son visualmente indistinguibles. A menos que esté trabajando con tamaños de muestra muy pequeños, generalmente no necesitará preocuparse por la elección del núcleo. Algunos analistas prefieren el núcleo uniforme debido a su interpretabilidad, otros prefieren el gaussiano debido a su suavidad y otros prefieren el núcleo de Epanechnikov como un compromiso razonable. Figura 15.2: Estimaciones de Densidad con Núcleo de las Tasas de Ocupación de Hogares de Ancianos con Diferentes Núcleos. De izquierda a derecha, los paneles utilizan los núcleos uniforme, de Epanechnikov y gaussiano. Código R para generar las Figuras 15.1 y 15.2 # Figura 15.1 library(HH) # DATOS DE HOGARES DE ANCIANOS NurseDat &lt;- read.csv(&quot;CSVData/WiscNursingHome.csv&quot;, header=TRUE) #str(NurseDat) NurseDat01 &lt;- subset(NurseDat,CRYEAR==2001) NurseDat01 &lt;- subset(NurseDat01,SQRFOOT&gt;5) # Se eliminan 5 hogares sin metraje cuadrado NurseDat01$RATE &lt;- 100*NurseDat01$TPY/NurseDat01$NUMBED NurseDat01 &lt;- subset(NurseDat01,RATE&gt;50) # Se elimina un hogar con RATE = 40 par(mfrow=c(1,2)) plot(density(NurseDat01$RATE, bw=0.1), main=&quot;&quot;, xlab=&quot;Tasa de Ocupación&quot;) plot(density(NurseDat01$RATE), main=&quot;&quot;, xlab=&quot;Tasa de Ocupación&quot;)# Núcleo gaussiano # CÁLCULO AUTOMÁTICO DEL ANCHO DE BANDA DE SILVERMAN # temp =summary(RATE) # IQ = temp[5]-temp[2] # A = min(sd(RATE),IQ/1.34) # bw = .9*A*length(RATE)^(-.2) # bw ##[1] 1.37471 # Figura 15.2 NurseDat02 &lt;- subset(NurseDat01, RATE&gt;110) par(mfrow=c(1,3)) plot(density(NurseDat02$RATE,bw=.5,kernel=c(&quot;rectangular&quot;) ), xlim=c(105,125) , ylim=c(0,0.4), main=&quot;&quot;, xlab=&quot;Tasa de Ocupación&quot;) plot(density(NurseDat02$RATE,bw=.5,kernel=c(&quot;epanechnikov&quot;) ), xlim=c(105,125) , ylim=c(0,0.4), main=&quot;&quot;, xlab=&quot;Tasa de Ocupación&quot;) plot(density(NurseDat02$RATE,bw=.5,kernel=c(&quot;gaussian&quot;) ), xlim=c(105,125) , ylim=c(0,0.4), main=&quot;&quot;, xlab=&quot;Tasa de Ocupación&quot;) Algunos suavizadores de diagramas de dispersión que muestran relaciones entre un \\(x\\) y un \\(y\\), también pueden describirse en términos de estimación con núcleo. Específicamente, una estimación con núcleo de la función de regresión E(\\(y|x\\)) es \\[ \\hat{\\mathrm{m}}(x) = \\frac{\\sum_{i=1}^n w_{i,x} y_i}{\\sum_{i=1}^n w_{i,x}} \\] con el peso “local” \\(w_{i,x} = \\mathrm{k}\\left( (x_i - x)/b_n \\right)\\). Esta es la ahora clásica estimación de Nadaraya-Watson (ver, por ejemplo, Ruppert, Wand y Carroll, 2003). Más generalmente, para un ajuste de polinomio local de orden \\(p\\), consideremos encontrar estimaciones de parámetros \\(\\beta_0, \\ldots, \\beta_p\\) que minimicen \\[\\begin{equation} \\sum_{i=1}^n \\left\\{ y_i - \\beta_0 - \\cdots - \\beta_p (x_i - x)^p \\right\\}^2 w_{i,x}. \\tag{15.17} \\end{equation}\\] El mejor valor de la intersección \\(\\beta_0\\) se toma como la estimación de la función de regresión E(\\(y|x\\)). Ruppert, Wand y Carroll (2003) recomiendan valores de \\(p=1\\) o 2 para la mayoría de aplicaciones (la elección \\(p=0\\) produce la estimación de Nadaraya-Watson). Como una variación, al tomar \\(p=1\\) y permitir que el ancho de banda varíe para que el número de puntos utilizados para estimar la función de regresión sea fijo, se obtiene el estimador lowess (por “regresión local”) debido a Cleveland (ver, por ejemplo, Ruppert, Wand y Carroll, 2003). Como ejemplo, utilizamos el estimador lowess en la Figura 6.11 para tener una idea de la relación entre los residuos y el nivel de riesgo de una industria medida por INDCOST. Como analista, encontrará que los estimadores de densidad con núcleo y los suavizadores de diagramas de dispersión son bastante sencillos de usar al buscar patrones y desarrollar modelos. 15.4 Modelos Aditivos Generalizados Los modelos lineales clásicos están basados en la función de regresión \\[ \\mu = \\mathrm{E}(y| x_1, \\ldots, x_k) = \\beta_0 + \\sum_{j=1}^k \\beta_j x_j. \\] Con un modelo lineal generalizado (GLM), hemos visto que podemos extender sustancialmente las aplicaciones mediante una función que vincula la media con el componente sistemático, \\[ \\mathrm{g} \\left(\\mu \\right) = \\beta_0 + \\sum_{j=1}^k \\beta_j x_j, \\] desde la ecuación (13.1). Como en los modelos lineales, el componente sistemático es lineal en los parámetros \\(\\beta_j\\), no necesariamente en las variables explicativas subyacentes. Por ejemplo, hemos visto que podemos usar funciones polinómicas (como \\(x^2\\) en el Capítulo 3), funciones trigonométricas (como \\(\\sin x\\) en el Capítulo 8), variables binarias y categorizaciones en el Capítulo 4, y la representación de “palos rotos” (lineal por tramos) en la Sección 3.5.2. El modelo aditivo generalizado (GAM) extiende el GLM al permitir que cada variable explicativa sea reemplazada por una función que puede ser no lineal, \\[\\begin{equation} \\mathrm{g} \\left( \\mu \\right) = \\beta_0 + \\sum_{j=1}^k \\beta_j ~\\mathrm{m}_j(x_j). \\tag{15.18} \\end{equation}\\] Aquí, la función \\(\\mathrm{m}_j(\\cdot)\\) puede diferir según la variable explicativa. Dependiendo de la aplicación, \\(\\mathrm{m}_j(\\cdot)\\) puede incluir las especificaciones paramétricas tradicionales (como polinomios y categorizaciones) así como especificaciones no paramétricas más flexibles como los suavizadores de diagramas de dispersión introducidos en la Sección 15.3. Por ejemplo, supongamos que tenemos una gran base de datos de seguros y deseamos modelar la probabilidad de un siniestro. Entonces, podríamos usar el modelo \\[ \\ln \\left(\\frac{\\pi}{1-\\pi} \\right) = \\beta_0 + \\sum_{j=1}^k \\beta_j x_j + \\mathrm{m}(z). \\] El lado izquierdo es la usual función de enlace logit utilizada en regresión logística, con \\(\\pi\\) siendo la probabilidad de un siniestro. Para el lado derecho, podríamos considerar una serie de variables de calificación, como territorio, género y tipo de vehículo o casa (dependiendo de la cobertura), que están incluidas en el componente lineal \\(\\beta_0 + \\sum_{j=1}^k \\beta_j x_j\\). La variable adicional \\(z\\) es una variable continua (como la edad) para la que deseamos permitir la posibilidad de efectos no lineales. Para la función \\(\\mathrm{m}(z)\\), podríamos usar un ajuste polinómico de orden \\(p\\) con discontinuidades en varias edades, como en la ecuación (15.17). Este es conocido como un modelo semiparamétrico, en el que el componente sistemático consta de partes paramétricas (\\(\\beta_0 + \\sum_{j=1}^k \\beta_j x_j\\)) y no paramétricas (\\(\\mathrm{m}(z)\\)). Aunque no presentamos los detalles aquí, el software estadístico moderno permite la estimación simultánea de los parámetros de ambos componentes. Por ejemplo, el software estadístico SAS implementa modelos aditivos generalizados en su procedimiento PROC GAM, al igual que el software R mediante el paquete VGAM. La especificación del GAM en la ecuación (15.18) es bastante general. Para una clase más limitada, la elección de g(\\(\\cdot\\)) como la función identidad produce el modelo aditivo. Aunque general, las formas no paramétricas de \\(\\mathrm{m}_j(\\cdot)\\) hacen que el modelo sea más flexible, pero la aditividad nos permite interpretar el modelo de manera similar a antes. Los lectores interesados en más información sobre los GAM encontrarán útiles los textos de Ruppert, Wand y Carroll (2003) y Hastie, Tibshirani y Freedman (2001). 15.5 Bootstrapping El bootstrap es una herramienta general para evaluar la distribución de una estadística. Primero describimos el procedimiento general y luego discutimos formas de implementarlo en un contexto de regresión. Supongamos que tenemos una muestra i.i.d. \\(\\{z_1, \\ldots, z_n \\}\\) de una población. A partir de estos datos, deseamos entender la confiabilidad de una estadística \\(\\mathrm{S}(z_1, \\ldots, z_n )\\). Para calcular una distribución bootstrap, realizamos: Muestra Bootstrap. Generar una muestra i.i.d. de tamaño \\(n\\), \\(\\{z^{\\ast}_{1r}, \\ldots, z^{\\ast}_{nr} \\}\\), a partir de \\(\\{z_1, \\ldots, z_n \\}\\). Replicación Bootstrap. Calcular la replicación bootstrap, \\(S^{\\ast}_r =\\mathrm{S}(z^{\\ast}_{1r}, \\ldots, z^{\\ast}_{nr} )\\). Repetir los pasos (i) y (ii) \\(r=1, \\ldots, R\\) veces, donde \\(R\\) es un gran número de replicaciones. En el primer paso, la muestra bootstrap se extrae aleatoriamente de la muestra original con reemplazo. Al repetir los pasos (i) y (ii), las muestras bootstrap son independientes entre sí, condicionado a la muestra original \\(\\{z_1, \\ldots, z_n \\}\\). La distribución bootstrap resultante, \\(\\{S^{\\ast}_1, \\ldots, S^{\\ast}_R\\}\\), puede usarse para evaluar la distribución de la estadística \\(S\\). Existen tres variaciones de este procedimiento básico usadas en regresión. En la primera variación, tratamos \\(z_i = (y_i, \\mathbf{x}_i)\\), y usamos el procedimiento básico bootstrap. Esta variación se conoce como remuestreo de pares. En la segunda variación, tratamos los residuos de la regresión como la muestra “original” y creamos una muestra bootstrap remuestreando los residuos. Esta variación se conoce como remuestreo de residuos. Específicamente, consideremos un modelo de regresión genérico de la forma \\(y_i = \\mathrm{F}(\\mathbf{x}_i, \\boldsymbol \\theta, \\varepsilon_i),\\) donde \\(\\boldsymbol \\theta\\) representa un vector de parámetros. Supongamos que estimamos este modelo y calculamos residuos \\(e_i, i=1, \\ldots, n\\). En la Sección 13.5, denotamos los residuos como \\(e_i = \\mathrm{R}(y_i; \\mathbf{x}_i,\\widehat{\\boldsymbol \\theta})\\) donde la función “R” fue determinada por la forma del modelo y \\(\\widehat{\\boldsymbol \\theta}\\) representa el vector estimado de parámetros. Los residuos pueden ser los residuos crudos, residuos de Pearson u otra elección. Usando un residuo bootstrap \\(e^{\\ast}_{jr}\\), podemos crear una pseudo-respuesta \\[ y^{\\ast}_{jr}= \\mathrm{F}(\\mathbf{x}_i, \\widehat{\\boldsymbol \\theta}, e^{\\ast}_{jr}). \\] Podemos entonces usar el conjunto de pseudo-observaciones \\(\\{(y^{\\ast}_{1r},\\mathbf{x}_1), \\ldots, (y^{\\ast}_{nr},\\mathbf{x}_n)\\}\\) para calcular la replicación bootstrap \\(S^{\\ast}_r\\). Como antes, la distribución bootstrap resultante, \\(\\{S^{\\ast}_1, \\ldots, S^{\\ast}_R\\}\\), puede usarse para evaluar la distribución de la estadística \\(S\\). Comparando estas dos opciones, las fortalezas de la primera variación son que emplea menos suposiciones y es más sencilla de interpretar. La limitación es que utiliza un conjunto diferente de variables explicativas \\(\\{\\mathbf{x}^{\\ast}_{1r}, \\ldots, \\mathbf{x}^{\\ast}_{nr} \\}\\) en el cálculo de cada replicación bootstrap. Algunos analistas consideran que su inferencia sobre la estadística \\(S\\) está condicionada a las variables explicativas observadas \\(\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\}\\) y usar un conjunto diferente aborda un problema que no es de interés. La segunda variación aborda esto, pero a costa de una menor generalidad. En esta variación, hay una suposición más fuerte de que el analista ha identificado correctamente el modelo y que el proceso de perturbación \\(\\varepsilon_i = \\mathrm{R}(y_i; \\mathbf{x}_i,\\boldsymbol \\theta)\\) es i.i.d. La tercera variación se conoce como bootstrap paramétrico. Aquí, asumimos que las perturbaciones, y por lo tanto las variables dependientes originales, provienen de un modelo que se conoce hasta un vector de parámetros. Por ejemplo, supongamos que deseamos evaluar la precisión de una estadística \\(S\\) a partir de una regresión de Poisson. Como se describe en el Capítulo 12, asumimos que \\(y_i \\sim Poisson (\\mu_i)\\), donde \\(\\mu_i = \\exp(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta )\\). La estimación de los parámetros de regresión es \\(\\mathbf{b}\\) y, por lo tanto, la media estimada es \\(\\widehat{\\mu}_i = \\exp(\\mathbf{x}_i^{\\prime} \\mathbf{b} )\\). A partir de esto, podemos simular para crear un conjunto de pseudo-respuestas \\[ y^{\\ast}_{ir}\\sim Poisson (\\widehat{\\mu}_i), i=1,\\ldots, n, ~~~r=1,\\ldots, R. \\] Estas pseudo-respuestas pueden usarse para formar la muestra bootstrap \\(r\\), \\(\\{(y^{\\ast}_{1r},\\mathbf{x}_1), \\ldots, (y^{\\ast}_{nr},\\mathbf{x}_n)\\}\\), y a partir de esta la replicación bootstrap, \\(S^{\\ast}_r\\). Así, la principal diferencia entre el bootstrap paramétrico y las dos primeras variaciones es que simulamos a partir de una distribución (Poisson, en este caso), no de una muestra empírica. El bootstrap paramétrico es fácil de interpretar y explicar porque el procedimiento es similar a la simulación de Monte Carlo habitual (ver, por ejemplo, Klugman et al., 2008). La diferencia es que con el bootstrap, usamos los parámetros estimados para calibrar la distribución de muestreo bootstrap, mientras que en la simulación de Monte Carlo esta distribución se asume conocida. Hay dos formas comúnmente usadas para resumir la precisión de la estadística \\(S\\) utilizando la distribución bootstrap, \\(\\{S^{\\ast}_1, \\ldots, S^{\\ast}_R\\}\\). La primera, un enfoque independiente del modelo, implica usar los percentiles de la distribución bootstrap para crear un intervalo de confianza para \\(S\\). Por ejemplo, podríamos usar los percentiles \\(2.5^{th}\\) y \\(97.5^{th}\\) de \\(\\{S^{\\ast}_1, \\ldots, S^{\\ast}_R\\}\\) para un intervalo de confianza del 95% para \\(S\\). En el segundo, se asume alguna distribución para \\(S^{\\ast}_r,\\) típicamente una normalidad aproximada. Con este enfoque, se puede estimar una media y una desviación estándar para obtener el intervalo de confianza usual para \\(S\\). Caso Especial: Bootstrap para Reservas de Pérdidas. England y Verrall (2002) discuten el uso de bootstrap para reservas de pérdidas. Como veremos en el Capítulo 19, al asumir que las pérdidas siguen un modelo Poisson sobredisperso, se pueden obtener predicciones para las reservas de pérdidas mediante un procedimiento mecánico simple conocido como la técnica chain-ladder. Para realizar un bootstrap de un modelo Poisson sobredisperso, como vimos en el Capítulo 12, esto es una variación de un modelo Poisson, no una verdadera distribución de probabilidad, y por lo tanto, el bootstrap paramétrico no está fácilmente disponible. En su lugar, England y Verrall mostraron cómo usar remuestreo de residuos, empleando residuos de Pearson. En muchos casos, el cálculo de la replicación bootstrap \\(S^{\\ast}_r\\) de la estadística puede ser computacionalmente intensivo, requiriendo software especializado. Sin embargo, como señalaron England y Verrall, el caso de reservas de pérdidas con un modelo Poisson sobredisperso es sencillo. Básicamente, se utiliza la técnica chain-ladder para estimar los parámetros del modelo y calcular los residuos de Pearson. Luego, se simula a partir de los residuos, se crean pseudo-respuestas y distribuciones bootstrap. Debido a que la simulación está ampliamente disponible, todo el procedimiento se puede mecanizar fácilmente para trabajar con paquetes de hojas de cálculo estándar, sin necesidad de software estadístico. Véase el Apéndice 3 de England y Verrall (2002) para más detalles sobre el algoritmo. 15.6 Lecturas Adicionales y Referencias La fórmula en las ecuaciones (15.10) no considera la incertidumbre en la estimación de los componentes de varianza. Se han propuesto factores de inflación que consideran esta incertidumbre adicional (Kackar y Harville, 1984), pero tienden a ser pequeños, al menos para conjuntos de datos comúnmente encontrados en la práctica. McCulloch y Searle (2001) ofrecen discusiones adicionales. Silverman (1986) es una introducción clásica a la estimación de densidad. Ruppert, Wand y Carroll (2003) proporcionan un excelente libro de introducción al suavizado de gráficos de dispersión. Además, ofrecen una discusión completa sobre suavizadores basados en splines, una alternativa al ajuste polinómico local. Efron y Tibshirani (1991) es una introducción clásica al bootstrap. Referencias Efron, Bradley and Robert Tibshirani (1991). An Introduction to the Bootstrap. Chapman and Hall, London. England, Peter D. and Richard J. Verrall (2002). Stochastic claims reserving in general insurance. British Actuarial Journal 8, 443-544. Frees, Edward W. (2004). Longitudinal and Panel Data: Analysis and Applications in the Social Sciences. Cambridge University Press, New York. Frees, Edward W., Virginia R. Young and Yu Luo (2001). Case studies using panel data models. North American Actuarial Journal 5 (4), 24-42. Gelman, A., J. B. Carlin, H. S. Stern and D. B. Rubin (2004). Bayesian Data Analysis, Second Edition. Chapman &amp; Hall, New York. Goldberger, Arthur S. (1962). Best linear unbiased prediction in the generalized linear regression model. Journal of the American Statistical Association 57, 369-75. Hastie, Trevor, Robert Tibshirani and Jerome Friedman (2001). The Elements of Statistical Learning: Data Mining, Inference and Prediction. Springer, New York. Henderson, C. R. (1973), Sire evaluation and genetic trends, in Proceedings of the Animal Breeding and Genetics Symposium in Honor of Dr. Jay L. Lush, 10-41. Amer. Soc. Animal Sci.-Amer. Dairy Sci. Assn. Poultry Sci. Assn., Champaign, Illinois. Kackar, R. N. and D. Harville (1984). Approximations for standard errors of estimators of fixed and random effects in mixed linear models. Journal of the American Statistical Association 79, 853-862. Klugman, Stuart A, Harry H. Panjer and Gordon E. Willmot (2008). Loss Models: From Data to Decisions. John Wiley &amp; Sons, Hoboken, New Jersey. McCulloch, Charles E. and Shayle R. Searle (2001). Generalized, Linear and Mixed Models. John Wiley &amp; Sons, New York. Pinheiro, José C. and Douglas M. Bates (2000). Mixed-Effects Models in S and S-PLUS. Springer-Verlag, New York. Ruppert, David, M.P. Wand and Raymond J. Carroll (2003). Semiparametric Regression. Cambridge University Press, Cambridge. Silverman, B. W. (1986). Density Estimation for Statistics and Data Analysis. Chapman and Hall, London. "],["C16FreqSev.html", "Capítulo 16 Modelos de Frecuencia-Severidad 16.1 Introducción 16.2 Modelo Tobit 16.3 Aplicación: Gastos Médicos 16.4 Modelo de Dos Partes 16.5 Modelo de Pérdidas Agregadas 16.6 Lecturas Adicionales y Referencias 16.7 Ejercicios", " Capítulo 16 Modelos de Frecuencia-Severidad Vista Previa del Capítulo. Muchos conjuntos de datos presentan variables dependientes con una gran proporción de ceros. Este capítulo introduce una herramienta econométrica estándar, conocida como el modelo tobit, para manejar este tipo de datos. El modelo tobit se basa en observar una variable dependiente censurada a la izquierda, como las ventas de un producto o un reclamo en una póliza de salud, donde se sabe que la variable dependiente no puede ser menor que cero. Aunque esta herramienta estándar puede ser útil, muchos conjuntos de datos actuariales con una gran proporción de ceros se modelan mejor en “dos partes”, una parte para la frecuencia y otra para la severidad. Este capítulo introduce modelos de dos partes y proporciona extensiones a un modelo de pérdida agregada, donde una unidad en estudio, como una póliza de seguro, puede resultar en más de un reclamo. 16.1 Introducción Muchos conjuntos de datos actuariales vienen en “dos partes”: Una parte para la frecuencia, indicando si ha ocurrido un reclamo o, más generalmente, el número de reclamos, y Una parte para la severidad, indicando el monto de un reclamo. Al predecir o estimar distribuciones de reclamos, a menudo asociamos el costo de los reclamos con dos componentes: el evento del reclamo y su monto, si ocurre. Los actuarios llaman a estos componentes frecuencia y severidad de los reclamos, respectivamente. Esta es la forma tradicional de descomponer datos de “dos partes”, donde se puede pensar en un cero como proveniente de una póliza sin reclamo (Bowers et al., 1997, Capítulo 2). Debido a esta descomposición, los modelos de dos partes también se conocen como modelos de frecuencia-severidad. Sin embargo, esta formulación tradicionalmente se ha usado sin covariables para explicar la frecuencia o la severidad. En la literatura econométrica, Cragg (1971) introdujo covariables en estos dos componentes, citando un ejemplo de seguro contra incendios. Los datos de salud también suelen tener una gran proporción de ceros que deben ser considerados en el modelado. Los valores cero pueden representar la falta de utilización de servicios de salud por parte de un individuo, ningún gasto o no participación en un programa. En el área de la salud, Mullahy (1998) cita algunos ámbitos destacados de aplicabilidad potencial: Investigación de resultados - cantidad de utilización de servicios de salud o gastos. Demanda de servicios de salud - cantidad de servicios solicitados, como el número de visitas al médico. Abuso de sustancias - cantidad consumida de tabaco, alcohol y drogas ilícitas. El aspecto de dos partes puede estar oscurecido por una forma natural de registrar datos; ingresar el monto del reclamo cuando ocurre (un número positivo) y un cero para la ausencia de reclamo. Es fácil pasar por alto una gran proporción de ceros, particularmente cuando el analista también está interesado en muchas covariables que pueden ayudar a explicar una variable dependiente. Como veremos en este capítulo, ignorar la naturaleza de dos partes puede llevar a un sesgo serio. Para ilustrarlo, recordemos del Capítulo 6 un gráfico de los ingresos de un individuo (\\(x\\)) frente a la cantidad de seguro adquirido (\\(y\\)) (Figura 6.3). Ajustar una sola línea a estos datos desinformaría a los usuarios sobre los efectos de \\(x\\) en \\(y\\). Figura 16.1: Cuando los individuos no compran seguros, se registran como ventas \\(y=0\\). La muestra en este gráfico representa dos submuestras, aquellos que compraron seguro, correspondientes a \\(y&gt;0\\), y aquellos que no lo hicieron, correspondientes a \\(y=0\\). En contraste, muchas aseguradoras mantienen archivos separados para la frecuencia y la severidad. Por ejemplo, las aseguradoras mantienen un archivo de “titulares de pólizas” que se establece cuando se suscribe una póliza. Este archivo registra mucha información de suscripción sobre los asegurados, como edad, género y experiencia previa de reclamos, información de la póliza como coberturas, deducibles y limitaciones, así como el evento del reclamo. Un archivo separado, a menudo conocido como el archivo de “reclamos”, registra detalles del reclamo contra la aseguradora, incluyendo el monto. (También puede haber un archivo de “pagos” que registre el momento de los pagos, aunque no trataremos este aquí). Este proceso de registro hace que sea natural para las aseguradoras modelar la frecuencia y la severidad como procesos separados. 16.2 Modelo Tobit Una forma de modelar una gran proporción de ceros es asumir que la variable dependiente está censurada a la izquierda en cero. Este capítulo introduce la regresión censurada a la izquierda, comenzando con el conocido modelo tobit basado en el trabajo pionero de James Tobin (1958). Posteriormente, Goldberger (1964) acuñó la frase “modelo tobit,” reconociendo el trabajo de Tobin y su similitud con el modelo probit. Al igual que con el modelo probit (y otros modelos de respuesta binaria), utilizamos una variable no observada, o latente, \\(y^{\\ast}\\) que se asume que sigue un modelo de regresión lineal de la forma \\[ y_i^{\\ast} = \\mathbf{x}_i^{\\prime} \\boldsymbol{\\beta} + \\varepsilon_i. \\tag{16.1} \\] Las respuestas están censuradas o “limitadas” en el sentido de que observamos \\(y_i = \\max \\left( y_i^{\\ast},d_i\\right)\\). El valor límite, \\(d_i\\), es una cantidad conocida. Muchas aplicaciones utilizan \\(d_i=0\\), correspondiente a ventas o gastos de cero, dependiendo de la aplicación. Sin embargo, también podríamos usar \\(d_i\\) para los gastos diarios reclamados como reembolso de viajes y permitir que el reembolso (como 50 o 100) varíe según el empleado \\(i\\). Algunos lectores pueden revisar la Sección 14.2 para una introducción al censurado. Los parámetros del modelo consisten en los coeficientes de regresión, \\(\\boldsymbol{\\beta}\\), y el término de variabilidad, \\(\\sigma^2 = \\mathrm{Var}~\\varepsilon_i\\). Con la ecuación (16.1), interpretamos los coeficientes de regresión como el cambio marginal en \\(\\mathrm{E~}y^{\\ast}\\) por unidad de cambio en cada variable explicativa. Esto puede ser satisfactorio en algunas aplicaciones, como cuando \\(y^{\\ast}\\) representa una pérdida de seguro. Sin embargo, para la mayoría de las aplicaciones, los usuarios están típicamente interesados en cambios marginales en \\(\\mathrm{E~}y\\), es decir, el valor esperado de la respuesta observada. Para interpretar estos cambios marginales, es habitual adoptar la suposición de normalidad para la variable latente \\(y_i^{\\ast}\\) (o equivalentemente para la perturbación \\(\\varepsilon_i\\)). Con esta suposición, cálculos estándar (ver Ejercicio 16.1) muestran que \\[ \\mathrm{E~}y_i = d_i + \\Phi \\left( \\frac{\\mathbf{x}_i^{\\prime} \\boldsymbol{\\beta} - d_i}{\\sigma}\\right) \\left( \\mathbf{x}_i^{\\prime} \\boldsymbol{\\beta} - d_i + \\sigma \\lambda_i\\right), \\tag{16.2} \\] donde \\[ \\lambda_i = \\frac{\\mathrm{\\phi}\\left( \\left(\\mathbf{x}_i^{\\prime} \\boldsymbol{\\beta} - d_i\\right)/\\sigma \\right)}{\\Phi\\left( \\left(\\mathbf{x}_i^{\\prime} \\boldsymbol{\\beta} - d_i\\right)/\\sigma \\right)}. \\] Aquí, \\(\\mathrm{\\phi}(\\cdot)\\) y \\(\\Phi(\\cdot)\\) son la función de densidad y la función de distribución de la normal estándar, respectivamente. La razón entre una función de densidad de probabilidad y una función de distribución acumulativa a veces se llama razón de Mills inversa. Aunque compleja en apariencia, la ecuación (16.2) permite calcular fácilmente \\(\\mathrm{E~} y\\). Para valores grandes de \\(\\left(\\mathbf{x}_i^{\\prime}\\boldsymbol{\\beta} - d_i\\right)/\\sigma\\), vemos que \\(\\lambda_i\\) se aproxima a 0 y \\(\\Phi\\left( \\left(\\mathbf{x}_i^{\\prime}\\boldsymbol{\\beta} - d_i\\right)/\\sigma \\right)\\) se aproxima a 1. Interpretamos esto como que, para valores grandes del componente sistemático \\(\\mathbf{x}_i^{\\prime}\\boldsymbol{\\beta}\\), la función de regresión \\(\\mathrm{E~}y_i\\) tiende a ser lineal y se aplican las interpretaciones habituales. La especificación del modelo tobit tiene el mayor impacto en las observaciones cercanas al valor límite \\(d_i\\). La ecuación (16.2) muestra que, si un analista ignora los efectos del censurado, la función de regresión puede ser muy diferente de la típica función de regresión lineal, \\(\\mathrm{E~}y=\\mathbf{x}^{\\prime}\\boldsymbol{\\beta}\\), lo que resulta en estimaciones sesgadas de los coeficientes. La otra vía tentadora es excluir observaciones limitadas (\\(y_i=d_i\\)) del conjunto de datos y nuevamente realizar una regresión ordinaria. Sin embargo, cálculos estándar también muestran que \\[ \\mathrm{E~}\\left( y_i|\\ y_i&gt;d_i\\right) =\\mathbf{x}_i^{\\prime}\\boldsymbol{\\beta} + \\sigma \\frac{\\mathrm{\\phi}\\left( (\\mathbf{x}_i^{\\prime} \\boldsymbol{\\beta} - d_i)/\\sigma \\right)}{1-\\Phi \\left( (\\mathbf{x}_i^{\\prime} \\boldsymbol{\\beta} - d_i)/\\sigma \\right)} \\tag{16.3} \\] Por lo tanto, este procedimiento también resulta en coeficientes de regresión sesgados. Un método comúnmente utilizado para estimar el modelo tobit es el de máxima verosimilitud. Empleando la suposición de normalidad, los cálculos estándar muestran que la log-verosimilitud puede expresarse como \\[ \\begin{array}{ll} \\ln L &amp;= \\sum\\limits_{i:y_i=d_i} \\ln \\left\\{ 1-\\Phi \\left( \\frac{\\mathbf{x}_i^{\\prime}\\boldsymbol{\\beta} - d_i}{\\sigma}\\right) \\right\\} \\\\ &amp; - \\frac{1}{2} \\sum\\limits_{i:y_i&gt;d_i} \\left\\{ \\ln 2\\pi \\sigma^2 + \\frac{(y_i-(\\mathbf{x}_i^{\\prime}\\boldsymbol{\\beta}-d_i))^2}{\\sigma^2}\\right\\}, \\tag{16.4} \\end{array} \\] donde \\(\\{i:y_i=d_i\\}\\) y \\(\\{i:y_i&gt;d_i\\}\\) indican la suma sobre las observaciones censuradas y no censuradas, respectivamente. Muchos paquetes de software estadístico pueden calcular fácilmente los estimadores de máxima verosimilitud, \\(\\mathbf{b}_{MLE}\\) y \\(s_{MLE}\\), así como los errores estándar correspondientes. La Sección 11.9 introduce la inferencia por verosimilitud. Para algunos usuarios, resulta conveniente disponer de un algoritmo que no dependa de software especializado. Un algoritmo en dos etapas, propuesto por Heckman (1976), satisface esta necesidad. Para este algoritmo, primero se resta \\(d_i\\) de cada \\(y_i\\), de modo que \\(d_i\\) pueda considerarse cero sin pérdida de generalidad. Incluso para aquellos que desean utilizar los estimadores de máxima verosimilitud más eficientes, el algoritmo de Heckman puede ser útil en la etapa de exploración del modelo, ya que utiliza regresión lineal para ayudar a seleccionar la forma adecuada de la ecuación de regresión. Algoritmo de Heckman para Estimar los Parámetros del Modelo Tobit En la primera etapa, defina la variable binaria \\[ r_i=\\left\\{ \\begin{array}{ll} 1 &amp; \\text{si } y_i&gt;0 \\\\ 0 &amp; \\text{si } y_i=0 \\end{array} \\right. , \\] indicando si la observación está censurada o no. Realice una regresión probit utilizando \\(r_i\\) como la variable dependiente y \\(\\mathbf{x}_i\\) como variables explicativas. Llame a los coeficientes de regresión resultantes \\(\\mathbf{g}_{PROBIT}\\). Para cada observación, calcule la variable estimada \\[ \\widehat{\\lambda}_i=\\frac{\\mathrm{\\phi }\\left( \\mathbf{x}_i^{\\prime} \\mathbf{g}_{PROBIT}\\right) }{\\Phi \\left( \\mathbf{x}_i^{\\prime} \\mathbf{g}_{PROBIT}\\right) }, \\] una razón de Mills inversa. Con esto, realice una regresión de \\(y_i\\) sobre \\(\\mathbf{x}_i\\) y \\(\\widehat{\\lambda}_i\\). Llame a los coeficientes de regresión resultantes \\(\\mathbf{b}_{2SLS}\\). La idea detrás de este algoritmo es que la ecuación (16.1) tiene la misma forma que el modelo probit; por lo tanto, se pueden calcular estimaciones consistentes de los coeficientes de regresión (hasta la escala). Los coeficientes de regresión \\(\\mathbf{b}_{2SLS}\\) proporcionan estimaciones consistentes y asintóticamente normales de \\(\\boldsymbol{\\beta}\\). Sin embargo, son menos eficientes en comparación con los estimadores de máxima verosimilitud, \\(\\mathbf{b}_{MLE}\\). Los cálculos estándar (ver Ejercicio 16.1) muestran que \\(\\mathrm{Var~}\\left( y_i|\\ y_i&gt;d_i\\right)\\) depende de \\(i\\) (incluso cuando \\(d_i\\) es constante). Por lo tanto, es habitual utilizar errores estándar consistentes con heterocedasticidad para \\(\\mathbf{b}_{2SLS}\\). 16.3 Aplicación: Gastos Médicos Esta sección considera datos del Medical Expenditure Panel Survey (MEPS), introducidos en la Sección 11.4. Recordemos que MEPS es una encuesta probabilística que proporciona estimaciones representativas a nivel nacional sobre el uso de atención médica, gastos, fuentes de pago y cobertura de seguros para la población civil de los Estados Unidos. Consideramos datos del MEPS del primer panel de 2003 y tomamos una muestra aleatoria de \\(n=2,000\\) individuos entre 18 y 65 años. La Sección 11.4 analizó el componente de frecuencia, tratando de entender los determinantes que influyen en si las personas fueron hospitalizadas o no. La Sección 13.4 analizó el componente de severidad; dado que una persona fue hospitalizada, ¿cuáles son los determinantes de los gastos médicos? Este capítulo busca unificar estos dos componentes en un solo modelo de utilización de atención médica. Estadísticas Resumidas Tabla 16.1 revisa estas variables explicativas y proporciona estadísticas resumidas que sugieren sus efectos sobre los gastos de visitas hospitalarias. La segunda columna, “Promedio Gastos”, muestra el promedio de los gastos logarítmicos por variable explicativa, tratando los casos sin gastos como un gasto (logarítmico) de cero. Este sería el principal interés si no se descompusiera el gasto total en un valor discreto de cero y una cantidad continua. Examinando este promedio general (logarítmico) de gastos, vemos que las mujeres tuvieron mayores gastos que los hombres. En términos de etnicidad, los nativos americanos y asiáticos tuvieron los gastos promedio más bajos. Sin embargo, estos dos grupos étnicos representaron solo el 5.4% del tamaño total de la muestra. Respecto a las regiones, parece que las personas del oeste tuvieron los gastos promedio más bajos. En términos de educación, las personas más educadas tuvieron menores gastos, lo que respalda la teoría de que las personas más educadas toman un papel más activo en mantener su salud. En cuanto a la autoevaluación de la salud, una peor salud física, mental y limitaciones relacionadas con la actividad llevaron a mayores gastos. Las personas de ingresos más bajos tuvieron mayores gastos, y aquellas con cobertura de seguro tuvieron mayores gastos promedio. Tabla 16.1 también describe los efectos de las variables explicativas sobre la frecuencia de uso y el promedio de gastos para quienes utilizaron servicios hospitalarios. Como en la Tabla 11.4, la columna “Porcentaje Positivos Gastos” muestra el porcentaje de individuos que tuvieron algún gasto positivo, por variable explicativa. La columna “Promedio Positivos Gastos” muestra el promedio (logarítmico) de gastos en los casos donde hubo un gasto, ignorando los ceros. Esto es comparable al gasto mediano en la Tabla 13.5 (dado en dólares, no en logaritmos). Para ilustrar, consideremos que las mujeres tuvieron mayores gastos promedio que los hombres observando la columna “Promedio Gastos”. Al desglosar esto en frecuencia y monto de utilización, vemos que las mujeres tuvieron una mayor frecuencia de utilización, pero cuando tuvieron una utilización positiva, el promedio (logarítmico) de gastos fue menor que el de los hombres. Un examen de la Tabla 16.1 muestra que esta observación se mantiene para otras variables explicativas. El efecto de una variable sobre los gastos generales puede ser positivo, negativo o no significativo; este efecto puede ser bastante diferente cuando descomponemos los gastos en componentes de frecuencia y monto. Tabla 16.2 compara la regresión de mínimos cuadrados ordinarios (OLS) con las estimaciones de máxima verosimilitud para el modelo tobit. De esta tabla, podemos ver que hay un acuerdo sustancial entre los valores-\\(t\\) de estos modelos ajustados. Este acuerdo proviene de examinar el signo (positivo o negativo) y la magnitud (como superar dos para significancia estadística) del valor-\\(t\\) de cada variable. Los coeficientes de regresión también coinciden en gran medida en el signo. Sin embargo, no es sorprendente que las magnitudes de los coeficientes de regresión difieran sustancialmente. Esto se debe a que, según la ecuación (16.2), podemos ver que los coeficientes tobit miden el cambio marginal de la variable latente esperada \\(y^{\\ast}\\), no el cambio marginal de la variable observada esperada \\(y\\), como lo hace OLS. Tabla 16.1. Porcentaje de Gastos Positivos y Gasto Promedio Logarítmico, por Variable Explicativa \\[ \\scriptsize{ \\begin{array}{lllrrrr}\\hline \\text{Categoría} &amp; \\text{Variable} &amp; \\text{Descripción} &amp; \\text{Porcentaje} &amp; \\text{Promedio} &amp; \\text{Porcentaje} &amp; \\text{Promedio} \\\\ &amp; &amp; &amp; \\text{de datos} &amp; \\text{Gastos} &amp; \\text{Positivos} &amp; \\text{Positivos} \\\\ &amp; &amp; &amp; &amp; &amp; \\text{Gastos} &amp; \\text{Gastos} \\\\ \\hline \\text{Demografía} &amp; AGE &amp; \\text{Edad en años} \\\\ &amp; &amp; \\ \\ \\ \\text{18 a 65 (promedio: 39.0)} \\\\ &amp; GENDER &amp; \\text{1 si es mujer} &amp; 52.7 &amp; 0.91 &amp; 10.7 &amp; 8.53 \\\\ &amp; GENDER &amp; \\text{0 si es hombre} &amp; 47.3 &amp; 0.40 &amp; 4.7 &amp; 8.66 \\\\ \\text{Etnicidad} &amp; ASIAN &amp; \\text{1 si es asiático} &amp; 4.3 &amp; 0.37 &amp; 4.7 &amp; 7.98 \\\\ &amp; BLACK &amp; \\text{1 si es negro} &amp; 14.8 &amp; 0.90 &amp; 10.5 &amp; 8.60 \\\\ &amp; NATIVE &amp; \\text{1 si es nativo} &amp; 1.1 &amp; 1.06 &amp; 13.6 &amp; 7.79 \\\\ &amp; WHITE &amp; \\text{Nivel de referencia} &amp; 79.9 &amp; 0.64 &amp; 7.5 &amp; 8.59 \\\\ \\text{Región} &amp; NORTHEAST &amp; \\text{1 si es noreste} &amp; 14.3 &amp; 0.83 &amp; 10.1 &amp; 8.17 \\\\ &amp; MIDWEST &amp; \\text{1 si es medio oeste} &amp; 19.7 &amp; 0.76 &amp; 8.7 &amp; 8.79 \\\\ &amp; SOUTH &amp; \\text{1 si es sur} &amp; 38.2 &amp; 0.72 &amp; 8.4 &amp; 8.65 \\\\ &amp; WEST &amp; \\text{Nivel de referencia} &amp; 27.9 &amp; 0.46 &amp; 5.4 &amp; 8.51 \\\\ \\hline \\text{Educación} &amp; COLLEGE &amp; \\text{1 si tiene título universitario o superior} &amp; 27.2 &amp; 0.58 &amp; 6.8 &amp; 8.50 \\\\ &amp; HIGHSCHOOL &amp; \\text{1 si tiene diploma de secundaria} &amp; 43.3 &amp; 0.67 &amp; 7.9 &amp; 8.54 \\\\ &amp; &amp; \\text{Nivel de referencia es menor} &amp; 29.5 &amp; 0.76 &amp; 8.8 &amp; 8.64 \\\\ &amp; &amp; \\ \\ \\ \\text{que diploma de secundaria} &amp; &amp; &amp; &amp; \\\\ \\hline \\text{Autoevaluación} &amp; POOR &amp; \\text{1 si pobre} &amp; 3.8 &amp; 3.26 &amp; 36.0 &amp; 9.07 \\\\ ~~\\text{salud física} &amp; FAIR &amp; \\text{1 si regular} &amp; 9.9 &amp; 0.66 &amp; 8.1 &amp; 8.12 \\\\ &amp; GOOD &amp; \\text{1 si buena} &amp; 29.9 &amp; 0.70 &amp; 8.2 &amp; 8.56 \\\\ &amp; VGOOD &amp; 1 \\text{si muy buena} &amp; 31.1 &amp; 0.54 &amp; 6.3 &amp; 8.64 \\\\ &amp; &amp; \\text{Nivel de referencia es excelente} &amp; 25.4 &amp; 0.42 &amp; 5.1 &amp; 8.22 \\\\ \\text{Autoevaluación} &amp; MNHPOOR &amp; \\text{1 si salud mental pobre o regular} &amp; 7.5 &amp; 1.45 &amp; 16.8 &amp; 8.67 \\\\ ~~\\text{salud mental} &amp; &amp; \\text{0 si buena a excelente} &amp; 92.5 &amp; 0.61 &amp; 7.1 &amp; 8.55 \\\\ \\text{Limitación} &amp; ANYLIMIT &amp; \\text{1 si hay limitación funcional o actividad} &amp; 22.3 &amp; 1.29 &amp; 14.6 &amp; 8.85 \\\\ ~~\\text{de actividad} &amp; &amp; \\text{0 si no hay limitación} &amp; 77.7 &amp; 0.50 &amp; 5.9 &amp; 8.36 \\\\ \\hline \\text{Ingreso} &amp; HINCOME &amp; \\text{1 si ingreso alto} &amp; 31.6 &amp; 0.47 &amp; 5.4 &amp; 8.73 \\\\ \\text{ comparado} &amp; MINCOME &amp; \\text{1 si ingreso medio} &amp; 29.9 &amp; 0.61 &amp; 7.0 &amp; 8.75 \\\\ \\text{ a la línea} &amp; LINCOME &amp; \\text{1 si ingreso bajo} &amp; 15.8 &amp; 0.73 &amp; 8.3 &amp; 8.87 \\\\ \\text{ de pobreza} &amp; NPOOR &amp; \\text{1 si cerca de la línea de pobreza} &amp; 5.8 &amp; 0.78 &amp; 9.5 &amp; 8.19 \\\\ &amp; &amp; \\text{Nivel de referencia es pobre/negativo} &amp; 17.0 &amp; 1.06 &amp; 13.0 &amp; 8.18 \\\\ \\hline \\text{Seguro} &amp; INSURE &amp; 1 \\text{si tiene cobertura médica pública o} &amp; 77.8 &amp; 0.80 &amp; 9.2 &amp; 8.68 \\\\ ~~\\text{cobertura}&amp; &amp; ~~\\text{ privada en algún mes de 2003} \\\\ &amp; &amp; \\text{0 si no tiene seguro en 2003} &amp; 22.3 &amp; 0.23 &amp; 3.1 &amp; 7.43 \\\\ \\hline \\text{Total} &amp; &amp; &amp; 100.0 &amp; 0.67 &amp; 7.9 &amp; 8.32 \\\\\\hline \\end{array} } \\] Código R para Generar la Tabla 16.1 # Table 16.1 Hexpend &lt;- read.csv(&quot;CSVData/HealthExpend.csv&quot;, header=TRUE) #Hexpend &lt;- read.csv(&quot;../../CSVData/HealthExpend.csv&quot;, header=TRUE) attach(Hexpend) # RECODE A FEW VARIABLES ; ASIAN &lt;- 1*(RACE1==1) BLACK &lt;- 1*(RACE1==2) NATIVE &lt;- 1*(RACE1==3) WHITE &lt;- 1-ASIAN-BLACK-NATIVE # combined other and white NORTHEAST &lt;- 1*(REGION1==1) MIDWEST &lt;- 1*(REGION1==2) SOUTH &lt;- 1*(REGION1==3) WEST &lt;- 1-NORTHEAST-MIDWEST-SOUTH HIGHSCHOOL&lt;- 1*(EDUC1==1) COLLEGE &lt;- 1*(EDUC1==2) LOWERHS &lt;- 1-HIGHSCHOOL-COLLEGE POOR &lt;- 1*(PHSTAT1==4) FAIR &lt;- 1*(PHSTAT1==3) GOOD &lt;- 1*(PHSTAT1==2) VGOOD &lt;- 1*(PHSTAT1==1) EXCELLNT &lt;- 1-POOR-FAIR-GOOD-VGOOD NPOOR &lt;- 1*(INCOME1==1) LINCOME &lt;- 1*(INCOME1==2) MINCOME &lt;- 1*(INCOME1==3) HINCOME &lt;- 1*(INCOME1==4) POORNEG &lt;- 1-NPOOR-LINCOME-MINCOME-HINCOME COUNT &lt;- COUNTIP EXP &lt;- EXPENDIP POSEXP &lt;- 1*(EXP&gt;0) LNEXP &lt;- ifelse(EXP&gt;0,log(EXP),0) LOWERBOUND &lt;- ifelse(EXP&gt;0, LNEXP,&#39;&#39;) RaceFactor &lt;- cbind(ASIAN, BLACK, NATIVE, WHITE) RegionFactor &lt;- cbind(NORTHEAST, MIDWEST, SOUTH, WEST) EducFactor &lt;- cbind(HIGHSCHOOL, COLLEGE, LOWERHS) PhstatFactor &lt;- cbind(POOR,FAIR,GOOD,VGOOD,EXCELLNT) IncomeFactor &lt;- cbind(NPOOR,LINCOME,MINCOME,HINCOME,POORNEG) PersonLevel &lt;- cbind(Hexpend, RaceFactor, RegionFactor, EducFactor, PhstatFactor, IncomeFactor, COUNT,EXP, POSEXP, LNEXP, LOWERBOUND) detach(Hexpend) Hexpend$POSEXP &lt;- 1*(Hexpend$EXPENDIP&gt;0) Hexpend$LNEXP &lt;- ifelse(Hexpend$EXPENDIP&gt;0,log(Hexpend$EXPENDIP),0) detach(&quot;package:Hmisc&quot;, unload = TRUE) library(dplyr) # Summarize using dplyr summary_by_group &lt;- function(group){ Hexpend %&gt;% group_by(!!sym(group)) %&gt;% summarize( Propn = 100 * n() /dim(Hexpend)[1], Mean1 = mean(LNEXP), Mean2 = mean(POSEXP), Mean3 = Mean1 / Mean2 ) %&gt;% rename(&quot;Var&quot; = group ) } # summary_by_group(group = &quot;GENDER&quot;) # # summary_by_group(group = &quot;RACE1&quot;) # POSITIVE EXPENDITURES ; PersonLevelPOSEXP &lt;-subset(PersonLevel, POSEXP==1) TabGender &lt;- summary_by_group(group = &quot;GENDER&quot;) Label &lt;- c(&quot;MALE&quot;,&quot;FEMALE&quot;) Table161Gender &lt;- cbind(TabGender,Label) TabRace &lt;- summary_by_group(group = &quot;RACE1&quot;) Label &lt;- c(&quot;OTHERS&quot;,&quot;ASIAN&quot;,&quot;BLACK&quot;,&quot;NATIVE&quot;,&quot;WHITE&quot;) Table161Race &lt;- cbind(TabRace,Label) TabRegion &lt;- summary_by_group(group = &quot;REGION1&quot;) Label &lt;- c(&quot;WEST&quot;,&quot;NORTHEAST&quot;,&quot;MIDWEST&quot;,&quot;SOUTH&quot;) Table161Region &lt;- cbind(TabRegion,Label) TabEduc &lt;- summary_by_group(group = &quot;EDUC1&quot;) Label &lt;- c(&quot;LOWERHS&quot;,&quot;HIGHSCHOOL&quot;,&quot;COLLEGE&quot;) Table161Educ &lt;- cbind(TabEduc,Label) TabPhstat &lt;- summary_by_group(group = &quot;PHSTAT1&quot;) Label &lt;- c(&quot;EXCELLENT&quot;,&quot;VGOOD&quot;,&quot;GOOD&quot;,&quot;FAIR&quot;,&quot;POOR&quot;) Table161Phstat &lt;- cbind(TabPhstat,Label) TabMnh &lt;- summary_by_group(group = &quot;MNHPOOR&quot;) Label &lt;- c(&quot;GOOD&quot;,&quot;POOR&quot;) Table161Mnh &lt;- cbind(TabMnh,Label) TabAnylimit &lt;- summary_by_group(group = &quot;ANYLIMIT&quot;) Label &lt;- c(&quot;NO&quot;,&quot;YES&quot;) Table161Anylimit &lt;- cbind(TabAnylimit,Label) TabIncome &lt;- summary_by_group(group = &quot;INCOME1&quot;) Label &lt;- c(&quot;POOR&quot;,&quot;NPOOR&quot;,&quot;LINCOME&quot;,&quot;MINCOME&quot;,&quot;HINCOME&quot;) Table161Income &lt;- cbind(TabIncome,Label) TabInsure &lt;- summary_by_group(group = &quot;insure&quot;) Label &lt;- c(&quot;UNINSURED&quot;,&quot;INSURED&quot;) Table161Insure &lt;- cbind(TabInsure,Label) # CREATE TABLE 16.1 Table161n &lt;- rbind(Table161Gender, Table161Race, Table161Region, Table161Educ, Table161Phstat, Table161Mnh, Table161Anylimit, Table161Income, Table161Insure) Category &lt;- c(&quot;GENDER&quot;,&quot; &quot;,&quot;RACE&quot;,&quot; &quot;,&quot; &quot;,&quot; &quot;,&quot; &quot;, &quot;REGION&quot;,&quot; &quot;,&quot; &quot;,&quot; &quot;, &quot;EDUCATION&quot;,&quot; &quot;,&quot; &quot;, &quot;PHSTAT&quot;,&quot; &quot;,&quot; &quot;,&quot; &quot;,&quot; &quot;, &quot;MNHEALTH&quot;,&quot; &quot;, &quot;ANYLIMIT&quot;,&quot; &quot;, &quot;INCOME&quot;,&quot; &quot;,&quot; &quot;,&quot; &quot;,&quot; &quot;, &quot;INSURED&quot;, &quot; &quot;) Table161 &lt;- cbind(Category,Table161n) knitr::kable(Table161, digits = 2) Category Var Propn Mean1 Mean2 Mean3 Label GENDER 0 47.30 0.40 0.05 8.66 MALE 1 52.70 0.91 0.11 8.53 FEMALE RACE 0 1.65 0.78 0.09 8.56 OTHERS 1 4.30 0.37 0.05 7.98 ASIAN 2 14.75 0.90 0.11 8.60 BLACK 3 1.10 1.06 0.14 7.79 NATIVE 4 78.20 0.64 0.07 8.60 WHITE REGION 0 27.85 0.46 0.05 8.51 WEST 1 14.30 0.83 0.10 8.17 NORTHEAST 2 19.65 0.76 0.09 8.79 MIDWEST 3 38.20 0.72 0.08 8.65 SOUTH EDUCATION 0 29.50 0.76 0.09 8.64 LOWERHS 1 43.30 0.67 0.08 8.54 HIGHSCHOOL 2 27.20 0.58 0.07 8.50 COLLEGE PHSTAT 0 25.40 0.42 0.05 8.22 EXCELLENT 1 31.10 0.54 0.06 8.64 VGOOD 2 29.90 0.70 0.08 8.56 GOOD 3 9.85 0.66 0.08 8.12 FAIR 4 3.75 3.26 0.36 9.07 POOR MNHEALTH 0 92.55 0.61 0.07 8.55 GOOD 1 7.45 1.45 0.17 8.67 POOR ANYLIMIT 0 77.70 0.50 0.06 8.36 NO 1 22.30 1.29 0.15 8.85 YES INCOME 0 16.95 1.06 0.13 8.18 POOR 1 5.80 0.78 0.09 8.19 NPOOR 2 15.75 0.73 0.08 8.87 LINCOME 3 29.90 0.61 0.07 8.75 MINCOME 4 31.60 0.47 0.05 8.73 HINCOME INSURED 0 22.25 0.23 0.03 7.43 UNINSURED 1 77.75 0.80 0.09 8.68 INSURED Tabla 16.2. Comparación de OLS, Tobit MLE y Estimaciones en Dos Etapas \\[ \\scriptsize{ \\begin{array}{l|rr|rr|rr} \\hline &amp; \\text{OLS} &amp; &amp;\\text{Tobit MLE} &amp; &amp; \\text{Dos Etapas} \\\\ &amp; \\text{Parámetro} &amp; &amp; \\text{Parámetro} &amp; &amp; \\text{Parámetro} &amp; \\\\ \\text{Efecto} &amp; \\text{Estimación} &amp; t\\text{-ratio} &amp; \\text{Estimación} &amp; t\\text{-ratio} &amp; \\text{Estimación} &amp; t\\text{-ratio}^{\\ast} \\\\ \\hline Intercepto &amp; -0.123 &amp; -0.525 &amp; -33.016 &amp; -8.233 &amp; 2.760 &amp; 0.617 \\\\ AGE &amp; 0.001 &amp; 0.091 &amp; -0.006 &amp; -0.118 &amp; 0.001 &amp; 0.129 \\\\ GENDER &amp; 0.379 &amp; 3.711 &amp; 5.727 &amp; 4.107 &amp; 0.271 &amp; 1.617 \\\\ ASIAN &amp; -0.115 &amp; -0.459 &amp; -1.732 &amp; -0.480 &amp; -0.091 &amp; -0.480 \\\\ BLACK &amp; 0.054 &amp; 0.365 &amp; 0.025 &amp; 0.015 &amp; 0.043 &amp; 0.262 \\\\ NATIVE &amp; 0.350 &amp; 0.726 &amp; 3.745 &amp; 0.723 &amp; 0.250 &amp; 0.445 \\\\ NORTHEAST &amp; 0.283 &amp; 1.702 &amp; 3.828 &amp; 1.849 &amp; 0.203 &amp; 1.065 \\\\ MIDWEST &amp; 0.255 &amp; 1.693 &amp; 3.459 &amp; 1.790 &amp; 0.196 &amp; 1.143 \\\\ SOUTH &amp; 0.146 &amp; 1.133 &amp; 1.805 &amp; 1.056 &amp; 0.117 &amp; 0.937 \\\\ \\hline COLLEGE &amp; -0.014 &amp; -0.089 &amp; 0.628 &amp; 0.329 &amp; -0.024 &amp; -0.149 \\\\ HIGHSCHOOL &amp; -0.027 &amp; -0.209 &amp; -0.030 &amp; -0.019 &amp; -0.026 &amp; -0.202 \\\\ \\hline POOR &amp; 2.297 &amp; 7.313 &amp; 13.352 &amp; 4.436 &amp; 1.780 &amp; 1.810 \\\\ FAIR &amp; -0.001 &amp; -0.004 &amp; 1.354 &amp; 0.528 &amp; -0.014 &amp; -0.068 \\\\ GOOD &amp; 0.188 &amp; 1.346 &amp; 2.740 &amp; 1.480 &amp; 0.143 &amp; 1.018 \\\\ VGOOD &amp; 0.084 &amp; 0.622 &amp; 1.506 &amp; 0.815 &amp; 0.063 &amp; 0.533 \\\\ MNHPOOR &amp; 0.000 &amp; -0.001 &amp; -0.482 &amp; -0.211 &amp; -0.011 &amp; -0.041 \\\\ ANYLIMIT &amp; 0.415 &amp; 3.103 &amp; 4.695 &amp; 3.000 &amp; 0.306 &amp; 1.448 \\\\ \\hline HINCOME &amp; -0.482 &amp; -2.716 &amp; -6.575 &amp; -3.035 &amp; -0.338 &amp; -1.290 \\\\ MINCOME &amp; -0.309 &amp; -1.868 &amp; -4.359 &amp; -2.241 &amp; -0.210 &amp; -0.952 \\\\ LINCOME &amp; -0.175 &amp; -0.976 &amp; -3.414 &amp; -1.619 &amp; -0.099 &amp; -0.438 \\\\ NPOOR &amp; -0.116 &amp; -0.478 &amp; -2.274 &amp; -0.790 &amp; -0.065 &amp; -0.243 \\\\ INSURE &amp; 0.594 &amp; 4.486 &amp; 8.534 &amp; 4.130 &amp; 0.455 &amp; 2.094 \\\\ \\hline \\text{Razón de Mill Inversa } \\widehat{\\lambda} &amp; &amp; &amp;&amp; &amp;-3.616 &amp; -0.642 \\\\ \\text{Escala } \\sigma^2&amp; 4.999 &amp; &amp; 14.738 &amp; &amp; 4.997 &amp; \\\\ \\hline \\end{array} } \\] Nota: \\(^{\\ast}\\) Los \\(t\\)-ratios en dos etapas se calculan utilizando errores estándar consistentes con heteroscedasticidad. Código R para Generar la Tabla 16.2 # Table 16.2 #Hexpend &lt;- read.csv(&quot;CSVData/HealthExpend.csv&quot;, header=TRUE) #Hexpend &lt;- read.csv(&quot;../../CSVData/HealthExpend.csv&quot;, header=TRUE) # TABLE 16.2 OLS REGRESSION ; modelOLS &lt;- lm(LNEXP ~ AGE+GENDER+ASIAN+BLACK+NATIVE+NORTHEAST+MIDWEST+SOUTH + COLLEGE+HIGHSCHOOL+POOR+FAIR+GOOD+VGOOD+MNHPOOR+ANYLIMIT+HINCOME+MINCOME+ LINCOME+NPOOR+insure, data=PersonLevel) OLSSum &lt;- summary(modelOLS) #OLSSum # The above result replicates the parameter estimates given in Table 16.2 # But the standard errors and t ratios are slightly different # That&#39;s because glm model in R uses &quot;reml&quot; to compute the standard errors # while the result given by Table 16.2 is based on a &quot;ml&quot; method # To replicate the result, run the following function # coefficients=result given by glm, n=number of observations, k=number of regressors including intercept; SummaryML &lt;- function(coefficients, n, k) { Estimate &lt;- coefficients[,1] Std_Error &lt;- coefficients[,2] * sqrt((n-k)/n) t_Ratio &lt;- coefficients[,3] * sqrt(n/(n-k)) coeffML &lt;- cbind(Estimate, Std_Error, t_Ratio) return(coeffML) } modelOlsML &lt;- SummaryML(OLSSum$coefficients,2000,22) #modelOlsML # TABLE 16.2 TOBIT REGRESSION ; library(survival) modelTobit &lt;- survreg(Surv(LNEXP,LNEXP&gt;0,type=&quot;left&quot;) ~ AGE+GENDER+ASIAN+BLACK+ NATIVE+NORTHEAST+MIDWEST+SOUTH+COLLEGE+HIGHSCHOOL+POOR+ FAIR+GOOD+VGOOD+MNHPOOR+ANYLIMIT+HINCOME+MINCOME+ LINCOME+NPOOR+insure, dist=&quot;gaussian&quot;, data=PersonLevel) TobitSum &lt;- summary(modelTobit) #TobitSum # TABLE 16.2 HECKMAN TWO-STAGE ALGORITHM ; # STAGE 1 - PROBIT MODEL model1S &lt;- glm(POSEXP ~ AGE+GENDER+ASIAN+BLACK+NATIVE+NORTHEAST+MIDWEST+SOUTH+ COLLEGE+HIGHSCHOOL+POOR+FAIR+GOOD+VGOOD+MNHPOOR+ANYLIMIT+ HINCOME+MINCOME+LINCOME+NPOOR+insure, binomial(link=probit),data=PersonLevel) model1SSum &lt;- summary(model1S) #model1SSum # COMPUTE INVERSE MILL&#39;S RATIO invMillRatio &lt;- dnorm(model1S$fitted.values)/pnorm(model1S$fitted.values) ordernum &lt;- seq(1,2000) PLnew &lt;- cbind(PersonLevel, invMillRatio, ordernum) # STAGE 2 model2S &lt;- lm(LNEXP ~ AGE+GENDER+ASIAN+BLACK+NATIVE+NORTHEAST+MIDWEST+SOUTH+ COLLEGE+HIGHSCHOOL+POOR+FAIR+GOOD+VGOOD+MNHPOOR+ ANYLIMIT+HINCOME+MINCOME+LINCOME+NPOOR+insure+ invMillRatio, data=PLnew) model2SSum &lt;- summary(model2S) #model2SSum # COMPUTE Heteroscedasticity-Corrected Covariance Matrix; #install.packages(&quot;car&quot;) library(car) #?hccm cov_corrected &lt;- hccm(model2S,&quot;hc1&quot;) var_corrected &lt;- diag(cov_corrected) StdErr_corrected &lt;- sqrt(var_corrected) tRatio_corrected &lt;- model2S$coefficients/StdErr_corrected summary_corrected &lt;- cbind(model2S$coefficients,StdErr_corrected,tRatio_corrected) #summary_corrected # REPLICATE RESULT GIVEN BY TABLE 16.2 ; model2SML &lt;- SummaryML(summary_corrected,2000,23) #model2SML # CREATE TABLE 16.2 ; Table162OLS &lt;- modelOlsML Table162Tobit &lt;- TobitSum$table[-c(23),] Table1622S &lt;- model2SML[-c(23),] Table162 &lt;- cbind(Table162OLS,Table162Tobit,Table1622S) Table162print &lt;- Table162[,c(1,3,4,6,8,10)] knitr::kable(Table162print, digits = 3) Estimate t_Ratio Value z Estimate t_Ratio (Intercept) -0.123 -0.525 -33.016 -8.233 2.760 0.617 AGE 0.000 0.091 -0.006 -0.118 0.001 0.129 GENDER 0.379 3.711 5.727 4.107 0.271 1.617 ASIAN -0.115 -0.459 -1.732 -0.480 -0.091 -0.480 BLACK 0.054 0.365 0.025 0.015 0.043 0.262 NATIVE 0.350 0.726 3.745 0.723 0.250 0.445 NORTHEAST 0.283 1.702 3.828 1.849 0.203 1.065 MIDWEST 0.255 1.693 3.459 1.790 0.196 1.143 SOUTH 0.146 1.133 1.805 1.056 0.117 0.937 COLLEGE -0.014 -0.089 0.628 0.329 -0.024 -0.149 HIGHSCHOOL -0.027 -0.209 -0.030 -0.019 -0.026 -0.202 POOR 2.297 7.313 13.352 4.436 1.780 1.810 FAIR -0.001 -0.004 1.354 0.528 -0.014 -0.068 GOOD 0.188 1.346 2.740 1.480 0.143 1.018 VGOOD 0.084 0.622 1.506 0.815 0.063 0.533 MNHPOOR 0.000 -0.001 -0.482 -0.211 -0.011 -0.041 ANYLIMIT 0.415 3.103 4.695 3.000 0.306 1.448 HINCOME -0.482 -2.716 -6.575 -3.035 -0.338 -1.290 MINCOME -0.309 -1.868 -4.359 -2.241 -0.210 -0.952 LINCOME -0.175 -0.976 -3.414 -1.619 -0.099 -0.438 NPOOR -0.116 -0.478 -2.274 -0.790 -0.065 -0.243 insure 0.594 4.486 8.534 4.130 0.455 2.094 Tabla 16.2 también reporta el ajuste utilizando el algoritmo de dos etapas de Heckman. El coeficiente asociado con la corrección de selección mediante la razón inversa de Mills no es estadísticamente significativo. Por lo tanto, hay un acuerdo general entre los coeficientes de OLS y los estimados utilizando el algoritmo de dos etapas. Los \\(t\\)-ratios de dos etapas fueron calculados usando errores estándar consistentes con heterocedasticidad, como se describe en la Sección 5.7.2. Aquí, observamos cierta discrepancia entre los \\(t\\)-ratios calculados con el algoritmo de Heckman y los valores de máxima verosimilitud calculados utilizando el modelo tobit. Por ejemplo, GENDER, POOR, HINCOME y MINCOME son estadísticamente significativos en el modelo tobit, pero no en el algoritmo de dos etapas. Esto resulta problemático porque ambas técnicas proporcionan estimadores consistentes siempre que se cumplan las suposiciones del modelo tobit. Por lo tanto, sospechamos de la validez de las suposiciones del modelo para estos datos; la siguiente sección presenta un modelo alternativo que resulta ser más adecuado para este conjunto de datos. 16.4 Modelo de Dos Partes Una desventaja del modelo tobit es su dependencia de la suposición de normalidad de la respuesta latente. Una segunda, y más importante, desventaja es que una única variable latente determina tanto la magnitud de la respuesta como el truncamiento. Como señaló Cragg (1971), hay muchos casos en los que el valor límite representa una elección o actividad que está separada de la magnitud. Por ejemplo, en una población de fumadores, cero cigarrillos consumidos durante una semana puede representar simplemente un límite inferior (o límite) y puede estar influenciado por el tiempo y el dinero disponibles. Sin embargo, en una población general, cero cigarrillos consumidos durante una semana puede indicar que una persona no es fumadora, una elección que podría estar influenciada por otras decisiones de estilo de vida (donde el tiempo y el dinero pueden o no ser relevantes). Como otro ejemplo, al estudiar los gastos de atención médica, un cero representa la decisión de una persona de no utilizar servicios de salud durante un período. Para muchos estudios, la cantidad de gasto en salud está fuertemente influenciada por un proveedor de salud (como un médico); la decisión de utilizar y la cantidad de servicios de salud pueden involucrar consideraciones muy diferentes. En la literatura actuarial tradicional (ver, por ejemplo, Bowers et al., 1997, Capítulo 2), el modelo de riesgo individual descompone una respuesta, típicamente un reclamo de seguro, en componentes de frecuencia (número) y severidad (monto). Específicamente, sea \\(r_i\\) una variable binaria que indica si el sujeto \\(i\\) tiene un reclamo de seguro y \\(y_i\\) describe el monto del reclamo. Entonces, el reclamo se modela como \\[ \\left( \\text{reclamo registrado}\\right)_i = r_i \\times y_i. \\] Esta es la base del modelo de dos partes, donde también utilizamos variables explicativas para comprender la influencia de cada componente. Definición. Modelo de Dos Partes Utilizar un modelo de regresión binaria con \\(r_i\\) como la variable dependiente y \\(\\mathbf{x}_{1i}\\) como el conjunto de variables explicativas. Denote el conjunto correspondiente de coeficientes de regresión como \\(\\boldsymbol{\\beta_{1}}\\). Los modelos típicos incluyen probabilidad lineal, logit y probit. Condicional a \\(r_i=1\\), especificar un modelo de regresión con \\(y_i\\) como la variable dependiente y \\(\\mathbf{x}_{2i}\\) como el conjunto de variables explicativas. Denote el conjunto correspondiente de coeficientes de regresión como \\(\\boldsymbol{\\beta_{2}}\\). Los modelos típicos incluyen regresión lineal y regresión gamma. A diferencia del tobit, en el modelo de dos partes no es necesario que el mismo conjunto de variables explicativas influya tanto en la frecuencia como en el monto de la respuesta. Sin embargo, generalmente hay superposición en los conjuntos de variables explicativas, donde las variables son miembros de ambos \\(\\mathbf{x}_{1}\\) y \\(\\mathbf{x}_{2}\\). Típicamente, se asume que \\(\\boldsymbol{\\beta_{1}}\\) y \\(\\boldsymbol{\\beta_{2}}\\) no están relacionados, de modo que la verosimilitud conjunta de los datos puede separarse en dos componentes y ejecutarse por separado, como se describe anteriormente. Ejemplo: Datos de Gasto MEPS - Continuación. Considere los datos de gasto MEPS de la Sección 16.3 utilizando un modelo probit para la frecuencia y un modelo de regresión lineal para la severidad. Tabla 16.3 muestra los resultados de utilizar todas las variables explicativas para entender su influencia en (i) la decisión de buscar atención médica (frecuencia) y (ii) la cantidad de atención médica utilizada (severidad). A diferencia del modelo tobit de la Tabla 16.2, los modelos de dos partes permiten que cada variable tenga una influencia separada en frecuencia y severidad. Para ilustrar, los resultados del modelo completo en la Tabla 16.3 muestran que COLLEGE no tiene un impacto significativo en la frecuencia pero tiene un impacto positivo fuerte en la severidad. Debido a la flexibilidad del modelo de dos partes, también se puede reducir la complejidad del modelo para cada componente eliminando variables innecesarias. Tabla 16.3 muestra un modelo reducido, donde se han eliminado las variables de edad y estado de salud mental del componente de frecuencia; y las variables regionales, educativas, estado físico e ingreso del componente de severidad. Tabla 16.3. Comparación de Modelos Completos y Reducidos de Dos Partes \\[ \\scriptsize{ \\begin{array}{l|rr|rr|rr|rr} \\hline &amp; \\text{Modelo} &amp; &amp; \\text{Modelo}&amp; &amp; \\text{Modelo}&amp; &amp;\\text{Modelo} \\\\ &amp; \\text{Completo} &amp; &amp; \\text{Completo}&amp; &amp; \\text{Reducido}&amp; &amp;\\text{ Reducido} \\\\ &amp; \\text{Frecuencia} &amp; &amp; \\text{Severidad} &amp; &amp; \\text{Frecuencia} &amp; &amp;\\text{Severidad} \\\\ &amp; \\text{Parámetro} &amp; &amp; \\text{Parámetro} &amp; &amp; \\text{Parámetro} &amp; &amp; \\text{Parámetro} \\\\ \\text{Efecto} &amp; \\text{Estimación} &amp; t\\text{-ratio} &amp; \\text{Estimación} &amp; t\\text{-ratio} &amp; \\text{Estimación} &amp; t\\text{-ratio} &amp; \\text{Estimación} &amp; t\\text{-ratio} \\\\ \\hline Intercepto &amp; -2.263 &amp; -10.015 &amp; 6.828 &amp; 13.336 &amp; -2.281 &amp; -11.432 &amp; 6.879 &amp; 14.403 \\\\ AGE &amp; -0.001 &amp; -0.154 &amp; 0.012 &amp; 1.368 &amp; &amp; &amp; 0.020 &amp; 2.437 \\\\ GENDER &amp; 0.395 &amp; 4.176 &amp; -0.104 &amp; -0.469 &amp; 0.395 &amp; 4.178 &amp; -0.102 &amp; -0.461 \\\\ ASIAN &amp; -0.108 &amp; -0.429 &amp; -0.397 &amp; -0.641 &amp; -0.108 &amp; -0.427 &amp; -0.159 &amp; -0.259 \\\\ BLACK &amp; 0.008 &amp; 0.062 &amp; 0.088 &amp; 0.362 &amp; 0.009 &amp; 0.073 &amp; 0.017 &amp; 0.072 \\\\ NATIVE &amp; 0.284 &amp; 0.778 &amp; -0.639 &amp; -0.905 &amp; 0.285 &amp; 0.780 &amp; -1.042 &amp; -1.501 \\\\ NORTHEAST &amp; 0.283 &amp; 1.958 &amp; -0.649 &amp; -2.035 &amp; 0.281 &amp; 1.950 &amp; -0.778 &amp; -2.422 \\\\ MIDWEST &amp; 0.239 &amp; 1.765 &amp; 0.016 &amp; 0.052 &amp; 0.237 &amp; 1.754 &amp; -0.005 &amp; -0.016 \\\\ SOUTH &amp; 0.132 &amp; 1.099 &amp; -0.078 &amp; -0.294 &amp; 0.130 &amp; 1.085 &amp; -0.022 &amp; -0.081 \\\\ \\hline COLLEGE &amp; 0.048 &amp; 0.356 &amp; -0.597 &amp; -2.066 &amp; 0.049 &amp; 0.362 &amp; -0.470 &amp; -1.743 \\\\ HIGHSCHOOL &amp; 0.002 &amp; 0.017 &amp; -0.415 &amp; -1.745 &amp; 0.003 &amp; 0.030 &amp; -0.256 &amp; -1.134 \\\\ \\hline POOR &amp; 0.955 &amp; 4.576 &amp; 0.597 &amp; 1.594 &amp; 0.939 &amp; 4.805 &amp; &amp; \\\\ FAIR &amp; 0.087 &amp; 0.486 &amp; -0.211 &amp; -0.527 &amp; 0.079 &amp; 0.450 &amp; &amp; \\\\ GOOD &amp; 0.184 &amp; 1.422 &amp; 0.145 &amp; 0.502 &amp; 0.182 &amp; 1.412 &amp; &amp; \\\\ VGOOD &amp; 0.095 &amp; 0.736 &amp; 0.373 &amp; 1.233 &amp; 0.094 &amp; 0.728 &amp; &amp; \\\\ MNHPOOR &amp; -0.027 &amp; -0.164 &amp; -0.176 &amp; -0.579 &amp; &amp; &amp; -0.177 &amp; -0.640 \\\\ ANYLIMIT &amp; 0.318 &amp; 2.941 &amp; 0.235 &amp; 0.981 &amp; 0.311 &amp; 3.022 &amp; 0.245 &amp; 1.052 \\\\ \\hline HINCOME &amp; -0.468 &amp; -3.131 &amp; 0.490 &amp; 1.531 &amp; -0.470 &amp; -3.224 &amp; &amp; \\\\ MINCOME &amp; -0.314 &amp; -2.318 &amp; 0.472 &amp; 1.654 &amp; -0.314 &amp; -2.345 &amp; &amp; \\\\ LINCOME &amp; -0.241 &amp; -1.626 &amp; 0.550 &amp; 1.812 &amp; -0.241 &amp; -1.633 &amp; &amp; \\\\ NPOOR &amp; -0.145 &amp; -0.716 &amp; 0.067 &amp; 0.161 &amp; -0.146 &amp; -0.721 &amp; &amp; \\\\ INSURE &amp; 0.580 &amp; 4.154 &amp; 1.293 &amp; 3.944 &amp; 0.579 &amp; 4.147 &amp; 1.397 &amp; 4.195 \\\\ \\hline \\text{Escala } \\sigma^2&amp; &amp; &amp; 1.249 &amp; &amp; &amp; &amp; 1.333 &amp; \\\\ \\hline \\end{array} } \\] Código R para Generar la Tabla 16.3 # Table 16.3 #Hexpend &lt;- read.csv(&quot;CSVData/HealthExpend.csv&quot;, header=TRUE) #Hexpend &lt;- read.csv(&quot;../../CSVData/HealthExpend.csv&quot;, header=TRUE) SummaryML &lt;- function(coefficients, n, k) { Estimate &lt;- coefficients[,1] Std_Error &lt;- coefficients[,2] * sqrt((n-k)/n) t_Ratio &lt;- coefficients[,3] * sqrt(n/(n-k)) coeffML &lt;- cbind(Estimate, Std_Error, t_Ratio) return(coeffML) } # TWO-PART MODEL - PART ONE; modelPart1 &lt;- glm(POSEXP ~ AGE+GENDER+ASIAN+BLACK+NATIVE+NORTHEAST+MIDWEST+ SOUTH+COLLEGE+HIGHSCHOOL+POOR+FAIR+GOOD+VGOOD+MNHPOOR+ ANYLIMIT+HINCOME+MINCOME+LINCOME+NPOOR+insure, binomial(link=probit),data=PersonLevel,na.action=na.pass) modelPart1Sum &lt;- summary(modelPart1) #modelPart1Sum # TWO-PART MODEL - PART TWO; modelPart2 &lt;- glm(LNEXP ~ AGE+GENDER+ASIAN+BLACK+NATIVE+NORTHEAST+MIDWEST+ SOUTH+COLLEGE+HIGHSCHOOL+POOR+FAIR+GOOD+VGOOD+MNHPOOR+ ANYLIMIT+HINCOME+MINCOME+LINCOME+NPOOR+insure, family=gaussian(link=&quot;identity&quot;),data=PersonLevelPOSEXP) modelPart2Sum &lt;- summary(modelPart2) modelPart2ML &lt;- SummaryML(modelPart2Sum$coefficients,157,22) #modelPart2ML # TWO-PART MODEL - PART ONE - INCLUDING ONLY IMPORTANT VARIABLES; reducedPart1 &lt;- glm(POSEXP ~ GENDER+ASIAN+BLACK+NATIVE+NORTHEAST+MIDWEST+ SOUTH+COLLEGE+HIGHSCHOOL+POOR+FAIR+GOOD+VGOOD+ANYLIMIT+ HINCOME+MINCOME+LINCOME+NPOOR+insure, binomial(link=probit),data=PersonLevel) reducedPart1Sum &lt;- summary(reducedPart1) #reducedPart1Sum # TWO-PART MODEL - PART TWO - INCLUDING ONLY IMPORTANT VARIABLES; reducedPart2 &lt;- glm(LNEXP ~ AGE+GENDER+ASIAN+BLACK+NATIVE+NORTHEAST+MIDWEST+ SOUTH+COLLEGE+HIGHSCHOOL+MNHPOOR+ANYLIMIT+insure, gaussian,data=PersonLevelPOSEXP) reducedPart2Sum &lt;- summary(reducedPart2) modelPart2ML &lt;- SummaryML(reducedPart2Sum$coefficients,157,14) #modelPart2ML Modelo Tobit Tipo II Para conectar los modelos tobit y de dos partes, asumamos que la frecuencia está representada por un modelo probit y usemos \\[ r_i^{\\ast}=\\mathbf{x}_{1i}^{\\prime}\\boldsymbol \\beta_{1}+\\eta_{1i} \\] como la tendencia latente a ser observada. Definimos \\(r_i=\\mathrm{I}\\left( r_i^{\\ast}&gt;0\\right)\\) como la variable binaria que indica que se ha observado un monto. Para el componente de severidad, definimos \\[ y_i^{\\ast}=\\mathbf{x}_{2i}^{\\prime}\\boldsymbol \\beta_{1}+\\eta_{2i} \\] como la variable latente de cantidad. La cantidad “observada” es \\[ y_i=\\left\\{ \\begin{array}{ll} y_i^{\\ast} &amp; \\mathrm{si~}r_i=1 \\\\ 0 &amp; \\mathrm{si~}r_i=0 \\end{array} \\right. . \\] Debido a que las respuestas están censuradas, el analista está al tanto del sujeto \\(i\\) y tiene información de covariables incluso cuando \\(r_i = 0\\). Si \\(\\mathbf{x}_{1i}=\\mathbf{x}_{2i}\\), \\(\\boldsymbol \\beta_{1}=\\boldsymbol \\beta _{2}\\) y \\(\\eta_{1i}=\\eta_{2i}\\), entonces esto es el marco tobit con \\(d_i=0\\). Si \\(\\boldsymbol\\beta_{1}\\) y \\(\\boldsymbol \\beta_{2}\\) no están relacionadas y si \\(\\eta_{1i}\\) y \\(\\eta_{2i}\\) son independientes, entonces esto es el marco de dos partes. Para el marco de dos partes, la verosimilitud de las respuestas observadas \\(\\left\\{ r_i,y_i\\right\\}\\) está dada por \\[\\begin{equation} L=\\prod\\limits_{i=1}^{n}\\left\\{ \\left( p_i\\right) ^{r_i}\\left( 1-p_i\\right) ^{1-r_i}\\right\\} \\prod\\limits_{r_i=1}\\mathrm{\\phi } \\left( \\frac{y_i-\\mathbf{x}_{2i}^{\\prime} \\boldsymbol \\beta_{2}}{ \\sigma_{\\eta 2}}\\right) , \\tag{16.5} \\end{equation}\\] donde \\(p_i=\\Pr \\left( r_i=1\\right)\\) \\(=\\Pr \\left( \\mathbf{x}_{1i}^{\\mathbf{ \\prime }}\\boldsymbol \\beta_{1}+\\eta_{1i}&gt;0\\right)\\) \\(=1-\\Phi \\left( -\\mathbf{x}_{1i}^{\\prime}\\boldsymbol \\beta_{1}\\right)\\) \\(=\\Phi \\left( \\mathbf{x} _{1i}^{\\prime}\\boldsymbol \\beta_{1}\\right)\\). Asumiendo que \\(\\boldsymbol \\beta_1\\) y \\(\\boldsymbol \\beta_2\\) no están relacionadas, uno puede maximizar por separado estas dos partes de la función de verosimilitud. En algunos casos, tiene sentido asumir que los componentes de frecuencia y severidad están relacionados. El modelo tobit considera una relación perfecta (con \\(\\eta_{1i}=\\eta_{2i}\\)) mientras que los modelos de dos partes asumen independencia. Para un modelo intermedio, el modelo tobit tipo II permite una correlación no nula entre \\(\\eta_{1i}\\) y \\(\\eta_{2i}\\). Ver Amemiya (1985) para más detalles. Hsiao et al. (1990) proporcionan una aplicación del modelo tobit tipo II a la cobertura de colisiones en Canadá para automóviles privados. 16.5 Modelo de Pérdidas Agregadas Ahora consideramos modelos de dos partes donde la frecuencia puede exceder uno. Por ejemplo, si estamos rastreando accidentes automovilísticos, un asegurado puede tener más de un accidente en un año. Como otro ejemplo, podríamos estar interesados en los reclamos de una ciudad o un estado y esperar muchos reclamos por unidad gubernamental. Para establecer la notación, para cada {\\(i\\)}, las respuestas observables consisten en: \\(N_i~-\\) el número de reclamos (eventos), y \\(y_{ij},~j=1,...,N_i~-\\) el monto de cada reclamo (pérdida). Por convención, el conjunto \\(\\{y_{ij}\\}\\) está vacío cuando \\(N_i=0\\). Si uno usa \\(N_i\\) como una variable binaria, este marco se reduce a la configuración de dos partes. Aunque tenemos información detallada sobre las pérdidas por evento, a menudo el interés se centra en las pérdidas agregadas, \\(S_i=y_{i1}+...+y_{i,N_i}\\). En el modelado actuarial tradicional, se asume que la distribución de las pérdidas es, condicional a la frecuencia \\(N_i\\), idéntica e independiente entre réplicas \\(~j\\). Esta representación se conoce como el modelo de riesgo colectivo, ver, por ejemplo, Klugman et al. (2008). También mantenemos esta suposición. Los datos típicamente están disponibles en dos formas: \\(\\{N_i,y_{i1},...,y_{i,N_i}\\}\\), por lo que está disponible información detallada sobre cada reclamo. Por ejemplo, al examinar reclamos de automóviles personales, están disponibles las pérdidas para cada reclamo. Sea \\(\\mathbf{y}_i=\\left( y_{i1},...,y_{i,N_i}\\right) ^{\\prime}\\)  el vector de pérdidas individuales. \\(\\{N_i,S_i\\}\\), por lo que solo están disponibles las pérdidas agregadas. Por ejemplo, al examinar pérdidas a nivel de ciudad, solo están disponibles las pérdidas agregadas. Estamos interesados en ambas formas. Dado que hay múltiples respuestas (eventos) por sujeto {\\(i\\)}, uno podría abordar el análisis utilizando modelos multinivel como se describe, por ejemplo, en Raudenbush y Bryk (2002). A diferencia de una estructura multinivel, consideramos datos donde el número de eventos es aleatorio y deseamos modelarlo estocásticamente, utilizando así un marco alternativo. Cuando solo \\(\\{S_i\\}\\) está disponible, el GLM Tweedie introducido en la Sección 13.6 puede ser utilizado. Para ver cómo modelar estos datos, consideremos la primera forma de datos. Suprimiendo el subíndice \\(\\{i\\}\\), descomponemos la distribución conjunta de las variables dependientes como: \\[\\begin{eqnarray*} \\mathrm{f}\\left( N,\\mathbf{y}\\right) &amp;=&amp;\\mathrm{f}\\left( N\\right) ~\\times ~ \\mathrm{f}\\left( \\mathbf{y|}N\\right) \\\\ \\text{conjunta} &amp;=&amp;\\text{frecuencia}~\\times ~\\text{severidad condicional,} \\end{eqnarray*}\\] donde \\(\\mathrm{f}\\left( N,\\mathbf{y}\\right)\\) denota la distribución conjunta de \\(\\left( N,\\mathbf{y}\\right)\\). Esta distribución conjunta es igual al producto de los dos componentes: frecuencia de reclamos: \\(\\mathrm{f}\\left( N\\right)\\) denota la probabilidad de tener \\(N\\) reclamos; y severidad condicional: \\(\\mathrm{f}\\left( \\mathbf{y|}N\\right)\\) denota la densidad condicional del vector de reclamos \\(\\mathbf{y}\\) dado \\(N\\). Representamos los componentes de frecuencia y severidad del modelo de pérdidas agregadas de la siguiente manera: Definición. Modelo de Pérdidas Agregadas I Usar un modelo de regresión para conteos con \\(N_i\\) como la variable dependiente y \\(\\mathbf{x}_{1i}\\) como el conjunto de variables explicativas. Denotamos el conjunto correspondiente de coeficientes de regresión como \\(\\boldsymbol \\beta_{1}\\). Los modelos típicos incluyen los modelos de Poisson y binomial negativa. Condicional a que \\(N_i&gt;0\\), usar un modelo de regresión con \\(y_{ij}\\) como la variable dependiente y \\(\\mathbf{x}_{2i}\\) como el conjunto de variables explicativas. Denotamos el conjunto correspondiente de coeficientes de regresión como \\(\\boldsymbol \\beta_{2}\\). Los modelos típicos incluyen la regresión lineal, la regresión gamma y los modelos lineales mixtos. Para los modelos lineales mixtos, se utiliza un intercepto específico para cada sujeto para tener en cuenta la heterogeneidad entre sujetos. Para modelar la segunda forma de datos, el enfoque es similar. El modelo de datos de conteo en el paso 1 no cambiará. Sin embargo, el modelo de regresión en el paso 2 usará \\(S_i\\) como la variable dependiente. Debido a que la variable dependiente es la suma sobre \\(N_i\\) réplicas independientes, puede ser necesario permitir que la variabilidad dependa de \\(N_i\\). Ejemplo: Datos de Gastos de MEPS - Continuación. Para obtener una idea de las observaciones empíricas de la frecuencia de reclamos, presentamos la frecuencia general de reclamos. Según esta tabla, hubo un total de 2,000 observaciones de las cuales el 92.15% no tuvieron reclamos. Hay un total de 203 (\\(=1\\times 130+2\\times 19+3\\times 2+4\\times 3+5\\times 2+6\\times 0+7\\times 1)\\) reclamos. Frecuencia de Reclamos \\[ \\small{ \\begin{array}{l|ccccccccc} \\hline \\text{Cuenta} &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; \\text{Total }\\\\ \\hline \\text{Número} &amp; 1,843 &amp; 130 &amp; 19 &amp; 2 &amp; 3 &amp; 2 &amp; 0 &amp; 1 &amp; 2,000 \\\\ \\text{Porcentaje} &amp; 92.15 &amp; 6.50 &amp; 0.95 &amp; 0.10 &amp; 0.15 &amp; 0.10 &amp; 0.00 &amp; 0.10 &amp; 100.00 \\\\ \\hline \\end{array} } \\] Tabla 16.4 resume los ajustes de los coeficientes de regresión usando el modelo de binomial negativa. Los resultados son comparables a los modelos probit ajustados en Tabla 16.3, donde muchas de las covariables son predictores estadísticamente significativos de la frecuencia de reclamos. Este modelo ajustado de frecuencia se basa en \\(n=2,000\\) personas. Los modelos de severidad ajustados en Tabla 16.4 se basan en \\(n_{1}+...+n_{2000}=203\\) reclamos. El modelo de regresión gamma se basa en un enlace logarítmico: \\[ \\mu_i=\\exp \\left(\\mathbf{x}_i^{\\prime}\\boldsymbol \\beta_2 \\right). \\] Tabla 16.4 muestra que los resultados de ajustar un modelo de regresión ordinaria son similares a los de ajustar el modelo de regresión gamma. Son similares en el sentido de que el signo y la significancia estadística de los coeficientes para cada variable son comparables. Como se discute en el Capítulo 13, la ventaja del modelo de regresión ordinaria es su relativa simplicidad, lo que facilita su implementación e interpretación. En contraste, el modelo de regresión gamma puede ser un mejor modelo para ajustar distribuciones con colas largas como los gastos médicos. Tabla 16.4. Modelos de Pérdidas Agregadas \\[ \\scriptsize{ \\begin{array}{l|rr|rr|rr} \\hline &amp; \\text{Binomial} &amp;\\text{Negativa} &amp; \\text{Regresión} &amp;\\text{Ordinaria} &amp; \\text{Regresión} &amp; \\text{Gamma} \\\\ &amp; \\text{Frecuencia} &amp; &amp; \\text{Severidad} &amp; &amp;\\text{Severidad} \\\\ &amp; \\text{Parámetro} &amp; &amp; \\text{Parámetro} &amp; &amp; \\text{Parámetro} &amp; \\\\ \\text{Efecto} &amp; \\text{Estimación} &amp; t\\text{-ratio} &amp; \\text{Estimación} &amp; t\\text{-ratio} &amp; \\text{Estimación} &amp; t\\text{-ratio} \\\\ \\hline Intercepto &amp; -4.214 &amp; -9.169 &amp; 7.424 &amp; 15.514 &amp; 8.557 &amp; 20.521 \\\\ AGE &amp; -0.005 &amp; -0.756 &amp; -0.006 &amp; -0.747 &amp; -0.011 &amp; -1.971 \\\\ GENDER &amp; 0.617 &amp; 3.351 &amp; -0.385 &amp; -1.952 &amp; -0.826 &amp; -4.780 \\\\ ASIAN &amp; -0.153 &amp; -0.306 &amp; -0.340 &amp; -0.588 &amp; -0.711 &amp; -1.396 \\\\ BLACK &amp; 0.144 &amp; 0.639 &amp; 0.146 &amp; 0.686 &amp; -0.058 &amp; -0.297 \\\\ NATIVE &amp; 0.445 &amp; 0.634 &amp; -0.331 &amp; -0.465 &amp; -0.512 &amp; -0.841 \\\\ NORTHEAST &amp; 0.492 &amp; 1.683 &amp; -0.547 &amp; -1.792 &amp; -0.418 &amp; -1.602 \\\\ MIDWEST &amp; 0.619 &amp; 2.314 &amp; 0.303 &amp; 1.070 &amp; 0.589 &amp; 2.234 \\\\ SOUTH &amp; 0.391 &amp; 1.603 &amp; 0.108 &amp; 0.424 &amp; 0.302 &amp; 1.318 \\\\ \\hline COLLEGE &amp; 0.023 &amp; 0.089 &amp; -0.789 &amp; -2.964 &amp; -0.826 &amp; -3.335 \\\\ HIGHSCHOOL &amp; -0.085 &amp; -0.399 &amp; -0.722 &amp; -3.396 &amp; -0.742 &amp; -4.112 \\\\ \\hline POOR &amp; 1.927 &amp; 5.211 &amp; 0.664 &amp; 1.964 &amp; 0.299 &amp; 0.989 \\\\ FAIR &amp; 0.226 &amp; 0.627 &amp; -0.188 &amp; -0.486 &amp; 0.080 &amp; 0.240 \\\\ GOOD &amp; 0.385 &amp; 1.483 &amp; 0.223 &amp; 0.802 &amp; 0.185 &amp; 0.735 \\\\ VGOOD &amp; 0.348 &amp; 1.349 &amp; 0.429 &amp; 1.511 &amp; 0.184 &amp; 0.792 \\\\ MNHPOOR &amp; -0.177 &amp; -0.583 &amp; -0.221 &amp; -0.816 &amp; -0.470 &amp; -1.877 \\\\ ANYLIMIT &amp; 0.714 &amp; 3.499 &amp; 0.579 &amp; 2.720 &amp; 0.792 &amp; 4.171 \\\\ \\hline HINCOME &amp; -0.622 &amp; -2.139 &amp; 0.723 &amp; 2.517 &amp; 0.557 &amp; 2.290 \\\\ MINCOME &amp; -0.482 &amp; -1.831 &amp; 0.720 &amp; 2.768 &amp; 0.694 &amp; 3.148 \\\\ LINCOME &amp; -0.460 &amp; -1.611 &amp; 0.631 &amp; 2.241 &amp; 0.889 &amp; 3.693 \\\\ NPOOR &amp; -0.465 &amp; -1.131 &amp; -0.056 &amp; -0.135 &amp; 0.217 &amp; 0.619 \\\\ INSURE &amp; 1.312 &amp; 4.207 &amp; 1.500 &amp; 4.551 &amp; 1.380 &amp; 4.912 \\\\ \\hline Dispersión &amp; 2.177 &amp; &amp; 1.314 &amp; &amp; 1.131 &amp; \\\\ \\hline \\end{array} } \\] 16.6 Lecturas Adicionales y Referencias Propiedad y Accidentes Existe una rica literatura sobre el modelado de la distribución conjunta de frecuencia y severidad de reclamos de seguros de automóviles. Para distinguir este modelado de las aplicaciones de teoría de riesgos clásica (ver, por ejemplo, Klugman et al., 2008), nos enfocamos en casos donde hay variables explicativas disponibles, como características del titular de la póliza. Ha habido un interés sustancial en el modelado estadístico de la frecuencia de reclamos, pero la literatura sobre el modelado de la severidad de reclamos, especialmente junto con la frecuencia de reclamos, es menos extensa. Una posible explicación, señalada por Coutts (1984), es que la mayor parte de la variación en la experiencia general de reclamos puede atribuirse a la frecuencia de reclamos (al menos cuando la inflación era pequeña). Coutts (1984) también señala que el primer artículo que analizó la frecuencia y severidad de reclamos por separado parece ser Kahane y Levy (1975). Brockman y Wright (1992) ofrecen una visión temprana de cómo el modelado estadístico de reclamos y severidad puede ser útil para la tarificación de seguros de automóviles. Por conveniencia computacional, se enfocaron en variables categóricas de tarificación para formar celdas que podrían usarse con formularios tradicionales de suscripción de seguros. Renshaw (1994) muestra cómo se pueden usar modelos lineales generalizados para analizar tanto la frecuencia como la severidad basándose en datos a nivel individual del titular de la póliza. Hsiao et al. (1990) observan el “exceso” de ceros en los datos de reclamos de titulares de pólizas (debido a la ausencia de reclamos) y comparan y contrastan los modelos Tobit, de dos partes y de ecuaciones simultáneas, basándose en el trabajo de Weisberg y Tomberlin (1982) y Weisberg et al. (1984). Todos estos artículos usan datos agrupados, no datos individuales como en este capítulo. A nivel de titulares individuales de pólizas, Frangos y Vrontos (2001) examinaron un modelo de frecuencia y severidad de reclamos, utilizando distribuciones binomial negativa y Pareto, respectivamente. Utilizaron su modelo estadístico para desarrollar primas basadas en experiencia (bonus-malus). Pinquet (1997, 1998) proporciona un enfoque estadístico más moderno, ajustando no solo datos transversales sino también siguiendo a los titulares de pólizas a lo largo del tiempo. Pinquet estaba interesado en dos líneas de negocio, reclamos con culpa y sin culpa respecto a un tercero. Para cada línea, Pinquet hipotetizó un componente de frecuencia y severidad que se permitía correlacionar entre sí. En particular, la distribución de frecuencia de reclamos se asumió como Poisson bivariada. Las severidades se modelaron utilizando distribuciones lognormales y gamma. Atención Médica El modelo de dos partes se volvió prominente en la literatura de atención médica tras su adopción por parte de los investigadores del Experimento de Seguro de Salud de Rand (Duan et al, 1983, Manning et al, 1987). Utilizaron el modelo de dos partes para analizar el efecto del costo compartido del seguro de salud en la utilización y los gastos de atención médica debido a la estrecha semejanza de la demanda de atención médica con los dos procesos de toma de decisiones. Es decir, el monto de los gastos en atención médica es en gran medida independiente de la decisión del individuo de buscar tratamiento. Esto se debe a que los médicos, como agentes de los pacientes (principal), tienden a decidir la intensidad de los tratamientos según lo sugiere el modelo de principal-agente de Zweifel (1981). El modelo de dos partes se ha convertido en una herramienta ampliamente utilizada en la literatura de atención médica a pesar de algunas críticas. Por ejemplo, Maddala (1985) argumentó que el modelado de dos partes no es apropiado para datos no experimentales debido a que la auto-selección de individuos en diferentes planes de seguro de salud es un problema. (En el Experimento de Seguro de Salud de Rand, la auto-selección no fue un problema porque los participantes fueron asignados aleatoriamente a planes de seguro de salud). Consulte Jones (2000) y Mullahy (1998) para obtener una visión general. Los modelos de dos partes siguen siendo atractivos para modelar el uso de atención médica porque brindan información sobre los determinantes de la iniciación y el nivel de uso de atención médica. La decisión de utilizar la atención médica por parte de los individuos está relacionada principalmente con características personales, mientras que el costo por usuario puede estar más relacionado con características del proveedor de atención médica. Código R para Generar Capítulos y Figuras Haga Clic Aquí para Código R Adicional del Capítulo 16 Referencias del Capítulo Amemiya, T. (1985). Advanced Econometrics. Harvard University Press, Cambridge, MA. Boucher, Jean-Philippe, Michel Denuit, and Montserratt Guillén (2006). Risk classification for claim counts: A comparative analysis of various zero-inflated mixed Poisson and hurdle models. Working paper. Bowers, Newton L., Hans U. Gerber, James C. Hickman, Donald A. Jones, and Cecil J. Nesbitt (1997). Actuarial Mathematics. Society of Actuaries, Schaumburg, IL. Brockman, M.J. and T.S. Wright. (1992). Statistical motor rating: making effective use of your data. Journal of the Institute of Actuaries 119, 457-543. Cameron, A. Colin and Pravin K. Trivedi. (1998) Regression Analysis of Count Data. Cambridge University Press, Cambridge. Coutts, S.M. (1984). Motor insurance rating, an actuarial approach. Journal of the Institute of Actuaries 111, 87-148. Cragg, John G. (1971). Some statistical models for limited dependent variables with application to the demand for durable goods. Econometrica 39(5), 829-844. Duan, Naihua, Willard G. Manning, Carl N. Morris, and Joseph P. Newhouse (1983). A comparison of alternative models for the demand for medical care. Journal of Business and Economics 1(2), 115-126. Frangos, Nicholas E. and Spyridon D. Vrontos (2001). Design of optimal bonus-malus systems with a frequency and a severity component on an individual basis in automobile insurance. ASTIN Bulletin 31(1), 1-22. Goldberger, Arthur S. (1964). Econometric Theory. John Wiley and Sons, New York. Heckman, James J. (1976). The common structure of statistical models of truncation, sample selection and limited dependent variables, and a simple estimator for such models. Ann. Econ. Soc. Meas. 5, 475-492. Hsiao, Cheng, Changseob Kim, and Grant Taylor (1990). A statistical perspective on insurance rate-making. Journal of Econometrics 44, 5-24. Jones, Andrew M. (2000). Health econometrics. Chapter 6 of the Handbook of Health Economics, Volume 1. Edited by Antonio J. Culyer, and Joseph P. Newhouse, Elsevier, Amsterdam. 265-344. Kahane, Yehuda and Haim Levy (1975). Regulation in the insurance industry: determination of premiums in automobile insurance. Journal of Risk and Insurance 42, 117-132. Klugman, Stuart A, Harry H. Panjer, and Gordon E. Willmot (2008). Loss Models: From Data to Decisions. John Wiley &amp; Sons, Hoboken, New Jersey. Maddala, G. S. (1985). A survey of the literature on selectivity as it pertains to health care markets. Advances in Health Economics and Health Services Research 6, 3-18. Mullahy, John (1998). Much ado about two: Reconsidering retransformation and the two-part model in health econometrics. Journal of Health Economics 17, 247-281. Manning, Willard G., Joseph P. Newhouse, Naihua Duan, Emmett B. Keeler, Arleen Leibowitz, and M. Susan Marquis (1987). Health insurance and the demand for medical care: Evidence from a randomized experiment. American Economic Review 77(3), 251-277. Pinquet, Jean (1997). Allowance for cost of claims in bonus-malus systems. ASTIN Bulletin 27(1), 33-57. Pinquet, Jean (1998). Designing optimal bonus-malus systems from different types of claims. ASTIN Bulletin 28(2), 205-229. Raudenbush, Steven W. and Anthony S. Bryk (2002). Hierarchical Linear Models: Applications and Data Analysis Methods. (Second Edition). London: Sage. Tobin, James (1958). Estimation of relationships for limited dependent variables. Econometrica 26, 24-36. Weisberg, Herbert I. and Thomas J. Tomberlin (1982). A statistical perspective on actuarial methods for estimating pure premiums from cross-classified data. Journal of Risk and Insurance 49, 539-563. Weisberg, Herbert I., Thomas J. Tomberlin, and Sangit Chatterjee (1984). Predicting insurance losses under cross-classification: A comparison of alternative approaches. Journal of Business &amp; Economic Statistics 2(2), 170-178. Zweifel, P. (1981). Supplier-induced demand in a model of physician behavior. In Health, Economics and Health Economics, pages 245-267. Edited by J. van der Gaag and M. Perlman, North-Holland, Amsterdam. 16.7 Ejercicios 16.1 Suponga que \\(y\\) sigue una distribución normal con media \\(\\mu\\) y varianza \\(\\sigma^2\\). Sean \\(\\mathrm{\\phi (.)}\\) y \\(\\Phi (.)\\) la densidad y la función de distribución acumulativa normales estándar, respectivamente. Defina \\(\\mathrm{h} (d) = \\mathrm{\\phi}(d)\\ \\mathrm{/} \\left( 1-\\Phi (d)\\right)\\), una tasa de riesgo (hazard rate). Sea \\(d\\) una constante conocida y \\(d_s=(d-\\mu )/\\sigma\\) la versión estandarizada. Determine la densidad de \\(y\\), condicionada a \\(\\{y&gt;d\\}\\). Demuestre que \\(\\mathrm{E}\\left( y|y&gt;d\\right) = \\mu + \\sigma\\mathrm{h}( d_s).\\) Demuestre que \\(\\mathrm{E\\ }\\left( y|y\\leq d\\right) =\\mu -\\sigma\\mathrm{\\phi}(d)\\ \\mathrm{/} \\Phi (d).\\) Demuestre que \\(\\mathrm{Var}\\left( y|y&gt;d\\right) =\\sigma \\left(1-\\delta \\left( d_s\\right) \\right)\\), donde \\(\\delta \\left( d\\right)=\\mathrm{h} \\left( d\\right) \\left( \\mathrm{h} \\left( d\\right)-d\\right) .\\) Demuestre que \\(\\mathrm{E\\ }\\max \\left( y,d\\right) =\\left( \\mu +\\sigma\\mathrm{h} \\left( d_s\\right) \\right) \\left( 1-\\Phi (d_s)\\right)+d\\Phi (d_s).\\) Demuestre que \\(\\mathrm{E~\\min }\\left( y,d\\right) =\\mu +d-\\left(\\left( \\mu +\\sigma \\mathrm{h} \\left( d_s\\right) \\right) \\left(1-\\Phi (d_s)\\right) +d\\Phi (d_s)\\right) .\\) 16.2 Verifique la log-verosimilitud en la ecuación (16.4) para el modelo tobit. 16.3 Verifique la log-verosimilitud en la ecuación (16.5) para el modelo de dos partes. 16.4 Derive la log-verosimilitud para el modelo tobit tipo dos. Demuestre que su log-verosimilitud se reduce a la ecuación (16.5) en el caso de términos de perturbación no correlacionados. "],["C17Fat.html", "Capítulo 17 Modelos de Regresión con Colas Gruesas 17.1 Introducción 17.2 Transformaciones 17.3 Modelos Lineales Generalizados 17.4 Distribuciones Generalizadas 17.5 Regresión por Cuantiles 17.6 Modelos de Valores Extremos 17.7 Lecturas Adicionales y Referencias 17.8 Ejercicios", " Capítulo 17 Modelos de Regresión con Colas Gruesas Vista Previa del Capítulo. Al modelar cantidades financieras, estamos tan interesados en los valores extremos como en el centro de la distribución; estos valores extremos pueden representar las reclamaciones, ganancias o ventas más inusuales. Los actuarios frecuentemente encuentran situaciones donde los datos presentan “colas gruesas,” lo que significa que los valores extremos en los datos son más probables que en los datos normalmente distribuidos. La regresión tradicional se centra en el centro de la distribución y minimiza la importancia de los valores extremos. En contraste, el enfoque de este capítulo es sobre toda la distribución. Este capítulo examina cuatro técnicas para el análisis de regresión de datos con colas gruesas: transformaciones, modelos lineales generalizados, distribuciones más generales y regresión cuantílica. 17.1 Introducción Los actuarios frecuentemente encuentran situaciones donde los datos presentan “colas gruesas,” lo que significa que los valores extremos en los datos son más probables que en los datos normalmente distribuidos. Estas distribuciones pueden describirse como “gruesas,” “pesadas,” “anchas” o “largas” en comparación con la distribución normal. (La Sección 17.3.1 será más precisa sobre qué constituye una “cola gruesa.”) En finanzas, por ejemplo, las teorías de precios de activos CAPM y APT asumen retornos de activos normalmente distribuidos. Sin embargo, las distribuciones empíricas de los retornos de activos financieros sugieren distribuciones con colas gruesas en lugar de distribuciones normales, como asumen las teorías de precios (ver, por ejemplo, Rachev, Menn y Fabozzi, 2005). En el área de la salud, los datos con colas gruesas también son comunes. Por ejemplo, los resultados de interés como el número de días de hospitalización o los gastos hospitalarios suelen ser asimétricos a la derecha y de colas pesadas debido a unos pocos pacientes de alto costo (Basu, Manning y Mullahy, 2004). Los actuarios también analizan regularmente datos con colas gruesas en el seguro de no vida (Klugman, Panjer y Willmot, 2008). Como ocurre con cualquier otro conjunto de datos, el resultado de interés puede estar relacionado con otros factores, por lo que el análisis de regresión es de interés. Sin embargo, emplear rutinas de regresión habituales sin abordar la naturaleza de las colas gruesas puede conducir a serias dificultades. Los coeficientes de regresión pueden expresarse como sumas ponderadas de las variables dependientes. Por lo tanto, los coeficientes pueden ser influenciados indebidamente por observaciones extremas. Debido a que la distribución tiene colas gruesas, las reglas generales de aproximación (muestras grandes) a la normalidad de los estimadores de parámetros ya no se aplican. Por ejemplo, los \\(t\\)-ratios estándar y los valores-\\(p\\) asociados con los estimadores de regresión pueden no ser indicadores significativos de significancia estadística. Las rutinas de regresión usuales minimizan una función de pérdida de error cuadrático. Para algunos problemas, nos preocupan más los errores en una dirección (ya sean pequeños o grandes), no una función simétrica. Los valores grandes en el conjunto de datos pueden ser los más importantes en un sentido financiero, por ejemplo, un gasto extremadamente alto al examinar costos médicos. Aunque atípico, no es una observación que deseemos descuidar ni desestimar simplemente porque no se ajusta al modelo de regresión basado en la normalidad. Este capítulo describe cuatro enfoques básicos para manejar datos de regresión con colas gruesas: transformaciones de la variable dependiente, modelos lineales generalizados, modelos que utilizan distribuciones positivas más flexibles, como la gamma generalizada, y modelos de regresión cuantílica. Las Secciones 17.2-17.5 abordan cada enfoque. Otro campo de la estadística dedicado al análisis del comportamiento en las colas es conocido como “estadística de valores extremos.” Este campo se centra en modelar el comportamiento en las colas, en gran medida a expensas de ignorar el resto de la distribución. En contraste, la regresión tradicional se enfoca en el centro de la distribución y minimiza la importancia de los valores extremos. Para las cantidades financieras, estamos tan interesados en los extremos como en el centro de la distribución; estos valores extremos pueden representar las reclamaciones, ganancias o ventas más inusuales. El enfoque de este capítulo es sobre toda la distribución. La modelación de regresión dentro de la estadística de valores extremos es un tema que apenas ha comenzado a recibir atención seria; la Sección 17.6 proporciona un breve resumen. 17.2 Transformaciones Como hemos visto a lo largo de este texto, el enfoque más utilizado para manejar datos con colas gruesas es simplemente transformar la variable dependiente. Como práctica habitual, los analistas toman una transformación logarítmica de \\(y\\) y luego utilizan mínimos cuadrados ordinarios en \\(\\ln (y)\\). Aunque esta técnica no siempre es apropiada, ha resultado efectiva para un sorprendente número de conjuntos de datos. Esta sección resume lo que hemos aprendido sobre transformaciones y proporciona algunas herramientas adicionales que pueden ser útiles en ciertas aplicaciones. La Sección 1.3 introdujo la idea de las transformaciones y mostró cómo las transformaciones de potencia podían usarse para simetrizar una distribución. Las transformaciones de potencia, como \\(y^{\\lambda}\\), pueden “ajustar” los valores extremos para que ninguna observación tenga un efecto indebido en los estimadores de parámetros. Además, las reglas generales para la aproximación a la normalidad de los estimadores de parámetros son más aplicables cuando los datos son aproximadamente simétricos (en comparación con datos sesgados). Sin embargo, existen tres principales desventajas en las transformaciones. La primera es que puede ser difícil interpretar los coeficientes de regresión resultantes. Una de las principales razones por las cuales introdujimos la transformación logarítmica natural en la Sección 3.2.2 fue su capacidad para proporcionar interpretaciones de los coeficientes de regresión como cambios proporcionales. Otras transformaciones pueden no disfrutar de esta interpretación intuitivamente atractiva. La segunda desventaja, introducida en la Sección 5.7.4, es que una transformación también afecta otros aspectos del modelo, como la heterocedasticidad. Por ejemplo, si el modelo original es un modelo multiplicativo (heterocedástico) de la forma \\(y_i=(\\mathrm{E}y_i)~\\varepsilon_i\\), entonces una transformación logarítmica significa que el nuevo modelo es: \\[ \\ln y_i=\\ln \\mathrm{E}(y_i)~+\\ln \\varepsilon_i. \\] A menudo, la capacidad de estabilizar la varianza se considera un aspecto positivo de las transformaciones. Sin embargo, el punto es que una transformación afecta tanto la simetría como la heterocedasticidad, cuando solo un aspecto podría considerarse problemático. La tercera desventaja de transformar la variable dependiente es que el analista está optimizando implícitamente en la escala transformada. Esto ha sido considerado negativamente por algunos académicos. Como señaló Manning (1998), “… nadie está interesado en los resultados de un modelo logarítmico por sí mismos. El Congreso no asigna dólares logarítmicos.” Nuestras discusiones sobre transformaciones se refieren a funciones de las variables dependientes. Como hemos visto en la Sección 3.5, es común transformar variables explicativas. El adjetivo “lineal” en la frase “regresión lineal múltiple” se refiere a combinaciones lineales de parámetros; las variables explicativas en sí mismas pueden ser altamente no lineales. Otra técnica que hemos utilizado implícitamente a lo largo del texto para manejar datos con colas gruesas se conoce como reescalado. En el reescalado, se divide la variable dependiente por una variable explicativa para que el cociente resultante sea más comparable entre observaciones. Por ejemplo, en la Sección 6.5 utilizamos primas de seguros de bienes y accidentes y pérdidas no aseguradas divididas por activos como la variable dependiente. Aunque el numerador, un proxy para los gastos anuales asociados con eventos asegurables, es la medida clave de interés, es común estandarizar por el tamaño de la empresa (medido por activos). Muchas transformaciones son casos especiales de la familia de transformaciones de Box-Cox, introducida en la Sección 1.3. Recuerde que esta familia se define como: \\[ y^{(\\lambda)}=\\left\\{ \\begin{array}{ll} \\frac{y^{\\lambda }-1}{\\lambda } &amp; \\mathrm{si}~\\lambda \\neq 0 \\\\ \\ln y &amp; \\mathrm{si}~\\lambda =0 \\end{array} \\right. , \\] donde \\(\\lambda\\) es el parámetro de transformación (típicamente \\(\\lambda =1,1/2,0~ \\mathrm{o}~-1\\)). Cuando los datos son no positivos, es común agregar una constante a cada observación para que todas las observaciones sean positivas antes de la transformación. Por ejemplo, la transformación \\(\\ln (1+y)\\) acomoda la presencia de ceros. También se puede multiplicar por una constante para que se retengan las unidades originales aproximadas. Por ejemplo, la transformación \\(100\\ln (1+y/100)\\) puede aplicarse a datos porcentuales donde a veces aparecen porcentajes negativos. Para las distribuciones binomial, de Poisson y gamma, también mostramos cómo usar transformaciones de potencia para aproximar la normalidad y la estabilización de la varianza en la Sección 13.5. De manera alternativa, para manejar datos no positivos, una modificación fácil de usar es la transformación logarítmica con signo, dada por \\(\\mathrm{sign}(y) \\ln(|y|+1)\\). Este es un caso especial de la familia introducida por John y Draper (1980): \\[ y^{(\\lambda)}=\\left\\{ \\begin{array}{lr} \\mathrm{sign}(y) \\left\\{(|y|+1)^\\lambda-1\\right\\}/\\lambda, &amp; \\lambda \\neq 0 \\\\ \\mathrm{sign}(y) \\ln(|y|+1),&amp; \\lambda=0 \\end{array} \\right. . \\] Una desventaja de la familia de John y Draper es que su derivada no es continua en cero, lo que significa que puede haber discontinuidades abruptas para observaciones cercanas a cero. Para abordar esto, Yeo y Johnson (2000) recomiendan la siguiente extensión de la familia Box-Cox: \\[ y^{(\\lambda )}=\\left\\{ \\begin{array}{ll} \\frac{(1+y)^{\\lambda }-1}{\\lambda } &amp; y\\geq 0,\\lambda \\neq 0 \\\\ \\ln (1+y) &amp; y\\geq 0,\\lambda =0 \\\\ -\\frac{(1+|y|)^{2-\\lambda }-1}{2-\\lambda } &amp; y&lt;0,\\lambda \\neq 2 \\\\ -\\ln (1+|y|) &amp; y&lt;0,\\lambda =2 \\end{array} \\right. . \\] Para valores no negativos de \\(y\\), esta transformación es igual a la familia Box-Cox utilizando \\(1+y\\) en lugar de \\(y\\) para acomodar ceros. Para valores negativos, la potencia \\(\\lambda\\) se reemplaza por \\(2-\\lambda\\), de modo que una distribución asimétrica a la derecha permanece asimétrica a la derecha después del cambio de signo. La Figura 17.1 muestra esta función para varios valores de \\(\\lambda\\). Figura 17.1: Transformaciones de Yeo-Johnson. De abajo hacia arriba, las curvas corresponden a \\(\\lambda =0,0.5,1,1.5\\) y 2. Tanto la familia de John y Draper como la de Yeo y Johnson se basan en transformaciones de potencia. Una familia alternativa, propuesta por Burbidge y Magee (1988), es una modificación de la transformación de seno hiperbólico inverso. Esta familia se define como: \\[ y^{(\\lambda)}=\\sinh^{-1}(\\lambda y)/\\lambda . \\] 17.3 Modelos Lineales Generalizados Como se introdujo en el Capítulo 13, el método de modelos lineales generalizados (GLM) se ha vuelto popular en las estadísticas financieras y actuariales. Una ventaja de esta metodología es la capacidad de ajustar distribuciones con colas más pesadas que la distribución normal. En particular, los métodos GLM se basan en la familia exponencial que incluye las distribuciones normal, gamma e inversa gaussiana. Como veremos en la Sección 17.3.1, es común considerar que la distribución gamma tiene colas intermedias y la inversa gaussiana colas pesadas en comparación con la distribución normal de cola delgada. La idea de un GLM es mapear un componente sistemático lineal \\(\\mathbf{x}^{\\prime }\\boldsymbol \\beta\\) en la media de la variable de interés a través de una función conocida. Así, los GLM proporcionan una forma natural de incluir covariables en el modelado. Con un GLM, no se requiere que la varianza sea constante como en el modelo lineal, sino que es una función de la media. Una vez que se han especificado la familia de distribución y la función de enlace, la estimación de los coeficientes de regresión del GLM depende únicamente de la media y, por lo tanto, es robusta a algunas especificaciones incorrectas del modelo de distribución. Esto es tanto una fortaleza como una debilidad del enfoque GLM. Aunque más flexible que el modelo lineal, este enfoque no maneja muchas de las distribuciones de colas largas tradicionalmente utilizadas para modelar datos de seguros. Por lo tanto, en la Sección 17.4 presentaremos distribuciones más flexibles. 17.3.1 ¿Qué significa “Cola Gruesa”? Muchos analistas comienzan las discusiones sobre la pesadez de colas a través de los coeficientes de asimetría y curtosis. La asimetría mide la falta de simetría, o el sesgo, de una distribución. Se cuantifica típicamente por el tercer momento estandarizado, \\(\\mathrm{E}(y-\\mathrm{E~}y)^3/ (\\mathrm{Var~}y)^{3/2}.\\) La curtosis mide la pesadez de las colas, o su inverso, la “puntiagudez.” Se cuantifica típicamente por el cuarto momento estandarizado menos 3, \\(\\mathrm{E}(y-\\mathrm{E~}y)^4/ (\\mathrm{Var~}y)^{2} -3.\\) El “menos 3” es para centrar las discusiones en torno a la distribución normal; es decir, para una distribución normal, se puede comprobar que \\(\\mathrm{E}(y-\\mathrm{E~}y)^4/(\\mathrm{Var~}y)^{2} =3.\\) Las distribuciones con curtosis positiva se denominan leptocúrticas mientras que aquellas con curtosis negativa se denominan platicúrticas. Estas definiciones se centran fuertemente en la normal, que tradicionalmente se ha considerado como la distribución de referencia. Para muchas aplicaciones actuariales y financieras, la distribución normal no es un punto de partida adecuado, por lo que buscamos otras definiciones de “cola gruesa.” Además de los momentos, el tamaño de la cola puede medirse utilizando una función de densidad (o masa, para distribuciones discretas), la función de supervivencia, o un momento condicional. Típicamente, la medida se usaría para comparar una distribución con otra. Por ejemplo, al comparar las colas derechas de la normal con una función de densidad gamma, tenemos: \\[ \\begin{array}{ll} \\frac{\\mathrm{f}_{normal}\\left( y\\right) }{\\mathrm{f}_{gamma}\\left( y\\right) } &amp;=\\frac{\\sqrt{2\\pi \\sigma ^{2}}\\exp \\left( -\\left( y-\\mu \\right) ^{2}/(2\\sigma ^{2})\\right) }{\\left[ \\lambda ^{\\alpha }\\Gamma \\left( \\alpha \\right) \\right] ^{-1}y^{\\alpha -1}\\exp \\left( -y/\\lambda \\right) } \\\\ &amp;=C_1 ~\\mathit{\\exp }\\left( -(\\alpha -1)\\ln y+y/\\lambda -\\left( y-\\mu \\right) /(2\\sigma ^{2})\\right) \\\\ &amp;\\rightarrow 0, \\end{array} \\] cuando \\(y \\rightarrow \\infty\\), lo que indica que la gamma tiene una cola más pesada, o gruesa, que la normal. Tanto la normal como la gamma son miembros de la familia exponencial de distribuciones. Para una comparación con otro miembro de esta familia, la distribución inversa gaussiana, consideremos: \\[ \\begin{array}{ll} \\frac{\\mathrm{f}_{gamma}\\left( y\\right) }{\\mathrm{f}_{invGaussian}\\left( y\\right) } &amp;=\\frac{\\left[ \\lambda ^{\\alpha }\\Gamma \\left( \\alpha \\right) \\right] ^{-1}y^{\\alpha -1}\\exp \\left( -y/\\lambda \\right) }{\\sqrt{\\theta /(2\\pi y^{3})}\\exp \\left( -\\theta \\left( y-\\mu \\right) ^{2}/(2y\\mu ^{2})\\right) } \\\\ &amp;=C_2 ~\\mathit{\\exp }\\left( (\\alpha +1/2)\\ln y-y/\\lambda +\\theta \\left( y-\\mu \\right) ^{2}/(2y\\mu ^{2}))\\right) . \\end{array} \\] Cuando \\(y\\rightarrow \\infty\\), esta razón tiende a cero para \\(\\theta /(2\\mu ^{2})&lt;\\lambda\\), lo que indica que la inversa gaussiana puede tener una cola más pesada que la gamma. Para una distribución que no es miembro de la familia exponencial, consideremos la distribución Pareto. Cálculos similares muestran: \\[ \\begin{array}{ll} \\frac{\\mathrm{f}_{gamma}\\left( y\\right) }{\\mathrm{f}_{Pareto}\\left( y\\right) } &amp;=\\frac{\\left[ \\lambda ^{\\alpha }\\Gamma \\left( \\alpha \\right) \\right] ^{-1}y^{\\alpha -1}\\exp \\left( -y/\\lambda \\right) }{\\alpha \\theta ^{-\\alpha }\\left( y+\\theta \\right) ^{-\\alpha -1}} \\\\ &amp;=C_3 ~\\mathit{\\exp }\\left( (\\alpha -1)\\ln y-y/\\lambda +\\left( \\alpha +1\\right) \\ln \\left( y+\\theta \\right) \\right) \\\\ &amp; \\rightarrow 0, \\end{array} \\] cuando \\(y\\rightarrow \\infty\\), lo que indica que la Pareto tiene una cola más pesada que la gamma. La razón de densidades es una medida fácilmente interpretable para comparar la pesadez de colas de dos distribuciones. Debido a que las densidades y las funciones de supervivencia tienen un valor límite de cero, por la regla de L’Hôpital, la razón de funciones de supervivencia es equivalente a la razón de densidades. Es decir, \\[ \\lim_{y\\rightarrow \\infty }\\frac{\\mathrm{S}_1\\left( y\\right) }{\\mathrm{S} _2\\left( y\\right) }=\\lim_{y\\rightarrow \\infty }\\frac{\\mathrm{S} _1^{\\prime }\\left( y\\right) }{\\mathrm{S}_2^{\\prime }\\left( y\\right) } = \\lim_{y\\rightarrow \\infty }\\frac{\\mathrm{f}_1 \\left( y\\right) }{\\mathrm{f}_2 \\left( y\\right) }. \\] Esto proporciona otra motivación para usar esta medida. 17.3.2 Aplicación: Asilos de Ancianos en Wisconsin La financiación de asilos de ancianos ha llamado la atención de los responsables políticos e investigadores durante las últimas décadas. Con poblaciones envejecidas y una mayor esperanza de vida, se espera que los gastos en asilos de ancianos y las demandas de atención a largo plazo aumenten en el futuro. En esta sección, analizamos los datos de 349 instalaciones de cuidado de ancianos en el estado de Wisconsin para el año fiscal 2001. El programa de Medicaid del estado de Wisconsin financia el cuidado en asilos de ancianos para individuos que califican según su necesidad y situación financiera. La mayoría, pero no todos, los asilos de ancianos en Wisconsin están certificados para proporcionar atención financiada por Medicaid. Aquellos que no aceptan Medicaid generalmente son pagados directamente por el residente o su aseguradora. De manera similar, la mayoría, pero no todos, los asilos de ancianos están certificados para proporcionar atención financiada por Medicare. Medicare proporciona atención post-aguda durante 100 días después de una hospitalización relacionada. Medicare no financia la atención proporcionada por instalaciones de cuidado intermedio para personas con discapacidades del desarrollo. Como parte de las condiciones para participar, los asilos de ancianos certificados por Medicare deben presentar un informe anual de costos al Departamento de Salud y Servicios Familiares de Wisconsin, que resume el volumen y costo de la atención proporcionada a todos sus residentes, ya sea financiada por Medicare o no. Los asilos de ancianos son propiedad y están operados por una variedad de entidades, incluidos el estado, condados, municipios, negocios con fines de lucro y organizaciones exentas de impuestos. Las empresas privadas a menudo poseen varios asilos de ancianos. Periódicamente, las instalaciones pueden cambiar de propietario y, con menos frecuencia, de tipo de propiedad. Típicamente, la utilización de la atención en asilos de ancianos se mide en días de paciente. Las instalaciones facturan al intermediario fiscal al final de cada mes por los días de paciente totales incurridos en el mes, desglosados por residente y nivel de atención. Las proyecciones de días de paciente por instalación y nivel de atención juegan un papel clave en el proceso anual de actualización de las tarifas de las instalaciones. Rosenberg et al. (2007) proporciona una discusión adicional. Resumen de los Datos Después de examinar los datos, encontramos algunas variaciones menores en el número de días que una instalación estuvo abierta, principalmente debido a aperturas y cierres de instalaciones. Por lo tanto, para hacer más comparable la utilización entre instalaciones, examinamos TPY, definido como el número total de días-paciente dividido por el número de días que la instalación estuvo abierta; esto tiene un valor mediano de 81.99 por instalación. La Tabla 17.1 describe las variables que se utilizarán para explicar la distribución de TPY. Más de la mitad de las instalaciones tienen autofinanciamiento del seguro. Aproximadamente el \\(90.5\\%\\) de las instalaciones están certificadas por Medicare. En cuanto a la estructura organizacional, cerca de la mitad \\((51.9\\%)\\) son gestionadas con fines de lucro, alrededor de un tercio \\((37.5\\%)\\) están organizadas como exentas de impuestos y el resto son organizaciones gubernamentales. Las instalaciones exentas de impuestos tienen las tasas de ocupación medianas más altas. Un poco más de la mitad de las instalaciones están ubicadas en un entorno urbano (53.3%). Tabla 17.1: Estadísticas Descriptivas de los Asilos de Ancianos Variable Descripción TPY Años-persona totales (mediana 81.89) NumBed Número de camas (mediana 90) SqrFoot Superficie neta del asilo (en miles, mediana 40.25) Variables explicativas categóricas Porcentaje Mediana TPY POPID Número de identificación del asilo SelfIns Autofinanciamiento del seguro Sí 62.8 88.4 No 37.2 67.84 MCert Certificado por Medicare Sí 90.5 84.06 No 9.5 53.38 Organizacional Pro (con fines de lucro) 51.9 77.23 Estructura TaxExempt (exento de impuestos) 37.5 81.13 Govt (unidad gubernamental) 10.6 106.7 Ubicación Urbana 53.3 91.55 Rural 46.7 74.12 Ajuste de Modelos Lineales Generalizados La Figura 17.2 muestra la distribución de la variable dependiente TPY. En esta figura, vemos evidencia clara de la asimetría hacia la derecha de la distribución. Una opción sería realizar una transformación como se describe en la Sección 17.2. Rosenberg et al. (2007) exploraron esta opción utilizando una transformación logarítmica. Figura 17.2: Histograma de TPY. Este gráfico demuestra la asimetría hacia la derecha de la distribución. Código R para Producir Figura 17.2 NurseDat &lt;- read.csv(&quot;CSVData/WiscNursingHome.csv&quot;, header=TRUE) #str(NurseDat) NurseDat01 &lt;- subset(NurseDat,CRYEAR==2001) NurseDat01 &lt;- subset(NurseDat01,SQRFOOT&gt;5) # Se eliminan 5 hogares sin datos de superficie NurseDat01$RATE &lt;- 100*NurseDat01$TPY/NurseDat01$NUMBED NurseDat01 &lt;- subset(NurseDat01,RATE&gt;50) # Se elimina 1 hogar con RATE = 40 - extraño ... # FIGURA 17.2 hist(NurseDat01$TPY, freq=FALSE, nclass=32, main=&quot;&quot;, xlab=&quot;TPY&quot;, ylab=&quot;&quot;, las=1, cex.lab=1.3) mtext(&quot;Densidad&quot;, side=2, at=.016,las=1, adj=.7,cex=1.3) Otra opción es ajustar directamente una distribución sesgada a los datos. La Figura 17.3 presenta los gráficos \\(qq\\) para las distribuciones gamma e inversa gaussiana. Los datos caen bastante cerca de la línea en ambos paneles, lo que indica que ambos modelos son elecciones razonables. El gráfico \\(qq\\) para la distribución normal, no mostrado aquí, indica que el modelo de regresión normal no es un ajuste razonable. Figura 17.3: Gráficos \\(qq\\) de TPY para las distribuciones Gamma e Inversa Gaussiana Código R para Producir Figura 17.3 # GRÁFICO QQ GAMMA # La distribución Gamma con parámetros shape = a y scale = s # La media y la varianza son E(X) = a*s y Var(X) = a*s^2. TPY &lt;- NurseDat01$TPY s &lt;- var(TPY)/mean(TPY) rate &lt;- 1/s a &lt;- mean(TPY)*rate p1 &lt;- seq(1, length(TPY), by = 1) / (1 + length(TPY)) theoreticalquantilesgam &lt;- qgamma(p1 , shape=a, scale=s) library(statmod) # La varianza de la distribución es $?^3/lambda$ mu &lt;- mean(TPY) lambda &lt;- (mu)^3/var(TPY) p1 &lt;- seq(1, length(TPY), by = 1) / (1 + length(TPY)) theoreticalquantilesIG &lt;- qinvgauss(p1 , mu, lambda) par(mfrow=c(1,2)) # FIGURA 17.3a plot(theoreticalquantilesgam, sort(TPY), cex.lab=1.3, xlab=&quot;Cuantiles Gamma&quot;,ylab=&quot;Cuantiles Empíricos&quot;) abline(a=0,b=1) # FIGURA 17.3b plot(theoreticalquantilesIG, sort(TPY), cex.lab=1.3, xlab=&quot;Cuantiles Inversa Gaussiana&quot;,ylab=&quot;Cuantiles Empíricos&quot;) abline(a=0,b=1) Ajustamos los modelos lineales generalizados utilizando las distribuciones gamma e inversa gaussiana. En ambos modelos, elegimos la función de enlace logarítmica. El componente sistemático lineal común a cada modelo es \\[\\begin{eqnarray} &amp;&amp; \\eta = \\beta_0 + \\beta_1 \\ln(\\text{NumBed}) + \\beta_2 \\ln(\\text{SqrFoot}) + \\beta_3 \\text{Pro} \\\\ &amp;&amp; + \\beta_4 \\text{TaxExempt} + \\beta_5 \\text{SelfIns} + \\beta_6 \\text{MCert} + \\beta_7 \\text{Urban}. \\notag \\tag{17.1} \\end{eqnarray}\\] Table 17.2 resume las estimaciones de los parámetros de los modelos. Al comparar las estadísticas BIC, o el AIC y el logaritmo de verosimilitud dado que el número de parámetros estimados y el tamaño de la muestra en ambos modelos son idénticos, encontramos que el modelo gamma se desempeña mejor que el inverso gaussiano. Como era de esperar, el coeficiente para la variable de tamaño NumBed es positivo y significativo. La única otra variable que es estadísticamente significativa es la variable SqrFoot, y esto solo en el modelo gamma. Table 17.2. Modelos Lineales Generalizados Ajustados para Hogares de Ancianos \\[ \\small{ \\begin{array}{l|rr|rr} \\hline\\hline &amp;\\text{Gamma} &amp; &amp; \\text{Inversa} &amp; \\text{Gaussiana} \\\\ \\text{Variables} &amp; \\text{Estimación} &amp; \\textit{t}\\text{-ratio} &amp; \\text{Estimación} &amp; \\textit{t}\\text{-ratio}\\\\ \\hline \\text{Intercepto} &amp; -0.159 &amp; -3.75 &amp; -0.196 &amp; -4.42 \\\\ \\text{ln(NumBed) } &amp; 0.996 &amp; 66.46 &amp; 1.023 &amp; 65.08 \\\\ \\text{ln(SqrFoot)} &amp; 0.026 &amp; 2.07 &amp; 0.003 &amp; 0.19 \\\\ \\text{SelfIns} &amp; 0.006 &amp; 0.75 &amp; 0.003 &amp; 0.27 \\\\ \\text{MCert } &amp; -0.008 &amp; -0.55 &amp; -0.008 &amp; -0.57 \\\\ \\text{Pro} &amp; 0.004 &amp; 0.29 &amp; 0.007 &amp; 0.36 \\\\ \\text{TaxExempt} &amp; 0.018 &amp; 1.28 &amp; 0.021 &amp; 1.12 \\\\ \\text{Urban} &amp; -0.011 &amp; -1.25 &amp; -0.006 &amp; -0.64 \\\\ \\text{Escala} &amp; 165.64&amp; &amp; 0.0112 \\\\ \\hline \\\\ \\hline \\end{array} \\\\ \\text{Estadísticas de Bondad de Ajuste} \\\\ \\begin{array}{lrr} \\hline \\text{Log-Verosimilitud} &amp; -1,131.24 &amp; -1,218.15 \\\\ \\text{AIC} &amp;2,280.47 &amp; 2,454.31\\\\ \\text{BIC} &amp; 2,315.17 &amp; 2,489.00 \\\\ \\hline\\hline \\end{array} } \\] La Figura 17.4 presenta los gráficos de residuos de desviación contra los valores ajustados de TPY para los modelos gamma e inversa gaussiana. No se encuentran patrones en los gráficos, lo que respalda la posición de que estos modelos son ajustes razonables para los datos. Figura 17.4: Gráficos de Residuos de Desviación versus Valores Ajustados para los Modelos Gamma e Inversa Gaussiana. Código R para Producir Figura 17.4 # INICIO DEL AJUSTE DE DISTRIBUCIONES - AQUÍ ESTÁ EL GAMMA CON EL ENLACE LOG glmGammanurse1 &lt;- glm(TPY~log(NUMBED)+log(SQRFOOT)+PRO+TAXEXEMPT+ SELFFUNDINS+MCERT+URBAN,data=NurseDat01, control = glm.control(maxit = 50), family=Gamma(link=&quot;log&quot;)) # INVERSA GAUSSIANA CON EL ENLACE LOG glmInvGaunurse1 &lt;- glm(TPY~log(NUMBED)+log(SQRFOOT)+PRO+TAXEXEMPT+ SELFFUNDINS+MCERT+URBAN,data=NurseDat01, family=inverse.gaussian(link = &quot;log&quot;)) # GRÁFICOS DE RESIDUOS - FIGURA 17.4 par(mfrow=c(1,2)) library(boot) plot(glmGammanurse1$fitted.values, glm.diag(glmGammanurse1)$rd, cex.lab=1.1, xlab=&quot;Valor Ajustado de TPY&quot;, ylab=&quot;Residuos de Desviación Gamma&quot; ) plot(glmInvGaunurse1$fitted.values, glm.diag(glmInvGaunurse1)$rd, cex.lab=1.1, xlab=&quot;Valor Ajustado de TPY&quot;, ylab=&quot;Residuos de Desviación Inv Gauss&quot; ) 17.4 Distribuciones Generalizadas Otra forma de manejar datos de regresión con colas pesadas es usar distribuciones paramétricas, como las utilizadas en el modelado de supervivencia. Aunque el análisis de supervivencia se enfoca en datos censurados, los métodos pueden aplicarse perfectamente a datos completos. En la Sección 14.3 introdujimos un modelo de tiempo de falla acelerado (AFT). El AFT es un modelo logarítmico de ubicación-escala, de modo que \\(\\ln (y)\\) sigue una distribución paramétrica de densidad de ubicación-escala en la forma \\(\\mathrm{f}(y)=\\mathrm{f}_0\\left( (y-\\mu )/\\sigma \\right) /\\sigma\\), donde \\(\\mu\\) y \\(\\sigma &gt;0\\) son los parámetros de ubicación y escala, y \\(\\mathrm{f}_0\\) es la forma estándar de la distribución. Las distribuciones Weibull, lognormal y log-logística son distribuciones de tiempo de vida comúnmente usadas que son casos especiales del marco AFT. Para ajustar distribuciones de cola pesada de interés en ciencias actuariales, consideramos la siguiente variación menor y examinamos distribuciones a partir de la relación \\[\\begin{equation} \\ln y = \\mu + \\sigma \\ln y_0. \\tag{17.2} \\end{equation}\\] Como antes, la distribución asociada con \\(y_0\\) es una estándar y nos interesa la distribución de la variable aleatoria \\(y\\). Dos casos especiales importantes son la gamma generalizada y la beta generalizada del segundo tipo. Estas distribuciones se han utilizado extensamente en el modelado de datos de seguros, véase, por ejemplo, Klugman et al. (2008), aunque la mayoría de las aplicaciones no han utilizado covariables de regresión. La distribución gamma generalizada se obtiene cuando \\(y_0\\) tiene una distribución gamma con parámetro de forma \\(\\alpha\\) y parámetro de escala 1. Al incluir distribuciones límite (como permitir que los coeficientes se vuelvan arbitrariamente grandes), incluye las distribuciones exponencial, Weibull, gamma y lognormal como casos especiales. Por lo tanto, puede usarse para discriminar entre modelos alternativos. La distribución gamma generalizada también se conoce como la distribución gamma transformada (Klugman et al., 2008). Cuando \\(y_0\\) tiene una distribución que es la razón de dos gammas, entonces \\(y\\) se dice que tiene una distribución beta generalizada del segundo tipo, comúnmente conocida por el acrónimo GB2. Específicamente, asumimos que \\(y_0 = Gamma_1/Gamma_2\\), donde \\(Gamma_i\\) tiene una distribución gamma con parámetro de forma \\(\\alpha_i\\) y parámetro de escala 1, \\(i=1,2\\), y que \\(Gamma_1\\) y \\(Gamma_2\\) son independientes. Así, la familia GB2 tiene cuatro parámetros (\\(\\alpha_1\\), \\(\\alpha_2\\), \\(\\mu\\) y \\(\\sigma\\)) en comparación con los tres parámetros de la distribución gamma generalizada. Al incluir distribuciones límite, la GB2 abarca la gamma generalizada (permitiendo que \\(\\alpha_2 \\rightarrow \\infty\\)) y, por ende, la exponencial, Weibull, y así sucesivamente. También abarca la Burr Tipo 12 (permitiendo que \\(\\alpha_1 = 1\\)), así como otras familias de interés, incluidas las distribuciones de Pareto. La distribución de \\(y\\) de la ecuación (17.2) contiene el parámetro de ubicación \\(\\mu\\), el parámetro de escala \\(\\sigma\\) y parámetros adicionales que describen la distribución de \\(y_0\\). En principio, se podría permitir que cualquier parámetro de la distribución sea una función de las covariables. Sin embargo, seguir este principio llevaría a un gran número de parámetros; esto típicamente genera dificultades computacionales así como problemas de interpretación. Para limitar el número de parámetros, es habitual asumir que los parámetros de \\(y_0\\) no dependen de las covariables. Es natural permitir que el parámetro de ubicación sea una función lineal de las covariables, de modo que \\(\\mu =\\mu \\left( \\mathbf{x}\\right)= \\mathbf{x}^{\\prime } \\boldsymbol \\beta\\). También se puede permitir que el parámetro de escala \\(\\sigma\\) dependa de \\(\\mathbf{x}\\). Para \\(\\sigma\\) positivo, una especificación común es \\(\\sigma =\\sigma (\\mathbf{x})\\) \\(= \\exp(\\mathbf{x}^{\\prime }\\boldsymbol \\beta_{\\sigma })\\), donde \\(\\boldsymbol \\beta_{\\sigma}\\) son coeficientes de regresión asociados con el parámetro de escala. Los demás parámetros suelen mantenerse fijos. La interpretabilidad de los parámetros es una razón para mantener fijos el parámetro de escala y otros parámetros no relacionados con la ubicación. Al hacer esto, es sencillo demostrar que la función de regresión tiene la forma \\[ \\mathrm{E}\\left( y|\\mathbf{x}\\right) =C\\exp \\left( \\mu \\left( \\mathbf{x} \\right) \\right) =C~e^{\\mathbf{x}^{\\prime }\\boldsymbol \\beta}, \\] donde la constante \\(C\\) es una función de otros parámetros del modelo (no relacionados con la ubicación). Así, se pueden interpretar los coeficientes de regresión en términos de un cambio proporcional (una elasticidad en economía). Es decir, \\(\\partial \\left[ \\ln \\mathrm{E}(y) \\right] /\\partial x_k= \\beta_k.\\) Otra razón para mantener fijos los parámetros no relacionados con la ubicación es la facilidad de calcular residuos razonables y usar estos residuos para ayudar con la selección del modelo. Específicamente, con la ecuación (17.2), se pueden calcular residuos de la forma \\[ r_i = \\frac{\\ln y_i-\\widehat{\\mu}_i }{\\widehat{\\sigma }}, \\] donde \\(\\widehat{\\mu}_i\\) y \\(\\widehat{\\sigma }\\) son estimaciones de máxima verosimilitud. Para conjuntos de datos grandes, podemos asumir poco error de estimación, de modo que \\(r_i \\approx (\\ln y_i - \\mu_i) /\\sigma,\\) y la cantidad en el lado derecho tiene una distribución conocida. Para ilustrar, considere el caso en que \\(y\\) sigue una distribución GB2. En este caso, \\[ y_0 = \\frac{Gamma_1}{Gamma_2}= \\frac{\\alpha_1}{\\alpha_2} \\times \\frac{Gamma_1/(2\\alpha_1)}{Gamma_2/(2\\alpha_2)} = \\frac{\\alpha_1}{\\alpha_2} \\times F , \\] donde \\(F\\) tiene una distribución \\(F\\) con grados de libertad del numerador y denominador \\(df_1 = 2 \\alpha_1\\) y \\(df_2 = 2 \\alpha_2\\). Entonces, \\(\\exp(r_i) \\approx (\\alpha_1 /\\alpha_2) F_i\\), de modo que los residuos exponenciados deberían tener una distribución aproximada F (hasta un parámetro de escala). Este hecho permite calcular gráficos cuantiles-cuantiles (qq) para evaluar gráficamente la adecuación del modelo. Para ilustrar, consideramos algunos ejemplos relacionados con seguros que utilizan modelos de regresión con colas pesadas. McDonald y Butler (1990) discutieron modelos de regresión que incluyen los comúnmente usados, así como las distribuciones GB2 y gamma generalizada. Aplicaron el modelo a la duración de períodos de pobreza y encontraron que la GB2 mejoraba significativamente el ajuste del modelo sobre la lognormal. Beirlant et al. (1998) propusieron dos modelos de regresión Burr y los aplicaron a la segmentación de carteras para seguros contra incendios. La Burr es una extensión de la distribución de Pareto, aunque sigue siendo un caso especial de la GB2. Manning, Basu y Mullahy (2005) aplicaron la distribución gamma generalizada a los gastos hospitalarios usando datos de un estudio de hospitales realizado en la Universidad de Chicago. Debido a que el modelo de regresión es completamente paramétrico, la máxima verosimilitud es generalmente el método de estimación preferido. Si \\(y\\) sigue una distribución GB2, cálculos sencillos muestran que su densidad puede expresarse como \\[\\begin{equation} f(y; \\mu, \\sigma, \\alpha_1, \\alpha_2) = \\frac{[\\exp( z)]^{\\alpha_{1}}}{y |\\sigma| B(\\alpha_1, \\alpha_2) [1 + \\exp(z) ]^{\\alpha_1 + \\alpha_2} }, \\tag{17.3} \\end{equation}\\] donde \\(z= (\\ln y - \\mu)/{\\sigma}\\) y B(\\(\\cdot,\\cdot\\)) es la función beta, definida como \\(\\text{B}(\\alpha_1, \\alpha_2) = \\Gamma(\\alpha_1)\\Gamma(\\alpha_2)/\\Gamma(\\alpha_1+\\alpha_2)\\). Esta densidad puede usarse directamente en rutinas de verosimilitud de muchos paquetes estadísticos. Como se describe en la Sección 11.9, el método de máxima verosimilitud proporciona automáticamente: errores estándar para las estimaciones de los parámetros, métodos de selección de modelos mediante pruebas de razón de verosimilitudes y estadísticas de bondad de ajuste como AIC y BIC. Aplicación: Hogares de Ancianos en Wisconsin En los modelos lineales generalizados ajustados resumidos en la Tabla Tabla 17.2, vimos que los coeficientes asociados con ln(NumBed) estaban cerca de uno. Esto sugiere identificar ln(NumBed) como una variable de desplazamiento, es decir, forzar que el coeficiente asociado con ln(NumBed) sea 1. Para otra estrategia de modelado, también sugiere reescalar la variable dependiente por NumBed. Esto es razonable porque usamos una función de enlace logarítmica, de modo que el valor esperado de TPY es proporcional a NumBed. Siguiendo este enfoque, ahora definimos la tasa de ocupación anual (Rate) como \\[\\begin{equation} \\text{Tasa de Ocupación} = \\frac{\\text{Días Totales de Pacientes}}{\\text{Número de Camas} \\times \\text{Días Abierto}} \\times 100. \\tag{17.4} \\end{equation}\\] Esta nueva variable dependiente es fácil de interpretar: mide el porcentaje de camas ocupadas en un día cualquiera. Las tasas de ocupación se calcularon usando el número promedio de camas licenciadas dentro de un año de reporte de costos en lugar del número de camas licenciadas en un día específico. Esto da lugar a algunas tasas de ocupación mayores al 100. Una dificultad de usar tasas de ocupación es que su distribución no puede aproximarse razonablemente por un miembro de la familia exponencial. La Figura 17.5 muestra un histograma suavizado de la variable Rate (usando un suavizador de núcleo); esta distribución está sesgada a la izquierda. Superpuesta con la línea discontinua está la distribución inversa gaussiana donde los parámetros se ajustaron sin covariables, usando el método de momentos. Las distribuciones gamma y normal están muy cerca de la inversa gaussiana, y por lo tanto no se muestran aquí. En contraste, la distribución GB2 ajustada (también sin covariables) mostrada en la Figura 17.5 captura partes importantes de la distribución; en particular, captura la agudeza y el sesgo hacia la izquierda. Figura 17.5: Densidades de Hogares de Ancianos. La versión empírica, basada en una estimación de densidad por núcleo, se compara con densidades ajustadas de GB2 e inversa gaussiana. Código R para producir la Figura 17.5 library(statmod) RATE &lt;- NurseDat01$RATE dGBII &lt;- function(y, delta, sigma, alpha1, alpha2, log = TRUE){ lnfstterm &lt;- alpha1 * (log(y) - delta)/sigma lnsndterm &lt;- log(y) + log(abs(sigma))+ lgamma(alpha1) + lgamma(alpha2) - lgamma(alpha1+alpha2) lnfthterm &lt;- (alpha1+alpha2)*log(1 + exp( (log(y) - delta)/sigma) ) lnans &lt;- lnfstterm - lnsndterm - lnfthterm if (log) lnans else exp(lnans) } # GB2 SIN COVARIABLES myLoglik1 &lt;- function(beta,y){ sum(dGBII(y,delta=beta[1], sigma=exp(beta[2]), alpha1=exp(beta[3]), alpha2=exp(beta[4]), log=TRUE))} ip1 &lt;- c(4.6, -5, -2, -1.5 ) #myLoglik1(ip1,y=RATE) z1 &lt;- optim(ip1,myLoglik1,y=RATE, control=list(fnscale=-1,trace=1,maxit=3000,temp=20) ) save(z1, file = &quot;../../Chapters/Chapter17FatTail/GB2CalcsFig175.RData&quot;) dGBII &lt;- function(y, delta, sigma, alpha1, alpha2, log = TRUE){ lnfstterm &lt;- alpha1 * (log(y) - delta)/sigma lnsndterm &lt;- log(y) + log(abs(sigma))+ lgamma(alpha1) + lgamma(alpha2) - lgamma(alpha1+alpha2) lnfthterm &lt;- (alpha1+alpha2)*log(1 + exp( (log(y) - delta)/sigma) ) lnans &lt;- lnfstterm - lnsndterm - lnfthterm if (log) lnans else exp(lnans) } #save(z1, file = &quot;../../Chapters/Chapter17FatTail/GB2CalcsFig175.RData&quot;) load(file = &quot;Chapters/Chapter17FatTail/GB2CalcsFig175.RData&quot;) delta &lt;- z1$par[1] sigma &lt;- exp(z1$par[2]) alpha1 &lt;- exp(z1$par[3]) alpha2 &lt;- exp(z1$par[4]) # FIGURA 17.5 RATE &lt;- NurseDat01$RATE plot(density(RATE), main=&quot;&quot;, xlab=&quot;Tasa de Ocupación&quot;, ylim=c(0, 0.1) )#Núcleo Gaussiano x &lt;- seq(60, 120, 0.1) pdf &lt;- dnorm(x, mean=mean(RATE), sd=sd(RATE)) # Normal lines(x,pdf, col=&#39;blue&#39;) s &lt;- var(RATE)/mean(RATE) a &lt;- mean(RATE)/s pdf1 &lt;- dgamma(x, shape=a, scale=s) #Gamma lines(x,pdf1, col=&#39;red&#39;) mu &lt;- mean(RATE) lambda &lt;- (mu)^3/var(RATE) pdf2 &lt;- dinvgauss(x, mu, lambda) #Gaussiana Inversa lines(x,pdf2, col=&#39;green&#39;) pdf3 &lt;- dGBII(x,delta,sigma,alpha1,alpha2, log=FALSE) lines(x,pdf3, lty=3) text(105, 0.09, &quot;GB 2&quot;, cex=1.3) text(115, 0.02, &quot;inversa&quot;, cex=1.3) text(115, 0.01, &quot;gaussiana&quot;, cex=1.3) text(80, 0.085, &quot;datos&quot;, cex=1.3) text(80, 0.075, &quot;suavizados&quot;, cex=1.3) arrows(85, 0.07, 90, 0.068,code=2, angle=20, length=0.1) La distribución GB2 se ajustó utilizando máxima verosimilitud con las mismas covariables que en la Tabla 17.2. Específicamente, utilizamos el parámetro de ubicación \\(\\mu = \\exp(\\eta)\\), donde \\(\\eta\\) se especifica en la ecuación (17.1). Como es habitual en la estimación por verosimilitud, reparametrizamos la escala y los dos parámetros de forma, \\(\\sigma\\), \\(\\alpha_1\\) y \\(\\alpha_2\\), para ser transformados en la escala logarítmica, de modo que pudieran abarcar toda la línea real. De esta manera, evitamos problemas de límites que podrían surgir al intentar ajustar modelos con valores negativos de parámetros. La Tabla 17.3 resume el modelo ajustado. Desafortunadamente, para este modelo ajustado, ninguna de las variables explicativas resultó estadísticamente significativa. (Recordemos que reescalamos por número de camas, una variable explicativa muy importante.) Tabla 17.3. Modelos Generalizados de Hogares de Ancianos en Wisconsin \\[ \\small{ \\begin{array}{l|rr|rr} \\hline \\hline \\hline &amp; \\text{Gamma }&amp; \\text{Generalizada} &amp;\\text{GB2} \\\\ \\hline \\text{Variables} &amp; \\text{Estimación} &amp; \\textit{t}\\text{-ratio} &amp;\\text{Estimación} &amp; \\textit{t}\\text{-ratio} \\\\ \\hline \\text{Intercepto} &amp; 4.522 &amp; 78.15 &amp; 4.584 &amp; 198.47 \\\\ \\text{ln(NumBed)} &amp; -0.027 &amp; -2.06 &amp; -0.010 &amp; -1.17 \\\\ \\text{ln(SqrFoot)} &amp; 0.031 &amp; 2.89 &amp; 0.010 &amp; 1.28 \\\\ \\text{SelfIns} &amp; 0.003 &amp; 0.44 &amp; -0.001 &amp; -0.25 \\\\ \\text{MCert} &amp; -0.010 &amp; -0.81 &amp; -0.010 &amp; -1.30 \\\\ \\text{Pro } &amp; -0.021 &amp; -1.46 &amp; -0.002 &amp; -0.20 \\\\ \\text{TaxExempt} &amp; -0.007 &amp; -0.48 &amp; 0.015 &amp; 1.66 \\\\ \\text{Urbano} &amp; -0.014 &amp; -1.78 &amp; -0.003 &amp; -0.60 \\\\ \\hline &amp; \\text{Estimación} &amp;\\text{Error Estándar} &amp; \\text{Estimación} &amp; \\text{Error Estándar} \\\\ \\ln \\sigma &amp; -2.406 &amp; 0.131 &amp; -5.553 &amp; 1.716 \\\\ \\ln \\alpha_1 &amp; 0.655 &amp; 0.236 &amp; -2.906 &amp; 1.752 \\\\ \\ln \\alpha_2 &amp; &amp; &amp; -1.696 &amp; 1.750 \\\\ \\end{array} \\\\ \\begin{array}{l|rr} \\hline \\text{Log-Verosimilitud} &amp;-1,148.135 &amp; -1,098.723 \\\\ \\text{AIC} &amp; 2,316.270 &amp; 2,219.446 \\\\ \\text{BIC} &amp; 2,319.296 &amp; 2,223.822\\\\ \\hline\\hline \\end{array} } \\] Para evaluar más el ajuste del modelo, la Figura 17.6 muestra los residuos de este modelo ajustado. Para estas figuras, los residuos se calculan usando \\(r_i = (\\ln y_i-\\widehat{\\mu}_i)/\\widehat{\\sigma }.\\) El panel izquierdo muestra los residuos frente a los valores ajustados (\\(\\exp(\\widehat{\\mu}_i)\\)); no se evidencian patrones aparentes en esta visualización. El panel derecho es un gráfico \\(qq\\) de residuos, donde las distribuciones de referencia es la distribución \\(F\\) logarítmica (más una constante) descrita anteriormente. Esta figura muestra algunas discrepancias para valores más pequeños de hogares de ancianos. Debido a esto, la Tabla 17.3 también informa ajustes del modelo gamma generalizado. Este ajuste es más satisfactorio en el sentido de que dos de las variables explicativas son estadísticamente significativas. Sin embargo, a partir de las estadísticas de bondad de ajuste, vemos que la GB2 es un modelo mejor ajustado. Cabe señalar que las estadísticas de bondad de ajuste para el modelo gamma generalizado no son directamente comparables con los ajustes de regresión gamma en la Tabla 17.2; esto se debe únicamente a que la variable dependiente difiere por la variable de escala NumBeds. Figura 17.6: Análisis de Residuos del Modelo GB2. El panel izquierdo es un gráfico de residuos frente a valores ajustados. El panel derecho es un gráfico \\(qq\\) de residuos. Código R para producir la Figura 17.6 ## MODELOS GB2 Y GAMMA GENERALIZADA # CONFIGURAR DATOS dat &lt;- NurseDat01 dat$Rate &lt;- dat$RATE x1 &lt;- log(dat$NUMBED) x2 &lt;- log(dat$SQRFOOT) x3 &lt;- dat$PRO x4 &lt;- dat$TAXEXEMPT x5 &lt;- dat$SELFFUNDINS x6 &lt;- dat$MCERT x7 &lt;- dat$URBAN myLoglik &lt;- function(beta,y){ sum(dGBII(y, delta=beta[1]+beta[2]*x1+beta[3]*x2+beta[4]*x3+beta[5]*x4 +beta[6]*x5+beta[7]*x6+beta[8]*x7, sigma=exp(beta[9]), alpha1=exp(beta[10]), alpha2=exp(beta[11]), log=TRUE))} # PARÁMETROS INICIALES ip &lt;- c(4.6, rep(0, 7), -5, -2, -1.5) #myLoglik(ip,y=dat$Rate) # LLAMAR A LA RUTINA DE OPTIMIZACIÓN z &lt;- optim(ip,myLoglik,y=dat$Rate, control=list(fnscale=-1,trace=1,maxit=3000,temp=20)) # AQUÍ ESTÁN LOS PARÁMETROS DE ESTA EJECUCIÓN DE OPTIMIZACIÓN #z$par ## AHORA, EJECUTAR ESTO POR UN TIEMPO PARA ASEGURAR CONVERGENCIA for (i in 1:3) { z &lt;- optim(z$par,myLoglik,y=dat$Rate, control=list(fnscale=-1,trace=1,maxit=3000,temp=20) ) } # PROBAR UN GRÁFICO DE RESIDUOS beta &lt;- z$par delta &lt;- beta[1]+beta[2]*x1+beta[3]*x2+beta[4]*x3+beta[5]*x4+ beta[6]*x5+beta[7]*x6+beta[8]*x7 sigma &lt;- exp(beta[9]) alpha1 &lt;- exp(beta[10]) alpha2 &lt;- exp(beta[11]) dat$res &lt;- (log(dat$Rate) - delta) / sigma dat$fits &lt;- exp(delta) save(z, dat, file = &quot;../../Chapters/Chapter17FatTail/GB2Calcs.RData&quot;) #save(dat, file = &quot;../../Chapters/Chapter17FatTail/GB2Calcs.RData&quot;) load(file = &quot;Chapters/Chapter17FatTail/GB2Calcs.RData&quot;) par(mfrow=c(1,2)) plot(dat$fits, dat$res,xlab=&quot;Valores Ajustados&quot;, ylab=&quot;Residuos&quot;, cex.lab = 1.3) abline(0,0) beta &lt;- z$par sigma &lt;- exp(beta[9]) alpha1 &lt;- exp(beta[10]) alpha2 &lt;- exp(beta[11]) # GRÁFICO QQ p1 &lt;- seq(1, length(dat$res), by = 1) / (1 + length(dat$res)) # MÉTODO 1 CON DISTRIBUCIÓN F quan1 &lt;- qf(p1, 2*alpha1, 2*alpha2) quan2 &lt;- log(quan1*alpha1/alpha2) # MÉTODO 2 CON DISTRIBUCIÓN BETA quan3 &lt;- qbeta(p1, alpha1, alpha2) quan4 &lt;- log(quan3/(1-quan3)) quantile &lt;- quan4 for (i in 1:length(quantile)) { if (quan4[i]==Inf) quantile[i] &lt;- quan2[i] } # NINGUNO FUNCIONA - USAR AMBOS plot(quantile, sort(dat$res),ylab=&quot;cuantil empírico&quot;, xlab=&quot;cuantil teórico&quot;, cex.lab = 1.3) abline(0, 1) 17.5 Regresión por Cuantiles La regresión por cuantiles es una extensión de la regresión por la mediana, por lo que es útil introducir este concepto primero. En la regresión por la mediana, se encuentra el conjunto de coeficientes de regresión \\(\\boldsymbol \\beta\\) que minimiza \\[ \\sum_{i=1}^n | y_i - \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta |. \\] Es decir, simplemente reemplazamos la función de pérdida cuadrática habitual por una función de valor absoluto. Aunque no entraremos en detalles aquí, encontrar estos coeficientes óptimos es un problema de optimización simple en programación no lineal que puede implementarse fácilmente en software estadístico moderno. Debido a que este procedimiento utiliza el valor absoluto como función de pérdida, la regresión por la mediana también se conoce como LAD por mínima desviación absoluta en comparación con OLS (mínimos cuadrados ordinarios). El adjetivo “mediana” proviene del caso especial en el que no hay regresores, de modo que \\(\\mathbf{x}\\) es un escalar igual a 1. En este caso, el problema de minimización se reduce a encontrar un intercepto \\(\\beta_0\\) que minimice \\[ \\sum_{i=1}^n | y_i - \\beta_0 |. \\] La solución a este problema es la mediana de \\(\\{y_1, \\ldots, y_n\\}\\). Supongamos que también desea encontrar el percentil \\(25^{\\rm th}\\), \\(75^{\\rm th}\\), o algún otro percentil de \\(\\{y_1, \\ldots, y_n\\}\\). También se puede usar este procedimiento de optimización para encontrar cualquier percentil, o cuantil. Sea \\(\\tau\\) una fracción entre 0 y 1. Entonces, el cuantil \\(\\tau^{\\rm th}\\) de la muestra \\(\\{y_1, \\ldots, y_n\\}\\) es el valor de \\(\\beta_0\\) que minimiza \\[ \\sum_{i=1}^n \\rho_{\\tau}( y_i - \\beta_0). \\] Aquí, \\(\\rho_{\\tau}(u)=u(\\tau-{\\rm I}(u\\leq0))\\) se llama una función de chequeo y \\({\\rm I}(\\cdot)\\) es la función indicadora. Extendiendo este procedimiento, en la regresión por cuantiles se encuentra el conjunto de coeficientes de regresión \\(\\boldsymbol \\beta\\) que minimiza \\[ \\sum_{i=1}^n \\rho_{\\tau}( y_i - \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta ). \\] Los coeficientes de regresión estimados dependen de la fracción \\(\\tau\\), por lo que usamos la notación \\(\\widehat{\\boldsymbol \\beta}(\\tau)\\) para enfatizar esta dependencia. La cantidad \\(\\mathbf{x}_i^{\\prime}\\widehat{\\boldsymbol \\beta}(\\tau)\\) representa el cuantil \\(\\tau^{\\rm th}\\) de la distribución de \\(y_i\\) para el vector explicativo \\(\\mathbf{x}_i\\). Para ilustrar, para \\(\\tau = 0.5\\), \\(\\mathbf{x}_i^{\\prime}\\widehat{\\boldsymbol \\beta}(0.5)\\) representa la mediana estimada de la distribución de \\(y_i\\). En contraste, el valor ajustado por \\(OLS\\) \\(\\mathbf{x}_i^{\\prime}\\mathbf{b}\\) representa la media estimada de la distribución de \\(y_i\\). Ejemplo: Hogares de Ancianos en Wisconsin - Continuación. Para ilustrar las técnicas de regresión por cuantiles, ajustamos una regresión del metraje cuadrado (SqrFoot) sobre los años totales de personas (TPY). La Figura 17.7 muestra la relación entre estas dos variables, con líneas ajustadas de media (OLS) y mediana (LAD) superpuestas. A diferencia de la distribución original de TPY que está sesgada, para cada valor de SqrFoot podemos ver poca diferencia entre los valores de media y mediana. Esto sugiere que la distribución condicional de TPY dado SqrFoot no está sesgada. La Figura 17.7 también muestra las líneas ajustadas que resultan de ajustar regresiones por cuantiles en cuatro valores adicionales de \\(\\tau =0.05,0.25, 0.75\\) y 0.95. Estos ajustes están indicados por las líneas grises. Para cada valor de SqrFoot, visualmente podemos tener una idea de los percentiles \\(5^{\\rm th}\\), \\(25^{\\rm th}\\), \\(50^{\\rm th}\\), \\(75^{\\rm th}\\) y \\(95^{\\rm th}\\) de la distribución de TPY. Aunque los mínimos cuadrados ordinarios clásicos también proporcionan esto, las recetas clásicas generalmente asumen homocedasticidad. A partir de 17.7, vemos que la distribución de \\(y\\) parece ensancharse a medida que SqrFoot aumenta, lo que sugiere una relación heterocedástica. Figura 17.7: Ajustes de Regresión por Cuantiles del Metraje Cuadrado sobre los Años Totales de Personas. Se superponen los ajustes de las regresiones de media (OLS) y mediana (LAD), indicados en la leyenda. También se superponen, con líneas grises, los ajustes de regresión por cuantiles – de abajo hacia arriba, los ajustes corresponden a \\(\\tau =0.05,0.25, 0.75\\) y 0.95. Código R para producir la Figura 17.7 # Figura 17.7 plot(NurseDat01$SQRFOOT,NurseDat01$TPY,xlab=&quot;Metraje Cuadrado&quot;,ylab=&quot;Años Totales de Personas&quot;,type = &quot;n&quot;, cex=.5) points(NurseDat01$SQRFOOT,NurseDat01$TPY,cex=.5)#,col=&quot;blue&quot;) taus &lt;- c(.05,.25,.75,.95) library(SparseM) library(quantreg) xx &lt;- seq(0,260,1) f &lt;- coef(rq((NurseDat01$TPY)~(NurseDat01$SQRFOOT),tau=taus)) yy &lt;- cbind(1,xx)%*%f for(i in 1:length(taus)){ lines(xx,yy[,i],col = &quot;gray&quot;) } abline(lm(NurseDat01$TPY ~ NurseDat01$SQRFOOT),lty = 2,col=&quot;red&quot;) abline(rq(NurseDat01$TPY ~ NurseDat01$SQRFOOT), col=&quot;blue&quot;) legend(10,400,c(&quot;Ajuste OLS&quot;, &quot;Ajuste LAD&quot;), lty = c(2,1), col = c(&quot;red&quot;,&quot;blue&quot;) ) Las regresiones por cuantiles funcionan bien en situaciones donde los mínimos cuadrados ordinarios requieren atención cuidadosa para usarse con confianza. Como se demostró en el ejemplo de Hogares de Ancianos en Wisconsin, la regresión por cuantiles maneja fácilmente distribuciones sesgadas y heterocedasticidad. Así como los cuantiles ordinarios son relativamente robustos a observaciones inusuales, las estimaciones de la regresión por cuantiles son mucho menos sensibles a observaciones atípicas que las rutinas de regresión habituales. 17.6 Modelos de Valores Extremos Los modelos de valores extremos se enfocan en los extremos, la “punta del iceberg,” como la temperatura más alta en un mes, el tiempo más rápido para correr un kilómetro o el menor rendimiento del mercado de valores. Algunos modelos de valores extremos están motivados por estadísticas máximas. Supongamos que consideramos la compensación anual de los directores ejecutivos (CEO) en un país, \\(y_1, y_2, \\ldots\\). Entonces, \\(M = \\max(y_1, \\ldots, y_n)\\) representa la compensación del CEO mejor pagado durante el año. Si los valores \\(y\\) fueran observados, podríamos usar algunos supuestos adicionales leves (como independencia) para hacer inferencias sobre la distribución de \\(M\\). Sin embargo, en muchos casos, solo se observa \\(M\\) directamente, lo que nos obliga a basar los procedimientos de inferencia en observaciones “extremas” \\(M\\). Como variación, podríamos tener observaciones de los 20 principales CEO, no de toda la población. Esta variación utiliza inferencia basada en las “20” estadísticas de orden más grandes, véase por ejemplo Coles (2003, Sección 3.5.2). El modelado de \\(M\\) a menudo se basa en la distribución generalizada de valores extremos, o \\(GEV\\), definida por la función de distribución \\[\\begin{equation} \\Pr(M \\leq x) = \\exp \\left[-(1+ \\gamma z )^{-1/\\gamma} \\right], \\tag{17.5} \\end{equation}\\] donde \\(z=(x-\\mu)/ \\sigma\\). Este es un modelo de ubicación-escala, con parámetros de ubicación y escala \\(\\mu\\) y \\(\\sigma\\), respectivamente. En el caso estándar donde \\(\\mu=0\\) y \\(\\sigma=1\\), permitiendo \\(\\gamma \\rightarrow \\infty\\) significa que \\(\\Pr(M \\leq x) \\rightarrow \\exp \\left[- e^{-x} \\right],\\) la distribución clásica de valores extremos. Por lo tanto, el parámetro \\(\\gamma\\) proporciona la generalización de esta distribución clásica. Beirlant, Goegebeur, Segers y Teugels (2004) discuten formas en que se podrían introducir covariables de regresión en la distribución \\(GEV\\), esencialmente permitiendo que cada parámetro dependa de las covariables. La estimación se realiza mediante máxima verosimilitud. En sus procedimientos de inferencia, el enfoque está en el comportamiento de los cuantiles extremos (condicionales a las covariables). Otro enfoque para modelar valores extremos es centrarse en datos que deben ser grandes para ser incluidos en la muestra. Ejemplo: Grandes Reclamos Médicos. Cebrián, Denuit y Lambert (2003) analizaron 75,789 grandes reclamos de seguros médicos grupales de 1991. Para ser incluidos en esta base de datos, los reclamos debían superar los 25,000. Por lo tanto, estos datos están truncados a la izquierda en 25,000. El interés en su estudio fue interpretar la distribución de cola larga en términos de las covariables edad y sexo. El enfoque de picos sobre el umbral para modelar valores extremos está motivado por datos truncados a la izquierda donde el punto de truncamiento, o “umbral,” es grande. Para ser incluidos en el conjunto de datos, las observaciones deben superar un umbral grande al que nos referimos como un “pico.” Siguiendo nuestra discusión en la Sección 14.2 sobre truncamiento, si \\(C_L\\) es el punto de truncamiento a la izquierda, entonces la función de distribución de \\(y-C_L\\) dado que \\(y&gt;C_L\\) es \\(1 - \\Pr(y-C_L &gt; x |y&gt;C_L)\\) \\(= 1 - (1-F_y(C_L+x))/(1-F_y(C_L))\\). En lugar de modelar directamente la distribución de \\(y\\), \\(F_y\\), como en secciones anteriores, se asume que puede aproximarse directamente mediante una distribución Pareto generalizada. Es decir, asumimos \\[\\begin{equation} \\Pr(y-C_L \\leq x |y&gt;C_L) \\approx 1 - (1+ \\frac{z}{\\theta} )^{-\\theta} , \\tag{17.6} \\end{equation}\\] donde \\(z=x / \\sigma\\), \\(\\sigma\\) es un parámetro de escala, \\(x \\geq 0\\) si \\(\\theta \\geq 0\\) y \\(0 \\leq x \\leq - \\theta\\) si \\(\\theta &lt; 0\\). Aquí, el lado derecho de la ecuación (17.6) es la distribución Pareto generalizada. La distribución Pareto usual restringe \\(\\theta\\) a ser positivo; esta especificación permite valores negativos de \\(\\theta\\). Permitir que \\(\\theta \\rightarrow 0\\) implica que \\(1 - (1+ z/\\theta )^{-\\theta} \\rightarrow 1 - e^{-x/\\sigma},\\) la distribución exponencial. Ejemplo: Grandes Reclamos Médicos - Continuación. Para incorporar las covariables edad y sexo, Cebrián et al. (2003) categorizaron las variables, permitieron que los parámetros variaran por categoría y estimaron cada categoría de manera independiente. Alternativamente, enfoques más eficientes se describen en el Capítulo 7 de Beirlant et al. (2004). 17.7 Lecturas Adicionales y Referencias La literatura sobre modelado de reclamos de cola larga está en constante desarrollo. Una referencia estándar es Klugman et al. (2008). Kleiber y Kotz (2003) ofrecen una excelente revisión de la literatura univariada, con muchas referencias históricas. Carroll y Ruppert (1988) proporcionan extensas discusiones sobre transformaciones en el modelado de regresión. Este capítulo ha enfatizado la distribución GB2 con sus muchos casos especiales. Venter (2007) analiza extensiones del modelo lineal generalizado, enfocándose en aplicaciones de reserva de pérdidas. Balasooriya y Low (2008) presentan aplicaciones recientes al modelado de reclamos de seguros, aunque sin covariables de regresión. Otro enfoque es usar una distribución elíptica sesgada (como la normal o \\(t\\)-). Bali y Theodossiou (2008) presentan una aplicación reciente, mostrando cómo usar tales distribuciones en el modelado de series temporales de rendimientos de acciones. Koenker (2005) es una excelente introducción extensa a la regresión por cuantiles. Yu, Lu y Stander (2003) ofrecen una introducción breve y accesible. Coles (2003) y Beirlant et al. (2004) son dos excelentes introducciones extensas a las estadísticas de valores extremos. Referencias del Capítulo Balasooriya, Uditha and Chan-Kee Low (2008). Modeling insurance claims with extreme observations: Transformed kernel density and generalized lambda distribution. North American Actuarial Journal 11(2) 129-142. Bali, Turan G. and Panayiotis Theodossiou (2008). Risk measurement of alternative distribution functions. Journal of Risk and Insurance 75(2), 411-437. Beirlant, Jan, Yuir Goegebeur, Robert Verlaak and Petra Vynckier (1998). Burr regression and portfolio segmentation. Insurance: Mathematics and Economics 23, 231-250. Beirlant, Jan, Yuir Goegebeur, Johan Segers and Jozef Teugels (2004). Statistics of Extremes. Wiley, New York. Burbidge, J.B. and Magee, L. and Robb, A.L. (1988). Alternative transformations to handle extreme values of the dependent variable. Journal of the American Statistical Association 83, 123-127. Carroll, Raymond and David Ruppert (1988). Transformation and Weighting in Regression. Chapman-Hall. Cebrián, Ana C., Michel Denuit and Philippe Lambert (2003). Generalized Pareto fit to the Society of Actuaries’ large claims database. North American Actuarial Journal 7 (3), 18-36. Coles, Stuart (2003). An Introduction to Statistical Modeling of Extreme Values. Springer, New York. Cummins, J. David, Georges Dionne, James B. McDonald and B. Michael Pritchett (1990). Applications of the GB2 family of distributions in modeling insurance loss processes. Insurance: Mathematics and Economics 9, 257-272. John, J. A. and Norman R. Draper (1980). An alternative family of transformations. Applied Statistics 29 (2), 190-197. Kleiber, Christian and Samuel Kotz (2003). Statistical Size Distributions in Economics and Actuarial Sciences. John Wiley and Sons, New York. Klugman, Stuart A, Harry H. Panjer and Gordon E. Willmot (2008). Loss Models: From Data to Decisions. John Wiley &amp; Sons, Hoboken, New Jersey. Koenker, Roger (2005). Quantile Regression. Cambridge University Press, New York. Manning, William G (1998). The logged dependent variable, heteroscedasticity, and the retransformation problem. Journal of Health Economics 17, 283-295. Manning, William G, Anirban Basu and John Mullahy (2005). Generalized modeling approaches to risk adjustment of skewed outcomes data. Journal of Health Economics 24, 465-488. McDonald, James B. and Richard J. Butler (1990). Regression models for positive random variables. Journal of Econometrics 43, 227-251. Rachev, Svetiozar, T., Christian Menn and Frank Fabozzi (2005). Fat-Tailed and Skewed Asset Return Distributions: Implications for Risk Management, Portfolio Selection, and Option Pricing. Wiley, New York. Rosenberg, Marjorie A., Edward W. Frees, Jiafeng Sun, Paul Johnson and James M. Robinson (2007). Predictive modeling with longitudinal data: A case study of Wisconsin nursing homes. North American Actuarial Journal 11(3), 54-69. Sun, Jiafeng, Edward W. Frees and Marjorie A. Rosenberg (2008). Heavy-tailed longitudinal data modeling using copulas. Insurance: Mathematics and Economics 42(2), 817-830. Venter, Gary (2007). Generalized linear models beyond the exponential family with loss reserve applications. Astin Bulletin: Journal of the International Actuarial Association 37 (2), 345-364. Yeo, In-Kwon and Richard A. Johnson (2000). A new family of power transformations to improve normality or symmetry. Biometrika 87, 954-959. Yu, Keming, Zudi Lu and Julian Stander (2003). Quantile regression: applications and current research areas. Journal of the Royal Statistical Society Series D (The Statistician) 52 (3), 331-350. 17.8 Ejercicios 17.1. Cuantiles y Simulación. Use la ecuación (17.2) para establecer las siguientes relaciones distribucionales que son útiles para calcular cuantiles. Suponga que \\(y_0 = \\alpha_1 F/\\alpha_2\\) donde \\(F\\) tiene una distribución \\(F\\) con grados de libertad del numerador y denominador \\(df_1 = 2 \\alpha_1\\) y \\(df_2 = 2 \\alpha_2\\). Muestre que \\(y\\) tiene una distribución GB2. Suponga que \\(y_0 = B/(1-B),\\) donde \\(B\\) tiene una distribución beta con parámetros \\(\\alpha_1\\) y \\(\\alpha_2\\). Muestre que \\(y\\) tiene una distribución GB2. Describa cómo usar las partes (a) y (b) para calcular cuantiles. Describa cómo usar las partes (a) y (b) para simulación. 17.2 Considere una función de densidad de probabilidad GB2 dada en la ecuación (17.3). Reparametrize la distribución definiendo el nuevo parámetro \\(\\theta =e^{\\mu }.\\) Muestre que la densidad puede expresarse como: \\[ \\mathrm{f}_{GB2}(y;\\theta, \\sigma ,\\alpha _1,\\alpha _2)=\\frac{\\Gamma \\left( \\alpha _1+\\alpha _2\\right) }{\\Gamma \\left( \\alpha _1\\right) \\Gamma \\left( \\alpha _2\\right) }\\frac{\\left( y/\\theta \\right) ^{\\alpha _2/\\sigma }}{\\sigma y\\left[ 1+\\left( y/\\theta \\right) ^{1/\\sigma }\\right] ^{\\alpha _1+\\alpha _2}}, \\] Usando la parte (a), muestre que \\[ \\lim_{\\alpha _2\\rightarrow \\infty }\\mathrm{f}_{GB2}(y; \\theta \\alpha _2^{\\sigma },\\sigma ,\\alpha _1,\\alpha _2)=\\frac{1}{\\sigma y\\Gamma \\left( \\alpha _1\\right) }\\left( y/\\theta \\right) ^{\\alpha _1/\\sigma }\\exp \\left( -\\left( y/\\theta \\right) ^{1/\\sigma }\\right) =\\mathrm{f}_{GG}(y;\\theta, \\sigma, \\alpha _1), \\] una densidad gamma generalizada. Usando la parte (a), muestre que \\[ \\mathrm{f}_{GB2}(y;\\theta, \\sigma, 1, \\alpha_2)=\\frac{\\alpha _2\\left( y/\\theta \\right) ^{\\alpha _2/\\sigma }}{\\sigma y\\left[ 1+\\left( y/\\theta \\right) ^{1/\\sigma }\\right] ^{1+\\alpha _2}}=\\mathrm{f}_{Burr}(y;\\theta, \\sigma, \\alpha _2), \\] una densidad Burr Tipo 12. 17.3 Recuerde que la densidad de una distribución gamma con parámetro de forma \\(\\alpha\\) y parámetro de escala \\(\\theta\\) tiene una densidad dada por \\(\\mathrm{f}(y)=\\left[ \\theta ^{\\alpha }\\Gamma \\left( \\alpha \\right) \\right] ^{-1}y^{\\alpha -1}\\exp \\left( -y/\\theta \\right)\\) y el momento \\(k\\)-ésimo dado por \\(\\mathrm{E} (y^{k})=\\theta ^{k}\\Gamma \\left( \\alpha +k\\right) /\\Gamma \\left( \\alpha \\right)\\), para \\(k&gt;-\\alpha.\\) Para la distribución GB2, muestre que \\[ \\mathrm{E}(y)=e^{\\mu }\\frac{\\Gamma \\left( \\alpha _1+\\sigma \\right) \\Gamma \\left( \\alpha _2-\\sigma \\right) }{\\Gamma \\left( \\alpha _1\\right) \\Gamma \\left( \\alpha _2\\right) }. \\] Para la distribución gamma generalizada, muestre que \\[ \\mathrm{E}(y)=e^{\\mu }\\Gamma \\left( \\alpha_1 +\\sigma \\right) /\\Gamma \\left( \\alpha_1 \\right) . \\] Calcule los momentos de una densidad Burr Tipo 12. "],["C18Cred.html", "Capítulo 18 Credibilidad y Bonus-Malus 18.1 Clasificación de Riesgos y Experiencia 18.2 Credibilidad 18.3 Credibilidad y Regresión 18.4 Bonus-Malus", " Capítulo 18 Credibilidad y Bonus-Malus Vista previa del capítulo. Este capítulo introduce aplicaciones de regresión para la fijación de precios en los sistemas de experiencia de credibilidad y bonus-malus. Los sistemas de experiencia son métodos formales para incluir la experiencia de reclamos en las primas de renovación de contratos a corto plazo, como los de automóvil, salud y compensación laboral. Este capítulo proporciona breves introducciones a la credibilidad y el bonus-malus, enfatizando su relación con los métodos de regresión. 18.1 Clasificación de Riesgos y Experiencia La clasificación de riesgos es un ingrediente clave en la fijación de precios de seguros. Las aseguradoras venden cobertura a precios que son suficientes para cubrir los reclamos anticipados, los gastos administrativos y un beneficio esperado para compensar el costo del capital necesario para respaldar la venta de la cobertura. En muchos países y líneas de negocio, el mercado de seguros es maduro y altamente competitivo. Esta fuerte competencia induce a las aseguradoras a clasificar los riesgos que suscriben con el fin de recibir primas justas por el riesgo asumido. Esta clasificación se basa en características conocidas del asegurado, la persona o empresa que busca la cobertura de seguro. Por ejemplo, suponga que trabaja para una compañía que asegura pequeñas empresas por el tiempo perdido debido a empleados lesionados en el trabajo. Considere fijar el precio de este producto de seguro para dos empresas que son idénticas en cuanto al número de empleados, ubicación, distribución por edad y género, y así sucesivamente, excepto que una empresa es una firma de consultoría de gestión y la otra es una empresa de construcción. Basado en la experiencia, usted espera que la firma de consultoría tenga un nivel de reclamos menor que la empresa de construcción y necesita fijar precios en consecuencia. Si no lo hace, otra compañía de seguros ofrecerá un precio más bajo a la firma de consultoría y atraerá a este cliente potencial, dejando a su empresa solo con el negocio más costoso de la construcción. La competencia entre aseguradoras conduce a cobrar primas según características observables, conocidas como clasificación de riesgos. En el contexto del modelado de regresión, esto puede considerarse como modelar las distribuciones de reclamos en términos de variables explicativas. Muchas situaciones de fijación de precios se basan en una relación entre el asegurador y el asegurado que se desarrolla con el tiempo. Estas relaciones permiten a las aseguradoras basar los precios en características no observables del asegurado teniendo en cuenta la experiencia previa de reclamos del asegurado. Modificar las primas con el historial de reclamos se conoce como experiencia, a veces también llamada calificación por méritos. Los métodos de experiencia se aplican de manera retrospectiva o prospectiva. Con métodos retrospectivos, se proporciona un “reembolso” de una parte de la prima al asegurado en caso de experiencia favorable (para el asegurador). Las primas retrospectivas son comunes en arreglos de seguros de vida (donde los asegurados ganan “dividendos” en los EE.UU. y “bonificaciones” en el Reino Unido). En seguros de bienes y accidentes, los métodos prospectivos son más comunes, donde la experiencia favorable del asegurado se “recompensa” a través de una prima de renovación más baja. En este capítulo, discutimos dos métodos prospectivos que son adecuados para el modelado de regresión, la credibilidad y el bonus-malus. Los métodos de bonus-malus se usan ampliamente en Asia y Europa, aunque casi exclusivamente en seguros de automóviles. Como veremos en la Sección 18.4, la idea es usar la experiencia de reclamos para modificar la clasificación de un asegurado. Los métodos de credibilidad, introducidos en la Sección 18.2, se aplican más ampliamente en términos de líneas de negocio y geografía. 18.2 Credibilidad La credibilidad es una técnica para fijar precios de coberturas de seguro que es ampliamente utilizada por actuarios en salud, seguros de vida colectivos y seguros de bienes y accidentes. En los Estados Unidos, los estándares se describen bajo el Actuarial Standard of Practice Number 25 publicado por la Actuarial Standards Board de la American Academy of Actuaries (sitio web: http://www.actuary.org/). Además, varias leyes y regulaciones de seguros requieren el uso de credibilidad. La teoría de la credibilidad ha sido llamada un “pilar fundamental” del campo de la ciencia actuarial (Hickman y Heacox, 1999). La idea básica es usar la experiencia de reclamos y la información adicional para desarrollar una fórmula de precios, como a través de la relación \\[\\begin{equation} Nueva~Prima = \\zeta \\times Experiencia~de~Reclamos + (1 - \\zeta) \\times Prima~Anterior. \\tag{18.1} \\end{equation}\\] Aquí, \\(\\zeta\\) (la letra griega “zeta”) se conoce como el “factor de credibilidad;” los valores generalmente se encuentran entre cero y uno. El caso \\(\\zeta=1\\) se conoce como “credibilidad total,” donde solo se utiliza la experiencia de reclamos para determinar la prima. El caso \\(\\zeta=0\\) puede considerarse como “sin credibilidad,” donde se ignora la experiencia de reclamos y se utiliza información externa como la única base para fijar precios. Para que este capítulo sea autosuficiente, comenzamos introduciendo algunos conceptos básicos de credibilidad. La Sección 18.2.1 revisa los conceptos clásicos de credibilidad, incluyendo cuándo usarla y las fórmulas de precios lineales. La Sección 18.2.2 describe la versión moderna de la credibilidad introduciendo un modelo probabilístico formal que puede usarse para actualizar los precios de los seguros. La Sección 18.3 discute el vínculo con el modelado de regresión. 18.2.1 Credibilidad de Fluctuación Limitada La credibilidad tiene una larga historia en la ciencia actuarial, con contribuciones fundamentales que se remontan a Mowbray (1914). Posteriormente, Whitney (1918) introdujo el concepto intuitivamente atractivo de usar un promedio ponderado de (1) reclamos de la clase de riesgo y (2) reclamos de todas las clases de riesgo para predecir los reclamos esperados futuros. Estándares para la Credibilidad Completa El título del artículo de Mowbray fue “¿Qué tan extenso debe ser un historial de exposición a la nómina para obtener una prima pura confiable?” Sigue siendo la primera pregunta que un analista debe enfrentar: ¿cuándo necesito usar estimadores de credibilidad? Para comprender mejor esta pregunta, considere la siguiente situación. Ejemplo: Costos Dentales. Suponga que está fijando el precio de la cobertura de seguro dental para un pequeño empleador. Para hombres de entre 18 y 25 años, el empleador proporciona la siguiente experiencia: \\[ \\small{ \\begin{array}{l|rrr} \\hline \\text{Año} &amp; 2007 &amp; 2008 &amp; 2009 \\\\ \\hline \\text{Número} &amp; 8 &amp; 12 &amp; 10 \\\\ \\text{Costo Dental Promedio} &amp; 500 &amp; 400 &amp; 900 \\\\ \\hline \\end{array} } \\] La “tarifa manual,” disponible a partir de una tabulación de un conjunto de datos mucho mayor, es $700 por empleado. Ignorando la inflación y los gastos, ¿qué usaría para anticipar los costos dentales en 2010? ¿La tarifa manual? ¿El promedio de los datos disponibles? ¿O alguna combinación? Mowbray quería distinguir entre situaciones cuando (1) grandes empleadores con información sustancial podían usar su propia experiencia y (2) pequeños empleadores con experiencia limitada debían usar fuentes externas, conocidas como “tarifas manuales.” En terminología estadística, podemos pensar en formar un estimador de los costos medios verdaderos a partir de la experiencia de un empleador. Nos estamos preguntando si la distribución del estimador está lo suficientemente cerca de la media para ser confiable. Por supuesto, “suficientemente cerca” es la parte difícil, así que veamos una situación más concreta. El conjunto más simple supone que tiene reclamos \\(y_1, \\ldots, y_n\\) que son idénticamente e independientemente distribuidos (i.i.d.) con media \\(\\mu\\) y varianza \\(\\sigma^2\\). Como estándar para la credibilidad completa, podríamos requerir que \\(n\\) sea lo suficientemente grande para que \\[\\begin{equation} \\Pr ( (1-r) \\mu \\leq \\bar{y} \\leq (1+r) \\mu) \\geq p, \\tag{18.2} \\end{equation}\\] donde \\(r\\) y \\(p\\) son constantes dadas. Por ejemplo, si \\(r=0.05\\) y \\(p=0.9\\), entonces deseamos tener al menos un 90% de probabilidad de estar dentro del 5% de la media. Usando aproximaciones normales, es sencillo demostrar que es suficiente para la ecuación (18.2) que \\[\\begin{equation} n \\geq \\left(\\frac{\\Phi^{-1}(\\frac{p+1}{2}) \\sigma}{r \\mu} \\right)^2 . \\tag{18.3} \\end{equation}\\] Definimos \\(n_F\\), el número de observaciones requerido para la credibilidad completa, como el valor más pequeño de \\(n\\) que satisface la ecuación (18.3). Ejemplo: Costos Dentales - Continuación. De la tabla, los costos promedio son \\(\\bar{y} = \\left( 500 \\times 8 + 400 \\times 12 + 900 \\times 10 \\right)/30 = 593.33.\\) Suponga que tenemos disponible una estimación de la desviación estándar \\(\\sigma \\approx \\widehat{\\sigma}= 200.\\) Usando \\(p=0.90\\), el percentil 90 de la distribución normal es \\(\\Phi^{-1}(.95) = 1.645\\). Con \\(r=0.05\\), el tamaño de muestra aproximado requerido es \\[ \\left(\\frac{\\Phi^{-1}(.95) \\widehat{\\sigma}}{r \\bar{y}} \\right)^2 = \\left(\\frac{1.645 \\times 200}{0.05 \\times 593.33} \\right)^2= 122.99, \\] o \\(n_F=123\\). Basándonos en una muestra de tamaño 30, no tenemos suficientes observaciones para credibilidad completa. Los estándares para credibilidad completa dados en las ecuaciones (18.2) y (18.3) se basan en la aproximación de normalidad. Es fácil construir reglas similares para otras distribuciones, como datos de conteo binomiales y de Poisson o combinaciones de distribuciones para pérdidas agregadas. Consulte Klugman, Panjer y Willmot (2008) para más detalles. Credibilidad Parcial Los actuarios no siempre trabajan con conjuntos de datos masivos. Puede que esté trabajando con la experiencia de un pequeño empleador o asociación y no tenga suficiente experiencia para cumplir con el estándar de credibilidad completa. O puede que esté trabajando con un gran empleador pero haya decidido descomponer sus datos en subconjuntos pequeños y homogéneos. Por ejemplo, si está trabajando con reclamos dentales, puede que desee crear varios grupos pequeños basados en la edad y el género. Para grupos más pequeños que no cumplen con el umbral de credibilidad completa, Witney (1918) propuso usar un promedio ponderado entre la experiencia de reclamos del grupo y una tarifa manual. Suponiendo una aproximación de normalidad, la expresión para credibilidad parcial es \\[\\begin{equation} Nueva~Prima = Z \\times \\bar{y} + (1 - Z) \\times Prima~Manual, \\tag{18.4} \\end{equation}\\] donde \\(Z\\) es el “factor de credibilidad,” definido como \\[\\begin{equation} Z = \\min{\\LARGE\\{}1,\\sqrt{\\frac{n}{n_F}} ~~{\\LARGE\\}}. \\tag{18.5} \\end{equation}\\] Aquí, \\(n\\) es el tamaño de la muestra y \\(n_F\\) es el número de observaciones requerido para credibilidad completa. Ejemplo: Costos Dentales - Continuación. Según el trabajo anterior, el estándar para credibilidad completa es \\(n_F = 123\\). Así, el factor de credibilidad es \\(\\min\\{1,\\sqrt{\\frac{30}{123}} \\} = 0.494.\\) Con esto, la prima de credibilidad parcial es \\[ Nueva~Prima = 0.494 \\times 593.33 + (1 - 0.494) \\times 700 = 647.31. \\] Una justificación para las fórmulas de credibilidad parcial en las ecuaciones (18.4) y (18.5) se presenta en los ejercicios. Según la ecuación (18.5), vemos que el factor de credibilidad \\(Z\\) está limitado entre 0 y 1; a medida que el tamaño de la muestra \\(n\\) y, por lo tanto, la experiencia aumentan, \\(Z\\) tiende a 1. Esto significa que los grupos más grandes son más “creíbles.” A medida que el factor de credibilidad \\(Z\\) aumenta, se le da mayor peso a la experiencia del grupo (\\(\\bar{y}\\)). A medida que \\(Z\\) disminuye, se le da más peso a la prima manual, la tarifa que se desarrolla externamente basada en las características del grupo. 18.2.2 Credibilidad de Máxima Precisión La teoría de la credibilidad fue utilizada durante más de cincuenta años en la fijación de precios de seguros antes de que se estableciera sobre una base matemática firme por Bühlmann (1967). Para introducir este marco, a veces conocido como “credibilidad de máxima precisión,” comencemos con la suposición de que tenemos una muestra de reclamos \\(y_1, \\ldots, y_n\\) de un pequeño grupo y deseamos estimar la media para este grupo. Aunque el promedio muestral \\(\\bar{y}\\) es ciertamente un estimador razonable, el tamaño de la muestra puede ser demasiado pequeño para depender exclusivamente de \\(\\bar{y}\\). También suponemos que tenemos una estimación externa de la media general de reclamos, \\(M\\), que consideramos como una “prima manual.” La pregunta es si podemos combinar las dos estimaciones, \\(\\bar{y}\\) y \\(M\\), para proporcionar un estimador que sea superior a cualquiera de las alternativas. Bühlmann planteó la hipótesis de la existencia de características no observadas del grupo que denotamos como \\(\\alpha\\); él se refirió a estas como “variables estructurales.” Aunque no observadas, estas características son comunes a todas las observaciones del grupo. Para reclamos dentales, las variables estructurales pueden incluir la calidad del agua donde se encuentra el grupo, el número de dentistas que proporcionan atención preventiva en el área, el nivel educativo del grupo, y otros factores. Por lo tanto, asumimos que, condicionado a \\(\\alpha\\), \\(\\{y_1, \\ldots, y_n\\}\\) son una muestra aleatoria de una población desconocida y, por ende, son i.i.d. Por notación, dejaremos que \\(\\mathrm{E}(y | \\alpha)\\) denote los reclamos esperados condicionales y \\(\\mathrm{Var}(y | \\alpha)\\) sea la varianza condicional correspondiente. Nuestro objetivo es determinar un “estimador” razonable de \\(\\mathrm{E}(y | \\alpha)\\). Aunque no observadas, podemos aprender algo sobre las características \\(\\alpha\\) a partir de observaciones repetidas de los reclamos. Para cada grupo, las funciones de media y varianza (condicionales) son \\(\\mathrm{E}(y | \\alpha)\\) y \\(\\mathrm{Var}(y | \\alpha)\\), respectivamente. La expectativa sobre todos los grupos de las funciones de varianza es E \\(\\mathrm{Var}(y | \\alpha)\\). De manera similar, la varianza de las expectativas condicionales es Var \\(\\mathrm{E}(y | \\alpha)\\). Con estas cantidades en mano, podemos dar la prima de credibilidad de Bühlmann. \\[\\begin{equation} Nueva~Prima = \\zeta \\times \\bar{y} + (1 - \\zeta) \\times M, \\tag{18.6} \\end{equation}\\] donde \\(\\zeta\\) es el “factor de credibilidad,” definido como \\[\\begin{equation} \\zeta = \\frac{n}{n+Ratio}, ~\\mathrm{donde}~~~~~~~~Ratio = \\frac{\\mathrm{E~}\\mathrm{Var}(y | \\alpha)}{\\mathrm{Var}~\\mathrm{E}(y | \\alpha)}. \\tag{18.7} \\end{equation}\\] La fórmula de credibilidad en la ecuación (18.6) es la misma que la fórmula clásica de credibilidad parcial en la ecuación (18.4), con el factor de credibilidad \\(\\zeta\\) en lugar de \\(Z\\). Por lo tanto, comparte la misma expresión intuitivamente agradable como un promedio ponderado. Además, ambos factores de credibilidad están en el intervalo \\((0,1)\\) y ambos aumentan hacia uno a medida que el tamaño de la muestra \\(n\\) aumenta. Ejemplo: Costos Dentales - Continuación. A partir del trabajo anterior, sabemos que una estimación de la media condicional es 593.33. Use cálculos similares para mostrar que la varianza condicional estimada es 48,622.22. Ahora supongamos que hay tres grupos adicionales con medias y varianzas condicionales dadas como sigue: \\[ \\small{ \\begin{array}{c|cccc} \\hline &amp; &amp; \\text{Media} &amp; \\text{Varianza} \\\\ &amp; \\text{Variable} &amp; \\text{Condicional} &amp; \\text{Condicional} &amp; \\text{Probabilidad} \\\\ \\text{Grupo} &amp; \\text{No Observada} &amp; \\text{E} (y|\\alpha) &amp; \\text{Var} (y|\\alpha) &amp; \\Pr(\\alpha) \\\\ \\hline 1 &amp; \\alpha_1 &amp; 593.33 &amp; 48,622.22 &amp; 0.20 \\\\ 2 &amp; \\alpha_2&amp; 625.00 &amp; 50,000.00 &amp; 0.30 \\\\ 3 &amp; \\alpha_3&amp; 800.00 &amp; 70,000.00 &amp; 0.25 \\\\ 4 &amp; \\alpha_4&amp; 400.00 &amp; 40,000.00 &amp; 0.25 \\\\ \\hline \\end{array} } \\] Suponemos que la probabilidad de pertenecer a un grupo está dada como \\(\\Pr(\\alpha)\\). Por ejemplo, esto puede determinarse tomando las proporciones del número de miembros en cada grupo. Con esta información, es sencillo calcular la varianza condicional esperada, \\[ \\mathrm{E~}\\mathrm{Var}(y | \\alpha) = 0.2(48622.22) + 0.3(50000) + .25(70000) + .25(40000) = 52,224.44 . \\] Para calcular la varianza de las expectativas condicionales, se puede comenzar con la expectativa general \\[ \\mathrm{E~}\\mathrm{E}(y | \\alpha) = 0.2(593.33) + 0.3(625) + .25(800) + .25(400) = 606.166, \\] y luego usar un procedimiento similar para calcular el valor esperado del segundo momento condicional, \\(\\mathrm{E~}(\\mathrm{E}(y | \\alpha))^2 = 387,595.6\\). Con estas dos piezas, la varianza de las expectativas condicionales es \\(\\mathrm{E~}\\mathrm{Var}(y | \\alpha)\\) \\(= 387,595.6 - 606.166^2 = 20,158.\\) Esto produce el \\(Ratio=52224.44/20158 = 2.591\\) y, por lo tanto, el factor de credibilidad \\(\\zeta = \\frac{30}{30+2.591} = 0.9205\\). Con esto, la prima de credibilidad es \\[ Nueva~Prima = 0.9205 \\times 593.33 + (1 - 0.9205) \\times 700 = 601.81. \\] Para ver cómo usar la fórmula de credibilidad con distribuciones alternativas, considere lo siguiente. Ejemplo: Credibilidad con Datos de Conteo. Suponga que el número de reclamos cada año para un asegurado individual tiene una distribución de Poisson. La frecuencia de reclamos esperada anual de toda la población de asegurados está distribuida uniformemente en el intervalo (0,1). La frecuencia de reclamos esperada de un individuo es constante en el tiempo. Considere un asegurado particular que tuvo 3 reclamos durante los tres años anteriores. Bajo estas suposiciones, tenemos que los reclamos de un individuo \\(y\\) con características latentes \\(\\alpha\\) tienen una distribución de Poisson con media condicional \\(\\alpha\\) y varianza condicional \\(\\alpha\\). La distribución de \\(\\alpha\\) es uniforme en el intervalo (0,1), por lo que cálculos sencillos muestran que \\[ \\mathrm{E}~\\mathrm{Var}(y|\\alpha) = \\mathrm{E}~\\alpha = 0.5~~~\\mathrm{y}~~~ \\mathrm{Var}~\\mathrm{E}(y|\\alpha) = \\mathrm{Var}~\\alpha = 1/12 = 0.08333. \\] Por lo tanto, con \\(n=3\\), el factor de credibilidad es \\[ \\zeta = \\frac{3}{3+ 0.5/0.08333} = 0.3333. \\] Con \\(\\bar{y}=3/3 =1\\) y la media general \\(\\mathrm{E}~\\mathrm{E}(y|\\alpha)=0.5\\) como la prima manual, la prima de credibilidad es \\[ Nueva~Prima = 0.3333 \\times 1 + (1 - 0.3333) \\times 0.5 = 0.6667. \\] Más formalmente, la optimalidad del estimador de credibilidad se basa en lo siguiente. Propiedad. Suponga que, condicionado a \\(\\alpha\\), \\(\\{y_1, \\ldots, y_n \\}\\) son idénticamente e independientemente distribuidos con media y varianza condicionales \\(\\mathrm{E}(y | \\alpha)\\) y \\(\\mathrm{Var}(y | \\alpha)\\), respectivamente. Suponga que deseamos estimar \\(\\mathrm{E}~(y_{n+1}|\\alpha)\\). Entonces, la prima de credibilidad dada en las ecuaciones (18.6) y (18.7) tiene la menor varianza dentro de la clase de todos los predictores lineales no sesgados. Esta propiedad indica que la prima de credibilidad tiene la “máxima precisión” en el sentido de que tiene mínima varianza entre los predictores lineales no sesgados. Como hemos visto, está expresada en términos de medias y varianzas que pueden aplicarse a muchas distribuciones; a diferencia de la prima de credibilidad parcial, no hay suposición de normalidad. Es un resultado fundamental en el sentido de que se basa en observaciones (condicionalmente) i.i.d. No sorprendentemente, es fácil modificar este resultado básico para permitir diferentes exposiciones para las observaciones, tendencias en el tiempo, entre otros. La propiedad no especifica cómo se estimarían las cantidades asociadas con la distribución de \\(\\alpha\\). Para hacerlo, introduciremos un esquema de muestreo más detallado que nos permitirá incorporar métodos de regresión. Aunque no es la única forma de muestreo, este marco nos permitirá introducir muchas variaciones de interés y ayudará a interpretar la credibilidad de manera natural. 18.3 Credibilidad y Regresión Al expresar la credibilidad en el marco de modelos de regresión, los actuarios pueden obtener varios beneficios: Los modelos de regresión ofrecen una amplia variedad de modelos entre los cuales elegir. El software estadístico estándar facilita el análisis de datos. Los actuarios tienen otro método para explicar el proceso de fijación de tarifas. Los actuarios pueden utilizar herramientas gráficas y de diagnóstico para seleccionar un modelo y evaluar su utilidad. 18.3.1 Modelo de Efectos Aleatorios Unidireccional Supongamos que estamos interesados en fijar precios para \\(n\\) grupos y que para cada uno de los grupos \\(i=1,\\ldots,n\\), tenemos experiencia de reclamos \\(y_{it}, t=1, \\ldots, T\\). Aunque esta configuración de datos longitudinales se introdujo en el Capítulo 10, no necesitamos asumir que los reclamos evolucionan con el tiempo; los subíndices \\(t\\) pueden representar diferentes miembros de un grupo. Para comenzar, suponemos que no tenemos variables explicativas. La experiencia de reclamos sigue \\[\\begin{equation} y_{it} = \\mu + \\alpha_i + \\varepsilon_{it}, ~~~~~ t=1, \\ldots, T, i=1,\\ldots, n, \\tag{18.8} \\end{equation}\\] donde \\(\\mu\\) representa un promedio general de reclamos, \\(\\alpha_i\\) las características no observadas del grupo y \\(\\varepsilon_{it}\\) la variación individual de los reclamos. Suponemos que \\(\\{\\alpha_i\\}\\) son i.i.d. con media cero y varianza \\(\\sigma^2_{\\alpha}\\). Además, asumimos que \\(\\{\\varepsilon_{it}\\}\\) son i.i.d. con media cero y varianza \\(\\sigma^2\\) y son independientes de \\(\\alpha_i\\). Estas son las suposiciones de un modelo básico de “efectos aleatorios unidireccional” descrito en la Sección 10.5. Parece razonable usar la cantidad \\(\\mu + \\alpha_i\\) para predecir un nuevo reclamo del grupo \\(i\\)-ésimo. Para el modelo en la ecuación (18.8), parece intuitivamente plausible que \\(\\bar{y}\\) sea un estimador deseable de \\(\\mu\\) y que \\(\\bar{y}_i-\\bar{y}\\) sea un “estimador” deseable de \\(\\alpha_i\\). Así, \\(\\bar{y}_i\\) es un predictor deseable de \\(\\mu+\\alpha_i\\). Más generalmente, considere predictores de \\(\\mu+\\alpha_i\\) que sean combinaciones lineales de \\(\\bar{y}_i\\) y \\(\\bar{y}\\), es decir, \\(c_1 \\bar{y}_i+c_2\\bar{y}\\), para constantes \\(c_1\\) y \\(c_2\\). Para mantener la no sesgadez, usamos \\(c_2 = 1 - c_1\\). Algunos cálculos básicos muestran que el mejor valor de \\(c_1\\) que minimiza \\(\\mathrm{E} \\left( c_1 \\bar{y}_i+(1-c_1)\\bar{y} - (\\mu+\\alpha_i) \\right)^2\\) es \\[ c_1 = \\frac{T}{T+\\sigma^2/\\sigma^2_{\\alpha}} = \\zeta, \\] el factor de credibilidad. Esto produce el estimador de contracción, o predictor, de \\(\\mu+\\alpha_i\\), definido como \\[\\begin{equation} \\bar{y}_{i,s} = \\zeta \\bar{y}_i+(1-\\zeta)\\bar{y}. \\tag{18.9} \\end{equation}\\] El estimador de contracción es equivalente a la prima de credibilidad cuando vemos que \\[ \\mathrm{Var} \\left(\\mathrm{E}(y_{it}|\\alpha_i) \\right) = \\mathrm{Var} \\left(\\mathrm{E}(\\mu+\\alpha_i) \\right) = \\sigma^2_{\\alpha} \\] y \\[ \\mathrm{E} \\left(\\mathrm{Var}(y_{it}|\\alpha_i) \\right) = \\mathrm{E} \\left(\\mathrm{E}(\\sigma^2) \\right) = \\sigma^2 , \\] por lo que \\(Ratio = \\sigma^2/\\sigma^2_{\\alpha}\\). Así, el modelo de efectos aleatorios unidireccional a veces se conoce como el modelo “Bühlmann balanceado.” Este estimador de contracción también es un mejor predictor lineal no sesgado (\\(BLUP\\)), introducido en la Sección 15.1.3. Ejemplo: Visualización de Contracción. Considere los siguientes datos ilustrativos: \\[ \\small{ \\begin{array}{c|cccc|c} \\hline \\text{Grupo}&amp;&amp;&amp;&amp;&amp;\\text{Promedio}\\\\ i&amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; \\text{del Grupo } (\\bar{y}_i)\\\\ \\hline 1 &amp; 14 &amp; 12 &amp; 10 &amp; 12 &amp; 12 \\\\ 2 &amp; 9 &amp; 16 &amp; 15 &amp; 12 &amp; 13 \\\\ 3 &amp; 8 &amp; 10 &amp; 7 &amp; 7 &amp; 8 \\\\ \\hline \\end{array} } \\] Es decir, tenemos \\(n=3\\) grupos, cada uno con \\(T=4\\) observaciones. La media muestral es \\(\\bar{y} = 11\\) y las medias muestrales específicas por grupo son \\(\\bar{y}_1=12\\), \\(\\bar{y}_2=13\\) y \\(\\bar{y}_3=8\\). Ahora ajustamos el modelo ANOVA de efectos aleatorios unidireccional en la ecuación (18.8) utilizando la estimación de máxima verosimilitud, suponiendo normalidad. El software estadístico estándar muestra que las estimaciones de \\(\\sigma^2\\) y \\(\\sigma^2_{\\alpha}\\) son 4.889 y 5.778, respectivamente. Esto implica que el factor \\(\\zeta\\) estimado es 0.738. Usando la ecuación (18.9), las predicciones correspondientes para los grupos son 11.738, 12.476 y 8.786, respectivamente. La Figura 18.1 compara las medias específicas por grupo con las predicciones correspondientes. Aquí, observamos una menor dispersión en las predicciones en comparación con las medias específicas por grupo; la estimación de cada grupo se “contrae” hacia la media general, \\(\\bar{y}\\). Estas son las mejores predicciones asumiendo que \\(\\alpha_i\\) son aleatorias. En contraste, las medias específicas por grupo son las mejores predicciones asumiendo que \\(\\alpha_i\\) son determinísticas. Así, este “efecto de contracción” es una consecuencia de la especificación de efectos aleatorios. Figura 18.1: Comparación de las Medias Específicas por Grupo con las Estimaciones de Contracción. Para un conjunto de datos ilustrativo, las medias específicas por grupo y la media general se grafican en la escala superior. Las estimaciones de contracción correspondientes se grafican en la escala inferior. Esta figura muestra el aspecto de contracción de los modelos con efectos aleatorios. Bajo el modelo de efectos aleatorios unidireccionales, tenemos que \\(\\bar{y}_i\\) es un predictor no sesgado de \\(\\mu+\\alpha_i\\) en el sentido de que E (\\(\\bar{y}_i - (\\mu+\\alpha_i)\\))=0. Sin embargo, \\(\\bar{y}_i\\) es ineficiente en el sentido de que el estimador de contracción, \\(\\bar{y}_{i,s}\\), tiene un error cuadrático medio menor que \\(\\bar{y}_i\\). Intuitivamente, dado que \\(\\bar{y}_{i,s}\\) es una combinación lineal de \\(\\bar{y}_i\\) y \\(\\bar{y}\\), decimos que ha sido “contraído” hacia el estimador \\(\\bar{y}\\). Además, debido a la información adicional en \\(\\bar{y}_{i,s}\\), es habitual interpretar un estimador de contracción como “tomar prestada fuerza” del estimador de la media general. Tenga en cuenta que el estimador de contracción se reduce al estimador de efectos fijos \\(\\bar{y}_i\\) cuando el factor de credibilidad, \\(\\zeta\\), se convierte en 1. Es fácil ver que \\(\\zeta \\rightarrow 1\\) cuando (i) \\(T\\rightarrow\\infty\\) o (ii) \\(\\sigma^2/\\sigma^2_{\\alpha}\\rightarrow 0\\). Es decir, el mejor predictor se aproxima a la media del grupo cuando (i) el número de observaciones por grupo se vuelve grande o (ii) la variabilidad entre grupos se vuelve grande en relación con la variabilidad de la respuesta. En lenguaje actuarial, cualquiera de los casos respalda la idea de que la información del grupo \\(i\\) se está volviendo más “creíble.” 18.3.2 Modelos Longitudinales Como hemos visto en el contexto del modelo de efectos aleatorios unidireccionales con replicaciones balanceadas, el estimador de credibilidad de máxima precisión de Bühlmann es equivalente al mejor predictor lineal no sesgado presentado en la Sección 15.1.3. Esto también es cierto al considerar una configuración de muestreo longitudinal más general introducida en el Capítulo 10 (ver, por ejemplo, Frees, Young y Luo, 1999). Al expresar el problema de credibilidad en términos de un esquema de muestreo basado en regresión, podemos usar técnicas de regresión bien conocidas para estimar parámetros y predecir cantidades desconocidas. Ahora consideramos un modelo longitudinal que aborda muchos casos especiales de interés en la práctica actuarial. Supongamos que la experiencia de reclamos sigue \\[\\begin{equation} y_{it} = M_i + \\mathbf{x}_{it}^{\\prime} \\boldsymbol \\beta+ \\mathbf{z}_{it}^{\\prime} {\\boldsymbol \\alpha}_i + \\varepsilon_{it},~~~~~ t=1, \\ldots, T_i, i=1,\\ldots, n. \\tag{18.10} \\end{equation}\\] Consideremos cada componente del modelo por turno: \\(M_i\\) representa la prima manual que se supone conocida. En el lenguaje de los modelos lineales generalizados, \\(M_i\\) es una “variable de ajuste.” Al estimar el modelo de regresión, simplemente se usa \\(y_{it}^{\\ast} = y_{it} - M_i\\) como la variable dependiente. Si la prima manual no está disponible, entonces tome \\(M_i = 0\\). \\(\\mathbf{x}_{it}^{\\prime} \\boldsymbol \\beta\\) es la combinación lineal usual de variables explicativas. Estas pueden usarse para ajustar la prima manual. Por ejemplo, puede tener una proporción mayor de lo típico de hombres (o mujeres) en su grupo y desear considerar el género de los miembros del grupo. Esto puede hacerse utilizando una variable binaria de género como predictor y pensando en el coeficiente de regresión como la cantidad para ajustar la prima manual. De manera similar, podría usar la edad, experiencia u otras características de los miembros del grupo para ajustar las primas manuales. Las variables explicativas también pueden describir al grupo, no solo a los miembros. Por ejemplo, puede desear incluir variables explicativas que proporcionen información sobre la ubicación del lugar de trabajo (como urbano versus rural) para ajustar las primas manuales. \\(\\mathbf{z}_{it}^{\\prime} {\\boldsymbol \\alpha}_i\\) representa una combinación lineal de efectos aleatorios que puede escribirse como \\(\\mathbf{z}_{it}^{\\prime} {\\boldsymbol \\alpha}_i = z_{it,1} \\alpha_{i,1} + \\cdots +z_{it,q} \\alpha_{i,q}\\). A menudo, solo hay un único intercepto aleatorio, de modo que \\(q=1\\), \\(z_{it}=1\\) y \\(\\mathbf{z}_{it}^{\\prime} {\\boldsymbol \\alpha}_i = \\alpha_{i1} = \\alpha_i\\). Los efectos aleatorios tienen media cero, pero una media no nula puede incorporarse usando \\(\\mathbf{x}_{it}^{\\prime} \\boldsymbol \\beta\\). Por ejemplo, podríamos usar el tiempo \\(t\\) como una variable explicativa y definir \\(\\mathbf{x}_{it}=\\mathbf{z}_{it}=(1 ~t)^{\\prime}\\). Entonces, la ecuación (18.10) se reduce a \\(y_{it} = \\beta_0 + \\alpha_{i1} + ( \\beta_1 + \\alpha_{i2}) \\times t+ \\varepsilon_{it}\\), un modelo propuesto por Hachemeister en 1975. \\(\\varepsilon_{it}\\) es el término de perturbación con media cero. En muchas aplicaciones, se le asigna un peso en el sentido de que \\(\\mathrm{Var}~\\varepsilon_{it} = \\sigma^2 /w_{it}.\\) Aquí, el peso \\(w_{it}\\) es conocido y representa una exposición, como la cantidad de prima de seguro, el número de empleados, el tamaño de la nómina, el número de vehículos asegurados, entre otros. Introducir pesos fue propuesto por Bühlmann y Straub en 1970. Los términos de perturbación típicamente se asumen independientes entre grupos (sobre \\(i\\)), pero en algunas aplicaciones, pueden incorporar patrones temporales, como \\(AR\\)(1) (autoregresivo de orden 1). Ejemplo: Compensación de Trabajadores. Consideramos el seguro de compensación de trabajadores, examinando las pérdidas debido a reclamos por discapacidad parcial permanente. Los datos provienen de Klugman (1992), quien exploró representaciones del modelo bayesiano, y originalmente del National Council on Compensation Insurance. Consideramos \\(n=121\\) clases ocupacionales durante \\(T=7\\) años. Para proteger las fuentes de los datos, no se dispone de información adicional sobre las clases ocupacionales y los años. Resumimos el análisis en Frees, Young y Luo (2001). La variable de respuesta de interés es la prima pura (PP), definida como las pérdidas por discapacidad parcial permanente por dólar de PAYROLL. La variable PP es de interés para los actuarios porque las tasas de compensación de trabajadores se determinan y cotizan por unidad de PAYROLL. La medida de exposición, PAYROLL, es una de las posibles variables explicativas. Otras variables explicativas son YEAR (= 1, , 7) y la clase ocupacional. Entre otras representaciones, Frees et al. (2001) consideraron el modelo de Bühlmann-Straub, \\[\\begin{equation} \\ln (PP)_{it} = \\beta_0 + \\alpha_{i1}+ \\varepsilon_{it}, \\tag{18.11} \\end{equation}\\] el modelo de Hachemeister \\[\\begin{equation} \\ln (PP)_{it} = \\beta_0 + \\alpha_{i1} + (\\beta_1+\\alpha_{i2})YEAR_t + \\varepsilon_{it}, \\tag{18.12} \\end{equation}\\] y una versión intermedia \\[\\begin{equation} \\ln (PP)_{it} = \\beta_0 + \\alpha_{i1}+ \\alpha_{i2}YEAR_t + \\varepsilon_{it}. \\tag{18.13} \\end{equation}\\] En los tres casos, los pesos están dados por \\(w_{it}= PAYROLL_{it}\\). Estos modelos son todos casos especiales del modelo general en la ecuación (18.10) con \\(y_{it} = \\ln (PP)_{it}\\) y \\(M_i=0\\). La estimación de parámetros y la inferencia estadística relacionada, incluida la predicción, para el modelo de regresión lineal mixto en la ecuación (18.10) han sido ampliamente investigadas. La literatura se resume brevemente en la Sección 15.1. De la Sección 15.1.3, el mejor predictor lineal no sesgado de \\(\\mathrm{E}(y_{it} | \\boldsymbol \\alpha)\\) tiene la forma \\[\\begin{equation} M_i + \\mathbf{x}_{it}^{\\prime} \\mathbf{b}_{GLS}+ \\mathbf{z}_{it}^{\\prime} \\mathbf{a}_{BLUP,i}, \\tag{18.14} \\end{equation}\\] donde \\(\\mathbf{b}_{GLS}\\) es el estimador de mínimos cuadrados generalizados de \\(\\boldsymbol \\beta\\) y la expresión general para \\(\\mathbf{a}_{BLUP,i}\\) se da en la ecuación (15.11). Este es un estimador de credibilidad general que puede calcularse fácilmente utilizando paquetes estadísticos. Caso Especial: Modelo de Bühlmann-Straub. Para el modelo de Bühlmann-Straub, el factor de credibilidad es \\[ \\zeta_{i,w} = \\frac{WT_i}{WT_i + \\sigma^2/\\sigma^2_{\\alpha}}, \\] donde \\(WT_i\\) es la suma de pesos para el grupo \\(i\\)-ésimo, \\(WT_i = \\sum_{t=1}^{T_i} w_{it}\\). Usando la ecuación (18.14) en el modelo de Bühlmann-Straub, es fácil verificar que la predicción para el grupo \\(i\\)-ésimo es \\[ \\zeta_{i,w} \\bar{y}_{i,w} + (1-\\zeta_{i,w} ) \\bar{y}_w , \\] donde \\[ \\bar{y}_{i,w} =\\frac{\\sum_{t=1}^{T_i} w_{it}y_{it}}{WT_i} ~~~~\\mathrm{y}~~~~~~\\bar{y}_w =\\frac{\\sum_{t=1}^{T_i} \\zeta_{i,w} \\bar{y}_{i,w}}{\\sum_{t=1}^{T_i} \\zeta_{i,w} } \\] son la media ponderada del grupo \\(i\\)-ésimo y la media ponderada general, respectivamente. Esto se reduce al predictor balanceado de Bühlmann al tomar pesos idénticos a 1. Consulte, por ejemplo, Frees (2004, Sección 4.7) para más detalles. Ejemplo: Compensación de Trabajadores - Continuación. Estimamos los modelos en las ecuaciones (18.11)-(18.13), así como el modelo de Bühlmann no ponderado, utilizando máxima verosimilitud. Tabla 18.1 resume los resultados. Esta tabla sugiere que el factor de tendencia anual no es estadísticamente significativo, al menos para las medias condicionales. La tendencia anual que varía según la clase ocupacional parece ser útil. El criterio de información \\(AIC\\) sugiere que el modelo intermedio dado en la ecuación (18.13) proporciona el mejor ajuste a los datos. Tabla 18.1. Ajustes del Modelo de Compensación de Trabajadores \\[ \\small{ \\begin{array}{l|rrrr}\\hline &amp; &amp; \\text{Bühlmann-Straub} &amp; \\text{Hachemeister} \\\\ \\text{Parámetro} &amp; \\text{Bühlmann} &amp; Eq (18.11) &amp;Eq (18.12) &amp; Eq (18.13) \\\\ \\hline \\beta_0 &amp; -4.3665 &amp; -4.4003&amp; -4.3805&amp; -4.4036 \\\\ (t-\\text{estadístico}) &amp; (-50.38)&amp; (-51.47) &amp; (-44.38)&amp; (-51.90) \\\\ \\beta_1 &amp; &amp; &amp; -0.00446&amp; \\\\ (t-\\text{estadístico}) &amp; &amp; &amp; (-0.47) \\\\ \\hline \\sigma_{\\alpha,1} &amp;0.9106 &amp; 0.8865 &amp; 0.9634 &amp; 0.9594\\\\ \\sigma_{\\alpha,2} &amp; &amp; &amp; 0.0452 &amp; 0.0446\\\\ \\sigma &amp; 0.5871&amp; 42.4379&amp; 41.3386 &amp; 41.3582\\\\\\hline AIC &amp; 1,715.924 &amp; 1,571.391&amp; 1,567.769&amp;1,565.977 \\\\ \\hline \\end{array} } \\] Código R para Generar la Tabla 18.1 # Tabla 18.1 WorkersC &lt;- read.csv(&quot;CSVData/WorkersComp.csv&quot;, header=TRUE) #WorkersC &lt;- read.csv(&quot;../../CSVData/WorkersComp.csv&quot;, header=TRUE) WorkersC$PP &lt;- WorkersC$LOSS/WorkersC$PR # SOLO MODELO DE SEVERIDAD WorkersC3 &lt;- subset(WorkersC, PP&gt; 0 ) # MODELO DE EFECTOS FIJOS - NO NECESARIO PARA ESTA TABLA lm1 &lt;- lm(log(PP)~factor(CL), data = WorkersC3) lme1.sum &lt;- summary(lm1) Tab181 &lt;- matrix(NA, nrow = 8, ncol = 4) # MODELO DE EFECTOS ALEATORIOS library(nlme) # BUHLMANN lme2 &lt;- lme(log(PP)~1, random=~1|CL, method=&quot;ML&quot;, data = WorkersC3) lme2.sum &lt;- summary(lme2) Tab181[1,1] &lt;- as.numeric(lme2.sum$tTable[1,1]) Tab181[2,1] &lt;- as.numeric(lme2.sum$tTable[1,4]) Tab181[5,1] &lt;- as.numeric(VarCorr(lme2)[1,2] ) Tab181[7,1] &lt;- as.numeric(VarCorr(lme2)[2,2]) Tab181[8,1] &lt;- as.numeric(lme2.sum$AIC ) # BUHLMANN - STRAUB lme3 &lt;- lme(log(PP)~1, random=~1|CL, weights=varFixed(~1/sqrt(PR)), method=&quot;ML&quot;, data = WorkersC3) lme3.sum &lt;- summary(lme3) Tab181[1,2] &lt;- as.numeric(lme3.sum$tTable[1,1]) Tab181[2,2] &lt;- as.numeric(lme3.sum$tTable[1,4]) Tab181[5,2] &lt;- as.numeric(VarCorr(lme3)[1,2] ) Tab181[7,2] &lt;- as.numeric(VarCorr(lme3)[2,2]) Tab181[8,2] &lt;- as.numeric(lme3.sum$AIC) # HACHEMEISTER lme4 &lt;- lme(log(PP)~1+YR, random=~1+YR|CL, weights=varFixed(~1/sqrt(PR)), method=&quot;ML&quot;, data = WorkersC3) lme4.sum &lt;- summary(lme4) Tab181[1,3] &lt;- as.numeric(lme4.sum$tTable[1,1]) Tab181[2,3] &lt;- as.numeric(lme4.sum$tTable[1,4]) Tab181[3,3] &lt;- as.numeric(lme4.sum$tTable[2,1]) Tab181[4,3] &lt;- as.numeric(lme4.sum$tTable[2,4]) Tab181[5,3] &lt;- as.numeric(VarCorr(lme4)[1,2]) Tab181[6,3] &lt;- as.numeric(VarCorr(lme4)[2,2]) Tab181[7,3] &lt;- as.numeric(VarCorr(lme4)[3,2]) Tab181[8,3] &lt;- as.numeric(lme4.sum$AIC) # MODELO INTERMEDIO lme5 &lt;- lme(log(PP) ~1, random=~1+YR|CL, weights=varFixed(~1/sqrt(PR)), method=&quot;ML&quot;, data = WorkersC3) lme5.sum &lt;- summary(lme5) Tab181[1,4] &lt;- as.numeric(lme5.sum$tTable[1,1]) Tab181[2,4] &lt;- as.numeric(lme5.sum$tTable[1,4]) Tab181[5,4] &lt;- as.numeric(VarCorr(lme5)[1,2] ) Tab181[7,4] &lt;- as.numeric(VarCorr(lme5)[2,2]) Tab181[8,4] &lt;- as.numeric(lme5.sum$AIC) knitr::kable(Tab181, digits = 4) -4.3665 -4.4003 -4.3805 -4.4036 -50.3827 -51.4689 -44.3825 -51.9044 NA NA -0.0045 NA NA NA -0.4688 NA 0.9106 0.8865 0.9634 0.9594 NA NA 0.0452 NA 0.5871 42.4379 41.3387 0.0446 1715.9244 1571.3906 1567.7690 1565.9766 Tabla 18.2 ilustra las predicciones de credibilidad resultantes para las primeras cinco clases ocupacionales. Aquí, para cada método, después de realizar la predicción, se exponenció el resultado y se multiplicó por 100, de modo que estos son los centavos de pérdidas predichas por dólar de PAYROLL. También se incluyen las predicciones para el modelo de “efectos fijos”, que equivalen a tomar el promedio por clase ocupacional durante el período de siete años. Las predicciones para los modelos de Hachemeister y la ecuación (18.13) se realizaron para el Año 8. Tabla 18.2 muestra un acuerdo sustancial entre las predicciones de Bühlmann y Bühlmann-Straub, lo que indica que la ponderación por PAYROLL es menos importante para este conjunto de datos. También hay un acuerdo sustancial entre las predicciones del modelo de Hachemeister y la ecuación (18.13), lo que indica que la tendencia temporal general es menos importante. Comparar los dos conjuntos de predicciones indica que una tendencia temporal que varía según la clase ocupacional sí marca una diferencia. Tabla 18.2. Predicciones de Compensación de Trabajadores \\[ \\small{ \\begin{array}{c|rrrr}\\hline \\text{Clase} &amp; \\text{Efectos} &amp; &amp;&amp; &amp; \\\\ \\text{Ocupacional } &amp; \\text{Fijos} &amp; \\text{Bühlmann} &amp; \\text{Bühlmann-Straub} &amp; \\text{Hachemeister} &amp; Eq (18.13) \\\\ \\hline \\hline 1 &amp; 2.981 &amp; 2.842 &amp; 2.834 &amp; 2.736 &amp; 2.785 \\\\ 2 &amp; 1.941 &amp; 1.895 &amp; 1.875 &amp; 1.773 &amp; 1.803 \\\\ 3 &amp; 1.129 &amp; 1.137 &amp; 1.135 &amp; 1.124 &amp; 1.139 \\\\ 4 &amp; 0.795 &amp; 0.816 &amp; 0.765 &amp; 0.682 &amp; 0.692 \\\\ 5 &amp; 1.129 &amp; 1.137 &amp; 1.129 &amp; 1.062 &amp; 1.079 \\\\ \\hline \\end{array} } \\] 18.4 Bonus-Malus Los métodos bonus-malus de experiencia de clasificación se utilizan extensamente en la fijación de precios de seguros de automóviles en Europa y Asia. Para entender este tipo de clasificación por experiencia, primero consideremos precios basados en características observables. En el seguro de automóviles, estas incluyen características del conductor (como edad y género), características del vehículo (como tipo de coche y si se usa para trabajar o no) y características territoriales (como el condado de residencia). El uso exclusivo de estas características para los precios da como resultado una prima a priori. En los EE.UU. y Canadá, esta es la base principal de la prima; la clasificación por experiencia entra de forma limitada en forma de recargos por accidentes con culpa y violaciones de tránsito en movimiento. Los sistemas bonus-malus (BMS) proporcionan una integración más detallada de la experiencia de reclamos en los precios. Típicamente, un BMS clasifica a los asegurados en una de varias categorías ordenadas. Un asegurado entra al sistema en una categoría específica. En el año siguiente, los asegurados sin accidentes reciben un “bono” y suben de categoría. Los asegurados que tienen accidentes con culpa durante el año reciben un “malus” y bajan un número especificado de categorías. La categoría en la que uno reside dicta el factor bonus-malus, o BMF. El BMF multiplicado por la prima a priori se conoce como la prima a posteriori. Para ilustrar, Lemaire (1998) da un ejemplo de un sistema brasileño que se resume en la Tabla 18.3. En este sistema, uno comienza en la Clase 7, pagando el 100% de las primas dictadas por la prima a priori del asegurador. En el año siguiente, si el asegurado no tiene accidentes, paga solo el 90% de la prima a priori. De lo contrario, la prima es el 100% de la prima a priori. Tabla 18.3. Sistema Bonus-Malus Brasileño \\[ \\small{ \\begin{array}{c|rrrrrrrr}\\hline \\hline &amp; &amp; &amp;&amp;\\text{Clase}&amp; \\text{Después}\\\\ \\text{Clase } &amp; BMF &amp; \\text{Reclamos} &amp; \\text{Reclamos} &amp; \\text{Reclamos} &amp; \\text{Reclamos} &amp; \\text{Reclamos} &amp; \\text{Reclamos} &amp; \\text{Reclamos} \\\\ \\text{Antes} &amp; &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; \\geq 6 \\\\ \\hline 7 &amp; 100 &amp; 6 &amp; 7 &amp; 7 &amp; 7 &amp; 7 &amp; 7 &amp; 7 \\\\ 6 &amp; 90 &amp; 5 &amp; 7 &amp; 7 &amp; 7 &amp; 7 &amp; 7 &amp; 7 \\\\ 5 &amp; 85 &amp; 4 &amp; 6 &amp; 7 &amp; 7 &amp; 7 &amp; 7 &amp; 7 \\\\ 4 &amp; 80 &amp; 3 &amp; 5 &amp; 6 &amp; 7 &amp; 7 &amp; 7 &amp; 7 \\\\ 3 &amp; 75 &amp; 2 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 7 &amp; 7 \\\\ 2 &amp; 70 &amp; 1 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 7 \\\\ 1 &amp; 65 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 \\\\ \\hline \\end{array} } \\] Fuente: Lemaire (1998) Como se describe en Lemaire (1998), el sistema brasileño es simple en comparación con otros (el sistema belga tiene 23 clases). Las aseguradoras que operan en países con sistemas bonus-malus detallados no requieren extensas variables de calificación a priori en comparación con los EE.UU. y Canadá. Esto generalmente implica menores gastos de suscripción y, por lo tanto, un sistema de seguros menos costoso. Además, muchos argumentan que es más justo para los asegurados en el sentido de que aquellos con mala experiencia de reclamos soportan la carga de primas más altas y no se penaliza simplemente por género u otras variables de calificación que están fuera del control del asegurado. Consulte Lemaire (1995) para una amplia discusión sobre cuestiones institucionales, regulatorias y éticas relacionadas con los sistemas bonus-malus. A lo largo de este libro, hemos visto cómo utilizar técnicas de regresión para calcular primas a priori. Dionne y Vanasse (1992) señalaron las ventajas de usar un marco de regresión para calcular factores bonus-malus. Esencialmente, usaron una variable latente para representar las tendencias no observadas de un asegurado para involucrarse en un accidente (agresividad, rapidez de reflejos) con modelos de conteo de regresión para calcular primas a posteriori. Consulte Denuit et al. (2007) para una visión general reciente de esta área en desarrollo. Consulte Norberg (1986) para un relato temprano que relaciona la teoría de credibilidad con el marco de modelos lineales mixtos. El tratamiento aquí sigue a Frees, Young y Luo (1999). Al demostrar que muchos modelos importantes de credibilidad pueden verse en un marco de datos longitudinales (lineal), limitamos nuestra consideración a ciertos tipos de modelos de credibilidad. Específicamente, los modelos de datos longitudinales acomodan solo riesgos no observados que son aditivos. Este capítulo no aborda modelos de efectos aleatorios no lineales que han sido investigados en la literatura actuarial; consulte, por ejemplo, Taylor (1977) y Norberg (1980). Taylor (1977) permitió que los reclamos de seguros fueran posiblemente de dimensión infinita usando la teoría de espacios de Hilbert y estableció fórmulas de credibilidad en este contexto general. Norberg (1980) consideró el contexto más concreto, pero aún general, de reclamos multivariantes y estableció la relación entre la credibilidad y la estimación estadística Bayes empírica. Como se describe en la Sección 18.4, Denuit et al. (2007) proporciona una visión general reciente de modelos de conteo de reclamos longitudinales no lineales. Para considerar toda la distribución de reclamos, un enfoque común utilizado en credibilidad es adoptar una perspectiva bayesiana. Keffer (1929) inicialmente sugirió usar una perspectiva bayesiana para la calificación de experiencia en el contexto del seguro de vida grupal. Posteriormente, Bailey (1945, 1950) mostró cómo derivar la forma de credibilidad lineal desde una perspectiva bayesiana como la media de una distribución predictiva. Varios autores han proporcionado extensiones útiles de este paradigma. Jewell (1980) extendió los resultados de Bailey a una clase más amplia de distribuciones, la familia exponencial, con distribuciones previas conjugadas para las variables estructurales. Referencias del Capítulo Bailey, Arthur (1945). A generalized theory of credibility. Proceedings of the Casualty Actuarial Society 32. Bailey, Arthur (1950). Credibility procedures: LaPlace’s generalization of Bayes’ rule and the combination of collateral knowledge with observed data. Proceedings of the Casualty Actuarial Society Society 37, 7-23. Bühlmann, Hans (1967). Experience rating and credibility. ASTIN Bulletin 4, 199-207. Bühlmann, Hans and E. Straub (1970). Glaubwürdigkeit für schadensätze. Mitteilungen der Vereinigung Schweizerischer Versicherungs-Mathematiker 70, 111-133. Dionne, George and C. Vanasse (1992). Automobile insurance ratemaking in the presence of asymmetrical information. Journal of Applied Econometrics 7, 149-165. Denuit, Michel, Xavier Marechal, Sandra Pitrebois and Jean-Francois Walhin (2007). Actuarial Modelling of Claim Counts: Risk Classification, Credibility and Bonus-Malus Systems. Wiley, New York. Frees, Edward W., Virginia R. Young, and Yu Luo (1999). A longitudinal data analysis interpretation of credibility models. Insurance: Mathematics and Economics 24, 229-247. Frees, Edward W., Virginia R. Young, and Yu Luo (2001). Case studies using panel data models. North American Actuarial Journal 5(4), 24-42. Frees, Edward W. (2004). Longitudinal and Panel Data: Analysis and Applications in the Social Sciences. Cambridge University Press, New York. Hachemeister, Charles A. (1975). Credibility for regression models with applications to trend. In Credibility: Theory and Applications, editor Paul M. Kahn. Academic Press, New York, 129-163. Keffer, R (1929). An experience rating formula. Transactions of the Actuarial Society of America 30, 130-139. Klugman, Stuart A. (1992). Bayesian Statistics in Actuarial Science. Kluwer, Boston. Klugman, Stuart A, Harry H. Panjer and Gordon E. Willmot (2008). Loss Models: From Data to Decisions. John Wiley &amp; Sons, Hoboken, New Jersey. Lemaire, Jean (1995). Bonus-Malus Systems in Automobile Insurance. Kluwer, Boston. Lemaire, Jean (1998). Bonus-malus systems: The European and Asian approach to merit-rating. North American Actuarial Journal 2(1), 26-47. Mowbray, Albert H. (1914). How extensive a payroll exposure is necessary to give a dependable pure premium. Proceedings of the Casualty Actuarial Society 1, 24-30. Norberg, Ragnar (1980). Empirical Bayes credibility. Scandinavian Actuarial Journal 177-194. Norberg, Ragnar (1986). Hierarchical credibility: Analysis of a random effect linear model with nested classification. Scandinavian Actuarial Journal 204-22. Taylor, Greg C. (1977). Abstract credibility. Scandinavian Actuarial Journal 149-68. Whitney, Albert W. (1918). The theory of experience rating. Proceedings of the Casualty Actuarial Society 4. "],["C19Triangles.html", "Capítulo 19 Triángulos de Reclamos 19.1 Introducción 19.2 Regresión Usando Funciones del Tiempo como Variables Explicativas 19.3 Usando Desarrollos Pasados 19.4 Lecturas Adicionales y Referencias 19.5 Ejercicios", " Capítulo 19 Triángulos de Reclamos Vista Previa del Capítulo. Este capítulo introduce un problema clásico de reservación actuarial que se encuentra ampliamente en los seguros de propiedad y accidentes, así como en los seguros de salud. Los datos se presentan en un formato triangular para enfatizar su naturaleza longitudinal y censurada. Este capítulo explica cómo surgen naturalmente estos datos e introduce métodos de regresión para abordar el problema de la reservación actuarial. 19.1 Introducción En muchos tipos de seguros, poco tiempo transcurre entre el evento de un reclamo, la notificación a una compañía de seguros y el pago a los beneficiarios. Por ejemplo, en los seguros de vida, la notificación y el pago de beneficios típicamente ocurren dentro de las dos semanas posteriores al fallecimiento del asegurado. Sin embargo, en otras líneas de seguro, el tiempo desde la ocurrencia del reclamo hasta el pago final puede ser mucho más largo, tomando meses e incluso años. Para introducir esta situación, esta sección describe la evolución de un reclamo, introduce medidas de resumen utilizadas por las aseguradoras y luego describe el método determinista predominante para pronosticar reclamos, el método del eslabón en cadena. 19.1.1 Evolución de los Reclamos Por ejemplo, suponga que usted sufre una lesión en un accidente automovilístico cubierto por un seguro. Puede tomar meses para que la lesión se cure y todos los pagos por atención médica sean conocidos y realizados por la compañía de seguros. Además, pueden surgir disputas entre usted, otras partes involucradas en el accidente, su aseguradora y las aseguradoras de otras partes, alargando así el tiempo hasta que los reclamos se resuelvan y paguen. Cuando los reclamos tardan mucho en desarrollarse, las obligaciones del reclamo de una aseguradora pueden ser incurridas en un período contable pero no pagadas hasta un período contable posterior. En el ejemplo de su accidente, la aseguradora sabe que ha ocurrido un reclamo e incluso puede haber realizado algunos pagos en el período contable actual. Las cantidades de pagos futuros son desconocidas al final del período contable actual, pero la aseguradora desea realizar un pronóstico preciso de las obligaciones futuras para apartar una cantidad adecuada de dinero para estas obligaciones futuras, conocido como una reserva. El objetivo de la aseguradora es utilizar la información actual del reclamo para predecir el momento y la cantidad de los pagos futuros del reclamo. Para establecer la terminología, es útil seguir la línea de tiempo de un reclamo a medida que se desarrolla. En la Figura 19.1, el reclamo ocurre en el tiempo \\(t_1\\) y la compañía aseguradora es notificada en el tiempo \\(t_3\\). Puede haber un largo intervalo entre la ocurrencia y la notificación, tal que una fecha de valoración (\\(t_2\\)) puede ocurrir dentro de este intervalo. Aquí, \\(t_2\\) es el momento en que se valoran las obligaciones, que típicamente, pero no siempre, coincide con el final de un período de reporte financiero de la compañía. En este caso, se dice que el reclamo está incurrido pero no reportado en esta fecha de valoración. Después de la notificación del reclamo, puede haber uno o más pagos por pérdidas. No todos los pagos pueden realizarse antes de la siguiente fecha de valoración (\\(t_4\\)). A medida que el reclamo se desarrolla, eventualmente la compañía considera que sus obligaciones financieras sobre el reclamo están resueltas y declara el reclamo como “cerrado.” Sin embargo, es posible que surjan nuevos hechos y el reclamo deba reabrirse, dando lugar a pagos adicionales por pérdidas antes de cerrarse nuevamente. Figura 19.1: Cronología del Desarrollo de un Reclamo. 19.1.2 Triángulos de Reclamos Las aseguradoras no suelen modelar la evolución de cada reclamo y luego sumarlos (como se hace en un análisis basado en pólizas en los seguros de vida). En cambio, los portafolios de reclamos se resumen en cada fecha de valoración; son estos resúmenes los que se utilizan para realizar pronósticos de los reclamos pendientes. Específicamente, sea \\(i\\) el año en que se ha incurrido un reclamo y \\(j\\) el número de años desde el incurrimiento hasta el momento en que se realiza el pago, conocido como año de “desarrollo” (o “retraso”). Por lo tanto, \\(y_{ij}\\) representa la suma de los montos de pago en el \\(i\\)-ésimo año de incurrimiento y el \\(j\\)-ésimo año de desarrollo. Aquí, “año” se refiere al período contable; en ejemplos posteriores, verá que a menudo usamos mes, trimestre u otros períodos fijos. Tabla 19.1 muestra la información que típicamente enfrentan los actuarios. Table 19.1. Triángulo Clásico de Desarrollo de Reclamos \\[ \\small{ \\begin{array}{c|ccccc} \\hline \\text{Año de } &amp; \\text{Año de Desarrollo }&amp;(j) \\\\ \\text{Incurrencia }(i) &amp; 1&amp; 2&amp; 3&amp;4&amp;5 \\\\ \\hline 1 &amp; y_{11} &amp; y_{12} &amp; y_{13} &amp; y_{14} &amp; y_{15} \\\\ 2 &amp; y_{21} &amp; y_{22} &amp; y_{23} &amp; y_{24} &amp; . \\\\ 3 &amp; y_{31} &amp; y_{32} &amp; y_{33} &amp; . &amp; . \\\\ 4 &amp; y_{41} &amp; y_{42} &amp; . &amp;. &amp; . \\\\ 5 &amp; y_{51} &amp; . &amp; . &amp;. &amp; . \\\\ \\hline \\end{array} } \\] El término “triángulo de reclamos” es evidente a partir de Table 19.1. Observamos datos en el triángulo superior izquierdo, \\(y_{ij}, i=1,\\ldots, 5, j=1, \\ldots,6-i\\). El objetivo es “completar el triángulo,” es decir, pronosticar valores en el triángulo inferior derecho (\\(y_{ij}, i=2,\\ldots, 5, j=7-i, \\ldots,5\\)). Por ejemplo, el año más reciente de incurrencia es \\(i=5\\), para el cual solo tenemos un año de experiencia de reclamos, \\(y_{51}\\). Los otros valores, \\(y_{5j}, j=2, \\ldots,5\\), son desconocidos en la fecha de valoración. En algunas situaciones, también es de interés pronosticar reclamos en años de desarrollo seis y posteriores. Table 19.1 resalta el hecho de que los datos son tanto longitudinales como censurados. Este es el punto clave con respecto a los supuestos de modelado estadístico. En las aplicaciones, las observaciones pueden variar significativamente dependiendo del tipo y propósito. Cada elemento del triángulo puede representar pagos reales realizados por la compañía de seguros, conocidos como “pagos incrementales,” o la suma “acumulativa” de pagos desde el desarrollo. Para algunas líneas de negocio, están disponibles estimaciones de los pagos pendientes en una base reclamo por reclamo, conocidas como “estimaciones de caso.” Aquí, los elementos del triángulo pueden representar reclamos incurridos en lugar de reclamos pagados. Es decir, un reclamo incurrido es el monto pagado más la reserva. Debido a que las estimaciones de caso se revisan a medida que se recibe nueva información sobre un reclamo, no es raro que los pagos incurridos sean negativos. Además, incluso los pagos incrementales realizados pueden ser negativos debido a la reaseguradora o la recuperación de montos de reclamos por parte de otras partes responsables. (Muchas aseguradoras emiten cheques para pagar a los reclamantes rápidamente tras la ocurrencia de un accidente y luego son reembolsadas por otra parte responsable del accidente). Los datos de triángulos de reclamos también pueden estar en forma de número de notificaciones de reclamos. Este tipo de datos es particularmente útil para estimar reservas “incurridas pero no reportadas.” Ejemplo: Daños a Propiedades en Singapur. Table 19.2 reporta pagos incrementales de un portafolio de pólizas de automóviles de una aseguradora general (de propiedad y accidentes) en Singapur. Aquí, los pagos son por daños a terceros en propiedad de pólizas de seguros de autos de cobertura total. Todos los pagos han sido ajustados usando un índice de precios al consumidor de Singapur, por lo que están en dólares constantes. Los datos corresponden a pólizas con coberturas desde 1997 hasta 2001, inclusive. Table 19.2 también proporciona las primas de estas pólizas (en miles de dólares singapurenses) para proporcionar una idea de la creciente exposición de la aseguradora a posibles obligaciones de reclamos. Table 19.2. Pagos Incrementales en Singapur \\[ \\small{ \\begin{array}{lr|rrrrr} \\hline \\text{Año de } &amp; \\text{Prima} &amp;\\text{ Año de Desarrollo} \\\\ \\text{Incurrencia} &amp; \\text{(en miles)} &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\ \\hline 1997 &amp; 32,691 &amp; 1,188,675 &amp; 2,257,909 &amp; 695,237 &amp; 166,812 &amp; 92,129 \\\\ 1998 &amp; 33,425 &amp; 1,235,402 &amp; 3,250,013 &amp; 649,928 &amp; 211,344 &amp; . \\\\ 1999 &amp; 34,849 &amp; 2,209,850 &amp; 3,718,695 &amp; 818,367 &amp; . &amp; . \\\\ 2000 &amp; 37,011 &amp; 2,662,546 &amp; 3,487,034 &amp; . &amp; . &amp; . \\\\ 2001 &amp; 40,152 &amp; 2,457,265 &amp; . &amp; . &amp; . &amp; . \\\\ \\hline \\end{array} } \\] Código R para Producir Tabla 19.2 # Table 19.2 SingProperty &lt;- read.csv(&quot;CSVData/SingaporeProperty.csv&quot;, header=TRUE) #SingProperty &lt;- read.csv(&quot;../../CSVData/SingaporeProperty.csv&quot;, header=TRUE) SingPropertyA &lt;- SingProperty[c(&quot;Year&quot;, &quot;Delay&quot;, &quot;Payments&quot;)] library(ChainLadder) SingProperty_triangle &lt;- as.triangle(SingPropertyA, origin = &quot;Year&quot;, dev = &quot;Delay&quot;, value = &quot;Payments&quot;) SingProperty_triangle Delay Year 1 2 3 4 5 1997 1188675.0 2257908.9 695237.49 166811.73 92128.79 1998 1235401.8 3250012.7 649928.28 211344.22 NA 1999 2209849.6 3718694.9 818367.44 NA NA 2000 2662546.0 3487034.0 NA NA NA 2001 2457265.3 NA NA NA NA 19.1.3 Método de Escalera de Cadenas Para introducir el método básico de escalera de cadenas, continuamos trabajando en el contexto del ejemplo de daños a propiedades en Singapur. Table 19.3 muestra los mismos pagos que Table 19.2 pero en forma acumulativa en lugar de incremental. Sea \\(S_{ij} = y_{i1} + \\cdots + y_{ij}\\) denotar los reclamos acumulativos. Table 19.3. Pagos Acumulativos en Singapur con Estimaciones de Escalera de Cadenas \\[ \\scriptsize{ \\begin{array}{llrrrrr|r|c} \\hline \\hline &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;\\text{Pérdida}\\\\ &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp; \\text{Final} \\\\ \\text{Año de} &amp;&amp;\\text{Año de}&amp; \\text{Desarrollo} &amp; &amp;&amp;&amp;&amp;\\text{Relación} \\\\ \\text{Incurrencia} &amp; \\text{Prima} &amp;~~~~~~~~1 &amp; 2~~~ &amp; 3~~~&amp; 4~~~&amp; 5~~~&amp; \\text{Reserva} &amp; (\\%) \\\\ \\hline 1997 &amp; 32,691 &amp; 1,188,675 &amp; 3,446,584 &amp; 4,141,821 &amp; 4,308,633 &amp; 4,400,762 &amp; &amp; 13.5 \\\\ 1998 &amp; 33,425 &amp; 1,235,402 &amp; 4,485,415 &amp; 5,135,343 &amp; 5,346,687 &amp; {\\bf 5,461,012} &amp; 114,325 &amp; 16.3 \\\\ 1999 &amp; 34,849 &amp; 2,209,850 &amp; 5,928,544 &amp; 6,746,912 &amp; {\\bf 7,021,930} &amp; {\\bf 7,172,075} &amp; 425,163 &amp; 20.6 \\\\ 2000 &amp; 37,011 &amp; 2,662,546 &amp; 6,149,580 &amp; {\\bf 7,109,486} &amp; {\\bf 7,399,283} &amp; {\\bf 7,557,497} &amp; 1,407,917 &amp; 20.4 \\\\ 2001 &amp; 40,152 &amp; 2,457,265 &amp;{\\bf 6,738,898} &amp; {\\bf 7,790,792} &amp; {\\bf 8,108,361} &amp; {\\bf 8,281,737} &amp; 5,824,471 &amp; 20.6 \\\\ \\hline \\text{Total} &amp;\\text{Reserva} &amp; &amp; &amp; &amp; &amp;&amp;7,771,877 \\\\ Escalera &amp; de Cadenas&amp; Factores &amp; 2.742 &amp; 1.156 &amp; 1.041 &amp; 1.021 &amp; \\\\ \\hline \\hline \\end{array} } \\] La relación de escalera de cadenas para el \\(j\\)-ésimo año de desarrollo se calcula tomando el cociente de la suma de los reclamos sobre todos los años de incurrencia para el \\(j\\)-ésimo año de desarrollo, dividido por la suma de los mismos pagos del año de incurrencia para el \\(j-1^{er}\\) año de desarrollo. Utilizando notación, tenemos: \\[ CL_j = \\frac {\\sum_{i=1}^{6-j} S_{ij}}{\\sum_{i=1}^{6-j} S_{i,j-1}}. \\] Por ejemplo, \\(CL_5 = 4,400,762/4,308,633 = 1.021\\) y \\(CL_4 = (5,346,687+4,308,633)/(5,135,343+4,141,821)= 1.041\\). Los números en negrita son pronósticos calculados recursivamente utilizando las relaciones de escalera de cadenas y \\(\\widehat{S}_{i,j} = CL_{j} \\times \\widehat{S}_{i,j-1}.\\) La recursión comienza cuando se conoce el valor del pago acumulativo, de modo que \\(\\widehat{S}_{ij}=S_{ij}\\). Por ejemplo, para el año de incurrencia 2, \\(\\widehat{S}_{25} = CL_5 \\times S_{24} = (1.021)(5,346,687)=5,461,012\\). Para el año de incurrencia 3, \\(\\widehat{S}_{34} = CL_4 \\times S_{33} = (1.041)(6,746,912)=7,021,930\\) y \\(\\widehat{S}_{35} = CL_5 \\times \\widehat{S}_{34} = (1.021)(7,021,930)=7,172,075\\). Alternativamente, se puede ir directamente al último año de desarrollo y utilizar \\(\\widehat{S}_{35} = CL_5 \\times CL_4 \\times S_{33} = (1.041)(1.021)(6,746,912)=7,172,075.\\) En Tabla 19.3, la reserva es la cantidad final pronosticada menos el reclamo acumulativo pagado más reciente. La relación de pérdida final es la proporción del pronóstico de los reclamos acumulativos en el último año de desarrollo (5) respecto a las primas pagadas (expresadas como un porcentaje, recordando que las primas están en miles). 19.2 Regresión Usando Funciones del Tiempo como Variables Explicativas El método de escalera de cadenas es una herramienta importante que los actuarios utilizan ampliamente al pronosticar reclamos. Generalmente se presenta de manera determinista, como en la Sección 19.1.3. Los modelos estocásticos alternativos para el pronóstico de reclamos tienen dos ventajas principales: Al modelar explícitamente la distribución de los reclamos, se pueden realizar estimaciones de la incertidumbre de los pronósticos de reserva. Existen muchas variaciones de las técnicas de escalera de cadenas porque se aplican en diferentes situaciones. Como vimos en el Capítulo 5, los métodos estocásticos ofrecen una forma disciplinada de selección de modelos que puede ayudar a determinar el modelo más adecuado para un conjunto de datos dado. Como veremos, no es necesario elegir entre usar el método de escalera de cadenas y un modelo estocástico. La Sección 19.2.3 modelo Poisson sobredisperso y la Sección 19.3.1 modelo Mack generan pronósticos puntuales que son iguales a los de escalera de cadenas. 19.2.1 Modelo Lognormal Nuestro punto de partida es el modelo lognormal para los reclamos incrementales. Es decir, consideramos un modelo de dos factores de la forma: \\[\\begin{equation} \\ln y_{ij} = \\mu + \\alpha_i + \\tau_j + \\varepsilon_{ij}, \\tag{19.1} \\end{equation}\\] donde \\(\\{ \\alpha_i \\}\\) son parámetros para el factor del año de incurrencia y \\(\\{ \\tau_j \\}\\) son parámetros para el factor del año de desarrollo. Un modelo de regresión con dos factores se introdujo en la Sección 4.4. Recordemos que requerimos restricciones en los parámetros de los factores para su estimabilidad, como \\(\\sum_i \\alpha_i = 0\\) y \\(\\sum_j \\tau_j= 0\\). Asumiendo normalidad de \\(\\{\\varepsilon_{ij} \\}\\) se deriva la especificación lognormal para los reclamos incrementales \\(y_{ij}\\). Ejemplo: Lesiones a Terceros en Singapur. Tabla 19.4 reporta pagos de un portafolio de pólizas de automóviles para una aseguradora general (de bienes y accidentes) en Singapur. Los pagos, ajustados por inflación, corresponden a lesiones a terceros de pólizas de seguro integral. Los datos son para pólizas con coberturas desde 1993 hasta 2001, inclusive. En el seguro de automóviles, generalmente se tarda más en resolver y pagar las lesiones en comparación con los reclamos por daños a la propiedad. Por lo tanto, el número de años de desarrollo, “el desgaste”, es más largo en Tabla 19.4 que en Tabla 19.2. Tabla 19.4 también muestra una falta de estabilidad en los pagos por lesiones que la Figura 19.2 nos ayuda a visualizar. El panel izquierdo muestra líneas de tendencia por desarrollo para cada año de incurrencia. El panel derecho presenta diagramas de caja de pagos logarítmicos para cada año de desarrollo. Este gráfico muestra que los pagos tienden a aumentar en los dos primeros periodos de desarrollo (\\(j=1,2\\)), alcanzan un pico en el tercer periodo (\\(j=3\\)) y luego disminuyen. Tabla 19.4. Pagos Incrementales por Lesiones en Singapur, 1993-2001 \\[ \\small{ \\begin{array}{c|rrrrrrrrr} \\hline \\text{Año de }&amp;\\text{Año de} &amp; \\text{Desarrollo} \\\\ \\text{Incurrencia} &amp; 1 &amp; 2 &amp;3 &amp;4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 \\\\ \\hline 1993 &amp; 14,695 &amp; 205,515 &amp; 118,686 &amp; 416,286 &amp; 93,544 &amp; 185,043 &amp; 37,750 &amp; 0 &amp;14,086 \\\\ 1994 &amp; 153,615 &amp; 467,722 &amp; 645,513 &amp; 421,896 &amp; 146,576 &amp; 96,470 &amp; 27,765 &amp;38,017 &amp; . \\\\ 1995 &amp; 24,741 &amp; 547,862 &amp; 754,475 &amp; 417,573 &amp; 156,596 &amp; 55,155 &amp; 36,984 &amp; . &amp; . \\\\ 1996 &amp; 68,630 &amp; 188,627 &amp; 492,306 &amp; 179,249 &amp; 34,062 &amp; 443,436 &amp; . &amp; . &amp; . \\\\ 1997 &amp; 29,177 &amp; 364,672 &amp; 437,507 &amp; 385,571 &amp; 529,319 &amp; . &amp; . &amp; . &amp; . \\\\ 1998 &amp; 40,430 &amp; 241,809 &amp; 678,541 &amp; 528,026 &amp; . &amp; . &amp; . &amp; . &amp; . \\\\ 1999 &amp; 45,125 &amp; 372,935 &amp; 704,168 &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\\\ 2000 &amp; 21,695 &amp; 158,005 &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\\\ 2001 &amp; 6,626 &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\\\ \\hline \\hline \\end{array} } \\] Figura 19.2: Pagos Incrementales por Lesiones en Singapur. El panel izquierdo muestra pagos por año de desarrollo con cada línea conectando pagos del mismo año de incurrencia. El panel derecho muestra la distribución de los pagos logarítmicos para cada año de desarrollo. Código R para Producir Figura 19.2 library(lattice) library(cowplot) library(gridGraphics) library(gridExtra) # Figura 19.2 SingInjury &lt;- read.csv(&quot;CSVData/SingaporeInjury.csv&quot;, quote=&quot;&quot;, header=TRUE) SingInjuryA &lt;- subset(SingInjury, Payment != 0 ) SingInjuryA$lnPayment &lt;- log(SingInjuryA$Payment) #trellis.device(color=F) # telling the trellis device to mimic &#39;black and white&#39; tempPlotB &lt;- xyplot(SingInjuryA$Payment~SingInjuryA$Delay, groups=SingInjuryA$Year, type=&quot;b&quot;, pch=16, lty=1, ylab=&quot;&quot;, xlab=&quot;Periodo de Desarrollo&quot;, scales= list(y=list(at=seq(0, 600000,200000), labels=c(&quot;0&quot;,&quot;200,000&quot;,&quot;400,000&quot;,&quot;600,000&quot;))) ) # Capture the current plot as a grob boxplot_grob &lt;- as_grob(~ { boxplot(SingInjuryA$Payment ~ SingInjuryA$Delay, xlab = &quot;Periodo de Desarrollo&quot;, yaxt = &quot;n&quot;, ylab = &quot;Pago&quot;) axis(2, at = seq(0, 800000, 200000), labels = c(&quot;0&quot;, &quot;200,000&quot;, &quot;400,000&quot;, &quot;600,000&quot;, &quot;800,000&quot;)) }) grid.arrange( grobs = list(boxplot_grob, tempPlotB), ncol = 2 ) El modelo lognormal basado en la ecuación (19.1) se ajustó a estos datos. Como era de esperar, tanto los factores de incurrencia como los años de desarrollo fueron estadísticamente significativos. El coeficiente de determinación del ajuste es \\(R^2 = 73.3 \\%\\). Un gráfico \\(qq\\) (no presentado aquí) mostró una concordancia razonable con la suposición de normalidad. Los valores ajustados del modelo, después de la exponenciación para convertir de nuevo a dólares, aparecen en la Figura 19.3. Esta figura parece capturar los patrones de pago que aparecen en el panel izquierdo de la Figura 19.2. Cabe destacar que los valores ajustados para la porción no observada del triángulo son pronósticos. Figura 19.3: Valores Ajustados para los Pagos Incrementales por Lesiones en Singapur. Estas estimaciones están basadas en el modelo lognormal de dos factores. Código R para Producir la Figura 19.3 # Figura 19.3 # VALORES AJUSTADOS PARA EL MODELO LOGNORMAL model1 &lt;- lm(lnPayment ~ factor(Year) + factor(Delay), data = SingInjuryA ) beta &lt;- coefficients(model1) forecast &lt;- rep(0, 81) forei &lt;- forecast forej &lt;- forecast for (i in 1:9) { for (j in 1:9) { xij &lt;- c(1, rep(0, 16)) if (i &gt; 1) { xij[i] &lt;- 1 } if (j &gt; 1) { xij[8 + j] &lt;- 1 } foreij &lt;- xij %*% beta forecast[9 * (i - 1) + j] &lt;- exp(foreij) forei[9 * (i - 1) + j] &lt;- i forej[9 * (i - 1) + j] &lt;- j } } # forecast; forei; forej # trellis.device(color = F) xyplot(forecast ~ forej, groups = forei, type = &quot;b&quot;, pch = 16, lty = 1, ylab = &quot;&quot;, xlab = &quot;Periodo de Desarrollo&quot;, scales = list(y = list(at = seq(0, 600000, 200000), labels = c(&quot;0&quot;, &quot;200,000&quot;, &quot;400,000&quot;, &quot;600,000&quot;))) ) 19.2.2 Curva de Hoerl La componente sistemática de la ecuación (19.1) puede ser modificada fácilmente. Una posibilidad es la denominada “curva de Hoerl,” lo que lleva a la ecuación del modelo \\[\\begin{equation} \\ln y_{ij} = \\mu + \\alpha_i + \\beta_i \\ln (j) + \\gamma_i \\times j + \\varepsilon_{ij} . \\tag{19.2} \\end{equation}\\] Una ventaja de tratar el tiempo de desarrollo \\(j\\) como una covariable continua es que la extrapolación es posible más allá del rango de los tiempos de desarrollo observados. Como una variación, England y Verrall (2002) sugieren permitir que los primeros años de desarrollo tengan sus propios niveles e imponer el mismo patrón de agotamiento para todos los años de incurrencia (\\(\\beta_i = \\beta\\), \\(\\gamma_i = \\gamma\\)). Ejemplo: Lesiones de Terceros en Singapur - Continuación. El modelo básico de la ecuación (19.2) se ajustó bien, con un coeficiente de determinación de \\(R^2 = 87.8 \\%\\). También examinamos un modelo más simple basado en la ecuación \\[\\begin{equation} \\ln y_{ij} = \\mu + \\alpha_i + \\beta \\ln (j) + \\gamma \\times j + \\varepsilon_{ij} . \\tag{19.3} \\end{equation}\\] Este modelo más simple no se ajustó tan bien como el modelo más completo de Hoerl de la ecuación (19.2), teniendo \\(R^2 = 78.6 \\%\\). Sin embargo, una prueba parcial \\(F\\) estableció que los parámetros adicionales no eran estadísticamente significativos, por lo que se prefirió el modelo más simple de la ecuación (19.3). Basándonos en el modelo más simple, los valores ajustados se muestran en la Figura 19.4. Esta figura muestra los valores ajustados que disminuyen geométricamente comenzando en el cuarto período de desarrollo. Figura 19.4: Valores Ajustados del Modelo Reducido de Hoerl en la Ecuación (19.3). Código R para Producir la Figura 19.4 # Figura 19.4 # VALORES AJUSTADOS PARA LA FORMA REDUCIDA DE LA CURVA DE HOERL model3 &lt;- lm(lnPayment ~ factor(Year) + log(Delay) + Delay, data = SingInjuryA) beta &lt;- coefficients(model3) forecast &lt;- rep(0, 81) forei &lt;- forecast forej &lt;- forecast for (i in 1:9) { for (j in 1:9) { xij &lt;- c(1, rep(0, 8), log(j), j) if (i &gt; 1) { xij[i] &lt;- 1 } foreij &lt;- xij %*% beta forecast[9 * (i - 1) + j] &lt;- exp(foreij) forei[9 * (i - 1) + j] &lt;- i forej[9 * (i - 1) + j] &lt;- j } } # trellis.device(color = F) xyplot(forecast ~ forej, groups = forei, type = &quot;b&quot;, pch = 16, lty = 1, ylab = &quot;&quot;, xlab = &quot;Periodo de Desarrollo&quot;, scales = list(y = list(at = seq(0, 600000, 200000), labels = c(&quot;0&quot;, &quot;200,000&quot;, &quot;400,000&quot;, &quot;600,000&quot;))) ) 19.2.3 Modelos de Poisson Una desventaja del modelo lognormal es que las predicciones producidas por este no replican las estimaciones tradicionales de escalera en cadena. Esta sección introduce el modelo de Poisson sobredisperso, que sí tiene esta característica deseable. Para comenzar, a partir de la ecuación (19.1), podemos escribir \\[ \\mathrm{E}~ y_{ij} = \\exp(\\eta_{i,j})~~ \\mathrm{E}~ e^{\\varepsilon} \\] donde el componente sistemático es \\(\\eta_{i,j} = \\mu + \\alpha_i + \\tau_j\\). Este es un modelo con una función de enlace logarítmica (es decir, \\(\\ln \\mathrm{E}~y = \\eta\\)). En lugar de usar la distribución lognormal para \\(y\\), esta sección asume que \\(y\\) sigue un modelo de Poisson sobredisperso con función de varianza \\[ \\mathrm{Var}~ y_{ij} = \\phi \\exp(\\eta_{i,j}). \\] Note que hemos absorbido el escalar \\(\\mathrm{E}~ e^{\\varepsilon}\\) en el parámetro de sobredispersión \\(\\phi\\). Introducimos modelos de Poisson sobredispersos en la Sección 12.3. Por lo tanto, este modelo se puede estimar con software estadístico estándar y, al igual que con el modelo lognormal, se pueden producir fácilmente pronósticos. Se puede demostrar que los pronósticos producidos por el modelo de Poisson sobredisperso son equivalentes a los pronósticos determinísticos de escalera en cadena. Ver, por ejemplo, Taylor (2000) o Wüthrich y Merz (2008) para una prueba. Esto no solo nos da un mecanismo para cuantificar la incertidumbre asociada con los pronósticos de escalera en cadena, sino que también podemos usar software estadístico estándar para calcular estas estimaciones. Ejemplo: Lesiones de Terceros en Singapur - Continuación. El modelo de Poisson sobredisperso se ajustó a los datos de pagos por lesiones en Singapur. Se utilizó software estadístico estándar para calcular las estimaciones de parámetros, utilizando las técnicas descritas en la Sección 12.3. La Figura 19.5 resume los pronósticos de estos modelos. Esta figura muestra pagos acumulativos, no incrementales, marcados con símbolos de trazado opacos en la figura. Los pronósticos de pagos incrementales se produjeron y luego se sumaron para obtener los pronósticos de pagos acumulativos en la Figura 19.5; estos están marcados con los símbolos de trazado abiertos. Se invita al lector a verificar que estos pronósticos son idénticos a los producidos por el método determinístico de escalera en cadena hasta el octavo año de desarrollo. Aquí, el valor de cero para el primer año de incurrencia causa pequeñas diferencias entre los pronósticos del modelo de Poisson y la escalera en cadena. Figura 19.5: Valores Reales y Pronósticos para los Pagos Acumulativos por Lesiones en Singapur. Los valores reales están marcados con un símbolo opaco. Los pronósticos de escalera en cadena, de un modelo de Poisson sobredisperso, están marcados con un símbolo abierto. Código R para Producir la Figura 19.5 # Figura 19.5 # PAGOS ACUMULATIVOS # ordenar por Año y Retraso SingInjuryB &lt;- data.frame(SingInjury) newdata &lt;- SingInjuryB[order(SingInjury$Year, SingInjury$Delay),] cumpay &lt;- rep(0, 81) counter &lt;- 0 for (i in 1:9) { counter &lt;- counter + 1 cumpay[9 * (i - 1) + 1] &lt;- newdata$Payment[counter] for (j in 2:9) { if (i &lt;= 10 - j) { counter &lt;- counter + 1 cumpay[9 * (i - 1) + j] &lt;- cumpay[9 * (i - 1) + j - 1] + newdata$Payment[counter] } } } cumout &lt;- cbind(forei, forej, cumpay) # cumout # PRONÓSTICOS DE PAGOS ACUMULATIVOS cumfore &lt;- cumpay foretype &lt;- rep(0, 81) for (i in 1:9) { for (j in 2:9) { if (i &gt; 10 - j) { cumfore[9 * (i - 1) + j] &lt;- cumfore[9 * (i - 1) + j - 1] + forecast[9 * (i - 1) + j] foretype[9 * (i - 1) + j] &lt;- 1 } if (i == 10 - j) { foretype[9 * (i - 1) + j] &lt;- 2 } } } foretype[9 * (9 - 1) + 1] &lt;- 2 cumout &lt;- cbind(forei, forej, cumfore, foretype) CUMOUT &lt;- data.frame(cumout) plot1 &lt;- xyplot(cumfore ~ forej, data = subset(CUMOUT, foretype %in% c(1, 2)), groups = forei, type = &quot;b&quot;, pch = 1, lty = 1, ylab = &quot;&quot;, xlim = c(0, 10), ylim = c(0, 2500000), xlab = &quot;Periodo de Desarrollo&quot;) plot2 &lt;- xyplot(cumfore ~ forej, data = subset(CUMOUT, foretype %in% c(0, 2)), groups = forei, type = &quot;b&quot;, pch = 16, lty = 1, ylab = &quot;&quot;, xlim = c(0, 10), ylim = c(0, 2500000), xlab = &quot;Periodo de Desarrollo&quot;) # trellis.device(color = F) print(plot1, position = c(0, 0, 1, 1), more = TRUE) print(plot2, position = c(0, 0, 1, 1)) 19.3 Usando Desarrollos Pasados Al igual que con los modelos autorregresivos, se puede usar el historial previo para pronosticar pagos. ¿Qué es el historial previo? En el marco del triángulo de reclamaciones en Tabla 19.1, hay dos dimensiones de tiempo: año de incurrencia y año de desarrollo. La mayoría de los modelos se centran en usar el desarrollo previo (“\\(j\\)”) para realizar pronósticos. Al centrarnos en la experiencia de desarrollo previo, nos permitimos la flexibilidad de modelar pagos acumulativos (\\(S_{ij}\\)) o incrementales (\\(y_{ij}\\)). Como aprendimos en los Capítulos 7 y 8 al estudiar series de tiempo, es útil poder modelar tanto una serie (\\(S_{ij}\\)) como sus cambios (\\(y_{ij} = S_{ij} - S_{i,j-1}\\)). 19.3.1 Modelo de Mack El modelo propuesto por Mack (1993) especifica los dos primeros momentos condicionales de los pagos acumulativos y utiliza mínimos cuadrados generalizados para ajustar el modelo. Bajo esta especificación estocástica, se producen pronósticos tradicionales de escalera en cadena. Específicamente, asumimos \\[\\begin{equation} \\mathrm{E} \\left( S_{i,j} | S_{i,j-1} \\right) = \\nu_j S_{i,j-1} \\tag{19.4} \\end{equation}\\] y \\[\\begin{equation} \\mathrm{Var} \\left( S_{i,j} | S_{i,j-1} \\right) = \\sigma^2_j S_{i,j-1}, \\tag{19.5} \\end{equation}\\] donde \\(\\nu_j\\) y \\(\\sigma^2_j\\) son parámetros del modelo. Los parámetros de media, \\(\\nu_j\\), se determinan a través de mínimos cuadrados generalizados al minimizar la cantidad \\[ Q = \\sum_{j=2}^n \\sum_{i=1}^{n+1-i} \\frac{(S_{ij} - \\nu_j S_{i,j-1})^2}{\\sigma_j^2 S_{i,j-1}}. \\] Tomando derivadas con respecto a los parámetros \\(\\nu_j\\) y estableciendo estas igual a cero, se obtiene \\[ \\frac{\\partial}{\\partial \\nu_j}Q = \\sum_{i=1}^{n+1-i} \\frac{(-2) (S_{ij} - \\nu_j S_{i,j-1})}{\\sigma_j^2 S_{i,j-1}} = 0. \\] La solución de esta ecuación resulta en \\[ \\hat{\\nu_j}= \\frac{\\sum_{i=1}^{n+1-i} S_{ij}} {\\sum_{i=1}^{n+1-i} S_{i,j-1}}, \\] el factor de escalera en cadena. Aquí, el símbolo “hat” o “sombrero” en \\(\\nu_j\\) indica que \\(\\hat{\\nu_j}\\) es un estimador determinado por los datos. Con estas estimaciones de parámetros, se puede utilizar la ecuación (19.4) para producir valores ajustados que son iguales a las estimaciones de la escalera en cadena. Además, se pueden estimar los parámetros de escala \\(\\sigma^2_j\\) y luego usar la ecuación (19.5) para cuantificar la incertidumbre de las estimaciones. Consulte a England y Verrall (2002) o Wüthrich y Merz (2008) para obtener detalles sobre la estimación de parámetros de escala. La fortaleza y limitación de este modelo es que solo emplea suposiciones sobre los dos primeros momentos condicionales. Es una fortaleza en el sentido de que no necesitamos preocuparnos si la distribución subyacente se acerca a lognormal o Poisson. Por lo tanto, a veces se lo denomina un modelo “no paramétrico”. Es una limitación en el sentido de que las medidas de incertidumbre en la ecuación (19.5) están relacionadas con el segundo momento que utiliza una función de pérdida de error cuadrático. Para reclamaciones de seguros, los datos suelen estar sesgados, por lo que la varianza no es una buena medida de escala. Además, en la reserva de pérdidas, queremos saber si las reservas son demasiado altas o demasiado bajas; usar una medida de incertidumbre que solo informe desviaciones absolutas no proporciona al actuario el tipo de información necesaria. 19.3.2 Modelos Distribucionales Se han propuesto en la literatura modelos que complementan las suposiciones de momentos en las ecuaciones (19.4) y (19.5) con suposiciones distribucionales sobre los pagos. Por ejemplo, Verrall propuso usar la binomial negativa como distribución para \\(\\{y_{ij}\\}\\) con momentos condicionales \\[ \\mathrm{E} \\left( y_{i,j} | S_{i,j-1} \\right) = (\\nu_j-1) S_{i,j-1} \\] y \\[ \\mathrm{Var} \\left( y_{i,j} | S_{i,j-1} \\right) = \\phi \\nu_j (\\nu_j -1) S_{i,j-1}, \\] donde \\(\\phi\\) y \\(\\nu_j\\) son parámetros del modelo. Note que la suposición de media condicional es la misma que en la ecuación (19.4) debido a la relación \\(\\mathrm{E} \\left( S_{i,j} | S_{i,j-1} \\right) = S_{i,j-1} + \\mathrm{E} \\left( y_{i,j} | S_{i,j-1} \\right)\\). De manera similar, podemos expresar la varianza de los pagos acumulativos como \\(\\mathrm{Var} \\left( S_{i,j} | S_{i,j-1} \\right) = \\mathrm{Var} \\left( y_{i,j} | S_{i,j-1} \\right)\\). Por lo tanto, la suposición de varianza condicional es como en la ecuación (19.5) con parámetros \\(\\sigma^2_j = \\phi \\nu_j (\\nu_j -1)\\). Este modelo puede implementarse fácilmente utilizando software de modelos lineales generalizados especificando el modelo binomial negativo con función de enlace logarítmica \\[ \\ln \\mu_{ij} = \\ln \\lambda_j + \\ln S_{i,j-1}, \\] donde \\(\\mu_{ij} = \\mathrm{E} \\left( y_{i,j} | S_{i,j-1} \\right)\\). Consulte England y Verrall (2002, Sección 7.3) para mayor discusión. Otro modelo distribucional, sugerido por England y Verrall (2002, Sección 2.5), consiste en usar los momentos en las ecuaciones (19.4) y (19.5) pero especificar una distribución normal para los pagos \\(\\{y_{ij} \\}\\). Esto puede ser útil como una aproximación al modelo binomial negativo, particularmente en el caso cuando las estimaciones de \\(\\nu_j\\) son menores a uno, lo que indica que la varianza condicional está mal especificada. Como se enfatizó al comienzo de la Sección 19.2, las principales fortalezas de los modelos distribucionales son que (1) permiten a los analistas cuantificar la incertidumbre de los pronósticos y (2) proporcionan mecanismos disciplinados para la selección de modelos. 19.4 Lecturas Adicionales y Referencias Dos introducciones extensas a los triángulos de reclamaciones son Taylor (2000) y Wüthrich y Merz (2008). Los actuarios en ejercicio encontrarán útil el artículo de revisión de England y Verrall (2002). Cuando hay inestabilidad en los patrones de desarrollo en los primeros años, un método desarrollado por Bornheuetter y Ferguson (1972) puede ser útil, ya que permite al actuario incorporar una evaluación externa de las reclamaciones pagadas definitivas. Al igual que con la escalera en cadena, esta técnica puede expresarse en términos de modelado estocástico mediante modelado bayesiano. Vea, por ejemplo, la discusión de England y Verrall (2002), Wüthrich y Merz (2008) y de Alba (2006). Un problema emergente es desarrollar reservas cuando un triángulo está correlacionado con otro. Tal correlación podría esperarse entre líneas de negocio de seguros. Braun (2004) proporciona una introducción a este tema. Otra área emergente es desarrollar reservas basadas en patrones de desarrollo de reclamaciones individuales, como se describe en la Sección 19.1.1. Antonio et al. (2006) proporciona una introducción a este tema, donde usan modelos lineales mixtos para desarrollar reservas para reclamaciones que han sido reportadas pero aún no liquidadas. Vea también la Sección 14.5 sobre teoría de eventos recurrentes. Referencias del Capítulo de Alba, Enrique (2006). Claims reserving when there are negative values in the runoff triangle: Bayesian analysis using the three-parameter log-normal distribution. North American Actuarial Journal 10 (3), 45-59. Antonio, Katrien, Jan Beirlant, Tom Hoedemakers and Robert Verlaak (2006). Lognormal mixed models for reported claim reserves. North American Actuarial Journal 10 (1), 30-48. Bornheuetter, Ronald L. and Ronald E. Ferguson (1972). The actuary and IBNR. Proceedings of the Casualty Actuarial Society 59, 181-195. Braun, Christian (2004). The prediction error of the chain ladder method applied to correlated run-off triangles. Astin Bulletin 34 (2), 399-423. England, Peter D. and Richard J. Verrall (2002). Stochastic claims reserving in general insurance. British Actuarial Journal 8, 443-544. Gamage, Jinadasa, Jed Linfield, Krzysztof Ostaszewski and Steven Siegel (2007). Statistical methods for health actuaries - IBNR estimates: An introduction. Society of Actuaries working paper, Society of Actuaries, Schaumburg IL. Mack, Thomas (1993). Distribution-free calculation of the standard error of chain-ladder reserve estimates. ASTIN Bulletin 23 (2), 213-225. Mack, Thomas (1994). Measuring the variability of chain-ladder reserve estimates. Casualty Actuarial Society, Spring Forum. Taylor, Greg (2000). Loss Reserving: An Actuarial Perspective. Kluwer Academic Publishers, Boston. Wacek, Michael G. (2007). The path of the ultimate loss ratio estimate. Variance 1(2), 173-192. Wüthrich, Mario V. and Michael Merz (2008). Stochastic Claims Reserving Methods in Insurance. Wiley, New York. 19.5 Ejercicios 19.1. Los datos en Tabla 19.5 provienen de la edición de 1991 del “Historical Loss Development Study” publicado por la Reinsurance Association of America (página 91). Estos datos se han utilizado ampliamente para ilustrar métodos de triángulo, comenzando con Mack (1994) y posteriormente por England y Verrall (2002). Estos datos provienen de negocios de reaseguro facultativo automático en coberturas de responsabilidad civil general (excluyendo asbesto y medio ambiente). (Bajo una base facultativa, cada riesgo es suscrito por el reasegurador según sus propios méritos). Tabla 19.5 reporta pérdidas incurridas incrementales de 1981 a 1990, en miles de dólares estadounidenses. Comience calculando los factores determinísticos de la cadena en escalera. Note que el elemento en el segundo origen y el séptimo año de desarrollo es negativo. Puede que desee convertir primero los pagos incrementales en pagos acumulativos. Use estos factores para “completar el triángulo.” Utilice su trabajo en la parte (a) para calcular la estimación de la reserva. Elimine la observación en el segundo origen y el séptimo año de desarrollo. Ajuste un modelo lognormal a los datos restantes. Comente sobre la significancia estadística de cada factor y la bondad de ajuste. Ajuste el modelo de Hoerl a los datos de la parte (c). Genere un gráfico de los valores ajustados. Ajuste el modelo de Poisson sobredisperso a los datos de la parte (c). Verifique la proximidad de estos valores ajustados con los valores de la cadena en escalera producidos en la parte (a). Tabla 19.5. Estudio de Desarrollo de Pérdidas (1991) Reaseguro Facultativo \\[ \\small{ \\begin{array}{crrrrrrrrrr} \\hline \\text{Año} &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 \\\\ \\hline 1 &amp; 5,012 &amp; 3,257 &amp; 2,638 &amp; 898 &amp; 1,734 &amp; 2,642 &amp; 1,828 &amp; 599 &amp; 54 &amp; 172 \\\\ 2 &amp; 106 &amp; 4,179 &amp; 1,111 &amp; 5,270 &amp; 3,116 &amp; 1,817 &amp; -103 &amp; 673 &amp; 535 &amp; \\\\ 3 &amp; 3,410 &amp; 5,582 &amp; 4,881 &amp; 2,268 &amp; 2,594 &amp; 3,479 &amp; 649 &amp; 603 &amp; &amp; \\\\ 4 &amp; 5,655 &amp; 5,900 &amp; 4,211 &amp; 5,500 &amp; 2,159 &amp; 2,658 &amp; 984 &amp; &amp; &amp; \\\\ 5 &amp; 1,092 &amp; 8,473 &amp; 6,271 &amp; 6,333 &amp; 3,786 &amp; 225 &amp; &amp; &amp; &amp; \\\\ 6 &amp; 1,513 &amp; 4,932 &amp; 5,257 &amp; 1,233 &amp; 2,917 &amp; &amp; &amp; &amp; &amp; \\\\ 7 &amp; 557 &amp; 3,463 &amp; 6,926 &amp; 1,368 &amp; &amp; &amp; &amp; &amp; &amp; \\\\ 8 &amp; 1,351 &amp; 5,596 &amp; 6,165 &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ 9 &amp; 3,133 &amp; 2,262 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ 10 &amp; 2,063 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ \\hline \\end{array} } \\] 19.2. Los datos en Tabla 19.6 son un extracto de Braun (2004) basado en la edición de 2001 del “Historical Loss Development Study” publicado por la Reinsurance Association of America. Los datos más amplios (disponibles en el archivo “ReinsGL2004”) contienen datos de los años 1987-2000, inclusive. Repita las partes (a)-(e) del Ejercicio 19.1 para estos datos. Tabla 19.6. Reaseguro de Responsabilidad General \\[ \\small{ \\begin{array}{lrrrrrr} \\hline \\text{Año} &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\\\ \\hline 1995 &amp; 97,518 &amp; 343,218 &amp; 575,441 &amp; 769,017 &amp; 934,103 &amp; 1,019,303 \\\\ 1996 &amp; 173,686 &amp; 459,416 &amp; 722,336 &amp; 955,335 &amp; 1,141,750 &amp; \\\\ 1997 &amp; 139,821 &amp; 436,958 &amp; 809,926 &amp; 1,174,196 &amp; &amp; \\\\ 1998 &amp; 154,965 &amp; 528,080 &amp; 1,032,684 &amp; &amp; &amp; \\\\ 1999 &amp; 196,124 &amp; 772,971 &amp; &amp; &amp; &amp; \\\\ 2000 &amp; 204,325 &amp; &amp; &amp; &amp; &amp; \\\\ \\hline \\end{array} } \\] 19.3. Los datos en Tabla 19.7 provienen de Wacek (2007). Los datos representan agregados de la industria para coberturas de responsabilidad civil y médicas de automóviles particulares del año 2004, en millones de dólares. Están basados en los estados anuales de las compañías de seguros, específicamente, en el Anexo P, Parte 3B. Los elementos del triángulo representan pagos netos acumulados, incluidos los gastos de defensa y contención de costos. Repita las partes (a)-(e) del Ejercicio 19.1 para estos datos. Tabla 19.7. 2004 Agregados de la Industria de Seguros de los EE. UU. para Responsabilidad Civil y Médica de Automóviles Particulares \\[ \\small{ \\begin{array}{rrrrrrrrrrr} \\hline \\text{Año} &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 \\\\ \\hline 1995 &amp; 17,674 &amp; 32,062 &amp; 38,619 &amp; 42,035 &amp; 43,829 &amp; 44,723 &amp; 45,162 &amp; 45,375 &amp; 45,483 &amp; 45,540 \\\\ 1996 &amp; 18,315 &amp; 32,791 &amp; 39,271 &amp; 42,933 &amp; 44,950 &amp; 45,917 &amp; 46,392 &amp; 46,600 &amp; 46,753 &amp; \\\\ 1997 &amp; 18,606 &amp; 32,942 &amp; 39,634 &amp; 43,411 &amp; 45,428 &amp; 46,357 &amp; 46,681 &amp; 46,921 &amp; &amp; \\\\ 1998 &amp; 18,816 &amp; 33,667 &amp; 40,575 &amp; 44,446 &amp; 46,476 &amp; 47,350 &amp; 47,809 &amp; &amp; &amp; \\\\ 1999 &amp; 20,649 &amp; 36,515 &amp; 43,724 &amp; 47,684 &amp; 49,753 &amp; 50,716 &amp; &amp; &amp; &amp; \\\\ 2000 &amp; 22,327 &amp; 39,312 &amp; 46,848 &amp; 51,065 &amp; 53,242 &amp; &amp; &amp; &amp; &amp; \\\\ 2001 &amp; 23,141 &amp; 40,527 &amp; 48,284 &amp; 52,661 &amp; &amp; &amp; &amp; &amp; &amp; \\\\ 2002 &amp; 24,301 &amp; 42,168 &amp; 50,356 &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ 2003 &amp; 24,210 &amp; 41,640 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ 2004 &amp; 24,468 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ \\hline \\end{array} } \\] 19.4. Los datos en Tabla 19.8 provienen de Gamage et al. (2007). Estos datos corresponden a 36 meses de pagos de atención médica, desde enero de 2001 hasta diciembre de 2003, inclusive. Estos son pagos por cobertura médica sin deducible ni coseguro. Los copagos eran relativamente bajos, como $10 por visita al consultorio. Los pagos excluyen medicamentos recetados, que generalmente tienen un patrón de pago más corto en comparación con otras reclamaciones médicas. Repita las partes (a)-(e) del Ejercicio 19.1 para estos datos. Tabla 19.8. Pagos Mensuales de Atención Médica, 2001-2003 \\[ \\begin{array}{lrcrrrrrrrrrrrrr} \\hline \\text{Fecha} &amp; \\text{Miembros} &amp; \\text{ Mes} &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 &amp; 11 &amp; 12 &amp; 13 \\\\ \\hline Ene-01 &amp; 11,154 &amp; 1 &amp; 180 &amp; 436,082 &amp; 933,353 &amp; 116,978 &amp; 42,681 &amp; 41,459 &amp; 5,088 &amp; 22,566 &amp; 4,751 &amp; 3,281 &amp; -188 &amp; 1,464 &amp; 1,697 \\\\ Feb-01 &amp; 11,118 &amp; 2 &amp; 5,162 &amp; 940,722 &amp; 561,967 &amp; 21,694 &amp; 171,659 &amp; 11,008 &amp; 19,088 &amp; 5,213 &amp; 4,337 &amp; 7,844 &amp; 2,973 &amp; 4,061 &amp; 10,236 \\\\ Mar-01 &amp; 11,070 &amp; 3 &amp; 42,263 &amp; 844,293 &amp; 720,302 &amp; 94,634 &amp; 182,077 &amp; 32,216 &amp; 12,937 &amp; 22,815 &amp; 1,754 &amp; 4,695 &amp; 1,326 &amp; 758 &amp; 2,177 \\\\ Abr-01 &amp; 11,069 &amp; 4 &amp; 20,781 &amp; 762,302 &amp; 394,625 &amp; 78,043 &amp; 157,950 &amp; 46,173 &amp; 126,254 &amp; 4,839 &amp; 337 &amp; 1,573 &amp; 9,573 &amp; 1,947 &amp; 5,937 \\\\ May-01 &amp; 11,130 &amp; 5 &amp; 20,346 &amp; 772,404 &amp; 392,330 &amp; 315,888 &amp; 39,197 &amp; 21,360 &amp; 8,721 &amp; 5,452 &amp; 16,627 &amp; 2,118 &amp; 4,119 &amp; 5,666 &amp; -1,977 \\\\ Jun-01 &amp; 11,174 &amp; 6 &amp; 20,491 &amp; 831,793 &amp; 738,087 &amp; 65,526 &amp; 27,768 &amp; 12,185 &amp; 1,493 &amp; 11,265 &amp; 1,805 &amp; 29,278 &amp; 13,020 &amp; 2,967 &amp; -83 \\\\ Jul-01 &amp; 11,180 &amp; 7 &amp; 37,954 &amp; 1,126,675 &amp; 360,514 &amp; 89,317 &amp; 40,126 &amp; 16,576 &amp; 16,701 &amp; 2,444 &amp; 8,266 &amp; 11,310 &amp; 8,006 &amp; 1,403 &amp; 3,124 \\\\ Aug-01 &amp; 11,420 &amp; 8 &amp; 138,558 &amp; 806,362 &amp; 589,304 &amp; 273,117 &amp; 36,912 &amp; 16,831 &amp; 19,941 &amp; 13,310 &amp; 8,619 &amp; 4,679 &amp; 3,094 &amp; 4,609 &amp; 236 \\\\ Sep-01 &amp; 11,400 &amp; 9 &amp; 28,332 &amp; 954,543 &amp; 246,571 &amp; 205,528 &amp; 60,060 &amp; 15,198 &amp; 42,208 &amp; 17,568 &amp; 1,686 &amp; 9,897 &amp; 3,367 &amp; 2,062 &amp; 421 \\\\ Oct-01 &amp; 11,456 &amp; 10 &amp; 104,160 &amp; 704,796 &amp; 565,939 &amp; 323,789 &amp; 45,307 &amp; 32,518 &amp; 26,227 &amp; 7,976 &amp; 3,364 &amp; 992 &amp; 33,963 &amp; 2,200 &amp; 1,293 \\\\ Nov-01 &amp; 11,444 &amp; 11 &amp; 40,747 &amp; 927,158 &amp; 425,794 &amp; 146,145 &amp; 66,663 &amp; 31,214 &amp; 12,808 &amp; 15,859 &amp; 374 &amp; 3,079 &amp; 412 &amp; 937 &amp; 1,875 \\\\ Dic-01 &amp; 11,555 &amp; 12 &amp; 10,861 &amp; 847,338 &amp; 272,165 &amp; 134,798 &amp; 71,804 &amp; 27,800 &amp; 17,917 &amp; 3,930 &amp; 2,794 &amp; 846 &amp; 1,962 &amp; 1,879 &amp; 16,060 \\\\ Ene-02 &amp; 11,705 &amp; 13 &amp; 77,938 &amp; 896,195 &amp; 544,372 &amp; 173,606 &amp; 41,595 &amp; 4,209 &amp; 16,473 &amp; 6,000 &amp; -66 &amp; -1,881 &amp; -4,054 &amp; 84,233 &amp; 4,921 \\\\ Feb-02 &amp; 11,823 &amp; 14 &amp; 38,041 &amp; 1,035,439 &amp; 438,153 &amp; 115,587 &amp; 12,489 &amp; 22,260 &amp; 13,203 &amp; 6,395 &amp; 2,056 &amp; -3,323 &amp; 33,397 &amp; 3,479 &amp; -1,625 \\\\ Mar-02 &amp; 11,753 &amp; 15 &amp; 39,410 &amp; 1,022,024 &amp; 255,002 &amp; 169,881 &amp; 35,230 &amp; 40,307 &amp; 21,067 &amp; 5,378 &amp; 5,508 &amp; 17,606 &amp; -24,320 &amp; 1,298 &amp; 1,362 \\\\ Abr-02 &amp; 11,654 &amp; 16 &amp; 68,253 &amp; 1,414,379 &amp; 317,110 &amp; 91,880 &amp; 53,970 &amp; 10,888 &amp; 3,171 &amp; 11,660 &amp; 20,861 &amp; 1,033 &amp; -21,670 &amp; 2,634 &amp; 149 \\\\ May-02 &amp; 11,703 &amp; 17 &amp; 124,824 &amp; 1,053,972 &amp; 516,876 &amp; 145,954 &amp; 25,171 &amp; 12,609 &amp; 7,704 &amp; 29,633 &amp; 4,555 &amp; 6,203 &amp; 3,872 &amp; 1,116 &amp; 666 \\\\ Jun-02 &amp; 11,580 &amp; 18 &amp; 49,725 &amp; 1,119,099 &amp; 533,444 &amp; 80,182 &amp; 32,203 &amp; 23,205 &amp; 18,807 &amp; 7,944 &amp; 4,152 &amp; -910 &amp; 3,664 &amp; 608 &amp; 528 \\\\ Jul-02 &amp; 11,577 &amp; 19 &amp; 44,317 &amp; 1,297,335 &amp; 385,789 &amp; 141,155 &amp; 150,726 &amp; 35,075 &amp; 16,176 &amp; 8,070 &amp; 67 &amp; 14,217 &amp; 2,326 &amp; 7,091 &amp; 687 \\\\ Aug-02 &amp; 11,655 &amp; 20 &amp; 134,152 &amp; 1,111,151 &amp; 493,175 &amp; 101,439 &amp; 46,657 &amp; 22,824 &amp; 12,818 &amp; 3,781 &amp; 1,265 &amp; 2,467 &amp; -62,165 &amp; 247 &amp; -8,689 \\\\ Sep-02 &amp; 11,735 &amp; 21 &amp; 29,968 &amp; 1,382,043 &amp; 178,587 &amp; 71,030 &amp; 25,708 &amp; 15,068 &amp; 3,145 &amp; -4,058 &amp; -1,920 &amp; 4,984 &amp; -1,523 &amp; -3,539 &amp; -478 \\\\ Oct-02 &amp; 11,889 &amp; 22 &amp; 210,377 &amp; 999,963 &amp; 528,880 &amp; 201,410 &amp; 58,003 &amp; 26,174 &amp; -9,371 &amp; 2,017 &amp; 9,795 &amp; 6,688 &amp; -40 &amp; 453 &amp; -73 \\\\ Nov-02 &amp; 11,951 &amp; 23 &amp; 56,654 &amp; 1,206,370 &amp; 376,504 &amp; 56,322 &amp; 19,591 &amp; 12,055 &amp; 21,077 &amp; 11,573 &amp; 4,039 &amp; 822 &amp; 6,612 &amp; -9,678 &amp; 715 \\\\ Dic-02 &amp; 12,132 &amp; 24 &amp; 89,181 &amp; 1,240,938 &amp; 279,553 &amp; 57,164 &amp; 75,344 &amp; 12,665 &amp; 71,741 &amp; 9,049 &amp; 1,298 &amp; 12,164 &amp; 19,616 &amp; -4,604 &amp; -3,184 \\\\ Ene-03 &amp; 12,227 &amp; 25 &amp; 131,568 &amp; 1,301,927 &amp; 716,180 &amp; 150,253 &amp; 110,031 &amp; 78,148 &amp; 4,610 &amp; 19,855 &amp; 18,448 &amp; 14,432 &amp; 119 &amp; 2,748 &amp; \\\\ Feb-03 &amp; 12,201 &amp; 26 &amp; 76,262 &amp; 1,130,312 &amp; 692,736 &amp; 174,283 &amp; 38,891 &amp; 41,811 &amp; 8,834 &amp; 18,123 &amp; 4,268 &amp; -291 &amp; 2,119 &amp; &amp; \\\\ Mar-03 &amp; 12,130 &amp; 27 &amp; 159,575 &amp; 1,313,809 &amp; 704,116 &amp; 68,412 &amp; 30,185 &amp; 64,402 &amp; 19,229 &amp; -3,021 &amp; 3,220 &amp; 1,994 &amp; &amp; &amp; \\\\ Abr-03 &amp; 11,986 &amp; 28 &amp; 76,313 &amp; 1,505,842 &amp; 437,084 &amp; 50,872 &amp; 116,723 &amp; 18,160 &amp; 10,975 &amp; 12,664 &amp; 8,805 &amp; &amp; &amp; &amp; \\\\ May-03 &amp; 11,927 &amp; 29 &amp; 104,028 &amp; 1,667,823 &amp; 360,676 &amp; 153,274 &amp; 37,529 &amp; 34,840 &amp; 17,479 &amp; 9,374 &amp; &amp; &amp; &amp; &amp; \\\\ Jun-03 &amp; 11,814 &amp; 30 &amp; 79,688 &amp; 1,235,573 &amp; 776,240 &amp; 65,303 &amp; 18,723 &amp; 10,779 &amp; 10,615 &amp; &amp; &amp; &amp; &amp; &amp; \\\\ Jul-03 &amp; 11,787 &amp; 31 &amp; 76,395 &amp; 1,689,354 &amp; 442,965 &amp; 234,171 &amp; 36,806 &amp; 22,351 &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ Aug-03 &amp; 11,689 &amp; 32 &amp; 110,460 &amp; 1,492,980 &amp; 589,184 &amp; 93,366 &amp; 180,095 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ Sep-03 &amp; 11,731 &amp; 33 &amp; 196,687 &amp; 2,011,979 &amp; 313,416 &amp; 166,839 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ Oct-03 &amp; 11,843 &amp; 34 &amp; 268,365 &amp; 1,027,925 &amp; 897,097 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ Nov-03 &amp; 11,902 &amp; 35 &amp; 58,510 &amp; 1,225,307 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ Dic-03 &amp; 11,844 &amp; 36 &amp; 96,378 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ \\hline \\hline \\end{array} \\] "],["C20Report.html", "Capítulo 20 Redacción de Informes: Comunicando Resultados del Análisis de Datos 20.1 Visión General 20.2 Métodos para Comunicar Datos 20.3 Cómo Organizar 20.4 Sugerencias Adicionales para la Redacción de Informes 20.5 Estudio de Caso: Reclamos de Automóviles en Suecia 20.6 Lecturas Adicionales y Referencias 20.7 Ejercicio", " Capítulo 20 Redacción de Informes: Comunicando Resultados del Análisis de Datos Vista Previa del Capítulo. Los informes estadísticos deben ser accesibles para diferentes tipos de lectores. Estos informes informan a los gerentes que desean visiones generales en lenguaje no técnico, así como a analistas que requieren detalles técnicos para replicar el estudio. Este capítulo resume métodos para redactar y organizar informes estadísticos. Para ilustrar, consideraremos un informe de reclamaciones de seguros de automóviles de terceros. 20.1 Visión General Se ha explorado la última relación, se ha estimado el último parámetro, se ha realizado el último pronóstico, y ahora estás listo para compartir los resultados de tu análisis estadístico con el mundo. El medio de comunicación puede tomar muchas formas: puedes simplemente recomendar a un cliente “comprar barato, vender caro” o dar una presentación oral a tus colegas. Sin embargo, lo más probable es que necesites resumir tus hallazgos en un informe escrito. Comunicar información técnica es difícil por diversas razones. Primero, en la mayoría de los análisis de datos no hay una “respuesta correcta” que el autor trate de comunicar al lector. Para establecer una “respuesta correcta”, solo es necesario posicionar los pros y los contras de un tema y evaluar sus méritos relativos. En los informes estadísticos, el autor intenta comunicar las características de los datos y su relación con patrones más generales, una tarea mucho más compleja. Segundo, la mayoría de los informes están dirigidos a un cliente o audiencia principal. En contraste, los informes estadísticos suelen ser leídos por muchos lectores diferentes cuyo conocimiento de los conceptos estadísticos varía ampliamente; es importante considerar las características de esta audiencia heterogénea al juzgar el ritmo y orden en que se presenta el material. Esto es particularmente difícil cuando un escritor solo puede suponer quién será la audiencia secundaria. Tercero, los autores de informes estadísticos necesitan tener una base de conocimiento amplia y profunda, incluyendo un buen entendimiento de los temas subyacentes, conocimientos de conceptos estadísticos y habilidades lingüísticas. Combinar estas habilidades puede ser un desafío. Incluso para un escritor generalmente efectivo, cualquier confusión en el análisis inevitablemente se reflejará en el informe. La comunicación de los resultados del análisis de datos puede ser desde una breve recomendación oral a un cliente hasta una tesis doctoral de 500 páginas. Sin embargo, para la mayoría de los propósitos empresariales, basta con un informe de 10 a 20 páginas que resuma las principales conclusiones y detalle el análisis. Un aspecto clave de dicho informe es proporcionar al lector una comprensión de las características destacadas de los datos. Se deben incluir suficientes detalles del estudio para que el análisis pueda ser replicado de forma independiente con acceso a los datos originales. 20.2 Métodos para Comunicar Datos Para permitir que los lectores interpreten la información numérica de manera efectiva, los datos deben presentarse utilizando una combinación de palabras, números y gráficos que revelen su complejidad. Por lo tanto, los creadores de presentaciones de datos deben basarse en habilidades provenientes de varias áreas, incluyendo: un entendimiento del área temática subyacente, un conocimiento de los conceptos estadísticos relacionados, una apreciación de los atributos de diseño de las presentaciones de datos y una comprensión de las características de la audiencia a la que están dirigidos. Este equilibrio de conocimientos es vital si el propósito de la presentación de datos es informar. Si el propósito es animar los datos (“porque los datos son inherentemente aburridos”) o atraer atención, entonces los atributos de diseño pueden adquirir un papel más destacado. Por el contrario, algunos creadores con fuertes habilidades cuantitativas simplifican las presentaciones de datos en exceso para llegar a una audiencia amplia. Al no utilizar los atributos de diseño adecuados, solo revelan parte de la información numérica y ocultan la verdadera historia de sus datos. Para citar a Albert Einstein, “Deberías hacer tus modelos tan simples como sea posible, pero no más simples”. Esta sección presenta los elementos básicos y reglas para construir presentaciones de datos exitosas. Con este fin, discutimos tres modos de presentar información numérica: (i) datos dentro del texto, (ii) datos tabulares y (iii) gráficos de datos. Estos tres modos están ordenados aproximadamente por la complejidad de los datos que están diseñados para presentar; desde el modo de datos dentro del texto, que es más útil para representar los tipos más simples de datos, hasta el modo de gráficos de datos, que es capaz de transmitir información numérica de conjuntos de datos extremadamente grandes. Datos Dentro del Texto Los datos dentro del texto simplemente se refieren a cantidades numéricas que se citan dentro de la estructura habitual de las oraciones. Por ejemplo: El precio de la acción de Vigoro hoy es $36.50 por acción, un récord histórico. Al presentar datos dentro del texto, tendrás que decidir si usar cifras o escribir un número en palabras. Hay varias pautas para elegir entre cifras y palabras, aunque generalmente para la redacción empresarial usarás palabras si esta elección resulta en una declaración concisa. Algunas de las pautas importantes incluyen: Escribe con palabras los números enteros del uno al noventa y nueve. Usa cifras para números fraccionarios. Escribe con palabras los números redondos que son aproximaciones. Escribe con palabras los números que comienzan una oración. Usa cifras en oraciones que contienen varios números. Por ejemplo: Hay cuarenta y tres estudiantes en mi clase. Con 0.2267 dólares estadounidenses puedo comprar una corona sueca. Hay alrededor de cuarenta y tres mil estudiantes en esta universidad. Tres mil cuatrocientas cincuenta y seis personas votaron por mí. Esos niños tienen 3, 4, 6 y 7 años. El texto fluye de manera lineal; esto dificulta que el lector haga comparaciones de datos dentro de una oración. Cuando las listas de números se vuelven largas o cuando es importante hacer comparaciones, un dispositivo útil para presentar datos es la tabla dentro del texto, también llamada forma semitabular. Por ejemplo: Para 2005, las primas netas por las principales líneas de negocio escritas por aseguradoras de bienes y accidentes en miles de millones de dólares estadounidenses fueron: Auto particular — 159.57 Hogares múltiples riesgos — 53.01 Compensación laboral — 39.73 Otras líneas — 175.09. (Fuente: The Insurance Information Institute Fact Book 2007.) Tablas Cuando la lista de números es más larga, la forma tabular o tabla es la opción preferida para presentar datos. Los elementos básicos de una tabla aparecen en Tabla 20.1. Tabla 20.1. Estadísticas Resumidas de las Variables de Liquidez de Acciones \\[ \\small{ \\begin{array}{lrrrrr} \\hline &amp; &amp; &amp; \\text{Desviación} &amp; &amp; \\\\ &amp; \\text{Media} &amp; \\text{Mediana} &amp; \\text{Estándar} &amp; \\text{Mínimo} &amp; \\text{Máximo} \\\\ \\hline \\text{VOLUME} &amp; 13.423 &amp; 11.556 &amp; 10.632 &amp; 0.658 &amp; 64.572 \\\\ \\text{AVG}T &amp; 5.441 &amp; 4.284 &amp; 3.853 &amp; 0.590 &amp; 20.772 \\\\ \\text{NTRAN} &amp; 6436 &amp; 5071 &amp; 5310 &amp; 999 &amp; 36420 \\\\ \\text{PRICE} &amp; 38.80 &amp; 34.37 &amp; 21.37 &amp; 9.12 &amp; 122.37 \\\\ \\text{SHARE} &amp; 94.7 &amp; 53.8 &amp; 115.1 &amp; 6.7 &amp; 783.1 \\\\ \\text{VALUE} &amp; 4.116 &amp; 2.065 &amp; 8.157 &amp; 0.115 &amp; 75.437 \\\\ \\text{DEB_EQ} &amp; 2.697 &amp; 1.105 &amp; 6.509 &amp; 0.185 &amp; 53.628 \\\\\\hline \\\\ \\hline \\end{array} } \\] Fuente: Francis Emory Fitch, Inc., Standard &amp; Poor’s Compustat, y el Centro de Investigación de Precios de Seguridad de la Universidad de Chicago. Estos son: Título. Una breve descripción de los datos, ubicada encima o al lado de la tabla. Para documentos más largos, proporcione un número de tabla para facilitar la referencia dentro del cuerpo principal del texto. El título puede ser complementado por observaciones adicionales, formando así una leyenda. Encabezados de columnas. Indicaciones breves del contenido de las columnas. Etiqueta. La columna vertical izquierda. A menudo proporciona información identificativa para los elementos individuales de las filas. Cuerpo. Las demás columnas verticales de la tabla. Reglas. Líneas que separan la tabla en sus diversos componentes. Fuente. Proporciona el origen de los datos. Al igual que con la forma semitabular, las tablas pueden diseñarse para mejorar las comparaciones entre números. A diferencia de la forma semitabular, las tablas están separadas del cuerpo principal del texto. Debido a que están separadas, las tablas deben ser autosuficientes para que el lector pueda obtener información de la tabla con poca referencia al texto. El título debe llamar la atención sobre las características importantes de la tabla. El diseño debe guiar la vista del lector y facilitar las comparaciones. Tabla 20.1 ilustra la aplicación de algunas reglas básicas para construir tablas “amigables para el usuario”. Estas reglas incluyen: Para títulos y otros encabezados, LAS CADENAS DE MAYÚSCULAS SON DIFÍCILES DE LEER, mantenlas al mínimo. Reduzca el tamaño físico de una tabla para que la vista no tenga que recorrer tanto como de otro modo; use un espaciado sencillo y reduzca el tamaño de la fuente. Use columnas para las cifras a comparar en lugar de filas; las columnas son más fáciles de comparar, aunque esto hace que los documentos sean más largos. Use promedios y totales de filas y columnas para proporcionar enfoque. Esto permite a los lectores hacer comparaciones. Cuando sea posible, ordene las filas y/o columnas por tamaño para facilitar las comparaciones. En general, ordenar alfabéticamente las categorías hace poco para entender conjuntos de datos complejos. Use combinaciones de espaciado y reglas horizontales y verticales para facilitar las comparaciones. Las reglas horizontales son útiles para separar categorías principales; las reglas verticales deben usarse con moderación. Los espacios en blanco entre columnas sirven para separar categorías; los pares de columnas estrechamente espaciados fomentan la comparación. Use sombreado y diferentes tamaños y atributos de fuente para resaltar cifras. El uso de sombreado también es efectivo para romper la apariencia monótona de una tabla grande. La primera vez que se muestren los datos, proporcione la fuente. Gráficos Para representar conjuntos de datos grandes y complejos, o datos donde los valores numéricos reales son menos importantes que las relaciones que se deben establecer, las representaciones gráficas de los datos son útiles. La Figura 20.1 describe algunos de los elementos básicos de un gráfico, también conocido como diagrama, ilustración o figura. Estos incluyen: Figura 20.1: Gráfico de series temporales de los retornos de Lincoln National Corporation y del mercado. Contiene 60 retornos mensuales durante el período de enero de 1986 a diciembre de 1990. Título y Leyenda. Al igual que en una tabla, estos proporcionan breves descripciones de las características principales de la figura. Se pueden utilizar leyendas largas para describir todo lo que se está graficando, llamar la atención sobre las características importantes y describir las conclusiones que se deben extraer de los datos. Incluya la fuente de los datos aquí o en una línea separada inmediatamente debajo del gráfico. Líneas de escala (Ejes) y Etiquetas de escala. Elija las escalas de manera que los datos llenen la mayor parte posible de la región de datos. No insista en que se incluya el cero; suponga que el espectador observará el rango de las escalas y las comprenderá. Marcas de escala y Etiquetas de marcas de escala. Elija el rango de las marcas de escala para incluir casi todos los datos. Generalmente, de tres a diez marcas de escala son suficientes. Cuando sea posible, coloque las marcas fuera de la región de datos, para que no interfieran con los datos. Símbolos de graficado. Use diferentes símbolos de graficado para codificar distintos niveles de una variable. Los símbolos de graficado deben ser fáciles de identificar, por ejemplo, “O” para uno y “T” para dos. Sin embargo, asegúrese de que los símbolos sean fáciles de distinguir; por ejemplo, puede ser difícil diferenciar “F” y “E”. Leyenda (Claves). Son pequeñas visualizaciones textuales que ayudan a identificar ciertos aspectos de los datos. No permita que estas visualizaciones interfieran con los datos o saturen el gráfico. Al igual que las tablas, los gráficos están separados del cuerpo principal del texto y, por lo tanto, deben ser autosuficientes. Especialmente en documentos largos, las tablas y gráficos pueden contener una línea narrativa separada, proporcionando una visión del mensaje principal del documento de una manera diferente al cuerpo principal del texto. Cleveland (1994) y Tufte (1990) ofrecen varios consejos para hacer que los gráficos sean más “amigables para el usuario”. Haga las líneas tan delgadas como sea posible. Las líneas delgadas distraen menos la vista de los datos en comparación con las líneas más gruesas. Sin embargo, haga las líneas lo suficientemente gruesas para que la imagen no se degrade al reproducirla. Intente usar la menor cantidad de líneas posible. Nuevamente, varias líneas distraen la vista de los datos, que contienen la información. Evite usar líneas de “cuadrícula” si es posible. Si debe usar líneas de cuadrícula, elija una tinta clara, como un gris o medio tono. Escriba las palabras completas y evite abreviaturas. Raramente vale la pena el espacio ahorrado frente a la confusión potencial que la versión abreviada puede causar al espectador. Use un tipo de letra que incluya letras mayúsculas y minúsculas. Coloque los gráficos en la misma página que el texto que discute el gráfico. Haga que las palabras corran de izquierda a derecha, no en forma vertical. Use la sustancia de los datos para sugerir la forma y el tamaño del gráfico. Para gráficos de series temporales, haga el gráfico dos veces más ancho que alto. Para diagramas de dispersión, haga el gráfico tan ancho como alto. Si un gráfico muestra un mensaje importante, hágalo grande. Por supuesto, para la mayoría de los gráficos será imposible seguir simultáneamente todos estos consejos. Para ilustrar, si escribimos la etiqueta de la escala en un eje vertical izquierdo y la hacemos correr de izquierda a derecha, entonces invadimos la escala vertical. Esto nos obliga a reducir el tamaño del gráfico, posiblemente a expensas de reducir el mensaje. Un gráfico es una herramienta poderosa para resumir y presentar información numérica. Los gráficos pueden romper la monotonía de documentos largos; pueden provocar y mantener el interés del lector. Además, los gráficos pueden revelar aspectos de los datos que otros métodos no pueden. 20.3 Cómo Organizar Los expertos en redacción coinciden en que los resultados deben presentarse de manera organizada con un flujo lógico, aunque no existe consenso sobre cómo lograr este objetivo. Toda historia tiene un principio y un final, generalmente con un camino interesante que conecta los dos extremos. Hay muchos tipos de caminos, o métodos de desarrollo, que conectan el principio y el final. Para la redacción técnica general, el método de desarrollo puede organizarse cronológicamente, espacialmente, por orden de importancia, de lo general a lo específico o de lo específico a lo general, por causa y efecto o por cualquier otro desarrollo lógico de los temas. Esta sección presenta un método de organización para la redacción de informes estadísticos que ha demostrado ser efectivo en diversas circunstancias, incluido el informe de 10 a 20 páginas descrito anteriormente. Este formato, aunque no es apropiado para todas las situaciones, sirve como una estructura útil sobre la cual basar su primer informe estadístico. El esquema general del formato recomendado es: Título y Resumen Introducción Características de los Datos Selección e Interpretación del Modelo Resumen y Comentarios Finales Referencias y Apéndice Las secciones (1) y (2) sirven como material introductorio para orientar al lector. Las secciones (3) y (4) forman el cuerpo principal del informe, mientras que las secciones (5) y (6) constituyen las partes finales. Título y Resumen Si su informe se distribuye ampliamente (como espera), aquí hay una noticia desalentadora: la mayoría de su audiencia prevista no pasará del título y el resumen. Incluso para los lectores que lean su informe cuidadosamente, lo que generalmente recordarán serán las impresiones dejadas por el título y el resumen, a menos que sean expertos en el tema (lo que la mayoría de los lectores no serán). Elija cuidadosamente el título de su informe. Debe ser conciso y directo. No incluya expresiones innecesarias (como Un Estudio de, Un Análisis de), pero tampoco sea demasiado breve, por ejemplo, usando títulos de una sola palabra. Además de ser conciso, el título debe ser comprensible, completo y correcto. El resumen es un breve resumen de uno o dos párrafos sobre su investigación; entre 75 y 200 palabras es una guía razonable. El lenguaje debe ser no técnico, ya que está tratando de llegar a la audiencia más amplia posible. Esta sección debe resumir los principales hallazgos de su informe. Asegúrese de responder preguntas como: ¿Qué problema se estudió? ¿Cómo se estudió? ¿Cuáles fueron los hallazgos? Debido a que está resumiendo no solo sus resultados sino también su informe, generalmente es más eficiente escribir esta sección al final. Introducción Al igual que el informe general, la introducción debe dividirse en tres secciones: material de orientación, aspectos clave del informe y un plan del documento. Para comenzar el material de orientación, reintroduzca el problema al nivel de tecnicidad que desea usar en el informe. Puede ser más o menos técnico que la declaración del problema en el resumen. La introducción establece el ritmo o la velocidad a la que se introducen nuevas ideas en el informe. A lo largo del informe, mantenga un ritmo consistente. Para identificar claramente la naturaleza del problema, en algunos casos, es apropiado incluir una breve revisión de la literatura. La revisión de la literatura cita otros informes que brindan perspectivas sobre aspectos relacionados con el mismo problema. Esto ayuda a cristalizar las características nuevas de su informe. Como parte de los aspectos clave del informe, identifique la fuente y la naturaleza de los datos utilizados en su estudio. Asegúrese de que sea evidente cómo su conjunto de datos puede abordar el problema planteado. Dé una indicación de la clase de técnicas de modelado que pretende utilizar. ¿Es claro el propósito detrás de esta selección de modelo (por ejemplo, comprensión versus pronóstico)? En este punto, las cosas pueden volverse un poco complejas para muchos lectores. Es una buena idea proporcionar un esquema del resto del informe al final de la introducción. Esto sirve como un mapa para guiar al lector a través de los argumentos complejos del informe. Además, muchos lectores estarán interesados solo en aspectos específicos del informe y, con el esquema, podrán “avanzar rápidamente” a las secciones que más les interesen. Características de los Datos En un proyecto de análisis de datos, el objetivo es resumir los datos y usar esta información resumida para hacer inferencias sobre el estado del mundo. Gran parte de este resumen se realiza a través de estadísticas que se utilizan para estimar parámetros del modelo. Sin embargo, también es útil describir los datos sin hacer referencia a un modelo específico por al menos dos razones. Primero, al usar medidas básicas de resumen de los datos, puede llegar a una audiencia más amplia que si hubiera limitado sus consideraciones a un modelo estadístico específico. De hecho, con un dispositivo gráfico de resumen cuidadosamente construido, debería poder llegar prácticamente a cualquier lector interesado en el material. Por el contrario, la familiaridad con los modelos estadísticos requiere cierta sofisticación matemática, y es posible que desee o no restringir su audiencia en esta etapa del informe. Segundo, construir estadísticas que estén directamente relacionadas con modelos específicos lo expone a críticas si la selección de su modelo es incorrecta. En la mayoría de los informes, la selección de un modelo es un paso inevitable en el proceso de inferencia, pero no es necesario hacerlo en esta etapa relativamente temprana de su informe. En la sección de características de los datos, identifique la naturaleza de los datos. Por ejemplo, asegúrese de identificar las variables componentes y de indicar si los datos son longitudinales versus transversales, observacionales versus experimentales, etc. Presente cualquier estadística básica de resumen que ayude al lector a desarrollar una comprensión general de los datos. Es una buena idea incluir alrededor de dos gráficos. Use diagramas de dispersión para resaltar relaciones primarias en datos transversales y gráficos de series temporales para indicar las tendencias longitudinales más importantes. Los gráficos y las estadísticas de resumen concomitantes no solo deben destacar las relaciones más importantes, sino que también pueden servir para identificar puntos inusuales que merecen consideración especial. Elija cuidadosamente las estadísticas y resúmenes gráficos que presenta en esta sección. No abrume al lector con una multitud de números. Los detalles presentados en esta sección deben anticipar el desarrollo del modelo en la sección siguiente. Otros aspectos destacados de los datos pueden aparecer en el apéndice. Selección e Interpretación del Modelo Esta es la esencia de su informe. Los resultados reportados en esta sección generalmente tomaron más tiempo en lograrse. Sin embargo, la extensión de la sección no necesita ser proporcional al tiempo que le tomó realizar el análisis. Recuerde, está tratando de evitar a los lectores las dificultades que usted atravesó para llegar a sus conclusiones. Sin embargo, al mismo tiempo, desea convencer a los lectores de la solidez de sus recomendaciones. Aquí hay un esquema para la sección de Selección e Interpretación del Modelo que incorpora los elementos clave que deberían aparecer: Un esquema de la sección Una declaración del modelo recomendado Una interpretación del modelo, las estimaciones de los parámetros y cualquier implicación general del modelo Las justificaciones básicas del modelo Un esquema del proceso de pensamiento que llevó a este modelo Una discusión de modelos alternativos En esta sección, desarrolle sus ideas discutiendo primero los temas generales y los detalles específicos más tarde. Use las subsecciones (1)-(3) para abordar las preocupaciones generales y amplias que un gerente o cliente no técnico podría tener. Los detalles adicionales pueden proporcionarse en las subsecciones (4)-(6) para abordar las inquietudes de los lectores con inclinaciones técnicas. De esta manera, el esquema está diseñado para satisfacer las necesidades de estos dos tipos de lectores. A continuación, se describen con más detalle cada una de las subsecciones. Usted nuevamente enfrenta los objetivos en conflicto de querer alcanzar a una audiencia lo más amplia posible y, al mismo tiempo, abordar las preocupaciones de los revisores técnicos. Comience esta sección tan importante con un esquema de los temas a tratar. Esto permitirá al lector seleccionar lo que le interese. De hecho, muchos lectores solo querrán examinar su modelo recomendado y las interpretaciones correspondientes, y asumirán que sus justificaciones son confiables. Por lo tanto, después de proporcionar el esquema, brinde inmediatamente una declaración del modelo recomendado sin ambigüedades. Ahora bien, puede no ser claro a partir del conjunto de datos que su modelo recomendado sea superior a otros modelos alternativos y, si ese es el caso, simplemente dígalo. Sin embargo, asegúrese de expresar, sin ambigüedades, lo que consideró mejor. No permita que la confusión que surge de varios modelos en competencia que representan igualmente bien los datos se deslice en su declaración de un modelo. La declaración de un modelo suele hacerse en terminología estadística, un lenguaje utilizado para expresar ideas del modelo con precisión. Siga inmediatamente la declaración del modelo recomendado con las interpretaciones correspondientes. Las interpretaciones deben realizarse en un lenguaje no técnico. Además de discutir la forma general del modelo, las estimaciones de los parámetros pueden proporcionar una indicación de la fuerza de las relaciones que ha descubierto. A menudo, un modelo se asimila fácilmente por el lector cuando se discute en términos de las implicaciones resultantes, como un intervalo de confianza o de predicción. Aunque es solo un aspecto del modelo, una sola implicación puede ser importante para muchos lectores. Es una buena idea discutir brevemente algunas de las justificaciones técnicas del modelo en el cuerpo principal del informe. Esto es para convencer al lector de que sabe lo que está haciendo. Por lo tanto, para defender su selección de un modelo, cite algunas de las justificaciones básicas, como estadísticas t, coeficiente de determinación, desviación estándar residual, etc., en el cuerpo principal e incluya argumentos más detallados en el apéndice. Para convencer aún más al lector de que ha reflexionado seriamente sobre el problema, incluya una breve descripción de un proceso de pensamiento que lleve desde los datos hasta su modelo propuesto. No describa al lector todos los problemas que encontró en el camino. Describa, en cambio, un proceso limpio que vincule el modelo con los datos, con la menor complicación posible. Como se mencionó, en el análisis de datos rara vez hay una “respuesta correcta”. Para convencer al lector de que ha reflexionado profundamente sobre el problema, es una buena idea mencionar modelos alternativos. Esto demostrará que consideró el problema desde más de una perspectiva y que es consciente de que individuos cuidadosos y reflexivos pueden llegar a diferentes conclusiones. Sin embargo, al final, aún necesita dar su modelo recomendado y respaldar su recomendación. Agudizará sus argumentos al discutir un competidor cercano y compararlo con su modelo recomendado. Resumen y Comentarios Finales Esta sección debe recapitular los resultados del informe de manera concisa, usando palabras diferentes a las del resumen. El lenguaje puede ser más o menos técnico que el del resumen, dependiendo del tono que estableció en la introducción. Refiérase a las preguntas clave planteadas al inicio del estudio y relacione estas con los resultados. Esta sección puede reflexionar sobre el análisis y servir como punto de partida para preguntas y sugerencias sobre investigaciones futuras. Incluya ideas que tenga sobre futuras investigaciones, teniendo en cuenta los costos y otras consideraciones que puedan estar involucradas en la recopilación de más información. Referencias y Apéndice El apéndice puede contener muchas figuras auxiliares y análisis. El lector no le dará al apéndice el mismo nivel de atención que al cuerpo principal del informe. Sin embargo, el apéndice es un lugar útil para incluir muchos detalles cruciales para el lector técnicamente inclinado y características importantes que no son críticas para las principales recomendaciones de su informe. Debido a que el contenido técnico aquí generalmente es más alto que en el cuerpo principal del informe, es importante que cada parte del apéndice esté claramente identificada, especialmente en relación con el cuerpo principal del informe. 20.4 Sugerencias Adicionales para la Redacción de Informes Sea lo más breve posible, pero incluya todos los detalles importantes. Por un lado, los aspectos clave de varios resultados de regresión a menudo pueden resumirse en una tabla. A menudo, varios gráficos pueden resumirse en una sola oración. Por otro lado, reconozca el valor de un gráfico o tabla bien construidos para transmitir información importante. Tenga en cuenta a su audiencia al redactar su informe. Explique lo que ahora comprende sobre el problema, con poco énfasis en cómo llegó allí. Brinde interpretaciones prácticas de los resultados, en un lenguaje con el que el cliente se sienta cómodo. Esquema, esquema. Desarrolle sus ideas de manera lógica, paso a paso. Es vital que haya un flujo lógico en el informe. Comience con un esquema amplio que especifique la estructura básica del informe. Luego haga un esquema más detallado, enumerando cada tema que desea discutir en cada sección. Solo mantendrá la libertad literaria imponiendo estructura a su informe. Simplicidad, simplicidad, simplicidad. Enfatice sus ideas principales mediante un lenguaje simple. Sustituya palabras complejas por palabras más simples si el significado sigue siendo el mismo. Evite el uso de clichés y lenguaje trillado. Aunque se puede usar lenguaje técnico, evite el uso de jerga técnica o coloquial. La jerga estadística, como “Sea \\(x_1, x_2, \\ldots\\) i.i.d. variables aleatorias…”, rara vez es necesaria. Limite el uso de frases en latín (por ejemplo, i.e.) si una frase en inglés o español puede ser suficiente (como “es decir”). Incluya tablas y gráficos importantes de resumen en el cuerpo del informe. Etiquete todas las figuras y tablas para que sean comprensibles cuando se vean por sí solas. Use uno o más apéndices para proporcionar detalles de apoyo. Los gráficos de importancia secundaria, como gráficos de residuos, y los resultados del software estadístico, como ajustes de regresión, pueden incluirse en un apéndice. Incluya suficientes detalles para que otro analista, con acceso a los datos, pueda replicar su trabajo. Proporcione un vínculo sólido entre las ideas principales descritas en el cuerpo principal del informe y el material de apoyo en el apéndice. 20.5 Estudio de Caso: Reclamos de Automóviles en Suecia Determinantes de los Reclamos de Automóviles en Suecia Resumen La tarificación de automóviles depende de la capacidad de un actuario para estimar la probabilidad de un reclamo y, en caso de que ocurra, el monto probable. Este estudio examina un conjunto de datos clásico de seguros de automóviles de terceros en Suecia. Se ajustaron modelos de regresión Poisson y gamma para las porciones de frecuencia y severidad, respectivamente. Se demuestra que la distancia recorrida por un vehículo, el área geográfica, la experiencia reciente del conductor en reclamos y el tipo de automóvil son determinantes importantes de la frecuencia de reclamos. Solo el área geográfica y el tipo de automóvil resultan ser determinantes importantes de la severidad de los reclamos. Aunque la experiencia es antigua, las técnicas utilizadas y la importancia de estos determinantes brindan ideas útiles sobre la experiencia actual. Sección 1. Introducción Los actuarios buscan establecer primas que sean justas para los consumidores en el sentido de que cada asegurado pague según sus propios reclamos esperados. Estos reclamos esperados se basan en las características del asegurado, que pueden incluir edad, género y experiencia de conducción. La motivación detrás de este principio de tarificación no es completamente altruista; un actuario entiende que una tarificación incorrecta puede llevar a graves consecuencias financieras adversas para la aseguradora. Por ejemplo, si las tarifas son demasiado altas en relación con el mercado, entonces la empresa probablemente no obtendrá una cuota de mercado suficiente. Por el contrario, si las tarifas son demasiado bajas en relación con la experiencia real, las primas recibidas probablemente no cubrirán los reclamos y los gastos relacionados. Establecer tarifas adecuadas es importante en el seguro de automóviles, que indemniza a los asegurados y a otras partes en caso de un accidente automovilístico. Para una cobertura a corto plazo como el seguro de automóviles, los reclamos resultantes de las pólizas se realizan rápidamente y el actuario puede calibrar la fórmula de tarificación a la experiencia real. Para muchos analistas, los datos sobre reclamos de seguros pueden ser difíciles de obtener. Las aseguradoras desean proteger la privacidad de sus clientes y, por lo tanto, no quieren compartir datos. Para algunas aseguradoras, los datos no están almacenados en un formato electrónico conveniente para análisis estadísticos; puede ser costoso acceder a los datos incluso si están disponibles para la aseguradora. Quizás lo más importante es que las aseguradoras son reacias a liberar datos al público porque temen divulgar información propietaria que ayude a sus competidores en guerras de precios intensas. Debido a esta falta de datos actualizados sobre automóviles, este estudio examina un conjunto de datos clásico sueco sobre reclamos de seguros de automóviles de terceros ocurridos en 1977. Los reclamos de terceros implican pagos a alguien que no sea el asegurado y la compañía de seguros, generalmente a alguien lesionado como resultado de un accidente automovilístico. Aunque la experiencia está desactualizada, las técnicas de regresión utilizadas en este informe funcionan igualmente bien con datos actuales. Además, los determinantes de los reclamos investigados, como el uso del vehículo y la experiencia del conductor, probablemente sigan siendo importantes en el mundo de conducción actual. El esquema del resto de este informe es el siguiente. En la Sección 2, presento las características más importantes de los datos. Para resumir estas características, en la Sección 3 se discute un modelo para representar los datos. En la Sección 4 se encuentran los comentarios finales, y muchos de los detalles del análisis están en el apéndice. Sección 2. Características de los Datos Estos datos fueron recopilados por el Comité Sueco para el Análisis de la Prima de Riesgo en el Seguro de Automóviles, resumidos en Hallin e Ingenbleek (1983) y Andrews y Herzberg (1985). Los datos son transversales y describen reclamos de seguros de automóviles de terceros para el año 1977. Los resultados de interés son el número de reclamos (la frecuencia) y la suma de los pagos (la severidad), en coronas suecas. Los resultados se basan en 5 categorías de distancia recorrida por un vehículo, desglosadas por 7 zonas geográficas, 7 categorías de experiencia reciente del conductor en reclamos (capturada por el “bono”) y 9 tipos de automóviles. Aunque hay 2,205 combinaciones potenciales de distancia, zona, experiencia y tipo (\\(5 \\times 7 \\times 7 \\times 9 = 2,205\\)), solo se realizaron \\(n=2,182\\) en el conjunto de datos de 1977. Para cada combinación, además de los resultados de interés, tenemos disponible el número de años de asegurados como medida de exposición. Un “año de asegurado” es la fracción del año en que el asegurado tiene un contrato con la compañía emisora. Explicaciones más detalladas de estas variables están disponibles en el Apéndice A2. En estos datos, hubo 113,171 reclamos de 2,383,170 años de asegurados, con una tasa de reclamos del 4.75%. De estos reclamos, se pagaron un total de 560,790,681 coronas, con un promedio de 4,955 por reclamo. Como referencia, en junio de 1977, una corona sueca podía intercambiarse por 0.2267 dólares estadounidenses. Tabla 20.2 proporciona más detalles sobre los resultados de interés. Esta tabla está organizada por las \\(n=2,182\\) combinaciones de distancia, zona, experiencia y tipo. Por ejemplo, la combinación con la mayor exposición (127,687.27 años de asegurados) proviene de aquellos que conducen una cantidad mínima en áreas rurales del sur de Suecia, con al menos seis años sin accidentes y que conducen un automóvil que no es uno de los ocho tipos básicos (Kilómetros=1, Zona=4, Bono=7 y Marca=9, ver Apéndice A2). Esta combinación tuvo 2,894 reclamos con pagos de 15,540,162 coronas. Además, observo que hubo 385 combinaciones que no tuvieron reclamos. Tabla 20.2. Estadísticas Resumidas de Automóviles en Suecia \\[ \\small{ \\begin{array}{lrrrrr} \\hline &amp; &amp; &amp; \\text{Desviación} &amp; &amp; \\\\ &amp; \\text{Media} &amp; \\text{Mediana} &amp; \\text{Estándar} &amp;\\text{ Mínimo} &amp; \\text{Máximo} \\\\ \\hline \\text{Años de Asegurados } &amp; 1,092.20 &amp; 81.53 &amp; 5,661.16 &amp; 0.01 &amp; 127,687.27 \\\\ \\text{Reclamos} &amp; 51.87 &amp; 5.00 &amp; 201.71 &amp; 0.00 &amp; 3,338.00 \\\\ \\text{Pagos} &amp; 257,008&amp; 27,404 &amp; 1,017,283 &amp; 0 &amp; 18,245,026 \\\\ \\text{Promedio de Reclamos} &amp; 0.069 &amp; 0.051 &amp; 0.086 &amp; 0.000 &amp; 1.667 \\\\ ~~~ \\text{(por Año de Asegurado)} \\\\ \\text{Promedio de Pago} &amp; 5,206.05 &amp; 4,375.00 &amp; 4,524.56 &amp; 72.00 &amp; 31,442.00 \\\\ ~~~ \\text{(por Reclamo)} &amp; \\\\ \\hline \\end{array} } \\] Nota: Las distribuciones se basan en \\(n=2,182\\) combinaciones de distancia, zona, experiencia y tipo. Fuente: Hallin e Ingenbleek (1983) Código R para Generar la Tabla 20.2 # Tabla 20.2 Swedish &lt;- read.csv(&quot;CSVData/SwedishMotorInsurance.csv&quot;, header=TRUE) # Crear otras variables: Swedish$AveClmNumber &lt;- Swedish$Claims/Swedish$Insured Swedish$AveSeverity &lt;- Swedish$Payment/Swedish$Claims # Resumen estadístico de la tabla: Xymat &lt;- data.frame(cbind(Swedish$Insured,Swedish$Claims,Swedish$Payment, Swedish$AveClmNumber,Swedish$AveSeverity)) meanSummary &lt;- sapply(Xymat, mean, na.rm=TRUE) sdSummary &lt;- sapply(Xymat, sd, na.rm=TRUE) minSummary &lt;- sapply(Xymat, min, na.rm=TRUE) maxSummary &lt;- sapply(Xymat, max, na.rm=TRUE) medSummary &lt;- sapply(Xymat, median,na.rm=TRUE) Tab203 &lt;- cbind(meanSummary, medSummary, sdSummary, minSummary, maxSummary) knitr::kable(Tab203, digits = 2) meanSummary medSummary sdSummary minSummary maxSummary X1 1092.20 81.53 5661.16 0.01 127687.27 X2 51.87 5.00 201.71 0.00 3338.00 X3 257007.64 27403.50 1017282.59 0.00 18245026.00 X4 0.07 0.05 0.09 0.00 1.67 X5 5206.05 4375.00 4524.56 72.00 31442.00 Tabla 20.2 también muestra la distribución del promedio de reclamos por asegurado. No es sorprendente que el mayor promedio de reclamos ocurriera en una combinación donde solo hubo un reclamo con un pequeño número (0.6) de años de asegurados. Dado que utilizaremos los años de asegurados como un peso en nuestro análisis de la Sección 3, este tipo de comportamiento aberrante se ponderará automáticamente hacia abajo y, por lo tanto, no se requieren técnicas especiales para manejarlo. Para el pago promedio más alto, resulta que hay 27 combinaciones con un solo reclamo de 31,442 (y una combinación con dos reclamos de 31,442). Esto aparentemente representa algún tipo de límite de póliza impuesto del cual no tenemos documentación. Ignoraré esta característica en el análisis. La Figura 20.2 muestra las relaciones entre los resultados de interés y las bases de exposición. Para el número de reclamos, utilizamos los años de asegurados como la base de exposición. Es claro que el número de reclamos de seguros aumenta con la exposición. Además, los montos de los pagos aumentan con el número de reclamos de manera muy lineal. Figura 20.2: Gráficos de Dispersión de Reclamos versus Años de Asegurados y Pagos versus Reclamos. Código R para Generar la Figura 20.2 # Figura 202 # Datos de Seguro de Automóviles Sueco #Swedish &lt;- read.csv(&quot;CSVData/SwedishMotorInsurance.csv&quot;, header=TRUE) #Swedish &lt;- read.csv(&quot;../../CSVData/SwedishMotorInsurance.csv&quot;, header=TRUE) par(mfrow=c(1, 2)) plot(Swedish$Insured, Swedish$Claims, xlab=&quot;Años de Asegurados&quot;, ylab=&quot;Reclamos&quot;) plot(Swedish$Claims, Swedish$Payment, xlab=&quot;Reclamos&quot;, ylab = &quot;Pagos&quot;) Para entender los efectos de las variables explicativas sobre la frecuencia, la Figura 20.3 presenta diagramas de caja del promedio de reclamos por asegurado versus cada variable de clasificación. Para visualizar las relaciones, se han omitido tres combinaciones donde el promedio de reclamos supera 1.0. Esta figura muestra frecuencias más bajas asociadas con distancias de conducción más cortas, ubicaciones no urbanas y un mayor número de años sin accidentes. El tipo de automóvil también parece tener un impacto significativo en la frecuencia de reclamos. Figura 20.3: Diagramas de Caja de Frecuencia por Distancia Conducida, Zona Geográfica, Años sin Accidentes y Tipo de Automóvil Código R para Generar la Figura 20.3 # Figura 203 Swedish$Bonus1 = Swedish$Bonus - 1 # PARA GRÁFICOS DE FRECUENCIA, ELIMINAR RECLAMOS GRANDES POR ASEGURADO Swedish1 &lt;- subset(Swedish, Claims/Insured &lt; 1 ) par(mfrow=c(2, 2)) boxplot(Claims/Insured ~ Kilometres, data=Swedish1, ylab=&quot;Promedio de Reclamos&quot;,xlab=&quot;Distancia Conducida&quot;) boxplot(Claims/Insured ~ Zone, data=Swedish1, ylab=&quot;Promedio de Reclamos&quot;,xlab=&quot;Zona Geográfica&quot;) boxplot(Claims/Insured ~ Bonus1, data=Swedish1, ylab=&quot;Promedio de Reclamos&quot;,xlab=&quot;Años sin Accidentes&quot;) boxplot(Claims/Insured ~ Make, data=Swedish1, ylab=&quot;Promedio de Reclamos&quot;,xlab=&quot;Tipo de Automóvil&quot;) Para la severidad, la Figura 20.4 presenta diagramas de caja del pago promedio por reclamo versus cada variable de clasificación. Aquí, los efectos de las variables explicativas no son tan pronunciados como en la frecuencia. El panel superior derecho muestra que la severidad promedio es mucho menor para Zona=7. Esto corresponde a Gotland, un condado y municipio de Suecia que ocupa la isla más grande del Mar Báltico. La Figura 20.4 también sugiere cierta variación según el tipo de automóvil. Figura 20.4: Diagramas de Caja de Severidad por Distancia Conducida, Zona Geográfica, Años sin Accidentes y Tipo de Automóvil Código R para Generar la Figura 20.4 # Figura 204 # PARA GRAFICAR LA SEVERIDAD PROMEDIO, ELIMINAR CEROS Y MÁXIMOS Swedish2 &lt;- subset(Swedish, Claims &gt; 0, Payment/Claims &lt; 31000 ) Swedish2 &lt;- subset(Swedish2, Payment/Claims &lt; 31000 ) par(mfrow=c(2, 2)) boxplot(Payment/Claims ~ Kilometres, data=Swedish2, ylab=&quot;Monto Promedio de Reclamo&quot;, xlab=&quot;Distancia Conducida&quot;) boxplot(Payment/Claims ~ Zone, data=Swedish2, ylab=&quot;Monto Promedio de Reclamo&quot;, xlab=&quot;Zona Geográfica&quot;) boxplot(Payment/Claims ~ Bonus1, data=Swedish2, ylab=&quot;Monto Promedio de Reclamo&quot;, xlab=&quot;Años sin Accidentes&quot;) boxplot(Payment/Claims ~ Make, data=Swedish2, ylab=&quot;Monto Promedio de Reclamo&quot;, xlab=&quot;Tipo de Automóvil&quot;) Sección 3. Selección e Interpretación del Modelo La Sección 2 estableció que existen patrones reales entre la frecuencia y severidad de los reclamos y las variables de clasificación, a pesar de la gran variabilidad en estas variables. Esta sección resume estos patrones utilizando modelos de regresión. Tras la declaración del modelo y su interpretación, esta sección describe características de los datos que motivaron la selección del modelo recomendado. Como resultado de este estudio, recomiendo un modelo de regresión de Poisson utilizando una función de enlace logarítmica para la parte de frecuencia. El componente sistemático incluye los factores de clasificación distancia, zona, experiencia y tipo como variables categóricas aditivas, así como un término de compensación en el logaritmo del número de asegurados. Este modelo se ajustó utilizando máxima verosimilitud, con los coeficientes que aparecen en Tabla 20.3; más detalles aparecen en el Apéndice A4. Aquí, las categorías base corresponden al primer nivel de cada factor. Por ejemplo, considere un conductor que vive en Estocolmo (Zona=1), que conduce entre uno y quince mil kilómetros por año (Kilometres=2), ha tenido un accidente en el último año (Bonus=1) y maneja un automóvil del tipo “Make=6”. Entonces, de la Tabla 20.3, el componente sistemático es \\(-1.813 + 0.213 -0.336 = -1.936.\\) Para una póliza típica de esta combinación, estimaríamos un número de reclamos de Poisson con media \\(\\exp(-1.936) = 0.144.\\) Por ejemplo, la probabilidad de que no haya reclamos en un año es \\(\\exp(-0.144) = 0.866.\\) En 1977, hubo 354.4 años de asegurados en esta combinación, para un número esperado de reclamos de \\(354.4 \\times 0.144 = 51.03.\\) Resultó que solo hubo 48 reclamos en esta combinación en 1977. Table 20.3. Ajuste del Modelo de Regresión de Poisson \\[ \\small{ \\begin{array}{lrrlrr} \\hline \\text{Variable} &amp; \\text{Coeficiente} &amp; t-\\text{ratio} &amp; \\text{Variable} &amp; \\text{Coeficiente} &amp; t-\\text{ratio} \\\\ \\hline \\text{Intercepto} &amp; -1.813 &amp; -131.78 &amp; \\text{Bonus}=2 &amp; -0.479 &amp; -39.61 \\\\ \\text{Kilometres}=2 &amp; 0.213 &amp; 28.25 &amp; \\text{Bonus}=3 &amp; -0.693 &amp; -51.32 \\\\ \\text{Kilometres}=3 &amp; 0.320 &amp; 36.97 &amp; \\text{Bonus}=4 &amp; -0.827 &amp; -56.73 \\\\ \\text{Kilometres}=4 &amp; 0.405 &amp; 33.57 &amp; \\text{Bonus}=5 &amp; -0.926 &amp; -66.27 \\\\ \\text{Kilometres}=5 &amp; 0.576 &amp; 44.89 &amp; \\text{Bonus}=6 &amp; -0.993 &amp; -85.43 \\\\ \\text{Zone}=2 &amp; -0.238 &amp; -25.08 &amp; \\text{Bonus}=7 &amp; -1.327 &amp; -152.84 \\\\ \\text{Zone}=3 &amp; -0.386 &amp; -39.96 &amp; \\text{Make}=2 &amp; 0.076 &amp; 3.59 \\\\ \\text{Zone}=4 &amp; -0.582 &amp; -67.24 &amp; \\text{Make}=3 &amp; -0.247 &amp; -9.86 \\\\ \\text{Zone}=5 &amp; -0.326 &amp; -22.45 &amp; \\text{Make}=4 &amp; -0.654 &amp; -27.02 \\\\ \\text{Zone}=6 &amp; -0.526 &amp; -44.31 &amp; \\text{Make}=5 &amp; 0.155 &amp; 7.66 \\\\ \\text{Zone}=7 &amp; -0.731 &amp; -17.96 &amp; \\text{Make}=6 &amp; -0.336 &amp; -19.31 \\\\ &amp; &amp; &amp; \\text{Make}=7 &amp; -0.056 &amp; -2.40 \\\\ &amp; &amp; &amp; \\text{Make}=8 &amp; -0.044 &amp; -1.39 \\\\ &amp; &amp; &amp; \\text{Make}=9 &amp; -0.068 &amp; -6.84 \\\\ \\hline \\end{array} } \\] Código R para Generar la Tabla 20.3 # Tabla 20.3 #Swedish &lt;- read.csv(&quot;CSVData/SwedishMotorInsurance.csv&quot;, header=TRUE) #Swedish &lt;- read.csv(&quot;../../CSVData/SwedishMotorInsurance.csv&quot;, header=TRUE) #Swedish$AveClmNumber &lt;- Swedish$Claims/Swedish$Insured #Swedish$AveSeverity &lt;- Swedish$Payment/Swedish$Claims CountPoisson2 &lt;- glm(Claims ~ factor(Kilometres) + factor(Zone) + factor(Bonus) + factor(Make), offset=log(Insured),poisson(link=log), data = Swedish) library(modelsummary) modelsummary(CountPoisson2, output = &quot;html&quot;, statistic = &quot;statistic&quot;) /* tinytable css entries after */ .table td.tinytable_css_zybjbajf4pr69hwhah7x, .table th.tinytable_css_zybjbajf4pr69hwhah7x { text-align: center; } .table td.tinytable_css_qy1b2gn3ibi2uocowznq, .table th.tinytable_css_qy1b2gn3ibi2uocowznq { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_o4z54jhzzwfnae934atv, .table th.tinytable_css_o4z54jhzzwfnae934atv { text-align: left; } .table td.tinytable_css_mxait879x43nrtjb39zo, .table th.tinytable_css_mxait879x43nrtjb39zo { text-align: left; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_jp3qdk2niqhdc3s84p61, .table th.tinytable_css_jp3qdk2niqhdc3s84p61 { text-align: center; border-bottom: solid black 0.05em; } .table td.tinytable_css_hbdfwhb41a9aas38mwq7, .table th.tinytable_css_hbdfwhb41a9aas38mwq7 { text-align: center; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_94tl0486eatemv9k0ysa, .table th.tinytable_css_94tl0486eatemv9k0ysa { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_4xm49cp2v4ws37wefjyc, .table th.tinytable_css_4xm49cp2v4ws37wefjyc { text-align: left; border-bottom: solid black 0.05em; } (1) (Intercept) −1.813 (−131.775) factor(Kilometres)2 0.213 (28.255) factor(Kilometres)3 0.320 (36.974) factor(Kilometres)4 0.405 (33.571) factor(Kilometres)5 0.576 (44.892) factor(Zone)2 −0.238 (−25.082) factor(Zone)3 −0.386 (−39.959) factor(Zone)4 −0.582 (−67.243) factor(Zone)5 −0.326 (−22.446) factor(Zone)6 −0.526 (−44.309) factor(Zone)7 −0.731 (−17.962) factor(Bonus)2 −0.479 (−39.607) factor(Bonus)3 −0.693 (−51.316) factor(Bonus)4 −0.827 (−56.735) factor(Bonus)5 −0.926 (−66.269) factor(Bonus)6 −0.993 (−85.429) factor(Bonus)7 −1.327 (−152.845) factor(Make)2 0.076 (3.590) factor(Make)3 −0.247 (−9.859) factor(Make)4 −0.654 (−27.022) factor(Make)5 0.155 (7.656) factor(Make)6 −0.336 (−19.314) factor(Make)7 −0.056 (−2.396) factor(Make)8 −0.044 (−1.390) factor(Make)9 −0.068 (−6.836) Num.Obs. 2182 AIC 10654.0 BIC 10796.2 Log.Lik. −5301.998 F 1421.846 RMSE 13.35 Para la parte de severidad, recomiendo un modelo de regresión gamma utilizando una función de enlace logarítmica. El componente sistemático incluye los factores de clasificación zona y tipo como variables categóricas aditivas, así como un término de compensación en el logaritmo del número de reclamos. Además, se utilizó la raíz cuadrada del número de reclamos como variable de ponderación para dar mayor peso a aquellas combinaciones con mayor número de reclamos. Este modelo se ajustó utilizando máxima verosimilitud, con los coeficientes que aparecen en Tabla 20.4; más detalles aparecen en el Apéndice A6. Considere nuevamente nuestro conductor ilustrativo que vive en Estocolmo (Zone=1), conduce entre uno y quince mil kilómetros por año (=2), ha tenido un accidente en el último año (Bonus=1) y maneja un automóvil del tipo “Make=6”. Para esta persona, el componente sistemático es \\(8.388 + 0.108 = 8.496.\\) Por lo tanto, los reclamos esperados bajo el modelo son \\(\\exp(8.496) = 4,895.\\) En comparación, el pago promedio en 1977 fue de 3,467 para esta combinación y 4,955 por reclamo para todas las combinaciones. Table 20.4. Ajuste del Modelo de Regresión Gamma \\[ \\small{ \\begin{array}{lrrlrr} \\hline \\text{Variable} &amp; \\text{Coeficiente} &amp; t-\\text{ratio} &amp; \\text{Variable} &amp; \\text{Coeficiente} &amp; t-\\text{ratio} \\\\ \\hline Intercepto &amp; 8.388 &amp; 76.72 &amp; \\text{Make}=2 &amp; -0.050 &amp; -0.44 \\\\ \\text{Zone}=2 &amp; -0.061 &amp; -0.64 &amp; \\text{Make}=3 &amp; 0.253 &amp; 2.22 \\\\ \\text{Zone}=3 &amp; 0.153 &amp; 1.60 &amp; \\text{Make}=4 &amp; 0.049 &amp; 0.43 \\\\ \\text{Zone}=4 &amp; 0.092 &amp; 0.94 &amp; \\text{Make}=5 &amp; 0.097 &amp; 0.85 \\\\ \\text{Zone}=5 &amp; 0.197 &amp; 2.12 &amp; \\text{Make}=6 &amp; 0.108 &amp; 0.92 \\\\ \\text{Zone}=6 &amp; 0.242 &amp; 2.58 &amp; \\text{Make}=7 &amp; -0.020 &amp; -0.18 \\\\ \\text{Zone}=7 &amp; 0.106 &amp; 0.98 &amp; \\text{Make}=8 &amp; 0.326 &amp; 2.90 \\\\ &amp; &amp; &amp; \\text{Make}=9 &amp; -0.064 &amp; -0.42 \\\\ Dispersión &amp; 0.483 \\\\ \\hline \\end{array} } \\] Código R para Generar la Tabla 20.4 # Tabla 20.4 #Swedish &lt;- read.csv(&quot;CSVData/SwedishMotorInsurance.csv&quot;, header=TRUE) #Swedish &lt;- read.csv(&quot;../../CSVData/SwedishMotorInsurance.csv&quot;, header=TRUE) #Swedish$AveClmNumber &lt;- Swedish$Claims/Swedish$Insured #Swedish$AveSeverity &lt;- Swedish$Payment/Swedish$Claims Swedish3 &lt;- subset(Swedish, Claims&gt;0 ) Swedish3$Weight=1/sqrt(Swedish3$Claims) Tab204 &lt;- glm(Payment ~ factor(Zone) + factor(Make), offset=log(Claims), weights=Weight, Gamma(link=log), data = Swedish3) modelsummary(Tab204, output = &quot;html&quot;, statistic = &quot;statistic&quot;) /* tinytable css entries after */ .table td.tinytable_css_xq5ijh2iccrqbkm8iivr, .table th.tinytable_css_xq5ijh2iccrqbkm8iivr { text-align: center; } .table td.tinytable_css_wycocxzz1g9rx47foyax, .table th.tinytable_css_wycocxzz1g9rx47foyax { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_vlt4j35pzmvcp2oo7bf7, .table th.tinytable_css_vlt4j35pzmvcp2oo7bf7 { text-align: left; } .table td.tinytable_css_fjaqowlhrzvgh88504xo, .table th.tinytable_css_fjaqowlhrzvgh88504xo { text-align: center; border-bottom: solid black 0.05em; } .table td.tinytable_css_bvhp1vilawuqxu6gomwf, .table th.tinytable_css_bvhp1vilawuqxu6gomwf { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_b6olsxdixx5a7v7pwxrr, .table th.tinytable_css_b6olsxdixx5a7v7pwxrr { text-align: left; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_3ddaakty1hjoa12umg3d, .table th.tinytable_css_3ddaakty1hjoa12umg3d { text-align: left; border-bottom: solid black 0.05em; } .table td.tinytable_css_1ud9cbdix00x7w4qvuoq, .table th.tinytable_css_1ud9cbdix00x7w4qvuoq { text-align: center; border-bottom: solid #d3d8dc 0.1em; } (1) (Intercept) 8.388 (76.722) factor(Zone)2 −0.061 (−0.641) factor(Zone)3 0.153 (1.597) factor(Zone)4 0.092 (0.943) factor(Zone)5 0.197 (2.119) factor(Zone)6 0.242 (2.581) factor(Zone)7 0.106 (0.978) factor(Make)2 −0.050 (−0.439) factor(Make)3 0.253 (2.219) factor(Make)4 0.049 (0.425) factor(Make)5 0.097 (0.852) factor(Make)6 0.108 (0.925) factor(Make)7 −0.020 (−0.180) factor(Make)8 0.326 (2.900) factor(Make)9 −0.064 (−0.423) Num.Obs. 1797 AIC 16081.9 BIC 16169.8 Log.Lik. −8024.959 F 2.879 RMSE 172182.35 Discusión del Modelo de Frecuencia Ambos modelos proporcionaron un ajuste razonable a los datos disponibles. Para la parte de frecuencia, los \\(t\\)-ratios en Tabla 20.3 asociados con cada coeficiente exceden tres en valor absoluto, indicando una fuerte significancia estadística. Además, el Apéndice A5 demuestra que cada factor categórico es estadísticamente significativo. No se identificaron otros patrones importantes entre los residuales del modelo ajustado final y las variables explicativas. La Figura A1 muestra un histograma de los residuales de desviación, indicando normalidad aproximada, una señal de que los datos son congruentes con los supuestos del modelo. Se consideraron varios modelos de frecuencia competidores. Tabla 20.5 enumera dos más: un modelo de Poisson sin covariables y un modelo binomial negativo con las mismas covariables que el modelo de Poisson recomendado. Esta tabla muestra que el modelo recomendado es el mejor entre estos tres, basado en el estadístico de bondad de ajuste de Pearson y una versión ponderada por exposición. Recuerde que el estadístico de ajuste de Pearson tiene la forma \\(\\sum (O-E)^2/E\\), comparando los valores observados (\\(O\\)) con los esperados bajo el modelo ajustado (\\(E\\)). La versión ponderada resume \\(\\sum w(O-E)^2/E\\), donde nuestras ponderaciones son años de póliza en unidades de 100,000. En cada caso, preferimos modelos con estadísticas más pequeñas. Tabla 20.5 muestra que el modelo recomendado es la elección clara entre los tres competidores. Table 20.5. Bondad de Ajuste de Pearson para Tres Modelos de Frecuencia \\[ \\small{ \\begin{array}{lrr} \\hline \\text{Modelo} &amp; \\text{Pearson} &amp; \\text{Pearson Ponderado} \\\\ \\hline \\text{Poisson sin Covariables} &amp; 44,639 &amp; 653.49 \\\\ \\text{Modelo Final de Poisson} &amp; 3,003 &amp; 6.41 \\\\ \\text{Modelo Binomial Negativo} &amp; 3,077 &amp; 9.03 \\\\ \\hline \\end{array} } \\] Código R para Generar la Tabla 20.5 # Tabla 20.5 #Swedish &lt;- read.csv(&quot;CSVData/SwedishMotorInsurance.csv&quot;, header=TRUE) #Swedish &lt;- read.csv(&quot;../../CSVData/SwedishMotorInsurance.csv&quot;, header=TRUE) CountPoisson1 &lt;- glm(Claims ~ 1, offset=log(Insured),poisson(link=log), data = Swedish) # CountPoisson2 &lt;- glm(Claims ~ factor(Kilometres) + factor(Zone) + # factor(Bonus) + factor(Make), # offset=log(Insured),poisson(link=log), data = Swedish) library(MASS) # AJUSTAR UN MODELO BINOMIAL NEGATIVO; CountNB1 &lt;- glm.nb(Claims ~ factor(Kilometres) + factor(Zone) + factor(Bonus) + factor(Make)+ offset(log(Insured)), link=log, data = Swedish) #offset=log(Insured) # HACER ESTAS ESTADÍSTICAS DE FORMA RUTINARIA PARA AHORRAR TRABAJO PearsonI &lt;- function(y){ temp &lt;- (Swedish$Claims - y) PearsonIp &lt;- sum(temp*temp/y) return(PearsonIp) } PearsonWI &lt;- function(y){ temp &lt;- (Swedish$Claims - y)*sqrt(Swedish$Insured) PearsonWIp &lt;- sum(temp*temp/y) return(PearsonWIp/1e6) } Tab205 &lt;- matrix(0, nrow = 3, ncol = 2) # COMPARACIÓN UTILIZANDO CHI-CUADRADO DE PEARSON INDIVIDUAL Tab205[1,1] &lt;- PearsonI(CountPoisson1$fitted.values) Tab205[2,1] &lt;- PearsonI(CountPoisson2$fitted.values) Tab205[3,1] &lt;- PearsonI(CountNB1$fitted.values) Tab205[1,2] &lt;- PearsonWI(CountPoisson1$fitted.values) Tab205[2,2] &lt;- PearsonWI(CountPoisson2$fitted.values) Tab205[3,2] &lt;- PearsonWI(CountNB1$fitted.values) knitr::kable(Tab205, digits = 3) 44638.510 653.495 3002.581 6.413 3077.296 9.031 En el desarrollo del modelo final, la primera decisión tomada fue utilizar la distribución de Poisson para los conteos. Esto está en concordancia con la práctica aceptada y debido a que un histograma de los números de reclamos (no mostrado aquí) presentó una distribución sesgada similar a Poisson. Las covariables mostraron características importantes que podrían afectar la frecuencia, como se muestra en la Sección 2 y el Apéndice A3. Además de los modelos de Poisson y binomial negativo, también ajusté un modelo cuasi-Poisson con un parámetro adicional para la dispersión. Aunque esto pareció ser útil, finalmente decidí no recomendar esta variación porque el objetivo de tarificación es ajustar valores esperados. Todos los factores de calificación fueron muy estadísticamente significativos con y sin el factor adicional de dispersión, por lo que el parámetro adicional solo agregó complejidad al modelo. Por lo tanto, opté por no incluir este término. Discusión del Modelo de Severidad Para el modelo de severidad, los factores categóricos zona y marca son estadísticamente significativos, como se muestra en el Apéndice A7. Aunque no se muestran aquí, los residuos de este modelo fueron adecuados. Los residuos de desviación se distribuyeron aproximadamente de manera normal. Los residuos, al ser reescalados por la raíz cuadrada del número de reclamos, fueron aproximadamente homocedásticos. No se encontraron relaciones evidentes con las variables explicativas. Este modelo complejo se especificó después de un largo examen de los datos. Basándome en las relaciones evidentes entre los pagos y el número de reclamos en la Figura 20.2, el primer paso fue examinar la distribución de los pagos por reclamo. Esta distribución estaba sesgada, por lo que se intentó ajustar los pagos logarítmicos por reclamo. Después de ajustar las variables explicativas a esta variable dependiente, los residuos del ajuste del modelo fueron heterocedásticos. Estos se ponderaron por la raíz cuadrada del número de reclamos y lograron una homocedasticidad aproximada. Desafortunadamente, como se ve en la Figura A2 del Apéndice, el ajuste sigue siendo deficiente en las colas inferiores de la distribución. Se siguió un proceso similar utilizando la distribución gamma con una función de enlace logarítmica, con los pagos como la respuesta y el logaritmo del número de reclamos como el término de ajuste. Nuevamente, se estableció la necesidad de la raíz cuadrada del número de reclamos como factor de ponderación. El proceso comenzó con las cuatro variables explicativas, pero se descartaron la distancia y los años sin accidentes debido a su falta de significancia estadística. También creé una variable binaria “Seguro” para indicar que un conductor tenía seis o más años sin accidentes (basándome en mi examen de la Figura 20.4). Sin embargo, esta no resultó ser estadísticamente significativa y no se incluyó en la especificación final del modelo. Sección 4. Resumen y Conclusiones Aunque los reclamos de seguros varían significativamente, hemos visto que es posible establecer determinantes importantes del número de reclamos y los pagos. Los modelos de regresión recomendados concluyen que los resultados de los seguros pueden explicarse en términos de la distancia conducida por un vehículo, la zona geográfica, la experiencia reciente del conductor con reclamos y el tipo de automóvil. Se desarrollaron modelos separados para la frecuencia y la severidad de los reclamos. En parte, esto se motivó por la evidencia de que menos variables parecen influir en los montos de los pagos en comparación con el número de reclamos. Este estudio se basó en 113,171 reclamos de 2,383,170 años de pólizas, por un total de 560,790,681 coronas. Este es un conjunto de datos grande que nos permite desarrollar modelos estadísticos complejos. La forma agrupada de los datos nos permite trabajar con solo \\(n=2,182\\) celdas, relativamente pequeño para los estándares actuales. Los datos no agrupados tendrían la ventaja de permitirnos considerar variables explicativas adicionales. Uno podría conjeturar sobre cualquier número de variables adicionales que podrían incluirse; la edad, el género y el descuento por buen estudiante son algunos buenos candidatos. Cabe señalar que el artículo de Hallin e Ingenbleek (1983) consideró la edad del vehículo: esta variable no se incluyó en mi base de datos porque los analistas responsables de la publicación de los datos consideraron que era un determinante insignificante de los reclamos de seguros. Además, mi análisis de los datos se basa en la experiencia de 1977 de los conductores suecos. Las lecciones aprendidas de este informe pueden o no transferirse a conductores modernos más cercanos. Sin embargo, las técnicas exploradas en este informe deberían ser inmediatamente aplicables con el conjunto adecuado de datos modernos. Apéndice Contenido del Apéndice Referencias Definiciones de Variables Estadísticas Básicas Resumidas para la Frecuencia Modelo Final Ajustado de Regresión de Frecuencia—Salida de R Comprobación de la Significancia de los Factores en el Modelo Final Ajustado de Regresión de Frecuencia — Salida de R Modelo Final Ajustado de Regresión de Severidad—Salida de R Comprobación de la Significancia de los Factores en el Modelo Final Ajustado de Regresión de Severidad — Salida de R A1. Referencias Andrews, D. F. y A. M. Herzberg (1985). Capítulo 68 en: A Collection from Many Fields for the Student and Research Worker, pp. 413-421. Springer, Nueva York. Hallin, Marc y Jean-Franois Ingenbleek (1983). The Swedish automobile portfolio in 1977: A statistical study. Scandinavian Actuarial Journal 1983: 49-64. A2. Definiciones de Variables TABLA A.1. Definiciones de Variables \\[ \\small{ \\begin{array}{ll} \\hline \\text{Nombre} &amp;\\text{Descripción} \\\\ \\hline \\text{Kilometres} &amp; \\text{Kilómetros recorridos por año} \\\\ &amp; 1: &lt; 1,000 \\\\ &amp; 2: 1,000-15,000 \\\\ &amp; 3: 15,000-20,000 \\\\ &amp; 4: 20,000-25,000 \\\\ &amp; 5: &gt; 25,000 \\\\ \\hline \\text{Zone} &amp; \\text{Zona geográfica} \\\\ &amp; \\text{1: Estocolmo, Gotemburgo, Malmö y alrededores} \\\\ &amp; \\text{2: Otras grandes ciudades y alrededores} \\\\ &amp; \\text{3: Ciudades más pequeñas y alrededores en el sur de Suecia} \\\\ &amp; \\text{4: Áreas rurales en el sur de Suecia} \\\\ &amp; \\text{5: Ciudades más pequeñas y alrededores en el norte de Suecia} \\\\ &amp; \\text{6: Áreas rurales en el norte de Suecia} \\\\ &amp; \\text{7: Gotland} \\\\ \\hline \\text{Bonus} &amp; \\text{Bono por no reclamos.} \\\\ &amp; \\text{Igual al número de años, más uno, desde el último reclamo.} \\\\ \\text{Make} &amp; \\text{1-8 representan ocho modelos de automóviles comunes.} \\\\ &amp; \\text{Todos los demás modelos se combinan en la clase 9.} \\\\ \\text{Exposure} &amp; \\text{Cantidad de años de póliza} \\\\ \\text{Claims} &amp; \\text{Número de reclamos} \\\\ \\text{Payment} &amp; \\text{Valor total de los pagos en coronas suecas} \\\\ \\hline \\end{array} } \\] A3. Estadísticas Básicas Resumidas para la Frecuencia TABLA A.2. Promedios de Reclamos por Asegurado según Factor de Clasificación Kilometre 1 2 3 4 5 0.0561 0.0651 0.0718 0.0705 0.0827 Zone 1 2 3 4 5 6 7 0.1036 0.0795 0.0722 0.0575 0.0626 0.0569 0.0504 Bonus 1 2 3 4 5 6 7 0.1291 0.0792 0.0676 0.0659 0.0550 0.0524 0.0364 Make 1 2 3 4 5 6 7 8 9 0.0761 0.0802 0.0576 0.0333 0.0919 0.0543 0.0838 0.0729 0.0712 A4. Modelo Final Ajustado para Frecuencia — Salida de R Call: glm(formula = Claims ~ factor(Kilometres) + factor(Zone) + factor(Bonus) + factor(Make), family = poisson(link = log), offset = log(Insured)) Deviance Residuals: Min 1Q Median 3Q Max -6.985 -0.863 -0.172 0.600 6.401 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.81284 0.01376 -131.78 &lt; 2e-16 *** factor(Kilometres)2 0.21259 0.00752 28.25 &lt; 2e-16 *** factor(Kilometres)3 0.32023 0.00866 36.97 &lt; 2e-16 *** factor(Kilometres)4 0.40466 0.01205 33.57 &lt; 2e-16 *** factor(Kilometres)5 0.57595 0.01283 44.89 &lt; 2e-16 *** factor(Zone)2 -0.23817 0.00950 -25.08 &lt; 2e-16 *** factor(Zone)3 -0.38639 0.00967 -39.96 &lt; 2e-16 *** factor(Zone)4 -0.58190 0.00865 -67.24 &lt; 2e-16 *** factor(Zone)5 -0.32613 0.01453 -22.45 &lt; 2e-16 *** factor(Zone)6 -0.52623 0.01188 -44.31 &lt; 2e-16 *** factor(Zone)7 -0.73100 0.04070 -17.96 &lt; 2e-16 *** factor(Bonus)2 -0.47899 0.01209 -39.61 &lt; 2e-16 *** factor(Bonus)3 -0.69317 0.01351 -51.32 &lt; 2e-16 *** factor(Bonus)4 -0.82740 0.01458 -56.73 &lt; 2e-16 *** factor(Bonus)5 -0.92563 0.01397 -66.27 &lt; 2e-16 *** factor(Bonus)6 -0.99346 0.01163 -85.43 &lt; 2e-16 *** factor(Bonus)7 -1.32741 0.00868 -152.84 &lt; 2e-16 *** factor(Make)2 0.07624 0.02124 3.59 0.00033 *** factor(Make)3 -0.24741 0.02509 -9.86 &lt; 2e-16 *** factor(Make)4 -0.65352 0.02419 -27.02 &lt; 2e-16 *** factor(Make)5 0.15492 0.02023 7.66 1.9e-14 *** factor(Make)6 -0.33558 0.01738 -19.31 &lt; 2e-16 *** factor(Make)7 -0.05594 0.02334 -2.40 0.01655 * factor(Make)8 -0.04393 0.03160 -1.39 0.16449 factor(Make)9 -0.06805 0.00996 -6.84 8.2e-12 *** --- Signif. codes: 0 *** 0.001 ** 0.01 * 0.05 . 0.1 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 34070.6 on 2181 degrees of freedom Residual deviance: 2966.1 on 2157 degrees of freedom AIC: 10654 A5. Verificación de la Significancia de los Factores en el Modelo Final Ajustado para Frecuencia — Salida de R Analysis of Deviance Table Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev P(&gt;|Chi|) NULL 2181 34071 factor(Kilometres) 4 1476 2177 32594 2.0e-318 *** factor(Zone) 6 6097 2171 26498 0 *** factor(Bonus) 6 22041 2165 4457 0 *** factor(Make) 8 1491 2157 2966 1.4e-316 *** Figura 20.5: Figura A1. Histograma de residuos de devianza del modelo final de frecuencia. A6. Modelo Final Ajustado para Severidad — Salida de R Call: glm(formula = Payment ~ factor(Zone) + factor(Make), family = Gamma(link = log), weights = Weight, offset = log(Claims)) Deviance Residuals: Min 1Q Median 3Q Max -2.56968 -0.39928 -0.06305 0.07179 2.81822 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 8.38767 0.10933 76.722 &lt; 2e-16 *** factor(Zone)2 -0.06099 0.09515 -0.641 0.52156 factor(Zone)3 0.15290 0.09573 1.597 0.11041 factor(Zone)4 0.09223 0.09781 0.943 0.34583 factor(Zone)5 0.19729 0.09313 2.119 0.03427 * factor(Zone)6 0.24205 0.09377 2.581 0.00992 ** factor(Zone)7 0.10566 0.10804 0.978 0.32825 factor(Make)2 -0.04963 0.11306 -0.439 0.66071 factor(Make)3 0.25309 0.11404 2.219 0.02660 * factor(Make)4 0.04948 0.11634 0.425 0.67067 factor(Make)5 0.09725 0.11419 0.852 0.39454 factor(Make)6 0.10781 0.11658 0.925 0.35517 factor(Make)7 -0.02040 0.11313 -0.180 0.85692 factor(Make)8 0.32623 0.11247 2.900 0.00377 ** factor(Make)9 -0.06377 0.15061 -0.423 0.67205 --- Signif. codes: 0 *** 0.001 ** 0.01 * 0.05 . 0.1 1 (Dispersion parameter for Gamma family taken to be 0.4830309) Null deviance: 617.32 on 1796 degrees of freedom Residual deviance: 596.79 on 1782 degrees of freedom AIC: 16082 A7. Verificación de la Significancia de los Factores en el Modelo Final Ajustado para Severidad — Salida de R Analysis of Deviance Table Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev P(&gt;|Chi|) NULL 1796 617.32 factor(Zone) 6 8.06 1790 609.26 0.01 * factor(Make) 8 12.47 1782 596.79 0.001130 ** Figura 20.6: Figura A2. \\(qq\\) Plot de residuos ponderados de un modelo lognormal. La variable dependiente es la severidad promedio por reclamo. Los pesos son la raíz cuadrada del número de reclamos. El mal ajuste en las colas sugiere usar una alternativa al modelo lognormal. 20.6 Lecturas Adicionales y Referencias Puede encontrar una discusión adicional sobre las pautas para presentar datos en texto en The Chicago Manual of Style, una referencia conocida para preparar y editar documentos escritos. Para directrices sobre la presentación de datos en tablas, consulte Ehrenberg (1977) y Tufte (1983). Miller (2005) es un libro introductorio a la redacción de informes estadísticos con énfasis en métodos de regresión. Referencias del Capítulo The Chicago Manual of Style (1993). The University of Chicago Press, 14th ed. Chicago, Ill. Cleveland, William S. (1994). The Elements of Graphing Data. Monterey, Calif.: Wadsworth. Ehrenberg, A.S.C. (1977). Rudiments of numeracy. Journal of the Royal Statistical Society A 140:277-97. Miller, Jane E. (2005). The Chicago Guide to Writing about Multivariate Analysis. The University of Chicago Press, Chicago, Ill. Tufte, Edward R. (1983). The Visual Display of Quantitative Information. Graphics Press, Cheshire, Connecticut. Tufte, Edward R. (1990). Envisioning Information. Graphics Press, Cheshire, Connecticut. 20.7 Ejercicio 20.1. Determinantes de la Compensación de CEOs. La compensación de los directores ejecutivos (CEO, por sus siglas en inglés) varía significativamente entre empresas. Para este ejercicio, usted deberá realizar un informe sobre una muestra de empresas tomada de una encuesta de Forbes Magazine para identificar patrones importantes en la compensación de los CEOs. Específicamente, introduzca un modelo de regresión que explique los salarios de los CEOs en términos de las ventas de la empresa, la experiencia del CEO, su nivel educativo y su participación accionaria en la empresa. Entre otras cosas, este modelo debería mostrar que las empresas más grandes tienden a pagar más a sus CEOs y, algo sorprendente, que los CEOs con un nivel educativo más alto ganan menos que otros CEOs comparables. Además de identificar los factores que influyen en la compensación de los CEOs, este modelo debería usarse para predecir la compensación de los CEOs con fines de negociación salarial. "],["C21Design.html", "Capítulo 21 Diseñando Gráficos Efectivos 21.1 Introducción 21.2 Las Elecciones de Diseño Gráfico Marcan la Diferencia 21.3 Directrices de Diseño 21.4 Fundamentos Empíricos para las Directrices 21.5 Observaciones Finales 21.6 Lecturas Adicionales y Referencias", " Capítulo 21 Diseñando Gráficos Efectivos Vista Previa del Capítulo.1 Los actuarios, como otros profesionales de negocios, comunican ideas cuantitativas mediante gráficos. Dado que el proceso de leer, o decodificar, gráficos es más complejo que leer texto, los gráficos son vulnerables al abuso. Para resaltar esta vulnerabilidad, damos varios ejemplos de gráficos comúnmente encontrados que inducen a error y ocultan información. Para ayudar a los creadores a diseñar gráficos más efectivos y a los espectadores a reconocer gráficos engañosos, este capítulo resume pautas para diseñar gráficos que muestren información numérica importante. Al diseñar gráficos, los creadores deben: Evitar “chartjunk” (elementos gráficos innecesarios). Usar pequeños múltiples para promover comparaciones y evaluar cambios. Usar gráficos complejos para representar patrones complejos. Relacionar el tamaño del gráfico con el contenido de información. Usar formas gráficas que promuevan comparaciones. Integrar gráficos y texto. Demostrar un mensaje importante. Conocer a la audiencia. Algunas de estas pautas para diseñar gráficos efectivos, como (6), (7) y (8), se derivan directamente de principios para una escritura efectiva. Otras, como las pautas (3), (4) y (5), provienen de la psicología cognitiva, la ciencia de la percepción. Las pautas (1) y (2) tienen raíces tanto en la escritura efectiva como en la percepción gráfica. Por ejemplo, el principio de brevedad en la escritura demuestra cómo la eliminación de perspectivas tridimensionales pseudo y otras formas de “chartjunk” mejora los gráficos. Como otro ejemplo, el principio de estructura paralela en la escritura sugiere usar pequeñas variaciones múltiples de una forma gráfica básica para visualizar relaciones complejas entre diferentes grupos y a lo largo del tiempo. Para resaltar el aspecto científico de la percepción gráfica, examinamos el proceso de comunicación con un gráfico, comenzando con la interpretación de los datos por parte del emisor y terminando con la interpretación del gráfico por parte del receptor. En consonancia con la tradición científica, este capítulo discute varios estudios en la literatura sobre la efectividad de los gráficos. Concluimos que la profesión actuarial tiene muchas oportunidades para mejorar su práctica, haciendo la comunicación más eficiente y precisa. 21.1 Introducción Al igual que otros profesionales de negocios, los actuarios comunican ideas de manera oral y escrita, así como a través de presentaciones, que son formas interactivas de comunicación que abarcan mensajes orales y escritos. Los actuarios, así como otros analistas financieros, comunican ideas con componentes cuantitativos importantes. Los escritores expresan ideas cuantitativas como (1) números dentro de párrafos, (2) números dentro de formas tabulares, (3) relaciones funcionales como ecuaciones, y (4) datos o ecuaciones como gráficos. Los gráficos son un medio simple pero poderoso para la comunicación escrita de ideas cuantitativas. Los gráficos pueden presentar una gran cantidad de datos en un espacio pequeño, expresar relaciones importantes entre cantidades, comparar diferentes conjuntos de datos y describir datos, proporcionando así una imagen coherente de sistemas complejos. Los gráficos hacen más que simplemente declarar una idea; la demuestran. Los gráficos son poderosos porque son flexibles, pero esa flexibilidad puede ser una desventaja debido al potencial de abuso. Las referencias bien aceptadas que tratan métodos de presentación de datos cuantitativos mitigan las oportunidades de abuso. El Chicago Manual of Style (1993), una referencia estándar, discute la presentación de datos dentro del texto, y Ehrenberg (1977) y Tufte (1983) discuten la presentación de datos tabulares. En contraste, nos enfocamos en la presentación de datos a través de exhibiciones gráficas. Este capítulo busca mejorar la práctica actuarial en lo que respecta a exhibiciones gráficas. Pretendemos: (1) demostrar la importancia de las exhibiciones gráficas, (2) proporcionar pautas para mejorar la práctica gráfica, y (3) introducir algunos fundamentos científicos de la buena práctica gráfica. La agenda es ambiciosa, pero el objetivo de este capítulo es proporcionar a los actuarios practicantes herramientas básicas que puedan usar para convertirse en consumidores críticos y productores efectivos de gráficos. También esperamos que los lectores adopten nuestro entusiasmo y deseen explorar por su cuenta la literatura sobre diseño gráfico. Un tema importante de este capítulo es que los principios de la escritura vigorosa pueden y deben aplicarse a la práctica de hacer gráficos efectivos. El Elements of Style (Strunk y White 1979, p. xiv) resume la escritura vigorosa: La escritura vigorosa es concisa. Una oración debe contener solo las palabras necesarias, un párrafo solo las oraciones necesarias, por la misma razón que un dibujo no debe tener líneas innecesarias y una máquina no debe tener piezas innecesarias. Esto no requiere que el escritor haga todas sus oraciones cortas, o que evite todo detalle y trate sus temas solo en esquema, sino que cada palabra cuente. White atribuye esta cita a William Strunk. White la llama “un ensayo breve y valioso sobre la naturaleza y la belleza de la brevedad – sesenta y tres palabras que podrían cambiar el mundo.” Argumentamos que la brevedad es especialmente importante al crear gráficos efectivos. Esto también fue entendido por Strunk; como se señaló anteriormente, dijo “un dibujo no debe contener líneas innecesarias …” Usamos el término chartjunk, introducido por Tufte (1983), para cualquier elemento innecesario en un gráfico. Los principios de escritura vigorosa, además de la brevedad, también se aplican a la práctica de crear gráficos efectivos. Al igual que en la escritura, los gráficos efectivos son el resultado de revisiones y ediciones repetidas. Los gráficos mal diseñados pueden ocultar información y confundir. Los gráficos extravagantes o pretenciosos son una distracción cuando gráficos más simples son suficientes. Aunque los principios de escritura efectiva son valiosos, no son suficientes para producir gráficos efectivos. La escritura se procesa de manera secuencial, palabra por palabra, oración por oración, con un principio y un final. El proceso de “leer,” o decodificar, un gráfico es no lineal y más complejo. Estas complejidades adicionales significan que incluso los autores que siguen prácticas efectivas de escritura pueden producir gráficos ineficaces. A menudo, la forma de la prosa escrita es el único determinante de su valor, mientras que en los gráficos el proceso de comunicación desempeña el papel dominante. Asumimos que los lectores están familiarizados con las formas efectivas de escritura. Por lo tanto, primero revisamos el proceso de comunicación en el que un gráfico juega un papel crucial. Para subrayar la importancia de un diseño gráfico efectivo, la Sección 21.2 proporciona varias ilustraciones de gráficos que ocultan información y confunden; los defectos ilustrados son inconvenientes más serios que simples chartjunk. Las ilustraciones de la Sección 21.2 motivan la necesidad de pautas adicionales y métodos para construir gráficos efectivos. La Sección 21.3 introduce ocho pautas importantes para crear y visualizar gráficos. Aunque estas pautas no son una panacea para todos los defectos gráficos, sí proporcionan a los profesionales de negocios, como los actuarios, una lista clave para crear gráficos efectivos. Las pautas están organizadas de modo que las dos primeras, sobre chartjunk y el uso de múltiples, se basan tanto en perspectivas de escritura efectiva como en la percepción gráfica. Las pautas Tres, Cuatro y Cinco están relacionadas principalmente con la literatura de percepción gráfica, mientras que las pautas Seis, Siete y Ocho se basan principalmente en principios de escritura efectiva. Al igual que en la escritura efectiva, las cuestiones de estilo entran en la discusión de lo que es y lo que no es un gráfico efectivo. Muchas decisiones de estilo se basan en prácticas aceptadas sin una base científica firme. Sin embargo, el proceso de percibir gráficos ha sido objeto de estudio en varias disciplinas científicas, incluidas la psicofísica, la psicología cognitiva y las visiones computacionales (Cleveland 1995, Capítulo 4). La Sección 21.4 ilustra algunos tipos de evidencia experimental para determinar una forma gráfica efectiva basada tanto en el receptor como en el gráfico como unidades de estudio. La Sección 21.4 también muestra cómo algunos gráficos comunes en publicaciones de negocios, como los gráficos de barras y de pastel, son malos comunicadores de información numérica. Las Secciones 21.5 y 21.6 contienen observaciones finales y descripciones de algunos recursos para actuarios que deseen aprender más sobre cómo diseñar gráficos efectivos. La mayoría de los lectores están alejados de los datos detallados resumidos por un gráfico. Varias dificultades y conceptos erróneos pueden surgir debido a la distancia entre los datos originales y la interpretación del gráfico por parte del espectador. La Figura 21.1 ilustra el desafío de comunicarse con un gráfico. El emisor (y creador) del gráfico tiene un mensaje derivado de una interpretación de datos. Aunque algunos gráficos comunican datos en bruto, el propósito principal de la mayoría de los gráficos es comunicar la interpretación del emisor. El mensaje que el emisor pretende se codifica en un gráfico y se transmite al receptor. Figura 21.1: Diagrama de flujo del proceso de comunicación con un gráfico. El gráfico es un intermediario crucial en el proceso de comunicar la interpretación de los datos al receptor. En general, el receptor no está involucrado ni con la interpretación exacta pretendida por el emisor ni con los datos originales. Por lo tanto, el receptor debe decodificar el gráfico y desarrollar una interpretación de su mensaje. Surgen dos cuestiones: ¿La interpretación construida por el receptor es congruente con la interpretación del emisor? ¿La interpretación del receptor es consistente y está respaldada por los datos? El primer tema depende de la habilidad con la que el emisor construye el gráfico y de la habilidad con la que el receptor lo decodifica. Un gráfico mal diseñado puede ocultar o distorsionar el mensaje del emisor. Un gráfico difícil de leer puede desalentar al receptor de dedicar el tiempo necesario para decodificar el mensaje correctamente. El receptor puede ignorar o malinterpretar un gráfico que no está construido con cuidado. El segundo tema depende no solo de las habilidades mencionadas anteriormente, sino también de la habilidad con la que el emisor extrae significado de los datos. ¿Qué tan cuidadosamente documenta el emisor el proceso de interpretación? ¿Esto se comunica al receptor? ¿Es el receptor capaz de evaluar hasta qué punto el gráfico es un resumen creíble de los datos? El fallo en cualquiera de estos puntos podría resultar en que el receptor ignore o malinterprete el gráfico. Este capítulo asume que los gráficos incluidos en comunicaciones empresariales son objeto de escrutinio por lectores serios. Los gráficos que aparecen rápidamente en la pantalla de televisión, un rotafolio o un paquete de presentaciones están diseñados para atraer la atención y entretener al espectador. En estos medios, predominan las consideraciones de diseño sobre la información. Nos centramos, en cambio, en gráficos que forman parte de la escritura profesional y están diseñados para informar. Al igual que con la escritura efectiva, asumimos que al crear gráficos “uno debe creer… en la verdad y el valor del boceto, en la capacidad del lector para recibir y decodificar el mensaje” (Strunk y White 1979, p. 84). Ahora pasamos a ejemplos de gráficos que engañan. 21.2 Las Elecciones de Diseño Gráfico Marcan la Diferencia Como señaló Schmid (1992), el antiguo proverbio “Una imagen vale más que mil palabras,” cuando se aplica a los gráficos podría leerse como “Una imagen puede valer más que mil palabras o cifras.” El potencial gráfico no se realiza fácilmente. Debido a su flexibilidad, los gráficos con demasiada frecuencia muestran representaciones visuales de información cuantitativa que son poco informativas, confusas o incluso engañosas. Los ejemplos 21.2.1 a 21.2.5 ilustran cinco tipos diferentes de gráficos engañosos. En cada caso, los datos no se alteraron ni se representaron diferentes dimensiones de los datos. El tema común de los ejemplos es que, al alterar únicamente las escalas de los datos, el creador puede cambiar drásticamente la interpretación del espectador. Ejemplo 21.2.1: Incluir el cero para comprimir datos2. La Figura 21.2 muestra una serie temporal del porcentaje de trabajadores equivalentes a tiempo completo empleados en la industria de seguros. Los datos anuales, 1948-1993, provienen de las Cuentas Nacionales de Ingreso y Producto producidas por la Oficina de Estadísticas Laborales. El panel izquierdo, Figura 21.2(a), da la impresión de un entorno de empleo estable para la industria de seguros. Incluir el cero en el eje vertical produce esta aparente estabilidad. Al hacer esto, la mayor parte del gráfico se dedica al espacio en blanco que no muestra la variabilidad en los datos. En contraste, el panel derecho, Figura 21.2(b), utiliza los datos para establecer el rango en los ejes. Este panel muestra claramente los grandes aumentos de empleo en los años posteriores a la Guerra de Corea, alrededor de 1952. También permite al lector ver las disminuciones de empleo que la industria de seguros ha sufrido en los últimos tres años. Figura 21.2: Empleados anuales en seguros, 1948-1993. ‘Empleados de seguros’ es el porcentaje de empleados equivalentes a tiempo completo que trabajan para aseguradoras. Permitir que los datos determinen los rangos de escala revela aspectos interesantes de los datos. Código R para Generar la Figura 21.2 # Figura 212 insur &lt;- read.table(&quot;Chapters/Chapter21Graphs/Data/Chapter21Figure2data.txt&quot;, header=TRUE) #insur &lt;- read.table(&quot;Data/Chapter21Figure2data.txt&quot;, header=TRUE) par(mfrow=c(1, 2), cex=1.3, mar=c(6.1,3.1,2.5,1)) plot(insur$YEAR,insur$INSUR, type=&quot;l&quot;, ylim=c(0,1.6), xlab=&quot;Año&quot;, ylab=&quot;&quot;, las = 1, cex = 0.5) mtext(&quot;Empleados \\n de Seguros&quot;, side=2, at=1.83 ,las=1, cex=0.9, adj=.7) # Agregar pie de figura debajo del primer gráfico mtext(&quot;(a) Una industria de seguros estable&quot;, side = 1, line = 4, cex = 0.9, adj = 0.2) # Segundo gráfico plot(insur$YEAR, insur$INSUR, type = &quot;l&quot;, xlab = &quot;Año&quot;, las = 1, ylab = &quot;&quot;, cex = 0.5) mtext(&quot;Empleados \\n de Seguros&quot;, side=2, at=1.593, las=1, adj=.7, cex=0.9) # Agregar pie de figura debajo del segundo gráfico mtext(&quot;(b) La fuerza laboral en seguros \\n aumentó drásticamente en los años 50&quot;, side = 1, line = 4.5, cex = 0.9, adj = -0.01) Este ejemplo es similar a una ilustración popular del conocido libro de Huff, How to Lie with Statistics (Huff 1954). El punto es que la motivación externa a los datos, como incluir el cero en un eje, puede invitarnos a alterar la escala de los datos y cambiar la interpretación que tiene el espectador de los mismos. Como muestra el Ejemplo 21.2.2, los creadores de gráficos también pueden alterar la interpretación del espectador cambiando ambas escalas en un gráfico bidimensional. Ejemplo 21.2.2: Percepción de la Correlación. La Figura 21.3 relaciona la rentabilidad del manejo de riesgos con el tamaño de la empresa. Estos datos provienen de una encuesta a 73 gerentes de riesgo de grandes empresas internacionales con sede en los EE. UU., originalmente reportada en Schmit y Roth (1990). Los datos se analizan en la Sección 6.5. Aquí, la medida de la rentabilidad del manejo de riesgos, Firm Cost, se define como el logaritmo de las primas totales de propiedad y accidentes, más las pérdidas no aseguradas, como porcentaje de los activos totales de la empresa. La medida del tamaño de la empresa es el logaritmo de los activos totales. Figura 21.3: Eficiencia del Manejo de Riesgos versus Tamaño de la Empresa. Los datos representados en cada figura son los mismos. Sin embargo, las escalas más amplias en el panel (b) sugieren que los datos están más correlacionados. Código R para Generar la Figura 21.3 # Figura 213 FC &lt;- read.table(&quot;Chapters/Chapter21Graphs/Data/Chapter21Figure3data.txt&quot;, header=TRUE) par(mfrow=c(1, 2), cex=1.1, mar=c(6.1,3.1,2.5,1)) plot(FC$SIZELOG, FC$COSTLOG, xlim=c(5,11), ylim=c(-2,5), xlab=&quot;Tamaño Logarítmico de la Empresa&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Costo de la Empresa&quot;, side=2, las=1, at=5.7, cex=0.9, adj=.4) # Añadir pie de figura debajo del primer gráfico mtext(&quot;(a) Los datos en esta figura parecen \\n menos correlacionados.&quot;, side = 1, line = 5, cex = 0.9, adj = .8) # Segundo gráfico plot(FC$SIZELOG, FC$COSTLOG, xlim=c(0,15), ylim=c(-10,10), xlab=&quot;Tamaño Logarítmico de la Empresa&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Costo de la Empresa&quot;, side=2, las=1, at=12, cex=0.9, adj=.4) # Añadir pie de figura debajo del segundo gráfico mtext(&quot;(b) Los datos en esta figura parecen \\n más correlacionados.&quot;, side = 1, line = 5, cex = 0.9, adj = 0.8) El panel izquierdo, Figura 21.3(a), muestra una relación negativa entre los costos de la empresa y su tamaño, como anticiparon Schmit y Roth. El coeficiente de correlación entre las dos variables es -0.64. Los datos se encuentran en una porción central más pequeña de la Figura 21.3(b) en comparación con el panel izquierdo, Figura 21.3(a). La Figura 21.3(a) utiliza los datos para determinar los ejes y, por lo tanto, muestra más patrones en los datos. Como muestran Cleveland, Diaconis y McGill (1982), la escala hace que los datos en el panel derecho parezcan más correlacionados que en el panel izquierdo. El cambio de escalas también puede alterar la percepción del espectador sobre las tendencias en los datos de series temporales, como se ilustra en el Ejemplo 21.2.3. Ejemplo 21.2.3: Transformación a una Escala Logarítmica. La Figura 21.4 muestra una serie temporal del mercado de seguros de crédito en los EE. UU. durante 1950-1989. Estos datos se analizan en Frees (1996) y provienen originalmente del Life Insurance Fact Book (1990). Cuando se examina el monto del seguro en una escala lineal en la Figura 21.4(a), el mercado de seguros de crédito parece estar expandiéndose rápidamente. Sin embargo, la Figura 21.4(b) muestra que, cuando se examina en una escala logarítmica, el mercado se está estabilizando. Como se discute en la Sección 3.2.2, los cambios en una escala logarítmica pueden interpretarse como cambios proporcionales. Así, la Figura 21.4(a) muestra que el mercado está aumentando rápidamente, y la Figura 21.4(b) muestra que la tasa de aumento se está estabilizando. Estos mensajes no son contradictorios, pero los espectadores deben interpretar cada gráfico críticamente para entender el mensaje pretendido. Figura 21.4: Seguro de Vida a Crédito Anual en los EE. UU., 1950-1989. Diferentes escalas verticales dan diferentes impresiones sobre la tasa de crecimiento a lo largo del tiempo. Código R para Generar la Figura 21.4 # Figura 214 inforce &lt;- read.table(&quot;Chapters/Chapter21Graphs/Data/Chapter21Figure4data.txt&quot;, header=TRUE) inforce$LOGYEAR &lt;- log(inforce$YEAR) inforce$LOGINFORCE &lt;- log(inforce$INFORCE) par(mfrow=c(1, 2), cex=1.1, mar=c(6.1,4.1,2.5,1)) # Primer gráfico plot(inforce$YEAR, inforce$INFORCE, type=&quot;o&quot;, pch=20, yaxt=&quot;n&quot;, xlab=&quot;Año&quot;, ylab=&quot;&quot;, cex.axis=.9) axis(2, at=seq(0,200000,100000), labels=c(&quot;0&quot;, &quot;100,000&quot;, &quot;200,000&quot;), las = 1, cex.axis=.9) mtext(&quot;Seguro \\n en Vigor&quot;, side=2, las=1, at=294000, cex=1.1, adj=.9) # Añadir pie de figura debajo del primer gráfico mtext(&quot;(a) Mercado de seguros de vida a crédito \\n en EE. UU. en expansión&quot;, side = 1, line = 5, cex = 0.9, adj = 1) # Segundo gráfico plot(inforce$LOGYEAR, inforce$LOGINFORCE, type=&quot;o&quot;, pch=20, yaxt=&quot;n&quot;, xaxt=&quot;n&quot;, xlab=&quot;Año&quot;, ylab=&quot;&quot;, cex.axis=.9, ylim=c(7.5,12.5)) axis(2, at=seq(7.5,12.1,2), labels=c(&quot;1000&quot;, &quot;10,000&quot;, &quot;100,000&quot;), las =1, cex.axis=.9) mtext(&quot;Seguro \\n en Vigor&quot;, side=2, las=1, at=13.3, cex=1.1, adj=.9) axis(1, at=seq(7.575,7.595,.005), labels=c(&quot;1950&quot;, &quot;1960&quot;, &quot;1970&quot;, &quot;1980&quot;, &quot;1990&quot;), cex.axis=.9) # Añadir pie de figura debajo del segundo gráfico mtext(&quot;(b) Mercado de seguros de vida a crédito \\n en EE. UU. estabilizándose&quot;, side = 1, line = 5, cex = 0.9, adj = 1) Ejemplo 21.2.4: Doble Eje Y. La Figura 21.5 muestra dos medidas de inflación producidas por la Oficina de Estadísticas Laborales. En el eje izquierdo se encuentra CPI_U, el índice de precios al consumidor para consumidores urbanos. En el eje derecho se encuentra CPI_M, el componente médico del índice general. Cada serie consta de valores mensuales que abarcan desde enero de 1947 hasta abril de 1995. Figura 21.5: Valores Mensuales del Índice General de Precios al Consumidor (CPI) y del Componente Médico del CPI, enero de 1947 a abril de 1995. Diferentes rangos de escala alteran las apariencias de crecimiento relativo de las dos series. Código R para Generar la Figura 21.5 # Figura 215 cpi &lt;- read.table(&quot;Chapters/Chapter21Graphs/Data/Chapter21Figure5data.txt&quot;, header=TRUE) cex.insideplot &lt;- 0.7 cex.outsideplot &lt;- 0.9 par(mfrow=c(1, 2), mar=c(7.1,2.5,1.8,3), cex=cex.outsideplot) # Primer gráfico plot(cpi$YEAR, cpi$CPI_U, type=&quot;l&quot;, xlim=c(1945, 1995), ylim=c(20,170), xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab = &quot;AÑO&quot;) axis(1, at=seq(1945,1995,10), labels=c(&quot;1945&quot;, &quot;1955&quot;, &quot;1965&quot;, &quot;1975&quot;, &quot;1985&quot;, &quot;1995&quot;)) axis(2, at=seq(20,170,50), labels=c(&quot;20&quot;,&quot;70&quot;, &quot;120&quot;, &quot;170&quot;), las =1) mtext(&quot;CPI_U&quot;, side=2, las=1, at=184, cex=cex.outsideplot) text(1980, 110, &quot;CPI_U&quot;, cex=cex.insideplot) text(1990,80, &quot;CPI_M&quot;, cex=cex.insideplot) par(new=TRUE) plot(cpi$YEAR, cpi$CPI_M, type=&quot;l&quot;, ann=FALSE, axes = FALSE, ylim=c(0, 250)) axis(4, las=1) mtext(&quot;CPI_M&quot;, side=4, las=1, at=275, cex=cex.outsideplot) # Añadir pie de figura debajo del primer gráfico mtext(&quot;(a) El CPI general es similar al \\n componente médico del CPI&quot;, side = 1, line = 5, cex = 0.8, adj = .1) # Segundo gráfico plot(cpi$YEAR, cpi$CPI_U, type=&quot;l&quot;, xlim=c(1945, 1995), ylim=c(0,250), xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab = &quot;AÑO&quot;) axis(1, at=seq(1945,1995,10), labels=c(&quot;1945&quot;, &quot;1955&quot;, &quot;1965&quot;, &quot;1975&quot;, &quot;1985&quot;, &quot;1995&quot;)) axis(2, at=seq(0,250,50), labels=c(&quot;0&quot;,&quot;50&quot;, &quot;100&quot;, &quot;150&quot;, &quot;200&quot;, &quot;250&quot;), las =1) mtext(&quot;CPI_U&quot;, side=2, las=1, at=275, cex=cex.outsideplot) text(1958, 50, &quot;CPI_U&quot;, cex=cex.insideplot) text(1975,10, &quot;CPI_M&quot;, cex=cex.insideplot) par(new=TRUE) plot(cpi$YEAR, cpi$CPI_M, type=&quot;l&quot;, ann=FALSE, axes = FALSE, ylim=c(0, 250)) axis(4, las=1) mtext(&quot;CPI_M&quot;, side=4, las=1, at=275, cex=cex.outsideplot) # Añadir pie de figura debajo del segundo gráfico mtext(&quot;(b) El índice general de precios al consumidor (CPI) aumenta \\n más lentamente que el componente médico del CPI&quot;, side = 1, line = 5, cex = 0.8, adj = .45) El panel izquierdo, Figura 21.5(a), sugiere que el CPI_U y el CPI_M comienzan y terminan aproximadamente en la misma posición, lo que implica que han aumentado a tasas similares durante el período. El creador podría argumentar que cada índice mide el valor de un conjunto estándar de bienes, justificando el uso de una escala diferente para cada serie. El panel derecho, Figura 21.5(b), proporciona una representación más útil de los datos al usar la misma escala para cada serie. Aquí, el CPI_M comienza más bajo que el CPI_U y termina más alto. Es decir, el índice del componente médico ha aumentado más rápidamente que el índice de precios para consumidores urbanos. También son evidentes otros patrones en la Figura 21.5: cada serie aumentó a una tasa similar entre 1979-1983, y el CPI_M aumentó mucho más rápidamente de 1983 a 1994 en comparación con 1948-1979. Ejemplo 21.2.5: Relación de Aspecto. La Figura 21.6 muestra una serie temporal de la tasa de desempleo mensual desde abril de 1953 hasta diciembre de 1992. La tasa de desempleo es el porcentaje de la fuerza laboral civil desempleada, ajustada estacionalmente. Es parte de la Encuesta de Hogares producida por la Oficina de Estadísticas Laborales, Departamento de Trabajo. Esta serie fue analizada en Frees et al. (1997). El panel superior de la Figura 21.6 muestra que la tasa de desempleo promedió un 5.9%, alcanzando un máximo de 10.8% en el cuarto trimestre de 1982 y un mínimo de 2.7% en el tercer trimestre de 1953. Figura 21.6: Gráfico de Series Temporales de Valores Trimestrales de la Tasa de Desempleo en EE.UU., 1953-1992. El panel inferior muestra una característica que no es evidente en el panel superior; el desempleo disminuye más lentamente de lo que aumenta. Código R para Generar la Figura 21.6 # Figura 216 unemploy &lt;- read.table(&quot;Chapters/Chapter21Graphs/Data/Chapter21Figure6data.txt&quot;, header=TRUE) par(mfrow=c(2,1), mar=c(2,3,3,.2), cex=1.7) layout(matrix(c(2,1)), heights=c(4,2.5)) # Gráfico superior plot.ts(unemploy$YEAR,unemploy$UNEMPLOY, type=&quot;l&quot;, ylab=&quot;&quot;, xlim=c(1950, 1993), ylim=c(2,11), las=1, yaxt=&quot;n&quot;, xlab = &quot;AÑO&quot;) axis(2, at=seq(2,11,4), labels=c(&quot;2&quot;, &quot;6&quot;, &quot;10&quot;), las=1) mtext(&quot;Desempleo&quot;, side=2, las=1, at=15, adj=.2, cex=1.3) # Gráfico inferior plot.ts(unemploy$YEAR,unemploy$UNEMPLOY, type=&quot;l&quot;, ylab=&quot;&quot;, xlim=c(1950, 1993), ylim=c(2,11), las=1, xlab = &quot;AÑO&quot;) axis(2, at=seq(3,11,2), labels=c(&quot;3&quot;, &quot;5&quot;, &quot;7&quot;, &quot;9&quot;, &quot;11&quot;), las=1) mtext(&quot;Desempleo&quot;, side=2, las=1, at=12, adj=0.2, cex=1.3) Los dos paneles en la Figura 21.6 difieren solo en su forma, no en la escala de ninguna variable ni en la cantidad relativa de espacio que ocupan los datos dentro del marco de la figura. Para diferenciar estas dos formas, podemos usar el concepto de relación de aspecto de una figura, definida como la altura del marco de datos dividida por su ancho (algunas fuentes usan el recíproco de este valor para la relación de aspecto). El marco de datos es simplemente un rectángulo cuya altura y ancho permiten que el gráfico de los datos encaje dentro. Para ilustrar, en el panel superior de la Figura 21.6, la longitud del lado vertical es igual a la del lado horizontal. En el panel inferior, el lado vertical es solo el 25% del lado horizontal. Ambos paneles muestran que la serie de desempleo osciló ampliamente durante este período de 39 años. Sin embargo, el panel inferior muestra una característica que no es evidente en el panel superior: el aumento hacia el pico de un ciclo de desempleo es más pronunciado que el descenso desde el pico. Dentro de cada ciclo de desempleo, el porcentaje de trabajadores desempleados tiende a aumentar rápidamente hasta un máximo y luego disminuir gradualmente hasta un mínimo. Este comportamiento es sorprendentemente regular durante el período de casi 39 años mostrado en el gráfico. Relaciones de aspecto diferentes pueden dejar impresiones sustancialmente diferentes en el ojo, como ilustra la Figura 21.6. Por lo tanto, la relación de aspecto puede elegirse para enfatizar diferentes características de los datos. 21.3 Directrices de Diseño Comprender los problemas ilustrados en la Sección 21.2 puede ayudar a los actuarios y otros profesionales de negocios a crear e interpretar gráficos. Esta sección presenta ocho directrices para diseñar gráficos efectivos. Uno de nuestros puntos principales es que la práctica actual no está en concordancia con estas directrices. Por lo tanto, anticipamos que no todos nuestros lectores encontrarán las demostraciones de las directrices visualmente atractivas, pero, como se mencionó en la Sección 21.1, muchas de las directrices se basan en una base científica descrita en la Sección 21.4. La “intuición” es algo que aprendemos y cultivamos; el progreso en la ciencia no siempre se conforma con la intuición actual. En un momento se creía ampliamente que la tierra era plana y que el sol giraba alrededor de la tierra. Las demostraciones de esta sección pueden o no ser inmediatamente intuitivas, pero son conclusiones lógicas de las directrices de diseño aquí defendidas. Directriz Uno: Evitar Chartjunk En la Sección 21.1, definimos chartjunk como cualquier adorno innecesario en un gráfico. Los creadores de gráficos que usan chartjunk reducen su credibilidad con receptores serios. Incluso cuando los emisores transmiten una interpretación correcta acompañada de chartjunk, piden a los receptores que procesen e ignoren correctamente el chartjunk. Si el chartjunk es parte de las opciones predeterminadas, o de fácil uso, de un paquete de software, el emisor puede sobrecargar un gráfico, o incluso hacerlo engañoso, simplemente presionando un botón. Los emisores que evitan el chartjunk aumentan su credibilidad. Piden a los receptores que miren solo los caracteres y marcas significativos. Los emisores pueden tener que dedicar un tiempo considerable a su software para hacer gráficos efectivos, pero el respeto y la atención de sus receptores los recompensan. Otra forma de evitar el chartjunk es no usar un gráfico si unas pocas palabras bastarán. Si el mensaje de un gráfico puede resumirse en pocas palabras, entonces el gráfico no es necesario. ¡Evite imágenes que no valen diez mil palabras! Evitar el chartjunk se basa en parte en el concepto de brevedad en los principios de escritura vigorosa. Desde el punto de vista de la percepción gráfica, evitar el chartjunk reduce el ruido al comunicarse entre el emisor y el receptor del gráfico. Por lo tanto, esta directriz es importante porque tiene raíces tanto en los principios de escritura como en los de percepción. Ejemplo 21.3.1: Recibos de Primas de Compañías de Seguros de Vida. La Figura 21.7(a) es una adaptación de un gráfico de la página 69 del Life Insurance Fact Book (1994). El gráfico informa 15 piezas de información: 5 años y 2 porcentajes por cada año (un tercer porcentaje se encuentra por sustracción). Una caja tridimensional representa cada porcentaje, y cada caja muestra diferentes sombreados para representar las tres líneas de negocio: salud, anualidad y vida. Estas cifras podrían reportarse de manera compacta en una tabla pequeña. Sin embargo, considerando que un gráfico puede ayudar al receptor a apreciar las tendencias en las cifras, la simplicidad del gráfico debería reflejar la simplicidad de la información disponible en las cifras. En particular, un pequeño símbolo de trazado es suficiente para informar un porcentaje. Una caja tridimensional sombreada no es necesaria. Es interesante que la caja tridimensional fue una “innovación” en 1994. Las ediciones anteriores del Fact Book usaban cajas bidimensionales. El volumen de chartjunk dio un gran salto en 1994. Figura 21.7: Distribución de los Recibos de Primas, 1973-1993. El exceso de chartjunk en (a) oculta el gran cambio en los tipos de distribución entre 1983 y 1988. Código R para Generar la Figura 21.7 # Figura 217 img &lt;- png::readPNG(&quot;Chapters/Chapter21Graphs/3DBarChartNew.png&quot;) #img &lt;- png::readPNG(&quot;3DBarChartNew.png&quot;) Year &lt;- rep(seq(1973 , 1993, 5), each=3) PerCent &lt;- c(31.9, 13.9, 54.2, 32.8, 20.7, 46.5, 32.1, 25.7, 42.2, 22.8, 45.1, 32.1, 21.4, 49.0, 29.6) Line &lt;- rep(1:3,5) premium &lt;- data.frame(cbind(Year,PerCent,Line)) Line1 &lt;- subset(premium, Line==1) Line2 &lt;- subset(premium, Line==2) Line3 &lt;- subset(premium, Line==3) par(mfrow = c(1, 2), mar = c(8, 1, 0, 1)) # Panel derecho: Imagen desde archivo plot(1:2, type = &quot;n&quot;, ann = FALSE, axes = FALSE) # Gráfico vacío rasterImage(img, .9, .9, 2.0, 2.0) # Agregar leyenda debajo del primer gráfico mtext(&quot;(a) El gráfico de barras apiladas tridimensionales&quot;, side = 1, line = 4, cex = 0.9, adj = 0.1) mtext(&quot; es una forma gráfica pobre para realizar&quot;, side = 1, line = 5, cex = 0.9, adj = 0.1) mtext(&quot; comparaciones a lo largo del tiempo&quot;, side = 1, line = 6, cex = 0.9, adj = 0.1) dotchart(Line1$PerCent, xlab=&quot;Porcentaje de Ingresos de Primas&quot;, las=1, xaxt=NULL, xlim=c(10,60)) mtext(&quot;Año&quot;, side=2, at=6.3, las=1, cex=1.3, adj=1.5) par(new=TRUE) dotchart(Line2$PerCent, pch=3, xlab=NULL, xlim=c(10,60), lcolor = &quot;red&quot;) par(new=TRUE) dotchart(Line3$PerCent, pch=4, xlab=NULL, xlim=c(10,60)) axis(2, at=seq(1,5,1), labels=c(&quot;1973&quot;, &quot;1978&quot;,&quot;1983&quot;,&quot;1988&quot;,&quot;1993&quot;), las=1) legend(50,3.5, c(&quot;Salud&quot;, &quot;Anualidad&quot;, &quot;Vida&quot;), pch=c(1,3,4),cex=.8,y.intersp=1.5) mtext(&quot;(b) El gráfico de puntos permite comparaciones directas&quot;, side = 1, line = 4.5, cex = 0.9, adj = 0.0) mtext(&quot; a lo largo del tiempo y entre líneas de negocio.&quot;, side = 1, line = 5.5, cex = 0.9, adj = 0.0) La Figura 21.7(b) es un gráfico de puntos, discutido por Cleveland (1994). Diferentes símbolos de trazado muestran las diferentes líneas de negocio. Las marcas en el eje horizontal inferior nos ayudan a estimar los porcentajes, y las líneas de cuadrícula punteadas nos ayudan a escanear el gráfico para identificar los símbolos de interés. Los cambios importantes, y las magnitudes aproximadas de esos cambios, que ocurrieron entre 1983 y 1988 son claros aquí. Directriz Dos: Usar Múltiplos Pequeños para Fomentar Comparaciones y Evaluar Cambios El pensamiento estadístico se orienta hacia la comparación de mediciones de diferentes entidades y la evaluación del cambio de una medición a lo largo del tiempo u otra unidad de medición. Las representaciones gráficas están inherentemente limitadas al retratar comparaciones o evaluar cambios porque son medios estáticos y bidimensionales. Los gráficos que contienen múltiples versiones de una forma gráfica básica, cada versión retratando una variación del tema básico, fomentan comparaciones y evaluaciones de cambio. Al repetir una forma gráfica básica, promovemos el proceso de comunicación. Tufte (1997) afirma que usar múltiplos pequeños en representaciones gráficas logra los mismos efectos deseables que usar estructuras paralelas en la escritura. La estructura paralela en la escritura es exitosa porque permite a los lectores identificar una relación en una oración solo una vez y luego concentrarse en el significado de cada elemento individual de la oración, como una palabra, frase o cláusula. La estructura paralela ayuda a lograr economía de expresión y a vincular ideas relacionadas para comparación y contraste. De manera similar, los múltiplos pequeños en gráficos nos permiten visualizar relaciones complejas entre diferentes grupos y a lo largo del tiempo. La sección 21.2 ilustró el uso de múltiplos pequeños. En cada figura, los dos gráficos representados eran idénticos, excepto por el cambio en la escala; este uso de estructura paralela nos permitió demostrar la importancia de la escala al interpretar gráficos. El Ejemplo 21.3.2 ilustra otra aplicación de múltiplos pequeños en gráficos, el gráfico de puntos multi-vía de Cleveland (1993). Ejemplo 21.3.2: Importancia Relativa de las Fuentes de Riesgo. La Figura 21.8, llamada un gráfico de puntos multi-vía, demuestra conclusiones obtenidas utilizando un modelo introducido en Frees (1998) sobre la importancia relativa de las fuentes de riesgo dentro de un bloque de contratos de seguro a corto plazo. Las fuentes de riesgo son el entorno estocástico de intereses, la frecuencia de siniestros (mortalidad) y la posibilidad de un evento catastrófico (desastre). La importancia relativa de estas tres fuentes de riesgo se considera permitiendo que varíen dos parámetros de interés. Estos parámetros son el año esperado hasta el desastre y, en caso de desastre, la proporción esperada (probabilidad) de asegurados que sucumbirán al desastre. Figura 21.8: La Importancia Relativa de las Fuentes de Riesgo. Este gráfico complejo nos permite visualizar diferencias en las fuentes de riesgo (intereses, desastre y mortalidad), el año esperado hasta el desastre y la probabilidad de desastre. El gráfico de puntos multi-vía demuestra la rapidez con la que aumenta la importancia del componente de desastre a medida que aumenta la probabilidad de desastre. Código R para Generar la Figura 21.8 # Figura 218 risk &lt;- read.table(&quot;Chapters/Chapter21Graphs/Data/Chapter21Figure8data.txt&quot;, header=TRUE) defaultmar &lt;- c(1,9.2,3.2,8.2) par(mfrow=c(5,1)) q1s1 &lt;- subset(risk, q==1 &amp; Source==1) q1s2 &lt;- subset(risk, q==1 &amp; Source==2) q1s3 &lt;- subset(risk, q==1 &amp; Source==3) par(mar=defaultmar) dotchart(q1s1$Prop, xlim=c(0,100), pch=5) par(new=TRUE) dotchart(q1s2$Prop, xlim=c(0,100), pch=20) par(new=TRUE) dotchart(q1s3$Prop, xlim=c(0,100), pch=4) mtext(&quot;Año Esperado \\nHasta el Desastre&quot;, side=2, las=1, at=7.1, cex=.9, adj=0.95) axis(2, at=seq(1,3,1), labels=c(&quot;10&quot;, &quot;50&quot;, &quot;500&quot;), las=1) mtext(&quot;Prob de Desastre&quot;, side=2, at=3.3, las=1, cex=.6, adj=1.5) mtext(&quot;q = 0&quot;, side=2, at=1.5,las=1, cex=.7, adj=4) q2s1 &lt;- subset(risk, q==2 &amp; Source==1) q2s2 &lt;- subset(risk, q==2 &amp; Source==2) q2s3 &lt;- subset(risk, q==2 &amp; Source==3) dotchart(q2s1$Prop, xlim=c(0,100), pch=5) par(new=TRUE) dotchart(q2s2$Prop, xlim=c(0,100), pch=20) par(new=TRUE) dotchart(q2s3$Prop, xlim=c(0,100), pch=4) axis(2, at=seq(1,3,1), labels=c(&quot;10&quot;, &quot;50&quot;, &quot;500&quot;), las=1) mtext(&quot;q = 0.02&quot;, side=2, at=2,las=1, cex=.7, adj=2.5) q3s1 &lt;- subset(risk, q==3 &amp; Source==1) q3s2 &lt;- subset(risk, q==3 &amp; Source==2) q3s3 &lt;- subset(risk, q==3 &amp; Source==3) dotchart(q3s1$Prop, xlim=c(0,100), pch=5) par(new=TRUE) dotchart(q3s2$Prop, xlim=c(0,100), pch=20) par(new=TRUE) dotchart(q3s3$Prop, xlim=c(0,100), pch=4) axis(2, at=seq(1,3,1), labels=c(&quot;10&quot;, &quot;50&quot;, &quot;500&quot;), las=1) mtext(&quot;q = 0.20&quot;, side=2, at=2,las=1, cex=.7, adj=2.5) q4s1 &lt;- subset(risk, q==4 &amp; Source==1) q4s2 &lt;- subset(risk, q==4 &amp; Source==2) q4s3 &lt;- subset(risk, q==4 &amp; Source==3) dotchart(q4s1$Prop, xlim=c(0,100), pch=5) par(new=TRUE) dotchart(q4s2$Prop, xlim=c(0,100), pch=20) par(new=TRUE) dotchart(q4s3$Prop, xlim=c(0,100), pch=4) axis(2, at=seq(1,3,1), labels=c(&quot;10&quot;, &quot;50&quot;, &quot;500&quot;), las=1) mtext(&quot;q = 1.0&quot;, side=2, at=2,las=1, cex=.7, adj=3) mtext(&quot;Proporción de Variabilidad de los Montos de Reclamos&quot;, cex=.9, side=1, line=2.8) # LEYENDA DE LA FIGURA par(mar=c(0,21.5,0,0), cex=1.3) plot(.2,.6,xlim=c(.15,.7), pch=5, ylim=c(0,.9), axes=FALSE, ann = FALSE) par(new=TRUE) plot(.2,.4, xlim=c(.15,.7), pch=19, ylim=c(0,.9), axes=FALSE, ann = FALSE) par(new=TRUE) plot(.2,.2, xlim=c(.15,.7), pch=4, ylim=c(0,.9), axes=FALSE, ann = FALSE) text(.5,.6,&quot;Interés&quot;, cex=.7) text(.5,.4,&quot;Desastre&quot;, cex=.7) text(.5,.2,&quot;Mortalidad&quot;, cex=.7) La Figura 21.8 muestra que cuando ningún asegurado sucumbe a un desastre (\\(q = 0\\)), el componente de frecuencia, mortalidad, domina las otras fuentes de riesgo. En el extremo opuesto, cuando todos los asegurados sucumben a un desastre (\\(q = 1\\)), el componente de desastre domina los otros factores de riesgo. Esto es cierto incluso cuando el tiempo esperado hasta el desastre es de 500 años. Para los casos intermedios, ya sea que aumente la proporción esperada de asegurados que sucumben al desastre o que disminuya el tiempo esperado hasta el desastre, la importancia del componente de desastre aumenta a expensas del componente de mortalidad. Debido a la naturaleza a corto plazo del contrato considerado, el componente de interés no juega un papel importante en la Figura 21.8. Esta historia sobre la importancia relativa no podría contarse utilizando expresiones analíticas debido a la complejidad de los modelos subyacentes. Sin embargo, la historia detrás de la Figura 21.8 podría contarse usando tablas. La ventaja de la Figura 21.8 es que permite al espectador realizar comparaciones sobre tres diferentes fuentes de riesgo mientras dos parámetros de interés varían. Aunque dichas comparaciones son posibles con tablas, los gráficos son dispositivos más efectivos. Directriz Tres: Utilice Gráficos Complejos para Representar Patrones Complejos Muchos autores creen que un gráfico debe ser simple y comprendido de inmediato por el espectador. Los gráficos simples son deseables porque pueden transmitir su mensaje a una amplia audiencia y pueden mostrarse rápidamente y ser entendidos de inmediato. Aunque esta noción puede ser apropiada para la escritura popular, en la escritura profesional el concepto de comprensión instantánea es limitante, ya que excluye la idea de que los gráficos demuestren ideas complejas. Los patrones complejos deben representarse de la manera más simple posible, aunque los patrones en sí no deben simplificarse innecesariamente. Una forma en que un gráfico puede representar patrones complejos es permitiendo que algunos de sus elementos básicos cumplan más de un propósito. Tufte (1983) llamó a estos elementos multifuncionales. Por ejemplo, podemos usar símbolos de representación no solo para elementos correspondientes a las escalas horizontal y vertical, sino también para un nivel de una variable categórica. Ejemplo 21.3.3: Frecuencia y Severidad de los Costos Hospitalarios. La Figura 21.9 muestra la relación entre los costos hospitalarios promedio y la frecuencia de uso hospitalario. Estos datos corresponden al año 1989 y fueron obtenidos de la Oficina de Información de Atención Médica del Departamento de Servicios de Salud y Bienestar de Wisconsin, y se analizan más a fondo en la Sección 4.4. Los datos representan promedios en el estado de Wisconsin, desglosados por nueve áreas de servicio de salud, tres tipos de proveedores (pago por servicio, organización de mantenimiento de la salud y otros) y tres tipos de grupos relacionados con diagnósticos (GRDs). Los tres GRDs, números 209, 391 y 430, representan reparación mayor de articulaciones y miembros, recién nacidos normales y psicosis, respectivamente. Cada símbolo de representación en la Figura 21.9 representa una combinación de área de servicio de salud, tipo de pagador y tipo de GRD. El eje horizontal proporciona el número de pacientes admitidos en 1989 para cada combinación, en unidades logarítmicas naturales. La escala vertical muestra el costo promedio hospitalario por alta para cada combinación, también en unidades logarítmicas naturales. La historia en el panel izquierdo, Figura 21.9(a), es una de economías de escala crecientes. Es decir, las combinaciones de áreas de servicio de salud, tipo de pagador y GRD que tienen un mayor número de pacientes, medido por altas, tienen costos más bajos. En la Figura 21.9(a) se observa una relación negativa sustancial; el coeficiente de correlación es -0.43. Esto es cierto a pesar del punto aberrante en la región inferior izquierda de la Figura 21.9(a). El punto aberrante es menos importante económicamente que los demás, ya que representa una combinación con solo dos altas. Cuando se elimina este punto, la correlación pasa a ser -0.50, representando así una relación negativa aún más fuerte. Figura 21.9: Costo Logarítmico por Alta versus Número Logarítmico de Altas. Al agregar un código de símbolo de trazado para el nivel de GRD, se evidencian tres grupos distintos. Los tres GRDs, 209, 391 y 430, representan reparación mayor de articulaciones y extremidades, recién nacidos normales y psicosis, respectivamente. Código R para Generar la Figura 21.9 # Figura 219 discharge &lt;- read.table(&quot;Chapters/Chapter21Graphs/Data/Chapter21Figure9data.txt&quot;, header=TRUE) par(mfrow=c(1, 2), cex=0.8, mar=c(8.5, 3, 2.2, 2)) discharge$lnNUM= log(discharge$NO_DSCHG) discharge$lnCOST= log(discharge$CHG_NUM) plot(discharge$lnNUM, discharge$lnCOST, xlim=c(0,9), ylim=c(6,10), xlab=&quot;Número Logarítmico de Altas&quot;, las=1, ylab=&quot;&quot;, cex=.8) axis(1, at=seq(1,9,2), labels=c(&quot;1&quot;, &quot;3&quot;, &quot;5&quot;, &quot;7&quot;, &quot;9&quot;)) mtext(&quot;Costo Logarítmico \\n Por Alta&quot;, side=2, las=1, at=10.55, adj=.4, cex=0.8) # Añadir subtítulo debajo del primer gráfico mtext(&quot;(a) Con la excepción de una observación atípica \\nen la región inferior izquierda, parece haber \\n una relación negativa significativa entre \\n el costo y el número de altas hospitalarias.&quot;, side = 1, line = 7.5, cex = 0.8, adj = 0.02) # Segundo gráfico plot(discharge$lnNUM, discharge$lnCOST, xlim=c(0,9), ylim=c(6,10), xlab=&quot;Número Logarítmico de Altas&quot;, las=1, ylab=&quot;&quot;, pch=as.numeric(discharge$DRGCODE), cex=.8) axis(1, at=seq(1,9,2), labels=c(&quot;1&quot;, &quot;3&quot;, &quot;5&quot;, &quot;7&quot;, &quot;9&quot;)) mtext(&quot;Costo Logarítmico \\n Por Alta&quot;, side=2, las=1, at=10.55, adj=.4, cex=0.8) legend(0.1,8.1, c(&quot;209&quot;, &quot;391&quot;, &quot;430&quot;), pch=1:3, cex=.70, y.intersp=1.5, text.width=1.1) # Añadir subtítulo debajo del segundo gráfico mtext(&quot;(b) Al introducir los códigos GRD, vemos una \\npequeña relación positiva entre el costo y el \\n número de altas hospitalarias dentro de \\ncada grupo.&quot;, side = 1, line = 7.5, cex = 0.8, adj = 0.01) A pesar de su simplicidad, la Figura 21.9(a) oculta una relación importante. El panel derecho, Figura 21.9(b), es una reinterpretación de la Figura 21.9(a) que incluye diferentes símbolos para diferentes GRDs. Aquí, la historia es opuesta a la de las economías de escala crecientes. Para combinaciones que representan reparaciones mayores de articulaciones y extremidades y recién nacidos normales, la relación entre frecuencia y costo es bastante plana. Para estos GRDs hay pocas economías de escala. Para el GRD de psicosis, número 430, la Figura 21.9(b) muestra una pequeña relación positiva entre frecuencia y costo, incluso descontando la combinación con solo dos pacientes dados de alta. Los dos paneles ilustran un fenómeno en estadística conocido como la paradoja de Simpson, o un problema de agregación de datos. Consulte la Sección 4.4 para más discusión. El punto importante para este capítulo es que, a veces, los gráficos simples son engañosos. Los gráficos complejos pueden requerir más tiempo para que los espectadores los interpreten, pero resumen de manera más efectiva las relaciones complejas. Directriz Cuatro: Relacione el Tamaño del Gráfico con el Contenido Informativo “La pregunta de qué tan grande debe ser el gráfico” es importante. Los límites de tamaño son claros. Los gráficos no deben ser tan pequeños que no sean claramente legibles, especialmente después de reproducciones que degraden la imagen, ni deben ser tan grandes que excedan una página. Los gráficos grandes dificultan la comparación de elementos dentro del gráfico, lo que frustra un propósito principal de los gráficos. Dentro de estos límites, un gráfico debe ser proporcional a la cantidad de información que contiene. Para discutir la proporción de contenido informativo, Tufte (1983) introdujo la densidad de datos de un gráfico. Esto se define como el número de entradas de datos por unidad de área del gráfico. Para comparar el tamaño del gráfico y la información, la densidad de datos es una cantidad que se debe maximizar, ya sea aumentando el número de entradas de datos o reduciendo el tamaño del gráfico. Al examinar esta densidad en varias publicaciones populares, Tufte concluyó que la mayoría de los gráficos podrían reducirse de manera efectiva. Por ejemplo, la Figura 21.7(a) es un gráfico con una baja densidad de datos. Este gráfico representa solo 15 números. Con un área de aproximadamente 9 pulgadas cuadradas, la densidad de datos de este gráfico es aproximadamente 15/9. En comparación, la Figura 21.10 muestra aproximadamente 600 números. Aunque el área de la Figura 21.10 es aproximadamente el doble que la de la Figura 21.7(a), la densidad de datos es mucho mayor en la Figura 21.10 que en la Figura 21.7(a). Figura 21.10: Comparación de Intervalos de Predicción Estocásticos con Experiencia Real y Suposiciones de la Seguridad Social. Las líneas sólidas delgadas representan las tasas de inflación reales, y las líneas sólidas gruesas representan las proyecciones de expertos de la Seguridad Social. Las líneas punteadas representan intervalos de predicción generados por un modelo de series temporales estocástico. Este gráfico complejo permite a los espectadores realizar comparaciones basadas en aproximadamente 600 puntos. Código R para Generar la Figura 21.10 # Figura 2110 inflationa &lt;- read.table(&quot;Chapters/Chapter21Graphs/Data/Chapter21Figure10adata.txt&quot;, header=TRUE) inflationb &lt;- read.table(&quot;Chapters/Chapter21Graphs/Data/Chapter21Figure10bdata.txt&quot;, header=TRUE) inflationc &lt;- read.table(&quot;Chapters/Chapter21Graphs/Data/Chapter21Figure10cdata.txt&quot;, header=TRUE) par(mar=c(4.1,2,2,2.8), cex=1.3) plot(inflationa$time1, y=inflationa$obser, xaxt=&quot;n&quot;, xlab=&quot;Año&quot;, ylab=&quot;&quot;, las=1, xlim=c(1975, 2005), type=&quot;l&quot;, bty=&quot;l&quot;) mtext(&quot;Inflación&quot;, side=2, las=1, at=4.5, cex=1.6, adj=.5) axis(1, at=seq(1975,2005,10)) par(new=TRUE) plot(inflationb$time2, inflationb$low, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlim=c(1975,2005), type=&quot;l&quot;, lwd=2, ylim=c(-.6,4), bty=&quot;l&quot;) par(new=TRUE) plot(inflationb$time2, inflationb$high, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlim=c(1975,2005), type=&quot;l&quot;, lwd=2, ylim=c(-.6,4), bty=&quot;l&quot;) par(new=TRUE) plot(inflationb$time2, inflationb$intermed, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlim=c(1975,2005), type=&quot;l&quot;, lwd=2, ylim=c(-.6,4), bty=&quot;l&quot;) par(new=TRUE) plot(inflationc$time3, inflationc$X5.ile, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlim=c(1975,2005), type=&quot;l&quot;, ylim=c(-.6,4), lty=6, bty=&quot;l&quot;) par(new=TRUE) plot(inflationc$time3, inflationc$X95.ile, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlim=c(1975,2005), type=&quot;l&quot;, ylim=c(-.6,4), lty=6, bty=&quot;l&quot;) par(new=TRUE) plot(inflationc$time3, inflationc$X15.ile, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlim=c(1975,2005), type=&quot;l&quot;, ylim=c(-.6,4), lty=6, bty=&quot;l&quot;) par(new=TRUE) plot(inflationc$time3, inflationc$X85.ile, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlim=c(1975,2005), type=&quot;l&quot;, ylim=c(-.6,4), lty=6, bty=&quot;l&quot;) par(new=TRUE) plot(inflationc$time3, inflationc$X25.ile, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlim=c(1975,2005), type=&quot;l&quot;, ylim=c(-.6,4), lty=6, bty=&quot;l&quot;) par(new=TRUE) plot(inflationc$time3, inflationc$X75.ile, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlim=c(1975,2005), type=&quot;l&quot;, ylim=c(-.6,4), lty=6, bty=&quot;l&quot;) par(new=TRUE) plot(inflationc$time3, inflationc$X35.ile, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlim=c(1975,2005), type=&quot;l&quot;, ylim=c(-.6,4), lty=6, bty=&quot;l&quot;) par(new=TRUE) plot(inflationc$time3, inflationc$X65.ile, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlim=c(1975,2005), type=&quot;l&quot;, ylim=c(-.6,4), lty=6, bty=&quot;l&quot;) par(new=TRUE) plot(inflationc$time3, inflationc$X45.ile, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlim=c(1975,2005), type=&quot;l&quot;, ylim=c(-.6,4), lty=6, bty=&quot;l&quot;) par(new=TRUE) plot(inflationc$time3, inflationc$X55.ile, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlim=c(1975,2005), type=&quot;l&quot;, ylim=c(-.6,4), lty=6, bty=&quot;l&quot;) par(cex=1.1,las=1) mtext(&quot;90% P.I.&quot;, side=4, at=2.6) mtext(&quot;70% P.I.&quot;, side=4, at=2) mtext(&quot;50% P.I.&quot;, side=4, at=1.65) mtext(&quot;30% P.I.&quot;, side=4, at=1.4) mtext(&quot;10% P.I.&quot;, side=4, at=1.2) Ejemplo 21.3.4: Pronósticos de la Tasa de Inflación. La Figura 21.10 es un gráfico complejo que contiene mucha información sobre un tema complejo, la previsión de la tasa de inflación (CPI) para las proyecciones de los fondos de la Seguridad Social (Frees et al. 1997). El gráfico muestra la experiencia real de las tasas trimestrales de inflación hasta el primer trimestre de 1995. La experiencia hasta 1992 se utilizó para ajustar un modelo de series temporales descrito en Frees et al. (1997), y este modelo se utilizó para generar intervalos de predicción (PIs) de la tasa de inflación. Estos intervalos de predicción pueden compararse con datos retenidos que no se usaron para ajustar el modelo (1993-1995), así como con las proyecciones de inflación de los expertos en Seguridad Social. Las líneas gruesas representan las proyecciones de inflación de alto, intermedio y bajo costo determinadas por los expertos en Seguridad Social. La Figura 21.10 es compleja y puede no ser comprendida de inmediato por el espectador. Sin embargo, casi cada trazo dentro de la región de datos representa información numérica. Aunque es compleja, la Figura 21.10 permite al espectador comparar (1) 20 años de experiencia con un pronóstico de 10 años, (2) datos recientes retenidos con los pronósticos y (3) proyecciones de expertos con pronósticos generados por un modelo de series temporales. La complejidad del gráfico refleja la complejidad de pronosticar tasas de inflación; esta complejidad no se debe a elementos innecesarios que distraen a los espectadores y los hacen más “interesados” en el gráfico. Directriz Cinco: Utilizar Formas Gráficas que Promuevan Comparaciones Los creadores de gráficos a menudo enfrentan la elección de varias formas gráficas que podrían usarse para representar una característica de los datos. Como describimos en la Directriz Ocho, el conocimiento del receptor sobre formas gráficas puede influir en la elección. La percepción gráfica también es un determinante importante. En la Sección 21.4, discutimos este tema en detalle. Lo incluimos aquí como parte de la Sección de Directrices para completar. Directriz Seis: Integrar Gráficos y Texto Los gráficos de datos deben integrarse cuidadosamente con texto, tablas y otros gráficos. Una leyenda resume el gráfico y su mensaje principal, pero el texto circundante desarrolla el tema que lleva al mensaje y discute su impacto. Aunque “una imagen vale más que mil palabras,” un gráfico necesita texto de apoyo. Tufte (1983) alienta a los lectores y escritores a pensar en los gráficos de datos como párrafos y a tratarlos como tales. Los gráficos de datos pueden complementarse con una presentación tabular de datos: los gráficos pueden resaltar relaciones entre los datos, y las tablas pueden presentar descripciones numéricas precisas de los datos. Los dos modos son complementarios. Un buen dispositivo de escritura es colocar una presentación gráfica en el cuerpo principal del informe y reforzar el gráfico con una presentación tabular en un apéndice. La Asociación Estadounidense de Estadística, en su Guía de Estilo para publicaciones en revistas, nos recuerda que una leyenda detallada es útil para interpretar gráficos. La Guía de Estilo recomienda que una leyenda describa un gráfico, llame la atención sobre las características importantes del gráfico y explique su importancia. Directriz Siete: Demostrar un Mensaje Importante Las leyendas detalladas y los gráficos deben reforzar mensajes que se desarrollan en el cuerpo principal del texto. Para ilustrar, al considerar formas de representar un conjunto de datos complejo, elija una forma gráfica que destaque un mensaje importante. Con demasiada frecuencia, los creadores de gráficos muestran características de los datos que no forman parte del tema que se está desarrollando. Cleveland (1994) recomienda que “pongamos las conclusiones principales en una forma gráfica.” En el análisis de datos de regresión, las conclusiones principales se refieren a patrones en los datos que se resumen utilizando modelos. Generalmente, las conclusiones principales se presentan mejor de forma gráfica. Los gráficos muestran una gran cantidad de información que es retenida por el espectador porque se visualiza. Los gráficos comunican patrones directamente al espectador, sin usar una ecuación para representar los patrones. De esta manera, se puede llegar a una audiencia más amplia que si la presentación dependiera únicamente de una interpretación basada en modelos de los datos. Además, los patrones sugeridos por un gráfico refuerzan aquellos representados por un modelo, y viceversa. Así, las dos herramientas, gráficos y modelos, se refuerzan y fortalecen mutuamente. Tukey (1977) afirma que “el mayor valor de una imagen es cuando nos obliga a notar lo que nunca esperábamos ver.” Los fenómenos inesperados suelen ser eventos memorables; los espectadores de gráficos recuerdan estos resultados, lo que los hace poderosos. Al escribir este capítulo, no esperábamos los resultados de la Figura 21.6. Esta figura demuestra que el desempleo aumenta mucho más rápido de lo que disminuye; es un poderoso ejemplo del uso de relaciones de aspecto. Directriz Ocho: Conozca a su Audiencia Un precepto básico de la escritura efectiva, la familiaridad con la audiencia, también es válido para diseñar gráficos efectivos. Como se indicó en la Introducción, nuestra principal motivación al desarrollar directrices es fomentar la comunicación precisa y concisa de ideas cuantitativas a una audiencia científica utilizando un medio escrito. Como se discute en la Sección 21.4, la forma gráfica está subordinada al papel real de la presentación gráfica, comunicar ideas cuantitativas del creador al espectador de un gráfico. Si la audiencia no comprende la forma gráfica, entonces esta forma dificultará el flujo de comunicación en lugar de facilitarlo. Por lo tanto, cada una de las siete directrices ya discutidas puede modificarse o incluso ignorarse en ocasiones, dependiendo de la audiencia del gráfico. Para ilustrar, en el Ejemplo 21.3.1 argumentamos que el gráfico de puntos era superior al gráfico de barras apiladas tridimensional. Como otro ejemplo, en la Sección 21.4 argumentamos que los gráficos de pastel son comunicadores ineficaces de información según la ciencia de la percepción cognitiva. Sin embargo, para algunas audiencias, los creadores de gráficos preferirán las formas menos efectivas según el nivel de familiaridad de la audiencia. Esperamos que la práctica eventualmente cambie de estos modos de comunicación ineficaces. Aun así, es importante reconocer los antecedentes de la audiencia del gráfico. Recomendamos que los creadores de gráficos no naden tanto contra la corriente del mal diseño gráfico como que ajusten su curso hacia modos de comunicación más efectivos. 21.4 Fundamentos Empíricos para las Directrices Esta sección consiste en dos aspectos científicos diferentes de los estudios gráficos: la ciencia de la percepción y encuestas sobre la práctica gráfica. Este capítulo no incluye varias formas gráficas que son comunes en publicaciones empresariales y en la prensa popular, como gráficos de pastel, pictogramas y gráficos de barras apiladas. De hecho, hemos mostrado gráficos de barras apiladas en la Sección 21.3 solo como un ejemplo de cómo no dibujar figuras. ¿Por qué estas formas gráficas ampliamente utilizadas no son adoptadas en un capítulo que enfatiza los gráficos de datos? Las razones radican en cómo las formas gráficas comunican información y cómo percibimos la información gráfica. Demostramos que, dada nuestra percepción de la información, los gráficos de pastel y las barras apiladas son comunicadores deficientes de información numérica. Como se describe en la Sección 21.1, los gráficos de datos codifican información, y nosotros, como espectadores, desciframos esta información al visualizar un gráfico. La eficiencia de esta transmisión puede considerarse en el contexto de la psicología cognitiva, la ciencia de la percepción. Esta disciplina proporciona un marco para distinguir entre diferentes tipos de procesamiento de información que hacemos al descifrar gráficos. Identificar diferentes tipos de procesamiento de información nos ayudará a decidir qué formas gráficas son efectivas y cuáles no lo son. Tabla 21.1 es una lista ordenada de tareas básicas de percepción gráfica, según Cleveland (1994). Aquí, el orden comienza con un conjunto de tareas que es menos difícil para un espectador realizar y termina con un conjunto que es más difícil. Así, por ejemplo, juzgar la posición a lo largo de una escala común es lo menos difícil para los espectadores, y juzgar las sombras relativas de colores y densidad (la cantidad de tinta) es lo más difícil. Table 21.1. Tareas Básicas de Percepción Gráfica \\[ \\small{ \\begin{array}{l}\\hline \\text{1. Posición a lo largo de una escala común} \\\\ \\text{2. Posición a lo largo de escalas idénticas no alineadas} \\\\ \\text{3. Longitud} \\\\ \\text{4. Ángulos y pendientes} \\\\ \\text{5. Área} \\\\ \\text{6. Volumen} \\\\ \\text{7. Color y densidad} \\\\ \\hline \\end{array} } \\] Para comprender la dificultad relativa de las tareas, Cleveland y McGill (1984) realizaron una serie de pruebas en muchos sujetos experimentales. Para ilustrar, las Figuras 12.11(a)-(e) presentan una serie de pruebas que son análogas a las primeras cinco tareas. Cleveland y McGill resumieron el desempeño de los sujetos experimentales calculando la precisión con la que realizaron cada conjunto de tareas. A través de estas medidas de precisión relativa y argumentos de la psicología cognitiva, Cleveland y McGill desarrollaron el orden presentado en Tabla 21.1. Figura 21.11(a). Experimento para Juzgar la Posición a lo Largo de una Escala Común. Evaluar los valores relativos de A, B, C y D a lo largo de esta escala de 100 puntos. Figura 21.11(b). Experimento para Juzgar la Posición en Escalas Idénticas no Alineadas. Evalúe los valores relativos de A, B, C y D en una escala común de 100 puntos. Figura 21.11(c). Experimento para Entender Juicios de Longitud. Suponga que la línea A tiene 100 unidades de longitud. Evalúe las longitudes relativas de las líneas B, C y D. Figura 21.11(d). Experimento para Entender Juicios de Ángulos. Suponga que el ángulo A tiene 100 unidades. Evalúe los valores relativos de los ángulos B, C y D. Figura 21.11(e). Experimento para Entender Juicios de Áreas. Suponga que el círculo A tiene un área de 100 unidades. Evalúe las áreas relativas de los círculos B, C y D. Este capítulo no discute el uso de colores debido a las complejidades para codificarlos y decodificarlos de manera efectiva. Remitimos a los lectores interesados a Cleveland (1994, Sección 3.13) y Tufte (1990, Capítulo 5) para obtener más información. La lista ordenada de tareas de percepción gráfica puede ayudar al creador a elegir la forma gráfica adecuada para representar un conjunto de datos. Cuando se enfrenta a la elección entre dos formas gráficas, un creador debe seleccionar la que sea menos difícil para el espectador. En igualdad de condiciones, una tarea que el espectador puede realizar con menor dificultad significa que la información puede transmitirse de manera más confiable. Para ilustrar esto, discutimos dos ejemplos en los que Tabla 21.1 puede ayudar a decidir la forma gráfica adecuada para representar un conjunto de datos. Ejemplo 21.4.1: Distribución de los Ingresos por Primas. El primer ejemplo demuestra algunas limitaciones del gráfico de barras apiladas. Para esta discusión, volvemos al Ejemplo 21.3.1. La Figura 21.7(a) es un gráfico de barras apiladas tridimensional. Ya hemos discutido la cantidad considerable de elementos innecesarios en esta figura. Incluso sin la dimensión pseudo tridimensional, el gráfico de barras apiladas requiere que el espectador haga juicios de longitud para entender, por ejemplo, la distribución de los ingresos por anualidades a lo largo del tiempo. En contraste, el gráfico de puntos en la Figura 21.7(b) requiere que el espectador haga comparaciones solo en función de posiciones a lo largo de una escala común. Como se describe en la Tabla 21.1, esta última es una tarea más fácil, lo que resulta en información más confiable para el espectador. Por lo tanto, concluimos que el gráfico de puntos es preferible al gráfico de barras apiladas. Ejemplo 21.4.2: Distribución de Hipotecas. Nuestro segundo ejemplo demuestra la insuficiencia de los gráficos de pastel. La Figura 21.12 es una adaptación de la figura en la página 100 del Life Insurance Fact Book (1994). Representa, para los años 1973, 1983 y 1993, hipotecas comerciales, de 1 a 4 familias y agrícolas como porcentajes del total de hipotecas. Los gráficos de pastel dificultan las comparaciones. Por ejemplo, el gráfico dificulta detectar si las hipotecas agrícolas son más prevalentes que las hipotecas de 1 a 4 familias en 1983, o si los porcentajes de hipotecas agrícolas aumentaron o disminuyeron de 1973 a 1983. La comparación de porcentajes a lo largo de los años es una operación lineal, pero los gráficos de pastel requieren que decodifiquemos ángulos, una tarea difícil según el orden presentado en la Tabla 21.1. Como en el Ejemplo 21.3.1, los gráficos en la Figura 21.12 empeoran las cosas al presentarse en tres dimensiones; estas figuras no solo requieren que decodifiquemos volúmenes, sino que también aumentan considerablemente los elementos innecesarios en el gráfico. Solo nueve números se reportan en este gráfico, tres años y dos porcentajes en cada año. (El tercer porcentaje puede calcularse por sustracción). Figura 21.11: silly Figura 21.12: Distribución de Hipotecas para los Años 1973, 1983 y 1993. El gráfico de pastel tridimensional es una forma gráfica deficiente para hacer comparaciones a lo largo del tiempo y entre tipos de hipotecas. Código R para Generar la Figura 21.12 # Figura 2112 library(plotrix) x1973 &lt;- c(0.675, 0.249, 0.076) x1983 &lt;- c(0.813, 0.101, 0.086) x1993 &lt;- c(0.917,0.042,0.041) namesx &lt;- c(&quot;Comerciales&quot;,&quot;1-4 Familias&quot;,&quot;Agrícolas&quot; ) par(cex=1.3, mar=c(1, 0, 1, 0) + 0.1) pie3D(x1973, start=pi/2, col=color.scale(x1973,c(1,1,1),c(1,1,1),c(1,0,1))) text(-0.8,1.45, &quot;1973&quot;, cex =2.0) text(0.1,0.95, &quot;Agrícolas - 8 %&quot;, cex =1.6) text(1.3,0.5, &quot;1-4 Familias&quot;, cex =1.3) text(1.3,0.2, &quot;25 %&quot;, cex =1.3) text(-0.01,-1.1, &quot;Comerciales - 67 %&quot;, cex =1.6) pie3D(x1983, start=pi/2, col=color.scale(x1983,c(1,1,1),c(1,1,1),c(1,0,1))) text(-0.8,1.45, &quot;1983&quot;, cex =2.0) text(0.1,0.95, &quot;Agrícolas - 9 %&quot;, cex =1.6) text(1.2,0.5, &quot;1-4 Familias&quot;, cex =1.3) text(1.3,0.2, &quot;10 %&quot;, cex =1.3) text(-0.01,-1.1, &quot;Comerciales - 81 %&quot;, cex =1.6) pie3D(x1993, start=pi/2, col=color.scale(x1993,c(1,1,1),c(1,1,1),c(1,0,1))) text(-0.8,1.45, &quot;1993&quot;, cex =2.0) text(0.1,0.95, &quot;Agrícolas - 4 %&quot;, cex =1.6) text(1.2,0.5, &quot;1-4 Familias&quot;, cex =1.3) text(1.3,0.2, &quot; 4 %&quot;, cex =1.3) text(-0.01,-1.1, &quot;Comerciales - 92 %&quot;, cex =1.6) Si se necesita un gráfico, entonces el gráfico de puntos en la Figura 21.13 es más que suficiente. Aquí, las comparaciones se realizan según posiciones a lo largo de una escala común, una tarea más sencilla que comparar ángulos. Los gráficos de pastel requieren que hagamos comparaciones utilizando ángulos, que son más difíciles y menos confiables que las comparaciones usando otras formas gráficas. Figura 21.13: Hipotecas Comerciales, de 1-4 Familias y Agrícolas como Porcentajes del Total de Hipotecas para 1973, 1983 y 1993. Un aspecto negativo de este gráfico es la superposición de los símbolos de trazado para las hipotecas de 1-4 familias y las agrícolas en 1983 y 1993. Código R para Generar la Figura 21.13 # Figura 2113 mortgage &lt;- read.table(&quot;Chapters/Chapter21Graphs/Data/Chapter21Figure13data.txt&quot;, header=TRUE) Comm &lt;- subset(mortgage, Type==1) Fam &lt;- subset(mortgage, Type==2) Farm &lt;- subset(mortgage, Type==3) par(mar=c(4.2,3.1,1.2,.1), cex=1.3) dotchart(Comm$PerCent, xlab=&quot;Porcentaje de Hipotecas&quot;, las=1, xaxt=NULL, xlim=c(0,100)) mtext(&quot;Año&quot;, side=2, at=4.1, las=1, cex=1.3, adj=1.2) axis(1, at=seq(10,90,10)) par(new=TRUE) dotchart(Fam$PerCent, pch=3, xlab=NULL, xlim=c(0,100)) axis(1, at=seq(10,90,10)) par(new=TRUE) dotchart(Farm$PerCent, pch=4, xlab=NULL, xlim=c(0,100)) axis(2, at=seq(1,3,1), labels=c(&quot;1973&quot;,&quot;1983&quot;,&quot;1993&quot;), las=1) axis(1, at=seq(10,90,10)) legend(40,4.0, c(&quot;Comercial&quot;, &quot;1-4 Familias&quot;, &quot;Agrícola&quot;), pch=c(1,3,4),cex=1, y.intersp=1.1, text.width=25) Tabla A.2. Hipotecas Comerciales, de 1-4 Familias y Agrícolas como Porcentajes del Total de Hipotecas para 1973, 1983, 1993 \\[ \\small{ \\begin{array}{l|rrr} \\hline &amp; \\text{Año}\\\\ \\hline \\text{Tipo de Hipoteca} &amp; 1973 &amp; 1983 &amp; 1993 \\\\ \\hline \\text{Comercial} &amp; 67.5 &amp; 81.3 &amp; 91.7 \\\\ \\text{1-4 Familias} &amp; 24.9 &amp; 10.1 &amp; 4.1 \\\\ \\text{Agrícola} &amp; 7.6 &amp; 8.6 &amp; 4.2 \\\\ \\hline \\end{array} } \\] Aunque la Figura 21.13 es un gráfico más efectivo que la Figura 21.12, para estos datos recomendamos una presentación tabular (Tabla A.2), que permite comparaciones claras entre tipos de hipotecas y años. Además, la Tabla A.2 ofrece información más detallada sobre los porcentajes de hipotecas que las Figuras 21.12 o 21.13. Por supuesto, siempre podemos superponer los porcentajes reales, como se hace a menudo con gráficos de pastel y como se ilustra en la Figura 21.12. Nuestra respuesta a este enfoque es cuestionar la utilidad del gráfico completo. Como en la escritura, ¡cada trazo debería aportar nueva información; que los creadores de gráficos hagan que cada trazo cuente! 21.4.1 Gráficos como Unidades de Estudio Los estudios de prácticas gráficas en publicaciones profesionales proporcionan una base de datos importante para evaluar la prevalencia de buenas y malas prácticas, así como los cambios en estas prácticas a lo largo del tiempo. Tufte (1983, pp. 82-86) discute una encuesta de aproximadamente 4,000 gráficos seleccionados al azar de 15 publicaciones de noticias entre los años 1974 y 1980. Los gráficos se evaluaron en términos de “sofisticación”, definida como la presentación de relaciones entre variables, excluyendo series de tiempo o mapas. Cleveland y McGill (1985) reportan una encuesta similar en publicaciones científicas, evaluando la prevalencia de errores gráficos. Harbert (1995) evaluó cada gráfico y tabla en las ediciones de 1993 de cuatro revistas de psicología utilizando 34 medidas de calidad. Las medidas de calidad se obtuvieron de la literatura de investigación actual sobre calidad gráfica. Estas medidas se convirtieron en una hoja de verificación, y una hoja de verificación fue completada para cada gráfico y tabla en las revistas de psicología seleccionadas. El estudio de Harbert generó datos sobre 439 gráficos y tablas. Resumimos el análisis de los 212 gráficos. Harbert asignó calificaciones a los gráficos: A, AB, B, BC, C, CD, D, DF y F. Estas calificaciones reflejaron su evaluación general de los gráficos como comunicadores de información estadística. Las calificaciones fueron convertidas a valores numéricos: 4.0, 3.5, 3.0, 2.5, 2.0, 1.5, 1.0, 0.5 y 0.0. Los valores numéricos fueron la variable dependiente en una regresión. Las variables independientes fueron las 34 medidas de calidad, codificadas de manera adecuada. El propósito del estudio fue determinar qué factores eran predictores estadísticamente significativos de las calificaciones asignadas por un evaluador “experto” de gráficos. Por ensayo y error, Harbert seleccionó una ecuación de regresión lineal múltiple en la que todos los predictores eran estadísticamente significativos (nivel del 5%) y ningún otro predictor alcanzó este nivel de significancia cuando se añadió a la ecuación. La Tabla A.3 muestra las variables incluidas en la ecuación de regresión (\\(R^2 = 0.612\\)). Tabla A.3. Factores que Afectan la Evaluación de la Calidad Gráfica, Estudio de Harbert \\[ \\small{ \\begin{array}{ll} \\hline \\text{Variables con} &amp; \\text{Variables con}\\\\ \\text{Coeficientes Positivos} &amp; \\text{Coeficientes Negativos} \\\\ \\hline\\text{ Proporción de tinta de datos} &amp; \\text{Proporción de la página utilizada por el gráfico}\\\\ \\text{Comparaciones facilitadas} &amp;\\text{Etiquetas verticales en el eje Y} \\\\ \\text{Datos suficientes para} &amp;\\text{Uso de abreviaturas} \\\\ ~~~~\\text{un gráfico rico}&amp; \\text{Uso de arte óptico} \\\\ &amp;\\text{Comparaciones usando áreas o volúmenes} \\\\ \\hline \\end{array} } \\] La proporción de tinta de datos fue definida por Tufte (1983, p. 93) como la “proporción de tinta del gráfico dedicada a la representación no redundante de la información de datos” o, equivalentemente, como “1.0 menos la proporción de un gráfico que puede borrarse sin pérdida de información de datos”. La proporción de tinta de datos es más fácil de calcular que la medida de densidad de datos definida en la Sección 21.3 de este documento. El arte óptico es decoración que no le dice al espectador nada nuevo. Una variable que se había anticipado como muy significativa era la densidad de datos, que es difícil y requiere mucho tiempo para medir. Un hallazgo importante del estudio fue que la proporción de tinta de datos, más fácil de medir, y la proporción de página fueron suficientes para predecir las calificaciones. Una cita de la tesis de Harbert resume el hallazgo: “Las calificaciones más altas se otorgaron a aquellos gráficos que ocupan pequeñas proporciones de la página, tienen una alta proporción de tinta de datos, facilitan las comparaciones, tienen suficientes puntos de datos, tienen etiquetas impresas horizontalmente, no tienen abreviaturas, no tienen arte óptico y no utilizan comparaciones de volumen o en 3D” (Harbert 1995, p. 56). Como un pequeño estudio de seguimiento al trabajo de Harbert, examinamos cada uno de los 19 gráficos no tabulares en el Life Insurance Fact Book (1994), evaluándolos en siete factores negativos. La Tabla A.4 muestra el porcentaje de gráficos que exhibieron cada uno de los factores negativos. Tabla A.4. Porcentaje de Gráficos que Muestran Factores Negativos en Life Insurance Fact Book 1994 \\[ \\small{ \\begin{array}{lc} \\hline \\text{Factor Negativo} &amp;\\text{Porcentaje} \\\\ &amp; \\text{de Gráficos} \\\\ \\hline \\text{Uso de barras en 3-D} &amp; 79 \\\\ \\text{Líneas de cuadrícula demasiado densas} &amp;79 \\\\ \\text{Dificultad para comparar valores de series temporales} &amp;37\\\\ \\text{Uso de barras apiladas} &amp; 37 \\\\ \\text{Crecimiento representado de manera pobre} &amp;32\\\\ \\text{Uso de líneas más anchas de lo necesario} &amp; 16\\\\ \\text{Uso de gráficos circulares} &amp;5\\\\ \\hline \\end{array} } \\] Nuestra revisión sugiere que cada gráfico podría haberse reducido en un 50% a 75% sin pérdida de claridad. Esta observación está en línea con el hallazgo de Harbert sobre la variable de proporción de página. En resumen, los gráficos en el Life Insurance Fact Book podrían haberse producido de manera mucho más eficaz. Hacerlo mejoraría la calidad de la comunicación y potencialmente incrementaría el respeto con el que profesionales informados de otros campos ven a la industria aseguradora. Esperamos que otros investigadores lleven a cabo estudios adicionales sobre la práctica gráfica en publicaciones actuariales. Al utilizar datos de tales estudios, la profesión puede mejorar sus prácticas, haciendo las comunicaciones más eficientes y precisas. 21.5 Observaciones Finales El lema de la Sociedad de Actuarios es una cita de Ruskin: “El trabajo de la ciencia es sustituir hechos por apariencias y demostraciones por impresiones.” Armados con las pautas descritas en este capítulo y discutidas en las referencias, los actuarios pueden ser líderes en la presentación de datos de manera gráfica, sustituyendo demostraciones por impresiones. Encuestas de literatura actuarial reciente deberían ser la base para evaluar la práctica actual. Los editores y revisores de publicaciones profesionales pueden ser especialmente influyentes para lograr una mejora rápida en los estándares de práctica. Además, los actuarios pueden recomendar y usar libros de texto estadísticos que presten atención a la calidad gráfica. Como los actuarios leen material que contiene gráficos, son consumidores. ¡Deben convertirse en consumidores exigentes! Muy a menudo, los valores predeterminados en el software de hojas de cálculo y gráficos estadísticos se convierten en la norma. Los actuarios no deberían permitir que las decisiones tomadas por los programadores de software determinen la calidad o los estándares gráficos. Aunque es fácil crear gráficos usando los valores predeterminados del software gráfico, los gráficos resultantes rara vez son completamente satisfactorios. Si un gráfico no vale la pena hacerlo bien, dejémoslo fuera de nuestras publicaciones. 21.6 Lecturas Adicionales y Referencias Además de las referencias enumeradas, existen otros recursos disponibles para los actuarios interesados en mejorar sus habilidades en diseño gráfico. Al igual que la Sociedad de Actuarios, otra organización profesional, la American Statistical Association (ASA), tiene secciones de interés especial. En particular, la ASA ahora tiene una sección sobre gráficos estadísticos. Los actuarios interesados pueden unirse a la ASA y a esa sección para recibir el boletín Statistical Computing &amp; Graphics. Esta publicación contiene ejemplos de prácticas gráficas excelentes en el contexto de descubrimientos y aplicaciones científicas. La revista técnica Journal of Computational and Graphical Statistics contiene información más detallada sobre gráficos efectivos. Referencias del Capítulo American Council of Life Insurance. Various years. Life Insurance Fact Book. Washington, D.C.: ACLI. Cleveland, William S. (1994). The Elements of Graphing Data. Monterey, Calif.: Wadsworth. Cleveland, William S. (1993). Visualizing Data. Summit, N.J.: Hobart Press. Cleveland, William S., Diaconis, P., and McGill. R. (1982). Variables on scatterplots look more highly correlated when the scales are increased. Science 216, 1138-1141. Cleveland, William S., and McGill, R. (1984). Graphical perception: Theory, experimentation, and application to the development of graphical methods. Journal of the American Statistical Association 79, 531-454. Cleveland, William S., and McGill, R. (1985). Graphical perception and graphical methods for analyzing and presenting scientific data. Science 229, 828-833. Ehrenberg, A.S.C. (1977). Rudiments of Numeracy. Journal of the Royal Statistical Society A 140:277-97. Frees, Edward W. (1996). Data Analysis Using Regression Models. Englewood Cliffs, N.J.: Prentice Hall. Frees, Edward W. (1998). Relative Importance of Risk Sources in Insurance Systems, North American Actuarial Journal 2(2), 34-51. Frees, Edward W., Kung, Yueh C., Rosenberg, Marjorie A., Young, Virginia R., and Lai, Siu-Wai (1997). Forecasting Social Security Assumptions, North American Actuarial Journal 1(3), 49-82. Harbert, D. (1995). The Quality of Graphics in 1993 Psychology Journals, Senior honors thesis, University of Wisconsin-Madison. Huff, D. (1954). How To Lie with Statistics. New York: Norton. Schmid, C.F. (1992). Statistical Graphics: Design Principles and Practices Malabar, Fla.: Krieger Publishing Co. Schmit, Joan T., and Roth, K. (1990). Cost Effectiveness of Risk Management Practices, Journal of Risk and Insurance 57, 455-470. Strunk, W., and White, E.B. (1979). The Elements of Style. 3rd ed. New York: Macmillan. Tufte, Edward R. (1983). The Visual Display of Quantitative Information. Cheshire, Conn.: Graphics Press. Tufte, Edward R. (1990). Envisioning Information. Cheshire, Conn.: Graphics Press. Tufte, Edward R. (1997). Visual Explanations. Cheshire, Conn.: Graphics Press. Tukey, John (1977). Exploratory Data Analysis. Reading, Mass.: Addison-Wesley. University of Chicago Press (1993). The Chicago Manual of Style. 14th ed. Chicago, Ill. Notas al Pie Este capítulo se basa en “Designing Effective Graphs,” por Edward W. Frees y Robert B. Miller, 1990, North American Actuarial Journal, volumen 2, número 2, 53-70. Publicado por la Society of Actuaries - reproducido con permiso.↩︎ El código gráfico utiliza fragmentos pequeños de datos que pueden ser accedidos desde el libro Statistical Software Scripts.↩︎ "],["apéndices.html", "Capítulo 22 Apéndices Apéndice A1. Inferencia Estadística Básica Apéndice A2. Álgebra de Matrices Apéndice A3. Tablas de Probabilidad", " Capítulo 22 Apéndices Apéndice A1. Inferencia Estadística Básica Vista Previa del Apéndice. Este apéndice proporciona definiciones y hechos de un curso de inferencia estadística básica que son necesarios para el estudio del análisis de regresión. Distribuciones de Funciones de Variables Aleatorias Estadísticas y Distribuciones Muestrales. Una estadística resume la información en una muestra y, por lo tanto, es una función de las observaciones \\(y_1,\\ldots,y_n\\). Dado que las observaciones son realizaciones de variables aleatorias, el estudio de las distribuciones de funciones de variables aleatorias es realmente el estudio de las distribuciones de estadísticas, conocidas como distribuciones muestrales. Las combinaciones lineales de la forma \\(\\sum_{i=1}^n a_i y_i\\) representan un tipo importante de función. Aquí, \\(a_1,\\ldots,a_n\\) son constantes conocidas. Para comenzar, supongamos que \\(y_1,\\ldots,y_n\\) son variables aleatorias mutuamente independientes con \\(\\mathrm{E~}y_i = \\mu_i\\) y \\(\\mathrm{Var~}y_i = \\sigma_i^2\\). Entonces, por la linealidad de las esperanzas, tenemos \\[ \\mathrm{E}\\left( \\sum_{i=1}^n a_i y_i \\right) = \\sum_{i=1}^n a_i \\mu_i~~~\\mathrm{y}~~~\\mathrm{Var}\\left( \\sum_{i=1}^n a_i y_i \\right) = \\sum_{i=1}^n a_i^2 \\sigma_i^2. \\] Un teorema importante en la estadística matemática es que, si cada variable aleatoria se distribuye normalmente, entonces las combinaciones lineales también se distribuyen normalmente. Es decir, tenemos: Linealidad de Variables Aleatorias Normales. Supongamos que \\(y_1,\\ldots,y_n\\) son variables aleatorias mutuamente independientes con \\(y_i \\sim N(\\mu_i,\\sigma_i^2)\\). (Lea “\\(\\sim\\)” como “se distribuye como”). Entonces, \\[ \\sum_{i=1}^n a_i y_i \\sim N\\left( \\sum_{i=1}^n a_i \\mu_i, \\sum_{i=1}^n a_i^2 \\sigma_i^2 \\right) . \\] Hay varias aplicaciones de esta propiedad importante. Primero, se puede verificar que si \\(y \\sim N(\\mu ,\\sigma^2)\\), entonces \\((y - \\mu)/\\sigma \\sim N(0,1)\\). En segundo lugar, supongamos que \\(y_1,\\ldots,y_n\\) son idénticamente e independientemente distribuidos (i.i.d.) como \\(N(\\mu, \\sigma^2)\\) y tomamos \\(a_i = n^{-1}\\). Entonces, tenemos \\[ \\overline{y} = \\frac{1}{n}\\sum_{i=1}^n y_i \\sim N\\left( \\mu ,\\frac{\\sigma^2}{n}\\right) . \\] Equivalente a esto, \\(\\sqrt{n}\\left( \\overline{y}-\\mu \\right) /\\sigma\\) es normal estándar. Por lo tanto, la importante estadística muestral \\(\\overline{y}\\) tiene una distribución normal. Además, también se puede calcular la distribución de la varianza muestral \\(s_y^2\\). Para \\(y_1,\\ldots,y_n\\) que son i.i.d. \\(N(\\mu ,\\sigma^2)\\), tenemos que \\(\\left( n-1\\right) s_y^2 /\\sigma^2\\sim \\chi_{n-1}^2\\), una distribución \\(\\chi^2\\) (chi-cuadrado) con \\(n-1\\) grados de libertad. Además, \\(\\overline{y}\\) es independiente de \\(s_y^2\\). A partir de estos dos resultados, tenemos que \\[ \\frac{\\sqrt{n}}{s_y}\\left( \\overline{y}-\\mu \\right) \\sim t_{n-1}, \\] una distribución \\(t\\) con \\(n-1\\) grados de libertad. Estimación y Predicción Supongamos que \\(y_1,\\ldots,y_n\\) son variables aleatorias i.i.d. de una distribución que se puede resumir mediante un parámetro desconocido \\(\\theta\\). Nos interesa la calidad de una estimación de \\(\\theta\\) y denotamos \\(\\widehat{\\theta}\\) como este estimador. Por ejemplo, consideramos \\(\\theta = \\mu\\) con \\(\\widehat{\\theta} = \\overline{y}\\) y \\(\\theta = \\sigma^2\\) con \\(\\widehat{\\theta} = s_y^2\\) como nuestros principales ejemplos. Estimación Puntual e Imparcialidad. Dado que \\(\\widehat{\\theta}\\) proporciona una aproximación (única) de \\(\\theta\\), se le conoce como una estimación puntual de \\(\\theta\\). Como estadística, \\(\\widehat{\\theta}\\) es una función de las observaciones \\(y_1,\\ldots,y_n\\) que varía de una muestra a otra. Por lo tanto, los valores de \\(\\widehat{\\theta}\\) varían de una muestra a otra. Para examinar qué tan cerca tiende a estar \\(\\widehat{\\theta}\\) de \\(\\theta\\), examinamos varias propiedades de \\(\\widehat{\\theta}\\), en particular, el sesgo y la consistencia. Se dice que un estimador puntual \\(\\widehat{\\theta}\\) es un estimador imparcial de \\(\\theta\\) si \\(\\mathrm{E~}\\widehat{\\theta} = \\theta\\). Por ejemplo, dado que \\(\\mathrm{E~}\\overline{y} = \\mu\\), \\(\\overline{y}\\) es un estimador imparcial de \\(\\mu\\). Propiedades de Muestras Finitas versus Propiedades de Muestras Grandes de los Estimadores. Se dice que el sesgo es una propiedad de muestra finita ya que es válida para cada tamaño de muestra \\(n\\). Una propiedad límite o de muestra grande es la consistencia. La consistencia se expresa de dos maneras: consistencia débil y consistencia fuerte. Se dice que un estimador es débilmente consistente si \\[ \\lim_{n\\rightarrow \\infty }\\Pr \\left( |\\widehat{\\theta }-\\theta |&lt;h\\right) = 1, \\] para cada \\(h\\) positivo. Se dice que un estimador es fuertemente consistente si \\(\\lim_{n\\rightarrow \\infty }~\\widehat{\\theta }=\\theta\\), con probabilidad uno. Principio de Estimación de Mínimos Cuadrados. En este texto, se utilizan dos principios principales de estimación: la estimación por mínimos cuadrados y la estimación por máxima verosimilitud. Para el procedimiento de mínimos cuadrados, consideremos variables aleatorias independientes \\(y_1,\\ldots,y_n\\) con medias \\(\\mathrm{E~}y_i = \\mathrm{g}_i(\\theta )\\). Aquí, \\(\\mathrm{g}_i(.)\\) es una función conocida excepto por \\(\\theta\\), el parámetro desconocido. El estimador de mínimos cuadrados es el valor de \\(\\theta\\) que minimiza la suma de cuadrados \\[ \\mathrm{SS}(\\theta )=\\sum_{i=1}^n\\left( y_i-\\mathrm{g}_i(\\theta )\\right)^2. \\] Principio de Estimación de Máxima Verosimilitud. Las estimaciones por máxima verosimilitud son los valores del parámetro que son “más probables” de haber sido producidos por los datos. Consideremos las variables aleatorias independientes \\(y_1,\\ldots,y_n\\) con función de probabilidad \\(\\mathrm{f}_i(a_i,\\theta )\\). Aquí, \\(\\mathrm{f}_i(a_i,\\theta )\\) se interpreta como una función de masa de probabilidad para \\(y_i\\) discreto o una función de densidad de probabilidad para \\(y_i\\) continuo, evaluada en \\(a_i\\), la realización de \\(y_i\\). Se asume que la función \\(\\mathrm{f}_i(a_i,\\theta )\\) es conocida excepto por \\(\\theta\\), el parámetro desconocido. La verosimilitud de las variables aleatorias \\(y_1,\\ldots,y_n\\) tomando valores \\(a_1,\\ldots,a_n\\) es \\[ \\mathrm{L}(\\theta )=\\prod\\limits_{i=1}^n \\mathrm{f}_i(a_i,\\theta ). \\] El valor de \\(\\theta\\) que maximiza \\(\\mathrm{L}(\\theta )\\) se llama el estimador de máxima verosimilitud. Intervalos de Confianza. Aunque las estimaciones puntuales proporcionan una aproximación única a los parámetros, las estimaciones por intervalo proporcionan un rango que incluye parámetros con un cierto nivel de probabilidad preespecificado, o confianza. Un par de estadísticas, \\(\\widehat{\\theta }_1\\) y \\(\\widehat{\\theta }_{2}\\), proporcionan un intervalo de la forma \\(\\left[ \\widehat{\\theta }_1 &lt; \\widehat{\\theta }_{2}\\right]\\). Este intervalo es un intervalo de confianza de \\(100(1-\\alpha )\\%\\) para \\(\\theta\\) si \\[ \\Pr \\left( \\widehat{\\theta }_1 &lt; \\theta &lt; \\widehat{\\theta }_{2}\\right) \\geq 1-\\alpha . \\] Por ejemplo, supongamos que \\(y_1,\\ldots,y_n\\) son variables aleatorias i.i.d. \\(N(\\mu ,\\sigma^2)\\). Recuerde que \\(\\sqrt{n}\\left( \\overline{y}-\\mu\\right) /s_y\\sim t_{n-1}\\). Este hecho nos permite desarrollar un intervalo de confianza de \\(100(1-\\alpha )\\%\\) para \\(\\mu\\) de la forma \\(\\overline{y}\\pm (t-value)s_y/ \\sqrt{n}\\), donde \\(t-value\\) es el percentil \\((1-\\alpha /2)^{th}\\) de una distribución \\(t\\) con \\(n-1\\) grados de libertad. Intervalos de Predicción. Los intervalos de predicción tienen la misma forma que los intervalos de confianza. Sin embargo, un intervalo de confianza proporciona un rango para un parámetro, mientras que un intervalo de predicción proporciona un rango para valores externos de las observaciones. Basado en las observaciones \\(y_1,\\ldots,y_n\\), buscamos construir estadísticas \\(\\widehat{\\theta }_1\\) y \\(\\widehat{\\theta }_{2}\\) tal que \\[ \\Pr \\left( \\widehat{\\theta }_1 &lt; y^{\\ast } &lt; \\widehat{\\theta }_{2}\\right) \\geq 1-\\alpha . \\] Aquí, \\(y^{\\ast }\\) es una observación adicional que no forma parte de la muestra. Pruebas de Hipótesis Hipótesis Nula y Alternativa y Estadísticos de Prueba. Un procedimiento estadístico importante implica verificar ideas sobre los parámetros. Es decir, antes de que se observen los datos, se formulan ciertas ideas sobre los parámetros. En este texto, consideramos una hipótesis nula de la forma \\(H_0:\\theta =\\theta_0\\) frente a una hipótesis alternativa. Consideramos tanto una alternativa de dos colas, \\(H_{a}:\\theta \\neq \\theta_0\\), como alternativas de una cola, ya sea \\(H_{a}:\\theta &gt;\\theta_0\\) o \\(H_{a}:\\theta &lt;\\theta_0\\). Para elegir entre estas hipótesis competidoras, utilizamos un estadístico de prueba \\(T_n\\) que típicamente es una estimación puntual de \\(\\theta\\) o una versión reescalada para ajustarse a una distribución de referencia bajo \\(H_0\\). Por ejemplo, para probar \\(H_0:\\mu =\\mu_0\\), a menudo usamos \\(T_n= \\overline{y}\\) o \\(T_n=\\sqrt{n}\\left( \\overline{y}-\\mu_0\\right) /s_y\\). Nótese que esta última opción tiene una distribución \\(t_{n-1}\\), bajo los supuestos de datos normales i.i.d. Regiones de Rechazo y Nivel de Significancia. Con un estadístico en mano, ahora establecemos un criterio para decidir entre las dos hipótesis competidoras. Esto se puede hacer estableciendo una región de rechazo o región crítica. La región crítica consiste en todos los posibles resultados de \\(T_n\\) que nos llevan a rechazar \\(H_0\\) a favor de \\(H_{a}\\). Para especificar la región crítica, primero cuantificamos los tipos de errores que se pueden cometer en el procedimiento de toma de decisiones. Un error de Tipo I consiste en rechazar \\(H_0\\) falsamente y un error de Tipo II consiste en rechazar \\(H_{a}\\) falsamente. La probabilidad de un error de Tipo I se llama nivel de significancia. Preespecificar el nivel de significancia es a menudo suficiente para determinar la región crítica. Por ejemplo, supongamos que \\(y_1,\\ldots,y_n\\) son i.i.d. \\(N(\\mu ,\\sigma^2)\\) y estamos interesados en decidir entre \\(H_0:\\mu =\\mu_0\\) y \\(H_{a}:\\mu &gt; \\mu_0\\). Pensando en nuestro estadístico de prueba \\(T_n=\\overline{y}\\), sabemos que nos gustaría rechazar \\(H_0\\) si \\(\\overline{y}\\) es mayor que \\(\\mu_0\\). La pregunta es ¿cuánto mayor? Especificando un nivel de significancia \\(\\alpha\\), deseamos encontrar una región crítica de la forma \\(\\{\\overline{y}&gt;c\\}\\) para alguna constante \\(c\\). Con este fin, tenemos \\[ \\begin{array}{ll} \\alpha &amp;= \\Pr \\mathrm{(Error~Tipo~I)} = \\Pr (\\mathrm{Rechazar~}H_0 \\mathrm{~asumiendo~} H_0:\\mu =\\mu_0 \\mathrm{~es~verdadera)} \\\\ &amp; = \\Pr (\\overline{y}&gt;c) = \\Pr \\left(\\sqrt{n}\\left( \\overline{y}-\\mu_0\\right)/s_y&gt;\\sqrt{n}\\left( c-\\mu_0 \\right)/s_y\\right) \\\\ &amp;= \\Pr \\left(t_{n-1}&gt;\\sqrt{n}\\left( c-\\mu_0 \\right)/s_y\\right). \\end{array} \\] Con \\(df=n-1\\) grados de libertad, tenemos que \\(t-value = \\sqrt{n}\\left( c-\\mu_0\\right)/s_y\\) donde el \\(t-value\\) es el percentil \\((1-\\alpha)^{th}\\) de una distribución \\(t\\). Así, resolviendo para \\(c\\), nuestra región crítica es de la forma \\(\\{\\overline{y} &gt; \\mu_0 + (t-value)/s_y/\\sqrt{n}\\}\\). Relación entre Intervalos de Confianza y Pruebas de Hipótesis. Cálculos similares muestran que, para probar \\(H_0:\\mu = \\mu_0\\) frente a \\(H_{a}:\\theta \\neq \\theta_0\\), la región crítica es de la forma \\[ \\{ \\overline{y} &gt; \\mu_0 + (t-value)/s_y/\\sqrt{n} ~\\mathrm{o~} \\overline{y} &lt; \\mu_0 - (t-value)/s_y/\\sqrt{n}\\} . \\] Aquí, el \\(t\\)-value es el percentil \\((1-\\alpha /2)^{th}\\) de una distribución \\(t\\) con \\(df=n-1\\) grados de libertad. Es interesante notar que el evento de caer en esta región crítica de dos colas es equivalente al evento de que \\(\\mu_0\\) caiga fuera del intervalo de confianza \\(\\overline{y}\\pm (t-value)s_y/\\sqrt{n}\\). Esto establece el hecho de que los intervalos de confianza y las pruebas de hipótesis realmente están reportando la misma evidencia con un énfasis diferente en la interpretación de la inferencia estadística. \\(p\\)-valor. Otro concepto útil en las pruebas de hipótesis es el \\(p\\)-valor, que es la abreviatura de valor de probabilidad. Para un conjunto de datos, un \\(p\\)-valor se define como el nivel de significancia más pequeño para el cual se rechazaría la hipótesis nula. El \\(p\\)-valor es un estadístico resumen útil para que el analista de datos lo informe, ya que permite al lector comprender la fuerza de la desviación de la hipótesis nula. Apéndice A2. Álgebra de Matrices Definiciones Básicas Matriz - un arreglo rectangular de números organizados en filas y columnas (el plural de matriz es matrices). Dimensión de la matriz - el número de filas y columnas de la matriz. Consideremos una matriz \\(\\mathbf{A}\\) que tiene dimensiones \\(m \\times k\\). Sea \\(a_{ij}\\) el símbolo para el número en la fila \\(i\\) y la columna \\(j\\) de \\(\\mathbf{A}\\). En general, trabajamos con matrices de la forma \\[ \\mathbf{A} = \\left( \\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1k} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mk} \\end{array} \\right) . \\] Vector - un vector (columna) es una matriz que contiene solo una fila (\\(m=1\\)). Vector fila - una matriz que contiene solo una columna (\\(k=1\\)). Transpuesta - la transpuesta de una matriz \\(\\mathbf{A}\\) se define al intercambiar las filas y las columnas y se denota por \\(\\mathbf{A}^{\\prime}\\) (o \\(\\mathbf{A}^{\\mathrm{T}}\\)). Así, si \\(\\mathbf{A}\\) tiene dimensión \\(m \\times k\\), entonces \\(\\mathbf{A}^{\\prime}\\) tiene dimensión \\(k \\times m\\). Matriz cuadrada - una matriz donde el número de filas es igual al número de columnas, es decir, \\(m=k\\). Elemento diagonal - el número en la fila \\(r\\) y columna \\(r\\) de una matriz cuadrada, \\(r=1,2,\\ldots\\) Matriz diagonal - una matriz cuadrada donde todos los números no diagonales son iguales a cero. Matriz identidad - una matriz diagonal donde todos los elementos diagonales son iguales a uno y se denota por \\(\\mathbf{I}\\). Matriz simétrica - una matriz cuadrada \\(\\mathbf{A}\\) tal que la matriz permanece sin cambios si intercambiamos los roles de las filas y las columnas, es decir, si \\(\\mathbf{A} = \\mathbf{A}^{\\prime}\\). Nótese que una matriz diagonal es una matriz simétrica. Revisión de Operaciones Básicas Multiplicación por un escalar. Sea \\(c\\) un número real, llamado escalar (una matriz de \\(1 \\times 1\\)). Multiplicar un escalar \\(c\\) por una matriz \\(\\mathbf{A}\\) se denota por \\(c \\mathbf{A}\\) y se define por \\[ c\\mathbf{A} = \\left( \\begin{array}{cccc} ca_{11} &amp; ca_{12} &amp; \\cdots &amp; ca_{1k} \\\\ ca_{21} &amp; ca_{22} &amp; \\cdots &amp; ca_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ ca_{m1} &amp; ca_{m2} &amp; \\cdots &amp; ca_{mk} \\end{array} \\right) . \\] Suma y resta de matrices. Sean \\(\\mathbf{A}\\) y \\(\\mathbf{B}\\) matrices, cada una con dimensión \\(m \\times k\\). Utilice \\(a_{ij}\\) y \\(b_{ij}\\) para denotar los números en la fila \\(i\\) y columna \\(j\\) de \\(\\mathbf{A}\\) y \\(\\mathbf{B}\\), respectivamente. Entonces, la matriz \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) se define como la matriz con el número \\((a_{ij} + b_{ij})\\) para denotar el número en la fila \\(i\\) y la columna \\(j\\). De manera similar, la matriz \\(\\mathbf{C} = \\mathbf{A} - \\mathbf{B}\\) se define como la matriz con el número \\((a_{ij} - b_{ij})\\) para denotar los números en la fila \\(i\\) y la columna \\(j\\). Multiplicación de matrices. Si \\(\\mathbf{A}\\) es una matriz de dimensión \\(m \\times c\\) y \\(\\mathbf{B}\\) es una matriz de dimensión \\(c \\times k\\), entonces \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) es una matriz de dimensión \\(m \\times k\\). El número en la fila \\(i\\) y columna \\(j\\) de \\(\\mathbf{C}\\) es \\(\\sum_{s=1}^c a_{is} b_{sj}\\). Determinante - una función de una matriz cuadrada, denotada por \\(\\mathrm{det}(\\mathbf{A})\\), o \\(|\\mathbf{A}|\\). Para una matriz de \\(1 \\times 1\\), el determinante es \\(\\mathrm{det}(\\mathbf{A}) = a_{11}\\). Para definir determinantes de matrices más grandes, necesitamos dos conceptos adicionales. Sea \\(\\mathbf{A}_{rs}\\) la submatriz de dimensión \\((m-1) \\times (m-1)\\) de \\(\\mathbf{A}\\) definida al eliminar la fila \\(r\\) y la columna \\(s\\). Recursivamente, definimos \\(\\mathrm{det}(\\mathbf{A}) = \\sum_{s=1}^m (-1)^{r+s} a_{rs} \\mathrm{det}(\\mathbf{A}_{rs})\\), para cualquier \\(r=1,\\ldots,m\\). Por ejemplo, para \\(m=2\\), tenemos \\(\\mathrm{det}(\\mathbf{A}) = a_{11}a_{22} - a_{12}a_{21}\\). Inversa de una matriz. En álgebra de matrices, no existe el concepto de división. En su lugar, extendemos el concepto de recíprocos de números reales. Para comenzar, supongamos que \\(\\mathbf{A}\\) es una matriz cuadrada de dimensión \\(m \\times m\\) tal que \\(\\mathrm{det}(\\mathbf{A}) \\neq 0\\). Además, sea \\(\\mathbf{I}\\) la matriz identidad de \\(m \\times m\\). Si existe una matriz \\(m \\times m\\) \\(\\mathbf{B}\\) tal que \\(\\mathbf{AB = I = BA}\\), entonces \\(\\mathbf{B}\\) se llama la inversa de \\(\\mathbf{A}\\) y se escribe como \\(\\mathbf{B} = \\mathbf{A}^{-1}\\). Definiciones Adicionales Vectores linealmente dependientes – un conjunto de vectores \\(\\mathbf{c}_{1},\\ldots,\\mathbf{c}_{k}\\) se dice que son linealmente dependientes si uno de los vectores en el conjunto puede ser escrito como una combinación lineal de los otros. Vectores linealmente independientes – un conjunto de vectores \\(\\mathbf{c}_{1},\\ldots,\\mathbf{c}_{k}\\) se dice que son linealmente independientes si no son linealmente dependientes. Específicamente, un conjunto de vectores \\(\\mathbf{c}_{1},\\ldots,\\mathbf{c}_{k}\\) se dice que son linealmente independientes si y solo si la única solución de la ecuación \\(x_{1}\\mathbf{c}_{1} + \\ldots + x_{k}\\mathbf{c}_{k} = 0\\) es \\(x_{1} = \\ldots = x_{k} = 0\\). Rango de una matriz – el mayor número de columnas (o filas) linealmente independientes de una matriz. Matriz singular – una matriz cuadrada \\(\\mathbf{A}\\) tal que \\(\\mathrm{det}(\\mathbf{A}) = 0\\). Matriz no singular – una matriz cuadrada \\(\\mathbf{A}\\) tal que \\(\\mathrm{det}(\\mathbf{A}) \\neq 0\\). Matriz definida positiva – una matriz cuadrada simétrica \\(\\mathbf{A}\\) tal que \\(\\mathbf{x}^{\\prime}\\mathbf{Ax} &gt; 0\\) para \\(\\mathbf{x} \\neq 0\\). Matriz definida no negativa – una matriz cuadrada simétrica \\(\\mathbf{A}\\) tal que \\(\\mathbf{x}^{\\prime}\\mathbf{Ax} \\geq 0\\) para \\(\\mathbf{x} \\neq 0\\). Ortogonal – dos matrices \\(\\mathbf{A}\\) y \\(\\mathbf{B}\\) son ortogonales si \\(\\mathbf{A}^{\\prime}\\mathbf{B} = 0\\), una matriz cero. Idempotente – una matriz cuadrada tal que \\(\\mathbf{AA = A}\\). Traza – la suma de todos los elementos diagonales de una matriz cuadrada. Valores propios – las soluciones del polinomio de grado \\(n\\) \\(\\mathrm{det}(\\mathbf{A} - \\lambda \\mathbf{I}) = 0\\). También conocidos como raíces características y raíces latentes. Vector propio – un vector \\(\\mathbf{x}\\) tal que \\(\\mathbf{Ax} = \\lambda \\mathbf{x}\\), donde \\(\\lambda\\) es un valor propio de \\(\\mathbf{A}\\). También conocido como vector característico y vector latente. Inversa generalizada - de una matriz \\(\\mathbf{A}\\) es una matriz \\(\\mathbf{B}\\) tal que \\(\\mathbf{ABA = A}\\). Usamos la notación \\(\\mathbf{A}^{-}\\) para denotar la inversa generalizada de \\(\\mathbf{A}\\). En el caso de que \\(\\mathbf{A}\\) sea invertible, entonces \\(\\mathbf{A}^{-}\\) es única y es igual a \\(\\mathbf{A}^{-1}\\). Aunque existen varias definiciones de inversas generalizadas, la definición anterior es suficiente para nuestros propósitos. Ver Searle (1987) para una discusión adicional de definiciones alternativas de inversas generalizadas. Vector gradiente – un vector de derivadas parciales. Si \\(\\mathrm{f}(.)\\) es una función del vector \\(\\mathbf{x} = (x_1,\\ldots,x_m)^{\\prime}\\), entonces el vector gradiente es \\(\\partial \\mathrm{f}(\\mathbf{x})/\\partial \\mathbf{x}\\). La fila \\(i\\) del vector gradiente es \\(\\partial \\mathrm{f}(\\mathbf{x})/\\partial x_i\\). Matriz Hessiana – una matriz de segundas derivadas. Si \\(\\mathrm{f}(.)\\) es una función del vector \\(\\mathbf{x} = (x_1,\\ldots,x_m)^{\\prime}\\), entonces la matriz Hessiana es \\(\\partial^2 \\mathrm{f}(\\mathbf{x})/\\partial \\mathbf{x}\\partial \\mathbf{x}^{\\prime}\\). El elemento en la fila \\(i\\) y la columna \\(j\\) de la matriz Hessiana es \\(\\partial^{2}\\mathrm{f}(\\mathbf{x})/\\partial x_i\\partial x_j\\). Apéndice A3. Tablas de Probabilidad Distribución Normal Recuerde de la ecuación (1.1) que la función de densidad de probabilidad está definida por \\[ \\mathrm{f}(y) = \\frac{1}{\\sigma \\sqrt{2\\pi }}\\exp \\left( -\\frac{1}{2\\sigma^2 }\\left( y-\\mu \\right)^2\\right) \\] donde \\(\\mu\\) y \\(\\sigma^2\\) son parámetros que describen la curva. En este caso, escribimos \\(y \\sim N(\\mu,\\sigma^2)\\). Cálculos sencillos muestran que \\[ \\mathrm{E}~y = \\int_{-\\infty}^{\\infty} y \\mathrm{f}(y) \\, dy = \\int_{-\\infty}^{\\infty} y \\frac{1}{\\sigma \\sqrt{2\\pi }}\\exp \\left( -\\frac{1}{2\\sigma^2 }\\left( y-\\mu \\right)^2 \\right) \\, dy = \\mu \\] y \\[ \\mathrm{Var}~y = \\int_{-\\infty}^{\\infty} (y-\\mu)^2 \\mathrm{f}(y) \\, dy = \\int_{-\\infty}^{\\infty} (y-\\mu)^2 \\frac{1}{\\sigma \\sqrt{2\\pi }}\\exp \\left( -\\frac{1}{2\\sigma^2 }\\left( y-\\mu \\right)^2 \\right) \\, dy = \\sigma^2 . \\] Así, la notación \\(y \\sim N(\\mu,\\sigma^2)\\) se interpreta como que la variable aleatoria está distribuida normalmente con media \\(\\mu\\) y varianza \\(\\sigma^2\\). Si \\(y \\sim N(0,1)\\), entonces se dice que \\(y\\) es normal estándar. Figura 22.1: Función de densidad de probabilidad normal estándar Tabla 22.1: Función de Distribución Normal Estándar \\(y\\) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 0.5000 0.5398 0.5793 0.6179 0.6554 0.6915 0.7257 0.7580 0.7881 0.8159 1 0.8413 0.8643 0.8849 0.9032 0.9192 0.9332 0.9452 0.9554 0.9641 0.9713 2 0.9772 0.9821 0.9861 0.9893 0.9918 0.9938 0.9953 0.9965 0.9974 0.9981 3 0.9987 0.9990 0.9993 0.9995 0.9997 0.9998 0.9998 0.9999 0.9999 1.0000 Notas: Las probabilidades se pueden encontrar buscando en la fila adecuada para el dígito inicial y la columna para el decimal. Por ejemplo, \\(\\Pr ( y \\le 0.1) = 0.5398\\). Distribución Chi-Cuadrado Distribución Chi-Cuadrado. Varias distribuciones importantes pueden vincularse a la distribución normal. Si \\(y_1, \\ldots, y_n\\) son variables aleatorias i.i.d. tales que cada \\(y_i \\sim N(0,1)\\), entonces se dice que \\(\\sum_{i=1}^n y_i^2\\) tiene una distribución chi-cuadrado con parámetro \\(n\\). Más generalmente, una variable aleatoria \\(w\\) con función de densidad de probabilidad \\[ \\mathrm{f}(w) = \\frac{2^{-k/2}}{\\Gamma(k/2)} w^{k/2-1} \\exp (-w/2), ~~~~~~w &gt; 0 \\] se dice que tiene una distribución chi-cuadrado con \\(df = k\\) grados de libertad, escrita como \\(w \\sim \\chi_k^2\\). Cálculos sencillos muestran que para \\(w \\sim \\chi_k^2\\), tenemos \\(\\mathrm{E}~w = k\\) y \\(\\mathrm{Var}~w = 2k\\). En general, el parámetro de grados de libertad no necesita ser un número entero, aunque lo es para las aplicaciones de este texto. Figura 22.2: Varias funciones de densidad de probabilidad chi-cuadrado. Se muestran curvas para \\(df\\) = 3, \\(df\\) = 5, y \\(df\\) = 10. Un mayor número de grados de libertad conduce a curvas que son menos asimétricas. Tabla 22.2: Percentiles de Varias Distribuciones Chi-Cuadrado \\(df\\) 0.6 0.7 0.8 0.9 0.95 0.975 0.99 0.995 0.9975 0.999 1 0.71 1.07 1.64 2.71 3.84 5.02 6.63 7.88 9.14 10.83 2 1.83 2.41 3.22 4.61 5.99 7.38 9.21 10.60 11.98 13.82 3 2.95 3.66 4.64 6.25 7.81 9.35 11.34 12.84 14.32 16.27 4 4.04 4.88 5.99 7.78 9.49 11.14 13.28 14.86 16.42 18.47 5 5.13 6.06 7.29 9.24 11.07 12.83 15.09 16.75 18.39 20.52 10 10.47 11.78 13.44 15.99 18.31 20.48 23.21 25.19 27.11 29.59 15 15.73 17.32 19.31 22.31 25.00 27.49 30.58 32.80 34.95 37.70 20 20.95 22.77 25.04 28.41 31.41 34.17 37.57 40.00 42.34 45.31 25 26.14 28.17 30.68 34.38 37.65 40.65 44.31 46.93 49.44 52.62 30 31.32 33.53 36.25 40.26 43.77 46.98 50.89 53.67 56.33 59.70 35 36.47 38.86 41.78 46.06 49.80 53.20 57.34 60.27 63.08 66.62 40 41.62 44.16 47.27 51.81 55.76 59.34 63.69 66.77 69.70 73.40 60 62.13 65.23 68.97 74.40 79.08 83.30 88.38 91.95 95.34 99.61 120 123.29 127.62 132.81 140.23 146.57 152.21 158.95 163.65 168.08 173.62 Distribución t Supongamos que \\(y\\) y \\(w\\) son independientes con \\(y \\sim N(0,1)\\) y \\(w \\sim \\chi_k^2\\). Entonces, se dice que la variable aleatoria \\(t = y / \\sqrt{w/k}\\) tiene una distribución t con \\(df = k\\) grados de libertad. La función de densidad de probabilidad es \\[ \\mathrm{f}(t) = \\frac{\\Gamma \\left( k+ \\frac{1}{2} \\right)}{\\Gamma(k/2)} \\left( k \\pi \\right)^{-1/2} \\left( 1 + \\frac{t^2}{k} \\right)^{-(k+1/2)}, ~~~~~~-\\infty &lt; t &lt; \\infty \\] Esto tiene media 0, para \\(k &gt; 1\\), y varianza \\(k/(k-2)\\) para \\(k &gt; 2\\). Figura 22.3: Varias funciones de densidad de probabilidad de la distribución t. La distribución t con \\(df = \\infty\\) es la distribución normal estándar. Se muestran curvas para \\(df = 1\\), \\(df = 5\\) (no etiquetado) y \\(df = ∞\\). Un menor \\(df\\) significa colas más gruesas. Tabla 22.3: Percentiles de varias distribuciones \\(t\\) \\(df\\) 0.6 0.7 0.8 0.9 0.95 0.975 0.99 0.995 0.9975 0.999 1 0.325 0.727 1.376 3.078 6.314 12.706 31.821 63.657 127.321 318.309 2 0.289 0.617 1.061 1.886 2.920 4.303 6.965 9.925 14.089 22.327 3 0.277 0.584 0.978 1.638 2.353 3.182 4.541 5.841 7.453 10.215 4 0.271 0.569 0.941 1.533 2.132 2.776 3.747 4.604 5.598 7.173 5 0.267 0.559 0.920 1.476 2.015 2.571 3.365 4.032 4.773 5.893 10 0.260 0.542 0.879 1.372 1.812 2.228 2.764 3.169 3.581 4.144 15 0.258 0.536 0.866 1.341 1.753 2.131 2.602 2.947 3.286 3.733 20 0.257 0.533 0.860 1.325 1.725 2.086 2.528 2.845 3.153 3.552 25 0.256 0.531 0.856 1.316 1.708 2.060 2.485 2.787 3.078 3.450 30 0.256 0.530 0.854 1.310 1.697 2.042 2.457 2.750 3.030 3.385 35 0.255 0.529 0.852 1.306 1.690 2.030 2.438 2.724 2.996 3.340 40 0.255 0.529 0.851 1.303 1.684 2.021 2.423 2.704 2.971 3.307 60 0.254 0.527 0.848 1.296 1.671 2.000 2.390 2.660 2.915 3.232 120 0.254 0.526 0.845 1.289 1.658 1.980 2.358 2.617 2.860 3.160 ∞ 0.253 0.524 0.842 1.282 1.645 1.960 2.326 2.576 2.807 3.090 Distribución F Supongamos que \\(w_1\\) y \\(w_2\\) son independientes con distribuciones \\(w_1 \\sim \\chi_m^2\\) y \\(w_2 \\sim \\chi_n^2\\). Entonces, la variable aleatoria \\(F = (w_1/m) / (w_2/n)\\) tiene una distribución F con parámetros \\(df_1 = m\\) y \\(df_2 = n\\), respectivamente. La función de densidad de probabilidad es \\[ \\mathrm{f}(y) = \\frac{\\Gamma \\left(\\frac{m+n}{2} \\right)}{\\Gamma(m/2)\\Gamma(n/2)} \\left( \\frac{m}{n} \\right)^{m/2} \\frac{y^{(m-2)/2}} {\\left( 1 + \\frac{m}{n}y \\right)^{m+n+2}} , ~~~~~~y &gt; 0 \\] Esto tiene media \\(n/(n-2)\\), para \\(n &gt; 2\\), y varianza \\(2n^2(m+n-2)/[m(n-2)^2(n-4)]\\) para \\(n &gt; 4\\). Figura 22.4: Varias funciones de densidad de probabilidad de la distribución \\(F\\). Se muestran curvas para (i) \\(df_1\\) = 1, \\(df_2\\) = 5, (ii) \\(df_1\\) = 5, \\(df_2\\) = 1 (no etiquetado), y (iii) \\(df_1\\) = 60, \\(df_2\\) = 60. A medida que \\(df_2\\) tiende a \\(\\infty\\), la distribución \\(F\\) tiende a una distribución chi-cuadrado. Tabla 22.4: Percentiles de varias distribuciones \\(F\\) \\(df_1\\) 1 3 5 10 20 30 40 60 120 1 161.45 10.13 6.61 4.96 4.35 4.17 4.08 4.00 3.92 2 199.50 9.55 5.79 4.10 3.49 3.32 3.23 3.15 3.07 3 215.71 9.28 5.41 3.71 3.10 2.92 2.84 2.76 2.68 4 224.58 9.12 5.19 3.48 2.87 2.69 2.61 2.53 2.45 5 230.16 9.01 5.05 3.33 2.71 2.53 2.45 2.37 2.29 10 241.88 8.79 4.74 2.98 2.35 2.16 2.08 1.99 1.91 15 245.95 8.70 4.62 2.85 2.20 2.01 1.92 1.84 1.75 20 248.01 8.66 4.56 2.77 2.12 1.93 1.84 1.75 1.66 25 249.26 8.63 4.52 2.73 2.07 1.88 1.78 1.69 1.60 30 250.10 8.62 4.50 2.70 2.04 1.84 1.74 1.65 1.55 35 250.69 8.60 4.48 2.68 2.01 1.81 1.72 1.62 1.52 40 251.14 8.59 4.46 2.66 1.99 1.79 1.69 1.59 1.50 60 252.20 8.57 4.43 2.62 1.95 1.74 1.64 1.53 1.43 120 253.25 8.55 4.40 2.58 1.90 1.68 1.58 1.47 1.35 "],["respuestas-breves-a-ejercicios-seleccionados.html", "Respuestas Breves a Ejercicios Seleccionados", " Respuestas Breves a Ejercicios Seleccionados Capítulo 1 1.1 a(i). Media = 12,840, Mediana = 5,695 a(ii). Desviación estándar = 48,836.7 = 3.8 veces la media. Los datos parecen estar sesgados. b. Los gráficos no se presentan aquí. Al verlos, la distribución parece estar sesgada hacia la derecha. c(i). Los gráficos no se presentan aquí. Al verlos, aunque la distribución se ha acercado a la simetría, todavía es bastante asimétrica. c(ii). Los gráficos no se presentan aquí. Al verlos, sí, la distribución parece ser mucho más simétrica. d. Media = 1,854.0, Mediana = 625.7, Desviación estándar = 3,864.3. Un patrón similar se mantiene para pacientes ambulatorios como para pacientes hospitalizados. 1.2. Parte 1. a. Estadísticas descriptivas para los datos de 2000 \\[ \\small{ \\begin{array}{lrrrrrrr} \\hline &amp; &amp; 1er &amp; &amp; &amp; 3er &amp; &amp; \\text{Desviación} \\\\ &amp; \\text{Mínimo} &amp; \\text{Cuartil} &amp; \\text{Mediana} &amp; \\text{Media} &amp; \\text{Cuartil} &amp; \\text{Máximo} &amp; \\text{Estándar} \\\\\\hline \\text{TPY} &amp; 11.57 &amp; 56.72 &amp; 80.54 &amp; 88.79 &amp; 108.60 &amp; 314.70 &amp; 46.10 \\\\ \\text{NUMBED} &amp; 18.00 &amp; 60.25 &amp; 90.00 &amp; 97.08 &amp; 118.8 &amp; 320.00 &amp; 48.99 \\\\ \\text{SQRFOOT} &amp; 5.64 &amp; 28.64 &amp; 39.22 &amp; 50.14 &amp; 65.49 &amp; 262.00 &amp; 34.50 \\\\ \\hline \\end{array} } \\] b. Los gráficos no se presentan aquí. Al verlos, el histograma parece estar sesgado hacia la derecha, pero solo levemente. c. Los gráficos no se presentan aquí. Al verlos, tanto el histograma como el gráfico \\(qq\\) sugieren que la distribución transformada es cercana a una distribución normal. Parte 2. a. Estadísticas descriptivas para los datos de 2001 \\[ \\small{ \\begin{array}{lrrrrrrr} \\hline &amp; &amp; 1er &amp; &amp; &amp; 3er &amp; &amp; \\text{Desviación} \\\\ &amp; \\text{Mínimo} &amp; \\text{Cuartil} &amp; \\text{Mediana} &amp; \\text{Media} &amp; \\text{Cuartil} &amp; \\text{Máximo} &amp; \\text{Estándar} \\\\\\hline \\text{TPY} &amp; 12.31 &amp; 56.89 &amp; 81.13 &amp; 89.71 &amp; 109.90 &amp; 440.70 &amp; 49.05 \\\\ \\text{NUMBED} &amp; 18.00 &amp; 60.00 &amp; 90.00 &amp; 97.33 &amp; 119.00 &amp; 457.00 &amp; 51.97 \\\\ \\text{SQRFOOT} &amp; 5.64 &amp; 28.68 &amp; 40.26 &amp; 50.37 &amp; 63.49 &amp; 262.00 &amp; 35.56 \\\\ \\hline \\end{array} } \\] c. Tanto el histograma como el gráfico \\(qq\\) (no presentados aquí) sugieren que la distribución transformada es cercana a una distribución normal. 1.5 a. Media = 5.953, Mediana = 2.331 b. Los gráficos no se presentan aquí. Al verlos, el histograma parece estar sesgado hacia la derecha. El gráfico \\(qq\\) indica una desviación seria de la normalidad. c(i). Para ATTORNEY=1, tenemos Media = 9.863 y Mediana = 3.417. Para ATTORNEY=2, tenemos Media = 1.865 y Mediana = 0.986. Esto sugiere que las pérdidas asociadas con la intervención de un abogado (ATTORNEY=1) son mayores que cuando no hay un abogado involucrado (ATTORNEY=2). 1.7 a. Los gráficos no se presentan aquí. Al verlos, el histograma parece estar sesgado hacia la izquierda. El gráfico \\(qq\\) indica una desviación seria de la normalidad. b. Los gráficos no se presentan aquí. Al verlos, la transformación hace poco para simetrizar la distribución. Capítulo 2 2.1 \\(r=0.5491, b_0=4.2054, b_1=0.1279\\) 2.3 a. \\[\\begin{eqnarray*} 0 &amp; \\leq &amp; \\frac{1}{n-1}\\sum_{i=1}^n\\left( a\\frac{x_i-\\overline{x}}{s_{x}}-c \\frac{y_i-\\overline{y}}{s_{y}}\\right) ^{2} \\\\ &amp;=&amp;{\\frac{1}{n-1}}\\sum_{i=1}^n\\left[a^2\\frac{(x_i-\\overline{x})^2}{s_{x}^2} -2ac\\frac{(x_i-\\overline{x})(y_i-\\overline{y})}{s_x s_{y}}+c^2\\frac{(y_i-\\overline{y})^2}{s_{y}^2} \\right]\\\\ &amp;=&amp; a^2\\frac{1}{s_x^2}\\frac{1}{n-1}\\sum_{i=1}^n\\left(x_i-\\overline{x}\\right)^2-2ac\\frac{1}{s_x s_y}\\frac{1}{n-1}\\sum_{i=1}^n\\left(x_i-\\overline{x}\\right)\\left(y_i-\\overline{y}\\right)\\\\ &amp;&amp;+~c^2\\frac{1}{s_y^2}\\frac{1}{n-1}\\sum_{i=1}^n\\left(y_i-\\overline{y}\\right)^2\\\\ &amp;=&amp;a^2\\frac{1}{s_x^2}s_x^2-2acr+c^2\\frac{1}{s_y^2}s_y^2\\\\ &amp;=&amp; a^{2}+c^{2}-2acr. \\end{eqnarray*}\\] b. Según la parte (a), tenemos \\(a^{2}+c^{2}-2acr \\geq 0\\). Así que,\\[\\begin{eqnarray*} a^{2}+c^{2}-2ac+2ac &amp;\\geq&amp; 2acr\\\\ (a-c)^2&amp;\\geq&amp; 2acr-2ac\\\\ (a-c)^2&amp;\\geq&amp; 2ac(r-1) \\end{eqnarray*}\\] c. Usando el resultado en la parte b) y tomando \\(a = c\\), podemos obtener \\(2a^{2}(r-1)\\leq0\\). Además, \\(a^2\\geq 0\\), por lo que \\(r-1\\leq 0\\). Así, \\(r \\leq 1\\). d. Usando el resultado en la parte (b) y tomando \\(a = -c\\), podemos obtener \\(-2a^{2}(r-1)\\leq4a^2\\). Además, \\(-2a^2\\leq 0\\), por lo que \\(r-1\\geq -2\\). Así, \\(r \\geq -1\\). e. Si todos los datos se encuentran en una línea recta que pasa por los cuadrantes superior izquierdo e inferior derecho, entonces \\(r=-1\\). Si todos los datos se encuentran en una línea recta que pasa por los cuadrantes inferior izquierdo y superior derecho, entonces \\(r=1\\). 2.5 a. \\[\\begin{eqnarray*} b_1 = r\\frac{s_{y}}{s_{x}} &amp;=&amp; \\frac{1}{(n-1)s_{x}^{2}}\\sum_{i=1}^n\\left( x_{i}-\\overline{x}\\right) \\left( y_{i}-\\overline{y}\\right) \\\\ &amp;=&amp; \\frac{1}{\\sum_{i=1}^n\\left( x_{i}-\\overline{x}\\right)^{2}}\\sum_{i=1}^n\\left[\\frac{y_{i}-\\overline{y}}{x_{i}-\\overline{x}}\\left( x_{i}-\\overline{x}\\right)^2\\right]\\\\ &amp;&amp; \\\\ &amp;&amp;\\\\ &amp;=&amp; \\frac{\\sum_{i=1}^nweight_i~slope_i}{\\sum_{i=1}^nweight_{i}}. \\end{eqnarray*}\\] donde, \\[ slope_i=\\frac{y_{i}-\\overline{y}}{x_{i}-\\overline{x}}~~~~~\\mathrm{y}~~~~~weight_i=\\left( x_{i}-\\overline{x}\\right)^2 \\] b. \\(slope_1=-1.5, weight_1=4\\) 2.7 a. Para el modelo en este ejercicio, la estimación de mínimos cuadrados de \\(\\beta_1\\) es el \\(b_1\\) que minimiza la suma de cuadrados \\(\\mathrm{SS}(b_1^{\\ast} )=\\sum_{i=1}^n\\left( y_i - b_1^{\\ast }x_i\\right) ^{2}.\\) Entonces, tomando la derivada con respecto a \\(b_1^{\\ast}\\), tenemos \\[ \\frac{\\partial }{\\partial b_1^{\\ast }}SS(b_1^{\\ast })=\\sum_{i=1}^n(-2x_{i})\\left( y_{i}-b_1^{\\ast }x_{i}\\right) \\] Al igualar esta cantidad a cero y cancelar términos constantes, se obtiene \\[ \\sum_{i=1}^n\\left(x_{i}y_{i}-b_1^{\\ast }x_{i}^2\\right) =0 \\] Por lo tanto, concluimos que \\[ b_1 = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^nx_i^{2}}. \\] b. Del problema, tenemos \\(x_i = z_i^2\\). Usando el resultado de la parte (a), podemos concluir que \\[ b_1 = \\frac{\\sum_{i=1}^n z_i^2 y_i}{\\sum_{i=1}^nz_i^{4}}. \\] 2.10 a(i). Correlación\\(= 0.9372\\) a(ii). Tabla de correlaciones \\[ \\small{ \\begin{array}{llll} \\hline &amp;\\text{TPY} &amp; \\text{NUMBED} &amp; \\text{SQRFOOT} \\\\\\hline \\text{TPY} &amp; 1.0000 &amp; 0.9791 &amp; 0.8244\\\\ \\text{NUMBED} &amp; 0.9791 &amp; 1.0000 &amp; 0.8192\\\\ \\text{SQRFOOT} &amp; 0.8244 &amp; 0.8192 &amp; 1.0000\\\\ \\hline \\end{array} } \\] a(iii). Correlación\\(= 0.9791.\\) Las correlaciones no se ven afectadas por los cambios de escala. b. Los gráficos no se presentan aquí. Al verlos, existe una fuerte relación lineal entre NUMBED y TPY. La relación lineal entre SQRFOOT y TPY no es tan fuerte como la de NUMBED y TPY. c(i). \\(b_1=0.92142, t-\\mathrm{ratio}=91.346, R^2=0.9586\\) c(ii). \\(R^2 =0.6797.\\) El modelo que utiliza NUMBED es preferido. c(iii). \\(b_1 =1.01231, t-\\mathrm{ratio}=81.235, R^2=0.9483\\) c(iv). \\(b_1 =0.68737, t-\\mathrm{ratio}=27.25, R^2=0.6765\\) Parte 2: \\(b_1=0.932384, t-\\mathrm{ratio}=120.393, R^2=0.9762.\\) El patrón es similar al informe de costos para el año 2000. 2.11 \\(\\hat{e}_1 = -23.\\) 2.13 a. \\[ \\hat{y}_i - \\overline{y} = (b_0 + b_1 x_i) - \\overline{y} = (\\overline{y}-b_1 \\overline{x} + b_1 x_i) - \\overline{y} = b_1(x_i - \\overline{x}). \\] b. \\[ \\sum^n_{i=1}(y_i - \\overline{y})^2 = \\sum^n_{i=1}(b_1(x_i - \\overline{x}))^2 = b_1^2 \\sum_{i=1}^n(x_i - \\overline{x})^2 = b_1^2 s_x^2(n-1). \\] c. \\[ R^2 = \\frac{Regression ~SS}{Total ~SS} = \\frac{b_1^2 s_x^2(n-1)}{\\sum_{i=1}^n(y_i - \\overline{y})^2} = \\frac{b_1^2 s_x^2(n-1)}{s_y^2(n-1)} = \\frac{b_1^2 s_x^2}{s_y^2}. \\] 2.15 a. A partir de la definición del coeficiente de correlación y del Ejercicio 2.8(b), tenemos \\[ r(y,x)(n-1)s_y s_x = \\sum_{i=1}^n \\left( y_i-\\overline{y}\\right) \\left( x_i-\\overline{x}\\right) = \\sum_{i=1}^n y_i x_i - n \\overline{x} \\overline{y}. \\] Si \\(\\overline{y}=0,\\overline{x}=0\\) o ambos \\(\\overline{x}\\) y \\(\\overline{y}=0\\), entonces \\(r(y,x)(n-1)s_y s_x = \\sum_{i=1}^n y_i x_i\\). Por lo tanto, \\(r(y,x)=0\\) implica \\(\\sum_{i=1}^ny_i x_i=0\\) y viceversa. b. \\[\\begin{eqnarray*} \\sum_{i=1}^n x_i e_i &amp;=&amp; \\sum_{i=1}^n x_i (y_i - (\\overline{y} + b_1(x_i-\\overline{x}) )) \\\\ &amp;=&amp; \\sum_{i=1}^n x_i (y_i - \\overline{y}) - b_1 \\sum_{i=1}^n x_i (x_i-\\overline{x}) \\\\ &amp;=&amp; \\sum_{i=1}^n x_i b_1(x_i - \\overline{x}) - b_1 \\sum_{i=1}^n x_i (x_i-\\overline{x}) = 0, \\end{eqnarray*}\\] c. \\[\\begin{eqnarray*} \\sum_{i=1}^n \\widehat{y}_i e_i &amp;=&amp; \\sum_{i=1}^n ( \\overline{y}+b_1(x_i-\\overline{x}) ) e_i \\\\ &amp;=&amp; \\overline{y} \\sum_{i=1}^n e_i + b_1\\sum_{i=1}^n ( (x_i-\\overline{x})) e_i = 0, \\end{eqnarray*}\\] 2.17 Cuando \\(n = 100\\), \\(k = 1\\), \\(Error~SS = [n-(k+1)]s^2 = 98s^2\\) a. \\(e_{10}^2/(Error~SS) = (8s)^2/(98s^2) = 65.31\\%\\) b. \\(e_{10}^2/(Error~SS) = (4s)^2/(98s^2) = 16.33\\%\\) Cuando \\(n = 20\\), \\(k = 1\\), \\(Error~SS = [n-(k+1)]s^2 = 18s^2\\) c. \\(e_{10}^2/(Error~SS) = (4s)^2/(18s^2) = 88.89\\%\\) 2.20 a. Correlación=0.9830 Estadísticas descriptivas \\[ \\small{ \\begin{array} {lrrrrrrr} \\hline &amp; &amp; 1st &amp; &amp; &amp; 3rd &amp; &amp; \\text{Desviación} \\\\ &amp; \\text{Mínimo} &amp; \\text{Cuartil} &amp; \\text{Mediana} &amp; \\text{Media} &amp; \\text{Cuartil} &amp; \\text{Máximo} &amp; \\text{Estándar} \\\\\\hline \\text{LOGTPY} &amp; 2.51 &amp; 4.04 &amp; 4.40 &amp; 4.37 &amp; 4.70 &amp; 6.09 &amp; 0.51 \\\\ \\text{LOGNUMBED} &amp; 2.89 &amp; 4.09 &amp; 4.50 &amp; 4.46 &amp; 4.78 &amp; 6.13 &amp; 0.49 \\\\ \\hline \\end{array} } \\] b. \\(R^2 = 0.9664\\), \\(b_1 = 1.01923\\), \\(t(b_1) = 100.73\\). c(i). Los grados de libertad son \\(df = 355 - (1+1) = 353\\). El valor \\(t\\) correspondiente es 1.96. Como el estadístico \\(t\\) \\(t(b_1) = 100.73 &gt; 1.9667\\), rechazamos \\(H_0\\) a favor de la alternativa. c(ii). El estadístico \\(t\\) es \\(t - \\mathrm{ratio} = (b_1 - 1)/se(b_1) = (1.01923 - 1)/0.01012 = 1.9002.\\) Como \\(t-\\mathrm{ratio} &lt; 1.9667\\), no rechazamos \\(H_0\\) a favor de la alternativa. c(iii). El valor \\(t\\) correspondiente es 1.645. El estadístico \\(t\\) es \\(t - \\mathrm{ratio} = 1.9002.\\) Rechazamos \\(H_0\\) a favor de la alternativa. c(iv). El valor \\(t\\) correspondiente es -1.645. El estadístico \\(t\\) es \\(t - \\mathrm{ratio} = 1.9002.\\) No rechazamos \\(H_0\\) a favor de la alternativa. d(i). Una estimación puntual es 2.0384 d(ii). El intervalo de confianza al 95% para la pendiente \\(b_1\\) es \\(1.0192 \\pm 1.9667 \\times 0.0101 = (0.9993, 1.0391)\\). Un intervalo de confianza al 95% para el cambio esperado de LOGTPY es \\((0.9993 \\times 2, 1.0391 \\times 2) = (1.9987, 2.0781)\\) d(iii). El intervalo de confianza al 99% es $(2(1.0192 - 2.5898), 2(1.0192 + 2.5898) =(1.9861, 2.0907) $ e(i). \\(\\widehat{y} = -0.1747 + 1.0192 \\times \\ln 100 = 4.519037.\\) e(ii). El error estándar de la predicción \\[ se(pred) = s \\sqrt{1+\\frac{1}{n}+\\frac{\\left( x^{\\ast }-\\overline{x}\\right) ^{2} }{(n-1)s_{x}^{2}}} = 0.09373 \\sqrt{1+\\frac{1}{355}+\\frac{\\left( \\ln(100)-4.4573\\right) ^{2} }{(355-1)0.4924^{2}}}=0.0938. \\] e(iii). El intervalo de predicción al 95% en \\(x^*\\) es \\[ \\widehat{y}^{\\ast } \\pm t_{n-2,1-\\alpha /2} ~se(pred) = 4.519037 \\pm 1.9667(0.0938) = (4.3344, 4.7034). \\] e(iv). La predicción puntual es \\(e^{4.519037}= 91.747\\). El intervalo de predicción es \\((e^{4.334405}=76.280, e^{4.703668}=110.351 ).\\) e(v). El intervalo de predicción es \\((e^{4.364214}=78.588, e^{4.673859}=107.110 ).\\) 2.22 a. Ajustado US \\(LIFEEXP = 83.7381 - 5.2735 \\times 2.0 = 73.1911\\) b. Un intervalo de predicción al 95% para la esperanza de vida en Dominica es \\[ \\widehat{y}_{\\ast} \\pm t_{n-2,1-\\alpha /2} ~se(pred)=73.1911\\pm(1.973)(6.642)=(60.086, 86.296) \\] c. \\[ e_{i}=y_{i}-\\widehat{y}_{i}=y_{i}-\\left( b_0+b_1x_{i}\\right)= 72.5-(83.7381 - 5.2735 \\times 1.7)=-2.273 \\] Este residual es 2.273/6.615 = 0.3436 múltiplos de \\(s\\) por debajo de cero. d. Probar \\(H_0: \\beta_1 = -6.0\\) frente a \\(H_a: \\beta_1 &gt; -6.0\\) al nivel de significancia del 5% usando un valor \\(t\\) = 1.645. El estadístico \\(t\\) calculado \\(= \\frac{-5.2735-(-6)}{0.2887}=2.5165\\), que es \\(\\geq1.645\\). Por lo tanto, rechazamos \\(H_0\\) a favor de la alternativa. El valor \\(p\\) correspondiente \\(= 0.00637\\). Capítulo 3 3.1. a. \\(R^2_a = 1 - s/{s^2_y} = 1 - {(50)^2}/{(100)^2} = 1 - 1/4 = 0.75.\\) b. \\(Total ~SS =(n - 1)s^2_y = 99(100)^2 = 990000\\) y \\(Error ~SS = (n - (k + 1))s^2 = (100 - (3 + 1))(50)^2 = 240000.\\) \\[ \\small{ \\begin{array}{lcrcc} \\hline Fuente &amp; SS &amp; df &amp; MS &amp; F \\\\ \\hline \\text{Regresión} &amp; 750000 &amp; 3 &amp; 250000 &amp; 100 \\\\ \\text{Error} &amp; 240000 &amp; 96 &amp; 2500 &amp; \\\\ \\text{Total} &amp; 990000 &amp;99 &amp;&amp; \\\\ \\hline \\end{array} } \\] c. \\(R^2 = (Regression~SS)/(Total~SS) =750000/990000 = 75.76\\%\\). 3.3 a. \\({\\bf y}=(0~1~5~8)^{\\prime}\\), \\(\\mathbf{X}=\\left( \\begin{array}{ccc} 1 &amp; -1 &amp; 0 \\\\ 1 &amp; 2 &amp; 0 \\\\ 1 &amp; 4 &amp; 1 \\\\ 1 &amp; 6 &amp; 1 \\\\ \\end{array} \\right)\\). b. \\(\\hat{y}_{3}=x^{\\prime}_{3}\\mathbf{b}=(1~4~1)\\left( \\begin{array}{c} 0.15 \\\\ 0.692 \\\\ 2.88 \\\\ \\end{array} \\right)=5.798\\) c. \\(se(b_{2})=s\\sqrt{3rd~diagonal~element~of~(\\mathbf{X^{\\prime }X)}^{-1}} = 1.373\\sqrt{4.11538}=2.785\\) d. \\(t(b_1)=b_1/se(b_1)=0.692/(1.373\\times\\sqrt{0.15385})=1.286\\) 3.6 a. El coeficiente de regresión es -0.1846, lo que significa que cuando los gastos en educación pública aumentan en un 1% del PIB, se espera que la esperanza de vida disminuya en 0.1846 años, manteniendo constantes las demás variables. b. El coeficiente de regresión es -0.2358, lo que significa que cuando los gastos en salud aumentan en un 1% del PIB, se espera que la esperanza de vida disminuya en 0.2358 años, manteniendo constantes las demás variables. c. \\(H_{0}:\\beta_2=0, H_{1}:\\beta_2\\neq0\\). No podemos rechazar la hipótesis nula porque el valor \\(p\\) es mayor que el nivel de significancia, digamos 0.05. Por lo tanto, PUBLICEDUCATION no es una variable estadísticamente significativa. d(i). El propósito del gráfico de variables añadidas es explorar la correlación entre PUBLICEDUCATION y LIFEEXP después de eliminar los efectos de otras variables. d(ii). La correlación parcial es \\[ r=\\frac{t(b_2)}{\\sqrt{t(b_2)^2+n-(k+1)}}=\\frac{-0.6888}{\\sqrt{-0.6888^2+152-(3+1)}}=-0.0565 . \\] Capítulo 4 4.1 a. \\(R^2 = (Regression~SS)/(Total~SS)\\). b. \\(F\\)-ratio=\\((Regression~MS)/(Error~MS)\\). c. \\[ 1-R^2 = \\frac{Total~SS}{Total~SS}-\\frac{Regression~SS}{Total~SS}= \\frac{Error~SS}{Total~SS}. \\] Ahora, desde el lado derecho, tenemos \\[\\begin{align*} \\frac{R^2}{1-R^2} \\frac{(n-(k+1))}{k} &amp;=\\frac{(Regression~SS)/(Total~SS)}{(Error~SS)/(Total~SS)}\\frac{(n-(k+1))}{k}\\\\ &amp;= \\frac{Regression~SS}{Error~SS}\\frac{(n-(k+1))}{k}\\\\ &amp;=\\frac{(Regression~SS)/k}{(Error~SS)/(n-(k+1))}\\\\ &amp;=\\frac{Regression~MS}{Error~MS} = F-\\mathrm{ratio}. \\end{align*}\\] d. \\(F-\\mathrm{ratio}=1.7.\\) e. \\(F-\\mathrm{ratio}=19.7.\\) 4.3 a. El tercer nivel de la estructura organizacional será capturado por el término de intercepción de la regresión. b. \\(H_0\\):TAXEXEMPT no es importante, \\(H_1\\): TAXEXEMPT es importante. \\(p= 0.7694&gt;0.05\\), no rechazamos la hipótesis nula. c. Dado que el valor \\(p = 1.74e^{-6}\\) es menor que el nivel de significancia \\(\\alpha=0.05\\), MCERT es un factor importante para determinar LOGTPY. c(i). La estimación puntual de LOGTPY es 3.988. c(ii). El intervalo de confianza al 95% es $ 3.988  /() = (3.826, 4.150)$. d. \\(R^2=0.1448\\). Todas las variables son estadísticamente significativas. e. \\(R^2= 0.9673\\). Solo LOGNUMBED es estadísticamente significativo a \\(\\alpha=0.05\\). e(i). La correlación parcial es 0.0744. La correlación entre LOGTPY y LOGSQRFOOT es 0.8151. La correlación parcial elimina el efecto de otras variables en LOGTPY. e(ii). El \\(t\\)-ratio prueba si la variable explicativa individual es estadísticamente significativa. El \\(F\\)-ratio prueba si las variables explicativas tomadas en conjunto tienen un impacto significativo en la variable de respuesta. En este caso, solo LOGNUMBED es significativa y el \\(R^2\\) es alto, esto explica por qué el \\(F\\)-ratio es grande mientras que la mayoría de los \\(t\\)-ratios son pequeños. 4.7 a. \\(H_0\\): PUBLICEDUCATION y lnHEALTH no son conjuntamente estadísticamente significativos. Es decir, los coeficientes de las dos variables son iguales a cero. \\(H_1\\): PUBLICEDUCATION y lnHEALTH son conjuntamente estadísticamente significativos. Al menos uno de los coeficientes de las dos variables no es igual a cero. Para tomar la decisión, comparamos las estadísticas \\(F\\) con el valor crítico. Si las estadísticas \\(F\\) son mayores que el valor crítico, rechazamos la hipótesis nula. De lo contrario, no lo hacemos. \\(F-ratio = (6602.7 - 6535.7)/(2 \\times 44.2) = 0.76\\). El 95% de la distribución \\(F\\) con \\(df_1=2\\) y \\(df_2=148\\) es aproximadamente 3.00. Dado que el \\(F-ratio\\) es menor que el valor crítico, no podemos rechazar la hipótesis nula. Es decir, PUBLICEDUCATION y lnHEALTH no son conjuntamente significativos. b. Podemos ver que la esperanza de vida varía entre diferentes regiones. c. \\(H_0\\): \\(\\beta_{REGION}=0\\), \\(H_1\\): \\(\\beta_{REGION}\\neq0\\). Para tomar la decisión, comparamos el valor \\(p\\) con el nivel de significancia \\(\\alpha=0.05\\). Si \\(p&lt;\\alpha\\), rechazamos la hipótesis nula. De lo contrario, no lo hacemos. En este caso, \\(p=0.000 &lt; 0.05\\), por lo que rechazamos la hipótesis nula. REGION es un determinante estadísticamente significativo de LIFEEXP. d(i). Si REGION=Abrab state, \\(\\widehat{LIFEEXP} = 83.3971-2.7559 \\times 2-0.4333\\times 5-0.7939\\times 1=74.9249\\). Si REGION=Sub-Sahara Africa, \\(\\widehat{LIFEEXP} = 83.3971-2.7559\\times 2-0.4333 \\times 5-0.7939 \\times 1 -14.3567 = 60.5682\\). d(ii). El intervalo de confianza al 95% es \\(-14.3567\\pm 1.976\\times 1.8663=(-18.044,-10.669).\\) d(iii). La estimación puntual para la diferencia es 18.1886. Capítulo 5 5.1 a. De la ecuación (2.9), tenemos \\[\\begin{eqnarray*} h_{ii} &amp; = &amp; \\mathbf{x_i}^{\\prime}\\left(\\mathbf{X}^{\\prime }\\mathbf{X}\\right)^{-1}\\mathbf{x_i}\\\\ &amp;=&amp;\\left(\\begin{array}{cc}1 &amp; x_i\\end{array}\\right) \\frac{1}{ \\sum_{i=1}^{n}x_i^2-n\\overline{x}^2} \\left(\\begin{array}{cc}n^{-1}\\sum_{i=1}^{n}x_i^2 &amp; -\\overline{x} \\\\-\\overline{x} &amp; 1\\end{array}\\right) \\left(\\begin{array}{c}1 \\\\x_i\\end{array}\\right)\\\\ &amp;=&amp; \\frac{1}{\\sum_{i=1}^{n}x_i^2-n\\overline{x}^2} \\left( n^{-1}(\\sum_{i=1}^{n}x_i^2-n\\overline{x}^2)+\\overline{x}^2-2\\overline{x}x_i+x_i^2 \\right)\\\\ &amp;=&amp;\\frac{1}{n}+\\frac{(x_i-\\overline{x})^2}{(n-1)s_x^2} . \\end{eqnarray*}\\] El promedio del leverage es \\[ \\bar{h}=\\frac{1}{n} \\sum_{i=1}^n h_{ii} = \\frac{1}{n}+\\frac{1}{n}\\sum_{i=1}^n\\frac{(x_i-\\bar{x})^2}{(n-1)s_{x}^2}=\\frac{1}{n}+\\frac{1}{n}=\\frac{2}{n} \\] Sea \\(c = (x_i-\\bar{x})/s_x\\). Entonces, \\[ \\frac{6}{n}=h_{ii}=\\frac{1}{n}+\\frac{(x_i-\\bar{x})^2}{(n-1)s_{x}^2} = \\frac{1}{n}+\\frac{(cs_x)^2}{(n-1)s_{x}^2}=\\frac{1}{n}+\\frac{c^2}{n-1} . \\] Para \\(n\\) grande, \\(x_i\\) está aproximadamente a \\(c=\\sqrt{5}=2.236\\) desviaciones estándar del promedio. 5.3 a. Los gráficos no se presentan aquí. Al observarlos, es difícil detectar patrones lineales en el gráfico de GDP versus LIFEEXP. La transformación logarítmica de GDP distribuye los valores de GDP, permitiéndonos ver patrones lineales. Argumentos similares se aplican a HEALTH, donde el patrón en lnHEALTH es más lineal. c(ii). Es ambos. El residual estandarizado es -2.66, que excede el umbral de 2, en valor absoluto. El leverage es 0.1529, que es mayor que el umbral \\(3 \\times\\overline{h} =3\\times (k+1)/n = 0.08\\). c(iii). La variable PUBLICEDUCATION ya no es estadísticamente significativa. Capítulo 6 6.1 a. La variable involact está algo sesgada hacia la derecha, pero no de manera drástica. La variable involact tiene varios ceros que pueden ser un problema con variables dependientes limitadas. La variable age parece ser bimodal, con seis observaciones de 28 años o menos y las demás mayores o iguales a 40 años. \\[ \\small{ \\begin{array}{lrrrrrrr} \\hline &amp; &amp; &amp; \\text{Desviación} \\\\ &amp; \\text{Media} &amp; \\text{Mediana} &amp; \\text{Estándar} &amp; \\text{Mínimo} &amp; \\text{Máximo}\\\\ \\hline \\text{race} &amp; 34.9 &amp; 24.5 &amp; 32.6 &amp; 1.0 &amp; 99.7\\\\ \\text{fire} &amp; 12.3 &amp; 10.4 &amp; 9.3 &amp; 2.0 &amp; 39.7\\\\ \\text{theft} &amp; 32.4 &amp; 29.0 &amp; 22.3 &amp; 3.0 &amp; 147.0\\\\ \\text{age} &amp; 60.3 &amp; 65.0 &amp; 22.6 &amp; 2.0 &amp; 90.1\\\\ \\text{income} &amp; 10,696 &amp; 10,694.0 &amp; 2,754 &amp; 5,583 &amp; 21,480\\\\ \\text{volact} &amp; 6.5 &amp; 5.9 &amp; 3.9 &amp; 0.5 &amp; 14.3\\\\ \\text{involact} &amp; 0.6 &amp; 0.4 &amp; 0.6 &amp; 0.0 &amp; 2.2\\\\ \\hline \\end{array} } \\] b. La matriz de dispersión (no presentada aquí) muestra una relación negativa entre volact e involact, una relación negativa entre race y volact, y una relación positiva entre race e involact. Si existiera discriminación racial, esperaríamos que los códigos postales con más minorías tuvieran menos acceso al mercado voluntario (menos costoso), lo que significa que tendrían que recurrir al mercado involuntario para obtener seguro. Tabla de correlaciones \\[ \\small{ \\begin{array}{lrrrrrrr} \\hline &amp; \\text{race}&amp; \\text{fire} &amp;\\text{theft} &amp; \\text{age} &amp; \\text{income} &amp;\\text{volact} &amp;\\text{involact}\\\\ \\hline \\text{race} &amp; 1.000 &amp; &amp; &amp; &amp; &amp; &amp;\\\\ \\text{fire} &amp; 0.593 &amp; 1.000 &amp; &amp; &amp; &amp; &amp;\\\\ \\text{theft} &amp; 0.255 &amp; 0.556 &amp;1.000 &amp; &amp; &amp; &amp;\\\\ \\text{age} &amp; 0.251 &amp; 0.412 &amp;0.318 &amp; 1.000 &amp; &amp; &amp;\\\\ \\text{income} &amp; -0.704 &amp;-0.610 &amp;-0.173 &amp;-0.529 &amp; 1.000 &amp; &amp;\\\\ \\text{volact} &amp; -0.759 &amp;-0.686 &amp;-0.312 &amp;-0.606 &amp; 0.751 &amp;1.000 &amp;\\\\ \\text{involact} &amp; 0.714 &amp; 0.703 &amp; 0.150 &amp; 0.476 &amp; -0.665 &amp;-0.746 &amp; 1.000\\\\ \\hline \\end{array} } \\] d(i). El coeficiente asociado con race es negativo y estadísticamente significativo. d(ii). Los códigos postales con alta influencia son el número 7 y 24. La variable race sigue siendo estadísticamente significativa y negativa. La variable fire ya no es significativa, aunque income se vuelve significativa. La variable race sigue siendo positiva y estadísticamente significativa. De manera similar, el papel de las otras variables no cambia dependiendo de la presencia de los dos puntos de alta influencia. La variable race sigue siendo positiva y estadísticamente significativa. De manera similar, el papel de las otras variables no cambia dependiendo de la presencia de los dos puntos de alta influencia. La influencia depende de las variables explicativas, no de las variables dependientes. Como las variables explicativas permanecieron sin cambios en los tres análisis, las influencias permanecieron sin cambios. La demanda de seguros depende del tamaño de la pérdida a asegurar, la capacidad del solicitante de pagarla y el conocimiento de los contratos de seguros. Para los seguros de vivienda, el tamaño de la pérdida se relaciona con el precio de la vivienda, el tipo de estructura, las precauciones de seguridad disponibles y la susceptibilidad a catástrofes como tornados, inundaciones, etc. La capacidad de pago se basa en ingresos, patrimonio, número de dependientes y otros factores. El conocimiento de los contratos de seguros depende, por ejemplo, de la educación. Todos estos factores omitidos pueden estar relacionados con race. Se esperaría que los códigos postales contiguos compartieran experiencias económicas similares. Podríamos subdividir la ciudad en grupos homogéneos, como el centro de la ciudad y los suburbios. También podríamos realizar mínimos cuadrados ponderados donde los pesos estén dados por la distancia al centro de la ciudad. Capítulo 7 7.1 a. \\[\\begin{align*} \\textrm{E}~y_t &amp;= \\textrm{E}~(y_0+c_1+\\cdots+c_t)=\\textrm{E}~y_0+\\textrm{E}~c_1+\\cdots+\\textrm{E}~c_t\\\\ &amp;= y_0+\\mu_c+\\cdots+\\mu_c = y_0 + t \\mu_c. \\end{align*}\\] \\[\\begin{align*} \\textrm{Var}~y_t &amp;= \\textrm{Var}~(y_0+c_1+\\cdots+c_t)=\\textrm{Var}~c_1+\\cdots+\\textrm{Var}~c_t\\\\ &amp;=\\sigma_c^2+\\cdots+\\sigma_c^2 = t\\sigma_t^2. \\end{align*}\\] 7.3 a(ii). No. Hay una tendencia clara hacia la baja en la serie, lo que indica que la media cambia con el tiempo. b(i). Los \\(t-\\)ratios asociados con las porciones de tendencia lineal y cuadrática son altamente significativos desde el punto de vista estadístico. El \\(R^2 = 0.8733\\) indica que el modelo se ajusta bien. b(ii). El signo de un residuo es muy probable que sea el mismo que el de los residuos precedentes y posteriores. Esto sugiere un alto grado de autocorrelación en los residuos. b(iii). \\(\\widehat{EURO_{702}} = 0.808 + 0.0001295(702) - 4.639 \\times 10^{-7}(702)^2 = 0.6703.\\) c(i). Este es un modelo de caminata aleatoria. c(ii). \\(\\widehat{EURO_{702}} = 0.6795 + 3(-0.0001374) = 0.679088.\\) c(iii). Un intervalo de predicción aproximado al 95% para \\(EURO_{702}\\) es \\[ 0.679088\\pm2(0.003621979)\\sqrt{3} \\approx (0.66654, 0.691635). \\] Capítulo 8 8.1 \\[ \\begin{array}{ll} r_1 &amp;=\\left(\\sum_{t=2}^{5}(y_{t-1}-\\bar y)(y_{t}-\\bar y)\\right) /\\left(\\sum_{t=1}^{5}(y_{t}-\\bar y)^{2}\\right) = -0.0036/0.0134 = -0.2687 \\\\ r_2 &amp;= \\left(\\sum_{t=3}^{5}(y_{t-2}-\\bar y)(y_{t}-\\bar y)\\right) /\\left(\\sum_{t=1}^{5}(y_{t}-\\bar y)^{2}\\right) = 0.0821 \\end{array} \\] 8.3 a. \\(b_1= \\left(\\sum_{t=2}^{T}(y_{t-1}-\\bar y_{-})(y_t-\\bar y_{+})\\right) /\\left(\\sum_{t=2}^{T}(y_{t-1}-\\bar y_{-})^2\\right)\\), donde \\(\\bar y_{+}=\\left(\\sum_{t=2}^{T}y_{t}\\right)/(T-1)\\) y \\(\\bar y_{-}=\\left(\\sum_{t=1}^{T-1}y_{t} \\right)/(T-1)\\). b. \\(b_0=\\bar y_{+}-b_1 \\bar y_{-}\\). c. \\(b_0\\approx \\bar y \\left[ 1- \\left(\\sum_{t=2}^{T}(y_{t-1}-\\bar y_{-})(y_t-\\bar y_{+})\\right) /\\left(\\sum_{t=2}^{T}(y_{t-1}-\\bar y_{-})^2\\right) \\right] \\approx \\bar y \\left[1-r_1\\right]\\). 8.6 a. Dado que la media y la varianza de la secuencia no varían con el tiempo, se puede considerar que la secuencia es débilmente estacionaria. b. Las estadísticas resumen de la secuencia son las siguientes: \\[ \\small{ \\begin{array}{rrrrr} \\hline \\text{Media}&amp; \\text{Mediana}&amp; \\text{Desv. Estándar}&amp; \\text{Mínimo}&amp; \\text{Máximo}\\\\ 0.0004&amp; 0.0008&amp; 0.0064&amp; -0.0182 &amp; 0.0213\\\\ \\hline \\end{array} } \\] Bajo la suposición de ruido blanco, el pronóstico de una observación en el futuro es su media muestral, es decir, 0.0004. Este pronóstico no depende del número de pasos hacia adelante. c. Las autocorrelaciones para los rezagos 1 a 10 se muestran a continuación: \\[ \\small{ \\begin{array}{ccccccccccc} \\hline 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10\\\\ 1.000 &amp; -0.046 &amp; -0.096 &amp; 0.019 &amp; -0.002 &amp; -0.004 &amp; -0.054 &amp; -0.035 &amp; -0.034 &amp; -0.051 &amp; 0.026\\\\ \\hline \\end{array} } \\] Dado que \\(|r_k/se(r_k)|&lt;2\\) (\\(se(r_k)=1/\\sqrt{503} = 0.0446\\)) para \\(i=1,\\ldots,10\\), ninguna de las autocorrelaciones es fuertemente estadísticamente significativa diferente de cero, excepto para el rezago 2. Para el rezago 2, la autocorrelación es \\(0.096/0.0446 = 2.15\\) errores estándar por debajo de cero. Capítulo 11 11.1 a. La función de densidad de probabilidad es \\[ \\mathrm{f}(y)=\\frac{\\partial}{\\partial y}\\mathrm{F}(y) =(-1)(1+e^{-y})^{-2}e^{-y}(-1)= \\frac{e^y}{(1+e^y)^2}. \\] b. \\[ \\mu_y = \\int_{-\\infty}^{\\infty}\\ y \\mathrm{f}(y)dy = \\int_{-\\infty}^{\\infty}\\ y \\frac{e^{y}}{(1+e^{y})^{2}}dy= 0. \\] c. \\[ \\mathrm{E~}y^2 = \\int_{-\\infty}^{\\infty}\\ y^2 \\mathrm{f}(y)dy = \\pi^2 /3 . \\] Dado que \\(\\mu_y = 0\\), la desviación estándar es \\(\\sigma_y = \\pi / \\sqrt{3}=1.813798.\\) d. La función de densidad de probabilidad para \\(y^{\\ast \\ast}\\) es \\[\\begin{eqnarray*} \\mathrm{f}^{\\ast}(y)&amp;=&amp; \\frac{\\partial}{\\partial y}\\Pr(y^{\\ast \\ast} \\leq y) = \\frac{\\partial}{\\partial y}\\Pr(y^{\\ast } \\leq y \\sigma_y + \\mu_y) =\\sigma_y \\mathrm{f}(y \\sigma_y) = \\sigma_y \\frac{e^{y\\sigma_y}}{(1+e^{y\\sigma_y})^2}. \\end{eqnarray*}\\] 11.3 Sea \\(\\Pr(\\varepsilon_{i1} \\leq a) = F(a)=\\exp(-e^{-a})\\) y \\(f(a)=\\frac{dF(a)}{da}=\\exp(-e^{-a})e^{-a}.\\) Entonces \\[\\begin{align*} \\Pr(\\varepsilon_{i2}-\\varepsilon_{i1} \\leq a) &amp;=\\int_{-\\infty}^{\\infty}F(a+y)f(y)\\,dy=\\int_{-\\infty}^{\\infty}\\exp\\left[-e^{-y}(e^{-a}+1)\\right]e^{-y}\\,dy\\\\ &amp;=\\int_{\\infty}^0\\exp(-zA)z \\,d(-\\ln z)=-\\int_{\\infty}^0\\exp(-zA)\\,dz\\\\ &amp;= \\frac{\\exp(-zA)}{A}|_\\infty^0=\\frac{1}{A}=\\frac{1}{1+e^{-a}}, \\end{align*}\\] con \\(A=e^{-a}+1\\) y \\(z=e^{-y}\\). Así, \\[ \\pi_i=\\Pr (\\epsilon _{i2}-\\epsilon _{i1}&lt;V_{i1}-V_{i2})=\\Pr (\\epsilon _{i2}-\\epsilon _{i1}&lt;\\mathbf{x}_i^{\\mathbf{\\prime }}\\boldsymbol \\beta) =\\frac{1}{1+\\exp (-\\mathbf{x}_i^{\\mathbf{\\prime }}\\boldsymbol \\beta)}. \\] 11.5 De la ecuación (11.5) sabemos que \\[ \\sum\\limits_{i=1}^{n}\\mathbf{x}_i\\left( y_i-\\mathrm{\\pi }(\\mathbf{x} _i^{\\mathbf{\\prime}}\\mathbf{b}_{MLE})\\right) = \\sum\\limits_{i=1}^{n}(1~~x_{i1}~~\\cdots~~x_{ik})^\\prime \\left( y_i-\\mathrm{\\pi }(\\mathbf{x} _i^{\\mathbf{\\prime}}\\mathbf{b}_{MLE})\\right) =(0~~0~~\\cdots~~0)^{\\prime}. \\] De la primera fila, obtenemos \\(\\sum\\limits_{i=1}^{n} \\left( y_i-\\mathrm{\\pi }(\\mathbf{x} _i^{\\mathbf{\\prime}}\\mathbf{b}_{MLE})\\right) =0\\). Dividiendo por \\(n\\) se obtiene \\(\\overline{y} = n^{-1} \\sum_{i=1}^n \\widehat{y}_i .\\) 11.7 a. La derivada de la función logit es \\[ \\frac{\\partial}{\\partial y}\\mathrm{\\pi}(y) = \\mathrm{\\pi}(y)\\frac{1}{(1+e^y)}=\\mathrm{\\pi}(y)(1-\\mathrm{\\pi}(y)). \\] Por lo tanto, usando la regla de la cadena y la ecuación (11.5), tenemos \\[ \\begin{array}{ll} \\mathbf{I}(\\boldsymbol \\beta) &amp;=&amp; - \\mathrm{E~}\\frac{\\partial ^{2}}{\\partial \\boldsymbol \\beta\\partial \\boldsymbol \\beta ^{\\prime }}L(\\boldsymbol \\beta) = -\\mathrm{E~} \\frac{\\partial }{\\partial \\boldsymbol \\beta^{\\prime}} \\left( \\sum\\limits_{i=1}^{n}\\mathbf{x}_i\\left( y_i-\\mathrm{\\pi }(\\mathbf{x} _i^{\\mathbf{\\prime }}\\boldsymbol \\beta)\\right) \\right) \\\\ &amp;=&amp; \\sum\\limits_{i=1}^{n}\\mathbf{x}_i \\frac{\\partial }{\\partial \\boldsymbol \\beta^{\\prime}} \\mathrm{\\pi }(\\mathbf{x} _i^{\\mathbf{\\prime }}\\boldsymbol \\beta) =\\sum\\limits_{i=1}^{n}\\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\mathrm{\\pi}(\\mathbf{x} _i^{\\mathbf{\\prime}}\\boldsymbol \\beta)(1-\\mathrm{\\pi}(\\mathbf{x} _i^{\\mathbf{\\prime}}\\boldsymbol \\beta)). \\end{array} \\] Esto proporciona el resultado con \\(\\sigma_i^2 = \\mathrm{\\pi}(\\mathbf{x}_i^{\\prime} \\boldsymbol \\beta)(1-\\mathrm{\\pi}(\\mathbf{x}_i^{\\prime}\\boldsymbol \\beta))\\). Definir \\(\\mathbf{a}_i=\\mathbf{x}_i\\left( y_i-\\mathrm{\\pi }(\\mathbf{x} _i^{\\mathbf{\\prime}}\\boldsymbol \\beta)\\right)\\) y \\(\\mathbf{H}_i = \\frac{\\partial }{\\partial \\boldsymbol \\beta^{\\prime}}\\mathbf{a}_i =-\\mathbf{x}_i \\mathbf{x}_i^{\\mathbf{\\prime}} \\mathrm{\\pi }^{\\prime}( \\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta).\\) Nótese que \\(\\mathrm{E}(\\mathbf{a}_i)=\\mathbf{x}_i \\mathrm{E}\\left( y_i-\\mathrm{\\pi }(\\mathbf{x} _i^{\\mathbf{\\prime}}\\boldsymbol \\beta)\\right)= \\mathbf{0}\\). Además, definimos \\(b_i=\\frac{\\mathrm{\\pi }^{\\prime}( \\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta)}{\\mathrm{\\pi }(\\mathbf{x} _i^{\\mathbf{\\prime}}\\boldsymbol \\beta)(1-\\mathrm{\\pi }(\\mathbf{x}_i^{ \\mathbf{\\prime}}\\boldsymbol \\beta))}\\). Con esta notación, la función de score es \\(\\frac{\\partial }{\\partial \\boldsymbol \\beta}L(\\boldsymbol \\beta) = \\sum\\limits_{i=1}^{n} \\mathbf{a}_i b_i\\). Así, \\[ \\begin{array}{ll} \\mathbf{I}(\\boldsymbol \\beta) &amp; = - \\mathrm{E} \\left( \\frac{\\partial^2}{\\partial \\boldsymbol \\beta ~ \\partial \\boldsymbol \\beta ^{\\prime}}L(\\boldsymbol \\beta) \\right) = - \\mathrm{E} \\left( \\frac{\\partial}{\\partial \\boldsymbol \\beta^{\\prime}}\\sum\\limits_{i=1}^{n} \\mathbf{a}_i b_i\\right) \\\\ &amp;= - \\mathrm{E} \\left( \\sum\\limits_{i=1}^{n} \\left( \\left(\\frac{\\partial}{\\partial \\boldsymbol \\beta^{\\prime}}\\mathbf{a}_i \\right) b_i + \\mathbf{a}_i \\frac{\\partial}{\\partial \\boldsymbol \\beta^{\\prime}}b_i \\right) \\right) = - \\sum\\limits_{i=1}^{n} \\left[\\mathrm{E}(\\mathbf{H}_i)b_i + \\mathrm{E}(\\mathbf{a}_i) \\frac{\\partial}{\\partial \\boldsymbol \\beta^{\\prime}}b_i \\right] \\\\ &amp;= - \\sum\\limits_{i=1}^{n} \\mathbf{H}_i b_i = \\sum\\limits_{i=1}^{n} \\mathbf{x}_i \\mathbf{x}_i^{\\mathbf{\\prime}} \\frac {\\left( \\mathrm{\\pi }^{\\prime}( \\mathbf{x}_i^{\\mathbf{\\prime}}\\boldsymbol \\beta)\\right)^2} {\\mathrm{\\pi }(\\mathbf{x} _i^{\\mathbf{\\prime}}\\boldsymbol \\beta)(1-\\mathrm{\\pi }(\\mathbf{x}_i^{ \\mathbf{\\prime}}\\boldsymbol \\beta))} . \\end{array} \\] 11.8 a(i). Los gráficos no se presentan aquí. \\[ \\small{ \\begin{array}{lrrrrrrr} \\hline &amp; mean &amp; median &amp; std &amp; minimum &amp; maximum\\\\ \\hline \\text{CLMAGE} &amp; 32.531 &amp; 31.000 &amp; 17.089 &amp; 0.000 &amp; 95.000\\\\ \\text{LOSS} &amp; 5.954 &amp; 2.331 &amp; 33.136 &amp; 0.005 &amp; 1067.700\\\\ \\hline \\end{array} } \\] a(ii). No para CLMAGE, pero ambas versiones de LOSS parecen diferir por ATTORNEY. \\[ \\small{ \\begin{array}{crrr} \\hline \\text{ ATTORNEY} &amp; \\text{CLMAGE} &amp; \\text{LOSS} &amp; \\text{lnLOSS} \\\\ \\hline 1 &amp; 32.270 &amp; 9.863 &amp; 1.251\\\\ 2 &amp; 32.822 &amp; 1.865 &amp; -0.169\\\\ \\hline \\end{array} } \\] a(iii). SEATBELT y CLMINSUR parecen ser diferentes, CLMSEX y MARITAL menos. \\[ \\small{ \\begin{array}{cccccccc} \\hline \\text{ATTORNEY} &amp; \\text{CLMSEX} &amp; &amp; \\text{MARITAL} &amp;&amp;&amp; &amp; \\text{CLMINSUR} &amp; &amp; \\text{SEATBELT} \\\\ &amp; 1 &amp; 2 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 1 &amp; 2 &amp; 1 &amp; 2 \\\\ \\hline 1 &amp; 325 &amp; 352 &amp; 320 &amp; 329 &amp; 6 &amp; 20 &amp; 76 &amp; 585 &amp; 643 &amp; 16 \\\\ 2 &amp; 261 &amp; 390 &amp; 304 &amp; 321 &amp; 9 &amp; 15 &amp; 44 &amp; 594 &amp; 627 &amp; 6 \\\\ \\hline \\end{array} } \\] a(iv). El número de valores faltantes se muestra como: \\[ \\small{ \\begin{array}{cccccc} \\hline \\text{CLMAGE} &amp; \\text{LOSS} &amp; \\text{CLMSEX} &amp; \\text{MARITAL}&amp; \\text{CLMINSUR} &amp; \\text{SEATBELT}\\\\ 189 &amp; 12 &amp; 16 &amp; 41 &amp; 48 &amp; NA\\\\ \\hline \\end{array} } \\] b(i). La variable CLMSEX es estadísticamente significativa. La razón de probabilidades es \\(\\exp(-0.3218) = 0.7248\\), lo que indica que las mujeres tienen un 72% de probabilidad de usar un abogado en comparación con los hombres (o los hombres tienen 1/0.72 = 1.379 veces más probabilidades de usar un abogado que las mujeres). b(ii). CLMSEX y CLMINSUR son estadísticamente significativas, y SEATBELT es algo significativo, según los valores \\(p\\). CLMAGE no es significativo. MARITAL no parece ser estadísticamente significativo. b(iii). Los hombres usan abogados con más frecuencia: la razón de probabilidades es \\(\\exp(-0.37691) = 0.686\\), lo que indica que las mujeres tienen un 68.6% de probabilidad de usar un abogado en comparación con los hombres. b(iv). La versión logarítmica, lnLOSS, es más importante. En el modelo final sin LOSS, el valor \\(p\\) asociado con lnLOSS fue muy pequeño (\\(&lt; 2e-16\\)), lo que indica una fuerte significación estadística. b(v). Todas las variables permanecen iguales excepto una de las variables binarias de MARITAL, que se vuelve marginalmente estadísticamente significativa. La principal diferencia es que estamos utilizando 168 observaciones adicionales al no requerir que CLMAGE esté en el modelo. b(vi). Para el componente sistemático, tenemos: \\[ \\begin{array}{llll} \\mathbf{x}^{\\prime}\\mathbf{b}_{MLE} &amp;= 0.75424 \\\\ &amp;~~~ -0.51210* (\\text{CLMSEX}=2)&amp; + 0.04613*(\\text{MARITAL}=2) \\\\ &amp;~~~+ 0.37762*(\\text{MARITAL}=3) &amp;+ 0.12099*(\\text{MARITAL}=4) \\\\ &amp;~~~+ 0.13692*(\\text{SEATBELT}=2) &amp;-0.52960*(\\text{CLMINSUR}=2) \\\\ &amp;~~~-0.01628* \\text{CLMAGE} &amp;+ 0.98260*\\text{lnLOSS} \\\\ &amp;= 1.3312. \\end{array} \\] La probabilidad estimada de usar un abogado es \\[ \\widehat{\\pi} = \\frac {\\exp (1.3312)}{1+\\exp (1.3312)} = 0.791. \\] c. Las mujeres tienen menos probabilidades de usar abogados. Aquellos que no usan cinturón de seguridad (SEATBELT=2) tienen más probabilidades de usar un abogado (aunque no es significativo). Los solteros (MARITAL=2) tienen más probabilidades de usar un abogado. Los reclamantes asegurados (CLMINSUR=2) tienen menos probabilidades de usar un abogado. A mayor pérdida, mayor es la probabilidad de que un abogado esté involucrado. 11.11 a. El intercepto y las variables PLACE%, MSAT, RANK son significativas al nivel del 5%. b(i). La probabilidad de éxito para este caso es 0.482. b(ii). La probabilidad de éxito para este caso es 0.281. b(iii). La probabilidad de éxito para este caso es 0.366. b(iv). La probabilidad de éxito para este caso es 0.497. b(v). La probabilidad de éxito para este caso es 0.277. Chapter 12 12.1 Tomamos la derivada de la ecuación (12.2) con respecto a \\(\\mu\\) y establecemos la condición de primer orden igual a cero. Con esto, tenemos \\(\\partial L(\\mu)/\\partial \\mu=\\sum_{i=1}^{n}(-1+y_i/\\mu)=0\\), es decir, \\(\\hat\\mu=\\bar y\\). 12.3 a. De la expresión de la ecuación del score (12.5), \\[ \\left. \\frac{\\partial }{\\partial \\boldsymbol \\beta} L(\\boldsymbol \\beta )\\right\\vert _{\\mathbf{\\beta =b}}= \\sum_{i=1}^{n}\\left( y_i-\\widehat{\\mu }_i\\right) \\left( \\begin{array}{c} 1 \\\\ x_{i,1} \\\\ \\vdots \\\\ x_{i,k} \\\\ \\end{array} \\right) = \\mathbf{0}. \\] De la primera fila, tenemos que el promedio de los residuales \\(e_i = y_i - \\widehat{\\mu}_i\\) es igual a cero. b. De la fila \\((j+1)^{st}\\) de la ecuación del score (12.5), tenemos \\[ \\sum_{i=1}^{n} e_i x_{i,j}= 0. \\] Debido a que los residuales tienen un promedio igual a cero, la covarianza muestral entre los residuales y \\(x_j\\) es cero, y por lo tanto, la correlación muestral es cero. 12.5 a. La distribución de COUNTOP tiene una cola larga y está sesgada hacia la derecha. La varianza (\\(12.5^2 = 156.25\\)) es mucho mayor que la media, 5.67. \\[ \\small{ \\begin{array}{cccccc} \\hline &amp;1st &amp; &amp; &amp;3rd &amp; \\\\ \\text{Minimum} &amp; \\text{Quartile} &amp; \\text{Median} &amp; \\text{Mean} &amp;\\text{Quartile} &amp; \\text{Maximum}\\\\\\hline 0.00 &amp; 0.00 &amp; 2.00 &amp; 5.67 &amp; 6.00 &amp; 167.00\\\\ \\hline \\end{array} } \\] Sí, las tablas sugieren que la mayoría de las variables tienen un impacto significativo en COUNTOP. La estadística chi-cuadrado de Pearson es 55044. d(i). Todas las variables parecen ser muy significativas estadísticamente. d(ii). El coeficiente de GENDER es 0.4197. Aproximadamente, esperaríamos que las mujeres tengan un 42% más de gastos ambulatorios que los hombres. d(iii). La estadística chi-cuadrado es 33,214 - más baja que la parte (b) (55,044). Esto indica que las covariables ayudan en el proceso de ajuste. La significancia estadística también indica que las covariables son estadísticamente significativas, pero la sobredispersión es sospechosa - ver d(iv). d(iv). Ahora, la mayoría de las variables siguen siendo estadísticamente significativas, pero la fuerza de la significancia estadística ha disminuido dramáticamente. No está claro si la variable income es estadísticamente significativa. e(i). Todas las variables parecen ser estadísticamente significativas. La variable income es quizás la menos importante. e(ii). La estadística chi-cuadrado es 33,660 - más alta que el modelo Poisson (33,214) pero más baja que la parte (b) (55,044). Esto sugiere que los dos modelos se ajustan de manera similar, con el modelo Poisson teniendo una ligera ventaja. El AIC para el Poisson básico es 22,725 - lo cual es mucho más alto que el AIC para el binomial negativo (10,002). Por lo tanto, el binomial negativo es preferido al Poisson básico. Sin embargo, el quasi-Poisson probablemente sea tan bueno como el binomial negativo. e(iii). Según el output, la estadística de la prueba de razón de verosimilitud es 18.7 - basada en 4 grados de libertad, el \\(p\\)-valor es 0.000915. Esto indica que income es un factor estadísticamente significativo en el modelo. Para GENDER, educación, estado de salud personal, anylimit, income e insurance, los modelos reportan el mismo signo y efectos estadísticamente significativos. RACE no parece ser estadísticamente significativo en el modelo de regresión logística. Para REGION, los signos parecen ser los mismos aunque la significancia estadística ha cambiado. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
