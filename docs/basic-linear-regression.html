<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Basic Linear Regression | Regression Modeling with Actuarial and Financial Applications</title>
  <meta name="description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Basic Linear Regression | Regression Modeling with Actuarial and Financial Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Basic Linear Regression | Regression Modeling with Actuarial and Financial Applications" />
  
  <meta name="twitter:description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  

<meta name="author" content="Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap-1.html"/>
<link rel="next" href="bibliography.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript"  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleSolution(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}      
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>



<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Regression Modeling With Actuarial and Financial Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="it-takes-a-team.html"><a href="it-takes-a-team.html"><i class="fa fa-check"></i>It Takes a Team</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#forward"><i class="fa fa-check"></i>Forward</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#who-is-this-book-for"><i class="fa fa-check"></i>Who Is This Book For?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#what-is-this-book-about"><i class="fa fa-check"></i>What Is This Book About?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#how-does-this-book-deliver-its-message"><i class="fa fa-check"></i>How Does This Book Deliver Its Message?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="chap-1.html"><a href="chap-1.html"><i class="fa fa-check"></i><b>1</b> Chap 1</a></li>
<li class="chapter" data-level="2" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Basic Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#correlations-and-least-squares"><i class="fa fa-check"></i><b>2.1</b> Correlations and Least Squares</a>
<ul>
<li class="chapter" data-level="" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#scatter-plot-and-correlation-coefficients---basic-summary-tools"><i class="fa fa-check"></i>Scatter Plot and Correlation Coefficients - Basic Summary Tools</a></li>
<li class="chapter" data-level="" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#method-of-least-squares"><i class="fa fa-check"></i>Method of Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#basic-linear-regression-model"><i class="fa fa-check"></i><b>2.2</b> Basic Linear Regression Model</a></li>
<li class="chapter" data-level="2.3" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#S2:SummStats"><i class="fa fa-check"></i><b>2.3</b> Is the Model Useful? Some Basic Summary Measures</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#partitioning-the-variability"><i class="fa fa-check"></i><b>2.3.1</b> Partitioning the Variability</a></li>
<li class="chapter" data-level="2.3.2" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#the-size-of-a-typical-deviation-s"><i class="fa fa-check"></i><b>2.3.2</b> The Size of a Typical Deviation: <em>s</em></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#properties-of-regression-coefficient-estimators"><i class="fa fa-check"></i><b>2.4</b> Properties of Regression Coefficient Estimators</a></li>
<li class="chapter" data-level="2.5" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#statistical-inference"><i class="fa fa-check"></i><b>2.5</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#is-the-explanatory-variable-important-the-t-test"><i class="fa fa-check"></i><b>2.5.1</b> Is the Explanatory Variable Important?: The <em>t</em>-Test</a></li>
<li class="chapter" data-level="2.5.2" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>2.5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="2.5.3" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#prediction-intervals"><i class="fa fa-check"></i><b>2.5.3</b> Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#S2:ResidualAnalsis"><i class="fa fa-check"></i><b>2.6</b> Building a Better Model: Residual Analysis</a></li>
<li class="chapter" data-level="2.7" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#S2:CAPM"><i class="fa fa-check"></i><b>2.7</b> Application: Capital Asset Pricing Model</a>
<ul>
<li class="chapter" data-level="" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#data"><i class="fa fa-check"></i>Data</a></li>
<li class="chapter" data-level="" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#unusual-points"><i class="fa fa-check"></i>Unusual Points</a></li>
<li class="chapter" data-level="" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#sensitivity-analysis"><i class="fa fa-check"></i>Sensitivity Analysis</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#illustrative-regression-computer-output"><i class="fa fa-check"></i><b>2.8</b> Illustrative Regression Computer Output</a></li>
<li class="chapter" data-level="2.9" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#further-reading-and-references"><i class="fa fa-check"></i><b>2.9</b> Further Reading and References</a></li>
<li class="chapter" data-level="2.10" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#exercises"><i class="fa fa-check"></i><b>2.10</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#sections-2.3-2.4"><i class="fa fa-check"></i>Sections 2.3-2.4</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#technical-supplement---elements-of-matrix-algebra"><i class="fa fa-check"></i><b>2.11</b> Technical Supplement - Elements of Matrix Algebra</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#basic-definitions"><i class="fa fa-check"></i><b>2.11.1</b> Basic Definitions</a></li>
<li class="chapter" data-level="2.11.2" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#some-special-matrices"><i class="fa fa-check"></i><b>2.11.2</b> Some Special Matrices</a></li>
<li class="chapter" data-level="2.11.3" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#basic-operations"><i class="fa fa-check"></i><b>2.11.3</b> Basic Operations</a></li>
<li class="chapter" data-level="" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#scalar-multiplication"><i class="fa fa-check"></i>Scalar Multiplication</a></li>
<li class="chapter" data-level="" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#addition-and-subtraction-of-matrices"><i class="fa fa-check"></i>Addition and Subtraction of Matrices</a></li>
<li class="chapter" data-level="" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#matrix-multiplication"><i class="fa fa-check"></i>Matrix Multiplication</a></li>
<li class="chapter" data-level="" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#matrix-inverses"><i class="fa fa-check"></i>Matrix Inverses</a></li>
<li class="chapter" data-level="2.11.4" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#random-matrices"><i class="fa fa-check"></i><b>2.11.4</b> Random Matrices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTextDev/RegressionSpanish/" target="blank">Spanish Regression on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Regression Modeling with Actuarial and Financial Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="basic-linear-regression" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Basic Linear Regression<a href="basic-linear-regression.html#basic-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Chapter Preview</em>. This chapter considers regression in the case of only one explanatory variable. Despite this seeming simplicity, most of the deep ideas of regression can be developed in this framework. By limiting ourselves to the one variable case, we are able to express many calculations using simple algebra. This will allow us to develop our intuition about regression techniques by reinforcing it with simple demonstrations. Further, we can illustrate the relationships between two variables graphically because we are working in only two dimensions. Graphical tools prove to be important for developing a link between the data and a model.</p>
<div id="correlations-and-least-squares" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Correlations and Least Squares<a href="basic-linear-regression.html#correlations-and-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p></p>
<p>Regression is about relationships. Specifically, we will study how two variables, an <span class="math inline">\(x\)</span> and a <span class="math inline">\(y\)</span>, are related. We want to be able to answer questions such as, if we change the level of <span class="math inline">\(x\)</span>, what will happen to the level of <span class="math inline">\(y\)</span>? If we compare two “subjects” that appear similar except for the <span class="math inline">\(x\)</span> measurement, how will their <span class="math inline">\(y\)</span> measurements differ? Understanding relationships among variables is critical for quantitative management, particularly in actuarial science where uncertainty is so prevalent.</p>
<p>It is helpful to work with a specific example to become familiar with key concepts. Analysis of lottery sales has not been part of traditional actuarial practice but it is a growth area in which actuaries could contribute.</p>
<hr />
<p></p>
<p><strong>Example: Wisconsin Lottery Sales.</strong> State of Wisconsin lottery administrators are
interested in assessing factors that affect lottery sales. Sales
consists of online lottery tickets that are sold by selected retail
establishments in Wisconsin. These tickets are generally priced at
$1.00, so the number of tickets sold equals the lottery revenue. We
analyze average lottery sales (SALES) over a forty-week period,
April, 1998 through January, 1999, from fifty randomly selected
areas identified by postal (ZIP) code within the state of Wisconsin.</p>
<p>Although many economic and demographic variables might influence
sales, our first analysis focuses on population (POP) as a key
determinant. Chapter 3 will show how to consider additional
explanatory variables. Intuitively, it seems clear that geographic
areas with more people will have higher sales. So, other things being
equal, a larger <span class="math inline">\(x=POP\)</span> means a larger <span class="math inline">\(y=SALES.\)</span> However, the
lottery is an important source of revenue for the state and we want
to be as precise as possible.</p>
<p>A little additional notation will be useful subsequently. In this
sample, there are fifty geographic areas and we use subscripts to
identify each area. For example, <span class="math inline">\(y_1\)</span> = 1,285.4 represents sales
for the first area in the sample that has population <span class="math inline">\(x_1\)</span> = 435.
Call the ordered pair (<span class="math inline">\(x_1\)</span>, <span class="math inline">\(y_1\)</span>) = (435, 1285.4) the first
<em>observation</em>. Extending this notation, the entire sample
containing fifty observations may be represented by (<span class="math inline">\(x_1\)</span>, <span class="math inline">\(y_1\)</span>),
…, (<span class="math inline">\(x_{50}\)</span>, <span class="math inline">\(y_{50}\)</span>). The ellipses ( … ) mean
that the pattern is continued until the final object is encountered.
We will often speak of a generic member of the sample, referring to
(<span class="math inline">\(x_i\)</span>, <span class="math inline">\(y_i\)</span>) as the <span class="math inline">\(i\)</span>th observation.</p>
<p>Data sets can get complicated, so it will help if you begin by working with each variable separately. The two panels in Figure <a href="basic-linear-regression.html#fig:HistPopSales">2.1</a> show histograms that give a quick visual impression of the distribution of each variable in isolation of the other. Table <a href="basic-linear-regression.html#tab:SummaryStats">2.1</a> provides corresponding numerical summaries. To illustrate, for the population variable (POP), we see that the area with the smallest number contained 280 people whereas the largest contained 39,098. The average, over 50 ZIP codes, was 9,311.04. For our second variable, sales were as low as 189 and as high as 33,181.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:HistPopSales"></span>
<img src="RegressionMarkdown_files/figure-html/HistPopSales-1.png" alt="Histograms of Population and Sales. Each distribution is skewed to the right, indicating that there are many small areas compared to a few areas with larger sales and populations." width="60%" />
<p class="caption">
Figure 2.1: <strong>Histograms of Population and Sales.</strong> Each distribution is skewed to the right, indicating that there are many small areas compared to a few areas with larger sales and populations.
</p>
</div>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:SummaryStats">Table 2.1: </span><strong>Summary Statistics of Each Variable</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Median
</th>
<th style="text-align:right;">
Standard Deviation
</th>
<th style="text-align:right;">
Minimum
</th>
<th style="text-align:right;">
Maximum
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
POP
</td>
<td style="text-align:right;width: 1.6cm; ">
9311
</td>
<td style="text-align:right;width: 1.6cm; ">
4406
</td>
<td style="text-align:right;width: 1.6cm; ">
11098
</td>
<td style="text-align:right;width: 1.6cm; ">
280
</td>
<td style="text-align:right;width: 1.6cm; ">
39098
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SALES
</td>
<td style="text-align:right;width: 1.6cm; ">
6495
</td>
<td style="text-align:right;width: 1.6cm; ">
2426
</td>
<td style="text-align:right;width: 1.6cm; ">
8103
</td>
<td style="text-align:right;width: 1.6cm; ">
189
</td>
<td style="text-align:right;width: 1.6cm; ">
33181
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Source:</span> <sup></sup> Frees and Miller (2003)
</td>
</tr>
</tfoot>
</table>
<p>As Table <a href="basic-linear-regression.html#tab:SummaryStats">2.1</a> shows, the basic summary statistics
give useful ideas of the structure of key features of the data.
After we understand the information in each variable in isolation of
the other, we can begin exploring the relationship between the two
variables.</p>
<div id="scatter-plot-and-correlation-coefficients---basic-summary-tools" class="section level3 unnumbered hasAnchor">
<h3>Scatter Plot and Correlation Coefficients - Basic Summary Tools<a href="basic-linear-regression.html#scatter-plot-and-correlation-coefficients---basic-summary-tools" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p></p>
<p>The basic graphical tool used to investigate the relationship
between the two variables is a <em>scatter plot</em> such as in Figure
<a href="basic-linear-regression.html#fig:SalesVsPoP">2.2</a>. Although we may lose the exact values of the
observations when graphing data, we gain a visual impression of the
relationship between population and sales. From Figure
<a href="basic-linear-regression.html#fig:SalesVsPoP">2.2</a> we see that areas with larger populations tend
to purchase more lottery tickets. How strong is this relationship?
Can knowledge of the area’s population help us anticipate the
revenue from lottery sales? We explore these two questions below.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:SalesVsPoP"></span>
<img src="RegressionMarkdown_files/figure-html/SalesVsPoP-1.png" alt="A scatter plot of the lottery data. Each of the 50 plotting symbols corresponds to a zip code in the study. This figure suggests that postal areas with larger populations have larger lottery revenues." width="60%" />
<p class="caption">
Figure 2.2: <strong>A scatter plot of the lottery data</strong>. Each of the 50 plotting symbols corresponds to a zip code in the study. This figure suggests that postal areas with larger populations have larger lottery revenues.
</p>
</div>
<p>One way to summarize the strength of the relationship between two variables is through a <em>correlation</em> statistic.</p>
<p>
</p>
<div class="blackbox">
<p><em>Definition</em>. The <em>ordinary, or Pearson, correlation</em> coefficient is defined as
<span class="math display">\[\begin{equation*}
r=\frac{1}{(n-1)s_xs_y}\sum_{i=1}^{n}\left(
x_{i}-\overline{x}\right) \left( y_{i}-\overline{y}\right) .
\end{equation*}\]</span></p>
<p>Here, we use the sample standard deviation <span class="math inline">\(s_y = \sqrt{(n-1)^{-1} \sum_{i=1}^{n}\left( y_i - \overline{y}\right)^{2}}\)</span> defined in Section 1.2, with similar notation for <span class="math inline">\(s_x\)</span>.</p>
</div>
<p>Although there are other correlation statistics, the correlation
coefficient devised by Pearson (1895) has several desirable
properties. One important property is that, for any data set, <span class="math inline">\(r\)</span> is
bounded by -1 and 1, that is, <span class="math inline">\(-1\leq r\leq 1\)</span>. (Exercise 2A.9
provides steps for you to check this property.) If <span class="math inline">\(r\)</span> is greater
than zero, the variables are said to be <em>(positively)
correlated</em>. If <span class="math inline">\(r\)</span> is less than zero, the variables are said to be
<em>negatively correlated</em>. The larger the coefficient is in
absolute value, the stronger is the relationship. In fact, if <span class="math inline">\(r=1\)</span>,
then the variables are perfectly correlated. In this case, all of
the data lie on a straight line that goes through the lower left and
upper right-hand quadrants. If <span class="math inline">\(r=-1\)</span>, then all of the data lie on a
line that goes through the upper left and lower right-hand
quadrants. The coefficient <span class="math inline">\(r\)</span> is a measure of a <em>linear</em>
relationship between two variables.</p>
<p>The correlation coefficient is said to be <em>location and scale
invariant</em>. Thus, each variable’s center of location does not matter
in the calculation of <span class="math inline">\(r\)</span>. For example, if we add $100 to the sales
of each zip code, each <span class="math inline">\(y_i\)</span> will increase by 100. However,
<span class="math inline">\(\overline{y}\)</span>, the average purchase price will also increase by 100
so that the deviation <span class="math inline">\(y_i - \overline{y}\)</span> remains unchanged, or
invariant. Further, the scale of each variable does not matter in
the calculation of <span class="math inline">\(r\)</span>. For example, suppose we divide each
population by 1000 so that <span class="math inline">\(x_i\)</span> now represents population in
thousands. Thus, <span class="math inline">\(\overline{x}\)</span> is also divided by 1000 and you
should check that <span class="math inline">\(s_x\)</span> is also divided by 1000. Thus, the
standardized version of <span class="math inline">\(x_i\)</span>, <span class="math inline">\(\left( x_i-\overline{x}\right) /s_x\)</span>, remains unchanged, or invariant. Many statistical packages
compute a standardized version of a variable by subtracting the
average and dividing by the standard deviation. Now, let’s use
<span class="math inline">\(y_{i,std}=\left( y_i- \overline{y}\right) /s_y\)</span> and
<span class="math inline">\(x_{i,std}=\left( x_i-\overline{x} \right) /s_x\)</span> to be the
standardized versions of <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span>, respectively. With this
notation, we can express the correlation coefficient as
<span class="math inline">\(r=(n-1)^{-1}\sum_{i=1}^{n}x_{i,std}\times y_{i,std}.\)</span></p>
<p>The correlation coefficient is said to be a <em>dimensionless
measure</em>. This is because we have taken away dollars, and all other
units of measures, by considering the standardized variables
<span class="math inline">\(x_{i,std}\)</span> and <span class="math inline">\(y_{i,std}\)</span>. Because the correlation coefficient
does not depend on units of measure, it is a statistic that can
readily be compared across different data sets.</p>
<p></p>
<p>In the world of business, the term “correlation” is often used as synonymous with the term “relationship.” For the purposes of this text, we use the term correlation when referring only to linear relationships. The classic nonlinear relationship is <span class="math inline">\(y=x^{2}\)</span>, a quadratic relationship. Consider this relationship and the fictitious data set for <span class="math inline">\(x\)</span>, <span class="math inline">\(\{-2,1,0,1,2\}\)</span>. Now, as an exercise (2.<span class="math inline">\(\ref{Ex:ZeroCorr}\)</span>), produce a rough graph of the data set:</p>
<p><span class="math display">\[
\begin{array}{l|rrrrr}
\hline
i &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\ \hline
x_i &amp; -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 \\
y_i &amp; 4 &amp; 1 &amp; 0 &amp; 1 &amp; 4 \\ \hline
\end{array}
\]</span></p>
<p>The correlation coefficient for this data set turns out to be <span class="math inline">\(r=0\)</span>
(check this). Thus, despite the fact that there is a perfect relationship
between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> (<span class="math inline">\(=x^{2}\)</span>), there is a zero correlation. Recall that
location and scale changes are not relevant in correlation discussions, so
we could easily change the values of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> to be more representative
of a business data set.</p>
<p>How strong is the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> for the lottery
data? Graphically, the response is a scatter plot, as in Figure
<a href="basic-linear-regression.html#fig:SalesVsPoP">2.2</a>. Numerically, the main response is the
correlation coefficient which turns out to be <span class="math inline">\(r\)</span> = 0.886 for this
data set. We interpret this statistic by saying that SALES and POP
are (positively) correlated. The strength of the relationship is
strong because <span class="math inline">\(r\)</span> = 0.886 is close to one. In summary, we may
describe this relationship by saying that there is a strong
correlation between SALES and POP.</p>
</div>
<div id="method-of-least-squares" class="section level3 unnumbered hasAnchor">
<h3>Method of Least Squares<a href="basic-linear-regression.html#method-of-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p></p>
<p>Now we begin to explore the question, “Can knowledge of population help us understand sales?” To respond to this question, we identify sales as the <em>response</em>, or <em>dependent, variable</em>. The
population variable, which is used to help understand sales, is called the <em>explanatory</em>, or <em>independent, variable</em>.</p>
<p>Suppose that we have available the sample data of fifty sales <span class="math inline">\(\{y_1, \ldots, y_{50} \}\)</span> and your job is to predict the sales of a randomly selected ZIP code. Without knowledge of the population variable, a sensible predictor is simply <span class="math inline">\(\overline{y}=6,495\)</span>, the average of the available sample. Naturally, you anticipate that areas with larger populations will have larger sales. That is, if you also have knowledge of population, then can this estimate be improved? If so, then by how much?</p>
<p>To answer these questions, the first step assumes an approximate
linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. To fit a line to our data
set, we use the <em>method of least squares</em>. We need a general
technique so that, if different analysts agree on the data and agree
on the fitting technique, then they will agree on the line. If
different analysts fit a data set using eyeball approximations, in
general they will arrive at different lines, even using the same
data set.</p>
<p>The method begins with the line <span class="math inline">\(y=b_0^{\ast}+b_1^{\ast}x\)</span>, where
the intercept and slope, <span class="math inline">\(b_0^{\ast}\)</span> and <span class="math inline">\(b_1^{\ast}\)</span>, are merely
generic values. For the <span class="math inline">\(i\)</span>th observation, $y_i-(
b_0<sup>{}+b_1</sup>{}x_i) $ represents the deviation of the
observed value <span class="math inline">\(y_i\)</span> from the line at <span class="math inline">\(x_i\)</span>. The quantity
<span class="math display">\[\begin{equation*}
SS(b_0^{\ast},b_1^{\ast})=\sum_{i=1}^{n}\left( y_i-\left(
b_0^{\ast}+b_1^{\ast}x_i\right) \right) ^{2}
\end{equation*}\]</span>
represents the sum of squared deviations for this candidate line.
The least squares method consists of determining the values of
<span class="math inline">\(b_0^{\ast}\)</span> and <span class="math inline">\(b_1^{\ast}\)</span> that minimize
<span class="math inline">\(SS(b_0^{\ast},b_1^{\ast})\)</span>. This is an easy problem that can be
solved by calculus, as follows. Taking partial derivatives with
respect to each argument yields
<span class="math display">\[\begin{equation*}
\frac{\partial }{\partial
b_0^{\ast}}SS(b_0^{\ast},b_1^{\ast})=\sum_{i=1}^{n}(-2)\left(
y_i-\left( b_0^{\ast}+b_1^{\ast}x_i\right) \right)
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
\frac{\partial }{\partial
b_1^{\ast}}SS(b_0^{\ast},b_1^{\ast})=\sum_{i=1}^{n}(-2x_i)\left(
y_i-\left( b_0^{\ast}+b_1^{\ast}x_i\right) \right) .
\end{equation*}\]</span>
The reader is invited to take second partial derivatives to ensure
that we are minimizing, not maximizing, this function. Setting these
quantities equal to zero  and canceling constant terms yields
<span class="math display">\[\begin{equation*}
\sum_{i=1}^{n}\left( y_i-\left( b_0^{\ast}+b_1^{\ast}x_i\right)
\right) =0
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
\sum_{i=1}^{n}x_i\left( y_i-\left( b_0^{\ast}+b_1^{\ast}x_i\right)
\right) =0,
\end{equation*}\]</span>
which are known as the <em>normal equations</em>. Solving these
equations yields the values of <span class="math inline">\(b_0^{\ast}\)</span> and <span class="math inline">\(b_1^{\ast}\)</span> that
minimize the sum of squares, as follows.
</p>
<p>
</p>
<div class="blackbox">
<p><em>Definition.</em> The <em>least squares intercept and slope estimates</em> are</p>
<p><span class="math display">\[\begin{equation*}
b_1=r\frac{s_y}{s_x}~~~~~\mathrm{and}~~~~~b_0=\overline{y}-b_1
\overline{x}.
\end{equation*}\]</span>
The line that they determine, <span class="math inline">\(\widehat{y}=b_0+b_1x\)</span>, is called the <em>fitted regression line</em>.</p>
</div>
<p>We have dropped the asterisk, or star, notation because <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are no longer “candidate” values.</p>
<p>Does this procedure yield a sensible line for our Wisconsin lottery sales? Earlier, we computed <span class="math inline">\(r=0.886\)</span>. From this and the basic summary statistics in Table <a href="basic-linear-regression.html#tab:SummaryStats">2.1</a>, we have <span class="math inline">\(b_1 = 0.886 \left( 8,103\right) /11,098=0.647\)</span> and <span class="math inline">\(b_0 = 6,495-(0.647)9,311 = 469.7.\)</span> This yields the fitted regression line
<span class="math display">\[\begin{equation*}
\widehat{y} = 469.7 + (0.647)x.
\end{equation*}\]</span>
The carat, or “hat,” on top of the <span class="math inline">\(y\)</span> reminds us that this <span class="math inline">\(\widehat{y}\)</span>, or <span class="math inline">\(\widehat{SALES}\)</span>, is a fitted value. One application of the regression line is to estimate sales for a specific population say, <span class="math inline">\(x=10,000\)</span>. The estimate is the height of the regression line, which is <span class="math inline">\(469.7 + (0.647)(10,000) = 6,939.7\)</span>.</p>
<hr />
<p></p>
<p><strong>Example: Summarizing Simulations.</strong> Regression analysis is a tool for summarizing complex
data. In practical work, actuaries often simulate complicated
financial scenarios; it is often overlooked that regression can be
used to summarize relationships of interest.</p>
<p>To illustrate, Manistre and Hancock (2005) simulated many
realizations of a 10-year European put option and demonstrated the
relationship between two actuarial risk measures, the value-at-risk
(VaR) and the conditional tail expectation (CTE). For one example,
these authors examined lognormally distributed stock returns with an
initial stock price of $100, so that in 10 years the price of the
stock would be distributed as
<span class="math display">\[\begin{equation*}
S(Z)=100 \exp \left( (.08) 10 + .15 \sqrt{10} Z \right),
\end{equation*}\]</span>
based on an annual mean return of 10%, standard deviation 15% and
the outcome from a standard normal random variable <span class="math inline">\(Z\)</span>. The put
option pays the difference between the strike price, that will be
taken to be 110 for this example, and <span class="math inline">\(S(Z)\)</span>. The present value of
this option is
<span class="math display">\[\begin{equation*}
C(Z)= \mathrm{e}^{-0.06(10)} \mathrm{max} \left(0, 110-S(Z) \right),
\end{equation*}\]</span>
based on a 6% discount rate.</p>
<p>To estimate the VaR and CTE, for each <span class="math inline">\(i\)</span>, 1000 i.i.d. standard
normal random variables were simulated and used to calculate 1000
present values, <span class="math inline">\(C_{i1}, \ldots, C_{i,1000}.\)</span> The 95th percentile of
these present values is the estimate of the value at risk, denoted
as <span class="math inline">\(VaR_i.\)</span> The average of the highest 50 (<span class="math inline">\(= (1-.05) \times 1000\)</span>)
of the present values is the estimate of the conditional tail
expectation, denoted as <span class="math inline">\(CTE_i\)</span>. Manistre and Hancock (2005)
performed this calculation <span class="math inline">\(i=1, \ldots, 1000\)</span> times; the result is
presented in Figure <a href="basic-linear-regression.html#fig:VarCTEFig">2.3</a>. The scatterplot shows a strong
but not perfect relationship between the <span class="math inline">\(VaR\)</span> and the <span class="math inline">\(CTE\)</span>, the
correlation coefficient turns out to be <span class="math inline">\(r=0.782\)</span>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:VarCTEFig"></span>
<img src="RegressionMarkdown_files/figure-html/VarCTEFig-1.png" alt="Plot of Conditional Tail Expectation (CTE) versus Value at Risk (VaR). Based on \(n=1,000\) simulations from a 10-year European put bond. Source: Manistre and Hancock (2005)." width="60%" />
<p class="caption">
Figure 2.3: <strong>Plot of Conditional Tail Expectation (CTE) versus Value at Risk (VaR).</strong> Based on <span class="math inline">\(n=1,000\)</span> simulations from a 10-year European put bond. <em>Source</em>: Manistre and Hancock (2005).
</p>
</div>
</div>
</div>
<div id="basic-linear-regression-model" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Basic Linear Regression Model<a href="basic-linear-regression.html#basic-linear-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>
</p>
<p>The scatter plot, correlation coefficient and the fitted regression line are useful devices for summarizing the relationship between two variables for a specific data set. To infer general relationships, we need models to represent outcomes of broad populations.</p>
<p>This chapter focuses on a “basic linear regression” model. The “linear regression” part comes from the fact that we fit a line to the data. The “basic” part is because we use only one explanatory variable, <span class="math inline">\(x\)</span>. This model is also known as a “simple” linear regression. This text avoids this language because it gives the false impression that regression ideas and interpretations with one explanatory variable are always straightforward.</p>
<p>We now introduce two sets of assumptions of the basic model, the “observables” and the “error” representations. They are equivalent but each will help us as we later extend regression
models beyond the basics.
</p>
<p><span class="math display">\[
{\small
\begin{array}{l} \hline  \hline
&amp;\textbf{Basic Linear Regression Model} \\
&amp;\textbf{Observables Representation Sampling Assumptions} \\ \hline
\text{F1}. &amp; \mathrm{E}~y_i=\beta_0 + \beta_1 x_i . \\
\text{F2}.  &amp; \{x_1,\ldots ,x_n\}  \text{are non-stochastic variables}. \\
\text{F3}. &amp; \mathrm{Var}~y_i=\sigma ^{2}. \\
\text{F4}. &amp; \{ y_i\} \text{ are independent random variables}. \\ \hline\
\end{array}
}
\]</span></p>
<p>The “observables representation” focuses on variables that we can see (or observe), <span class="math inline">\((x_i,y_i)\)</span>. Inference about the distribution of <span class="math inline">\(y\)</span> is conditional on the observed explanatory variables, so that we may treat <span class="math inline">\(\{x_1,\ldots ,x_n\}\)</span> as non-stochastic variables (assumption F2). When considering types of sampling mechanisms for <span class="math inline">\((x_i,y_i)\)</span>, it is convenient to think of a <em>stratified random sampling</em> scheme, where values of <span class="math inline">\(\{x_1,\ldots ,x_n\}\)</span> are treated as the strata, or group. Under stratified sampling, for each unique value of <span class="math inline">\(x_i\)</span>, we draw a random sample from a population. To illustrate, suppose you are drawing from a database of firms to understand stock return performance (<span class="math inline">\(y\)</span>) and wish to stratify based on the size of the firm. If the amount of assets is a continuous variable, then we can imagine drawing a sample of size 1 for each firm. In this way, we hypothesize a distribution of stock returns conditional on firm asset size.</p>
<p><em>Digression</em>: You will often see reports that summarize results for the “top 50 managers” or the “best 100 universities,” measured by some outcome variable. In regression applications, make sure that you do not select observations based on a dependent variable, such as the highest stock return, because this is stratifying based on the <span class="math inline">\(y\)</span>, not the <span class="math inline">\(x\)</span>. Chapter 6 will discuss sampling procedures in greater detail.</p>
<p>Stratified sampling also provides motivation for assumption F4, the independence among responses. One can motivate assumption F1 by thinking of <span class="math inline">\((x_i,y_i)\)</span> as a draw from a population, where the mean of the conditional distribution of <span class="math inline">\(y_i\)</span> given {<span class="math inline">\(x_i\)</span>} is linear in the explanatory variable. Assumption F3 is known as <em>homoscedasticity</em> that we will discuss extensively in Section 5.7. See Goldberger (1991) for additional background on this representation.
</p>
<p>A fifth assumption that is often implicitly used is:</p>
<p>This assumption is not required for many statistical inference procedures because central limit theorems provide approximate normality for many statistics of interest. However, formal justification for some, such as <span class="math inline">\(t\)</span>-statistics, do require this additional assumption.</p>
<p>In contrast to the observables representation, an alternative set of assumptions focuses on the deviations, or “errors,” in the regression, defined as $_i=y_i-( _0 + _1
x_i ) <span class="math inline">\(. \index{model assumptions!error representation} \index{symbols!\)</span>_i$, “error,” or disturbance term}</p>
<p><span class="math display">\[
{\small
\begin{array}{l} \hline  \hline
&amp;\textbf{Basic Linear Regression Model} \\
&amp;\textbf{Error Representation Sampling Assumptions} \\ \hline
\text{E1}. &amp; y_i=\beta_0 + \beta_1 x_i + \varepsilon_i . \\
\text{E2}.  &amp; \{x_1,\ldots ,x_n\}  \text{ are non-stochastic variables}. \\
\text{E3}. &amp; \mathrm{E}~\varepsilon _i=0 \text{ and } \mathrm{Var}~\varepsilon _i=\sigma ^{2}. \\
\text{E4}. &amp; \{ \varepsilon_i\} \text{ are independent random variables}. \\ \hline\
\end{array}
}
\]</span></p>
<p>The “error representation” is based on the Gaussian theory of errors (see Stigler, 1986, for a historical background). Assumption E1 assumes that <span class="math inline">\(y\)</span> is in part due to a linear function of the
observed explanatory variable, <span class="math inline">\(x\)</span>. Other unobserved variables that influence the measurement of <span class="math inline">\(y\)</span> are interpreted to be included in the “error” term <span class="math inline">\(\varepsilon _i\)</span>, which is also known as the “disturbance” term. The independence of errors, E4, can be motivated by assuming that {<span class="math inline">\(\varepsilon _i\)</span>} are realized through a simple random sample from an unknown population of errors.</p>
<p>Assumptions E1-E4 are equivalent to F1-F4. The error representation provides a useful springboard for motivating goodness of fit measures (Section <a href="basic-linear-regression.html#S2:SummStats">2.3</a>). However, a drawback of the
error representation is that it draws the attention from the observable quantities <span class="math inline">\((x_i,y_i)\)</span> to an unobservable quantity, {<span class="math inline">\(\varepsilon _i\)</span>}. To illustrate, the sampling basis, viewing
{<span class="math inline">\(\varepsilon _i\)</span>} as a simple random sample, is not directly verifiable because one cannot directly observe the sample {$ _i$}. Moreover, the assumption of additive errors in E1 will be troublesome when we consider nonlinear regression models.</p>
<p>Figure <a href="basic-linear-regression.html#fig:NormalCurve">2.4</a> illustrates some of the assumptions of
the basic linear regression model. The data (<span class="math inline">\(x_1,y_1\)</span>), (<span class="math inline">\(x_2,y_2\)</span>)
and (<span class="math inline">\(x_3,y_3\)</span>) are observed and are represented by the circular
opaque plotting symbols. According to the model, these observations
should be close to the regression line <span class="math inline">\(\mathrm{E}~y = \beta_0 + \beta_1 x\)</span>. Each deviation from the line is random. We will often
assume that the distribution of deviations may be represented by a
normal curve, as in Figure <a href="basic-linear-regression.html#fig:NormalCurve">2.4</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:NormalCurve"></span>
<img src="RegressionMarkdown_files/figure-html/NormalCurve-1.png" alt="The distribution of the response varies by the level of the explanatory variable." width="60%" />
<p class="caption">
Figure 2.4: <strong>The distribution of the response varies by the level of the explanatory variable.</strong>
</p>
</div>
<p>The basic linear regression model assumptions describe the underlying population. <a href="#tab:2.2">Table 2.2</a> highlights the idea that characteristics of this population can be summarized by the parameters <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\sigma ^{2}\)</span>. In Section 2.1, we summarized data from a sample, introducing the statistics <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>. Section <a href="basic-linear-regression.html#S2:SummStats">2.3</a> will introduce <span class="math inline">\(s^{2}\)</span>,
the statistic corresponding to the parameter <span class="math inline">\(\sigma ^{2}\)</span>.</p>
<p><a id=tab:1.3></a></p>
<p>Table 2.2. <strong>Summary Measures of the Population and Sample</strong></p>
<p><span class="math display">\[
{\small
\begin{array}{llccc}\hline\hline
&amp; \text{Summary} \\
\text{Data} &amp; \text{Measures} &amp; \text{Intercept} &amp; \text{Slope} &amp; \text{Variance} \\\hline
\text{Population} &amp; \text{Parameters} &amp; \beta_0 &amp; \beta_1 &amp; \sigma^2 \\
\text{sample} &amp; \text{Statistics} &amp; b_0 &amp; b_1 &amp; s^2 \\
\hline
\end{array}
}
\]</span></p>
</div>
<div id="S2:SummStats" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Is the Model Useful? Some Basic Summary Measures<a href="basic-linear-regression.html#S2:SummStats" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Although statistics is the science of summarizing data, it is also
the art of arguing with data. This section develops some of the
basic tools used to justify the basic linear regression model. A
scatter plot may provide strong <em>visual</em> evidence that <span class="math inline">\(x\)</span>
influences <span class="math inline">\(y\)</span>; developing <em>numerical</em> evidence will enable us
to quantify the strength of the relationship. Further, numerical
evidence will be useful when we consider other data sets where the
graphical evidence is not compelling.</p>
<div id="partitioning-the-variability" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Partitioning the Variability<a href="basic-linear-regression.html#partitioning-the-variability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The squared deviations, <span class="math inline">\(\left( y_i-\overline{y}\right) ^2\)</span>, provide
a basis for measuring the spread of the data. If we wish to estimate
the <span class="math inline">\(i\)</span>th dependent variable <em>without</em> knowledge of <span class="math inline">\(x\)</span>, then
<span class="math inline">\(\overline{y}\)</span> is an appropriate estimate and <span class="math inline">\(y_i- \overline{y}\)</span>
represents the deviation of the estimate. We use
<span class="math inline">\(Total~SS=\sum_{i=1}^{n}\left( y_i-\overline{y}\right) ^2\)</span>, the
total sum of squares, to represent the variation in all of the
responses.
</p>
<p>Suppose now that we also have knowledge of <span class="math inline">\(x\)</span>, an explanatory variable. Using the fitted regression line, for each observation we can compute the corresponding <em>fitted value</em>, <span class="math inline">\(\widehat{y}_i = b_0 + b_1x_i\)</span>. The fitted value is our estimate <em>with</em> knowledge of the explanatory variable. As before, the difference between the response and the fitted value, <span class="math inline">\(y_i- \widehat{y}_i\)</span>, represents the deviation of this estimate. We now have two “estimates” of <span class="math inline">\(y_i\)</span>, these are <span class="math inline">\(\widehat{y}_i\)</span> and <span class="math inline">\(\overline{y}\)</span>. Presumably, if the regression line is useful, then $
_i$ is a more accurate measure than <span class="math inline">\(\overline{y}\)</span>. To judge this usefulness, we algebraically decompose the total deviation as:</p>
<p><span class="math display" id="eq:deviationdecomp">\[\begin{equation}
\begin{array}{ccccc}
\underbrace{y_i-\overline{y}} &amp; = &amp;
\underbrace{y_i-\widehat{y}_i}
&amp; + &amp; \underbrace{\widehat{y}_i-\overline{y}} \\
\text{total} &amp; = &amp; \text{unexplained} &amp; + &amp; \text{explained} \\
\text{deviation} &amp;  &amp; \text{deviation} &amp;  &amp; \text{deviation} \\
\end{array}
\tag{2.1}
\end{equation}\]</span>
Interpret this equation as “the deviation without knowledge of <span class="math inline">\(x\)</span>
equals the deviation with knowledge of <span class="math inline">\(x\)</span> plus the deviation
explained by <span class="math inline">\(x\)</span>.” Figure <a href="basic-linear-regression.html#fig:ANOVADecomp">2.5</a> is a geometric
display of this decomposition. In the figure, an observation above
the line was chosen, yielding a positive deviation from the fitted
regression line, to make the graph easier to read. A good exercise
is to draw a rough sketch corresponding to Figure
<a href="basic-linear-regression.html#fig:ANOVADecomp">2.5</a> with an observation below the fitted regression
line.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ANOVADecomp"></span>
<img src="RegressionMarkdown_files/figure-html/ANOVADecomp-1.png" alt="Geometric display of the deviation decomposition." width="60%" />
<p class="caption">
Figure 2.5: <strong>Geometric display of the deviation decomposition.</strong>
</p>
</div>
<p>Now, from the algebraic decomposition in equation <a href="basic-linear-regression.html#eq:deviationdecomp">(2.1)</a>, square each side of the equation and sum
over all observations. After a little algebraic manipulation, this
yields
<span class="math display" id="eq:ANOVADecomposition">\[\begin{equation}
\sum_{i=1}^{n}\left( y_i-\overline{y}\right) ^2=\sum_{i=1}^{n}\left(
y_i-\widehat{y}_i\right) ^2+\sum_{i=1}^{n}\left( \widehat{y}_i-
\overline{y}\right) ^2.
\tag{2.2}
\end{equation}\]</span>
We rewrite this as <span class="math inline">\(Total~SS=Error~SS+Regression~SS\)</span> where <span class="math inline">\(SS\)</span> stands for
sum of squares. We interpret:</p>
<ul>
<li><p><span class="math inline">\(Total~SS\)</span> as the total variation without knowledge of <span class="math inline">\(x\)</span>,</p></li>
<li><p><span class="math inline">\(Error~SS\)</span> as the total variation remaining after the introduction of <span class="math inline">\(x\)</span>, and</p></li>
<li><p><span class="math inline">\(Regression~SS\)</span> as the difference between the <span class="math inline">\(Total~SS\)</span> and <span class="math inline">\(Error~SS\)</span>
, or the total variation “explained” through knowledge of <span class="math inline">\(x\)</span>.</p></li>
</ul>
<p></p>
<p>When squaring the right-hand side of equation <a href="basic-linear-regression.html#eq:deviationdecomp">(2.1)</a>, we have the cross-product term <span class="math inline">\(2\left( y_i-\widehat{y}_i\right) \left( \widehat{y}_i-\overline{y}\right)\)</span>. With the “algebraic manipulation,” one can check that the sum of the cross-products over all observations is zero. This result is not true for all fitted lines but is a special property of the least squares fitted line.</p>
<p>In many instances, the variability decomposition is reported through only a single statistic.</p>
<p>
</p>
<div class="blackbox">
<p><em>Definition</em>. The <em>coefficient of determination</em> is denoted by the symbol <span class="math inline">\(R^2\)</span>, called “<span class="math inline">\(R\)</span>-square, and defined as
<span class="math display">\[\begin{equation*}
R^2=\frac{Regression~SS}{Total~SS}.
\end{equation*}\]</span></p>
</div>
<p>We interpret <span class="math inline">\(R^2\)</span> to be the proportion of variability
explained by the regression line. In one extreme case where the
regression line fits the data perfectly, we have <span class="math inline">\(Error~SS=0\)</span> and
<span class="math inline">\(R^2=1\)</span>. In the other extreme case where the regression line
provides no information about the response, we have
<span class="math inline">\(Regression~SS=0\)</span> and <span class="math inline">\(R^2=0.\)</span> The coefficient of determination is
constrained by the inequalities <span class="math inline">\(0 \leq R^2 \leq 1\)</span> with larger
values implying a better fit.</p>
</div>
<div id="the-size-of-a-typical-deviation-s" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> The Size of a Typical Deviation: <em>s</em><a href="basic-linear-regression.html#the-size-of-a-typical-deviation-s" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the basic linear regression model, the deviation of the response
from the regression line, $y_i-( _0+_1x_i) $,
is not an observable quantity because the parameters <span class="math inline">\(\beta_0\)</span> and
<span class="math inline">\(\beta_1\)</span>  are not observed. However, by using estimators <span class="math inline">\(b_0\)</span> and
<span class="math inline">\(b_1\)</span>, we can approximate this deviation using
<span class="math display">\[\begin{equation*}
e_i=y_i-\widehat{y}_i=y_i-\left( b_0+b_1x_i\right) ,
\end{equation*}\]</span>
known as the <em>residual</em>.
</p>
<p>Residuals will be critical to developing strategies for improving
model specification in Section <a href="basic-linear-regression.html#S2:ResidualAnalsis">2.6</a>. We now show
how to use the residuals to estimate <span class="math inline">\(\sigma ^2\)</span>. From a first
course in statistics, we know that if one could observe the
deviations <span class="math inline">\(\varepsilon _i\)</span>, then a desirable estimate of <span class="math inline">\(\sigma ^2\)</span> would be <span class="math inline">\((n-1)^{-1}\sum_{i=1}^{n}\left( \varepsilon _i-\overline{\varepsilon }\right) ^2\)</span>. Because {<span class="math inline">\(\varepsilon _i\)</span>}
are not observed, we use the following.</p>
<p>
</p>
<div class="blackbox">
<p><em>Definition</em>. An estimator of <span class="math inline">\(\sigma ^2\)</span>, the <em>mean square error (MSE)</em>, is defined as
<span class="math display">\[\begin{equation}
s^2=\frac{1}{n-2}\sum_{i=1}^{n}e_i{}^2.  \label{BLRs2}
\end{equation}\]</span>
The positive square root, <span class="math inline">\(s=\sqrt{s^2},\)</span> is called the <em>residual standard deviation</em>.</p>
</div>
<p>Comparing the definitions of <span class="math inline">\(s^2\)</span> and <span class="math inline">\((n-1)^{-1}\sum_{i=1}^{n}\left( \varepsilon _i-\overline{\varepsilon }\right) ^2\)</span>, you will see two important differences. First, in defining <span class="math inline">\(s^2\)</span> we have not subtracted the average residual from each
residual before squaring. This is because the average residual is zero, a special property of least squares estimation (see Exercise 2.<span class="math inline">\(\ref{Ex:AverageResid}\)</span>). This result can be shown using algebra and is guaranteed for all data sets.</p>
<p>Second, in defining <span class="math inline">\(s^2\)</span> we have divided by <span class="math inline">\(n-2\)</span> instead of <span class="math inline">\(n-1\)</span>. Intuitively, dividing by either <span class="math inline">\(n\)</span> or <span class="math inline">\(n-1\)</span> tends to underestimate <span class="math inline">\(\sigma ^2\)</span>. The reason is that, when fitting lines to data, we need at least two observations to determine a line. For example, we must
have at least three observations for there to be any variability about a line. How much “freedom” is there for variability about a line? We will say that the error degrees of freedom is the number of observations available, <span class="math inline">\(n\)</span>, minus the number of observations needed to determine a line, 2 (with symbols, <span class="math inline">\(df=n-2\)</span>). However, as we saw in the least squares estimation subsection, we do not need to identify two actual observations to determine a line. The idea is that if an analyst knows the line and <span class="math inline">\(n-2\)</span> observations, then the remaining two observations can be determined, without variability. When
dividing by <span class="math inline">\(n-2\)</span>, it can be shown that <span class="math inline">\(s^2\)</span> is an unbiased estimator of <span class="math inline">\(\sigma ^2\)</span>.</p>
<p>We can also express <span class="math inline">\(s^2\)</span> in terms of the sum of squares quantities. That is,</p>
<p><span class="math display">\[\begin{equation*}
s^2=\frac{1}{n-2}\sum_{i=1}^{n}\left( y_i-\widehat{y}_i\right) ^2=
\frac{Error~SS}{n-2}=MSE.
\end{equation*}\]</span></p>
<p></p>
<p>This leads us to the <em>analysis of variance</em>, or <em>ANOVA</em>, table:</p>
<p><span class="math display">\[
\begin{array}{llcl}
\hline \hline
\text{ANOVA Table} \\ \hline
\text{Source} &amp; \text{Sum of Squares} &amp; df &amp; \text{Mean Square} \\ \hline
\text{Regression} &amp; Regression~SS &amp; 1 &amp; Regression~MS \\
\text{Error} &amp; Error~SS &amp; n-2 &amp; MSE \\
\text{Total} &amp; Total~SS &amp; n-1 &amp;  \\ \hline \hline
\end{array}
\]</span></p>
<p>The ANOVA table is merely a bookkeeping device used to
keep track of the sources of variability; it routinely appears in
statistical software packages as part of the regression output. The
mean square column figures are defined to be the sums of square
(<span class="math inline">\(SS\)</span>) figures divided by their respective degrees of freedom
(<span class="math inline">\(df\)</span>). In particular, the mean square for errors (<span class="math inline">\(MSE\)</span>) equals $
s^2$ and the regression sum of squares equals the regression mean
square. This latter property is specific to the regression with one
variable case; it is not true where we consider more than one
explanatory variable.</p>
<p>The error degrees of freedom in the ANOVA table is <span class="math inline">\(n-2\)</span>. The total degrees of freedom is <span class="math inline">\(n-1\)</span>, reflecting the fact that the total sum of squares is centered about the mean (at least two observations are required for positive variability). The single degree of freedom associated with the regression portion means that the slope, plus one observation, is enough information to
determine the line. This is because it takes two observations to determine a line and at least three observations for there to be any variability about the line.</p>
<p>The analysis of variance table for the lottery data is:</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-7">Table 2.2: </span><strong>ANOVA Table</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Sum of Squares
</th>
<th style="text-align:right;">
<span class="math inline">\(df\)</span>
</th>
<th style="text-align:right;">
Mean Square
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Regression
</td>
<td style="text-align:right;width: 1.8cm; ">
2,527,165,015
</td>
<td style="text-align:right;width: 1.8cm; ">
1
</td>
<td style="text-align:right;width: 1.8cm; ">
2,527,165,015
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Error
</td>
<td style="text-align:right;width: 1.8cm; ">
690,116,755
</td>
<td style="text-align:right;width: 1.8cm; ">
48
</td>
<td style="text-align:right;width: 1.8cm; ">
14,377,432
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Total
</td>
<td style="text-align:right;width: 1.8cm; ">
3,217,281,770
</td>
<td style="text-align:right;width: 1.8cm; ">
49
</td>
<td style="text-align:right;width: 1.8cm; ">
</td>
</tr>
</tbody>
</table>
<p>From this table, you can check that <span class="math inline">\(R^2=78.5\%\)</span> and <span class="math inline">\(s=3,792.\)</span></p>
</div>
</div>
<div id="properties-of-regression-coefficient-estimators" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Properties of Regression Coefficient Estimators<a href="basic-linear-regression.html#properties-of-regression-coefficient-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The least squares estimates can be expressed as weighted sum of the responses. To see this, define the weights
<span class="math display">\[\begin{equation*}
w_i=\frac{x_i-\overline{x}}{s_x^2(n-1)}.
\end{equation*}\]</span>
Because the sum of <span class="math inline">\(x\)</span>-deviations (<span class="math inline">\(x_i-\overline{x}\)</span>) is zero, we see that <span class="math inline">\(\sum_{i=1}^{n}w_i=0\)</span>. Thus, we can express the slope estimate
<span class="math display" id="eq:weightb1">\[\begin{equation}
b_1=r\frac{s_y}{s_x}=\frac{1}{(n-1)s_x^2}\sum_{i=1}^{n}\left(
x_i-\overline{x}\right) \left( y_i-\overline{y}\right)
=\sum_{i=1}^{n}w_i\left( y_i-\overline{y}\right)
=\sum_{i=1}^{n}w_iy_i.
\tag{2.3}
\end{equation}\]</span></p>
<p>The exercises ask the reader to verify that <span class="math inline">\(b_0\)</span> can also be expressed as a weighted sum of responses, so our discussion pertains to both regression coefficients. Because regression coefficients are weighted sums of responses, they can be affected dramatically by unusual observations (see Section <a href="basic-linear-regression.html#S2:ResidualAnalsis">2.6</a>).</p>
<p>Because <span class="math inline">\(b_1\)</span> is a weighted sum, it is straightforward to derive the expectation and variance of this statistic. By the linearity of expectations and Assumption F1, we have
<span class="math display">\[\begin{equation*}
\mathrm{E}~b_1=\sum_{i=1}^{n}w_i~\mathrm{E}~y_i=\beta_0\sum_{i=1}^{n}w_i+\beta_1\sum_{i=1}^{n}w_ix_i=\beta_1.
\end{equation*}\]</span>
That is, <span class="math inline">\(b_1\)</span> is an unbiased estimator of <span class="math inline">\(\beta_1\)</span>. Here, the sum $ _{i=1}^{n}w_ix_i$ <span class="math inline">\(=\)</span> <span class="math inline">\(\left[ s_x^2(n-1)\right] ^{-1}\sum_{i=1}^{n}\left( x_i-\overline{x}\right) x_i\)</span> <span class="math inline">\(=\left[s_x^2(n-1)\right] ^{-1}\sum_{i=1}^{n}\left( x_i-\overline{x}\right) ^2=1.\)</span> From the definition of the weights, some easy algebra also shows that <span class="math inline">\(\sum_{i=1}^{n}w_i^2=1/\left( s_x^2(n-1)\right)\)</span>. Further, the independence of the responses implies that the variance of the sum is the sum of the variances, and thus we have
<span class="math display">\[\begin{equation*}
\mathrm{Var}~b_1
=\sum_{i=1}^{n}w_i^2\mathrm{Var}~y_i=\frac{\sigma^2}{s_x^2(n-1)}.
\end{equation*}\]</span>
Replacing <span class="math inline">\(\sigma ^2\)</span> by its estimator <span class="math inline">\(s^2\)</span> and taking square roots leads to the following.</p>
<p></p>
<div class="blackbox">
<p><em>Definition</em>. The <em>standard error</em> of <span class="math inline">\(b_1\)</span>, the estimated standard deviation of <span class="math inline">\(b_1\)</span>, is defined as
<span class="math display">\[\begin{equation}
se(b_1)=\frac{s}{s_x\sqrt{n-1}}.  \label{seb1a}
\end{equation}\]</span></p>
</div>
<p>This is our measure of the reliability, or precision, of the slope
estimator. Using equation (<span class="math inline">\(\ref{seb1a}\)</span>), we see that <span class="math inline">\(se(b_1)\)</span> is
determined by three quantities, <span class="math inline">\(n\)</span>, <span class="math inline">\(s\)</span> and <span class="math inline">\(s_x\)</span>, as follows:</p>
<ul>
<li>If we have more observations so that <span class="math inline">\(n\)</span> becomes larger, then $
se(b_1)$ becomes smaller, other things equal.</li>
<li>If the observations have a greater tendency to lie closer to the line
so that <span class="math inline">\(s\)</span> becomes smaller, then <span class="math inline">\(se(b_1)\)</span> becomes smaller, other
things equal.</li>
<li>If values of the explanatory variable become more spread out so that $
s_x$ increases, then <span class="math inline">\(se(b_1)\)</span> becomes smaller, other things equal.</li>
</ul>
<p>Smaller values of <span class="math inline">\(se(b_1)\)</span> offer a better opportunity to detect
relations between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>. Figure <a href="#BasicLSRE"><strong>??</strong></a> illustrates
these relationships. Here, the scatter plot in the middle has the
smallest value of <span class="math inline">\(se(b_1)\)</span>. Compared with the middle plot, the
left-hand plot has a larger value of <span class="math inline">\(s\)</span> and thus <span class="math inline">\(se(b_1)\)</span>.
Compared with the right-hand plot, the middle plot has a larger
<span class="math inline">\(s_x\)</span>, and thus smaller value of <span class="math inline">\(se(b_1)\)</span>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:BasicLSRE"></span>
<img src="RegressionMarkdown_files/figure-html/BasicLSRE-1.png" alt="These three scatter plots exhibit the same linear relationship between \(y\) and \(x\). The plot on the left exhibits greater variability about the line than the plot in the middle. The plot on the right exhibits a smaller standard deviation in \(x\) than the plot in the middle." width="100%" />
<p class="caption">
Figure 2.6: <strong>These three scatter plots exhibit the same linear relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>.</strong> The plot on the left exhibits greater variability about the line than the plot in the middle. The plot on the right exhibits a smaller standard deviation in <span class="math inline">\(x\)</span> than the plot in the middle.
</p>
</div>
<p>Equation <a href="#weightb1"><strong>??</strong></a> also implies that the regression
coefficient $b_1 $ is normally distributed. That is, recall from
mathematical statistics that linear combinations of normal random
variables are also normal. Thus, if Assumption F5 holds, then <span class="math inline">\(b_1\)</span>
is normally distributed. Moreover, several versions of central limit
theorems exists for weighted sums (see, for example, Serfling,
1980). Thus, as discussed in Section 1.4, if the responses <span class="math inline">\(y_i\)</span><br />
are even approximately normally distributed, then it will be
reasonable to use a normal approximation for the sampling
distribution of <span class="math inline">\(b_1\)</span>. Using <span class="math inline">\(se(b_1)\)</span> as the estimated standard
deviation of <span class="math inline">\(b_1\)</span>, for large values of <span class="math inline">\(n\)</span> we have that <span class="math inline">\(\left( b_1-\beta_1\right) /se(b_1)\)</span> has an approximate standard normal
distribution. Although we will not prove it here, under Assumption
F5 <span class="math inline">\(\left( b_1-\beta_1\right) /se(b_1)\)</span> follows a $t $-distribution
with degrees of freedom <span class="math inline">\(df=n-2\)</span>.</p>
</div>
<div id="statistical-inference" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Statistical Inference<a href="basic-linear-regression.html#statistical-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Having fit a model with a data set, we can make a number of important
statements. Generally, it is useful to think about these statements in three
categories: (i) tests of hypothesized ideas, (ii) estimates of model
parameters and (ii) predictions of new outcomes.</p>
<div id="is-the-explanatory-variable-important-the-t-test" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Is the Explanatory Variable Important?: The <em>t</em>-Test<a href="basic-linear-regression.html#is-the-explanatory-variable-important-the-t-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p></p>
<p>We respond to the question of whether the explanatory variable is
important by investigating whether or not <span class="math inline">\(\beta_1=0\)</span>. The logic is
that if <span class="math inline">\(\beta_1=0\)</span>, then the basic linear regression model no
longer includes an explanatory variable <span class="math inline">\(x\)</span>. Thus, we translate our
question of the importance of the explanatory variable into a
narrower question that can be answered using the hypothesis testing
framework. This narrower question is, is $ H_0:_1=0$ valid? We
respond to this question by looking at the test statistic:</p>
<div class="blackbox">
<p><span class="math display">\[\begin{equation*}
t-\mathrm{ratio}=\frac{\mathrm{estimator-hypothesized~value~of~parameter}}
{\mathrm{standard~error~of~the~estimator}}.
\end{equation*}\]</span></p>
</div>
<p>
</p>
<p>For the case of <span class="math inline">\(H_0:\beta_1=0\)</span> , we examine <span class="math inline">\(t\)</span>-ratio <span class="math inline">\(t(b_1)=b_1/se(b_1)\)</span> because the hypothesized value of <span class="math inline">\(\beta_1\)</span> is 0. This is the appropriate standardization because, under the null hypothesis and the model assumptions described in Section 2.4, the sampling distribution of <span class="math inline">\(t(b_1)\)</span> can be shown to be the <span class="math inline">\(t\)</span>-distribution with $ df=n-2$ degrees of freedom. Thus, to test the null hypothesis <span class="math inline">\(H_0\)</span> against the alternative <span class="math inline">\(H_{a}:\beta_1\neq 0\)</span>, we reject <span class="math inline">\(H_0\)</span> if favor of <span class="math inline">\(H_{a}\)</span> if <span class="math inline">\(|t(b_1)|\)</span> exceeds a <span class="math inline">\(t\)</span>-value. Here, this <span class="math inline">\(t\)</span>-value is a percentile from the
<span class="math inline">\(t\)</span>-distribution using <span class="math inline">\(df=n-2\)</span> degrees of freedom.  We denote the significance level as $$  and this <span class="math inline">\(t\)</span>-value as <span class="math inline">\(t_{n-2,1-\alpha /2}\)</span>.
</p>
<hr />
<p><strong>Example: Lottery Sales - Continued.</strong> For the lottery sales
example, the residual standard deviation is <span class="math inline">\(s=3,792\)</span>. From Table
<a href="basic-linear-regression.html#tab:SummaryStats">2.1</a>, we have <span class="math inline">\(s_x = 11,098\)</span>. Thus, the standard
error of the slope is <span class="math inline">\(se(b_1) = 3792/(11098\sqrt{50-1})=0.0488\)</span>.
From Section 2.1, the slope estimate is <span class="math inline">\(b_1=0.647\)</span>. Thus, the
<span class="math inline">\(t\)</span>-statistic is <span class="math inline">\(t(b_1) = 0.647/0.0488 = 13.4\)</span>. We interpret this
by saying that the slope is 13.4 standard errors above zero. For the significance level,
we use the customary value of
$$ = 5%. The 97.5th percentile from a <span class="math inline">\(t\)</span>-distribution with
<span class="math inline">\(df=50-2=48\)</span> degrees of freedom is <span class="math inline">\(t_{48,0.975}=2.011\)</span>. Because
<span class="math inline">\(|13.4|&gt;2.011\)</span>, we reject the null hypothesis that the slope
<span class="math inline">\(\beta_1 = 0\)</span> in favor of the alternative that <span class="math inline">\(\beta_1 \neq 0\)</span>.</p>
<hr />
<p>
</p>
<p>Making decisions by comparing a <span class="math inline">\(t\)</span>-ratio to a <span class="math inline">\(t\)</span>-value is called a
<span class="math inline">\(t\)</span><em>-test</em>. Testing <span class="math inline">\(H_0:\beta_1=0\)</span> versus <span class="math inline">\(H_{a}:\beta_1\neq 0\)</span> is just one of many hypothesis tests that can be performed,
although it is the most common. Table <span class="math inline">\(\ref{T2:DecMakingProc}\)</span>
outlines alternative decision-making procedures. These procedures
are for testing <span class="math inline">\(H_0:\beta_1 = d\)</span> where <span class="math inline">\(d\)</span> is a user-prescribed
value that may be equal to zero or any other known value. For
example, in our Section 2.7 example, we will use <span class="math inline">\(d=1\)</span> to test
financial theories about the stock market.</p>
\begin{table}[h]
<p><span class="math display">\[
\begin{array}{c|c}
\hline \text{Alternative Hypothesis} (H_{a}) &amp; \text{Procedure: Reject } H_0 \text{ in
favor of }  H_{a} \text{ if} \\ \hline
\beta_1&gt;d &amp; t-\mathrm{ratio}&gt;t_{n-2,1-\alpha }. \\
\beta_1&lt;d &amp; t-\mathrm{ratio}&lt;-t_{n-2,1-\alpha }. \\
\beta_1\neq d &amp; |t-\mathrm{ratio}\mathit{|}&gt;t_{n-2,1-\alpha /2}. \\
\end{array} \\
{\small
\begin{array}{l}
\hline \text{Notes: The significance level is } \alpha . \text{Here, }t_{n-2,1-\alpha}  \text{  the } (1-\alpha )th \text{ percentile}\\
~~\text{from the } t-\text{distribution using } df=n-2 \text{ degrees of freedom}.\\
~~\text{The test statistic is }t-\mathrm{ratio} = (b_1 -d)/se(b_1) . \\  \hline
\end{array}
}
\]</span></p>
<p>Alternatively, one can construct probability (<span class="math inline">\(p\)</span>-) values and
compare these to given significant levels. The <span class="math inline">\(p\)</span>-value is a useful
summary statistic for the data analyst to report since it allows the
report reader to understand the strength of the deviation from the
null hypothesis. Table <span class="math inline">\(\ref{T2:PvalueProc}\)</span> summarizes the procedure
for calculating <span class="math inline">\(p\)</span>-values.</p>
<p></p>
\begin{table}[h]
<p><span class="math display">\[
\begin{array}{c|ccc}
\hline
\text{Alternative} &amp;  &amp;  &amp;  \\
\text{Hypothesis} (H_a) &amp; \beta_1&gt;d &amp; \beta_1&lt;d &amp; \beta_1\neq d
\\ \hline
p-value &amp; \Pr(t_{n-2}&gt;t-\mathrm{ratio}) &amp;
\Pr(t_{n-2}&lt;t-\mathrm{ratio}) &amp; \Pr
(|t_{n-2}|&gt;|t-\mathrm{ratio}\mathit{|}) \\\hline
\end{array} \\
{\small
\begin{array}{l}
\hline \text{Notes: Here, }t_{n-2}  \text{ is a } t-\text{distributed random variable with } df=n-2 \text{ degrees of freedom.}\\
~~\text{The test statistic is }t-\mathrm{ratio} = (b_1 -d)/se(b_1) . \\  \hline
\end{array}
}
\]</span></p>
<p>Another interesting way of addressing the question of the importance of an explanatory variable is through the correlation coefficient. Remember that the correlation coefficient is a measure of linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Let’s denote this statistic by
<span class="math inline">\(r(y,x)\)</span>. This quantity is unaffected by scale changes in either variable. For example, if we multiply the <span class="math inline">\(x\)</span> variable by the number <span class="math inline">\(b_1\)</span>, then the correlation coefficient remains unchanged. Further, correlations are unchanged by additive shifts. Thus, if we add a number, say <span class="math inline">\(b_0\)</span>, to each <span class="math inline">\(x\)</span> variable, then the correlation coefficient remains unchanged. Using a scale change and an additive shift on the <span class="math inline">\(x\)</span> variable can be used to produce the fitted value <span class="math inline">\(\widehat{y}=b_0+b_1x\)</span>. Thus, using notation, we have
<span class="math inline">\(r(y,x)=r(y,\widehat{y}).\)</span> We may thus interpret the correlation between the responses and the explanatory variable to be equal to the correlation between the responses and the fitted values. This leads then to the following interesting algebraic fact, <span class="math inline">\(R^2=r^2.\)</span> That
is, the coefficient of determination equals the correlation coefficient squared. This is much easier to interpret if one thinks of <span class="math inline">\(r\)</span> as the correlation between observed and fitted values. See Exercise 2.<span class="math inline">\(\ref{Ex:Chap2Corr}\)</span> for steps useful in confirming this
result.</p>
</div>
<div id="confidence-intervals" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Confidence Intervals<a href="basic-linear-regression.html#confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p></p>
<p>Investigators often cite the formal hypothesis testing mechanism to respond to the question “Does the explanatory variable have a real influence on the response?” A natural follow-up question is “To what extent does <span class="math inline">\(x\)</span> affect <span class="math inline">\(y\)</span>?” To a certain degree, one could
respond using the size of the <span class="math inline">\(t\)</span>-ratio or the <span class="math inline">\(p\)</span>-value. However, in many instances a <em>confidence interval</em> for the slope is more useful.</p>
<p>To introduce confidence intervals for the slope, recall that <span class="math inline">\(b_1\)</span> is our point estimator of the true, unknown slope <span class="math inline">\(\beta_1\)</span>. Section 2.4 argued that this estimator has standard error <span class="math inline">\(se(b_1)\)</span> and that <span class="math inline">\(\left( b_1-\beta_1\right) /se(b_1)\)</span> follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom. Probability statements can be inverted to yield confidence intervals. Using this logic, we have the following confidence interval for the slope <span class="math inline">\(\beta_1\)</span>.</p>
<div class="blackbox">
<p><em>Definition</em>. A <span class="math inline">\(100(1-\alpha)\)</span>% confidence interval for the slope <span class="math inline">\(\beta_1\)</span> is
<span class="math display">\[\begin{equation}\label{E2:ConfIntb1}
b_1\pm t_{n-2,1-\alpha /2} ~se(b_1).
\end{equation}\]</span></p>
</div>
<p>As with hypothesis testing, <span class="math inline">\(t_{n-2,1-\alpha /2}\)</span> is the (1-$ $/2)th percentile from the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df=n-2\)</span> degrees of freedom. Because of the two-sided nature of confidence intervals, the percentile is 1 - (1 - confidence level) / 2. In this text, for notational simplicity we generally use a 95% confidence interval, so the percentile is 1-(1-.0.95)/2 = 0.975. The confidence interval provides a range of reliability that measures the usefulness of the estimate.</p>
<p>In Section 2.1, we established that the least squares slope estimate for the lottery sales example is <span class="math inline">\(b_1=0.647\)</span>. The interpretation is that if a zip code’s population differs by 1,000, then we expect mean lottery sales to differ by $647. How reliable is this estimate? It turns out that $ se(b_1)=0.0488$ and thus an approximate 95% confidence interval for the slope is
<span class="math display">\[\begin{equation*}
0.647\pm (2.011)(.0488),
\end{equation*}\]</span>
or (0.549, 0.745). Similarly, if population differs by 1,000, a 95% confidence interval for the expected change in sales is (549, 745). Here, we use the <span class="math inline">\(t\)</span>-value <span class="math inline">\(t_{48,0.975}=2.011\)</span> because there are 48 (= $ n$-2) degrees of freedom and, for a 95% confidence interval, we need the 97.5th percentile.</p>
</div>
<div id="prediction-intervals" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Prediction Intervals<a href="basic-linear-regression.html#prediction-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Section 2.1, we showed how to use least squares estimators to
predict the lottery sales for a zip code, outside of our sample,
having a population of 10,000. Because prediction is such an
important task for actuaries, we formalize the procedure so that it
can be used on a regular basis.</p>
<p>To predict an additional observation, we assume that the level of
explanatory variable is known and is denoted by <span class="math inline">\(x_{\ast}\)</span>. For
example, in our previous lottery sales example we used <span class="math inline">\(x_{\ast} = 10,000\)</span>. We also assume that the additional observation follows the
same linear regression model as the observations in the sample.</p>
<p>Using our least square estimators, our point prediction is
<span class="math inline">\(\widehat{y}_{\ast} = b_0 + b_1 x_{\ast}\)</span>, the height of the fitted
regression line at <span class="math inline">\(x_{\ast}\)</span> We may decompose the prediction error
into two parts:</p>
<p>It can be shown that the standard error of the prediction is
<span class="math display">\[\begin{equation*}
se(pred) = s \sqrt{1+\frac{1}{n}+\frac{\left(
x_{\ast}-\overline{x}\right) ^2}{(n-1)s_x^2}}.
\end{equation*}\]</span>
As with <span class="math inline">\(se(b_1)\)</span>, the terms <span class="math inline">\(n^{-1}\)</span> and $(
x_{}- ) ^2/$ become
close to zero as the sample size <span class="math inline">\(n\)</span> becomes large. Thus, for large
<span class="math inline">\(n\)</span>, we have that <span class="math inline">\(se(pred)\approx s\)</span>, reflecting that the error in
estimating the regression line at a point becomes negligible and
deviation of the additional response from its mean becomes the
entire source of uncertainty.
</p>
<div class="blackbox">
<p><em>Definition</em>. A <span class="math inline">\(100(1-\alpha)\)</span>% prediction interval at <span class="math inline">\(x_{\ast}\)</span> is
<span class="math display">\[\begin{equation}\label{E2:predinteval}
\widehat{y}_{\ast} \pm t_{n-2,1-\alpha /2} ~se(pred)
\end{equation}\]</span>
where the <span class="math inline">\(t\)</span>-value <span class="math inline">\(t_{n-2,1-\alpha /2}\)</span> is the same as used for hypothesis testing and the confidence interval.</p>
</div>
<p>For example, the point prediction at <span class="math inline">\(x_{\ast} = 10,000\)</span> is <span class="math inline">\(\widehat{y}_{\ast}\)</span>= 469.7 + 0.647 (10000) = 6,939.7. The standard error of this prediction is
<span class="math display">\[\begin{equation*}
se(pred) = 3,792 \sqrt{1+\frac{1}{50} + \frac{\left(
10,000-9,311\right)^2}{(50-1)(11,098)^2}} = 3,829.6.
\end{equation*}\]</span>
With a <span class="math inline">\(t\)</span>-value equal to 2.011, this yields an approximate 95% prediction interval
<span class="math display">\[\begin{equation*}
6,939.7 \pm (2.011)(3,829.6) = 6,939.7 \pm 7,701.3 = (-761.6,
~14,641.0).
\end{equation*}\]</span>
We interpret these results by first pointing out that our best estimate of lottery sales for a zip code with a population of 10,000 is $6,939.70. Our 95% prediction interval represents a range of reliability for this prediction. If we could see many zip codes, each with a population of 10,000, on average we expect about 19 out
of 20, or 95%, would have lottery sales between 0 and $14,641. It is customary to truncate the lower bound of the prediction interval to zero if negative values of the response are deemed to be inappropriate.</p>
</div>
</div>
<div id="S2:ResidualAnalsis" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Building a Better Model: Residual Analysis<a href="basic-linear-regression.html#S2:ResidualAnalsis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Quantitative disciplines calibrate models with data. Statistics takes this one step further, using discrepancies between the assumptions and the data to improve model specification. We will examine the Section 2.2 modeling assumptions in light of the data and use any mismatch to specify a better model; this process is known as <em>diagnostic checking</em> (like when you go to a doctor and he or she performs diagnostic routines to check your health).</p>
<p>
</p>
<p>We will begin with the Section 2.2 error representation. Under this
set of assumptions, the deviations {<span class="math inline">\(\varepsilon _i\)</span>} are
identically and independently distributed (i.i.d), and under
assumption F5, normally distributed. To assess the validity of these
assumptions, one uses (observed) residuals {<span class="math inline">\(e_i\)</span>} as
approximations for the (unobserved) deviations {<span class="math inline">\(\varepsilon _i\)</span>}.
The basic theme is that if the residuals are related to a variable
or display any other recognizable pattern, then we should be able to
take advantage of this information and improve our model
specification. The residuals should contain little or no information
and represent only natural variation from the sampling that cannot
be attributed to any specific source. <em>Residual analysis</em> is
the exercise of checking the residuals for patterns.</p>
<p>There are five types of model discrepancies that analysts commonly look for.
If detected, the discrepancies can be corrected with the appropriate
adjustments in the model specification.</p>
<div class="blackbox">
<p><strong>Model Misspecification Issues</strong></p>
<ul>
<li><p><strong>Lack of Independence</strong>. There may exist relationships among the deviations {<span class="math inline">\(\varepsilon _i\)</span>} so that they are not independent.</p></li>
<li><p><strong>Heteroscedasticity</strong>. Assumption E3 that indicates that all observations have a common (although unknown) variability, known as <em>homoscedasticity</em>. <em>Heteroscedascity</em> is the term used when the variability varies by observation.
</p></li>
<li><p><strong>Relationships between Model Deviations and Explanatory Variables</strong>.
If an explanatory variable has the ability to help explain the deviation $$, then one should be able to use this information to better predict <span class="math inline">\(y\)</span>.</p></li>
<li><p><strong>Nonnormal Distributions</strong>. If the distribution of the deviation represents a serious departure from normality, then the usual inference procedures are no longer valid.</p></li>
<li><p><strong>Unusual Points</strong>. Individual observations may have a large effect on the regression model fit, meaning that the results may be sensitive to the impact of a single observation.
\end{enumerate}</p></li>
</ul>
</div>
<p>This list will serve the reader throughout your study of regression analysis. Of course, with only an introduction to basic models we have not yet seen alternative models that might be used when we encounter these model discrepancies. In this book’s Part II on time series models, we will study lack of independence among data ordered over time. Chapter 5 will consider heteroscedasticity in further detail. The introduction to multiple linear regression in Chapter 3
will be our first look at handling relationships between {<span class="math inline">\(\varepsilon _i\)</span>} and additional explanatory variables. We have, however, already had an introduction to the effect of normal
distributions, seeing that <span class="math inline">\(qq\)</span> plots can detect non-normality and that transformations can help induce approximate normality. In this section, we discuss the effects of unusual points.</p>
<p>Much of residual analysis is done by examining a <em>standardized residual</em>, a residual divided by its standard error. An approximate standard error of the residual is <span class="math inline">\(s\)</span>; in Chapter 3 we will give a precise mathematical definition. There are two reasons why we often examine standardized residuals in lieu of basic residuals. First, if responses are normally distributed, then standardized residuals are approximately realizations from a standard normal distribution. This
provides a reference distribution to compare values of standardized residuals. For example, if a standardized residual exceeds two in absolute value, this is considered unusually large and the
observation is called an <em>outlier</em>. Second, because standardized residuals are dimensionless, we get carryover of experience from one data set to another. This is true regardless of whether or not the normal reference distribution is applicable.</p>
<div id="outliers-and-high-leverage-points" class="section level4 unnumbered hasAnchor">
<h4>Outliers and High Leverage Points<a href="basic-linear-regression.html#outliers-and-high-leverage-points" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p></p>
<p>Another important part of residual analysis is the identification of unusual observations in a data set. Because regression estimates are weighted averages with weights that vary by observation, some observations are more important than others. This weighting is more
important than many users of regression analysis realize. In fact, the example below demonstrates that a single observation can have a dramatic effect in a large data set.</p>
<p>There are two directions in which a data point can be unusual, the horizontal and vertical directions. By “unusual,” we mean that an observation under consideration seems to be far from the majority of the data set. An observation that is unusual in the vertical direction is called an <em>outlier</em>. An observation that is unusual in the horizontal directional is called a <em>high leverage point</em>. An observation may be both an outlier and a high
leverage point.</p>
<hr />
<p></p>
<p><strong>Example: Outliers and High Leverage Points.</strong> Consider the fictitious data set of 19 points plus three points, labeled A, B, and C, given in Figure <span class="math inline">\(\ref{F2:Outlier}\)</span> and Table <span class="math inline">\(\ref{T2:Outliers}\)</span>. Think of the first 19 points as “good” observations that represent some type of phenomena. We want to investigate the effect of adding a single aberrant point.</p>
<p>To investigate the effect of each type of aberrant point, Table <span class="math inline">\(\ref{T2:OutlierRegression}\)</span> summarizes the results of four separate regressions. The first regression is for the nineteen base points. The other three regressions use the nineteen base points plus each
type of unusual observation.</p>
<p>Table <span class="math inline">\(\ref{T2:OutlierRegression}\)</span> shows that a regression line provides a good fit for the nineteen base points. The coefficient of determination, <span class="math inline">\(R^2\)</span>, indicates about 89% of the variability has been explained by the line. The size of the typical error, <span class="math inline">\(s\)</span>, is
about 0.29, small compared to the scatter in the <span class="math inline">\(y\)</span>-values. Further, the <span class="math inline">\(t\)</span>-ratio for the slope coefficient is large.</p>
<p>When the outlier point A is added to the nineteen base points, the situation deteriorates dramatically. The <span class="math inline">\(R^2\)</span> drops from 89% to 53.7% and <span class="math inline">\(s\)</span> increases from about 0.29 to about 0.85. The fitted regression line itself does not change that much even though our
confidence in the estimates has decreased.</p>
<p>An outlier is unusual in the <span class="math inline">\(y\)</span>-value, but “unusual in the <span class="math inline">\(y\)</span>-value” depends on the <span class="math inline">\(x\)</span>-value. To see this, keep the <span class="math inline">\(y\)</span>-value of Point A the same, but increase the <span class="math inline">\(x\)</span>-value and call the point B.</p>
<p>When the point B is added to the nineteen base points, the
regression line provides a <em>better</em> fit. Point B is close to
being on the line of the regression fit generated by the nineteen
base points. Thus, the fitted regression line and the size of the
typical error, <span class="math inline">\(s\)</span>, do not change much. However, <span class="math inline">\(R^2\)</span> increases
from 89% to nearly 95 percent. If we think of $ R^2$ as
<span class="math inline">\(1-(Error~SS)/(Total~SS)\)</span>, by adding point B we have increased $
Total~SS$, the total squared deviations in the <span class="math inline">\(y\)</span>’s, even though
leaving $ Error~SS$ relatively unchanged. Point B is not an outlier,
but it is a high leverage point.</p>
<p>To show how influential this point is, drop the <span class="math inline">\(y\)</span>-value
considerably and call this the new point C. When this point is added
to the nineteen base points, the situation deteriorates
dramatically. The <span class="math inline">\(R^2\)</span> coefficient drops from 89% to 10%, and the
<span class="math inline">\(s\)</span> more than triples, from 0.29 to 0.87. Further, the regression
line coefficients change dramatically.</p>
<p>Most users of regression at first do not believe that one point in twenty
can have such a dramatic effect on the regression fit. The fit of a
regression line can always be improved by removing an outlier. If the point
is a high leverage point and not an outlier, it is not clear whether the fit
will be improved when the point is removed.</p>
<hr />
<p>Simply because you can dramatically improve a regression fit by
omitting an observation does not mean you should always do so! The
goal of data analysis is to understand the information in the data.
Throughout the text, we will encounter many data sets where the
unusual points provide some of the most interesting information
about the data. The goal of this subsection is to recognize the
effects of unusual points; Chapter 5 will provide options for
handling unusual points in your analysis.</p>
<p>All quantitative disciplines, such as accounting, economics, linear
programming, and so on, practice the art of <em>sensitivity analysis</em>.
Sensitivity analysis is a description of the global changes in a system due
to a small local change in an element of the system. Examining the effects
of individual observations on the regression fit is a type of sensitivity
analysis.</p>
<div style="page-break-after: always;"></div>
<hr />
<p><strong>Example: Lottery Sales – Continued.</strong> Figure
<span class="math inline">\(\ref{F2:PlotWithKenosha}\)</span> exhibits an outlier; the point in the upper
left-hand side of the plot represents a zip code that includes
Kenosha, Wisconsin. Sales for this zip code are unusually high given
its population. Kenosha is close to the Illinois border; residents
from Illinois probably participate in the Wisconsin lottery thus
effectively increasing the potential pool of sales in Kenosha. Table
<span class="math inline">\(\ref{T2:RegressionKenosha}\)</span> summarizes the regression fit both with
and without this zip code.</p>
<p>For the purposes of inference about the slope, the presence of Kenosha does not alter the results dramatically. Both slope estimates are qualitatively similar and the corresponding
<span class="math inline">\(t\)</span>-statistics are very high, well above cut-offs for statistical significance. However, there are dramatic differences when assessing the quality of the fit. The coefficient of determination, <span class="math inline">\(R^2\)</span>, increased from 78.5% to 88.3% when deleting Kenosha. Moreover, our
“typical deviation” <span class="math inline">\(s\)</span> dropped by over $1,000. This is particularly important if we wish to tighten our prediction intervals.</p>
<p>To check the accuracy of our assumptions, it is also customary to check the normality assumption. One way of doing this is the <span class="math inline">\(qq\)</span> plot, introduced in Section 1.2. The two panels in Figures <span class="math inline">\(\ref{F2:QQplotsKenosha}\)</span> are <span class="math inline">\(qq\)</span> plots with and without the Kenosha
zip code. Recall that points “close” to linear indicate approximate normality. In the right-hand panel of Figure <span class="math inline">\(\ref{F2:QQplotsKenosha}\)</span>, the sequence does appear to be linear so that residuals are approximately normally distributed. This is not the case in the left-hand panel, where the sequence of points appears to climb dramatically for large quantiles. The interesting thing is that the non-normality of the distribution is due to a
single outlier, not a pattern of skewness that is common to all the observations.</p>
<hr />
</div>
</div>
<div id="S2:CAPM" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Application: Capital Asset Pricing Model<a href="basic-linear-regression.html#S2:CAPM" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p></p>
<p>In this section, we study a financial application, the Capital Asset Pricing Model, often referred to by the acronym CAPM. The name is something of a misnomer in that the model is really about <em>returns</em> based on capital assets, not the prices themselves. The types of assets that we examine are equity securities that are traded on an active market, such as the New York Stock Exchange (NYSE). For a stock on the exchange, we can relate returns to prices through the following expression:</p>
<p><span class="math display">\[\begin{equation*}
\mathrm{return=}\frac{\mathrm{
price~at~the~end~of~a~period+dividends-price~at~the~beginning~of~a~period}}{
\mathrm{price~at~the~beginning~of~a~period}}.
\end{equation*}\]</span>
</p>
<p>If we can estimate the returns that a stock generates, then knowledge of the price at the beginning of a generic financial period allows us to estimate the value at the end of the period (ending price plus dividends). Thus, we follow standard practice and model returns of a security.</p>
<p>An intuitively appealing idea, and one of the basic characteristics of the CAPM, is that there should be a relationship between the performance of a security and the market. One rationale is simply that if economic forces are such that the market improves, then those same forces should act upon an individual stock, suggesting that it also improve. As noted above, we measure performance of a security through the return. To measure performance of the market, several market indices exist that summarize the performance of each exchange. We will use the “equally-weighted” index of the Standard &amp; Poor’s 500. The Standard &amp; Poor’s 500 is the collection of the 500 largest companies traded on the NYSE, where “large” is identified by Standard &amp; Poor’s, a financial services rating organization. The equally-weighted index is defined by assuming a portfolio is created by investing one dollar in each of the 500 companies.</p>
<p>
Another rationale for a relationship between security and market returns comes from financial economics theory. This is the CAPM theory, attributed to Sharpe (1964) and Lintner (1965) and based on the portfolio diversification ideas of Harry Markowitz (1959). Other things equal, investors would like to select a return with a high expected value and low standard deviation, the latter being a measure of risk. One of the desirable properties about using standard deviations as a measure of riskiness is that it is straight-forward to calculate the standard deviation of a portfolio. One only needs to know the standard deviation of each security and the correlations among securities. A notable security is a risk-free one, that is, a security that theoretically has a zero standard deviation. Investors often use a 30-day U.S. Treasury bill as an approximation of a risk-free security, arguing that the probability of default of the U.S. government within 30 days is negligible. Positing the existence of a risk-free asset and some other mild conditions, under the CAPM theory there exists an efficient frontier called the <em>securities market line</em>. This frontier specifies the minimum expected return that investors should demand for a specified level of risk. To estimate this line, we can use the equation
<span class="math display">\[\begin{equation*}
\mathrm{E}~r = \beta_0 + \beta_1 r_m
\end{equation*}\]</span>
where <span class="math inline">\(r\)</span> is the security return and <span class="math inline">\(r_m\)</span> is the market return. We
interpret <span class="math inline">\(\beta_1 r_m\)</span> as a measure of the amount of security
return that is attributed to the behavior of the market.</p>
<p>Testing economic theory, or models arising from any discipline, involves
collecting data. The CAPM theory is about ex-ante (before the fact) returns
even though we can only test with ex-post (after the fact) returns. Before
the fact, the returns are unknown and there is an entire distribution of
returns. After the fact, there is only a single realization of the security
and market return. Because at least two observations are required to
determine a line, CAPM models are estimated using security and market data
gathered over time. In this way, several observations can be made. For the
purposes of our discussions, we follow standard practice in the securities
industry and examine monthly prices.</p>
<div id="data" class="section level3 unnumbered hasAnchor">
<h3>Data<a href="basic-linear-regression.html#data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p></p>
<p>To illustrate, consider monthly returns over the five year period
from January, 1986 to December, 1990, inclusive. Specifically, we
use the security returns from the Lincoln National Insurance
Corporation as the dependent variable (<span class="math inline">\(y\)</span>) and the market returns
from the index of the Standard &amp; Poor’s 500 Index as the
explanatory variable (<span class="math inline">\(x\)</span>). At the time, the Lincoln was a large,
multi-line, insurance company, headquartered in the midwest of the
U.S., specifically in Fort Wayne, Indiana. Because it was well known
for its’ prudent management and stability, it is a good company to
begin our analysis of the relationship between the market and an
individual stock.</p>
<p>We begin by interpreting some of basic summary statistics, in Table
<span class="math inline">\(\ref{T2:SumStatsCAPM}\)</span>, in terms of financial theory. First, an
investor in the Lincoln will be concerned that the five year average
return, <span class="math inline">\(\overline{y}=0.00510\)</span>, is below the return of the market,
<span class="math inline">\(\overline{x}=0.00741\)</span>. Students of interest theory recognize that
monthly returns can be converted to an annual basis using geometric
compounding. For example, the annual return of the Lincoln is
<span class="math inline">\((1.0051)^{12}-1=0.062946\)</span>, or roughly 6.29 percent. This is
compared to an annual return of 9.26% (= (1<span class="math inline">\(00((1.00741)^{12}-1\)</span>))
for the market. A measure of risk, or volatility, that is used in
finance is the standard deviation. Thus, interpret <span class="math inline">\(s_y\)</span> = 0.0859
<span class="math inline">\(&gt;\)</span> 0.05254 = <span class="math inline">\(s_x\)</span> to mean that an investment in the Lincoln is
riskier than that of the market. Another interesting aspect of Table
<span class="math inline">\(\ref{T2:SumStatsCAPM}\)</span> is that the smallest market return, -0.22052,
is 4.338 standard deviations below its average
((-0.22052-0.00741)/0.05254 = -4.338). This is highly unusual with
respect to a normal distribution.</p>
<p>We next examine the data over time, as is given graphically in
Figure <span class="math inline">\(\ref{F2:TimeSeriesPlots}\)</span>. These are scatter plots of the
returns versus time, called <em>time series plots</em>. In Figure
<span class="math inline">\(\ref{F2:TimeSeriesPlots}\)</span>, one can clearly see the smallest market
return and a quick glance at the horizontal axis reveals that this
unusual point is in October, 1987, the time of the well-known market
crash.</p>
<p>The scatter plot in Figure <span class="math inline">\(\ref{F2:LincolnvsMarket}\)</span> graphically
summarizes the relationship between Lincoln’s return and the return
of the market. The market crash is clearly evident in Figure
<span class="math inline">\(\ref{F2:LincolnvsMarket}\)</span> and represents a high leverage point. With
the regression line (described below) superimposed, the two outlying
points that can be seen in Figure <span class="math inline">\(\ref{F2:TimeSeriesPlots}\)</span> are also
evident. Despite these anomalies, the plot in Figure
<span class="math inline">\(\ref{F2:LincolnvsMarket}\)</span> does suggest that there is a linear
relationship between Lincoln and market returns.</p>
</div>
<div id="unusual-points" class="section level3 unnumbered hasAnchor">
<h3>Unusual Points<a href="basic-linear-regression.html#unusual-points" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To summarize the relationship between the market and Lincoln’s return, a regression model was fit. The fitted regression is
<span class="math display">\[\begin{equation*}
\widehat{LINCOLN}=-0.00214+0.973 MARKET.
\end{equation*}\]</span>
The resulting estimated standard error, <span class="math inline">\(s\)</span> = 0.0696 is lower than the standard deviation of Lincoln’s returns, <span class="math inline">\(s_y=0.0859\)</span>. Thus, the regression model explains some of the variability of Lincoln’s returns. Further, the <span class="math inline">\(t\)</span>-statistic associated with the slope <span class="math inline">\(b_1\)</span> turns out to be <span class="math inline">\(t(b_1)=5.64\)</span>, which is significantly large. One disappointing aspect is that the statistic <span class="math inline">\(R^2=35.4\%\)</span> can be interpreted as saying that the market explains only a little over a third of the variability. Thus, even though the market is clearly an
important determinant, as evidenced by the high <span class="math inline">\(t\)</span>-statistic, it provides only a partial explanation of the performance of the Lincoln’s returns.</p>
<p>In the context of the market model, we may interpret the standard deviation of the market, <span class="math inline">\(s_x\)</span>, as <em>non-diversifiable risk</em>. Thus, the risk of a security can be decomposed into two components, the diversifiable component and the market component, which is non-diversifiable. The idea here is that by combining several securities we can create a portfolio of securities that, in most instances, will reduce the riskiness of our holdings when compared with a single security. Again, the rationale for holding a security is that we are compensated through higher expected returns by holding a security with higher riskiness. To quantify the relative riskiness, it is not hard to show that
<span class="math display">\[\begin{equation}
s_y^2 = b_1^2 s_x^2 + s^2 \frac{n-2}{n-1}.
\end{equation}\]</span></p>
<p>The riskiness of a security is due to the riskiness due to the market plus the riskiness due to a diversifiable component. Note that the riskiness due to the market component, <span class="math inline">\(s_x^2\)</span>, is larger for securities with larger slopes. For this reason, investors think
of securities with slopes <span class="math inline">\(b_1\)</span> greater than one as “aggressive” and slopes less than one as “defensive.”</p>
</div>
<div id="sensitivity-analysis" class="section level3 unnumbered hasAnchor">
<h3>Sensitivity Analysis<a href="basic-linear-regression.html#sensitivity-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The above summary immediately raises two additional issues. First,
what is the effect of the October, 1987 crash on the fitted
regression equation? We know that unusual observations, such as the
crash, may potentially influence the fit a great deal. To this end,
the regression was re-run without the observation corresponding to
the crash. The motivation for this is that the October 1987 crash
represents a combination of highly unusual events (the interaction
of several automated trading programs operated by the large stock
brokerage houses) that we do not wish to represent using the same
model as our other observations. Deleting this observation, the
fitted regression is
<span class="math display">\[\begin{equation*}
\widehat{LINCOLN} = -0.00181 + 0.956 MARKET,
\end{equation*}\]</span>
with <span class="math inline">\(R^2=26.4\%\)</span>, <span class="math inline">\(t(b_1)=4.52\)</span>, <span class="math inline">\(s=0.0702\)</span> and <span class="math inline">\(s_y=0.0811\)</span>. We
interpret these statistics in the same fashion as the fitted model
including the October 1987 crash. It is interesting to note,
however, that the proportion of variability explained has actually
<em>decreased</em> when excluding the influential point. This serves
to illustrate an important point. High leverage points are often
looked upon with dread by data analysts because they are, by
definition, unlike other observations in the data set and require
special attention. However, when fitting relationships among
variables, they also represent an opportunity because they allow the
data analyst to observe the relationship between variables over
broader ranges than otherwise possible. The downside is that these
relationships may be nonlinear or follow an entirely different
pattern when compared to the relationships observed in the main
portion of the data.</p>
<p>The second question raised by the regression analysis is what can be
said about the unusual circumstances that gave rise to the unusual
behavior of Lincoln’s returns in October and November of 1990. A
useful feature of regression analysis is to identify and raise the
question; it does not resolve it. Because the analysis clearly
pinpoints two highly unusual points, it suggests to the data analyst
to go back and ask some specific questions about the sources of the
data. In this case, the answer is straightforward. In October of
1990, the Travelers’ Insurance Company, a competitor, announced that
it would take a large write-off in their real estate portfolio. due
to an unprecedented number of mortgage defaults. The market reacted
quickly to this news, and investors assumed that other large stock
life insurers would also soon announce large write-offs.
Anticipating this news, investors tried to sell their portfolios of,
for example, Lincoln’s stock, thus causing the price to plummet.
However, it turned out that investors overreacted to this news and
that Lincoln’s portfolio of real estate was indeed sound. Thus,
prices quickly returned to their historical levels.</p>
</div>
</div>
<div id="illustrative-regression-computer-output" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Illustrative Regression Computer Output<a href="basic-linear-regression.html#illustrative-regression-computer-output" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Computers and statistical software packages that perform specialized
calculations play a vital role in modern-day statistical analyses.
Inexpensive computing capabilities have allowed data analysts to
focus on relationships of interest. Specifying models that are
attractive merely for their computational simplicity is much less
important now compared to times before the widespread availability
of inexpensive computing. An important theme of this text is to
focus on relationships of interest and to rely on widely available
statistical software to estimate the models that we specify.</p>
<p>With any computer package, generally the most difficult parts of operating
the package are the (i) input, (ii) using the commands and (iii)
interpreting the output. You will find that most modern statistical software
packages accept spreadsheet or text-based files, making input of data
relatively easy. Personal computer statistical software packages have
menu-driven command languages with easily accessible on-line help
facilities. Once you decide what to do, finding the right commands is
relatively easy.</p>
<p>This section provides guidance in interpreting the output of
statistical packages. Most statistical packages generate similar
output. Below, three examples of standard statistical software
packages, EXCEL, SAS and R are given. The annotation symbol
“.]” marks a statistical quantity that is described in the
legend. Thus, this section provides a link between the notation used
in the text and output from some of the standard statistical
software packages.</p>
<p>{<strong>EXCEL Output</strong>}</p>
<p>{<strong>SAS Output</strong>}</p>

<p>{<strong>R Output</strong>}</p>

<p>*Legend Annotation Definition, Symbol**
</p>
</div>
<div id="further-reading-and-references" class="section level2 hasAnchor" number="2.9">
<h2><span class="header-section-number">2.9</span> Further Reading and References<a href="basic-linear-regression.html#further-reading-and-references" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Relatively few applications of regression are basic in the sense
that they use only one explanatory variable; the purpose of
regression analysis is to reduce complex relationships among many
variables. Section <a href="basic-linear-regression.html#S2:CAPM">2.7</a> described an important exception to
this general rule, the CAPM finance model; see Panjer et al. (1998)
for additional actuarial descriptions of this model. Campbell et al.
(1997) gives a financial econometrics perspective.</p>
<div style="page-break-after: always;"></div>
<p><strong>Chapter References</strong></p>
<p>\begin{multicols}{2}</p>
<p>Anscombe, Frank (1973). Graphs in statistical analysis. <em>The American Statistician</em> 27, 17-21.</p>
<p>Campbell, John Y., Andrew W. Lo and A. Craig MacKinlay (1997). <em>The Econometrics of Financial Markets</em>. Princeton University Press, Princeton, New Jersey.</p>
<p>Frees, Edward W. and Tom W. Miller (2003). Sales forecasting using longitudinal data models. <em>International Journal of Forecasting</em> 20, 97-111.</p>
<p>Goldberger, Arthur (1991). <em>A Course in Econometrics</em>. Harvard University Press, Cambridge.</p>
<p>Koch, Gary J. (1985). A basic demonstration of the [-1, 1] range for the correlation coefficient. <em>American Statistician</em> 39, 201-202.</p>
<p>Linter, J. (1965). The valuation of risky assets and the selection of risky investments in stock portfolios and capital budgets. <em>Review of Economics and Statistics</em>, 13-37.</p>
<p>Manistre, B. John and Geoffrey H. Hancock (2005). Variance of the CTE estimator. <em>North American Actuarial Journal</em> 9(2), 129-156.</p>
<p>Markowitz, Harry (1959). <em>Portfolio Selection: Efficient Diversification of Investments</em>. John Wiley, New York.</p>
<p>Panjer, Harry H., Phelim P. Boyle, Samuel H. Cox, Daniel Dufresne, Hans U. Gerber, Heinz H. Mueller, Hal W. Pedersen, Stanley R. Pliska, Michael Sherris, Elias S. Shiu and Ken S. Tan (1998). <em>Financial Economics: With Applications to Investment, Insurance and Pensions</em>. Society of Actuaries, Schaumburg, Illinois.</p>
<p>Pearson, Karl (1895). <em>Royal Society Proceedings</em> 58, 241.</p>
<p>Serfling, Robert J. (1980). <em>Approximation Theorems of Mathematical Statistics</em>. John Wiley and Sons, New York.</p>
<p>Sharpe, William F. (1964). Capital asset prices: A theory of market equilibrium under risk. <em>Journal of Finance</em>, 425-442.</p>
<p>Stigler, Steven M. (1986). <em>The History of Statistics: The Measurement of Uncertainty before 1900</em>. Harvard University Press, Cambridge, MA.</p>
</div>
<div id="exercises" class="section level2 hasAnchor" number="2.10">
<h2><span class="header-section-number">2.10</span> Exercises<a href="basic-linear-regression.html#exercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
Consider the following data set~~
<p>.</p>
<p>Fit a regression line using the method of least squares. Determine
<span class="math inline">\(r\)</span>, <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_0\)</span>.</p>
<strong>A perfect relationship, yet zero correlation.</strong> Consider the
quadratic relationship <span class="math inline">\(y=x^2\)</span>, with data $
<p>.$</p>
<ol style="list-style-type: lower-alpha">
<li><p>Produce a rough graph for this data set.</p></li>
<li><p>Check that the correlation coefficient is <span class="math inline">\(r=0\)</span>.</p></li>
</ol>
<p>*Boundedness of the correlation coefficient.** Use the
following steps to show that <span class="math inline">\(r\)</span> is bounded by -1 and 1 (These steps
are due to Koch, 1990).</p>
<ol style="list-style-type: lower-alpha">
<li><p>Let <span class="math inline">\(a\)</span> and <span class="math inline">\(c\)</span> be generic constants. Verify
<span class="math display">\[\begin{eqnarray*}
0 &amp; \leq &amp; \frac{1}{n-1}\sum_{i=1}^{n}\left(
a\frac{x_i-\overline{x}}{s_x}-c
\frac{y_i-\overline{y}}{s_y}\right) ^2 \\
&amp;=&amp; a^2+c^2-2acr.
\end{eqnarray*}\]</span></p></li>
<li><p>Use the results in part (a) to show <span class="math inline">\(2ac(r-1)\leq (a-c)^2.\)</span></p></li>
<li><p>By taking <span class="math inline">\(a=c\)</span>, use the result in part (b) to show <span class="math inline">\(r\leq 1\)</span>.</p></li>
<li><p>By taking <span class="math inline">\(a=-c\)</span>, use the results in part (b) to show <span class="math inline">\(r\geq -1\)</span>.</p></li>
<li><p>Under what conditions is <span class="math inline">\(r=-1\)</span>? Under what conditions is <span class="math inline">\(r=1\)</span>?</p></li>
</ol>
<p>*Regression coefficients are weighted sums.** Show that
the intercept term, <span class="math inline">\(b_0\)</span>, can be expressed as a weighted sum of the
dependent variables. That is, show that
<span class="math inline">\(b_0=\sum_{i=1}^{n}w_{i,0}y_i.\)</span> Further, express the weights in
terms of the slope weights, <span class="math inline">\(w_i\)</span>.</p>
<p>*Another expression for the slope as a weighted sum**</p>
<ol style="list-style-type: lower-alpha">
<li><p>Using algebra, establish an alternative expression
<span class="math display">\[\begin{equation*}
b_1=\frac{\sum_{i=1}^{n}weight_i~slope_i}{ \sum_{i=1}^{n}weight_i}.
\end{equation*}\]</span>
Here, <span class="math inline">\(slope_i\)</span> is the slope between <span class="math inline">\((x_i,y_i)\)</span> and
<span class="math inline">\((\bar{x},\bar{y})\)</span>. Give a precise form for the weight <span class="math inline">\(weight_i\)</span>
as a function of the explanatory variable <span class="math inline">\(x\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(\bar{x} = 4,\bar{y} = 3, x_1 = 2 \mathrm{~and~} y_1 = 6\)</span>. Determine the slope and weight for the first observation, that
is, <span class="math inline">\(slope_1\)</span> and <span class="math inline">\(weight_1\)</span>.</p></li>
</ol>
<p>Consider two variables, <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>. Do a regression
of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span> to get a slope coefficient which we call <span class="math inline">\(b_{1,x,y}\)</span>.
Do another regression of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span> to get a slope coefficient which
we call <span class="math inline">\(b_{1,y,x}\)</span>. Show that the correlation coefficient between
<span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is the geometric mean of the two slope coefficients up
to sign, that is, show that <span class="math inline">\(|r|=\sqrt{ b_{1,x,y}b_{1,y,x}}.\)</span></p>
<p>*Regression through the origin.** Consider the model
<span class="math inline">\(y_i=\beta_1 x_i + \varepsilon _i\)</span>, that is, regression with one
explanatory variable <em>without</em> the intercept term. This model
is called <em>regression through the origin</em> because the true
regression line <span class="math inline">\(\mathrm{E}y = \beta_1 x\)</span> passes through the origin
(the point (0, 0)). For this model, the least squares estimate of
<span class="math inline">\(\beta_1\)</span> is that number <span class="math inline">\(b_1\)</span> that minimizes the sum of squares
<span class="math inline">\(\mathrm{SS}(b_1^{\ast} )=\sum_{i=1}^{n}\left( y_i - b_1^{\ast}x_i\right) ^2.\)</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>Verify that
<span class="math display">\[\begin{equation*}
b_1 = \frac{\sum_{i=1}^{n} x_i y_i}{\sum_{i=1}^{n}x_i^2}.
\end{equation*}\]</span></p></li>
<li><p>Consider the model <span class="math inline">\(y_i=\beta_1 z_i^2 + \varepsilon _i\)</span>, a
quadratic model passing through the origin. Use the result of part</p></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>to determine the least squares estimate of <span class="math inline">\(\beta_1\)</span>.</li>
</ol>
<p> a. Show that
<span class="math display">\[\begin{equation*}
s_y^2=\frac{1}{n-1}\sum_{i=1}^{n}\left( y_i-\overline{y}\right) ^2=
\frac{1}{n-1}\left( \sum_{i=1}^{n}y_i^2-n\overline{y}^2\right) .
\end{equation*}\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>Follow the same steps to show <span class="math inline">\(\sum_{i=1}^{n}\left( y_i - \overline{y} \right) \left( x_i-\overline{x}\right) =\sum_{i=1}^{n} x_i y_i - n \overline{x}~\overline{y}.\)</span></p></li>
<li><p>Show that
<span class="math display">\[\begin{equation*}
b_{1}=\frac{\sum_{i=1}^{n}\left( y_i-\overline{y}\right) \left( x_i-
\overline{x}\right) }{\sum_{i=1}^{n}\left( x_i - \overline{x}
\right) ^2}
\end{equation*}\]</span></p></li>
<li><p>Establish the commonly used formula
\begin{equation*}
b_{1}=\frac{<em>{i=1}^{n}x_iy_i-n~}{</em>{i=1}<sup>{n}x_i</sup>2</p></li>
</ol>
<ul>
<li>n^2}.
\end{equation*}</li>
</ul>

<p>*Interpretation of coefficients associated with a binary
explanatory variable.** Suppose that <span class="math inline">\(x_i\)</span> only
takes on the values 0 and 1. Out of the <span class="math inline">\(n\)</span> observations, <span class="math inline">\(n_1\)</span> take
on the value <span class="math inline">\(x=0\)</span>. These $n_1 $ observations have an average <span class="math inline">\(y\)</span>
value of <span class="math inline">\(\overline{y}_1\)</span>. the remaining <span class="math inline">\(n-n_1\)</span> observations have
value <span class="math inline">\(x=1\)</span> and an average <span class="math inline">\(y\)</span> value of <span class="math inline">\(\overline{y}_2\)</span>. Use
Exercise 2.<span class="math inline">\(\ref{Ex:SimpleBLR}\)</span> to show that <span class="math inline">\(b_1 = \overline{y}_2 - \overline{y}_1.\)</span></p>
<div style="page-break-after: always;"></div>
<p></p>
<p>*Nursing Home Utilization.**
This exercise considers nursing home data provided by the Wisconsin
Department of Health and Family Services (DHFS) and described in
Exercise 1.<span class="math inline">\(\ref{Ex:NursHome}\)</span>.</p>
<p><strong>Part 1:</strong> Use cost report year 2000 data, and do the
following analysis.</p>
<ol style="list-style-type: lower-alpha">
<li>Correlations</li>
</ol>
<p>a(i). Calculate the correlation between TPY and LOGTPY. Comment on
your result.</p>
<p>a(ii). Calculate the correlation among TPY, NUMBED and SQRFOOT. Do
these variables appear highly correlated?</p>
<p>a(iii). Calculate the correlation between TPY and NUMBED/10. Comment
on your result.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>Scatter plots. Plot TPY versus NUMBED and TPY versus SQRFOOT.
Comment on the plots.</p></li>
<li><p>Basic linear regression.</p></li>
</ol>
<p>c(i). Fit a basic linear regression model using TPY as the outcome
variable and NUMBED as the explanatory variable. Summarize the fit
by quoting the coefficient of determination, <span class="math inline">\(R^2\)</span>, and the
<span class="math inline">\(t\)</span>-statistic for NUMBED.</p>
<p>c(ii). Repeat c(i), using SQRFOOT instead of NUMBED. In terms of
<span class="math inline">\(R^2\)</span>, which model fits better?</p>
<p>c(iii). Repeat c(i), using LOGTPY for the outcome variable and
LOG(NUMBED) as the explanatory variable.</p>
<p>c(iv). Repeat c(iii), using LOGTPY for the outcome variable and
LOG(SQRFOOT) as the explanatory variable.</p>
<p><strong>Part 2:</strong> Fit the model in Part 1.c(1) using 2001 data. Are
the patterns stable over time?</p>
<div id="sections-2.3-2.4" class="section level3 unnumbered hasAnchor">
<h3>Sections 2.3-2.4<a href="basic-linear-regression.html#sections-2.3-2.4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose that, for a sample size of <span class="math inline">\(n\)</span> = 3, you have $
e_2$ = 24 and <span class="math inline">\(e_{3}\)</span> = -1. Determine <span class="math inline">\(e_{1}\)</span>.</p>
<p>Suppose that <span class="math inline">\(r=0\)</span>, <span class="math inline">\(n=15\)</span> and <span class="math inline">\(s_y = 10\)</span>. Determine
<span class="math inline">\(s\)</span>.</p>
<p>*The correlation coefficient and
the coefficient of determination.** Use the
following steps to establish a relationship between the coefficient
of determination and the correlation coefficient.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Show that <span class="math inline">\(\widehat{y}_i-\overline{y}=b_1(x_i-\overline{x}).\)</span></p></li>
<li><p>Use part (a) to show that <span class="math inline">\(Regress~SS=\)</span> <span class="math inline">\(\sum_{i=1}^{n}\left( \widehat{y}_i - \overline{y} \right)^2 = b_1^2s_x^2(n-1).\)</span></p></li>
<li><p>Use part (b) to establish <span class="math inline">\(R^2=r^2.\)</span></p></li>
</ol>
<p> Show that the average residual is zero, that is, show
that <span class="math inline">\(n^{-1}\sum_{i=1}^{n} e_i=0.\)</span></p>
<p>*Correlation between residuals and explanatory variables.**
Consider a generic sequence of pairs of numbers (<span class="math inline">\(x_1,y_1\)</span>), …,
(<span class="math inline">\(x_n,y_n\)</span>) with the correlation coefficient computed as<br />
<span class="math inline">\(r(y,x)=\left[ (n-1)s_ys_x\right] ^{-1}\sum_{i=1}^{n}\left( y_i-\overline{y}\right) \left( x_i-\overline{x}\right) .\)</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>Suppose that either <span class="math inline">\(\overline{y}=0,\overline{x}=0\)</span> or both
<span class="math inline">\(\overline{x}\)</span>  and <span class="math inline">\(\overline{y}=0.\)</span> Then, check that <span class="math inline">\(r(y,x)=0\)</span>
implies <span class="math inline">\(\sum_{i=1}^{n}y_i x_i=0\)</span> and vice-versa.</p></li>
<li><p>Show that the correlation between the residuals and the
explanatory variables is zero. Do this by using part (a) of Exercise
2.13 to show that <span class="math inline">\(\sum_{i=1}^{n} x_i e_i = 0\)</span> and then apply part
(a).</p></li>
<li><p>Show that the correlation between the residuals and fitted values
is zero. Do this by showing that <span class="math inline">\(\sum_{i=1}^n \widehat{y}_i e_i = 0\)</span>  and then apply part (a).</p></li>
</ol>
<p>*Correlation and <span class="math inline">\(t\)</span>-statistics.** Use the following steps to
establish a relationship between the correlation coefficient and the
<span class="math inline">\(t\)</span>-statistic for the slope.</p>
<p>a Use algebra to check that
<span class="math display">\[\begin{equation*}
R^2=1-\frac{n-2}{n-1}\frac{s^2}{s_y^2}.
\end{equation*}\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>Use part (a) to establish the following quick computational
formula for <span class="math inline">\(s,\)</span>
<span class="math display">\[\begin{equation*}s = s_y \sqrt{(1-r^2)\frac{n-1}{n-2}}.\end{equation*}\]</span></p></li>
<li><p>Use part (b) to show that
<span class="math display">\[\begin{equation*}
t(b_1) = \sqrt{n-2}\frac{r}{\sqrt{1-r^2}}.
\end{equation*}\]</span></p></li>
</ol>
<div id="sections-2.6-2.7" class="section level4 unnumbered hasAnchor">
<h4>Sections 2.6-2.7<a href="basic-linear-regression.html#sections-2.6-2.7" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>*Effects of an unusual point.** You are analyzing a data
set of size <span class="math inline">\(n=100\)</span>. You have just performed a regression analysis
using one predictor variable and notice that the residual for the
10th observation is unusually large.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Suppose that, in fact, it turns out that <span class="math inline">\(e_{10}=8s\)</span>. What
percentage of the error sum of squares, <span class="math inline">\(Error~SS\)</span>, is due to the
10th observation?</p></li>
<li><p>Suppose that <span class="math inline">\(e_{10}=4s\)</span>. What percentage of the error sum of
squares, <span class="math inline">\(Error~SS\)</span>, is due to the 10th observation?</p></li>
<li><p>Suppose that you reduce the data set to size <span class="math inline">\(n=20\)</span>. After
running the regression, it turns out that we still have <span class="math inline">\(10=4s\)</span>.
What percentage of the error sum of squares, <span class="math inline">\(Error~SS\)</span>, is due to
the 10th observation?</p></li>
</ol>
<p>Consider a data set consisting of 20 observations
with the following summary statistics: <span class="math inline">\(\overline{x}=0\)</span>,
<span class="math inline">\(\overline{y}=9\)</span>, <span class="math inline">\(s_x=1\)</span> and <span class="math inline">\(s_y=10\)</span>. You run a regression using
using one variable and determine that <span class="math inline">\(s=7\)</span>. Determine the standard
error of a prediction at <span class="math inline">\(x_{\ast}=1.\)</span></p>
<p></p>
<p>*Summary statistics can hide important relationships.** The data in Table <span class="math inline">\(\ref{Ex:Anscombes}\)</span> is due to
Anscombe (1973). The purpose of this exercise is to demonstrate how
plotting data can reveal important information that is not evident
in numerical summary statistics.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Compute the averages and standard deviations of each column of
data. Check that the averages and standard deviations of each of the
<span class="math inline">\(x\)</span> columns are the same, within two decimal places, and similarly
for each of the <span class="math inline">\(y\)</span> columns.</p></li>
<li><p>Run four regressions, (1) <span class="math inline">\(y_{1}\)</span> on <span class="math inline">\(x_{1}\)</span>, (2) <span class="math inline">\(y_2\)</span> on
<span class="math inline">\(x_{1}\)</span>, (3) <span class="math inline">\(y_{3}\)</span> on <span class="math inline">\(x_{1}\)</span> and (4) <span class="math inline">\(y_{4}\)</span> on <span class="math inline">\(x_2\)</span>. Verify,
for each of the four regressions fits, that <span class="math inline">\(b_0\approx 3.0\)</span>,
<span class="math inline">\(b_{1}\approx 0.5\)</span>, <span class="math inline">\(s\approx 1.237\)</span> and <span class="math inline">\(R^2\approx 0.677\)</span>, within
two decimal places.</p></li>
<li><p>Produce scatter plots for each of the four regression models that
you fit in part (b).</p></li>
<li><p>Discuss the fact that the fitted regression models produced in
part (b) imply that the four data sets are similar although the four
scatter plots produced in part (c) yield a dramatically different
story.</p></li>
</ol>
<p></p>
<p>*Nursing Home Utilization.**
This exercise considers nursing home data provided by the Wisconsin
Department of Health and Family Services (DHFS) and described in
Exercise 1.<span class="math inline">\(\ref{Ex:NursHome}\)</span> and 2.<span class="math inline">\(\ref{Ex:NursHome2a}\)</span>.</p>
<p>You decide to examine the relationship between total patient years
(LOGTPY) and the number of beds (LOGNUMBED), both in logarithmic
units, using cost report year 2001 data.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Summary statistics. Create basic summary statistics for each
variable. Summarize the relationship through a correlation statistic
and a scatter plot.</p></li>
<li><p>Fit the basic linear model. Cite the basic summary statistics,
include the coefficient of determination, the regression coefficient
for LOGNUMBED and the corresponding <span class="math inline">\(t\)</span>-statistic.</p></li>
<li><p>Hypothesis testing. Test the following hypotheses at the 5 level
of significance using a <span class="math inline">\(t\)</span>-statistic. Also compute the
corresponding <span class="math inline">\(p\)</span>-value.</p></li>
</ol>
<p>c(i). Test <span class="math inline">\(H_0: \beta_1 = 0\)</span> versus <span class="math inline">\(H_a: \beta_1 \neq 0\)</span>.</p>
<p>c(ii). Test <span class="math inline">\(H_0: \beta_1 = 1\)</span> versus <span class="math inline">\(H_a: \beta_1 \neq 1\)</span>.</p>
<p>c(iii). Test <span class="math inline">\(H_0: \beta_1 = 1\)</span> versus <span class="math inline">\(H_a: \beta_1 &gt; 1\)</span>.</p>
<p>c(iv). Test <span class="math inline">\(H_0: \beta_1 = 1\)</span> versus <span class="math inline">\(H_a: \beta_1 &lt; 1\)</span>.</p>
<ol start="4" style="list-style-type: lower-alpha">
<li>You are interested in the effect that a marginal change in
LOGNUMBED has on the expected value of LOGTPY.</li>
</ol>
<p>d(i). Suppose that there is a marginal change in LOGNUMBED of 2.
Provide a point estimate of the expected change in LOGTPY.</p>
<p>d(ii). Provide a 95% confidence interval corresponding to the point
estimate in part d(i).</p>
<p>d(iii). Provide a 99% confidence interval corresponding to the
point estimate in part d(i).</p>
<ol start="5" style="list-style-type: lower-alpha">
<li>At a specified number of beds estimate <span class="math inline">\(x_{*} = 100\)</span>, do these
things:</li>
</ol>
<p>e(i). Find the predicted value of LOGTPY.</p>
<p>e(ii). Obtain the standard error of the prediction.</p>
<p>e(iii). Obtain a 95% prediction interval for your prediction.</p>
<p>e(iv). Convert the point prediction in part e(i) and the prediction
interval obtained in part e(iii) into total person years (through
exponentiation).</p>
<p>e(v). Obtain a prediction interval as in part e(iv), corresponding
to a 90% level (in lieu of 95%).</p>
<p></p>
<p>*Initial Public Offerings.** As a financial analyst, you wish to convince a client of the merits
of investing in firms that have just entered a stock exchange, as an
IPO (initial public offering). Thus, you gather data on 116 firms
that priced during the six-month time frame of January 1, 1998
through June 1, 1998. By looking at this recent historical data, you
are able to compute RETURN, the firm’s one-year return (in percent).</p>
<p>You are also interested in looking at financial characteristics of
the firm that may help you understand (and predict) the return. You
initially examine REVENUE, the firm’s 1997 revenues in millions of
dollars. Unfortunately, this variable was not available for six
firms. Thus, the statistics below are for the 110 firms that have
both REVENUES and RETURNS. In addition, Table <span class="math inline">\(\ref{Ex:IPOSumStats}\)</span>
provides information on the (natural) logarithmic revenues, denoted
as LnREV, and the initial price of the stock, denoted as PRICEIPO.</p>
<ol style="list-style-type: lower-alpha">
<li>You hypothesize that larger firms, as measured by revenues, are
more stable and thus should enjoy greater returns. You have
determined that the correlation between RETURN and REVENUE is
-0.0175.</li>
</ol>
<p>a(i). Calculate the least squares fit using REVENUE to predict
RETURN. Determine <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>.</p>
<p>a(ii). For Hyperion Telecommunications, REVENUEs are 95.55
(millions of dollars). Calculate the fitted RETURN using the
regression fit in part a(i).</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Logarithmic revenues and returns.</li>
</ol>
<p>b(i). Suppose instead that you use LnREVs to predict RETURN.
Calculate the fitted RETURN under this regression model. Is this
equal your answer in part a(ii)?</p>
<p>b(ii) Do logarithmic revenues significantly affect returns? To this
end, provide a formal test of hypothesis. State your null and
alternative hypotheses, decision-making criterion and your
decision-making rule. Use a 10% level of significance.</p>
<p>b(iii). You conjecture that, other things equal, that firms with
larger revenues will be more stable and thus enjoy a larger initial
return. Thus, you wish to consider the null hypothesis of no
relation between LnREV and RETURN versus the alternative hypothesis
that there is a positive relation between LnREV and RETURN. To this
end, provide a formal test of hypothesis. State your null and
alternative hypotheses, decision-making criterion and your
decision-making rule. Use a 10% level of significance.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li><p>Determine the correlation between LnREV and RETURN. Be sure to
state whether this correlation is positive, negative or zero.</p></li>
<li><p>You are considering investing in a firm that has LnREV = 2 (so
revenues are <span class="math inline">\(e^2\)</span> = 7.389 millions of dollars).</p></li>
</ol>
<p>d(i). Using the fitted regression model, determine the least squares
point prediction.</p>
<p>d(ii). Determine the 95% prediction interval corresponding to your
prediction in part d(i).</p>
<ol start="5" style="list-style-type: lower-alpha">
<li>The <span class="math inline">\(R^2\)</span> from the fitted regression model is a disappointing
3.5%. Part of the difficulty is due to observation number 59, the
Inktomi Corporation. Inktomi sales are 12th smallest of the data
set, with LnREV = 1.76 (so revenues are <span class="math inline">\(e^{1.76} = 5.79\)</span> millions
of dollars), yet it has the highest first year return, with RETURN =
433.33.</li>
</ol>
<p>e(i). Calculate the residual for this observation.</p>
<p>e(ii). What proportion of the unexplained variability (error sum of
squares) does this observation account for?</p>
<p>e(iii). Define the idea of a high leverage observation.</p>
<p>e(iv). Would this observation be considered a high leverage
observation? Justify your answer.</p>
<p></p>
<p>*National Life Expectancies.** We
continue the analysis begun in Exercise 1.<span class="math inline">\(\ref{Ex:UNLIFE}\)</span> by
examining the relation between <span class="math inline">\(y= LIFEEXP\)</span> and <span class="math inline">\(x=FERTILITY\)</span>, shown
in Figure <span class="math inline">\(\ref{Ex:UNLIFEPlot1}\)</span>. Fit a linear regression model of $
LIFEEXP$ using the explanatory variable <span class="math inline">\(x=FERTILITY\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>The US has a FERTILITY rate of 2.0. Determine the fitted life
expectancy.</p></li>
<li><p>The island nation Dominica did not report a FERTILITY rate and
thus was not included in the regression. Suppose that its FERTILITY
rate is 2.0. Provide a 95% prediction interval for the life
expectancy in Dominica.</p></li>
<li><p>China has a FERTILITY rate of 1.7 and a life expectancy of 72.5.
Determine the residual under the model. How many multiples of <span class="math inline">\(s\)</span> is
this residual from zero?</p></li>
<li><p>Suppose that your prior hypothesis is that the FERTILITY slope is
-6.0 and you wish to test the null hypothesis that the slope has
increased (that is, the slope is greater than -6.0). Test this
hypothesis at the 5% level of significance. Also compute an
approximate <span class="math inline">\(p\)</span>-value.</p></li>
</ol>
</div>
</div>
</div>
<div id="technical-supplement---elements-of-matrix-algebra" class="section level2 hasAnchor" number="2.11">
<h2><span class="header-section-number">2.11</span> Technical Supplement - Elements of Matrix Algebra<a href="basic-linear-regression.html#technical-supplement---elements-of-matrix-algebra" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Examples are an excellent tool for introducing technical topics such as regression. However, this chapter has also used algebra as well as basic probability and statistics to give you further insights into regression analysis. Going forward, we will be studying multivariate relationships. With many things happening concurrently in several dimensions, algebra is no longer useful for providing insights. Instead, we will need <em>matrix</em> algebra. This supplement provides a brief introduction to matrix algebra to allow you to study the linear regression chapters of this text. It re-introduces basic linear regression to give you a feel for things that will be coming up in subsequent chapters when we extend basic linear regression to the multivariate case. Appendix A3 defines additional matrix concepts.</p>
<div id="basic-definitions" class="section level3 hasAnchor" number="2.11.1">
<h3><span class="header-section-number">2.11.1</span> Basic Definitions<a href="basic-linear-regression.html#basic-definitions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A <em>matrix</em> is a rectangular array of numbers arranged in rows and columns (the plural of matrix is matrices). For example, consider the income and age of 3 people.
<span class="math display">\[\begin{equation*}
\mathbf{A}=
\begin{array}{c}
Row~1 \\
Row~2 \\
Row~3
\end{array}
\overset{
\begin{array}{cc}
~~~Col~1~ &amp; Col~2
\end{array}
}{\left(
\begin{array}{cc}
6,000 &amp; 23 \\
13,000 &amp; 47 \\
11,000 &amp; 35
\end{array}
\right) }
\end{equation*}\]</span></p>
<p>Here, column 1 represents income and column 2 represents
age. Each row corresponds to an individual. For example, the first
individual is 23 years old with an income of $6,000.</p>
<p>The number of rows and columns is called the <em>dimension</em> of the
matrix. For example, the dimension of the matrix <span class="math inline">\(\mathbf{A}\)</span> above
is <span class="math inline">\(3\times 2\)</span> (read 3 “by” 2). This stands for 3 rows and 2
columns. If we were to represent the income and age of 100 people,
then the dimension of the matrix would be <span class="math inline">\(100\times 2\)</span>.</p>
<p>It is convenient to represent a matrix using the notation
<span class="math display">\[\begin{equation*}
\mathbf{A}=\left(
\begin{array}{cc}
a_{11} &amp; a_{12} \\
a_{21} &amp; a_{22} \\
a_{31} &amp; a_{31}
\end{array}
\right) .
\end{equation*}\]</span>
Here, <span class="math inline">\(a_{ij}\)</span> is the symbol for the number in the <span class="math inline">\(i\)</span>th row and
<span class="math inline">\(j\)</span>th column of <span class="math inline">\(\mathbf{A}\)</span>. In general, we work with matrices of
the form
<span class="math display">\[\begin{equation*}
\mathbf{A}=\left(
\begin{array}{cccc}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nk}
\end{array}
\right) .
\end{equation*}\]</span>
In this case, the matrix <span class="math inline">\(\mathbf{A}\)</span> has dimension <span class="math inline">\(n\times k\)</span>.</p>
<p>A <em>vector</em> is a special matrix. A <em>row vector</em> is a matrix containing only 1 row (<span class="math inline">\(k=1\)</span>). A <em>column vector</em> is a matrix containing only 1 column (<span class="math inline">\(n=1\)</span>). For example,
<span class="math display">\[\begin{equation*}
\text{column vector}\rightarrow \left(
\begin{array}{c}
2 \\
3 \\
4 \\
5 \\
6
\end{array}
\right) ~~~~\text{row vector}\rightarrow \left(
\begin{array}{ccccc}
2 &amp; 3 &amp; 4 &amp; 5 &amp; 6
\end{array}
\right) .
\end{equation*}\]</span>
Notice above that the row vector takes much less room on a printed page than the corresponding column vector. A basic operation that relates these two quantities is the <em>transpose</em>. The transpose of a matrix <span class="math inline">\(\mathbf{A}\)</span> is defined by interchanging the rows and columns and is denoted by <span class="math inline">\(\mathbf{A }^{\prime }\)</span> (or <span class="math inline">\(\mathbf{A}^{T}\)</span>). For example,</p>
<p><span class="math display">\[\begin{equation*}
\mathbf{A}=\left(
\begin{array}{cc}
6,000 &amp; 23 \\
13,000 &amp; 47 \\
11,000 &amp; 35
\end{array}
\right) ~~~\mathbf{A}^{\prime }=\left(
\begin{array}{ccc}
6,000 &amp; 13,000 &amp; 11,000 \\
23 &amp; 47 &amp; 35
\end{array}
\right) .
\end{equation*}\]</span>
Thus, if <span class="math inline">\(\mathbf{A}\)</span> has dimension <span class="math inline">\(n\times k\)</span>, then
<span class="math inline">\(\mathbf{A}^{\prime }\)</span> has dimensions <span class="math inline">\(k\times n\)</span>.</p>
</div>
<div id="some-special-matrices" class="section level3 hasAnchor" number="2.11.2">
<h3><span class="header-section-number">2.11.2</span> Some Special Matrices<a href="basic-linear-regression.html#some-special-matrices" class="anchor-section" aria-label="Anchor link to header"></a></h3>

</div>
<div id="basic-operations" class="section level3 hasAnchor" number="2.11.3">
<h3><span class="header-section-number">2.11.3</span> Basic Operations<a href="basic-linear-regression.html#basic-operations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="scalar-multiplication" class="section level3 unnumbered hasAnchor">
<h3>Scalar Multiplication<a href="basic-linear-regression.html#scalar-multiplication" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(\mathbf{A}\)</span> by a <span class="math inline">\(n\times k\)</span> matrix and let <span class="math inline">\(c\)</span> be a real number. That is, a real number is a <span class="math inline">\(1\times 1\)</span> matrix and is also called a <em>scalar</em>. Multiplying a scalar <span class="math inline">\(c\)</span> by a matrix
<span class="math inline">\(\mathbf{A}\)</span> is denoted by <span class="math inline">\(c\mathbf{A}\)</span> and defined by
<span class="math display">\[\begin{equation*}
c\mathbf{A}=\left(
\begin{array}{cccc}
ca_{11} &amp; ca_{12} &amp; \cdots &amp; ca_{1k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
ca_{n1} &amp; ca_{n2} &amp; \cdots &amp; ca_{nk}
\end{array}
\right) .
\end{equation*}\]</span>
For example, suppose that <span class="math inline">\(c=10\)</span> and
<span class="math display">\[\begin{equation*}
\mathbf{A}=\left(
\begin{array}{cc}
1 &amp; 2 \\
6 &amp; 8
\end{array}
\right) \text{\ \ \ \ then \ \ \ }\mathbf{B}=c\mathbf{A}=\left(
\begin{array}{cc}
10 &amp; 20 \\
60 &amp; 80
\end{array}
\right) .
\end{equation*}\]</span>
Note that <span class="math inline">\(c\mathbf{A}=\mathbf{A}c\)</span>.</p>
</div>
<div id="addition-and-subtraction-of-matrices" class="section level3 unnumbered hasAnchor">
<h3>Addition and Subtraction of Matrices<a href="basic-linear-regression.html#addition-and-subtraction-of-matrices" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> be matrices with dimensions
<span class="math inline">\(n\times k\)</span>. Use <span class="math inline">\(a_{ij}\)</span> and <span class="math inline">\(b_{ij}\)</span> to denote the numbers in the
<span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column of <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span>,
respectively. Then, the matrix $ =+$
is defined to be the matrix with $ (a_{ij}+b_{ij})$ in the <span class="math inline">\(i\)</span>th row
and <span class="math inline">\(j\)</span>th column. Similarly, the matrix $
=-$ is defined to be the matrix with $
(a_{ij}-b_{ij})$ in the <span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column. Symbolically, we
write this as the following.
<span class="math display">\[\begin{equation*}
\text{If \ \ \ }\mathbf{A=}\left( a_{ij}\right) _{ij}\text{ \ \ and
\ \ } \mathbf{B=}\left( b_{ij}\right) _{ij}\text{, then}
\end{equation*}\]</span>
<span class="math display">\[\begin{equation*}
\mathbf{C}=\mathbf{A}+\mathbf{B=}\left( a_{ij}+b_{ij}\right)
_{ij}\text{ \ \ and \ \ }\mathbf{C}=\mathbf{A}-\mathbf{B=}\left(
a_{ij}-b_{ij}\right) _{ij}.
\end{equation*}\]</span>
For example, consider
<span class="math display">\[\begin{equation*}
\mathbf{A}=\left(
\begin{array}{cc}
2 &amp; 5 \\
4 &amp; 1
\end{array}
\right) \text{\textbf{\ \ \ \ \ }}\mathbf{B}=\left(
\begin{array}{cc}
4 &amp; 6 \\
8 &amp; 1
\end{array}
\right).
\end{equation*}\]</span>
Then
<span class="math display">\[\begin{equation*}
\mathbf{A}+\mathbf{B}=\left(
\begin{array}{cc}
6 &amp; 11 \\
12 &amp; 2
\end{array}
\right) \text{\textbf{\ \ \ \ \ }}\mathbf{A}-\mathbf{B}=\left(
\begin{array}{cc}
-2 &amp; -1 \\
-4 &amp; 0
\end{array}
\right) .
\end{equation*}\]</span></p>
<p>***</p>
<p><strong>Basic Linear Regression Example of Addition and Subtraction</strong>. Now, recall that the basic linear regression model can
be written as <span class="math inline">\(n\)</span> equations:
<span class="math display">\[\begin{equation*}
\begin{array}{c}
y_1=\beta_0+\beta_1x_1+\varepsilon _1 \\
\vdots \\
y_n=\beta_0+\beta_1x_n+\varepsilon _n.
\end{array}
\end{equation*}\]</span>
We can define
<span class="math display">\[\begin{equation*}
\mathbf{y}=\left(
\begin{array}{c}
y_1 \\
\vdots \\
y_n
\end{array}
\right) \text{\textbf{\ \ \ \ \ }}\boldsymbol \varepsilon =\left(
\begin{array}{c}
\varepsilon _1 \\
\vdots \\
\varepsilon _n
\end{array}
\right) \text{\textbf{\ \ \ \ \ \ \ \ }and \ \ \ \
}\mathrm{E~}\mathbf{y} =\left(
\begin{array}{c}
\beta_0+\beta_1x_1 \\
\vdots \\
\beta_0+\beta_1x_n
\end{array}
\right) .
\end{equation*}\]</span>\end{equation*}
With this notation, we can express the <span class="math inline">\(n\)</span> equations more compactly
as $ .$</p>
<hr />
</div>
<div id="matrix-multiplication" class="section level3 unnumbered hasAnchor">
<h3>Matrix Multiplication<a href="basic-linear-regression.html#matrix-multiplication" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In general, if <span class="math inline">\(\mathbf{A}\)</span> is a matrix of dimension <span class="math inline">\(n\times c\)</span> and
$ $ is a matrix of dimension <span class="math inline">\(c\times k\)</span>, then
<span class="math inline">\(\mathbf{C}=\mathbf{AB }\)</span> is a matrix of dimension <span class="math inline">\(n\times k\)</span> and
is defined by
<span class="math display">\[\begin{equation*}
\mathbf{C}=\mathbf{AB=}\left( \sum_{s=1}^{c}a_{is}b_{sj}\right)
_{ij}.
\end{equation*}\]</span>
For example consider the <span class="math inline">\(2\times 2\)</span> matrices
<span class="math display">\[\begin{equation*}
\mathbf{A}=\left(
\begin{array}{cc}
2 &amp; 5 \\
4 &amp; 1
\end{array}
\right) \text{\textbf{\ \ \ \ \ }}\mathbf{B}=\left(
\begin{array}{cc}
4 &amp; 6 \\
8 &amp; 1
\end{array}
\right) .
\end{equation*}\]</span>
The matrix <span class="math inline">\(\mathbf{AB}\)</span> has dimension <span class="math inline">\(2\times 2\)</span>. To illustrate
the calculation, consider the number in the first row and second
column of $ $. By the rule presented above, with <span class="math inline">\(i=1\)</span>
and <span class="math inline">\(j=2\)</span>, the corresponding element of <span class="math inline">\(\mathbf{AB}\)</span> is $
<em>{s=1}^2a</em>{1s}b_{s2}=a_{11}b_{12}+a_{12}b_{22}=2(6)+5(1)=17$.
The other calculations are summarized as
<span class="math display">\[\begin{equation*}
\mathbf{AB}=\left(
\begin{array}{cc}
2(4)+5(8) &amp; 2(6)+5(1) \\
4(4)+1(8) &amp; 4(6)+1(1)
\end{array}
\right) =\left(
\begin{array}{cc}
48 &amp; 17 \\
24 &amp; 25
\end{array}
\right) .
\end{equation*}\]</span>
As another example, suppose
<span class="math display">\[\begin{equation*}
\mathbf{A}=\left(
\begin{array}{ccc}
1 &amp; 2 &amp; 4 \\
0 &amp; 5 &amp; 8
\end{array}
\right) \text{\textbf{\ \ \ \ \ }}\mathbf{B}=\left(
\begin{array}{c}
3 \\
5 \\
2
\end{array}
\right) .
\end{equation*}\]</span>
Because <span class="math inline">\(\mathbf{A}\)</span> has dimension <span class="math inline">\(2\times 3\)</span> and <span class="math inline">\(\mathbf{B}\)</span> has
dimension <span class="math inline">\(3\times 1\)</span>, this means that the product <span class="math inline">\(\mathbf{AB}\)</span> has
dimension <span class="math inline">\(2\times 1\)</span>.. The calculations are summarized as
<span class="math display">\[\begin{equation*}
\mathbf{AB}=\left(
\begin{array}{c}
1(3)+2(5)+4(2) \\
0(3)+5(5)+(2)
\end{array}
\right) =\left(
\begin{array}{c}
21 \\
41
\end{array}
\right) .
\end{equation*}\]</span>
For some additional examples, we have
<span class="math display">\[\begin{equation*}
\left(
\begin{array}{cc}
4 &amp; 2 \\
5 &amp; 8
\end{array}
\right) \left(
\begin{array}{c}
a_1 \\
a_2
\end{array}
\right) =\left(
\begin{array}{c}
4a_1+2a_2 \\
5a_1+8a_2
\end{array}
\right) .
\end{equation*}\]</span>
<span class="math display">\[\begin{equation*}
\left(
\begin{array}{ccc}
2 &amp; 3 &amp; 5
\end{array}
\right) \left(
\begin{array}{c}
2 \\
3 \\
5
\end{array}
\right) =2^2+3^2+5^2=38\text{\textbf{\ \ \ \ \ }}\left(
\begin{array}{c}
2 \\
3 \\
5
\end{array}
\right) \left(
\begin{array}{ccc}
2 &amp; 3 &amp; 5
\end{array}
\right) =\left(
\begin{array}{ccc}
4 &amp; 6 &amp; 10 \\
6 &amp; 9 &amp; 15 \\
10 &amp; 15 &amp; 25
\end{array}
\right) .
\end{equation*}\]</span>
In general, you see that <span class="math inline">\(\mathbf{AB\neq BA}\)</span> in matrix
multiplication, unlike multiplication of scalars (real numbers).
Further, we remark that the identity matrix serves the role of
“one” in matrix multiplication, in that <span class="math inline">\(\mathbf{AI=A}\)</span> and
<span class="math inline">\(\mathbf{IA=A}\)</span> for any matrix <span class="math inline">\(\mathbf{A}\)</span>, providing that the
dimensions are compatible to allow matrix multiplication.</p>
<p></p>
<p><strong>Basic Linear Regression Example of Matrix Multiplication</strong>. Define
<span class="math display">\[\begin{equation*}
\mathbf{X}=\left(
\begin{array}{cc}
1 &amp; x_1 \\
\vdots &amp; \vdots \\
1 &amp; x_n
\end{array}
\right) \text{ \ and\ \ }\boldsymbol \beta =\left(
\begin{array}{c}
\beta_0 \\
\beta_1
\end{array}
\right) \text{, to get  } \mathbf X \boldsymbol \beta =\left(
\text{\ }
\begin{array}{c}
\beta_0+\beta_1x_1 \\
\vdots \\
\beta_0+\beta_1x_n
\end{array}
\right) =\mathbf{\mathrm{E~}\mathbf{y.}}
\end{equation*}\]</span>
Thus, this yields the familiar matrix expression of the regression
model, $ +
.$ Other useful quantities include
<span class="math display">\[\begin{equation*}
\mathbf{y}^{\prime }\mathbf{y=}\left(
\begin{array}{ccc}
y_1 &amp; \cdots &amp; y_n
\end{array}
\right) \left(
\begin{array}{c}
y_1 \\
\vdots \\
y_n
\end{array}
\right) =y_1^2+\cdots +y_n^2=\sum_{i=1}^{n}y_i^2,
\end{equation*}\]</span>
<span class="math display">\[\begin{equation*}
\mathbf{X}^{\prime }\mathbf{y=}\left(
\begin{array}{ccc}
1 &amp; \cdots &amp; 1 \\
x_1 &amp; \cdots &amp; x_n
\end{array}
\right) \left(
\begin{array}{c}
y_1 \\
\vdots \\
y_n
\end{array}
\right) =\left(
\begin{array}{c}
\sum_{i=1}^{n}y_i \\
\sum_{i=1}^{n}x_iy_i
\end{array}
\right)
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
\mathbf{X}^{\prime }\mathbf{X=}\left(
\begin{array}{ccc}
1 &amp; \cdots &amp; 1 \\
x_1 &amp; \cdots &amp; x_n
\end{array}
\right) \left(
\begin{array}{cc}
1 &amp; x_1 \\
\vdots &amp; \vdots \\
1 &amp; x_n
\end{array}
\right) =\left(
\begin{array}{cc}
n &amp; \sum_{i=1}^{n}x_i \\
\sum_{i=1}^{n}x_i &amp; \sum_{i=1}^{n} x_i^2
\end{array}
\right) .
\end{equation*}\]</span>
Note that <span class="math inline">\(\mathbf{X}^{\prime }\mathbf{X}\)</span> is a symmetric matrix.</p>
<hr />
</div>
<div id="matrix-inverses" class="section level3 unnumbered hasAnchor">
<h3>Matrix Inverses<a href="basic-linear-regression.html#matrix-inverses" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p></p>
<p>In matrix algebra, there is no concept of “division.” Instead, we extend the concept of “reciprocals” of real numbers. To begin, suppose that <span class="math inline">\(\mathbf{A}\)</span> is a square matrix of dimension <span class="math inline">\(k \times k\)</span> and let <span class="math inline">\(\mathbf{I}\)</span> be the <span class="math inline">\(k\times k\)</span> identity matrix. If there
exists a <span class="math inline">\(k \times k\)</span> matrix <span class="math inline">\(\mathbf{B}\)</span> such that <span class="math inline">\(\mathbf{AB}=\mathbf{I=BA}\)</span>, then <span class="math inline">\(\mathbf{B}\)</span> is called the <em>inverse</em>of <span class="math inline">\(\mathbf{A}\)</span> and is written
<span class="math display">\[\begin{equation*}
\mathbf{B}=\mathbf{A}^{-1}.
\end{equation*}\]</span>
Now, not all square matrices have inverses. Further, even when an
inverse exists, it may not be easy to compute by hand. One exception
to this rule are diagonal matrices. Suppose that <span class="math inline">\(\mathbf{A}\)</span> is
diagonal matrix of the form
<span class="math display">\[\begin{equation*}
\mathbf{A=}\left(
\begin{array}{ccc}
a_{11} &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; a_{kk}
\end{array}
\right). \text{ \ \ Then \ \ }\mathbf{A}^{-1}\mathbf{=}\left(
\begin{array}{ccc}
\frac{1}{a_{11}} &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \frac{1}{a_{kk}}
\end{array}
\right).
\end{equation*}\]</span>
For example,
<span class="math display">\[\begin{equation*}
\begin{array}{cccc}
\left(
\begin{array}{cc}
2 &amp; 0 \\
0 &amp; -19
\end{array}
\right) &amp; \left(
\begin{array}{cc}
\frac{1}{2} &amp; 0 \\
0 &amp; -\frac{1}{19}
\end{array}
\right) &amp; = &amp; \left(
\begin{array}{cc}
1 &amp; 0 \\
0 &amp; 1
\end{array}
\right) \\
\mathbf{A} &amp; \mathbf{A}^{-1} &amp; = &amp; \mathbf{I}
\end{array}
.
\end{equation*}\]</span>
In the case of a matrix of dimension <span class="math inline">\(2\times 2\)</span>, the inversion
procedure can be accomplished by hand easily even when the matrix is
not diagonal. In the <span class="math inline">\(2\times 2\)</span> case, we suppose that if
<span class="math display">\[\begin{equation*}
\mathbf{A=}\left(
\begin{array}{cc}
a &amp; b \\
c &amp; d
\end{array}
\right), \text{ \ \ then \ \
}\mathbf{A}^{-1}\mathbf{=}\frac{1}{ad-bc}\left(
\begin{array}{cc}
d &amp; -b \\
-c &amp; a
\end{array}
\right) \text{.}
\end{equation*}\]</span>}
\end{equation<em>}
Thus, for example, if
<span class="math display">\[\begin{equation*}
\mathbf{A=}\left(
\begin{array}{cc}
2 &amp; 2 \\
3 &amp; 4
\end{array}
\right) \text{ \ \ then \ \
}\mathbf{A}^{-1}\mathbf{=}\frac{1}{2(4)-2(3)} \left(
\begin{array}{cc}
4 &amp; -2 \\
-3 &amp; 2
\end{array}
\right) =\left(
\begin{array}{cc}
2 &amp; -1 \\
-3/2 &amp; 1
\end{array}
\right) \text{.}
\end{equation*}\]</span>}
\end{equation</em>}
As a check, we have
<span class="math display">\[\begin{equation*}
\mathbf{A\mathbf{A}^{-1}=}\left(
\begin{array}{cc}
2 &amp; 2 \\
3 &amp; 4
\end{array}
\right) \left(
\begin{array}{cc}
2 &amp; -1 \\
-3/2 &amp; 1
\end{array}
\right) =\left(
\begin{array}{cc}
2(2)-2(3/2) &amp; 2(-1)+2(1) \\
3(2)-4(3/2) &amp; 3(-1)+4(1)
\end{array}
\right) =\left(
\begin{array}{cc}
1 &amp; 0 \\
0 &amp; 1
\end{array}
\right) =\mathbf{I}\text{.}
\end{equation*}\]</span></p>
<hr />
<p><strong>Basic Linear Regression Example of Matrix Inverses.</strong> With
<span class="math display">\[\begin{equation*}
\mathbf{X}^{\prime }\mathbf{X=}\left(
\begin{array}{cc}
n &amp; \sum\limits_{i=1}^{n}x_i \\
\sum\limits_{i=1}^{n}x_i &amp; \sum\limits_{i=1}^{n}x_i^2
\end{array}
\right),
\end{equation*}\]</span>
we have
<span class="math display">\[\begin{equation*}
\left( \mathbf{X}^{\prime }\mathbf{X}\right)
^{-1}\mathbf{=}\frac{1}{n\sum_{i=1}^{n}x_i^2-\left(
\sum_{i=1}^{n}x_i\right) ^2}\left(
\begin{array}{cc}
\sum\limits_{i=1}^{n}x_i^2 &amp; -\sum\limits_{i=1}^{n}x_i \\
-\sum\limits_{i=1}^{n}x_i &amp; n
\end{array}
\right).
\end{equation*}\]</span>
To simplify this expression, recall that <span class="math inline">\(\overline{x}=n^{-1} \sum_{i=1}^{n}x_i\)</span>. Thus,
<span class="math display">\[\begin{equation}\label{E2:XPXInv}
\left( \mathbf{X}^{\prime }\mathbf{X}\right)
^{-1}\mathbf{=}\frac{1}{ \sum_{i=1}^{n}x_i^2-n\overline{x}^2}\left(
\begin{array}{cc}
n^{-1}\sum\limits_{i=1}^{n}x_i^2 &amp; -\overline{x} \\
-\overline{x} &amp; 1
\end{array}
\right) .
\end{equation}\]</span></p>
<p>Section 3.1 will discuss the relation <span class="math inline">\(\mathbf{b}=\left( \mathbf{X}^{\prime}\mathbf{X}\right)^{-1} \mathbf{X}^{\prime}\mathbf{y}\)</span>. To illustrate the calculation, we
have
<span class="math display">\[\begin{eqnarray*}
\mathbf{b} &amp;=&amp;\left( \mathbf{X}^{\prime }\mathbf{X}\right)
^{-1}\mathbf{X} ^{\prime
}\mathbf{y=}\frac{1}{\sum_{i=1}^{n}x_i^2-n\overline{x}^2} \left(
\begin{array}{cc}
n^{-1}\sum\limits_{i=1}^{n}x_i^2 &amp; -\overline{x} \\
-\overline{x} &amp; 1
\end{array}
\right) \left(
\begin{array}{c}
\sum\limits_{i=1}^{n}y_i \\
\sum\limits_{i=1}^{n}x_iy_i
\end{array}
\right) \\
&amp;=&amp;\frac{1}{\sum_{i=1}^{n}x_i^2-n\overline{x}^2}\left(
\begin{array}{c}
\sum\limits_{i=1}^{n}\left( \overline{y}x_i^2-\overline{x}
x_iy_i\right) \\
\sum\limits_{i=1}^{n}x_iy_i-n\overline{x}\overline{y}
\end{array}
\right) =\left(
\begin{array}{c}
b_0 \\
b_1
\end{array}
\right) .
\end{eqnarray*}\]</span>
From this expression, we may see
<span class="math display">\[\begin{equation*}
b_1=\frac{\sum\limits_{i=1}^{n}x_iy_i-n\overline{x}\overline{y}}{
\sum\limits_{i=1}^{n}x_i^2-n\overline{x}^2}
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
b_0=\frac{\overline{y}\sum\limits_{i=1}^{n}x_i^2-\overline{x}
\sum\limits_{i=1}^{n}x_iy_i}{\sum\limits_{i=1}^{n}x_i^2-n\overline{x}
^2}=\frac{\overline{y}\left(
\sum\limits_{i=1}^{n}x_i^2-n\overline{x} ^2\right)
-\overline{x}\left( \sum\limits_{i=1}^{n}x_i y_i - n\overline{x}
\overline{y}\right) }{\sum\limits_{i=1}^{n}x_i^2-n\overline{x}^2}=
\overline{y}-b_1\overline{x}.
\end{equation*}\]</span>
These are the usual expressions for the slope <span class="math inline">\(b_1\)</span> (Exercise 2A.8)
and intercept <span class="math inline">\(b_0\)</span>.</p>
<hr />
</div>
<div id="random-matrices" class="section level3 hasAnchor" number="2.11.4">
<h3><span class="header-section-number">2.11.4</span> Random Matrices<a href="basic-linear-regression.html#random-matrices" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Expectations.</strong> Consider a matrix of random variables
<span class="math display">\[\begin{equation*}
\mathbf{U=}\left(
\begin{array}{cccc}
u_{11} &amp;  u_{12}  &amp; \cdots &amp; u_{1c}  \\
u_{21} &amp;  u_{22}  &amp; \cdots &amp; u_{2c}  \\
\vdots &amp;  \vdots  &amp; \ddots &amp; \vdots  \\
u_{n1} &amp;  u_{n2}  &amp; \cdots &amp; u_{nc}
\end{array}
\right).
\end{equation*}\]</span>
When we write the expectation of a matrix, this is short-hand for
the matrix of expectations. Specifically, suppose that the joint
probability function of <span class="math inline">\({u_{11}, u_{12}, ..., u_{1c}, ..., u_{n1},  ..., u_{nc}}\)</span> is available to define the expectation operator.
Then we define
<span class="math display">\[\begin{equation*}
\mathrm{E} ~ \mathbf{U} = \left(
\begin{array}{cccc}
\mathrm{E }u_{11} &amp;  \mathrm{E }u_{12}  &amp; \cdots &amp; \mathrm{E }u_{1c}  \\
\mathrm{E }u_{21} &amp;  \mathrm{E }u_{22}  &amp; \cdots &amp; \mathrm{E }u_{2c}  \\
\vdots &amp;  \vdots  &amp; \ddots &amp; \vdots  \\
\mathrm{E }u_{n1} &amp;  \mathrm{E }u_{n2}  &amp; \cdots &amp; \mathrm{E }u_{nc}
\end{array}
\right).
\end{equation*}\]</span>
As an important special case, consider the joint probability
function for the random variables <span class="math inline">\(y_1, \ldots, y_n\)</span> and the
corresponding expectations operator. Then
<span class="math display">\[\begin{equation*}
\mathrm{E}~ \mathbf{y=} \mathrm{E } \left(
\begin{array}{cccc}
y_1   \\
\vdots \\
y_n
\end{array}
\right) =  \left(
\begin{array}{cccc}
\mathrm{E }y_1  \\
\vdots   \\
\mathrm{E }y_n
\end{array}
\right).
\end{equation*}\]</span>
By the linearity of expectations, for a non-random matrix <strong>A</strong>
and vector , we have E ( + ) =
E .</p>
<p><strong>Variances.</strong> We can also work with second moments of random
vectors. The variance of a vector of random variables is called the
<em>variance-covariance matrix</em>. It is defined by
<span class="math display">\[\begin{equation}\label{E2:MatrixVar}
\mathrm{Var} ~ \mathbf{y}  =  \mathrm{E} ( (\mathbf{y} - \mathrm{E}
\mathbf{y})(\mathbf{y} - \mathrm{E} \mathbf{y})^{\prime} ).
\end{equation}\]</span> That is, we can
express
<span class="math display">\[\begin{equation*}
\mathrm{Var}~\mathbf{y=} \mathrm{E } \left( \left( \begin{array}{c}
y_1 -\mathrm{E } y_1    \\
\vdots \\
y_n -\mathrm{E } y_n
\end{array}\right)
\left(\begin{array}{ccc} y_1 - \mathrm{E } y_1   &amp; \cdots &amp; y_n -
\mathrm{E } y_n
\end{array}\right) \right)
\end{equation*}\]</span>
<span class="math display">\[\begin{equation*} = \left( \begin{array}{cccc}
\mathrm{Var}~y_1 &amp; \mathrm{Cov}(y_1, y_2) &amp; \cdots &amp;\mathrm{Cov}(y_1, y_n)   \\
\mathrm{Cov}(y_2, y_1) &amp; \mathrm{Var}~y_2 &amp; \cdots &amp; \mathrm{Cov}(y_2, y_n)   \\
\vdots  &amp; \vdots &amp; \ddots &amp; \vdots\\
\mathrm{Cov}(y_n, y_1) &amp; \mathrm{Cov}(y_n, y_2) &amp; \cdots &amp; \mathrm{Var}~y_n   \\
\end{array}\right),
\end{equation*}\]</span>
because <span class="math inline">\(\mathrm{E} ( (y_i - \mathrm{E} y_i)(y_j - \mathrm{E} y_j) ) = \mathrm{Cov}(y_i, y_j)\)</span> for <span class="math inline">\(i \neq j\)</span> and <span class="math inline">\(\mathrm{Cov}(y_i, y_i) = \mathrm{Var}~y_i\)</span>.</p>
<p>In the case that <span class="math inline">\(y_1, \ldots, y_n\)</span> are mutually uncorrelated, we
have that <span class="math inline">\(\mathrm{Cov}(y_i, y_j)=0\)</span> for <span class="math inline">\(i \neq j\)</span> and thus
<span class="math display">\[\begin{equation*}
\mathrm{Var}~\mathbf{y=} \left( \begin{array}{cccc}
\mathrm{Var}~y_1 &amp; 0 &amp; \cdots &amp; 0   \\
0 &amp; \mathrm{Var}~y_2 &amp; \cdots &amp; 0   \\
\vdots  &amp; \vdots &amp; \ddots &amp; \vdots\\
0 &amp; 0 &amp; \cdots &amp; \mathrm{Var}~y_n   \\
\end{array}\right).
\end{equation*}\]</span>
Further, if the variances are identical so that <span class="math inline">\(\mathrm{Var}~y_i=\sigma ^2\)</span>, then we can write <span class="math inline">\(\mathrm{Var} ~\mathbf{y} = \sigma ^2 \mathbf{I}\)</span>, where <strong>I</strong> is the <span class="math inline">\(n \times n\)</span> identity matrix. For example, if <span class="math inline">\(y_1, \ldots, y_n\)</span> are i.i.d., then <span class="math inline">\(\mathrm{Var} ~\mathbf{y} = \sigma ^2 \mathbf{I}\)</span>.</p>
<p>From equation (<span class="math inline">\(\ref{E2:MatrixVar}\)</span>), it can be shown that
<span class="math display">\[\begin{equation}\label{E2:MatrixVarCalc}
\mathrm{Var}\left( \mathbf{Ay +B} \right) = \mathrm{Var}\left(
\mathbf{Ay} \right) = \mathbf{A} \left( \mathrm{Var}~\mathbf{y}
\right) \mathbf{A}^{\prime}.
\end{equation}\]</span>
For example, if <span class="math inline">\(\mathbf{A} = (a_1, a_2, \ldots,a_n)= \mathbf{a}^{\prime}\)</span> and <strong>B = 0</strong>, then equation
(<span class="math inline">\(\ref{E2:MatrixVarCalc}\)</span>) reduces to
<span class="math display">\[\begin{equation*}
\mathrm{Var}\left( \sum_{i=1}^n a_i y_i \right) = \mathrm{Var}
\left( \mathbf{a^{\prime} y}  \right) = \mathbf{a^{\prime}} \left(
\mathrm{Var} ~\mathbf{y} \right) \mathbf{a} = (a_1, a_2, \ldots,a_n)
\left( \mathrm{Var} ~\mathbf{y} \right) \left(\begin{array}{c} a_1 \\
\vdots \\ a_n \end{array}\right)
\end{equation*}\]</span>
<span class="math display">\[\begin{equation*}
= \sum_{i=1}^n a_i^2 \mathrm{Var} ~y_i ~+~2 \sum_{i=2}^n
\sum_{j=1}^{i-1} a_i a_j \mathrm{Cov}(y_i, y_j).
\end{equation*}\]</span></p>
<p><strong>Definition - Multivariate Normal Distribution.</strong> A vector of random variables <span class="math inline">\(\mathbf{y} = \left(y_1, \ldots, y_n \right)^{\prime}\)</span> is said to be <em>multivariate normal</em> if all linear combinations of the form <span class="math inline">\(\sum_{i=1}^n a_i y_i\)</span> are normally distributed. In this case, we write <span class="math inline">\(\mathbf{y}\sim N (\mathbf{\boldsymbol \mu}, \mathbf{\Sigma} )\)</span>, where $ = ~ $ is the expected value of <strong>y</strong> and <span class="math inline">\(\mathbf{\Sigma}= \mathrm{Var}~\mathbf{y}\)</span> is the variance-covariance matrix of <strong>y</strong>. From the definition, we have that <span class="math inline">\(\mathbf{y}\sim N (\mathbf{\boldsymbol \mu}, \mathbf{\Sigma} )\)</span> implies that <span class="math inline">\(\mathbf{a^{\prime}y}\sim N (\mathbf{a^{\prime} \boldsymbol \mu}, \mathbf{a^{\prime}\Sigma a} )\)</span>. Thus, if <span class="math inline">\(y_i\)</span> are i.i.d., then <span class="math inline">\(\sum_{i=1}^n a_i y_i\)</span> is distributed normally with mean $_{i=1}^n a_i $ and variance <span class="math inline">\(\sigma ^2 \sum_{i=1}^n a_i ^2\)</span>.
</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chap-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliography.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
