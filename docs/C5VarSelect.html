<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Selección de Variables | Modelado de Regresión con Aplicaciones Actuariales y Financieras</title>
  <meta name="description" content="Spanish Translation of ‘Regression Modeling with Actuarial and Financial Applications’" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Selección de Variables | Modelado de Regresión con Aplicaciones Actuariales y Financieras" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Spanish Translation of ‘Regression Modeling with Actuarial and Financial Applications’" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Selección de Variables | Modelado de Regresión con Aplicaciones Actuariales y Financieras" />
  
  <meta name="twitter:description" content="Spanish Translation of ‘Regression Modeling with Actuarial and Financial Applications’" />
  

<meta name="author" content="Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="C4MLRANOVA.html"/>
<link rel="next" href="interpretación-de-resultados-de-regresión.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript"  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleSolution(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}      
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>



<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Regression Modeling With Actuarial and Financial Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prólogo"><i class="fa fa-check"></i>Prólogo</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#para-quién-es-este-libro"><i class="fa fa-check"></i>¿Para Quién Es Este Libro?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#de-qué-trata-este-libro"><i class="fa fa-check"></i>¿De Qué Trata Este Libro?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#cómo-transmite-este-libro-su-mensaje"><i class="fa fa-check"></i>¿Cómo Transmite Este Libro Su Mensaje?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#agradecimientos"><i class="fa fa-check"></i>Agradecimientos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedicación"><i class="fa fa-check"></i>Dedicación</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="translation.html"><a href="translation.html"><i class="fa fa-check"></i>Translation</a></li>
<li class="chapter" data-level="1" data-path="regresión-y-la-distribución-normal.html"><a href="regresión-y-la-distribución-normal.html"><i class="fa fa-check"></i><b>1</b> Regresión y la Distribución Normal</a>
<ul>
<li class="chapter" data-level="1.1" data-path="regresión-y-la-distribución-normal.html"><a href="regresión-y-la-distribución-normal.html#Sec11"><i class="fa fa-check"></i><b>1.1</b> ¿Qué es el Análisis de Regresión?</a></li>
<li class="chapter" data-level="1.2" data-path="regresión-y-la-distribución-normal.html"><a href="regresión-y-la-distribución-normal.html#Sec12"><i class="fa fa-check"></i><b>1.2</b> Ajuste de Datos a una Distribución Normal</a></li>
<li class="chapter" data-level="1.3" data-path="regresión-y-la-distribución-normal.html"><a href="regresión-y-la-distribución-normal.html#Sec13"><i class="fa fa-check"></i><b>1.3</b> Transformaciones de Potencia</a></li>
<li class="chapter" data-level="1.4" data-path="regresión-y-la-distribución-normal.html"><a href="regresión-y-la-distribución-normal.html#Sec14"><i class="fa fa-check"></i><b>1.4</b> Muestreo y el Papel de la Normalidad</a></li>
<li class="chapter" data-level="1.5" data-path="regresión-y-la-distribución-normal.html"><a href="regresión-y-la-distribución-normal.html#Sec15"><i class="fa fa-check"></i><b>1.5</b> Regresión y Diseños de Muestreo</a></li>
<li class="chapter" data-level="1.6" data-path="regresión-y-la-distribución-normal.html"><a href="regresión-y-la-distribución-normal.html#Sec16"><i class="fa fa-check"></i><b>1.6</b> Aplicaciones Actuariales de la Regresión</a></li>
<li class="chapter" data-level="1.7" data-path="regresión-y-la-distribución-normal.html"><a href="regresión-y-la-distribución-normal.html#Sec17"><i class="fa fa-check"></i><b>1.7</b> Lecturas Adicionales y Referencias</a></li>
<li class="chapter" data-level="1.8" data-path="regresión-y-la-distribución-normal.html"><a href="regresión-y-la-distribución-normal.html#Sec18"><i class="fa fa-check"></i><b>1.8</b> Ejercicios</a></li>
<li class="chapter" data-level="1.9" data-path="regresión-y-la-distribución-normal.html"><a href="regresión-y-la-distribución-normal.html#Sec19"><i class="fa fa-check"></i><b>1.9</b> Suplemento Técnico - Teorema del Límite Central</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="C2BasicLR.html"><a href="C2BasicLR.html"><i class="fa fa-check"></i><b>2</b> Regresión Lineal Básica</a>
<ul>
<li class="chapter" data-level="2.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec21"><i class="fa fa-check"></i><b>2.1</b> Correlaciones y Mínimos Cuadrados</a></li>
<li class="chapter" data-level="2.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec22"><i class="fa fa-check"></i><b>2.2</b> Modelo Básico de Regresión Lineal</a></li>
<li class="chapter" data-level="2.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec23"><i class="fa fa-check"></i><b>2.3</b> ¿Es Útil el Modelo? Algunas Medidas de Resumen Básicas</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec231"><i class="fa fa-check"></i><b>2.3.1</b> Particionando la Variabilidad</a></li>
<li class="chapter" data-level="2.3.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec232"><i class="fa fa-check"></i><b>2.3.2</b> El Tamaño de una Desviación Típica: <em>s</em></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec24"><i class="fa fa-check"></i><b>2.4</b> Propiedades de los Estimadores del Coeficiente de Regresión</a></li>
<li class="chapter" data-level="2.5" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec25"><i class="fa fa-check"></i><b>2.5</b> Inferencia Estadística</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec251"><i class="fa fa-check"></i><b>2.5.1</b> ¿Es Importante la Variable Explicativa?: La Prueba <em>t</em></a></li>
<li class="chapter" data-level="2.5.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec252"><i class="fa fa-check"></i><b>2.5.2</b> Intervalos de Confianza</a></li>
<li class="chapter" data-level="2.5.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec253"><i class="fa fa-check"></i><b>2.5.3</b> Intervalos de Predicción</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec26"><i class="fa fa-check"></i><b>2.6</b> Construyendo un Mejor Modelo: Análisis de Residuos</a></li>
<li class="chapter" data-level="2.7" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec27"><i class="fa fa-check"></i><b>2.7</b> Aplicación: Modelo de Valoración de Activos Financieros</a></li>
<li class="chapter" data-level="2.8" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec28"><i class="fa fa-check"></i><b>2.8</b> Salida Computacional Ilustrativa de Regresión</a></li>
<li class="chapter" data-level="2.9" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec29"><i class="fa fa-check"></i><b>2.9</b> Lecturas Adicionales y Referencias</a></li>
<li class="chapter" data-level="2.10" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec210"><i class="fa fa-check"></i><b>2.10</b> Ejercicios</a></li>
<li class="chapter" data-level="2.11" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec211"><i class="fa fa-check"></i><b>2.11</b> Suplemento Técnico - Elementos del Álgebra de Matrices</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2111"><i class="fa fa-check"></i><b>2.11.1</b> Definiciones Básicas</a></li>
<li class="chapter" data-level="2.11.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2112"><i class="fa fa-check"></i><b>2.11.2</b> Algunas Matrices Especiales</a></li>
<li class="chapter" data-level="2.11.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2113"><i class="fa fa-check"></i><b>2.11.3</b> Operaciones Básicas</a></li>
<li class="chapter" data-level="2.11.4" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2114"><i class="fa fa-check"></i><b>2.11.4</b> Matrices Aleatorias</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html"><i class="fa fa-check"></i><b>3</b> Regresión Lineal Múltiple - I</a>
<ul>
<li class="chapter" data-level="3.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec31"><i class="fa fa-check"></i><b>3.1</b> Método de Mínimos Cuadrados</a></li>
<li class="chapter" data-level="3.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec32"><i class="fa fa-check"></i><b>3.2</b> Modelo de Regresión Lineal y Propiedades de los Estimadores</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec321"><i class="fa fa-check"></i><b>3.2.1</b> Función de Regresión</a></li>
<li class="chapter" data-level="3.2.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec322"><i class="fa fa-check"></i><b>3.2.2</b> Interpretación del Coeficiente de Regresión</a></li>
<li class="chapter" data-level="3.2.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec323"><i class="fa fa-check"></i><b>3.2.3</b> Suposiciones del Modelo</a></li>
<li class="chapter" data-level="3.2.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec324"><i class="fa fa-check"></i><b>3.2.4</b> Propiedades de los Estimadores de los Coeficientes de Regresión</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec33"><i class="fa fa-check"></i><b>3.3</b> Estimación y Bondad de Ajuste</a></li>
<li class="chapter" data-level="3.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec34"><i class="fa fa-check"></i><b>3.4</b> Inferencia Estadística para un Coeficiente Único</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec341"><i class="fa fa-check"></i><b>3.4.1</b> La Prueba <em>t</em></a></li>
<li class="chapter" data-level="3.4.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec342"><i class="fa fa-check"></i><b>3.4.2</b> Intervalos de Confianza</a></li>
<li class="chapter" data-level="3.4.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec343"><i class="fa fa-check"></i><b>3.4.3</b> Gráficos de Variables Añadidas</a></li>
<li class="chapter" data-level="3.4.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec344"><i class="fa fa-check"></i><b>3.4.4</b> Coeficientes de Correlación Parcial</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec35"><i class="fa fa-check"></i><b>3.5</b> Algunas Variables Explicativas Especiales</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec351"><i class="fa fa-check"></i><b>3.5.1</b> Variables Binarias</a></li>
<li class="chapter" data-level="3.5.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec352"><i class="fa fa-check"></i><b>3.5.2</b> Transformación de Variables Explicativas</a></li>
<li class="chapter" data-level="3.5.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec353"><i class="fa fa-check"></i><b>3.5.3</b> Términos de Interacción</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec36"><i class="fa fa-check"></i><b>3.6</b> Lectura Adicional y Referencias</a></li>
<li class="chapter" data-level="3.7" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec37"><i class="fa fa-check"></i><b>3.7</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html"><i class="fa fa-check"></i><b>4</b> Regresión Lineal Múltiple - II</a>
<ul>
<li class="chapter" data-level="4.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec41"><i class="fa fa-check"></i><b>4.1</b> El Papel de las Variables Binarias</a></li>
<li class="chapter" data-level="4.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec42"><i class="fa fa-check"></i><b>4.2</b> Inferencia Estadística para Varios Coeficientes</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec421"><i class="fa fa-check"></i><b>4.2.1</b> Conjuntos de Coeficientes de Regresión</a></li>
<li class="chapter" data-level="4.2.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec422"><i class="fa fa-check"></i><b>4.2.2</b> La Hipótesis Lineal General</a></li>
<li class="chapter" data-level="4.2.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec423"><i class="fa fa-check"></i><b>4.2.3</b> Estimando y Prediciendo Varios Coeficientes</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec43"><i class="fa fa-check"></i><b>4.3</b> Modelo ANOVA de Un Factor</a></li>
<li class="chapter" data-level="4.4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec44"><i class="fa fa-check"></i><b>4.4</b> Combinando Variables Explicativas Categóricas y Continuas</a></li>
<li class="chapter" data-level="4.5" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec45"><i class="fa fa-check"></i><b>4.5</b> Lecturas Adicionales y Referencias</a></li>
<li class="chapter" data-level="4.6" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec46"><i class="fa fa-check"></i><b>4.6</b> Ejercicios</a></li>
<li class="chapter" data-level="4.7" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec47"><i class="fa fa-check"></i><b>4.7</b> Suplemento Técnico - Expresiones Matriciales</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec471"><i class="fa fa-check"></i><b>4.7.1</b> Expresión de Modelos con Variables Categóricas en Forma Matricial</a></li>
<li class="chapter" data-level="4.7.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec472"><i class="fa fa-check"></i><b>4.7.2</b> Cálculo Recursivo de Mínimos Cuadrados</a></li>
<li class="chapter" data-level="4.7.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec473"><i class="fa fa-check"></i><b>4.7.3</b> Modelo Lineal General</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="C5VarSelect.html"><a href="C5VarSelect.html"><i class="fa fa-check"></i><b>5</b> Selección de Variables</a>
<ul>
<li class="chapter" data-level="5.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec51"><i class="fa fa-check"></i><b>5.1</b> Un Enfoque Iterativo para el Análisis de Datos y Modelado</a></li>
<li class="chapter" data-level="5.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec52"><i class="fa fa-check"></i><b>5.2</b> Procedimientos Automáticos de Selección de Variables</a></li>
<li class="chapter" data-level="5.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec53"><i class="fa fa-check"></i><b>5.3</b> Análisis de Residuales</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec531"><i class="fa fa-check"></i><b>5.3.1</b> Residuales</a></li>
<li class="chapter" data-level="5.3.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec532"><i class="fa fa-check"></i><b>5.3.2</b> Uso de los Residuales para Identificar Valores Atípicos</a></li>
<li class="chapter" data-level="5.3.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec533"><i class="fa fa-check"></i><b>5.3.3</b> Uso de los Residuales para Seleccionar Variables Explicativas</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec54"><i class="fa fa-check"></i><b>5.4</b> Puntos Influyentes</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec541"><i class="fa fa-check"></i><b>5.4.1</b> Apalancamiento</a></li>
<li class="chapter" data-level="5.4.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec542"><i class="fa fa-check"></i><b>5.4.2</b> Distancia de Cook</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec55"><i class="fa fa-check"></i><b>5.5</b> Colinealidad</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec551"><i class="fa fa-check"></i><b>5.5.1</b> ¿Qué es la Colinealidad?</a></li>
<li class="chapter" data-level="5.5.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec552"><i class="fa fa-check"></i><b>5.5.2</b> Factores de Inflación de Varianza</a></li>
<li class="chapter" data-level="5.5.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec553"><i class="fa fa-check"></i><b>5.5.3</b> Colinealidad e Influencia</a></li>
<li class="chapter" data-level="5.5.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec554"><i class="fa fa-check"></i><b>5.5.4</b> Variables Suprensoras</a></li>
<li class="chapter" data-level="5.5.5" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec555"><i class="fa fa-check"></i><b>5.5.5</b> Variables Ortogonales</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec56"><i class="fa fa-check"></i><b>5.6</b> Criterios de Selección</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec561"><i class="fa fa-check"></i><b>5.6.1</b> Bondad de Ajuste</a></li>
<li class="chapter" data-level="5.6.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec562"><i class="fa fa-check"></i><b>5.6.2</b> Validación del Modelo</a></li>
<li class="chapter" data-level="5.6.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec563"><i class="fa fa-check"></i><b>5.6.3</b> Validación Cruzada</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec57"><i class="fa fa-check"></i><b>5.7</b> Heterocedasticidad</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec571"><i class="fa fa-check"></i><b>5.7.1</b> Detección de Heterocedasticidad</a></li>
<li class="chapter" data-level="5.7.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec572"><i class="fa fa-check"></i><b>5.7.2</b> Errores Estándar Consistentes con Heterocedasticidad</a></li>
<li class="chapter" data-level="5.7.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec573"><i class="fa fa-check"></i><b>5.7.3</b> Mínimos Cuadrados Ponderados</a></li>
<li class="chapter" data-level="5.7.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec574"><i class="fa fa-check"></i><b>5.7.4</b> Transformaciones</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec58"><i class="fa fa-check"></i><b>5.8</b> Lectura Adicional y Referencias</a></li>
<li class="chapter" data-level="5.9" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec59"><i class="fa fa-check"></i><b>5.9</b> Ejercicios</a></li>
<li class="chapter" data-level="5.10" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec510"><i class="fa fa-check"></i><b>5.10</b> Suplementos Técnicos para el Capítulo 5</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5101"><i class="fa fa-check"></i><b>5.10.1</b> Matriz de Proyección</a></li>
<li class="chapter" data-level="5.10.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5102"><i class="fa fa-check"></i><b>5.10.2</b> Estadísticas Leave-One-Out</a></li>
<li class="chapter" data-level="5.10.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5103"><i class="fa fa-check"></i><b>5.10.3</b> Omisión de Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html"><i class="fa fa-check"></i><b>6</b> Interpretación de Resultados de Regresión</a>
<ul>
<li class="chapter" data-level="6.1" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec61"><i class="fa fa-check"></i><b>6.1</b> Lo que nos dice el proceso de modelado</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec611"><i class="fa fa-check"></i><b>6.1.1</b> Interpretación de efectos individuales</a></li>
<li class="chapter" data-level="6.1.2" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec612"><i class="fa fa-check"></i><b>6.1.2</b> Otras Interpretaciones</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec62"><i class="fa fa-check"></i><b>6.2</b> La Importancia de la Selección de Variables</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec621"><i class="fa fa-check"></i><b>6.2.1</b> Sobreajuste del Modelo</a></li>
<li class="chapter" data-level="6.2.2" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec622"><i class="fa fa-check"></i><b>6.2.2</b> Subajuste del Modelo</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec63"><i class="fa fa-check"></i><b>6.3</b> La Importancia de la Recolección de Datos</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec631"><i class="fa fa-check"></i><b>6.3.1</b> Error en el Marco Muestral y Selección Adversa</a></li>
<li class="chapter" data-level="6.3.2" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec632"><i class="fa fa-check"></i><b>6.3.2</b> Regiones de Muestreo Limitadas</a></li>
<li class="chapter" data-level="6.3.3" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec633"><i class="fa fa-check"></i><b>6.3.3</b> Variables Dependientes Limitadas, Censura y Truncamiento</a></li>
<li class="chapter" data-level="6.3.4" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec634"><i class="fa fa-check"></i><b>6.3.4</b> Variables Omitidas y Endógenas</a></li>
<li class="chapter" data-level="6.3.5" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec635"><i class="fa fa-check"></i><b>6.3.5</b> Datos Faltantes</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec64"><i class="fa fa-check"></i><b>6.4</b> Modelos de Datos Faltantes</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec641"><i class="fa fa-check"></i><b>6.4.1</b> Faltante al Azar</a></li>
<li class="chapter" data-level="6.4.2" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec642"><i class="fa fa-check"></i><b>6.4.2</b> Datos Faltantes No Ignorables</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec65"><i class="fa fa-check"></i><b>6.5</b> Aplicación: Eficiencia en el Costo de los Gestores de Riesgos</a></li>
<li class="chapter" data-level="6.6" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec66"><i class="fa fa-check"></i><b>6.6</b> Lecturas Adicionales y Referencias</a></li>
<li class="chapter" data-level="6.7" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec67"><i class="fa fa-check"></i><b>6.7</b> Ejercicios</a></li>
<li class="chapter" data-level="6.8" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec68"><i class="fa fa-check"></i><b>6.8</b> Suplementos Técnicos para el Capítulo 6</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="interpretación-de-resultados-de-regresión.html"><a href="interpretación-de-resultados-de-regresión.html#Sec681"><i class="fa fa-check"></i><b>6.8.1</b> Efectos de la Especificación Incorrecta del Modelo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap-7.html"><a href="chap-7.html"><i class="fa fa-check"></i><b>7</b> Chap 7</a></li>
<li class="chapter" data-level="8" data-path="chap-8.html"><a href="chap-8.html"><i class="fa fa-check"></i><b>8</b> Chap 8</a></li>
<li class="chapter" data-level="9" data-path="chap-9.html"><a href="chap-9.html"><i class="fa fa-check"></i><b>9</b> Chap 9</a></li>
<li class="chapter" data-level="10" data-path="chap-10.html"><a href="chap-10.html"><i class="fa fa-check"></i><b>10</b> Chap 10</a></li>
<li class="chapter" data-level="11" data-path="C11Binary.html"><a href="C11Binary.html"><i class="fa fa-check"></i><b>11</b> Variables Dependientes Categóricas</a>
<ul>
<li class="chapter" data-level="11.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec111"><i class="fa fa-check"></i><b>11.1</b> Variables Dependientes Binarias</a></li>
<li class="chapter" data-level="11.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec112"><i class="fa fa-check"></i><b>11.2</b> Modelos de Regresión Logística y Probit</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1121"><i class="fa fa-check"></i><b>11.2.1</b> Uso de Funciones No Lineales de Variables Explicativas</a></li>
<li class="chapter" data-level="11.2.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1122"><i class="fa fa-check"></i><b>11.2.2</b> Interpretación del Umbral</a></li>
<li class="chapter" data-level="11.2.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1123"><i class="fa fa-check"></i><b>11.2.3</b> Interpretación de Utilidad Aleatoria</a></li>
<li class="chapter" data-level="11.2.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1124"><i class="fa fa-check"></i><b>11.2.4</b> Regresión Logística</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec113"><i class="fa fa-check"></i><b>11.3</b> Inferencia para Modelos de Regresión Logística y Probit</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="C11Binary.html"><a href="C11Binary.html#estimación-de-parámetros"><i class="fa fa-check"></i><b>11.3.1</b> Estimación de Parámetros</a></li>
<li class="chapter" data-level="11.3.2" data-path="C11Binary.html"><a href="C11Binary.html#inferencia-adicional"><i class="fa fa-check"></i><b>11.3.2</b> Inferencia Adicional</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec114"><i class="fa fa-check"></i><b>11.4</b> Aplicación: Gastos Médicos</a></li>
<li class="chapter" data-level="11.5" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec115"><i class="fa fa-check"></i><b>11.5</b> Variables Dependientes Nominales</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1151"><i class="fa fa-check"></i><b>11.5.1</b> Logit Generalizado</a></li>
<li class="chapter" data-level="11.5.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1152"><i class="fa fa-check"></i><b>11.5.2</b> Logit Multinomial</a></li>
<li class="chapter" data-level="11.5.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1153"><i class="fa fa-check"></i><b>11.5.3</b> Logit Anidado</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec116"><i class="fa fa-check"></i><b>11.6</b> Variables Dependientes Ordinales</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="C11Binary.html"><a href="C11Binary.html#logit-acumulativo"><i class="fa fa-check"></i><b>11.6.1</b> Logit Acumulativo</a></li>
<li class="chapter" data-level="11.6.2" data-path="C11Binary.html"><a href="C11Binary.html#probit-acumulativo"><i class="fa fa-check"></i><b>11.6.2</b> Probit Acumulativo</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec117"><i class="fa fa-check"></i><b>11.7</b> Lecturas Adicionales y Referencias</a></li>
<li class="chapter" data-level="11.8" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec118"><i class="fa fa-check"></i><b>11.8</b> Ejercicios</a></li>
<li class="chapter" data-level="11.9" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec119"><i class="fa fa-check"></i><b>11.9</b> Suplementos Técnicos - Inferencia Basada en Verosimilitud</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1191"><i class="fa fa-check"></i><b>11.9.1</b> Propiedades de las Funciones de Verosimilitud</a></li>
<li class="chapter" data-level="11.9.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1192"><i class="fa fa-check"></i><b>11.9.2</b> Estimadores de Máxima Verosimilitud</a></li>
<li class="chapter" data-level="11.9.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1193"><i class="fa fa-check"></i><b>11.9.3</b> Pruebas de Hipótesis</a></li>
<li class="chapter" data-level="11.9.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1194"><i class="fa fa-check"></i><b>11.9.4</b> Criterios de Información</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="C12Count.html"><a href="C12Count.html"><i class="fa fa-check"></i><b>12</b> Variables Dependientes de Conteo</a>
<ul>
<li class="chapter" data-level="12.1" data-path="C12Count.html"><a href="C12Count.html#S:Sec121"><i class="fa fa-check"></i><b>12.1</b> Regresión de Poisson</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="C12Count.html"><a href="C12Count.html#S:Sec1211"><i class="fa fa-check"></i><b>12.1.1</b> Distribución de Poisson</a></li>
<li class="chapter" data-level="12.1.2" data-path="C12Count.html"><a href="C12Count.html#S:Sec1212"><i class="fa fa-check"></i><b>12.1.2</b> Modelo de Regresión</a></li>
<li class="chapter" data-level="12.1.3" data-path="C12Count.html"><a href="C12Count.html#S:Sec1213"><i class="fa fa-check"></i><b>12.1.3</b> Estimación</a></li>
<li class="chapter" data-level="12.1.4" data-path="C12Count.html"><a href="C12Count.html#S:Sec1214"><i class="fa fa-check"></i><b>12.1.4</b> Inferencia Adicional</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="C12Count.html"><a href="C12Count.html#S:Sec122"><i class="fa fa-check"></i><b>12.2</b> Aplicación: Seguro de Automóviles en Singapur</a></li>
<li class="chapter" data-level="12.3" data-path="C12Count.html"><a href="C12Count.html#S:Sec123"><i class="fa fa-check"></i><b>12.3</b> Sobre dispersión y Modelos Binomiales Negativos</a></li>
<li class="chapter" data-level="12.4" data-path="C12Count.html"><a href="C12Count.html#S:Sec124"><i class="fa fa-check"></i><b>12.4</b> Otros Modelos de Conteo</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="C12Count.html"><a href="C12Count.html#modelos-inflados-en-ceros"><i class="fa fa-check"></i><b>12.4.1</b> Modelos Inflados en Ceros</a></li>
<li class="chapter" data-level="12.4.2" data-path="C12Count.html"><a href="C12Count.html#modelos-hurdle"><i class="fa fa-check"></i><b>12.4.2</b> Modelos Hurdle</a></li>
<li class="chapter" data-level="12.4.3" data-path="C12Count.html"><a href="C12Count.html#modelos-de-heterogeneidad"><i class="fa fa-check"></i><b>12.4.3</b> Modelos de Heterogeneidad</a></li>
<li class="chapter" data-level="12.4.4" data-path="C12Count.html"><a href="C12Count.html#modelos-de-clases-latentes"><i class="fa fa-check"></i><b>12.4.4</b> Modelos de Clases Latentes</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="C12Count.html"><a href="C12Count.html#S:Sec125"><i class="fa fa-check"></i><b>12.5</b> Lecturas Adicionales y Referencias</a></li>
<li class="chapter" data-level="12.6" data-path="C12Count.html"><a href="C12Count.html#S:Sec126"><i class="fa fa-check"></i><b>12.6</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="C13GLM.html"><a href="C13GLM.html"><i class="fa fa-check"></i><b>13</b> Modelos Lineales Generalizados</a>
<ul>
<li class="chapter" data-level="13.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec131"><i class="fa fa-check"></i><b>13.1</b> Introducción</a></li>
<li class="chapter" data-level="13.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec132"><i class="fa fa-check"></i><b>13.2</b> Modelo GLM</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1321"><i class="fa fa-check"></i><b>13.2.1</b> Familia Exponencial Lineal de Distribuciones</a></li>
<li class="chapter" data-level="13.2.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1322"><i class="fa fa-check"></i><b>13.2.2</b> Funciones de Enlace</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec133"><i class="fa fa-check"></i><b>13.3</b> Estimación</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1331"><i class="fa fa-check"></i><b>13.3.1</b> Estimación de Máxima Verosimilitud para Enlaces Canónicos</a></li>
<li class="chapter" data-level="13.3.2" data-path="C13GLM.html"><a href="C13GLM.html#sobredispersión"><i class="fa fa-check"></i><b>13.3.2</b> Sobredispersión</a></li>
<li class="chapter" data-level="13.3.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1333"><i class="fa fa-check"></i><b>13.3.3</b> Estadísticas de Bondad de Ajuste</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec134"><i class="fa fa-check"></i><b>13.4</b> Aplicación: Gastos Médicos</a></li>
<li class="chapter" data-level="13.5" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec135"><i class="fa fa-check"></i><b>13.5</b> Residuales</a></li>
<li class="chapter" data-level="13.6" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec136"><i class="fa fa-check"></i><b>13.6</b> Distribución de Tweedie</a></li>
<li class="chapter" data-level="13.7" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec137"><i class="fa fa-check"></i><b>13.7</b> Lecturas adicionales y referencias</a></li>
<li class="chapter" data-level="13.8" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec138"><i class="fa fa-check"></i><b>13.8</b> Ejercicios</a></li>
<li class="chapter" data-level="13.9" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec139"><i class="fa fa-check"></i><b>13.9</b> Suplementos Técnicos - Familia Exponencial</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1391"><i class="fa fa-check"></i><b>13.9.1</b> Familia Exponencial Lineal de Distribuciones</a></li>
<li class="chapter" data-level="13.9.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1392"><i class="fa fa-check"></i><b>13.9.2</b> Momentos</a></li>
<li class="chapter" data-level="13.9.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1393"><i class="fa fa-check"></i><b>13.9.3</b> Estimación de Máxima Verosimilitud para Enlaces Generales</a></li>
<li class="chapter" data-level="13.9.4" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1394"><i class="fa fa-check"></i><b>13.9.4</b> Mínimos Cuadrados Reponderados Iterativos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="apéndices.html"><a href="apéndices.html"><i class="fa fa-check"></i><b>14</b> Apéndices</a>
<ul>
<li class="chapter" data-level="" data-path="apéndices.html"><a href="apéndices.html#apéndice-a1.-inferencia-estadística-básica"><i class="fa fa-check"></i>Apéndice A1. Inferencia Estadística Básica</a>
<ul>
<li class="chapter" data-level="" data-path="apéndices.html"><a href="apéndices.html#distribuciones-de-funciones-de-variables-aleatorias"><i class="fa fa-check"></i>Distribuciones de Funciones de Variables Aleatorias</a></li>
<li class="chapter" data-level="" data-path="apéndices.html"><a href="apéndices.html#estimación-y-predicción"><i class="fa fa-check"></i>Estimación y Predicción</a></li>
<li class="chapter" data-level="" data-path="apéndices.html"><a href="apéndices.html#pruebas-de-hipótesis"><i class="fa fa-check"></i>Pruebas de Hipótesis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="apéndices.html"><a href="apéndices.html#apéndice-a2.-álgebra-de-matrices"><i class="fa fa-check"></i>Apéndice A2. Álgebra de Matrices</a>
<ul>
<li class="chapter" data-level="" data-path="apéndices.html"><a href="apéndices.html#definiciones-básicas"><i class="fa fa-check"></i>Definiciones Básicas</a></li>
<li class="chapter" data-level="" data-path="apéndices.html"><a href="apéndices.html#revisión-de-operaciones-básicas"><i class="fa fa-check"></i>Revisión de Operaciones Básicas</a></li>
<li class="chapter" data-level="" data-path="apéndices.html"><a href="apéndices.html#definiciones-adicionales"><i class="fa fa-check"></i>Definiciones Adicionales</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="apéndices.html"><a href="apéndices.html#apéndice-a3.-tablas-de-probabilidad"><i class="fa fa-check"></i>Apéndice A3. Tablas de Probabilidad</a>
<ul>
<li class="chapter" data-level="" data-path="apéndices.html"><a href="apéndices.html#distribución-normal"><i class="fa fa-check"></i>Distribución Normal</a></li>
<li class="chapter" data-level="" data-path="apéndices.html"><a href="apéndices.html#distribución-chi-cuadrado"><i class="fa fa-check"></i>Distribución Chi-Cuadrado</a></li>
<li class="chapter" data-level="" data-path="apéndices.html"><a href="apéndices.html#distribución-t"><i class="fa fa-check"></i>Distribución <em>t</em></a></li>
<li class="chapter" data-level="" data-path="apéndices.html"><a href="apéndices.html#distribución-f"><i class="fa fa-check"></i>Distribución <em>F</em></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTextDev/RegressionSpanish/" target="blank">Spanish Regression on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modelado de Regresión con Aplicaciones Actuariales y Financieras</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="C5VarSelect" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Selección de Variables<a href="C5VarSelect.html#C5VarSelect" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Vista previa del capítulo</em>. Este capítulo describe herramientas y técnicas para ayudar a seleccionar las variables a incluir en un modelo de regresión lineal, comenzando con un proceso iterativo de selección de modelos. En aplicaciones con muchas variables explicativas potenciales, los procedimientos automáticos de selección de variables ayudan a evaluar rápidamente muchos modelos. Sin embargo, los procedimientos automáticos tienen serias limitaciones, incluida la incapacidad de manejar adecuadamente las no linealidades como el impacto de puntos inusuales; este capítulo amplía la discusión del Capítulo 2 sobre puntos inusuales. También se describe la colinealidad, una característica común de los datos de regresión donde las variables explicativas están linealmente relacionadas entre sí. Otros temas que afectan la selección de variables, como la heterocedasticidad y la validación fuera de muestra, también se introducen.</p>
<div id="Sec51" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Un Enfoque Iterativo para el Análisis de Datos y Modelado<a href="C5VarSelect.html#Sec51" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En nuestra introducción a la regresión lineal básica en el Capítulo 2, examinamos los datos gráficamente, formulamos una hipótesis sobre la estructura del modelo y comparamos los datos con un modelo candidato para formular un modelo mejorado. Box (1980) describe esto como un <em>proceso iterativo</em>, que se muestra en la Figura <a href="C5VarSelect.html#fig:Fig51">5.1</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig51"></span>
<img src="RegressionMarkdown_files/figure-html/Fig51-1.png" alt="El proceso iterativo de especificación del modelo" width="80%" />
<p class="caption">
Figura 5.1: <strong>El proceso iterativo de especificación del modelo</strong>
</p>
</div>
<p>Este proceso iterativo proporciona una receta útil para estructurar la tarea de especificar un modelo que represente un conjunto de datos. El primer paso, la etapa de formulación del modelo, se realiza examinando los datos gráficamente y utilizando el conocimiento previo de las relaciones, como de la teoría económica o de la práctica estándar de la industria. El segundo paso en la iteración se basa en los supuestos del modelo especificado. Estos supuestos deben ser consistentes con los datos para hacer un uso válido del modelo. El tercer paso, <em>verificación diagnóstica</em>, también se conoce como <em>crítica de datos y modelo</em>; los datos y el modelo deben ser consistentes entre sí antes de que se puedan hacer inferencias adicionales. La verificación diagnóstica es una parte importante de la formulación del modelo; puede revelar errores cometidos en pasos anteriores y proporcionar formas de corregir estos errores.</p>
</div>
<div id="Sec52" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Procedimientos Automáticos de Selección de Variables<a href="C5VarSelect.html#Sec52" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Las relaciones en negocios y economía son complicadas; típicamente hay muchas variables que podrían servir como predictores útiles de la variable dependiente. Al buscar una relación adecuada, hay una gran cantidad de modelos potenciales que se basan en combinaciones lineales de variables explicativas y un número infinito de modelos que pueden formarse a partir de combinaciones no lineales. Para buscar entre los modelos basados en combinaciones lineales, existen varios procedimientos automáticos para seleccionar las variables que se incluirán en el modelo. Estos procedimientos automáticos son fáciles de usar y sugerirán uno o más modelos que se pueden explorar con mayor detalle.</p>
<p>Para ilustrar cuán grande es el número potencial de modelos lineales, supongamos que solo hay cuatro variables, <span class="math inline">\(x_{1}, x_2, x_3\)</span> y <span class="math inline">\(x_4\)</span>, bajo consideración para ajustar un modelo a <span class="math inline">\(y\)</span>. Sin considerar la multiplicación u otras combinaciones no lineales de las variables explicativas, ¿cuántos modelos posibles hay? La Tabla <a href="C5VarSelect.html#tab:Tab51">5.1</a> muestra que la respuesta es 16.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab51">Tabla 5.1: </span><strong>Dieciséis Modelos Posibles</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Expression
</th>
<th style="text-align:center;">
Combinations
</th>
<th style="text-align:left;">
Models
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 7cm; border-right:1px solid;">
E <span class="math inline">\(y=\beta_0\)</span>
</td>
<td style="text-align:center;width: 3cm; border-right:1px solid;">
</td>
<td style="text-align:left;width: 3cm; width: 7cm; ">
1 modelo sin variables independientes
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 7cm; border-right:1px solid;">
E <span class="math inline">\(y=\beta_0+\beta_1x_i\)</span>
</td>
<td style="text-align:center;width: 3cm; border-right:1px solid;">
<span class="math inline">\(i\)</span> = 1,2,3,4
</td>
<td style="text-align:left;width: 3cm; width: 7cm; ">
4 modelos con una variable independiente
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 7cm; border-right:1px solid;">
E <span class="math inline">\(y = \beta_0 + \beta_1 x_i + \beta_2 x_j\)</span>
</td>
<td style="text-align:center;width: 3cm; border-right:1px solid;">
(<span class="math inline">\(i,j\)</span>) = (1,2),(1,3),(1,4),(2,3),(2,4),(3,4)
</td>
<td style="text-align:left;width: 3cm; width: 7cm; ">
6 modelos con dos variables independientes
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 7cm; border-right:1px solid;">
E <span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \beta_2 x_j +\beta_3x_{k}\)</span>
</td>
<td style="text-align:center;width: 3cm; border-right:1px solid;">
(<span class="math inline">\(i,j,k\)</span>) = (1,2,3),(1,2,4),(1,3,4),(2,3,4)
</td>
<td style="text-align:left;width: 3cm; width: 7cm; ">
4 modelos con tres variables independientes
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 7cm; border-right:1px solid;">
E <span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 +\beta_3 x_3 + \beta_4 x_4\)</span>
</td>
<td style="text-align:center;width: 3cm; border-right:1px solid;">
</td>
<td style="text-align:left;width: 3cm; width: 7cm; ">
1 modelo con todas las variables independientes
</td>
</tr>
</tbody>
</table>
<p>Si solo hubiera tres variables explicativas, entonces se puede usar la misma lógica para verificar que hay ocho modelos posibles. Extrapolando a partir de estos dos ejemplos, ¿cuántos modelos lineales habrá si hay diez variables explicativas? La respuesta es 1,024, lo cual es bastante. En general, la respuesta es <span class="math inline">\(2^k\)</span>, donde <span class="math inline">\(k\)</span> es el número de variables explicativas. Por ejemplo, <span class="math inline">\(2^3\)</span> es 8, <span class="math inline">\(2^4\)</span> es 16, y así sucesivamente.</p>
<p>En cualquier caso, para un número moderadamente grande de variables explicativas, hay muchos modelos potenciales que se basan en combinaciones lineales de variables explicativas. Nos gustaría tener un procedimiento para buscar rápidamente entre estos modelos potenciales y darnos más tiempo para pensar en otros aspectos interesantes de la selección de modelos. La <em>regresión por pasos</em> son procedimientos que emplean pruebas <span class="math inline">\(t\)</span> para verificar la “significancia” de las variables explicativas que se incluyen o eliminan del modelo.</p>
<p>Para comenzar, en la versión de <em>selección hacia adelante</em> de la regresión por pasos, las variables se agregan una a la vez. En la primera etapa, de todas las variables candidatas, se agrega al modelo la que es más estadísticamente significativa. En la siguiente etapa, con la variable de la primera etapa ya incluida, se agrega la siguiente variable más estadísticamente significativa. Este procedimiento se repite hasta que se hayan agregado todas las variables estadísticamente significativas. Aquí, la significancia estadística generalmente se evalúa utilizando el cociente <span class="math inline">\(t\)</span> de una variable; el umbral para la significancia estadística es típicamente un valor <span class="math inline">\(t\)</span> predefinido (como dos, que corresponde a un nivel de significancia aproximado del 95%).</p>
<p>La versión de <em>selección hacia atrás</em> funciona de manera similar, excepto que todas las variables se incluyen en la etapa inicial y luego se eliminan una a la vez (en lugar de agregarse).</p>
<p>Más generalmente, un algoritmo que agrega y elimina variables en cada etapa a veces se conoce como <em>el</em> algoritmo de regresión por pasos.</p>
<div class="blackbox">
<p><em>Algoritmo de Regresión por Pasos.</em> Suponga que el analista ha identificado una variable como la respuesta, <span class="math inline">\(y\)</span>, y <span class="math inline">\(k\)</span> variables explicativas potenciales, <span class="math inline">\(x_1, x_2, \ldots, x_k\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Considere todas las regresiones posibles usando una variable explicativa. Para cada una de las <span class="math inline">\(k\)</span> regresiones, calcule <span class="math inline">\(t(b_1)\)</span>, el cociente <span class="math inline">\(t\)</span> para la pendiente. Elija la variable con el cociente <span class="math inline">\(t\)</span> más grande. Si el cociente <span class="math inline">\(t\)</span> no supera un valor <span class="math inline">\(t\)</span> predefinido (como dos), entonces no elija ninguna variable y detenga el procedimiento.</li>
<li>Agregue una variable al modelo del paso anterior. La variable a ingresar es la que hace la contribución más significativa. Para determinar el tamaño de la contribución, use el valor absoluto del cociente <span class="math inline">\(t\)</span> de la variable. Para ingresar, el cociente <span class="math inline">\(t\)</span> debe superar un valor <span class="math inline">\(t\)</span> especificado en valor absoluto.</li>
<li>Elimine una variable del modelo del paso anterior. La variable a eliminar es la que hace la menor contribución. Para determinar el tamaño de la contribución, use el valor absoluto del cociente <span class="math inline">\(t\)</span> de la variable. Para ser eliminada, el cociente <span class="math inline">\(t\)</span> debe ser menor que un valor <span class="math inline">\(t\)</span> especificado en valor absoluto.</li>
<li>Repita los pasos (ii) y (iii) hasta que se realicen todas las posibles adiciones y eliminaciones.</li>
</ol>
</div>
<p>Al implementar esta rutina, algunos paquetes de software estadístico usan una prueba <span class="math inline">\(F\)</span> en lugar de pruebas <span class="math inline">\(t\)</span>. Recuerde que, cuando solo se considera una variable, <span class="math inline">\((t\text{-cociente})^2 = F\)</span>-cociente, y por lo tanto, estos procedimientos son equivalentes.</p>
<p>Este algoritmo es útil porque busca rápidamente entre varios modelos candidatos. Sin embargo, presenta varias desventajas:</p>
<ol style="list-style-type: decimal">
<li>El procedimiento “husmea” entre un gran número de modelos y puede ajustar los datos “demasiado bien.”</li>
<li>No hay garantía de que el modelo seleccionado sea el mejor. El algoritmo no considera modelos que se basan en combinaciones no lineales de variables explicativas. También ignora la presencia de valores atípicos y puntos de alta influencia.</li>
<li>Además, el algoritmo no busca todos los <span class="math inline">\(2^{k}\)</span> regresiones lineales posibles.</li>
<li>El algoritmo utiliza un criterio, un cociente <span class="math inline">\(t\)</span>, y no considera otros criterios como <span class="math inline">\(s\)</span>, <span class="math inline">\(R^2\)</span>, <span class="math inline">\(R_a^2\)</span>, y así sucesivamente.</li>
<li>Hay una secuencia de pruebas de significancia involucradas. Por lo tanto, el nivel de significancia que determina el valor <span class="math inline">\(t\)</span> no es significativo.</li>
<li>Al considerar cada variable por separado, el algoritmo no toma en cuenta el efecto conjunto de las variables explicativas.</li>
<li>Los procedimientos puramente automáticos pueden no tener en cuenta el conocimiento especial de un investigador.</li>
</ol>
<p>Muchas de las críticas al algoritmo básico de regresión paso a paso pueden abordarse con software de computación moderno que ahora está ampliamente disponible. Ahora consideraremos cada inconveniente, en orden inverso. Para responder a la desventaja número (7), muchas rutinas de software estadístico tienen opciones para forzar la inclusión de variables en una ecuación de modelo. De esta manera, si otras evidencias indican que una o más variables deben incluirse en el modelo, el investigador puede forzar la inclusión de estas variables.</p>
<p>Para la desventaja número (6), en la Sección <a href="C5VarSelect.html#Sec554">5.5.4</a> sobre <em>variables supresoras</em>, proporcionaremos ejemplos de variables que no tienen efectos individuales importantes pero son importantes cuando se consideran en conjunto. Estas combinaciones de variables pueden no ser detectadas con el algoritmo básico, pero serán detectadas con el algoritmo de selección hacia atrás. Dado que el procedimiento de selección hacia atrás comienza con todas las variables, detectará y conservará las variables que son importantes en conjunto.</p>
<p>La desventaja número (5) es realmente una sugerencia sobre la forma de utilizar la regresión paso a paso. Bendel y Afifi (1977) sugirieron usar un valor de corte más pequeño del que normalmente se usaría. Por ejemplo, en lugar de usar un <span class="math inline">\(t\)</span>-valor = 2 que corresponde aproximadamente a un nivel de significancia del 5%, considere usar un <span class="math inline">\(t\)</span>-valor = 1.645 que corresponde aproximadamente a un nivel de significancia del 10%. De esta manera, hay menos posibilidad de excluir variables que pueden ser importantes. Un límite inferior, pero aún una buena opción para trabajo exploratorio, es un corte tan pequeño como <span class="math inline">\(t\)</span>-valor = 1. Esta elección está motivada por un resultado algebraico: cuando una variable entra en un modelo, <span class="math inline">\(s\)</span> disminuirá si el <span class="math inline">\(t\)</span>-ratio excede uno en valor absoluto.</p>
<p>Para abordar las desventajas número (3) y (4), ahora introducimos la rutina de <em>mejores regresiones</em>. Las mejores regresiones es un algoritmo útil que ahora está ampliamente disponible en paquetes de software estadístico. El algoritmo de mejor regresión busca en todas las combinaciones posibles de variables explicativas, a diferencia de la regresión paso a paso, que agrega y elimina una variable a la vez. Por ejemplo, suponga que hay cuatro posibles variables explicativas, <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math inline">\(x_3\)</span> y <span class="math inline">\(x_4\)</span>, y el usuario desea saber cuál es el mejor modelo de dos variables. El algoritmo de mejor regresión busca entre los seis modelos de la forma <span class="math inline">\(\mathrm{E}~y = \beta_0 + \beta_1 x_i + \beta_2 x_j\)</span>. Típicamente, una rutina de mejor regresión recomienda uno o dos modelos para cada modelo con coeficiente <span class="math inline">\(p\)</span>, donde <em>p</em> es un número especificado por el usuario. Debido a que se ha especificado el número de coeficientes que entrarán en el modelo, no importa qué criterio usemos: <span class="math inline">\(R^2\)</span>, <span class="math inline">\(R_a^2\)</span> o <span class="math inline">\(s\)</span>.</p>
<p>El algoritmo de mejor regresión realiza su búsqueda mediante un uso ingenioso del hecho algebraico de que, cuando se añade una variable al modelo, la suma de cuadrados del error no aumenta. Debido a este hecho, ciertas combinaciones de variables incluidas en el modelo no necesitan ser calculadas. Un inconveniente importante de este algoritmo es que puede tomar mucho tiempo cuando el número de variables consideradas es grande.</p>
<p>Los usuarios de la regresión no siempre aprecian la profundidad del inconveniente número (1), <em>data-snooping</em> (exploración de datos). La exploración de datos ocurre cuando el analista ajusta un gran número de modelos a un conjunto de datos. Abordaremos el problema de la exploración de datos en la Sección <a href="C5VarSelect.html#Sec562">5.6.2</a> sobre validación de modelos. Aquí, ilustraremos el efecto de la exploración de datos en la regresión paso a paso.</p>
<hr />
<p><strong>Ejemplo: Exploración de Datos en Regresión Paso a Paso.</strong> La idea de esta ilustración es de Rencher y Pun (1980). Considere <span class="math inline">\(n = 100\)</span> observaciones de <span class="math inline">\(y\)</span> y cincuenta variables explicativas, <span class="math inline">\(x_1, x_2, \ldots, x_{50}\)</span>. Los datos que consideramos aquí se simularon usando variables aleatorias normales estándar independientes. Debido a que las variables se simularon de manera independiente, estamos trabajando bajo la hipótesis nula de que no hay relación entre la respuesta y las variables explicativas, es decir, <span class="math inline">\(H_0: \beta_1 = \beta_2 = \ldots = \beta_{50} = 0\)</span>. De hecho, cuando se ajustó el modelo con las cincuenta variables explicativas, resultó que <span class="math inline">\(s = 1.142\)</span>, <span class="math inline">\(R^2 = 46.2\%\)</span>, y el <span class="math inline">\(F\)</span>-ratio = <span class="math inline">\(\frac{Regression~MS}{Error~MS} = 0.84\)</span>. Usando una distribución <span class="math inline">\(F\)</span> con <span class="math inline">\(df_1 = 50\)</span> y <span class="math inline">\(df_2 = 49\)</span>, el percentil 95 es 1.604. De hecho, 0.84 es el percentil 27 de esta distribución, lo que indica que el valor <span class="math inline">\(p\)</span> es 0.73. Por lo tanto, como era de esperar, los datos están en congruencia con <span class="math inline">\(H_0\)</span>.</p>
<p>A continuación, se realizó una regresión paso a paso con <span class="math inline">\(t\)</span>-valor = 2. Dos variables fueron retenidas por este procedimiento, lo que resultó en un modelo con <span class="math inline">\(s = 1.05\)</span>, <span class="math inline">\(R^2 = 9.5\%\)</span> y <span class="math inline">\(F\)</span>-ratio = 5.09. Para una distribución <span class="math inline">\(F\)</span> con <span class="math inline">\(df_1 = 2\)</span> y <span class="math inline">\(df_2 = 97\)</span>, el percentil 95 es un <span class="math inline">\(F\)</span>-valor = 3.09. Esto indica que las dos variables son predictores estadísticamente significativos de <span class="math inline">\(y\)</span>. A primera vista, este resultado es sorprendente. Los datos se generaron de manera que <span class="math inline">\(y\)</span> no estuviera relacionado con las variables explicativas. Sin embargo, debido a que <span class="math inline">\(F\)</span>-ratio <span class="math inline">\(&gt;\)</span> <span class="math inline">\(F\)</span>-valor, la prueba <span class="math inline">\(F\)</span> indica que dos variables explicativas están significativamente relacionadas con <span class="math inline">\(y\)</span>. La razón es que la regresión paso a paso ha realizado muchas pruebas de hipótesis en los datos. Por ejemplo, en el Paso 1, se realizaron cincuenta pruebas para encontrar variables significativas. Recuerde que un nivel del 5% significa que esperamos cometer aproximadamente un error en 20. Por lo tanto, con cincuenta pruebas, esperamos encontrar <span class="math inline">\(50 \times 0.05 = 2.5\)</span> variables “significativas”, incluso bajo la hipótesis nula de que no hay relación entre <span class="math inline">\(y\)</span> y las variables explicativas.</p>
<p>Para continuar, se realizó una regresión paso a paso con <span class="math inline">\(t\)</span>-valor = 1.645. Seis variables fueron retenidas por este procedimiento, lo que resultó en un modelo con <span class="math inline">\(s = 0.99\)</span>, <span class="math inline">\(R^2 = 22.9\%\)</span> y <span class="math inline">\(F\)</span>-ratio = 4.61. Como antes, una prueba <span class="math inline">\(F\)</span> indica una relación significativa entre la respuesta y estas seis variables explicativas.</p>
<p>Para resumir, utilizando simulación, construimos un conjunto de datos de manera que las variables explicativas no tuvieran relación con la respuesta. Sin embargo, al utilizar la regresión paso a paso para examinar los datos, “encontramos” relaciones aparentemente significativas entre la respuesta y ciertos subconjuntos de las variables explicativas. Este ejemplo ilustra una advertencia general en la selección de modelos: cuando las variables explicativas se seleccionan utilizando los datos, los <span class="math inline">\(t\)</span>-ratios y los <span class="math inline">\(F\)</span>-ratios serán demasiado grandes, exagerando así la importancia de las variables en el modelo.</p>
<hr />
<p>La regresión paso a paso y las mejores regresiones son ejemplos de <em>procedimientos automáticos de selección de variables</em>. En su trabajo de modelado, encontrará que estos procedimientos son útiles porque pueden buscar rápidamente entre varios modelos candidatos. Sin embargo, estos procedimientos ignoran alternativas no lineales, así como el efecto de los valores atípicos y los puntos de alta influencia. El objetivo principal de estos procedimientos es mecanizar ciertas tareas rutinarias. Este enfoque de selección automática se puede extender, y de hecho, hay varios “sistemas expertos” disponibles en el mercado. Por ejemplo, hay algoritmos disponibles que manejan “automáticamente” puntos inusuales como valores atípicos y puntos de alta influencia. Un modelo sugerido por los procedimientos automáticos de selección de variables debe estar sujeto a los mismos procedimientos cuidadosos de verificación diagnóstica que un modelo obtenido por cualquier otro medio.</p>
</div>
<div id="Sec53" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Análisis de Residuales<a href="C5VarSelect.html#Sec53" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recuerde el papel de un residual en el modelo de regresión lineal introducido en la Sección 2.6. Un residual es una respuesta menos el valor ajustado correspondiente bajo el modelo. Dado que el modelo resume el efecto lineal de varias variables explicativas, podemos pensar en un residual como una respuesta controlada por los valores de las variables explicativas. Si el modelo es una representación adecuada de los datos, entonces los residuales deberían aproximarse a errores aleatorios. Los errores aleatorios se utilizan para representar la variación natural en el modelo; representan el resultado de un mecanismo impredecible. Por lo tanto, en la medida en que los residuales se parezcan a errores aleatorios, no debería haber patrones discernibles en los residuales. Los patrones en los residuales indican la presencia de información adicional que esperamos incorporar en el modelo. La ausencia de patrones en los residuales indica que el modelo parece explicar las relaciones principales en los datos.</p>
<div id="Sec531" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Residuales<a href="C5VarSelect.html#Sec531" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Hay al menos cuatro tipos de patrones que pueden descubrirse a través del análisis de residuales. En esta sección, discutimos los dos primeros: residuales que son inusuales y aquellos que están relacionados con otras variables explicativas. Luego introducimos el tercer tipo, residuales que muestran un patrón heterocedástico, en la Sección <a href="C5VarSelect.html#Sec57">5.7</a>. En nuestro estudio de datos de series temporales que comienza en el Capítulo 7, introduciremos el cuarto tipo, residuales que muestran patrones a lo largo del tiempo.</p>
<p>Al examinar los residuales, generalmente es más fácil trabajar con un <em>residual estandarizado</em>, un residual que ha sido reescalado para no tener dimensiones. Generalmente trabajamos con residuales estandarizados porque así logramos transferir cierta experiencia de un conjunto de datos a otro y podemos enfocarnos en relaciones de interés. Al usar residuales estandarizados, podemos entrenarnos para observar una variedad de gráficos de residuales y reconocer inmediatamente un punto inusual al trabajar en unidades estándar.</p>
<p>Hay varias formas de definir un residual estandarizado. Usando <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span> como el <span class="math inline">\(i\)</span>-ésimo residual, aquí hay tres definiciones comúnmente usadas:</p>
<p><span class="math display" id="eq:eq51">\[\begin{equation}
\text{(a) }\frac{e_i}{s}, \quad \text{(b) }\frac{e_i}{s\sqrt{1 - h_{ii}}}, \quad \text{(c) }\frac{e_i}{s_{(i)}\sqrt{1 - h_{ii}}}.
\tag{5.1}
\end{equation}\]</span></p>
<p>Aquí, <span class="math inline">\(h_{ii}\)</span> es la influencia del <span class="math inline">\(i\)</span>-ésimo punto. Se calcula en función de los valores de las variables explicativas y se definirá en la Sección <a href="C5VarSelect.html#Sec541">5.4.1</a>. Recuerde que <span class="math inline">\(s\)</span> es la desviación estándar de los residuales (definida en la ecuación 3.8). De manera similar, definimos <span class="math inline">\(s_{(i)}\)</span> como la desviación estándar de los residuales al ejecutar una regresión después de eliminar la <span class="math inline">\(i\)</span>-ésima observación.</p>
<p>Ahora, la primera definición en (a) es simple y fácil de explicar. Un cálculo simple muestra que la desviación estándar de la muestra de los residuales es aproximadamente <span class="math inline">\(s\)</span> (una razón por la que <span class="math inline">\(s\)</span> a menudo se denomina desviación estándar de los residuales). Por lo tanto, parece razonable estandarizar los residuales dividiendo por <span class="math inline">\(s\)</span>.</p>
<p>La segunda opción presentada en (b), aunque más compleja, es más precisa. La varianza del <span class="math inline">\(i\)</span>-ésimo residual es</p>
<p><span class="math display">\[
\text{Var}(e_i) = \sigma^2(1 - h_{ii}).
\]</span></p>
<p>Este resultado se establecerá en la ecuación <a href="C5VarSelect.html#eq:eq515">(5.15)</a> de la Sección <a href="C5VarSelect.html#Sec510">5.10</a>. Tenga en cuenta que esta varianza es menor que la varianza del término de error, Var<span class="math inline">\((\varepsilon_i) = \sigma^2\)</span>. Ahora, podemos reemplazar <span class="math inline">\(\sigma\)</span> por su estimación, <span class="math inline">\(s\)</span>. Entonces, este resultado lleva a usar la cantidad <span class="math inline">\(s\sqrt{1 - h_{ii}}\)</span> como una desviación estándar estimada, o error estándar, para <span class="math inline">\(e_i\)</span>. Por lo tanto, definimos el error estándar de <span class="math inline">\(e_i\)</span> como</p>
<p><span class="math display">\[
\text{se}(e_i) = s \sqrt{1 - h_{ii}}.
\]</span></p>
<p>Siguiendo las convenciones introducidas en la Sección 2.6, en este texto usamos <span class="math inline">\(e_i / \text{se}(e_i)\)</span> como nuestro <em>residual estandarizado</em>.</p>
<p>La tercera opción presentada en (c) es una modificación de (b) y se conoce como un <em>residual studentizado</em>. Como se enfatiza en la Sección <a href="C5VarSelect.html#Sec532">5.3.2</a>, un uso importante de los residuales es identificar respuestas inusualmente grandes. Ahora, supongamos que la <span class="math inline">\(i\)</span>-ésima respuesta es inusualmente grande y que esto se mide a través de su residual. Este residual inusualmente grande también hará que el valor de <span class="math inline">\(s\)</span> sea grande. Debido a que el efecto grande aparece tanto en el numerador como en el denominador, el residual estandarizado puede no detectar esta respuesta inusual. Sin embargo, esta respuesta grande no inflará <span class="math inline">\(s_{(i)}\)</span> porque se construye después de eliminar la <span class="math inline">\(i\)</span>-ésima observación. Por lo tanto, al usar residuales studentizados, obtenemos una mejor medida de las observaciones que tienen residuales inusualmente grandes. Al omitir esta observación de la estimación de <span class="math inline">\(\sigma\)</span>, el tamaño de la observación solo afecta al numerador <span class="math inline">\(e_i\)</span> y no al denominador <span class="math inline">\(s_{(i)}\)</span>.</p>
<p>Como otra ventaja, los residuales studentizados siguen una distribución <span class="math inline">\(t\)</span> con <span class="math inline">\(n - (k + 1)\)</span> grados de libertad, asumiendo que los errores están distribuidos normalmente (suposición E5). Este conocimiento de la distribución precisa nos ayuda a evaluar el grado de ajuste del modelo y es particularmente útil en muestras pequeñas. Es esta relación con la distribución <span class="math inline">\(t\)</span> de “Student” la que sugiere el nombre de “residuales studentizados”.</p>
</div>
<div id="Sec532" class="section level3 hasAnchor" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Uso de los Residuales para Identificar Valores Atípicos<a href="C5VarSelect.html#Sec532" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Una función importante del análisis de residuales es identificar valores atípicos. Un valor atípico es una observación que no se ajusta bien al modelo; son observaciones donde el residual es inusualmente grande. Una regla general utilizada por muchos paquetes estadísticos es que una observación se marca como un valor atípico si el residual estandarizado excede dos en valor absoluto. En la medida en que la distribución de los residuales estandarizados imite la curva normal estándar, esperamos que solo una de cada 20 observaciones, o el 95%, exceda dos en valor absoluto y muy pocas observaciones excedan tres.</p>
<p>Los valores atípicos proporcionan una señal de que una observación debe investigarse para entender las causas especiales asociadas con este punto. Un valor atípico es una observación que parece inusual con respecto al resto del conjunto de datos. A menudo sucede que la razón de este comportamiento atípico puede descubrirse después de una investigación adicional. De hecho, este puede ser el propósito principal del análisis de regresión de un conjunto de datos.</p>
<p>Consideremos un ejemplo simple de lo que se llama <em>análisis de desempeño</em>. Supongamos que tenemos disponible una muestra de <span class="math inline">\(n\)</span> vendedores y estamos tratando de entender las ventas de cada persona en el segundo año en función de sus ventas en el primer año. Hasta cierto punto, esperamos que las ventas más altas en el primer año estén asociadas con ventas más altas en el segundo año. Las altas ventas pueden deberse a la habilidad natural del vendedor, ambición, buen territorio, etc. Las ventas del primer año pueden considerarse como una variable proxy que resume estos factores. Esperamos variación en el desempeño de ventas tanto de manera transversal como a lo largo de los años. Es interesante cuando un vendedor tiene un desempeño inusualmente bueno (o malo) en el segundo año en comparación con su desempeño en el primer año. Los residuales proporcionan un mecanismo formal para evaluar las ventas del segundo año después de controlar los efectos de las ventas del primer año.</p>
<p>Hay varias opciones disponibles para manejar valores atípicos.</p>
<div class="blackbox">
<p><em>Opciones para Manejar Valores Atípicos</em></p>
<ul>
<li>Incluir la observación en las estadísticas resumen habituales pero comentar sobre sus efectos. Un valor atípico puede ser grande pero no tan grande como para sesgar los resultados de todo el análisis. Si no se pueden determinar causas especiales para esta observación inusual, entonces esta observación puede simplemente reflejar la variabilidad en los datos.</li>
<li>Eliminar la observación del conjunto de datos. Puede determinarse que la observación no es representativa de la población de la cual se extrae la muestra. Si este es el caso, entonces puede haber poca información contenida en la observación que pueda usarse para hacer afirmaciones generales sobre la población. Esta opción implica que omitiríamos la observación de las estadísticas resumen de la regresión y la discutiríamos en nuestro informe como un caso separado.</li>
<li>Crear una variable binaria para indicar la presencia de un valor atípico. Si se han identificado una o varias causas especiales para explicar un valor atípico, entonces estas causas podrían introducirse formalmente en el procedimiento de modelado mediante la introducción de una variable que indique la presencia (o ausencia) de estas causas. Este enfoque es similar a la eliminación de puntos, pero permite que el valor atípico se incluya formalmente en la formulación del modelo, de modo que, si surgen observaciones adicionales afectadas por las mismas causas, se puedan manejar de forma automática.</li>
</ul>
</div>
</div>
<div id="Sec533" class="section level3 hasAnchor" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Uso de los Residuales para Seleccionar Variables Explicativas<a href="C5VarSelect.html#Sec533" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Otra función importante del análisis de residuales es ayudar a identificar variables explicativas adicionales que puedan usarse para mejorar la formulación del modelo. Si hemos especificado el modelo correctamente, entonces los residuales deberían parecerse a errores aleatorios y no contener patrones discernibles. Por lo tanto, al comparar residuales con variables explicativas, no esperamos ninguna relación. Si detectamos una relación, esto sugiere la necesidad de controlar esta variable adicional. Esto se puede lograr introduciendo la variable adicional en el modelo de regresión.</p>
<p>Las relaciones entre los residuales y las variables explicativas pueden establecerse rápidamente utilizando estadísticas de correlación. Sin embargo, si una variable explicativa ya está incluida en el modelo de regresión, entonces la correlación entre los residuales y una variable explicativa será cero (ver Sección <a href="C5VarSelect.html#Sec5101">5.10.1</a> para la demostración algebraica). Es una buena idea reforzar esta correlación con un diagrama de dispersión. Un gráfico de residuales frente a variables explicativas no solo reforzará gráficamente la estadística de correlación, sino que también servirá para detectar posibles relaciones no lineales. Por ejemplo, una relación cuadrática puede detectarse utilizando un diagrama de dispersión, no una estadística de correlación.</p>
<p>Si detecta una relación entre los residuales de un ajuste de modelo preliminar y una variable explicativa adicional, introducir esta variable adicional no siempre mejorará la especificación de su modelo. La razón es que la variable adicional puede estar relacionada linealmente con las variables que ya están en el modelo. Si desea una garantía de que agregar una variable adicional mejorará su modelo, entonces construya un gráfico de variables añadidas (ver Sección 3.4.3).</p>
<p>En resumen, después de un ajuste preliminar del modelo, debe:</p>
<ul>
<li>Calcular estadísticas resumen y mostrar la distribución de los residuales (estandarizados) para identificar valores atípicos.</li>
<li>Calcular la correlación entre los residuales (estandarizados) y las variables explicativas adicionales para buscar relaciones lineales.</li>
<li>Crear gráficos de dispersión entre los residuales (estandarizados) y las variables explicativas adicionales para buscar relaciones no lineales.</li>
</ul>
<hr />
<p><strong>Ejemplo: Liquidez del Mercado de Valores.</strong> La decisión de un inversor de comprar una acción generalmente se toma teniendo en cuenta varios criterios. Primero, los inversores suelen buscar un alto rendimiento esperado. Un segundo criterio es el riesgo de una acción, que puede medirse mediante la variabilidad de los rendimientos. Tercero, muchos inversores están preocupados por el tiempo que están comprometiendo su capital con la compra de un valor. Muchas acciones de ingresos, como las de servicios públicos, devuelven regularmente partes de las inversiones de capital en forma de dividendos. Otras acciones, particularmente las de crecimiento, no devuelven nada hasta la venta del valor. Por lo tanto, la duración promedio de la inversión en un valor es otro criterio. Cuarto, a los inversores les preocupa la capacidad de vender la acción en cualquier momento que sea conveniente para ellos. Nos referimos a este cuarto criterio como la <em>liquidez</em> de la acción. Cuanto más líquida sea la acción, más fácil será venderla. Para medir la liquidez, en este estudio utilizamos el número de acciones negociadas en una bolsa durante un período de tiempo específico (llamado VOLUME). Estamos interesados en estudiar la relación entre el volumen y otras características financieras de una acción.</p>
<p>Comenzamos este estudio con 126 empresas cuyas opciones se negociaron el 3 de diciembre de 1984. Los datos de las acciones fueron obtenidos de Francis Emory Fitch, Inc. para el período del 3 de diciembre de 1984 al 28 de febrero de 1985. Para las variables de actividad comercial, examinamos:</p>
<ul>
<li>El volumen total de negociación de tres meses (VOLUME, en millones de acciones),</li>
<li>El número total de transacciones de tres meses (NTRAN), y</li>
<li>El tiempo promedio entre transacciones (AVGT, medido en minutos).</li>
</ul>
<p>Para las variables de tamaño de la empresa, utilizamos:</p>
<ul>
<li>El precio de apertura de la acción el 2 de enero de 1985 (PRICE),</li>
<li>El número de acciones en circulación el 31 de diciembre de 1984 (SHARE, en millones de acciones), y</li>
<li>El valor de mercado del capital (VALUE, en miles de millones de dólares) obtenido al tomar el producto de PRICE y SHARE.</li>
</ul>
<p>Finalmente, para el apalancamiento financiero, examinamos la relación deuda-capital (DEB_EQ) obtenida de la Cinta Industrial de Compustat y el manual de Moody’s. Los datos en SHARE se obtienen de la cinta mensual del Centro de Investigación en Precios de Seguridad (CRSP).</p>
<p>Después de examinar algunas estadísticas resumen preliminares de los datos, se eliminaron tres empresas porque tenían un volumen inusualmente alto o un precio elevado. Estas son Teledyne y Capital Cities Communication, cuyos precios eran más de cuatro veces el precio promedio de las demás empresas, y American Telephone and Telegraph, cuyo volumen total era más de siete veces el volumen total promedio de las demás empresas. Basado en una investigación adicional, cuyos detalles no se presentan aquí, estas empresas fueron eliminadas porque parecían representar circunstancias especiales que no deseábamos modelar.</p>
<p>La Tabla <a href="C5VarSelect.html#tab:Tab52">5.2</a> resume las estadísticas descriptivas basadas en las <span class="math inline">\(n = 123\)</span> empresas restantes. Por ejemplo, en la Tabla <a href="C5VarSelect.html#tab:Tab52">5.2</a>, vemos que el tiempo promedio entre transacciones es de aproximadamente cinco minutos y este tiempo varía desde un mínimo de menos de 1 minuto hasta un máximo de aproximadamente 20 minutos.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab52">Tabla 5.2: </span><strong>Estadísticas Resumen de las Variables de Liquidez de las Acciones</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Media
</th>
<th style="text-align:right;">
Mediana
</th>
<th style="text-align:right;">
Desviación Estándar
</th>
<th style="text-align:right;">
Mínimo
</th>
<th style="text-align:right;">
Máximo
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
VOLUME
</td>
<td style="text-align:right;width: 1.6cm; ">
13.423
</td>
<td style="text-align:right;width: 1.6cm; ">
11.556
</td>
<td style="text-align:right;width: 1.6cm; ">
10.632
</td>
<td style="text-align:right;width: 1.6cm; ">
0.658
</td>
<td style="text-align:right;width: 1.6cm; ">
64.572
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
AVGT
</td>
<td style="text-align:right;width: 1.6cm; ">
5.441
</td>
<td style="text-align:right;width: 1.6cm; ">
4.284
</td>
<td style="text-align:right;width: 1.6cm; ">
3.853
</td>
<td style="text-align:right;width: 1.6cm; ">
0.590
</td>
<td style="text-align:right;width: 1.6cm; ">
20.772
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
NTRAN
</td>
<td style="text-align:right;width: 1.6cm; ">
6436.000
</td>
<td style="text-align:right;width: 1.6cm; ">
5071.000
</td>
<td style="text-align:right;width: 1.6cm; ">
5310.000
</td>
<td style="text-align:right;width: 1.6cm; ">
999.000
</td>
<td style="text-align:right;width: 1.6cm; ">
36420.000
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
PRICE
</td>
<td style="text-align:right;width: 1.6cm; ">
38.800
</td>
<td style="text-align:right;width: 1.6cm; ">
34.380
</td>
<td style="text-align:right;width: 1.6cm; ">
21.370
</td>
<td style="text-align:right;width: 1.6cm; ">
9.120
</td>
<td style="text-align:right;width: 1.6cm; ">
122.380
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SHARE
</td>
<td style="text-align:right;width: 1.6cm; ">
94.730
</td>
<td style="text-align:right;width: 1.6cm; ">
53.830
</td>
<td style="text-align:right;width: 1.6cm; ">
115.100
</td>
<td style="text-align:right;width: 1.6cm; ">
6.740
</td>
<td style="text-align:right;width: 1.6cm; ">
783.050
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
VALUE
</td>
<td style="text-align:right;width: 1.6cm; ">
4.116
</td>
<td style="text-align:right;width: 1.6cm; ">
2.065
</td>
<td style="text-align:right;width: 1.6cm; ">
8.157
</td>
<td style="text-align:right;width: 1.6cm; ">
0.115
</td>
<td style="text-align:right;width: 1.6cm; ">
75.437
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
DEBEQ
</td>
<td style="text-align:right;width: 1.6cm; ">
2.697
</td>
<td style="text-align:right;width: 1.6cm; ">
1.105
</td>
<td style="text-align:right;width: 1.6cm; ">
6.509
</td>
<td style="text-align:right;width: 1.6cm; ">
0.185
</td>
<td style="text-align:right;width: 1.6cm; ">
53.628
</td>
</tr>
</tbody>
</table>
<p><strong>Fuente:</strong> Francis Emory Fitch, Inc., Standard &amp; Poor’s Compustat, y el Centro de Investigación de Precios de Valores de la Universidad de Chicago.</p>
<p>La Tabla <a href="C5VarSelect.html#tab:Tab53">5.3</a> reporta los coeficientes de correlación y la Figura <a href="C5VarSelect.html#fig:Fig52">5.2</a> proporciona la matriz de dispersión correspondiente. Si tienes conocimientos en finanzas, te resultará interesante notar que el apalancamiento financiero, medido por DEB_EQ, no parece estar relacionado con las otras variables. A partir del diagrama de dispersión y la matriz de correlación, vemos una fuerte relación entre VOLUME y el tamaño de la empresa, medido por SHARE y VALUE. Además, las tres variables de actividad de negociación, VOLUME, AVGT y NTRAN, están altamente relacionadas entre sí.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab53">Tabla 5.3: </span><strong>Matriz de Correlación de la Liquidez de las Acciones</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
AVGT
</th>
<th style="text-align:right;">
NTRAN
</th>
<th style="text-align:right;">
PRICE
</th>
<th style="text-align:right;">
SHARE
</th>
<th style="text-align:right;">
VALUE
</th>
<th style="text-align:right;">
DEB_EQ
</th>
<th style="text-align:right;">
VOLUME
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
AVGT
</td>
<td style="text-align:right;width: 1.4cm; ">
1.000
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.668
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.128
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.429
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.318
</td>
<td style="text-align:right;width: 1.4cm; ">
0.094
</td>
<td style="text-align:right;">
-0.674
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
NTRAN
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.668
</td>
<td style="text-align:right;width: 1.4cm; ">
1.000
</td>
<td style="text-align:right;width: 1.4cm; ">
0.190
</td>
<td style="text-align:right;width: 1.4cm; ">
0.817
</td>
<td style="text-align:right;width: 1.4cm; ">
0.760
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.092
</td>
<td style="text-align:right;">
0.913
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
PRICE
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.128
</td>
<td style="text-align:right;width: 1.4cm; ">
0.190
</td>
<td style="text-align:right;width: 1.4cm; ">
1.000
</td>
<td style="text-align:right;width: 1.4cm; ">
0.177
</td>
<td style="text-align:right;width: 1.4cm; ">
0.457
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.038
</td>
<td style="text-align:right;">
0.168
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SHARE
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.429
</td>
<td style="text-align:right;width: 1.4cm; ">
0.817
</td>
<td style="text-align:right;width: 1.4cm; ">
0.177
</td>
<td style="text-align:right;width: 1.4cm; ">
1.000
</td>
<td style="text-align:right;width: 1.4cm; ">
0.829
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.077
</td>
<td style="text-align:right;">
0.773
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
VALUE
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.318
</td>
<td style="text-align:right;width: 1.4cm; ">
0.760
</td>
<td style="text-align:right;width: 1.4cm; ">
0.457
</td>
<td style="text-align:right;width: 1.4cm; ">
0.829
</td>
<td style="text-align:right;width: 1.4cm; ">
1.000
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.077
</td>
<td style="text-align:right;">
0.702
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
DEB_EQ
</td>
<td style="text-align:right;width: 1.4cm; ">
0.094
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.092
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.038
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.077
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.077
</td>
<td style="text-align:right;width: 1.4cm; ">
1.000
</td>
<td style="text-align:right;">
-0.052
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
VOLUME
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.674
</td>
<td style="text-align:right;width: 1.4cm; ">
0.913
</td>
<td style="text-align:right;width: 1.4cm; ">
0.168
</td>
<td style="text-align:right;width: 1.4cm; ">
0.773
</td>
<td style="text-align:right;width: 1.4cm; ">
0.702
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.052
</td>
<td style="text-align:right;">
1.000
</td>
</tr>
</tbody>
</table>
<h5 style="text-align: center;">
<a id="displayCode.Tab52.Hide" href="javascript:togglecode('toggleCode.Tab52.Hide','displayCode.Tab52.Hide');"><i><strong>Código R para Producir las Tablas 5.2 y 5.3</strong></i></a>
</h5>
<div id="toggleCode.Tab52.Hide" style="display: none">
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="C5VarSelect.html#cb52-1" tabindex="-1"></a>liquidity <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;CSVData/Liquidity.csv&quot;</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb52-2"><a href="C5VarSelect.html#cb52-2" tabindex="-1"></a>varLiquid <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;AVGT&quot;</span>, <span class="st">&quot;NTRAN&quot;</span>, <span class="st">&quot;PRICE&quot;</span>, <span class="st">&quot;SHARE&quot;</span>, <span class="st">&quot;VALUE&quot;</span>, <span class="st">&quot;DEBEQ&quot;</span>, <span class="st">&quot;VOLUME&quot;</span>)</span>
<span id="cb52-3"><a href="C5VarSelect.html#cb52-3" tabindex="-1"></a>liquidMat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(liquidity[varLiquid])</span>
<span id="cb52-4"><a href="C5VarSelect.html#cb52-4" tabindex="-1"></a><span class="fu">names</span>(liquidMat)[<span class="fu">names</span>(liquidMat) <span class="sc">==</span> <span class="st">&quot;DEBEQ&quot;</span>] <span class="ot">&lt;-</span> <span class="st">&quot;DEB_EQ&quot;</span></span>
<span id="cb52-5"><a href="C5VarSelect.html#cb52-5" tabindex="-1"></a></span>
<span id="cb52-6"><a href="C5VarSelect.html#cb52-6" tabindex="-1"></a><span class="co">#  TABLA 5.2 ESTADÍSTICAS RESUMEN</span></span>
<span id="cb52-7"><a href="C5VarSelect.html#cb52-7" tabindex="-1"></a>BookSummStats <span class="ot">&lt;-</span> <span class="cf">function</span>(Xymat){</span>
<span id="cb52-8"><a href="C5VarSelect.html#cb52-8" tabindex="-1"></a>meanSummary <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, mean,  <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb52-9"><a href="C5VarSelect.html#cb52-9" tabindex="-1"></a>sdSummary   <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, sd,    <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb52-10"><a href="C5VarSelect.html#cb52-10" tabindex="-1"></a>minSummary  <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, min,   <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb52-11"><a href="C5VarSelect.html#cb52-11" tabindex="-1"></a>maxSummary  <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, max,   <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb52-12"><a href="C5VarSelect.html#cb52-12" tabindex="-1"></a>medSummary  <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, median,<span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb52-13"><a href="C5VarSelect.html#cb52-13" tabindex="-1"></a>tableMat  <span class="ot">&lt;-</span> <span class="fu">cbind</span>(meanSummary, medSummary, sdSummary, minSummary, maxSummary)</span>
<span id="cb52-14"><a href="C5VarSelect.html#cb52-14" tabindex="-1"></a><span class="fu">return</span>(tableMat)</span>
<span id="cb52-15"><a href="C5VarSelect.html#cb52-15" tabindex="-1"></a>}</span>
<span id="cb52-16"><a href="C5VarSelect.html#cb52-16" tabindex="-1"></a></span>
<span id="cb52-17"><a href="C5VarSelect.html#cb52-17" tabindex="-1"></a>liquidMat1 <span class="ot">&lt;-</span> liquidMat[,<span class="fu">c</span>(<span class="dv">7</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)]</span>
<span id="cb52-18"><a href="C5VarSelect.html#cb52-18" tabindex="-1"></a>tableMat  <span class="ot">&lt;-</span> <span class="fu">BookSummStats</span>(liquidMat1)</span>
<span id="cb52-19"><a href="C5VarSelect.html#cb52-19" tabindex="-1"></a><span class="fu">colnames</span>(tableMat)  <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Media&quot;</span>, <span class="st">&quot;Mediana&quot;</span>, <span class="st">&quot;Desviación Estándar&quot;</span>, </span>
<span id="cb52-20"><a href="C5VarSelect.html#cb52-20" tabindex="-1"></a>                         <span class="st">&quot;Mínimo&quot;</span>, <span class="st">&quot;Máximo&quot;</span>)</span>
<span id="cb52-21"><a href="C5VarSelect.html#cb52-21" tabindex="-1"></a><span class="fu">rownames</span>(tableMat)  <span class="ot">&lt;-</span> varLiquid[<span class="fu">c</span>(<span class="dv">7</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)]</span>
<span id="cb52-22"><a href="C5VarSelect.html#cb52-22" tabindex="-1"></a>tableMat       <span class="ot">&lt;-</span> <span class="fu">round</span>(tableMat, <span class="at">digits =</span> <span class="dv">3</span>)</span>
<span id="cb52-23"><a href="C5VarSelect.html#cb52-23" tabindex="-1"></a>tableMat[<span class="dv">3</span>,]   <span class="ot">&lt;-</span> <span class="fu">round</span>(tableMat[<span class="dv">3</span>,], <span class="at">digits =</span> <span class="dv">0</span>) </span>
<span id="cb52-24"><a href="C5VarSelect.html#cb52-24" tabindex="-1"></a>tableMat[<span class="dv">4</span><span class="sc">:</span><span class="dv">5</span>,] <span class="ot">&lt;-</span> <span class="fu">round</span>(tableMat[<span class="dv">4</span><span class="sc">:</span><span class="dv">5</span>,], <span class="at">digits =</span> <span class="dv">2</span>)</span>
<span id="cb52-25"><a href="C5VarSelect.html#cb52-25" tabindex="-1"></a></span>
<span id="cb52-26"><a href="C5VarSelect.html#cb52-26" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>tableMat, </span>
<span id="cb52-27"><a href="C5VarSelect.html#cb52-27" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Estadísticas Resumen de las Variables de Liquidez de las Acciones&#39;</span>, </span>
<span id="cb52-28"><a href="C5VarSelect.html#cb52-28" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>, <span class="at">Digits=</span><span class="dv">3</span>, <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,</span>
<span id="cb52-29"><a href="C5VarSelect.html#cb52-29" tabindex="-1"></a>         <span class="at">ColWidth =</span> ColWidth5) </span></code></pre></div>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="C5VarSelect.html#cb53-1" tabindex="-1"></a>cor_matrix <span class="ot">&lt;-</span> <span class="fu">cor</span>(liquidMat)</span>
<span id="cb53-2"><a href="C5VarSelect.html#cb53-2" tabindex="-1"></a><span class="fu">rownames</span>(cor_matrix) <span class="ot">&lt;-</span> <span class="fu">colnames</span>(cor_matrix) <span class="ot">&lt;-</span> </span>
<span id="cb53-3"><a href="C5VarSelect.html#cb53-3" tabindex="-1"></a>        <span class="fu">c</span>(<span class="st">&quot;AVGT&quot;</span>, <span class="st">&quot;NTRAN&quot;</span>, <span class="st">&quot;PRICE&quot;</span>, <span class="st">&quot;SHARE&quot;</span>, <span class="st">&quot;VALUE&quot;</span>, <span class="st">&quot;DEB_EQ&quot;</span>, <span class="st">&quot;VOLUME&quot;</span>)</span>
<span id="cb53-4"><a href="C5VarSelect.html#cb53-4" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>cor_matrix, </span>
<span id="cb53-5"><a href="C5VarSelect.html#cb53-5" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Matriz de Correlación de la Liquidez de las Acciones&#39;</span>, </span>
<span id="cb53-6"><a href="C5VarSelect.html#cb53-6" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>, <span class="at">Digits=</span><span class="dv">3</span>, <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,</span>
<span id="cb53-7"><a href="C5VarSelect.html#cb53-7" tabindex="-1"></a>         <span class="at">ColWidth =</span> ColWidth6) </span></code></pre></div>
</div>
<p>La Figura <a href="C5VarSelect.html#fig:Fig52">5.2</a> muestra que la variable AVGT está inversamente relacionada con VOLUME y NTRAN está inversamente relacionada con AVGT. De hecho, resultó que la correlación entre el tiempo promedio entre transacciones y el recíproco del número de transacciones fue del <span class="math inline">\(99.98\%!\)</span> Esto no es tan sorprendente cuando se piensa en cómo se podría calcular AVGT. Por ejemplo, en la Bolsa de Valores de Nueva York, el mercado está abierto de 10:00 A.M. a 4:00 P.M. Para cada acción en un día particular, el tiempo promedio entre transacciones multiplicado por el número de transacciones es casi igual a 360 minutos (= 6 horas). Por lo tanto, excepto por errores de redondeo porque las transacciones solo se registran al minuto más cercano, hay una relación lineal perfecta entre AVGT y el recíproco de NTRAN.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig52"></span>
<img src="RegressionMarkdown_files/figure-html/Fig52-1.png" alt="Matriz de dispersión para las variables de liquidez de las acciones. La variable del número de transacciones (NTRAN) parece estar fuertemente relacionada con el VOLUME de acciones negociadas e inversamente relacionada con AVGT." width="100%" />
<p class="caption">
Figura 5.2: <strong>Matriz de dispersión para las variables de liquidez de las acciones.</strong> La variable del número de transacciones (NTRAN) parece estar fuertemente relacionada con el VOLUME de acciones negociadas e inversamente relacionada con AVGT.
</p>
</div>
<p>Para comenzar a entender la medida de liquidez VOLUME, primero ajustamos un modelo de regresión utilizando NTRAN como una variable explicativa. El modelo de regresión ajustado es:</p>
<p><span class="math display">\[
\small{
\begin{array}{lcc}
\text{VOLUME} &amp;= 1.65 &amp;+ 0.00183 \text{ NTRAN} \\
\text{errores estándar} &amp; (0.6173) &amp; (0.000074)
\end{array}
}
\]</span></p>
<p>con <span class="math inline">\(R^2 = 83.4\%\)</span> y <span class="math inline">\(s = 4.35\)</span>. Note que el cociente <span class="math inline">\(t\)</span> para la pendiente asociada con NTRAN es</p>
<p><span class="math display">\[
t(b_1) = \frac{b_1}{se(b_1)} = \frac{0.00183}{0.000074} = 24.7
\]</span></p>
<p>indicando una fuerte significancia estadística. Los residuos se calcularon utilizando este modelo estimado. Para ver si los residuos están relacionados con otras variables explicativas, la Tabla <a href="C5VarSelect.html#tab:Tab54">5.4</a> muestra las correlaciones.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab54">Tabla 5.4: </span><strong>Primera Tabla de Correlaciones</strong>
</caption>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Variable
</td>
<td style="text-align:center;width: 3cm; ">
AVGT
</td>
<td style="text-align:center;width: 3cm; ">
PRICE
</td>
<td style="text-align:center;width: 3cm; ">
SHARE
</td>
<td style="text-align:center;width: 3cm; ">
VALUE
</td>
<td style="text-align:center;width: 3cm; ">
DEB_EQ
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
RESID
</td>
<td style="text-align:center;width: 3cm; ">
-0.159
</td>
<td style="text-align:center;width: 3cm; ">
-0.014
</td>
<td style="text-align:center;width: 3cm; ">
0.064
</td>
<td style="text-align:center;width: 3cm; ">
0.018
</td>
<td style="text-align:center;width: 3cm; ">
0.078
</td>
</tr>
</tbody>
</table>
<p><em>Nota</em>: Los residuos se crearon a partir de una regresión de VOLUME sobre NTRAN.</p>
<p>La correlación entre el residuo y AVGT y el diagrama de dispersión (no mostrado aquí) indica que puede haber alguna información en la variable AVGT en el residuo. Por lo tanto, parece razonable usar AVGT directamente en el modelo de regresión. Recuerde que estamos interpretando el residuo como el valor de VOLUME habiendo controlado el efecto de NTRAN.</p>
<p>A continuación, ajustamos un modelo de regresión utilizando NTRAN y AVGT como variables explicativas. El modelo de regresión ajustado es:</p>
<p><span class="math display">\[
\small{
\begin{array}{lccc}
\text{VOLUME} &amp;= 4.41 &amp;- 0.322 \text{ AVGT} &amp;+ 0.00167 \text{ NTRAN} \\
\text{errores estándar} &amp; (1.30)&amp; (0.135)&amp; (0.000098)
\end{array}
}
\]</span></p>
<p>con <span class="math inline">\(R^2 = 84.2\%\)</span> y <span class="math inline">\(s = 4.26\)</span>. Basado en el cociente <span class="math inline">\(t\)</span> para AVGT, <span class="math inline">\(t(b_{AVGT}) = \frac{-0.322}{0.135} = -2.39\)</span>, parece que AVGT es una variable explicativa útil en el modelo. Note también que <span class="math inline">\(s\)</span> ha disminuido, lo que indica que <span class="math inline">\(R_a^2\)</span> ha aumentado.</p>
<p>La Tabla <a href="C5VarSelect.html#tab:Tab55">5.5</a> proporciona correlaciones entre los residuos del modelo y otras posibles variables explicativas e indica que no parece haber mucha información adicional en las variables explicativas. Esto se reafirma por la tabla correspondiente de diagramas de dispersión en la Figura <a href="C5VarSelect.html#fig:Fig53">5.3</a>. Los histogramas en la Figura <a href="C5VarSelect.html#fig:Fig53">5.3</a> sugieren que, aunque la distribución de los residuos es bastante simétrica, la distribución de cada variable explicativa está sesgada. Debido a esto, se exploraron transformaciones de las variables explicativas. Esta línea de pensamiento no proporcionó mejoras reales y, por lo tanto, no se proporcionan detalles aquí.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig53"></span>
<img src="RegressionMarkdown_files/figure-html/Fig53-1.png" alt="Matriz de dispersión de los residuos de la regresión de VOLUME sobre NTRAN y AVGT en el eje vertical y las variables predictoras restantes en los ejes horizontales." width="100%" />
<p class="caption">
Figura 5.3: <strong>Matriz de dispersión de los residuos de la regresión de VOLUME sobre NTRAN y AVGT en el eje vertical y las variables predictoras restantes en los ejes horizontales.</strong>
</p>
</div>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab55">Tabla 5.5: </span><strong>Segunda Tabla de Correlaciones</strong>
</caption>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Variable
</td>
<td style="text-align:center;width: 3cm; ">
PRICE
</td>
<td style="text-align:center;width: 3cm; ">
SHARE
</td>
<td style="text-align:center;width: 3cm; ">
VALUE
</td>
<td style="text-align:center;width: 3cm; ">
DEB_EQ
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
RESID
</td>
<td style="text-align:center;width: 3cm; ">
-0.015
</td>
<td style="text-align:center;width: 3cm; ">
0.100
</td>
<td style="text-align:center;width: 3cm; ">
0.074
</td>
<td style="text-align:center;width: 3cm; ">
0.089
</td>
</tr>
</tbody>
</table>
<p><em>Nota</em>: Los residuos se crearon a partir de una regresión de VOLUME sobre NTRAN y AVGT.</p>
</div>
</div>
<div id="Sec54" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Puntos Influyentes<a href="C5VarSelect.html#Sec54" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>No todos los puntos son creados iguales; en esta sección veremos que ciertas observaciones pueden tener un efecto desproporcionado en el ajuste general de la regresión. A estos puntos los llamaremos “influyentes.” Esto no es tan sorprendente; ya hemos visto que las estimaciones de los coeficientes de regresión son sumas <em>ponderadas</em> de respuestas (ver Sección 3.2.4). Algunas observaciones tienen pesos mayores que otras y, por lo tanto, tienen una mayor influencia en las estimaciones de los coeficientes de regresión. Por supuesto, el hecho de que una observación sea influyente no significa que sea incorrecta o que su impacto en el modelo sea engañoso. Como analistas, simplemente nos gustaría saber si nuestro modelo ajustado es sensible a cambios leves, como la eliminación de un solo punto, para sentirnos cómodos al generalizar nuestros resultados de la muestra a una población más grande.</p>
<p>Para evaluar la influencia, pensamos en observaciones como respuestas inusuales, dadas un conjunto de variables explicativas, o que tienen un conjunto inusual de valores de variables explicativas. Ya hemos visto en la Sección <a href="C5VarSelect.html#Sec53">5.3</a> cómo evaluar respuestas inusuales utilizando residuos. Esta sección se centra en conjuntos inusuales de valores de variables explicativas.</p>
<div id="Sec541" class="section level3 hasAnchor" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Apalancamiento<a href="C5VarSelect.html#Sec541" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Introdujimos este tema en la Sección 2.6, donde llamamos a una observación con una variable explicativa inusual un “punto de alto apalancamiento.” Con más de una variable explicativa, determinar si una observación es un punto de alto apalancamiento no es tan sencillo. Por ejemplo, es posible que una observación “no sea inusual” para ninguna variable individual y, sin embargo, sea inusual en el espacio de variables explicativas. Considere el conjunto de datos ficticio representado en la Figura <a href="C5VarSelect.html#fig:Fig54">5.4</a>. Visualmente, parece claro que el punto marcado en la esquina superior derecha es inusual. Sin embargo, no es inusual cuando se examina el histograma de <span class="math inline">\(x_1\)</span> o de <span class="math inline">\(x_2\)</span>. Es inusual solo cuando se consideran las variables explicativas de manera conjunta.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig54"></span>
<img src="RegressionMarkdown_files/figure-html/Fig54-1.png" alt="El elipsoide representa la mayoría de los datos. La flecha marca un punto inusual." width="60%" />
<p class="caption">
Figura 5.4: <strong>El elipsoide representa la mayoría de los datos.</strong> La flecha marca un punto inusual.
</p>
</div>
<p>Para dos variables explicativas, esto es evidente al examinar los datos gráficamente. Debido a que es difícil examinar gráficamente los datos con más de dos variables explicativas, necesitamos un procedimiento numérico para evaluar el apalancamiento.</p>
<p>Para definir el concepto de apalancamiento en la regresión lineal múltiple, utilizamos algunos conceptos de álgebra matricial. Específicamente, en la Sección 3.1 mostramos que el vector de coeficientes de regresión de mínimos cuadrados se puede calcular usando
<span class="math inline">\(\mathbf{b} = (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{y}\)</span>.
Así, podemos expresar el vector de valores ajustados <span class="math inline">\(\hat{\mathbf{y}} = (\hat{y}_1, \ldots, \hat{y}_n)^{\prime}\)</span> como</p>
<p><span class="math display" id="eq:eq52">\[\begin{equation}
\mathbf{\hat{y}} = \mathbf{Xb} .
\tag{5.2}
\end{equation}\]</span></p>
<p>De manera similar, el vector de residuos es el vector de respuesta menos el vector de valores ajustados, es decir, <span class="math inline">\(\mathbf{e} = \mathbf{y - \hat{y}}\)</span>.</p>
<p>A partir de la expresión para los coeficientes de regresión <span class="math inline">\(\mathbf{b}\)</span> en la ecuación (3.4), tenemos</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{X} (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{y}
\]</span></p>
<p>Esta ecuación sugiere definir</p>
<p><span class="math display">\[
\mathbf{H} = \mathbf{X} (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime}
\]</span></p>
<p>de modo que</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{Hy}
\]</span></p>
<p>A partir de esto, se dice que la matriz <span class="math inline">\(\mathbf{H}\)</span> <em>proyecta</em> el vector de respuestas <span class="math inline">\(\mathbf{y}\)</span> en el vector de valores ajustados <span class="math inline">\(\mathbf{\hat{y}}\)</span>. Alternativamente, puede pensar en <span class="math inline">\(\mathbf{H}\)</span> como la matriz que pone el “sombrero,” o circunflejo, en <span class="math inline">\(\mathbf{y}\)</span>. A partir de la <span class="math inline">\(i\)</span>-ésima fila de la ecuación vectorial <span class="math inline">\(\mathbf{\hat{y}} = \mathbf{Hy}\)</span>, tenemos</p>
<p><span class="math display">\[
\hat{y}_i = h_{i1} y_1 + h_{i2} y_2 + \cdots + h_{ii} y_i + \cdots + h_{in} y_n
\]</span></p>
<p>Aquí, <span class="math inline">\(h_{ij}\)</span> es el número en la <span class="math inline">\(i\)</span>-ésima fila y <span class="math inline">\(j\)</span>-ésima columna de <span class="math inline">\(\mathbf{H}\)</span>. A partir de esta expresión, vemos que cuanto mayor sea <span class="math inline">\(h_{ii}\)</span>, mayor será el efecto que la <span class="math inline">\(i\)</span>-ésima respuesta <span class="math inline">\((y_i)\)</span> tiene en el valor ajustado correspondiente <span class="math inline">\((\hat{y}_i)\)</span>. Por lo tanto, llamamos a <span class="math inline">\(h_{ii}\)</span> el <em>apalancamiento</em> para la <span class="math inline">\(i\)</span>-ésima observación. Debido a que <span class="math inline">\(h_{ii}\)</span> es el elemento diagonal <span class="math inline">\(i\)</span>-ésimo de <span class="math inline">\(\mathbf{H}\)</span>, una expresión directa para <span class="math inline">\(h_{ii}\)</span> es</p>
<p><span class="math display" id="eq:eq53">\[\begin{equation}
h_{ii} = \mathbf{x}_i^{\prime} (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{x}_i
\tag{5.3}
\end{equation}\]</span></p>
<p>donde <span class="math inline">\(\mathbf{x}_i = (x_{i0}, x_{i1}, \ldots, x_{ik})^{\prime}\)</span>. Debido a que los valores de <span class="math inline">\(h_{ii}\)</span> se calculan en base a las variables explicativas, los valores de la variable de respuesta no afectan el cálculo de los apalancamientos.</p>
<p>Los valores altos de apalancamiento indican que una observación puede tener un efecto desproporcionado en el ajuste, esencialmente porque está distante de las otras observaciones (al observar el espacio de variables explicativas). ¿Qué tan grande es grande? Existen algunas pautas de álgebra matricial, donde tenemos que</p>
<p><span class="math display">\[
\frac{1}{n} \leq h_{ii} \leq 1
\]</span></p>
<p>y</p>
<p><span class="math display">\[
\bar{h} = \frac{1}{n} \sum_{i=1}^{n} h_{ii} = \frac{k+1}{n}.
\]</span></p>
<p>Por lo tanto, cada apalancamiento está limitado por <span class="math inline">\(n^{-1}\)</span> y <span class="math inline">\(1\)</span>, y el apalancamiento promedio es igual al número de coeficientes de regresión dividido por el número de observaciones. A partir de estos y argumentos relacionados, utilizamos una convención ampliamente adoptada y declaramos que una observación es un <em>punto de alto apalancamiento</em> si el apalancamiento supera tres veces el promedio, es decir, si</p>
<p><span class="math display">\[
h_{ii} &gt; \frac{3(k+1)}{n}.
\]</span></p>
<p>Una vez identificados los puntos de alto apalancamiento, al igual que con los valores atípicos, es importante que el analista busque causas especiales que puedan haber producido estos puntos inusuales. Para ilustrar, en la Sección 2.7 identificamos el colapso del mercado de 1987 como la razón detrás del punto de alto apalancamiento. Además, los puntos de alto apalancamiento a menudo se deben a errores administrativos al codificar los datos, que pueden o no ser fáciles de rectificar. En general, las opciones para manejar puntos de alto apalancamiento son similares a las disponibles para tratar con valores atípicos.</p>
<div class="blackbox">
<p><em>Opciones para Manejar Puntos de Alto Apalancamiento</em></p>
<ol style="list-style-type: decimal">
<li>Incluir la observación en las estadísticas resumidas pero comentar sobre su efecto. Por ejemplo, una observación puede apenas superar un límite y su efecto puede no ser importante en el análisis general.</li>
<li>Eliminar la observación del conjunto de datos. Nuevamente, la justificación básica para esta acción es que se considera que la observación no es representativa de una población más grande. Una opción intermedia entre (1) y (2) es presentar el análisis tanto con como sin el punto de alto apalancamiento. De esta manera, se demuestra completamente el impacto del punto y el lector de su análisis puede decidir cuál opción es más adecuada.</li>
<li>Elegir otra variable para representar la información. En algunos casos, otra variable explicativa estará disponible para servir como reemplazo. Por ejemplo, en un ejemplo de alquileres de apartamentos, podríamos usar el número de habitaciones para reemplazar una variable de metros cuadrados como medida del tamaño del apartamento. Aunque los metros cuadrados de un apartamento pueden ser inusualmente grandes, lo que lo convierte en un punto de alto apalancamiento, puede tener una, dos o tres habitaciones, dependiendo de la muestra examinada.</li>
<li>Usar una transformación no lineal de una variable explicativa. Para ilustrar, con nuestro ejemplo de Liquidez de Acciones en la Sección <a href="C5VarSelect.html#Sec553">5.5.3</a>, podemos transformar la variable continua de razón deuda a capital DEB_EQ en una variable que indique la presencia de “alta” razón deuda a capital. Por ejemplo, podríamos codificar DE_IND = 1 si DEB_EQ &gt; 5 y DE_IND = 0 si DEB_EQ ≤ 5. Con esta recodificación, aún conservamos información sobre el apalancamiento financiero de una empresa sin permitir que los valores grandes de DEB_EQ influyan en el ajuste de la regresión.</li>
</ol>
</div>
<p>Algunos analistas usan metodologías de estimación “robustas” como alternativa a la estimación de mínimos cuadrados. La idea básica de estas técnicas es reducir el efecto de cualquier observación en particular. Estas técnicas son útiles para reducir el efecto tanto de valores atípicos como de puntos de alto apalancamiento. Esta táctica puede considerarse intermedia entre un procedimiento extremo, ignorando el efecto de puntos inusuales, y otro extremo, dando plena credibilidad a los puntos inusuales al eliminarlos del conjunto de datos. La palabra <em>robusto</em> sugiere que estas metodologías de estimación son “saludables” incluso cuando son atacadas por una observación ocasionalmente mala (un germen). Hemos visto que esto no es cierto para la estimación de mínimos cuadrados.</p>
</div>
<div id="Sec542" class="section level3 hasAnchor" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> Distancia de Cook<a href="C5VarSelect.html#Sec542" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Para cuantificar la influencia de un punto, una medida que considera tanto las variables de respuesta como las explicativas es la <em>Distancia de Cook</em>. Esta distancia, <span class="math inline">\(D_i\)</span>, se define como</p>
<p><span class="math display" id="eq:eq54">\[\begin{equation}
\begin{array}{ll}
D_i &amp;= \frac{\sum_{j=1}^{n} (\hat{y}_j - \hat{y}_{j(i)})^2}{(k+1) s^2} \tag{5.4} \\
&amp;= \left( \frac{e_i}{se(e_i)} \right)^2 \frac{h_{ii}}{(k+1)(1 - h_{ii})}.
\end{array}
\end{equation}\]</span></p>
<p>La primera expresión proporciona una definición. Aquí, <span class="math inline">\(\hat{y}_{j(i)}\)</span> es la predicción de la <span class="math inline">\(j\)</span>-ésima observación, calculada excluyendo la <span class="math inline">\(i\)</span>-ésima observación del ajuste de regresión. Para medir el impacto de la <span class="math inline">\(i\)</span>-ésima observación, comparamos los valores ajustados con y sin la <span class="math inline">\(i\)</span>-ésima observación. Cada diferencia se eleva al cuadrado y se suma en todas las observaciones para resumir el impacto.</p>
<p>La segunda ecuación proporciona otra interpretación de la distancia <span class="math inline">\(D_i\)</span>. La primera parte, <span class="math inline">\(\left( \frac{e_i}{se(e_i)} \right)^2\)</span>, es el cuadrado del residuo estandarizado <span class="math inline">\(i\)</span>-ésimo. La segunda parte, <span class="math inline">\(\frac{h_{ii}}{(k+1)(1 - h_{ii})}\)</span>, se atribuye únicamente al apalancamiento. Así, la distancia <span class="math inline">\(D_i\)</span> se compone de una medida para valores atípicos multiplicada por una medida de apalancamiento. De esta manera, la distancia de Cook tiene en cuenta tanto las variables de respuesta como las explicativas. La Sección <a href="C5VarSelect.html#Sec5103">5.10.3</a> establece la validez de la ecuación <a href="C5VarSelect.html#eq:eq54">(5.4)</a>.</p>
<p>Para tener una idea del tamaño esperado de <span class="math inline">\(D_i\)</span> para un punto que no es inusual, recuerde que esperamos que los residuos estandarizados sean aproximadamente uno y que el apalancamiento <span class="math inline">\(h_{ii}\)</span> sea aproximadamente <span class="math inline">\(\frac{k+1}{n}\)</span>. Por lo tanto, anticipamos que <span class="math inline">\(D_i\)</span> debería ser aproximadamente <span class="math inline">\(\frac{1}{n}\)</span>. Otra regla general es comparar <span class="math inline">\(D_i\)</span> con una distribución <span class="math inline">\(F\)</span> con <span class="math inline">\(df_1 = k+1\)</span> y <span class="math inline">\(df_2 = n - (k+1)\)</span> grados de libertad. Los valores de <span class="math inline">\(D_i\)</span> que son grandes en comparación con esta distribución merecen atención.</p>
<hr />
<p><strong>Ejemplo: Valores Atípicos y Puntos de Alto Apalancamiento - Continuación.</strong> Para ilustrar, volvemos a nuestro ejemplo de la Sección 2.6. En este ejemplo, consideramos 19 puntos “buenos” o base, más cada uno de los tres tipos de puntos inusuales, etiquetados como A, B y C. La Tabla <a href="C5VarSelect.html#tab:Tab56">5.6</a> resume los cálculos.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab56">Tabla 5.6: </span><strong>Medidas de Tres Tipos de Puntos Inusuales</strong>
</caption>
<thead>
<tr>
<th style="text-align:center;">
Observation
</th>
<th style="text-align:center;">
Standardized Residual <span class="math inline">\(e / se(e)\)</span>
</th>
<th style="text-align:center;">
Leverage <span class="math inline">\(h\)</span>
</th>
<th style="text-align:center;">
Cook’s Distance <span class="math inline">\(D\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
A
</td>
<td style="text-align:center;width: 1.8cm; ">
4.00
</td>
<td style="text-align:center;width: 1.8cm; ">
0.067
</td>
<td style="text-align:center;width: 1.8cm; ">
0.577
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
B
</td>
<td style="text-align:center;width: 1.8cm; ">
0.77
</td>
<td style="text-align:center;width: 1.8cm; ">
0.550
</td>
<td style="text-align:center;width: 1.8cm; ">
0.363
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
C
</td>
<td style="text-align:center;width: 1.8cm; ">
-4.01
</td>
<td style="text-align:center;width: 1.8cm; ">
0.550
</td>
<td style="text-align:center;width: 1.8cm; ">
9.832
</td>
</tr>
</tbody>
</table>
<p>Como se mencionó en la Sección 2.6, de la columna de residuos estandarizados vemos que tanto los puntos A como C son valores atípicos. Para juzgar el tamaño de los apalancamientos, dado que hay <span class="math inline">\(n=20\)</span> puntos, los apalancamientos están limitados por 0.05 y 1.00, con el apalancamiento promedio siendo <span class="math inline">\(\bar{h} = \frac{2}{20} = 0.10\)</span>. Usando 0.3 (<span class="math inline">\(= 3 \times \bar{h}\)</span>) como un umbral, tanto los puntos B como C son puntos de alto apalancamiento. Nótese que sus valores son los mismos. Esto se debe a que, según la Figura 2.7, los valores de las variables explicativas son los mismos y solo la variable de respuesta ha cambiado. La columna de la distancia de Cook captura ambos tipos de comportamiento inusual. Dado que el valor típico de <span class="math inline">\(D_i\)</span> es <span class="math inline">\(\frac{1}{n}\)</span> o 0.05, la distancia de Cook proporciona una estadística para alertarnos de que cada punto es inusual en un aspecto u otro. En particular, el punto C tiene un <span class="math inline">\(D_i\)</span> muy grande, lo que refleja el hecho de que es tanto un valor atípico como un punto de alto apalancamiento. El percentil 95 de una distribución <span class="math inline">\(F\)</span> con <span class="math inline">\(df_1 = 2\)</span> y <span class="math inline">\(df_2 = 18\)</span> es 3.555. El hecho de que el punto C tenga un valor de <span class="math inline">\(D_i\)</span> que supera con creces este umbral indica la influencia sustancial de este punto.</p>
<hr />
</div>
</div>
<div id="Sec55" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Colinealidad<a href="C5VarSelect.html#Sec55" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="Sec551" class="section level3 hasAnchor" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> ¿Qué es la Colinealidad?<a href="C5VarSelect.html#Sec551" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Colinealidad</em>, o <em>multicolinealidad</em>, ocurre cuando una variable explicativa es, o casi es, una combinación lineal de las otras variables explicativas. Intuitivamente, con datos colineales, es útil pensar en las variables explicativas como altamente correlacionadas entre sí. Si una variable explicativa es colineal, surge la pregunta de si es redundante, es decir, si la variable proporciona poca información adicional sobre la información que ya está en las otras variables explicativas. Las preguntas son: ¿Es importante la colinealidad? Si es así, ¿cómo afecta el ajuste de nuestro modelo y cómo la detectamos? Para abordar la primera pregunta, considere un ejemplo algo patológico.</p>
<hr />
<p><strong>Ejemplo: Variables Explicativas Perfectamente Correlacionadas.</strong> Joe Finance fue solicitado para ajustar el modelo <span class="math inline">\(\mathrm{E} ~y = \beta_0 + \beta_1 x_1 + \beta_2 x_2\)</span> a un conjunto de datos. Su modelo ajustado resultante fue <span class="math inline">\(\hat{y} = -87 + x_1 + 18 x_2.\)</span> El conjunto de datos considerado es:</p>
<p><span class="math display">\[
\begin{array}{l|cccc} \hline
i &amp; 1 &amp; 2 &amp; 3 &amp; 4 \\ \hline
y_i &amp; 23 &amp; 83 &amp; 63 &amp; 103 \\
x_{i1} &amp; 2 &amp; 8  &amp;6 &amp; 10 \\
x_{i2} &amp; 6 &amp; 9 &amp; 8 &amp; 10 \\ \hline
\end{array}
\]</span></p>
<p>Joe verificó el ajuste para cada observación. Joe estaba muy contento porque ajustó los datos perfectamente. Por ejemplo, para la tercera observación, el valor ajustado es
<span class="math inline">\(\hat{y}_3 = -87 + 6 + 18 \times 8 = 63\)</span>, que es igual a la tercera respuesta, <span class="math inline">\(y_3\)</span>. Debido a que la respuesta es igual al valor ajustado, el residuo es cero. Puede verificar que esto es cierto para cada observación, y así, el <span class="math inline">\(R^2\)</span> resultó ser <span class="math inline">\(100\%\)</span>.</p>
<p>Sin embargo, Jane Actuary llegó y ajustó el modelo
<span class="math inline">\(\hat{y} = -7 + 9 x_1 + 2 x_2.\)</span> Jane realizó las mismas comprobaciones cuidadosas que Joe hizo y también obtuvo un ajuste perfecto (<span class="math inline">\(R^2 = 1\)</span>). ¿Quién tiene razón?</p>
<p>La respuesta es ambos y ninguno. De hecho, hay un número infinito de ajustes. Esto se debe a la relación perfecta
<span class="math inline">\(x_2 = 5 + \frac{x_1}{2}\)</span> entre las dos variables explicativas.</p>
<hr />
<p>Este ejemplo ilustra algunos hechos importantes sobre la colinealidad.</p>
<div class="blackbox">
<p><strong>Hechos sobre la Colinealidad</strong></p>
<ul>
<li>La colinealidad no nos impide obtener buenos ajustes ni hacer predicciones de nuevas observaciones. Nótese que en el ejemplo anterior obtuvimos ajustes perfectos.</li>
<li>Las estimaciones de las varianzas de error y, por lo tanto, las pruebas de adecuación del modelo, siguen siendo fiables.</li>
<li>En casos de colinealidad severa, los errores estándar de los coeficientes de regresión individuales son mayores que en los casos en que, ceteris paribus, no existe colinealidad severa. Con errores estándar grandes, los coeficientes de regresión individuales pueden no ser significativos. Además, debido a que un error estándar grande significa que el correspondiente cociente <span class="math inline">\(t\)</span> es pequeño, es difícil detectar la importancia de una variable.</li>
</ul>
</div>
<p>Para detectar la colinealidad, comience con una matriz de coeficientes de correlación de las variables explicativas. Esta matriz es fácil de crear, fácil de interpretar y captura rápidamente las relaciones lineales entre pares de variables. Una matriz de diagramas de dispersión proporciona un refuerzo visual de las estadísticas resumidas en la matriz de correlación.</p>
</div>
<div id="Sec552" class="section level3 hasAnchor" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> Factores de Inflación de Varianza<a href="C5VarSelect.html#Sec552" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Las matrices de correlación y diagramas de dispersión capturan solo las relaciones entre pares de variables. Para capturar relaciones más complejas entre varias variables, introducimos el <em>factor de inflación de varianza (VIF)</em>. Para definir un <em>VIF</em>, suponga que el conjunto de variables explicativas está etiquetado como <span class="math inline">\(x_1, x_2, \ldots, x_{k}\)</span>. Ahora, ejecute la regresión utilizando <span class="math inline">\(x_j\)</span> como la “respuesta” y los otros <span class="math inline">\(x\)</span> (<span class="math inline">\(x_1, x_2, \ldots, x_{j-1}, x_{j+1}, \ldots, x_{k}\)</span>) como las variables explicativas. Denote el coeficiente de determinación de esta regresión por <span class="math inline">\(R_j^2\)</span>. Interpretamos <span class="math inline">\(R_j = \sqrt{R_j^2}\)</span> como el coeficiente de correlación múltiple entre <span class="math inline">\(x_j\)</span> y las combinaciones lineales de los otros <span class="math inline">\(x\)</span>. A partir de este coeficiente de determinación, definimos el factor de inflación de varianza</p>
<p><span class="math display">\[
VIF_j = \frac{1}{1 - R_j^2}, \text{ para } j = 1, 2, \ldots, k.
\]</span></p>
<p>Un mayor <span class="math inline">\(R_j^2\)</span> resulta en un mayor <span class="math inline">\(VIF_j\)</span>; esto significa una mayor colinealidad entre <span class="math inline">\(x_j\)</span> y los otros <span class="math inline">\(x\)</span>. Ahora, <span class="math inline">\(R_j^2\)</span> por sí solo es suficiente para capturar la relación lineal de interés. Sin embargo, usamos <span class="math inline">\(VIF_j\)</span> en lugar de <span class="math inline">\(R_j^2\)</span> como nuestra medida de colinealidad debido a la relación algebraica</p>
<p><span class="math display" id="eq:eq55">\[\begin{equation}
se(b_j) = s \frac{\sqrt{VIF_j}}{s_{x_j} \sqrt{n - 1}}.
\tag{5.5}
\end{equation}\]</span></p>
<p>Aquí, <span class="math inline">\(se(b_j)\)</span> y <span class="math inline">\(s\)</span> son errores estándar y la desviación estándar residual de un ajuste completo de regresión de <span class="math inline">\(y\)</span> sobre <span class="math inline">\(x_1, \ldots, x_{k}\)</span>. Además, <span class="math inline">\(s_{x_j} = \sqrt{(n - 1)^{-1} \sum_{i=1}^{n} (x_{ij} - \bar{x}_j)^2 }\)</span> es la desviación estándar muestral de la <span class="math inline">\(j\)</span>-ésima variable <span class="math inline">\(x_j\)</span>. La Sección <a href="C5VarSelect.html#Sec5103">5.10.3</a> proporciona una verificación de la ecuación <a href="C5VarSelect.html#eq:eq55">(5.5)</a>.</p>
<p>Así, un mayor <span class="math inline">\(VIF_j\)</span> resulta en un mayor error estándar asociado con la pendiente <span class="math inline">\(j\)</span>-ésima, <span class="math inline">\(b_j\)</span>. Recuerde que <span class="math inline">\(se(b_j)\)</span> es <span class="math inline">\(s\)</span> veces la raíz cuadrada del <span class="math inline">\((j+1)\)</span>-ésimo elemento diagonal de <span class="math inline">\((\mathbf{X}^{\prime} \mathbf{X})^{-1}\)</span>. La idea es que cuando ocurre colinealidad, la matriz <span class="math inline">\(\mathbf{X}^{\prime} \mathbf{X}\)</span> tiene propiedades similares al número cero. Cuando intentamos calcular la inversa de <span class="math inline">\(\mathbf{X}^{\prime} \mathbf{X}\)</span>, esto es análogo a dividir por cero en números escalares. Como regla general, cuando <span class="math inline">\(VIF_j\)</span> supera 10 (lo cual es equivalente a <span class="math inline">\(R_j^2 &gt; 90\%\)</span>), decimos que existe colinealidad severa. Esto puede indicar la necesidad de acción. <em>Tolerancia</em>, definida como el recíproco del factor de inflación de varianza, es otra medida de colinealidad utilizada por algunos analistas.</p>
<p>Por ejemplo, con <span class="math inline">\(k = 2\)</span> variables explicativas en el modelo, entonces <span class="math inline">\(R_1^2\)</span> es la correlación cuadrada entre las dos variables explicativas, digamos <span class="math inline">\(r_{12}^2\)</span>. Entonces, a partir de la ecuación anterior, tenemos que</p>
<p><span class="math display">\[
se(b_j) = s \left(s_{x_j} \sqrt{n - 1} \right)^{-1} \left(1 - r_{12}^2 \right)^{-1/2}, \text{ para } j = 1, 2.
\]</span></p>
<p>A medida que la correlación se acerca a uno en valor absoluto, <span class="math inline">\(|r_{12}| \rightarrow 1\)</span>, entonces el error estándar se vuelve grande, lo que significa que el estadístico <span class="math inline">\(t\)</span> correspondiente se vuelve pequeño. En resumen, un alto <span class="math inline">\(VIF\)</span> puede significar pequeños estadísticos <span class="math inline">\(t\)</span> a pesar de que las variables sean importantes. Además, se puede verificar que la correlación entre <span class="math inline">\(b_1\)</span> y <span class="math inline">\(b_2\)</span> es <span class="math inline">\(-r_{12}\)</span>, indicando que las estimaciones de los coeficientes están altamente correlacionadas.</p>
<p><strong>Ejemplo: Liquidez del Mercado de Valores - Continuación.</strong> Como ejemplo, considere una regresión de VOLUME sobre PRICE, SHARE y VALUE. A diferencia de las variables explicativas consideradas en la Sección <a href="C5VarSelect.html#Sec553">5.5.3</a>, estas tres variables explicativas no son medidas de actividad de trading. A partir de un ajuste de regresión, tenemos <span class="math inline">\(R^2 = 61\%\)</span> y <span class="math inline">\(s = 6.72\)</span>. Las estadísticas asociadas con los coeficientes de regresión están en la Tabla <a href="C5VarSelect.html#tab:Tab57">5.7</a>.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab57">Tabla 5.7: </span><strong>Estadísticas de una regresión de VOLUME sobre PRICE, SHARE y VALUE</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
<span class="math inline">\(x_j\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(s_{x_j}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(b_j\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(se(b_j)\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(t(b_j)\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(VIF_j\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
PRICE
</td>
<td style="text-align:right;width: 1.6cm; ">
21.370
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.022
</td>
<td style="text-align:right;width: 1.6cm; ">
0.035
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.63
</td>
<td style="text-align:right;width: 1.6cm; ">
1.5
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SHARE
</td>
<td style="text-align:right;width: 1.6cm; ">
115.100
</td>
<td style="text-align:right;width: 1.6cm; ">
0.054
</td>
<td style="text-align:right;width: 1.6cm; ">
0.010
</td>
<td style="text-align:right;width: 1.6cm; ">
5.19
</td>
<td style="text-align:right;width: 1.6cm; ">
3.8
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
VALUE
</td>
<td style="text-align:right;width: 1.6cm; ">
8.157
</td>
<td style="text-align:right;width: 1.6cm; ">
0.313
</td>
<td style="text-align:right;width: 1.6cm; ">
0.162
</td>
<td style="text-align:right;width: 1.6cm; ">
1.94
</td>
<td style="text-align:right;width: 1.6cm; ">
4.7
</td>
</tr>
</tbody>
</table>
<p>Puede verificar que la relación en la ecuación <a href="C5VarSelect.html#eq:eq55">(5.5)</a> es válida para cada una de las variables explicativas en la Tabla <a href="C5VarSelect.html#tab:Tab57">5.7</a>. Dado que cada estadístico <span class="math inline">\(VIF\)</span> es menor a diez, hay poca razón para sospechar colinealidad severa. Esto es interesante porque puede recordar que existe una relación perfecta entre PRICE, SHARE y VALUE en el sentido de que definimos el valor de mercado como VALUE = PRICE <span class="math inline">\(\times\)</span> SHARE. Sin embargo, la relación es multiplicativa y, por lo tanto, es no lineal. Debido a que las variables no están relacionadas linealmente, es válido incluir las tres en el modelo de regresión. Desde una perspectiva financiera, la variable VALUE es importante porque mide el valor de una empresa. Desde una perspectiva estadística, la variable VALUE cuantifica la interacción entre PRICE y SHARE (las variables de interacción se introdujeron en la Sección 3.5.3).</p>
<p>Para la colinealidad, solo nos interesa detectar tendencias lineales, por lo que las relaciones no lineales entre variables no son un problema aquí. Por ejemplo, hemos visto que a veces es útil mantener tanto una variable explicativa <span class="math inline">\(x\)</span> como su cuadrado <span class="math inline">\(x^2\)</span>, a pesar de que existe una relación perfecta (no lineal) entre las dos. Sin embargo, debemos verificar que las relaciones no lineales no sean aproximadamente lineales en la región de muestreo. Aunque la relación es teóricamente no lineal, si es cercana a lineal para nuestra muestra disponible, pueden surgir problemas de colinealidad. La Figura <a href="C5VarSelect.html#fig:Fig55">5.5</a> ilustra esta situación.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig55"></span>
<img src="RegressionMarkdown_files/figure-html/Fig55-1.png" alt="La relación entre \(x_1\) y \(x_2\) es no lineal. Sin embargo, en la región muestreada, las variables tienen una relación casi lineal." width="60%" />
<p class="caption">
Figura 5.5: <strong>La relación entre <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span> es no lineal.</strong> Sin embargo, en la región muestreada, las variables tienen una relación casi lineal.
</p>
</div>
<p>¿Qué podemos hacer en presencia de colinealidad? Una opción es centrar cada variable, restando su promedio y dividiendo por su desviación estándar. Por ejemplo, crear una nueva variable <span class="math inline">\(x_{ij}^{\ast} = (x_{ij} - \bar{x}_j) / s_{x_j}\)</span>. A veces, una variable aparece en millones de unidades y otra en fracciones de unidades. Comparado con la primera variable, la segunda parece ser casi una columna constante de ceros (dado que las computadoras retienen típicamente un número finito de dígitos). Si esto es cierto, entonces la segunda variable se parece mucho a un desplazamiento lineal de la columna constante de unos correspondiente al intercepto. Esto es un problema porque, con las operaciones de mínimos cuadrados, estamos implícitamente elevando al cuadrado números que pueden hacer que estas columnas parezcan aún más similares.</p>
<p>Este problema es simplemente computacional y es fácil de corregir. Simplemente recodifique las variables para que las unidades sean de magnitud similar. Algunos analistas de datos centran automáticamente todas las variables para evitar estos problemas. Este es un enfoque legítimo porque las técnicas de regresión buscan relaciones lineales; los desplazamientos en ubicación y escala no afectan las relaciones lineales.</p>
<p>Otra opción es simplemente no tener en cuenta explícitamente la colinealidad en el análisis, pero discutir algunas de sus implicaciones al interpretar los resultados del análisis de regresión. Este enfoque es probablemente el más comúnmente adoptado. Es un hecho que, al tratar con datos de negocios y económicos, la colinealidad tiende a existir entre las variables. Dado que los datos tienden a ser observacionales en lugar de experimentales, hay poco que el analista pueda hacer para evitar esta situación.</p>
<p>En la mejor de las situaciones, una variable auxiliar que proporcione información similar y que facilite el problema de colinealidad está disponible para reemplazar una variable. Similar a nuestra discusión sobre puntos de alta influencia, una versión transformada de la variable explicativa también puede ser un sustituto útil. En algunas situaciones, un reemplazo ideal no está disponible y nos vemos obligados a eliminar una o más variables. Decidir qué variables eliminar es una elección difícil. Al decidir entre variables, a menudo la elección estará dictada por el juicio del investigador sobre cuál es el conjunto de variables más relevante.</p>
</div>
<div id="Sec553" class="section level3 hasAnchor" number="5.5.3">
<h3><span class="header-section-number">5.5.3</span> Colinealidad e Influencia<a href="C5VarSelect.html#Sec553" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Las medidas de colinealidad e influencia comparten características comunes y, sin embargo, están diseñadas para capturar diferentes aspectos de un conjunto de datos. Ambas son útiles para la crítica de datos y del modelo; se aplican después de un ajuste preliminar del modelo con el objetivo de mejorar la especificación del modelo. Además, ambas se calculan utilizando solo las variables explicativas; los valores de las respuestas no entran en ninguno de los cálculos.</p>
<p>Nuestra medida de colinealidad, el factor de inflación de la varianza, está diseñada para ayudar con la crítica del modelo. Es una medida calculada para cada variable explicativa, diseñada para explicar la relación con otras variables explicativas.</p>
<p>La estadística de influencia está diseñada para ayudarnos con la crítica de datos. Es una medida calculada para cada observación para ayudarnos a explicar cuán inusual es una observación con respecto a otras observaciones.</p>
<p>La colinealidad puede estar enmascarada o inducida por puntos de alta influencia, como lo señalaron Mason y Gunst (1985) y Hadi (1988). Las Figuras <a href="C5VarSelect.html#fig:Fig56">5.6</a> y <a href="C5VarSelect.html#fig:Fig57">5.7</a> proporcionan ilustraciones de cada caso. Estos ejemplos simples subrayan un punto importante: la crítica de datos y la crítica del modelo no son ejercicios separados.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig56"></span>
<img src="RegressionMarkdown_files/figure-html/Fig56-1.png" alt="Con la excepción del punto marcado, \(x_1\) y \(x_2\) están altamente relacionados linealmente." width="60%" />
<p class="caption">
Figura 5.6: <strong>Con la excepción del punto marcado, <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span> están altamente relacionados linealmente.</strong>
</p>
</div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig57"></span>
<img src="RegressionMarkdown_files/figure-html/Fig57-1.png" alt="La relación lineal altamente entre \(x_1\) y \(x_2\) es principalmente debido al punto marcado." width="60%" />
<p class="caption">
Figura 5.7: <strong>La relación lineal altamente entre <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span> es principalmente debido al punto marcado.</strong>
</p>
</div>
<p>Los ejemplos en las Figuras <a href="C5VarSelect.html#fig:Fig56">5.6</a> y <a href="C5VarSelect.html#fig:Fig57">5.7</a> también nos ayudan a ver una forma en que los puntos de alta influencia pueden afectar los errores estándar de los coeficientes de regresión. Recuerde que, en la Sección <a href="C5VarSelect.html#Sec541">5.4.1</a>, vimos que los puntos de alta influencia pueden afectar los valores ajustados del modelo. En las Figuras <a href="C5VarSelect.html#fig:Fig56">5.6</a> y <a href="C5VarSelect.html#fig:Fig57">5.7</a>, vemos que los puntos de alta influencia afectan la colinealidad. Por lo tanto, a partir de la ecuación <a href="C5VarSelect.html#eq:eq55">(5.5)</a>, tenemos que los puntos de alta influencia también pueden afectar nuestros errores estándar de los coeficientes de regresión.</p>
</div>
<div id="Sec554" class="section level3 hasAnchor" number="5.5.4">
<h3><span class="header-section-number">5.5.4</span> Variables Suprensoras<a href="C5VarSelect.html#Sec554" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Como hemos visto, la colinealidad severa puede inflar seriamente los errores estándar de los coeficientes de regresión. Dado que dependemos de estos errores estándar para evaluar la utilidad de las variables explicativas, nuestros procedimientos de selección de modelos e inferencias pueden ser deficientes en presencia de colinealidad severa. A pesar de estos inconvenientes, la colinealidad leve en un conjunto de datos no debe considerarse una deficiencia del conjunto de datos; es simplemente una característica de las variables explicativas disponibles.</p>
<p>Incluso si una variable explicativa es casi una combinación lineal de las demás, eso no significa necesariamente que la información que proporciona sea redundante. Para ilustrar, ahora consideramos una <em>variable suprensora</em>, una variable explicativa que aumenta la importancia de otras variables explicativas cuando se incluye en el modelo.</p>
<p><strong>Ejemplo: Variable Suprensora.</strong> La Figura <a href="C5VarSelect.html#fig:Fig58">5.8</a> muestra una matriz de dispersión de un conjunto de datos hipotético con cincuenta observaciones. Este conjunto de datos contiene una variable dependiente y dos variables explicativas. La Tabla <a href="C5VarSelect.html#tab:Tab58">5.8</a> proporciona la matriz de coeficientes de correlación correspondiente. Aquí, vemos que las dos variables explicativas están altamente correlacionadas. Ahora recuerde que, para una regresión con una variable explicativa, el coeficiente de correlación al cuadrado es el coeficiente de determinación. Así, usando la Tabla <a href="C5VarSelect.html#tab:Tab58">5.8</a>, para una regresión de <span class="math inline">\(y\)</span> sobre <span class="math inline">\(x_1\)</span>, el coeficiente de determinación es <span class="math inline">\((0.188)^2 = 3.5\%\)</span>. De manera similar, para una regresión de <span class="math inline">\(y\)</span> sobre <span class="math inline">\(x_2\)</span>, el coeficiente de determinación es <span class="math inline">\((-0.022)^2 = 0.04\%\)</span>. Sin embargo, para una regresión de <span class="math inline">\(y\)</span> sobre <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span>, el coeficiente de determinación resulta ser sorprendentemente alto, <span class="math inline">\(80.7\%\)</span>. La interpretación es que, individualmente, tanto <span class="math inline">\(x_1\)</span> como <span class="math inline">\(x_2\)</span> tienen poco impacto en <span class="math inline">\(y\)</span>. Sin embargo, cuando se toman conjuntamente, las dos variables explicativas tienen un efecto significativo en <span class="math inline">\(y\)</span>. Aunque la Tabla <a href="C5VarSelect.html#tab:Tab58">5.8</a> muestra que <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span> están fuertemente relacionados linealmente, esta relación no significa que <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span> proporcionen la misma información. De hecho, en este ejemplo, las dos variables se complementan entre sí.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig58"></span>
<img src="RegressionMarkdown_files/figure-html/Fig58-1.png" alt="Matriz de dispersión de una variable dependiente y dos variables explicativas para el ejemplo de variable suprensora" width="80%" />
<p class="caption">
Figura 5.8: <strong>Matriz de dispersión de una variable dependiente y dos variables explicativas para el ejemplo de variable suprensora</strong>
</p>
</div>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab58">Tabla 5.8: </span><strong>Matriz de Correlación para el Ejemplo de Suprensor</strong>
</caption>
<tbody>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
</td>
<td style="text-align:center;width: 1.5cm; ">
<span class="math inline">\(x_1\)</span>
</td>
<td style="text-align:center;width: 1.5cm; ">
<span class="math inline">\(x_2\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
<span class="math inline">\(x_2\)</span>
</td>
<td style="text-align:center;width: 1.5cm; ">
0.972
</td>
<td style="text-align:center;width: 1.5cm; ">
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
<span class="math inline">\(y\)</span>
</td>
<td style="text-align:center;width: 1.5cm; ">
0.188
</td>
<td style="text-align:center;width: 1.5cm; ">
-0.022
</td>
</tr>
</tbody>
</table>
</div>
<div id="Sec555" class="section level3 hasAnchor" number="5.5.5">
<h3><span class="header-section-number">5.5.5</span> Variables Ortogonales<a href="C5VarSelect.html#Sec555" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Otra forma de entender el impacto de la colinealidad es estudiar el caso en el que no hay relaciones entre conjuntos de variables explicativas. Matemáticamente, se dice que dos matrices <span class="math inline">\(\mathbf{X}_1\)</span> y <span class="math inline">\(\mathbf{X}_2\)</span> son <em>ortogonales</em> si <span class="math inline">\(\mathbf{X}_1^{\prime} \mathbf{X}_2 = \mathbf{0}\)</span>. Intuitivamente, dado que generalmente trabajamos con variables centradas (con medias cero), esto significa que cada columna de <span class="math inline">\(\mathbf{X}_1\)</span> no está correlacionada con cada columna de <span class="math inline">\(\mathbf{X}_2\)</span>. Aunque es poco probable que ocurra con datos observacionales en las ciencias sociales, al diseñar tratamientos experimentales o construir polinomios de alto grado, las aplicaciones de variables ortogonales se utilizan regularmente (véase, por ejemplo, Hocking, 2003). Para nuestros propósitos, trabajaremos con variables ortogonales simplemente para entender las consecuencias lógicas de una ausencia total de colinealidad.</p>
<p>Supongamos que <span class="math inline">\(\mathbf{x}_2\)</span> es una variable explicativa que es ortogonal a <span class="math inline">\(\mathbf{X}_1\)</span>, donde <span class="math inline">\(\mathbf{X}_1\)</span> es una matriz de variables explicativas que incluye la intersección. Entonces, es sencillo comprobar que la adición de <span class="math inline">\(\mathbf{x}_2\)</span> a la ecuación de regresión no cambia el ajuste para los coeficientes correspondientes a <span class="math inline">\(\mathbf{X}_1\)</span>. Es decir, sin <span class="math inline">\(\mathbf{x}_2\)</span>, los coeficientes correspondientes a <span class="math inline">\(\mathbf{X}_1\)</span> se calcularían como <span class="math inline">\(\mathbf{b}_1 = \left(\mathbf{X}_1^{\prime} \mathbf{X}_1 \right)^{-1} \mathbf{X}_1^{\prime} \mathbf{y}\)</span>. Usar el ortogonal <span class="math inline">\(\mathbf{x}_2\)</span> como parte del cálculo de mínimos cuadrados no cambiaría el resultado para <span class="math inline">\(\mathbf{b}_1\)</span> (véase el cálculo recursivo de mínimos cuadrados en la Sección 4.7.2).</p>
<p>Además, el factor de inflación de la varianza para <span class="math inline">\(\mathbf{x}_2\)</span> es 1, lo que indica que el error estándar no se ve afectado por las otras variables explicativas. De manera similar, la reducción en la suma de errores al agregar la variable ortogonal <span class="math inline">\(\mathbf{x}_2\)</span> se debe únicamente a esa variable, y no a su interacción con otras variables en <span class="math inline">\(\mathbf{X}_1\)</span>.</p>
<p>Las variables ortogonales pueden ser creadas para datos observacionales en ciencias sociales (así como otros datos colineales) utilizando el método de <em>componentes principales</em>. Con este método, se utiliza una transformación lineal de la matriz de variables explicativas de la forma, <span class="math inline">\(\mathbf{X}^{\ast} = \mathbf{X} \mathbf{P}\)</span>, de manera que la matriz resultante <span class="math inline">\(\mathbf{X}^{\ast}\)</span> esté compuesta por columnas ortogonales. La función de regresión transformada es <span class="math inline">\(\mathrm{E~}\mathbf{y} = \mathbf{X} \boldsymbol \beta = \mathbf{X} \mathbf{P} \mathbf{P}^{-1} \boldsymbol \beta = \mathbf{X}^{\ast} \boldsymbol \beta^{\ast}\)</span>, donde <span class="math inline">\(\boldsymbol \beta^{\ast} = \mathbf{P}^{-1} \boldsymbol \beta\)</span> es el conjunto de nuevos coeficientes de regresión. La estimación procede como antes, con el conjunto ortogonal de variables explicativas. Al elegir la matriz <span class="math inline">\(\mathbf{P}\)</span> apropiadamente, cada columna de <span class="math inline">\(\mathbf{X}^{\ast}\)</span> tiene una contribución identificable. Así, podemos usar técnicas de selección de variables para identificar las porciones de “componentes principales” de <span class="math inline">\(\mathbf{X}^{\ast}\)</span> para usar en la ecuación de regresión. La regresión por componentes principales es un método ampliamente utilizado en algunas áreas de aplicación, como la psicología. Puede abordar fácilmente datos altamente colineales de manera disciplinada. La principal desventaja de esta técnica es que las estimaciones de parámetros resultantes son difíciles de interpretar.</p>
</div>
</div>
<div id="Sec56" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Criterios de Selección<a href="C5VarSelect.html#Sec56" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="Sec561" class="section level3 hasAnchor" number="5.6.1">
<h3><span class="header-section-number">5.6.1</span> Bondad de Ajuste<a href="C5VarSelect.html#Sec561" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>¿Qué tan bien se ajusta el modelo a los datos? Los criterios que miden la proximidad entre el modelo ajustado y los datos reales se conocen como estadísticas de <em>bondad de ajuste</em>. Específicamente, interpretamos el valor ajustado <span class="math inline">\(\hat{y}_i\)</span> como la mejor aproximación del modelo para la <span class="math inline">\(i\)</span>-ésima observación y lo comparamos con el valor real <span class="math inline">\(y_i\)</span>. En la regresión lineal, examinamos la diferencia a través del residuo <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span>; residuos pequeños implican un buen ajuste del modelo. Hemos cuantificado esto a través del tamaño del error típico <span class="math inline">\((s)\)</span>, incluyendo el coeficiente de determinación <span class="math inline">\((R^2)\)</span> y una versión ajustada <span class="math inline">\((R_{a}^2)\)</span>.</p>
<p>Para modelos no lineales, necesitaremos medidas adicionales, y es útil introducir estas medidas en este caso lineal más simple. Una de estas medidas es el <em>Criterio de Información de Akaike</em> que se definirá en términos de ajustes de verosimilitud en la Sección 11.9.4. Para la regresión lineal, se reduce a</p>
<p><span class="math display" id="eq:eq56">\[\begin{equation}
AIC = n \ln (s^2) + n \ln (2 \pi) + n + 3 + k.
\tag{5.6}
\end{equation}\]</span></p>
<p>Para la comparación de modelos, cuanto menor sea el <span class="math inline">\(AIC\)</span>, mejor es el ajuste. Comparar modelos con el mismo número de variables (<span class="math inline">\(k\)</span>) significa que seleccionar un modelo con valores bajos de <span class="math inline">\(AIC\)</span> lleva a la misma elección que seleccionar un modelo con valores bajos de la desviación estándar de los residuos <span class="math inline">\(s\)</span>. Además, un pequeño número de parámetros implica un valor bajo de <span class="math inline">\(AIC\)</span>, manteniéndose todo lo demás constante. La idea es que esta medida equilibra el ajuste (<span class="math inline">\(n \ln (s^2)\)</span>) con una penalización por complejidad (el número de parámetros, <span class="math inline">\(k+2\)</span>). Los paquetes estadísticos a menudo omiten constantes como <span class="math inline">\(n \ln (2 \pi)\)</span> y <span class="math inline">\(n+3\)</span> al reportar <span class="math inline">\(AIC\)</span> porque no importan al comparar modelos.</p>
<p>La Sección 11.9.4 presentará otra medida, el Criterio de Información de Bayes (<span class="math inline">\(BIC\)</span>), que da un peso menor a la penalización por complejidad. Una tercera medida de bondad de ajuste que se usa en modelos de regresión lineal es la estadística <span class="math inline">\(C_p\)</span>. Para definir esta estadística, supongamos que tenemos disponibles <span class="math inline">\(k\)</span> variables explicativas <span class="math inline">\(x_1, ..., x_{k}\)</span> y realizamos una regresión para obtener <span class="math inline">\(s_{full}^2\)</span> como el error cuadrático medio. Ahora, supongamos que consideramos usar solo <span class="math inline">\(p-1\)</span> variables explicativas de modo que haya <span class="math inline">\(p\)</span> coeficientes de regresión. Con estas <span class="math inline">\(p-1\)</span> variables explicativas, realizamos una regresión para obtener la suma de cuadrados del error <span class="math inline">\((Error~SS)_p\)</span>. Así, estamos en posición de definir</p>
<p><span class="math display">\[
C_{p} = \frac{(Error~SS)_p}{s_{full}^2} - n + 2p.
\]</span></p>
<p>Como criterio de selección, elegimos el modelo con un coeficiente <span class="math inline">\(C_{p}\)</span> “pequeño”, donde pequeño se entiende en relación con <span class="math inline">\(p\)</span>. En general, los modelos con valores más pequeños de <span class="math inline">\(C_{p}\)</span> son más deseables.</p>
<p>Al igual que las estadísticas <span class="math inline">\(AIC\)</span> y <span class="math inline">\(BIC\)</span>, la estadística <span class="math inline">\(C_{p}\)</span> busca un equilibrio entre el ajuste del modelo y la complejidad. Es decir, cada estadística resume el compromiso entre el ajuste del modelo y la complejidad, aunque con diferentes pesos. Para la mayoría de los conjuntos de datos, recomiendan el mismo modelo, por lo que un analista puede reportar cualquiera o todas las tres estadísticas. Sin embargo, para algunas aplicaciones, llevan a diferentes modelos recomendados. En este caso, el analista necesita confiar más en criterios no basados en datos para la selección del modelo (los cuales siempre son importantes en cualquier aplicación de regresión).</p>
</div>
<div id="Sec562" class="section level3 hasAnchor" number="5.6.2">
<h3><span class="header-section-number">5.6.2</span> Validación del Modelo<a href="C5VarSelect.html#Sec562" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La validación del modelo es el proceso de confirmar que nuestro modelo propuesto es apropiado, especialmente a la luz de los propósitos de la investigación. Recuerda el proceso iterativo de formulación y selección de modelos descrito en la Sección <a href="C5VarSelect.html#Sec51">5.1</a>. Una crítica importante a este proceso iterativo es que es culpable de <em>búsqueda de datos</em>, es decir, ajustar un gran número de modelos a un solo conjunto de datos. Como vimos en la Sección <a href="C5VarSelect.html#Sec52">5.2</a> sobre la búsqueda de datos en la regresión paso a paso, al mirar una gran cantidad de modelos podemos sobreajustar los datos y subestimar la variación natural en nuestra representación.</p>
<p>Podemos responder a esta crítica utilizando una técnica llamada <em>validación fuera de muestra</em>. La situación ideal es tener disponibles dos conjuntos de datos, uno para el desarrollo del modelo y otro para la validación del modelo. Inicialmente desarrollamos uno o varios modelos en el primer conjunto de datos. Los modelos desarrollados a partir del primer conjunto de datos se llaman nuestros modelos <em>candidatos</em>. Luego, el rendimiento relativo de los modelos candidatos podría medirse en un segundo conjunto de datos. De esta manera, los datos utilizados para validar el modelo no se ven afectados por los procedimientos utilizados para formular el modelo.</p>
<p>Desafortunadamente, rara vez estarán disponibles dos conjuntos de datos para el investigador. Sin embargo, podemos implementar el proceso de validación dividiendo el conjunto de datos en dos submuestras. A estas las llamamos las <em>submuestras de desarrollo del modelo</em> y <em>submuestras de validación</em>, respectivamente. También se conocen como muestras de <em>entrenamiento</em> y <em>prueba</em>, respectivamente. Para ver cómo funciona el proceso en el contexto de la regresión lineal, considera el siguiente procedimiento.</p>
<div class="blackbox">
<p><em>Procedimiento de Validación Fuera de Muestra</em></p>
<ol style="list-style-type: decimal">
<li>Comienza con un tamaño de muestra de <span class="math inline">\(n\)</span> y divídelo en dos submuestras, llamadas la submuestra de desarrollo del modelo y la submuestra de validación. Sea <span class="math inline">\(n_1\)</span> y <span class="math inline">\(n_2\)</span> el tamaño de cada submuestra. En regresión transversal, realiza esta división usando un mecanismo de muestreo aleatorio. Usa la notación <span class="math inline">\(i=1,...,n_1\)</span> para representar las observaciones de la submuestra de desarrollo del modelo y <span class="math inline">\(i=n_1+1,...,n_1+n_2=n\)</span> para las observaciones de la submuestra de validación. La Figura <a href="C5VarSelect.html#fig:Fig59">5.9</a> ilustra este procedimiento.</li>
<li>Usando la submuestra de desarrollo del modelo, ajusta un modelo candidato al conjunto de datos <span class="math inline">\(i=1,...,n_1\)</span>.</li>
<li>Usando el modelo creado en el Paso (ii) y las variables explicativas de la submuestra de validación, “predice” las variables dependientes en la submuestra de validación, <span class="math inline">\(\hat{y}_i\)</span>, donde <span class="math inline">\(i=n_1+1,...,n_1+n_2\)</span>. (Para obtener estas predicciones, puede que necesites transformar las variables dependientes de nuevo a la escala original.)</li>
<li>Evalúa la proximidad de las predicciones a los datos retenidos. Una medida es la <em>suma de errores cuadráticos de predicción</em>
<span class="math display" id="eq:eq57">\[\begin{equation}
SSPE = \sum_{i=n_1+1}^{n_1+n_2} (y_i - \hat{y}_i)^2 .
\tag{5.7}
\end{equation}\]</span>
Repite los Pasos (ii) a (iv) para cada modelo candidato. Elige el modelo con el menor <em>SSPE</em>.</li>
</ol>
</div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig59"></span>
<img src="RegressionMarkdown_files/figure-html/Fig59-1.png" alt="Para la validación del modelo, un conjunto de datos de tamaño \(n\) se divide aleatoriamente en dos submuestras" width="80%" />
<p class="caption">
Figura 5.9: <strong>Para la validación del modelo, un conjunto de datos de tamaño <span class="math inline">\(n\)</span> se divide aleatoriamente en dos submuestras</strong>
</p>
</div>
<p>Existen varias críticas a la <em>SSPE</em>. Primero, es evidente que calcular esta estadística para cada uno de varios modelos candidatos lleva una cantidad considerable de tiempo y esfuerzo. Sin embargo, como ocurre con muchas técnicas estadísticas, esto es simplemente una cuestión de tener disponible software estadístico especializado para realizar los pasos descritos anteriormente. Segundo, dado que la estadística en sí se basa en un subconjunto aleatorio de la muestra, su valor variará de un analista a otro. Esta objeción podría superarse utilizando las primeras <span class="math inline">\(n_1\)</span> observaciones de la muestra. En la mayoría de las aplicaciones, esto no se hace por si hay una relación oculta en el orden de las observaciones. Tercero, y quizás lo más importante, es el hecho de que la elección de los tamaños relativos de los subconjuntos, <span class="math inline">\(n_1\)</span> y <span class="math inline">\(n_2\)</span>, no está clara. Varios investigadores recomiendan diferentes proporciones para la asignación. Snee (1977) sugiere que la división de datos no se realice a menos que el tamaño de la muestra sea moderadamente grande, específicamente, <span class="math inline">\(n \geq 2(k+1) + 20\)</span>. Las directrices de Picard y Berk (1990) muestran que cuanto mayor es el número de parámetros a estimar, mayor es la proporción de observaciones necesarias para la submuestra de desarrollo del modelo. Como regla general, para conjuntos de datos con 100 observaciones o menos, usa alrededor del 25-35% de la muestra para validación fuera de muestra. Para conjuntos de datos con 500 o más observaciones, usa el 50% de la muestra para validación fuera de muestra. Hastie, Tibshirani y Friedman (2001) señalan que una división típica es 50% para desarrollo/entrenamiento, 25% para validación, y el 25% restante para una tercera etapa de validación adicional que ellos llaman <em>prueba</em>.</p>
<p>Debido a estas críticas, los analistas utilizan varias variantes del proceso básico de validación fuera de muestra. Aunque no existe un procedimiento teóricamente mejor, se acuerda ampliamente que la validación del modelo es una parte importante para confirmar la utilidad de un modelo.</p>
</div>
<div id="Sec563" class="section level3 hasAnchor" number="5.6.3">
<h3><span class="header-section-number">5.6.3</span> Validación Cruzada<a href="C5VarSelect.html#Sec563" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La validación cruzada es una técnica de validación de modelos que divide los datos en dos conjuntos disjuntos. La Sección <a href="C5VarSelect.html#Sec562">5.6.2</a> discutió la validación fuera de muestra, donde los datos se dividieron aleatoriamente en dos subconjuntos, ambos conteniendo un porcentaje considerable de los datos. Otro método popular es la validación cruzada de <em>dejar uno fuera</em>, donde la muestra de validación consiste en una sola observación y la muestra de desarrollo se basa en el resto del conjunto de datos.</p>
<p>Especialmente para tamaños de muestra pequeños, una estadística atractiva de validación cruzada de dejar uno fuera es <em>PRESS</em>, la <em>Suma de Cuadrados de Residuos Predichos</em>. Para definir la estadística, considera el siguiente procedimiento donde suponemos que hay un modelo candidato disponible.</p>
<div class="blackbox">
<p><em>Procedimiento de Validación PRESS</em></p>
<ol style="list-style-type: decimal">
<li>Desde la muestra completa, omite el <span class="math inline">\(i\)</span>-ésimo punto y usa las <span class="math inline">\(n-1\)</span> observaciones restantes para calcular los coeficientes de regresión.</li>
<li>Usa los coeficientes de regresión calculados en el primer paso y las variables explicativas para la <span class="math inline">\(i\)</span>-ésima observación para calcular la respuesta predicha, <span class="math inline">\(\hat{y}_{(i)}\)</span>. Esta parte del procedimiento es similar al cálculo de la estadística <em>SSPE</em> con <span class="math inline">\(n_1=n-1\)</span> y <span class="math inline">\(n_2=1\)</span>.</li>
<li>Ahora, repite (i) y (ii) para <span class="math inline">\(i=1,...,n\)</span>. Resumiendo, define
<span class="math display" id="eq:eq58">\[\begin{equation}
PRESS = \sum_{i=1}^{n} (y_i - \hat{y}_{(i)})^2 .
\tag{5.8}
\end{equation}\]</span>
Al igual que con <em>SSPE</em>, esta estadística se calcula para cada uno de varios modelos competidores. Bajo este criterio, elegimos el modelo con el <em>PRESS</em> más pequeño.</li>
</ol>
</div>
<p>Basado en esta definición, la estadística parece ser muy intensiva en cálculos ya que requiere <span class="math inline">\(n\)</span> ajustes de regresión para evaluarla. Para abordar esto, los lectores interesados encontrarán que la Sección <a href="C5VarSelect.html#Sec5102">5.10.2</a> establece</p>
<p><span class="math display" id="eq:eq59">\[\begin{equation}
y_i - \hat{y}_{(i)} = \frac{e_i}{1 - h_{ii}} .
\tag{5.9}
\end{equation}\]</span></p>
<p>Aquí, <span class="math inline">\(e_i\)</span> y <span class="math inline">\(h_{ii}\)</span> representan el <span class="math inline">\(i\)</span>-ésimo residuo y la influencia del ajuste de regresión utilizando el conjunto de datos completo. Esto da lugar a</p>
<p><span class="math display" id="eq:eq510">\[\begin{equation}
PRESS = \sum_{i=1}^{n} \left( \frac{e_i}{1 - h_{ii}} \right)^2 ,
\tag{5.10}
\end{equation}\]</span></p>
<p>lo cual es una fórmula computacionalmente mucho más fácil. Así, la estadística <em>PRESS</em> es menos intensiva en cálculos que <em>SSPE</em>.</p>
<p>Otra ventaja importante de esta estadística, en comparación con <em>SSPE</em>, es que no necesitamos hacer una elección arbitraria sobre los tamaños relativos de los subconjuntos. De hecho, dado que estamos realizando una validación “fuera de muestra” para cada observación, se puede argumentar que este procedimiento es más eficiente, una consideración especialmente importante cuando el tamaño de la muestra es pequeño (por ejemplo, menos de 50 observaciones). Una desventaja es que, dado que el modelo se vuelve a ajustar para cada punto eliminado, <em>PRESS</em> no goza de la apariencia de independencia entre los aspectos de estimación y predicción, a diferencia de <em>SSPE</em>.</p>
</div>
</div>
<div id="Sec57" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Heterocedasticidad<a href="C5VarSelect.html#Sec57" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En la mayoría de las aplicaciones de regresión, el objetivo es entender los determinantes de la función de regresión <span class="math inline">\(\mathrm{E~}y_i = \mathbf{x}_i^{\prime} \boldsymbol \beta = \mu_i\)</span>. Nuestra capacidad para entender la media está fuertemente influenciada por la cantidad de dispersión respecto a la media, que cuantificamos usando la varianza <span class="math inline">\(\mathrm{E}\left(y_i - \mu_i\right)^2\)</span>. En algunas aplicaciones, como cuando me peso en una balanza, hay relativamente poca variabilidad; las mediciones repetidas dan casi el mismo resultado. En otras aplicaciones, como el tiempo que me toma volar a Nueva York, las mediciones repetidas muestran una variabilidad sustancial y están llenas de incertidumbre inherente.</p>
<p>La cantidad de incertidumbre también puede variar de un caso a otro. Denotamos el caso de “variabilidad variable” con la notación <span class="math inline">\(\sigma_i^2 = \mathrm{E}\left(y_i - \mu_i\right)^2\)</span>. Cuando la variabilidad varía según la observación, esto se conoce como <em>heterocedasticidad</em>, que significa “dispersión diferente”. En contraste, la suposición habitual de variabilidad común (suposición E3/F3 en la Sección 3.2) se llama <em>homocedasticidad</em>, lo que significa “misma dispersión”.</p>
<p>Nuestras estrategias de estimación dependen del grado de heterocedasticidad. Para conjuntos de datos con solo una ligera heterocedasticidad, se puede usar mínimos cuadrados para estimar los coeficientes de regresión, tal vez combinado con un ajuste para los errores estándar (descritos en la Sección <a href="C5VarSelect.html#Sec572">5.7.2</a>). Esto se debe a que los estimadores de mínimos cuadrados son insesgados incluso en presencia de heterocedasticidad (ver Propiedad 1 en la Sección 3.2).</p>
<p>Sin embargo, con variables dependientes heterocedásticas, el teorema de Gauss-Markov ya no se aplica, por lo que los estimadores de mínimos cuadrados no están garantizados como óptimos. En casos de heterocedasticidad severa, se utilizan estimadores alternativos, siendo los más comunes aquellos basados en transformaciones de la variable dependiente, como se describirá en la Sección <a href="C5VarSelect.html#Sec574">5.7.4</a>.</p>
<div id="Sec571" class="section level3 hasAnchor" number="5.7.1">
<h3><span class="header-section-number">5.7.1</span> Detección de Heterocedasticidad<a href="C5VarSelect.html#Sec571" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Para decidir una estrategia para manejar la posible heterocedasticidad, primero debemos evaluar o detectar su presencia.</p>
<p>Para detectar heterocedasticidad de manera gráfica, una buena idea es realizar un ajuste preliminar de regresión de los datos y trazar los residuos frente a los valores ajustados. Para ilustrar, la Figura <a href="C5VarSelect.html#fig:Fig510">5.10</a> muestra un gráfico de un conjunto de datos ficticio con una variable explicativa donde la dispersión aumenta a medida que aumenta la variable explicativa. Se realizó una regresión por mínimos cuadrados: se calcularon los residuos y los valores ajustados. La Figura <a href="C5VarSelect.html#fig:Fig511">5.11</a> es un ejemplo de un gráfico de residuos frente a valores ajustados. El ajuste preliminar de regresión elimina muchos de los patrones principales en los datos y deja al ojo libre para concentrarse en otros patrones que pueden influir en el ajuste. Trazamos los residuos frente a los valores ajustados porque los valores ajustados son una aproximación del valor esperado de la respuesta y, en muchas situaciones, la variabilidad crece con la respuesta esperada.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig510"></span>
<img src="RegressionMarkdown_files/figure-html/Fig510-1.png" alt="El área sombreada representa los datos." width="60%" />
<p class="caption">
Figura 5.10: <strong>El área sombreada representa los datos.</strong>
</p>
</div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig511"></span>
<img src="RegressionMarkdown_files/figure-html/Fig511-1.png" alt="Residuos trazados frente a los valores ajustados para los datos en la Figura 5.10." width="60%" />
<p class="caption">
Figura 5.11: <strong>Residuos trazados frente a los valores ajustados para los datos en la Figura 5.10.</strong>
</p>
</div>
<p>Más pruebas formales de heterocedasticidad también están disponibles en la literatura de regresión. Por ejemplo, consideremos una prueba de Breusch y Pagan (1980). Específicamente, esta prueba examina la hipótesis alternativa <span class="math inline">\(H_a\)</span>: $ y_i = ^2 + _i^{} $, donde <span class="math inline">\(\mathbf{z}_i\)</span> es un vector conocido de variables y <span class="math inline">\(\boldsymbol \gamma\)</span> es un vector de parámetros de dimensión <span class="math inline">\(p\)</span>. Así, la hipótesis nula es <span class="math inline">\(H_0:~ \boldsymbol \gamma = \mathbf{0}\)</span>, que es equivalente a homocedasticidad, <span class="math inline">\(\mathrm{Var~} y_i = \sigma^2.\)</span></p>
<p><em>Procedimiento para la Prueba de Heterocedasticidad</em></p>
<ol style="list-style-type: decimal">
<li>Ajuste un modelo de regresión y calcule los residuos del modelo, <span class="math inline">\(e_i\)</span>.</li>
<li>Calcule los residuos estandarizados al cuadrado, <span class="math inline">\(e_i^{\ast 2} = e_i^2 / s^2\)</span>.</li>
<li>Ajuste un modelo de regresión de <span class="math inline">\(e_i^{\ast 2}\)</span> sobre <span class="math inline">\(\mathbf{z}_i\)</span>.</li>
<li>La estadística de la prueba es <span class="math inline">\(LM = \frac{\text{Regress~SS}_z}{2}\)</span>, donde <span class="math inline">\(Regress~SS_z\)</span> es la suma de cuadrados de la regresión del ajuste del modelo en el paso (iii).</li>
<li>Rechace la hipótesis nula si <span class="math inline">\(LM\)</span> excede un percentil de una distribución chi-cuadrado con <span class="math inline">\(p\)</span> grados de libertad. El percentil es uno menos el nivel de significancia de la prueba.</li>
</ol>
<p>Aquí usamos <span class="math inline">\(LM\)</span> para denotar la estadística de la prueba porque Breusch y Pagan la derivaron como una estadística de multiplicador de Lagrange; consulte Breusch y Pagan (1980) para más detalles.</p>
</div>
<div id="Sec572" class="section level3 hasAnchor" number="5.7.2">
<h3><span class="header-section-number">5.7.2</span> Errores Estándar Consistentes con Heterocedasticidad<a href="C5VarSelect.html#Sec572" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Para conjuntos de datos con solo una leve heterocedasticidad, una estrategia sensata es emplear estimadores de mínimos cuadrados de los coeficientes de regresión y ajustar el cálculo de errores estándar para tener en cuenta la heterocedasticidad.</p>
<p>En la Sección 3.2 sobre propiedades, vimos que los coeficientes de regresión por mínimos cuadrados pueden escribirse como <span class="math inline">\(\mathbf{b} = \sum_{i=1}^n \mathbf{w}_i y_i,\)</span>
donde <span class="math inline">\(\mathbf{w}_i = \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1} \mathbf{x}_i\)</span>. Así, con <span class="math inline">\(\sigma_i^2 = \mathrm{Var~} y_i\)</span>, tenemos
<span class="math display" id="eq:eq511">\[\begin{equation}
\mathrm{Var~}\mathbf{b} = \sum_{i=1}^n \mathbf{w}_i \mathbf{w}_i^{\prime} \sigma_i^2
= \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1} \left( \sum_{i=1}^n \sigma_i^2 \mathbf{x}_i \mathbf{x}_i^{\prime} \right) \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1}.
\tag{5.11}
\end{equation}\]</span>
Esta cantidad es conocida excepto por <span class="math inline">\(\sigma_i^2\)</span>. Podemos calcular los residuos usando los coeficientes de regresión por mínimos cuadrados como <span class="math inline">\(e_i = y_i - \mathbf{x}_i^{\prime} \mathbf{b}\)</span>. Con estos, podemos definir la estimación <em>empírica</em>, o <em>robusta</em>, de la matriz varianza-covarianza como
<span class="math display">\[
\widehat{\mathrm{Var~}\mathbf{b}} = \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1} \left( \sum_{i=1}^n e_i^2 \mathbf{x}_i \mathbf{x}_i^{\prime} \right) \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1}.
\]</span>
Los correspondientes errores estándar “consistentes con heterocedasticidad” son
<span class="math display" id="eq:eq512">\[\begin{equation}
se_r(b_j) = \sqrt{(j+1)^{\text{er}}~ \text{elemento diagonal de }\widehat{\mathrm{Var~}\mathbf{b}}}.
\tag{5.12}
\end{equation}\]</span>
La lógica detrás de este estimador es que cada residual al cuadrado, <span class="math inline">\(e_i^2\)</span>, puede ser una mala estimación de <span class="math inline">\(\sigma_i^2\)</span>. Sin embargo, nuestro interés es estimar una (ponderada) suma de varianzas en la ecuación <a href="C5VarSelect.html#eq:eq511">(5.11)</a>; estimar la suma es una tarea mucho más fácil que estimar cualquier estimación de varianza individual.</p>
<p>Los errores estándar robustos, o consistentes con heterocedasticidad, están ampliamente disponibles en paquetes de software estadístico. Aquí, también verá definiciones alternativas de residuos empleados, como en la Sección <a href="C5VarSelect.html#Sec531">5.3.1</a>. Si su paquete estadístico ofrece opciones, el estimador robusto que utiliza residuos studentizados es generalmente preferido.</p>
</div>
<div id="Sec573" class="section level3 hasAnchor" number="5.7.3">
<h3><span class="header-section-number">5.7.3</span> Mínimos Cuadrados Ponderados<a href="C5VarSelect.html#Sec573" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Los estimadores de mínimos cuadrados son menos útiles para conjuntos de datos con heterocedasticidad severa. Una estrategia es usar una variación de la estimación por mínimos cuadrados mediante el <em>ponderado</em> de las observaciones. La idea es que, al minimizar la suma de errores cuadrados usando datos heterocedásticos, la variabilidad esperada de algunas observaciones es menor que la de otras. Intuitivamente, parece razonable que, cuanto menor es la variabilidad de la respuesta, más confiable es esa respuesta y mayor peso debería recibir en el procedimiento de minimización. <em>Los mínimos cuadrados ponderados</em> son una técnica que tiene en cuenta esta “variabilidad variable”.</p>
<p>Específicamente, usamos los supuestos E1, E2 y E4 de la Sección 3.2.3, con E3 reemplazado por E <span class="math inline">\(\varepsilon_i = 0\)</span> y <span class="math inline">\(\text{Var} \varepsilon_i = \sigma^2 / w_i\)</span>, de modo que la variabilidad es proporcional a un peso conocido <span class="math inline">\(w_i\)</span>. Por ejemplo, si la unidad de análisis <span class="math inline">\(i\)</span> representa una entidad geográfica como un estado, podrías usar el número de personas en el estado como peso. O, si <span class="math inline">\(i\)</span> representa una empresa, podrías usar los activos de la empresa para la variable de ponderación. Valores mayores de <span class="math inline">\(w_i\)</span> indican una variable de respuesta más precisa a través de una menor variabilidad. En aplicaciones actuariales, se usan pesos para tener en cuenta una exposición, como el monto de la prima de seguro, el número de empleados, el tamaño de la nómina, el número de vehículos asegurados, etc. (una discusión más detallada está en el Capítulo 18).</p>
<p>Este modelo puede convertirse fácilmente en el problema de “mínimos cuadrados ordinarios” multiplicando todas las variables de regresión por <span class="math inline">\(\sqrt{w_i}\)</span>. Es decir, si definimos <span class="math inline">\(y_i^{\ast} = y_i \times \sqrt{w_i}\)</span> y <span class="math inline">\(x_{ij}^{\ast} = x_{ij} \times \sqrt{w_i}\)</span>, entonces, a partir del supuesto E1, tenemos
<span class="math display">\[
\begin{array}{ll}
y_i^{\ast} &amp; = y_i \times \sqrt{w_i} = \left( \beta_0 x_{i0} + \beta_1 x_{i1} + \ldots + \beta_k x_{ik} + \varepsilon_i \right) \sqrt{w_i} \\
&amp;= \beta_0 x_{i0}^{\ast} + \beta_1 x_{i1}^{\ast} + \ldots + \beta_k x_{ik}^{\ast} + \varepsilon_i^{\ast}
\end{array}
\]</span>
donde <span class="math inline">\(\varepsilon_i^{\ast} = \varepsilon_i \times \sqrt{w_i}\)</span> tiene una varianza homocedástica <span class="math inline">\(\sigma^2\)</span>. Así, con las variables reescaladas, toda la inferencia puede proceder como antes.</p>
<p>Este trabajo ha sido automatizado en paquetes estadísticos donde el usuario simplemente especifica los pesos <span class="math inline">\(w_i\)</span> y el paquete hace el resto. En términos de álgebra de matrices, este procedimiento se puede llevar a cabo definiendo una matriz de pesos <span class="math inline">\(n \times n\)</span> <span class="math inline">\(\mathbf{W} = \text{diag}(w_i)\)</span>, de modo que el elemento diagonal <span class="math inline">\(i\)</span>-ésimo de <span class="math inline">\(\mathbf{W}\)</span> sea <span class="math inline">\(w_i\)</span>. Ampliando la ecuación (3.14), por ejemplo, las estimaciones de mínimos cuadrados ponderados se pueden expresar como
<span class="math display" id="eq:eq513">\[\begin{equation}
\mathbf{b}_{WLS} = \left( \mathbf{X}^{\prime} \mathbf{W} \mathbf{X} \right)^{-1} \mathbf{X}^{\prime} \mathbf{W} \mathbf{y}.
\tag{5.13}
\end{equation}\]</span>
Discuciones adicionales sobre la estimación de mínimos cuadrados ponderados se presentarán en la Sección 15.1.1.</p>
</div>
<div id="Sec574" class="section level3 hasAnchor" number="5.7.4">
<h3><span class="header-section-number">5.7.4</span> Transformaciones<a href="C5VarSelect.html#Sec574" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Otro enfoque que maneja la heterocedasticidad severa, introducido en la Sección 1.3, es transformar la variable dependiente, típicamente con una transformación logarítmica de la forma <span class="math inline">\(y^{\ast} = \ln y\)</span>. Como vimos en la Sección 1.3, las transformaciones pueden servir para “reducir” la dispersión de los datos y simetrizar una distribución. A través de un cambio de escala, una transformación también cambia la variabilidad, potencialmente alterando un conjunto de datos heterocedástico en uno homocedástico. Esta es tanto una fortaleza como una limitación del enfoque de transformación: una transformación afecta simultáneamente tanto la distribución como la heterocedasticidad.</p>
<p>Las transformaciones de potencia, como la transformación logarítmica, son más útiles cuando la variabilidad de los datos crece con la media. En este caso, la transformación servirá para “reducir” los datos a una escala que parece ser homocedástica. Por el contrario, dado que las transformaciones son funciones monótonas, no ayudarán con patrones de variabilidad que son no monótonos. Además, si tus datos son razonablemente simétricos pero heterocedásticos, una transformación no será útil porque cualquier elección que mitigue la heterocedasticidad sesgará la distribución.</p>
<p>Cuando los datos no son positivos, es común agregar una constante a cada observación para que todas las observaciones sean positivas antes de la transformación. Por ejemplo, la transformación <span class="math inline">\(\ln(1+y)\)</span> acomoda la presencia de ceros. También se puede multiplicar por una constante para que se mantengan las unidades originales aproximadas. Por ejemplo, la transformación <span class="math inline">\(100 \ln(1 + y/100)\)</span> puede aplicarse a datos porcentuales donde a veces aparecen porcentajes negativos.</p>
<p>Nuestras discusiones sobre transformaciones se han centrado en transformar variables dependientes. Como se señaló en la Sección 3.5, también es posible transformar variables explicativas. Esto se debe a que los supuestos de regresión condicionan las variables explicativas (Sección 3.2.3). Algunos analistas prefieren transformar variables para aproximarse a la normalidad, considerando las distribuciones normales multivariadas como una base para el análisis de regresión. Otros son reacios a transformar variables explicativas debido a las dificultades para interpretar los modelos resultantes. El enfoque aquí es usar transformaciones que sean fácilmente interpretables, como las introducidas en la Sección 3.5. Otras transformaciones son ciertamente candidatas para incluir en un modelo seleccionado, pero deben proporcionar dividendos sustanciales en términos de ajuste o poder predictivo si son difíciles de comunicar.</p>
</div>
</div>
<div id="Sec58" class="section level2 hasAnchor" number="5.8">
<h2><span class="header-section-number">5.8</span> Lectura Adicional y Referencias<a href="C5VarSelect.html#Sec58" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Long y Ervin (2000) reúnen pruebas convincentes sobre el uso de estimadores alternativos consistentes con la heterocedasticidad para los errores estándar que tienen un mejor rendimiento en muestras finitas que las versiones clásicas. Las propiedades de gran muestra de los estimadores empíricos han sido establecidas por Eicker (1967), Huber (1967) y White (1980) en el caso de la regresión lineal. Para el caso de la regresión lineal, MacKinnon y White (1985) sugieren alternativas que proporcionan mejores propiedades en muestras pequeñas. Para muestras pequeñas, la evidencia se basa en (1) el sesgo de los estimadores, (2) su motivación como estimadores jackknife y (3) su rendimiento en estudios de simulación.</p>
<p>Otros métodos para medir la colinealidad basados en conceptos de álgebra de matrices que involucran valores propios, como los números de condición y los índices de condición, son utilizados por algunos analistas. Consulta a Belsey, Kuh y Welsch (1980) para un tratamiento sólido de la colinealidad y los diagnósticos de regresión. Hocking (2003) proporciona lecturas adicionales sobre colinealidad y componentes principales. Consulta a Carroll y Ruppert (1988) para más discusiones sobre transformaciones en la regresión.</p>
<p>Hastie, Tibshirani y Friedman (2001) ofrecen una discusión avanzada sobre problemas de selección de modelos, centrándose en los aspectos predictivos de los modelos en el lenguaje del aprendizaje automático.</p>
<p><strong>Referencias del Capítulo</strong></p>
<ul>
<li>Belseley, David A., Edwin Kuh and Roy E. Welsch (1980). <em>Regression Diagnostics: Identifying Influential Data and Sources of Collinearity</em>. Wiley, New York.</li>
<li>Bendel, R. B. and Afifi, A. A. (1977). Comparison of stopping rules in forward “stepwise” regression. <em>Journal of the American Statistical Association</em> 72, 46-53.</li>
<li>Box, George E. P. (1980). Sampling and Bayes inference in scientific modeling and robustness (with discussion). <em>Journal of the Royal Statistical Society</em>, Series A, 143, 383-430.</li>
<li>Breusch, T. S. and A. R. Pagan (1980). The Lagrange multiplier test and its applications to model specification in econometrics. <em>Review of Economic Studies</em>, 47, 239-53.</li>
<li>Carroll, Raymond J. and David Ruppert (1988). <em>Transformation and Weighting in Regression</em>, Chapman-Hall.</li>
<li>Eicker, F. (1967). Limit theorems for regressions with unequal and dependent errors. <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em> 1, LeCam, L. M. and J. Neyman, editors, University of California Press, pp, 59-82.</li>
<li>Hadi, A. S. (1988). Diagnosing collinearity-influential observations. <em>Computational Statistics and Data Analysis</em> 7, 143-159.</li>
<li>Hastie, Trevor, Robert Tibshirani and Jerome Friedman (2001). <em>The Elements of Statistical Learning: Data Mining, Inference and Prediction</em>. Springer-Verlag, New York.</li>
<li>Hocking, Ronald R. (2003). <em>Methods and Applications of Linear Models: Regression and the Analysis of Variance</em>. Wiley, New York.</li>
<li>Huber, P. J. (1967). The behaviour of maximum likelihood estimators under non-standard conditions. <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em> 1, LeCam, L. M. and Neyman, J. editors, University of California Press, pp, 221-33.</li>
<li>Long, J.S. and L.H. Ervin (2000). Using heteroscedasticity consistent standard errors in the linear regression model. <em>American Statistician</em> 54, 217-224.</li>
<li>MacKinnon, J.G. and H. White (1985). Some heteroskedasticity consistent covariance matrix estimators with improved finite sample properties. <em>Journal of Econometrics</em> 29, 53-57.</li>
<li>Mason, R. L. and Gunst, R. F. (1985). Outlier-induced collinearities. <em>Technometrics</em> 27, 401-407.</li>
<li>Picard, R. R. and Berk, K. N. (1990). Data splitting. <em>The American Statistician</em> 44, 140-147.</li>
<li>Rencher, A. C. and Pun, F. C. (1980). Inflation of <span class="math inline">\(R^2\)</span> in best subset regression. <em>Technometrics</em> 22, 49-53.</li>
<li>Snee, R. D. (1977). Validation of regression models. Methods and examples. <em>Technometrics</em> 19, 415-428.</li>
</ul>
</div>
<div id="Sec59" class="section level2 hasAnchor" number="5.9">
<h2><span class="header-section-number">5.9</span> Ejercicios<a href="C5VarSelect.html#Sec59" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>5.1. Estás realizando una regresión con una variable explicativa, por lo que considera el modelo de regresión lineal básico <span class="math inline">\(y_i = \beta_0 + \beta_1 x_i + \varepsilon_i\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Muestra que el apalancamiento <span class="math inline">\(i\)</span>-ésimo se puede simplificar a
<span class="math display">\[
h_{ii} = \frac{1}{n} + \frac{(x_i - \overline{x})^2}{(n-1) s_x^2}.
\]</span></li>
<li>Muestra que <span class="math inline">\(\overline{h} = 2 / n\)</span>.</li>
<li>Supón que <span class="math inline">\(h_{ii} = 6/n\)</span>. ¿Cuántas desviaciones estándar está <span class="math inline">\(x_i\)</span> alejado (ya sea por encima o por debajo) de la media?</li>
</ol>
<p>5.2. Considera los resultados de una regresión usando una variable explicativa con <span class="math inline">\(n=3\)</span> observaciones. Los residuos y los apalancamientos son:
<span class="math display">\[
\small{
   \begin{array}{l|ccc}
   \hline
   i &amp; 1 &amp; 2 &amp; 3 \\ \hline
   \text{Residuos } e_i &amp; 3.181 &amp; -6.362 &amp; 3.181 \\
   \text{Apalancamientos } h_{ii} &amp; 0.8333 &amp; 0.3333 &amp; 0.8333 \\
   \hline
   \end{array}
}   
\]</span></p>
<p>Calcula el estadístico <span class="math inline">\(PRESS\)</span>.</p>
<p>5.3. <strong>Expectativas de Vida Nacionales.</strong> Continuamos con el análisis iniciado en los Ejercicios 1.7, 2.22, 3.6 y 4.7. El enfoque de este ejercicio es la selección de variables.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Comienza con los datos de <span class="math inline">\(n=185\)</span> países de todo el mundo que tienen expectativas de vida válidas (no faltantes). Grafica la expectativa de vida frente al producto interno bruto y los gastos privados en salud. A partir de estos gráficos, describe por qué es deseable utilizar transformaciones logarítmicas, lnGDP y lnHEALTH, respectivamente. También grafica la expectativa de vida frente a lnGDP y lnHEALTH para confirmar tu intuición.</p></li>
<li><p>Utiliza un algoritmo de regresión paso a paso para ayudarte a seleccionar un modelo. No consideres las variables RESEARCHERS, SMOKING y FEMALEBOSS ya que tienen muchos valores faltantes. Para las variables restantes, utiliza solo las observaciones sin valores faltantes. Hazlo dos veces, con y sin la variable categórica REGION.</p></li>
<li><p>Regresa al conjunto de datos completo de <span class="math inline">\(n=185\)</span> países y ejecuta un modelo de regresión usando FERTILITY, PUBLICEDUCATION y lnHEALTH como variables explicativas.</p>
<p>c(i). Proporciona histogramas de residuos estandarizados y apalancamientos. <br>
c(ii). Identifica el residuo estandarizado y el apalancamiento asociados con Lesoto, anteriormente Basutolandia, un reino rodeado por Sudáfrica. ¿Es esta observación un valor atípico, un punto de alto apalancamiento, o ambos? <br>
c(iii). Repite la regresión sin Lesoto. Cita cualquier diferencia en los coeficientes estadísticos entre este modelo y el del apartado c(i).</p></li>
</ol>
<p>5.4. <strong>Seguro de Vida a Término.</strong> Continuamos con nuestro estudio de la Demanda de Seguro de Vida a Término de los Capítulos 3 y 4. Específicamente, examinamos la Encuesta de Finanzas del Consumidor (SCF) de 2004, una muestra representativa a nivel nacional que contiene información extensa sobre activos, pasivos, ingresos y características demográficas de los encuestados (potenciales clientes de EE.UU.). Estudiamos una muestra aleatoria de 500 familias con ingresos positivos. De la muestra de 500, inicialmente consideramos una submuestra de <span class="math inline">\(n=275\)</span> familias que compraron seguro de vida a término.</p>
<p>Considera una regresión lineal de LNINCOME, EDUCATION, NUMHH, MARSTAT, AGE y GENDER sobre LNFACE.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Colinealidad. No todas las variables resultaron ser estadísticamente significativas. Para investigar una posible explicación, calcula los factores de inflación de la varianza.</p>
<p>a(i). Explica brevemente la idea de colinealidad y un factor de inflación de la varianza. <br>
a(ii). ¿Qué constituye un gran factor de inflación de la varianza? <br>
a(iii). Si se detecta un gran factor de inflación de la varianza, ¿qué posibles acciones podemos tomar para abordar este aspecto de los datos? <br>
a(iv). Complementa las estadísticas de los factores de inflación de la varianza con una tabla de correlaciones de las variables explicativas. Basado en estas estadísticas, ¿es la colinealidad un problema con este modelo ajustado? ¿Por qué o por qué no?</p></li>
<li><p>Puntos Inusuales. A veces, un ajuste deficiente del modelo puede deberse a puntos inusuales.</p>
<p>b(i). Define la idea de apalancamiento para una observación. <br>
b(ii). Para este modelo ajustado, da reglas generales para identificar puntos con apalancamiento inusual. Identifica cualquier punto inusual. <br>
b(iii). Un analista está preocupado por los valores de apalancamiento de este modelo ajustado y sugiere usar FACE como la variable dependiente en lugar de LNFACE. Describe cómo cambiarían los valores de apalancamiento usando esta variable dependiente alternativa.</p></li>
<li><p>Análisis de Residuos. Podemos aprender cómo mejorar los ajustes del modelo a partir de los análisis de residuos.</p>
<p>c(i). Proporciona un gráfico de residuos frente a valores ajustados. ¿Qué esperamos aprender de este tipo de gráfico? ¿Este gráfico muestra alguna inadecuación del modelo? <br>
c(ii). Proporciona un gráfico <span class="math inline">\(qq\)</span> de residuos. ¿Qué esperamos aprender de este tipo de gráfico? ¿Este gráfico muestra alguna inadecuación del modelo? <br>
c(iii). Proporciona un gráfico de residuos frente a apalancamientos. ¿Qué esperamos aprender de este tipo de gráfico? ¿Este gráfico muestra alguna inadecuación del modelo?</p></li>
<li><p>Regresión Paso a Paso. Ejecuta un algoritmo de regresión paso a paso. Supón que este algoritmo sugiere un modelo utilizando LNINCOME, EDUCATION, NUMHH y GENDER como variables explicativas para predecir la variable dependiente LNFACE.</p>
<p>d(i). ¿Cuál es el propósito de la regresión paso a paso? <br>
d(ii). Describe dos desventajas importantes de los algoritmos de regresión paso a paso.</p></li>
</ol>
</div>
<div id="Sec510" class="section level2 hasAnchor" number="5.10">
<h2><span class="header-section-number">5.10</span> Suplementos Técnicos para el Capítulo 5<a href="C5VarSelect.html#Sec510" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="Sec5101" class="section level3 hasAnchor" number="5.10.1">
<h3><span class="header-section-number">5.10.1</span> Matriz de Proyección<a href="C5VarSelect.html#Sec5101" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Matriz de Sombrero.</strong> Definimos la matriz de sombrero como <span class="math inline">\(\mathbf{H} = \mathbf{X(X}^{\prime}\mathbf{X)}^{-1} \mathbf{X}^{\prime}\)</span>, de manera que <span class="math inline">\(\mathbf{\hat{y}} = \mathbf{X b} = \mathbf{Hy}\)</span>. De esto, se dice que la matriz <span class="math inline">\(\mathbf{H}\)</span> <em>proyecta</em> el vector de respuestas <span class="math inline">\(\mathbf{y}\)</span> sobre el vector de valores ajustados <span class="math inline">\(\mathbf{\hat{y}}\)</span>.</p>
<p>Dado que <span class="math inline">\(\mathbf{H}^{\prime} = \mathbf{H}\)</span>, la matriz de sombrero es simétrica. Además, también es una matriz <em>idempotente</em> debido a la propiedad de que <span class="math inline">\(\mathbf{HH} = \mathbf{H}\)</span>. Para ver esto, tenemos que
<span class="math display">\[
\begin{array}{ll}
\mathbf{HH} &amp;= \mathbf{(X(\mathbf{X}^{\prime}X)}^{-1}\mathbf{X}^{\prime}\mathbf{)(X(\mathbf{X}^{\prime}X)}^{-1}\mathbf{X}^{\prime}\mathbf{)} \\
&amp;= \mathbf{X(\mathbf{X}^{\prime}X)}^{-1}\mathbf{(\mathbf{X}^{\prime}X)(\mathbf{X}^{\prime}X)}^{-1}\mathbf{X}^{\prime} = \mathbf{X(\mathbf{X}^{\prime}X)}^{-1}\mathbf{X}^{\prime} = \mathbf{H}.
\end{array}
\]</span>
De manera similar, es fácil verificar que <span class="math inline">\(\mathbf{I-H}\)</span> es idempotente. Dado que <span class="math inline">\(\mathbf{H}\)</span> es idempotente, a partir de algunos resultados en álgebra de matrices, es sencillo mostrar que
<span class="math display">\[
\sum_{i=1}^{n} h_{ii} = k + 1.
\]</span>
Como se discutió en la Sección <a href="C5VarSelect.html#Sec541">5.4.1</a>, usamos nuestros límites y el apalancamiento promedio, <span class="math inline">\(\bar{h} = (k + 1)/n\)</span>, para ayudar a identificar observaciones con apalancamiento inusualmente alto.</p>
<p><strong>Varianza de los Residuos.</strong> Usando la ecuación del modelo <span class="math inline">\(\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}\)</span>, podemos expresar el vector de residuos como
<span class="math display" id="eq:eq514">\[\begin{equation}
\mathbf{e} = \mathbf{y} - \mathbf{\hat{y}} = \mathbf{y - Hy} = \mathbf{(I-H)(X \boldsymbol{\beta} + \boldsymbol{\varepsilon})} = \mathbf{(I-H) \boldsymbol{\varepsilon}}.
\tag{5.14}
\end{equation}\]</span>
La última igualdad se debe al hecho de que <span class="math inline">\(\mathbf{(I-H)X} = \mathbf{X - HX} = \mathbf{X - X} = \mathbf{0}\)</span>. Usando <span class="math inline">\(\text{Var~} \boldsymbol{\varepsilon} = \sigma^2 \mathbf{I}\)</span>, tenemos
<span class="math display">\[
\begin{array}{ll}
\text{Var } \mathbf{e} &amp;= \text{Var }\left[ \mathbf{(I-H)\boldsymbol{\varepsilon}} \right] = \mathbf{(I-H)} \text{Var } \boldsymbol{\varepsilon} \mathbf{(I-H)} \\
&amp;= \sigma^2 \mathbf{(I-H)} \mathbf{I} \mathbf{(I-H)} = \sigma^2 \mathbf{(I-H)}.
\end{array}
\]</span>
La última igualdad proviene del hecho de que <span class="math inline">\(\mathbf{I-H}\)</span> es idempotente. Así, tenemos que
<span class="math display" id="eq:eq515">\[\begin{equation}
\text{Var } e_i = \sigma^2 (1 - h_{ii}) \text{   y   Cov } (e_i, e_j) = -\sigma^2 h_{ij}.
\tag{5.15}
\end{equation}\]</span>
Así, aunque los errores verdaderos <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> son no correlacionados, hay una pequeña correlación negativa entre los residuos <span class="math inline">\(\mathbf{e}\)</span>.</p>
<p><strong>Dominio del Error en el Residuo.</strong> Examinando la fila <span class="math inline">\(i\)</span>-ésima de la ecuación <a href="C5VarSelect.html#eq:eq514">(5.14)</a>, tenemos que el residuo <span class="math inline">\(i\)</span>-ésimo
<span class="math display" id="eq:eq516">\[\begin{equation}
e_i = \varepsilon_i - \sum_{j=1}^{n} h_{ij} \varepsilon_j
\tag{5.16}
\end{equation}\]</span>
se puede expresar como una combinación lineal de errores independientes. La relación <span class="math inline">\(\mathbf{H} = \mathbf{HH}\)</span> da lugar a
<span class="math display" id="eq:eq517">\[\begin{equation}
h_{ii} = \sum_{j=1}^{n} h_{ij}^2.
\tag{5.17}
\end{equation}\]</span>
Dado que <span class="math inline">\(h_{ii}\)</span> es, en promedio, <span class="math inline">\((k + 1)/n\)</span>, esto indica que cada <span class="math inline">\(h_{ij}\)</span> es pequeño en relación con 1. Así, al interpretar la ecuación <a href="C5VarSelect.html#eq:eq516">(5.16)</a>, decimos que la mayor parte de la información en <span class="math inline">\(e_i\)</span> se debe a <span class="math inline">\(\varepsilon_i\)</span>.</p>
<p><strong>Correlaciones con los Residuos.</strong> Primero define <span class="math inline">\(\mathbf{x}^j = (x_{1j}, x_{2j}, \dots, x_{nj})^{\prime}\)</span> como la columna que representa la <span class="math inline">\(j\)</span>-ésima variable. Con esta notación, podemos particionar la matriz de variables explicativas como <span class="math inline">\(\mathbf{X} = \left( \mathbf{x}^{0}, \mathbf{x}^{1}, \dots, \mathbf{x}^{k} \right)\)</span>. Ahora, examinando la columna <span class="math inline">\(j\)</span>-ésima de la relación <span class="math inline">\(\mathbf{(I-H)X} = \mathbf{0}\)</span>, tenemos <span class="math inline">\(\mathbf{(I-H)x}^{j} = \mathbf{0}\)</span>. Con <span class="math inline">\(\mathbf{e} = \mathbf{(I-H) \boldsymbol{\varepsilon}}\)</span>, esto da
<span class="math display">\[
\mathbf{e}^{\prime} \mathbf{x}^{j} = \boldsymbol{\varepsilon}^{\prime} \mathbf{(I-H)x}^{j} = 0,
\]</span>
para <span class="math inline">\(j = 0, 1, \ldots, k.\)</span> Este resultado tiene varias implicaciones. Si el intercepto está en el modelo, entonces <span class="math inline">\(\mathbf{x}^{0} = (1, 1, \ldots, 1)^{\prime}\)</span> es un vector de unos. Aquí, <span class="math inline">\(\mathbf{e}^{\prime} \mathbf{x}^{0} = 0\)</span> significa que <span class="math inline">\(\sum_{i=1}^{n} e_i = 0\)</span> o, el residuo promedio es cero. Además, dado que <span class="math inline">\(\mathbf{e}^{\prime} \mathbf{x}^{j} = 0\)</span>, es fácil verificar que la correlación muestral entre <span class="math inline">\(\mathbf{e}\)</span> y <span class="math inline">\(\mathbf{x}^{j}\)</span> es cero. En la misma línea, también tenemos que <span class="math inline">\(\mathbf{e}^{\prime} \mathbf{\hat{y}} = \mathbf{e}^{\prime} \mathbf{(I-H)Xb} = \mathbf{0}\)</span>. Así, usando el mismo argumento que antes, la correlación muestral entre <span class="math inline">\(\mathbf{e}\)</span> y <span class="math inline">\(\mathbf{\hat{y}}\)</span> es cero.</p>
<p><strong>Coeficiente de Correlación Múltiple.</strong> Para un ejemplo de una correlación diferente de cero, considera <span class="math inline">\(r(\mathbf{y, \hat{y}})\)</span>, la correlación muestral entre <span class="math inline">\(\mathbf{y}\)</span> y <span class="math inline">\(\mathbf{\hat{y}}\)</span>. Dado que <span class="math inline">\(\mathbf{(I-H)x}^{0} = \mathbf{0}\)</span>, tenemos <span class="math inline">\(\mathbf{x}^{0} = \mathbf{Hx}^{0}\)</span> y, por lo tanto, <span class="math inline">\(\mathbf{\hat{y}}^{\prime} \mathbf{x}^{0} = \mathbf{y}^{\prime} \mathbf{Hx}^{0} = \mathbf{y^{\prime} x}^{0}\)</span>. Asumiendo que <span class="math inline">\(\mathbf{x}^{0} = (1, 1, \ldots, 1)^{\prime}\)</span>, esto significa que <span class="math inline">\(\sum_{i=1}^{n} \hat{y}_i = \sum_{i=1}^{n} y_i\)</span>, por lo que el valor promedio ajustado es <span class="math inline">\(\bar{y}\)</span>.</p>
<p><span class="math display">\[
r(\mathbf{y, \hat{y}}) = \frac{\sum_{i=1}^{n} (y_i - \bar{y})(\hat{y}_i - \bar{y})}{(n-1) s_y s_{\hat{y}}}.
\]</span></p>
<p>Recuerda que <span class="math inline">\((n-1) s_y^2 = \sum_{i=1}^{n} (y_i - \bar{y})^2 = Total ~SS\)</span> y <span class="math inline">\((n-1) s_{\hat{y}}^2 = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 = Regress ~SS\)</span>. Además, con <span class="math inline">\(\mathbf{x}^0 = (1, 1, \ldots, 1)^{\prime}\)</span>,
<span class="math display">\[
\begin{array}{ll}
\sum_{i=1}^{n} (y_i - \bar{y})(\hat{y}_i - \bar{y}) &amp;= (\mathbf{y} - \bar{y} \mathbf{x}^0)^{\prime} (\mathbf{\hat{y}} - \bar{y} \mathbf{x}^0) = \mathbf{y}^{\prime} \mathbf{\hat{y}} - \bar{y}^2 \mathbf{x}^{0 \prime} \mathbf{x}^0 \\
&amp;= \mathbf{y}^{\prime} \mathbf{Xb} - n \bar{y}^2 = Regress ~SS.
\end{array}
\]</span></p>
<p>Esto da
<span class="math display" id="eq:eq518">\[\begin{equation}
r(\mathbf{y, \hat{y}}) = \frac{Regress ~SS}{\sqrt{\left( Total ~SS \right) \left( Regress ~SS \right)}} = \sqrt{\frac{Regress ~SS}{Total ~SS}} = \sqrt{R^2}.
\tag{5.18}
\end{equation}\]</span>
Es decir, el coeficiente de determinación se puede interpretar como la raíz cuadrada de la correlación entre las respuestas observadas y las ajustadas.</p>
</div>
<div id="Sec5102" class="section level3 hasAnchor" number="5.10.2">
<h3><span class="header-section-number">5.10.2</span> Estadísticas Leave-One-Out<a href="C5VarSelect.html#Sec5102" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Notación.</strong> Para probar la sensibilidad de las cantidades de regresión, hay varias estadísticas de interés que se basan en la noción de “dejar fuera” u omitir una observación. Con este fin, la notación de subíndice <span class="math inline">\((i)\)</span> significa <em>dejar fuera</em> la <span class="math inline">\(i\)</span>-ésima observación. Por ejemplo, omitir la fila de variables explicativas <span class="math inline">\(\mathbf{x}_i^{\prime} = (x_{i0}, x_{i1}, \dots, x_{ik})\)</span> de <span class="math inline">\(\mathbf{X}\)</span> da lugar a <span class="math inline">\(\mathbf{X}_{(i)}\)</span>, una matriz de <span class="math inline">\((n-1) \times (k+1)\)</span> de variables explicativas. De manera similar, <span class="math inline">\(\mathbf{y}_{(i)}\)</span> es un vector de <span class="math inline">\((n-1) \times 1\)</span>, basado en eliminar la <span class="math inline">\(i\)</span>-ésima fila de <span class="math inline">\(\mathbf{y}\)</span>.</p>
<p><strong>Resultado Básico de Matrices.</strong> Supongamos que <span class="math inline">\(\mathbf{A}\)</span> es una matriz invertible de <span class="math inline">\(p \times p\)</span> y <span class="math inline">\(\mathbf{z}\)</span> es un vector de <span class="math inline">\(p \times 1\)</span>. El siguiente resultado de álgebra de matrices proporciona una herramienta importante para entender las estadísticas leave-one-out en el análisis de regresión lineal.
<span class="math display" id="eq:eq519">\[\begin{equation}
\left( \mathbf{A - zz}^{\prime} \right)^{-1} = \mathbf{A}^{-1} + \frac{\mathbf{A}^{-1} \mathbf{zz}^{\prime} \mathbf{A}^{-1}}{1 - \mathbf{z}^{\prime} \mathbf{A}^{-1} \mathbf{z}}.
\tag{5.19}
\end{equation}\]</span>
Para verificar este resultado, simplemente multiplica <span class="math inline">\(\mathbf{A - zz}^{\prime}\)</span> por el lado derecho de la ecuación para obtener <span class="math inline">\(\mathbf{I}\)</span>, la matriz identidad.</p>
<strong>Vector de Coeficientes de Regresión.</strong> Al omitir la <span class="math inline">\(i\)</span>-ésima observación, nuestro nuevo vector de coeficientes de regresión es <span class="math inline">\(\mathbf{b}_{(i)} = \left( \mathbf{X}_{(i)}^{\prime} \mathbf{X}_{(i)} \right)^{-1} \mathbf{X}_{(i)}^{\prime} \mathbf{y}_{(i)}.\)</span> Una expresión alternativa para <span class="math inline">\(\mathbf{b}_{(i)}\)</span> que resulta ser más simple de calcular es
<span class="math display" id="eq:eq520">\[\begin{equation}
\mathbf{b}_{(i)} = \mathbf{b} - \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}}.
\tag{5.20}
\end{equation}\]</span>
Para verificar esto, primero usa el resultado de inversión de matrices con <span class="math inline">\(\mathbf{A} = \mathbf{X}^{\prime} \mathbf{X}\)</span> y <span class="math inline">\(\mathbf{z} = \mathbf{x}_i\)</span> para obtener
<span class="math display">\[
\left( \mathbf{X}_{(i)}^{\prime} \mathbf{X}_{(i)} \right)^{-1} = (\mathbf{X}^{\prime} \mathbf{X} - \mathbf{x}_i \mathbf{x}_i^{\prime})^{-1} = (\mathbf{X}^{\prime} \mathbf{X})^{-1} + \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1}}{1 - h_{ii}},
\]</span>
donde, a partir del resultado de apalancamiento, tenemos <span class="math inline">\(h_{ii} = \mathbf{x}_i^{\prime} (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{x}_i\)</span>. Multiplicando cada lado por
<span class="math display">\[
\mathbf{X}_{(i)}^{\prime} \mathbf{y}_{(i)} = \mathbf{X}^{\prime} \mathbf{y} - \mathbf{x}_i y_i
\]</span>
da
$$
<span class="math display">\[\begin{array}{ll}
\mathbf{b}_{(i)} &amp;= \left( \mathbf{X}_{(i)}^{\prime} \mathbf{X}_{(i)} \right)^{-1} \mathbf{X}_{(i)}^{\prime} \mathbf{y}_{(i)} \\
&amp;= \left( (\mathbf{X}^{\prime} \mathbf{X})^{-1} + \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1}}{1 - h_{ii}} \right) \left( \mathbf{X}^{\prime} \mathbf{y} - \mathbf{x}_i y_i \right) \\
&amp;= \mathbf{b} - \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i y_i + \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \mathbf{b} - \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i y_i}{1 - h_{ii}} \\
&amp;= \mathbf{b} - \frac{\left( 1 - h_{ii} \right) \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i y_i - \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \mathbf{b} - \left( \mathbf{

X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i h_{ii} y_i}{1 - h_{ii}} \\
&amp;= \mathbf{b} - \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i y_i - \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \mathbf{b}}{1 - h_{ii}} \\
&amp;= \mathbf{b} - \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}}.
\end{array}\]</span>
<p>$$
Esto establece el resultado.</p>
<p><strong>Distancia de Cook.</strong> Para medir el efecto, o <em>influencia</em>, de omitir la <span class="math inline">\(i\)</span>-ésima observación, Cook examinó la diferencia entre los valores ajustados con y sin la observación. Definimos la Distancia de Cook como
<span class="math display">\[
D_i = \frac{\left( \mathbf{\hat{y} - \hat{y}}_{(i)} \right)^{\prime} \left( \mathbf{\hat{y} - \hat{y}}_{(i)} \right)}{(k+1) s^2}
\]</span>
donde <span class="math inline">\(\mathbf{\hat{y}}_{(i)} = \mathbf{Xb}_{(i)}\)</span> es el vector de valores ajustados calculado omitiendo el punto <span class="math inline">\(i\)</span>-ésimo. Usando la ecuación <a href="C5VarSelect.html#eq:eq520">(5.20)</a> y <span class="math inline">\(\mathbf{\hat{y}} = \mathbf{Xb}\)</span>, una expresión alternativa para la Distancia de Cook es
<span class="math display">\[
\begin{array}{ll}
D_i &amp;= \frac{\left( \mathbf{b - b}_{(i)} \right)^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right) \left( \mathbf{b - b}_{(i)} \right)}{(k+1) s^2} \\
&amp;= \frac{e_i^2}{(1 - h_{ii})^2} \frac{\mathbf{x}_i^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \left( \mathbf{X}^{\prime} \mathbf{X} \right) \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i}{(k+1) s^2} \\
&amp; = \frac{e_i^2}{(1 - h_{ii})^2} \frac{h_{ii}}{(k+1) s^2} \\
&amp;= \left( \frac{e_i}{s \sqrt{1 - h_{ii}}} \right)^2 \frac{h_{ii}}{(k+1) (1 - h_{ii})}.
\end{array}
\]</span>
Este resultado no solo es útil computacionalmente, sino que también sirve para descomponer la estadística en la parte debida al residuo estandarizado, <span class="math inline">\((e_i/(s \sqrt{1 - h_{ii}}))^2\)</span>, y en la parte debida al apalancamiento, <span class="math inline">\(\frac{h_{ii}}{(k+1) (1 - h_{ii})}\)</span>.</p>
<p><strong>Residuo Leave-One-Out.</strong> El residuo leave-one-out se define como <span class="math inline">\(e_{(i)} = y_i - \mathbf{x}_i^{\prime} \mathbf{b}_{(i)}\)</span>. Se usa en el cálculo de la estadística <em>PRESS</em>, descrita en la Sección <a href="C5VarSelect.html#Sec563">5.6.3</a>. Una expresión computacional simple es <span class="math inline">\(e_{(i)} = \frac{e_i}{1 - h_{ii}}\)</span>. Para verificar esto, usa la ecuación <a href="C5VarSelect.html#eq:eq520">(5.20)</a> para obtener
<span class="math display">\[
e_{(i)} = y_i - \mathbf{x}_i^{\prime} \mathbf{b}_{(i)} = y_i - \mathbf{x}_i^{\prime} \left( \mathbf{b} - \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}} \right)
\]</span>
<span class="math display">\[
= e_i + \frac{\mathbf{x}_i \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}} = e_i + \frac{h_{ii} e_i}{1 - h_{ii}} = \frac{e_i}{1 - h_{ii}}.
\]</span></p>
<p><strong>Estimación de Varianza Leave-One-Out.</strong> La estimación leave-one-out de la varianza se define como
<span class="math display">\[
s_{(i)}^2 = \frac{((n - 1) - (k + 1))^{-1} \sum_{j \ne i} \left( y_j - \mathbf{x}_j^{\prime} \mathbf{b}_{(i)} \right)^2}{(n - 1) - (k + 1)}.
\]</span>
Se usa en la definición del <em>residuo estandarizado</em>, definido en la Sección <a href="C5VarSelect.html#Sec531">5.3.1</a>. Una expresión computacional simple está dada por
<span class="math display" id="eq:eq521">\[\begin{equation}
s_{(i)}^2 = \frac{(n - (k + 1)) s^2 - \frac{e_i^2}{1 - h_{ii}}}{(n - 1) - (k + 1)}.
\tag{5.21}
\end{equation}\]</span>
Para ver esto, primero nota que, a partir de la ecuación <a href="C5VarSelect.html#eq:eq514">(5.14)</a>, tenemos <span class="math inline">\(\mathbf{He} = \mathbf{H(I - H) \boldsymbol{\varepsilon}} = \mathbf{0}\)</span>, porque <span class="math inline">\(\mathbf{H} = \mathbf{HH}\)</span>. En particular, desde la fila <span class="math inline">\(i\)</span>-ésima de <span class="math inline">\(\mathbf{He} = \mathbf{0}\)</span>, tenemos <span class="math inline">\(\sum_{j=1}^{n} h_{ij} e_j = 0\)</span>. Ahora, usando las ecuaciones <a href="C5VarSelect.html#eq:eq517">(5.17)</a> y <a href="C5VarSelect.html#eq:eq520">(5.20)</a>, tenemos
<span class="math display">\[
\begin{array}{ll}
\sum_{j \ne i} \left( y_j - \mathbf{x}_j^{\prime} \mathbf{b}_{(i)} \right)^2 &amp;= \sum_{j=1}^{n} \left( y_j - \mathbf{x}_j^{\prime} \mathbf{b}_{(i)} \right)^2 - \left( y_i - \mathbf{x}_i^{\prime} \mathbf{b}_{(i)} \right)^2 \\
&amp;= \sum_{j=1}^{n} \left( y_j - \mathbf{x}_j^{\prime} \mathbf{b} + \frac{\mathbf{x}_j^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}} \right) - e_{(i)}^2 \\
&amp;= \sum_{j=1}^{n} \left( e_j + \frac{h_{ij} e_i}{1 - h_{ii}} \right)^2 - \frac{e_i^2}{(1 - h_{ii})^2} \\
&amp;= \sum_{j=1}^{n} e_j^2 + 0 + \frac{e_i^2}{(1 - h_{ii})^2} h_{ii} - \frac{e_i^2}{(1 - h_{ii})^2} \\
&amp;= \sum_{j=1}^{n} e_j^2 - \frac{e_i^2}{1 - h_{ii}} = (n - (k + 1)) s^2 - \frac{e_i^2}{1 - h_{ii}}.
\end{array}
\]</span></p>
<p>Esto establece la ecuación <a href="C5VarSelect.html#eq:eq521">(5.21)</a>.</p>
</div>
<div id="Sec5103" class="section level3 hasAnchor" number="5.10.3">
<h3><span class="header-section-number">5.10.3</span> Omisión de Variables<a href="C5VarSelect.html#Sec5103" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Notación.</strong> Para medir el efecto en las cantidades de regresión, hay una serie de estadísticas de interés basadas en la noción de omitir una variable explicativa. A tal fin, la notación de superíndice <span class="math inline">\((j)\)</span> significa omitir la <span class="math inline">\(j\)</span>-ésima variable, donde <span class="math inline">\(j=0,1,\ldots,k\)</span>. Primero, recuerda que <span class="math inline">\(\mathbf{x}^{j} = (x_{1j}, x_{2j}, \ldots, x_{nj})^{\prime}\)</span> es la columna que representa la <span class="math inline">\(j\)</span>-ésima variable. Además, define <span class="math inline">\(\mathbf{X}^{(j)}\)</span> como la matriz <span class="math inline">\(n \times k\)</span> de variables explicativas definida al eliminar <span class="math inline">\(\mathbf{x}^{j}\)</span> de <span class="math inline">\(\mathbf{X}\)</span>. Por ejemplo, tomando <span class="math inline">\(j=k\)</span>, a menudo particionamos <span class="math inline">\(\mathbf{X}\)</span> como <span class="math inline">\(\mathbf{X} = \left( \mathbf{X}^{(k)}: \mathbf{x}^k \right)\)</span>. Usando los resultados de la Sección 4.7.2, utilizaremos <span class="math inline">\(\mathbf{X}^{(k)} = \mathbf{X}_1\)</span> y <span class="math inline">\(\mathbf{x}^k = \mathbf{X}_2\)</span>.</p>
<p><strong>Factor de Inflación de la Varianza.</strong> Primero, nos gustaría establecer la relación entre la definición del error estándar de <span class="math inline">\(b_j\)</span> dada por
<span class="math display">\[
se(b_j) = s \sqrt{(j+1)\text{ésimo elemento diagonal de }(\mathbf{X}^{\prime}\mathbf{X})^{-1}}
\]</span>
y la relación que involucra el factor de inflación de la varianza,
<span class="math display">\[
se(b_j) = s \frac{\sqrt{VIF_j}}{s_{x_j}\sqrt{n-1}}.
\]</span>
Por simetría de las variables independientes, solo necesitamos considerar el caso donde <span class="math inline">\(j=k\)</span>. Así, nos gustaría establecer
<span class="math display" id="eq:eq522">\[\begin{equation}
(k+1)\text{ésimo elemento diagonal de }(\mathbf{X}^{\prime}\mathbf{X})^{-1} = \frac{VIF_{k}}{(n-1) s_{x_{k}}^2}.
\tag{5.22}
\end{equation}\]</span>
Primero considera el modelo reparametrizado en la ecuación (4.22). A partir de la ecuación (4.23), podemos expresar la estimación del coeficiente de regresión
<span class="math display">\[
b_{k} = \frac{\mathbf{e}_1^{\prime}\mathbf{y}}{\mathbf{e}_1^{\prime}\mathbf{e}_1}.
\]</span>
De la ecuación (4.23), tenemos que <span class="math inline">\(\text{Var} \, b_{k} = \sigma^2 (\mathbf{E}_2^{\prime} \mathbf{E}_2)^{-1}\)</span> y así
<span class="math display" id="eq:eq523">\[\begin{equation}
se(b_{k}) = s (\mathbf{E}_2^{\prime} \mathbf{E}_2)^{-1/2}.
\tag{5.23}
\end{equation}\]</span>
Así, <span class="math inline">\((\mathbf{E}_2^{\prime} \mathbf{E}_2)^{-1}\)</span> es el <span class="math inline">\((k+1)\)</span>-ésimo elemento diagonal de
<span class="math display">\[
\left(
\begin{bmatrix}
\mathbf{X}_1^{\prime} \\
\mathbf{E}_2^{\prime}
\end{bmatrix}
\begin{bmatrix}
\mathbf{X}_1 &amp; \mathbf{E}_2
\end{bmatrix}
\right)^{-1}
\]</span>
y también es el <span class="math inline">\((k+1)\)</span>-ésimo elemento diagonal de <span class="math inline">\((\mathbf{X}^{\prime} \mathbf{X})^{-1}\)</span>. Alternativamente, esto se puede verificar directamente utilizando la inversa de la matriz particionada en la ecuación (4.19).</p>
<p>Ahora, supongamos que realizamos una regresión usando <span class="math inline">\(\mathbf{x}^{k} = \mathbf{X}_2\)</span> como el vector de respuesta y <span class="math inline">\(\mathbf{X}^{(k)} = \mathbf{X}_1\)</span> como la matriz de variables explicativas. Como se anotó arriba en la ecuación (4.22), <span class="math inline">\(\mathbf{E}_2\)</span> representa los “residuos” de esta regresión y así <span class="math inline">\(\mathbf{E}_2^{\prime} \mathbf{E}_2\)</span> representa la suma de cuadrados del error. Para esta regresión, la suma total de cuadrados es
<span class="math display">\[
\sum_{i=1}^{n} (x_{ik} - \bar{x}_{k})^2 = (n-1) s_{x_{k}}^2
\]</span>
y el coeficiente de determinación es <span class="math inline">\(R_{k}^2\)</span>. Así,
<span class="math display">\[
\mathbf{E}_2^{\prime} \mathbf{E}_2 = Error ~SS = Total ~SS (1 - R_{k}^2) = \frac{(n-1) s_{x_{k}}^2}{VIF_{k}}.
\]</span>
Esto establece el resultado.</p>
<p><strong>Estableciendo</strong> <span class="math inline">\(t^2 = F\)</span>. Para probar la hipótesis nula <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\beta_{k} = 0\)</span>, el material en la Sección 3.4.1 proporciona una descripción de una prueba basada en el estadístico <span class="math inline">\(t\)</span>, <span class="math inline">\(t(b_{k}) = \frac{b_{k}}{se(b_{k})}\)</span>. Un procedimiento alternativo de prueba, descrito en las Secciones 4.2.2, utiliza el estadístico de prueba
<span class="math display">\[
F-\text{ratio} = \frac{(Error ~SS)_{reducido} - (Error ~SS)_{completo}}{p \times (Error~MS)_{completo}} = \frac{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right)^2}{s^2 \mathbf{E}_2^{\prime} \mathbf{E}_2}
\]</span>
de la ecuación (4.26). Alternativamente, a partir de las ecuaciones (4.23) y <a href="C5VarSelect.html#eq:eq523">(5.23)</a>, tenemos
<span class="math display" id="eq:eq524">\[\begin{equation}
t(b_{k}) = \frac{b_{k}}{se(b_{k})} = \frac{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right) / \left( \mathbf{E}_2^{\prime} \mathbf{E}_2 \right)}{s / \sqrt{\mathbf{E}_2^{\prime} \mathbf{E}_2}} = \frac{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right)}{s \sqrt{\mathbf{E}_2^{\prime} \mathbf{E}_2}}.
\tag{5.24}
\end{equation}\]</span></p>
<p>Así, <span class="math inline">\(t(b_{k})^2 = F\)</span>-ratio.</p>
<p><strong>Coeficientes de Correlación Parcial.</strong> A partir del modelo de regresión completo
<span class="math display">\[
\mathbf{y} = \mathbf{X}^{(k)} \boldsymbol{\beta}^{(k)} + \mathbf{x}_{k} \beta_{k} + \boldsymbol{\varepsilon},
\]</span>
considera dos regresiones separadas. Una regresión usando <span class="math inline">\(\mathbf{x}^{k}\)</span> como el vector de respuesta y <span class="math inline">\(\mathbf{X}^{(k)}\)</span> como la matriz de variables explicativas produce los residuos <span class="math inline">\(\mathbf{E}_2\)</span>. De manera similar, una regresión con <span class="math inline">\(\mathbf{y}\)</span> como el vector de respuesta y <span class="math inline">\(\mathbf{X}^{(k)}\)</span> como la matriz de variables explicativas produce los residuos
<span class="math display">\[
\mathbf{E}_1 = \mathbf{y} - \mathbf{X}^{(k)} \left( \mathbf{X}^{(k)\prime} \mathbf{X}^{(k)} \right)^{-1} \mathbf{X}^{(k)} \mathbf{y}.
\]</span>
Si <span class="math inline">\(x^{0} = (1,1,\ldots,1)^{\prime}\)</span>, entonces el promedio de <span class="math inline">\(\mathbf{E}_1\)</span> y <span class="math inline">\(\mathbf{E}_2\)</span> es cero. En este caso, la correlación muestral entre <span class="math inline">\(\mathbf{E}_1\)</span> y <span class="math inline">\(\mathbf{E}_2\)</span> es
<span class="math display">\[
r(\mathbf{E}_1, \mathbf{E}_2) = \frac{\sum_{i=1}^{n} E_{1i} E_{2i}}{\sqrt{\left( \sum_{i=1}^{n} E_{1i}^2 \right) \left( \sum_{i=1}^{n} E_{2i}^2 \right)}} = \frac{\mathbf{E}_1^{\prime} \mathbf{E}_2}{\sqrt{\left( \mathbf{E}_1^{\prime} \mathbf{E}_1 \right) \left( \mathbf{E}_2^{\prime} \mathbf{E}_2 \right)}}.
\]</span>
Como <span class="math inline">\(\mathbf{E}_2\)</span> es un vector de residuos usando <span class="math inline">\(\mathbf{X}^{(k)}\)</span> como la matriz de variables explicativas, tenemos que <span class="math inline">\(\mathbf{E}_2^{\prime} \mathbf{X}^{(k)} = 0\)</span>. Así, para el numerador, tenemos
<span class="math display">\[
\mathbf{E}_2^{\prime} \mathbf{E}_1 = \mathbf{E}_2^{\prime} \left( \mathbf{y} - \mathbf{X}^{(k)} \left( \mathbf{X}^{(k)\prime} \mathbf{X}^{(k)} \right)^{-1} \mathbf{X}^{(k)} \mathbf{y} \right) = \mathbf{E}_2^{\prime} \mathbf{y}.
\]</span>
A partir de las ecuaciones (4.24) y (4.25), tenemos que
<span class="math display">\[
(n - (k+1)) s^2 = (Error ~SS)_{completo} = \mathbf{E}_1^{\prime} \mathbf{E}_1 - \frac{\left( \mathbf{E}_1^{\prime} \mathbf{y} \right)^2}{\mathbf{E}_2^{\prime} \mathbf{E}_2} = \mathbf{E}_1^{\prime} \mathbf{E}_1 - \frac{\left( \mathbf{E}_1^{\prime} \mathbf{E}_2 \right)^2}{\mathbf{E}_2^{\prime} \mathbf{E}_2}.
\]</span>
Así, a partir de la ecuación <a href="C5VarSelect.html#eq:eq524">(5.24)</a>
<span class="math display">\[
\begin{array}{ll}
\frac{t(b_{k})}{\sqrt{t(b_{k})^2 + n - (k+1)}} &amp;= \frac{\mathbf{E}_2^{\prime} \mathbf{y} / \left(s \sqrt{\mathbf{E}_2^{\prime} \mathbf{E}_2}\right)}{\sqrt{\frac{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right)^2}{s^2 \mathbf{E}_2^{\prime} \mathbf{E}_2} + n - (k+1)}} \\
&amp; = \frac{\mathbf{E}_2^{\prime} \mathbf{y}}{\sqrt{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right)^2 + \mathbf{E}_2^{\prime} \mathbf{E}_2 s^2 \left(n - (k+1) \right)}} \\
&amp; = \frac{\mathbf{E}_2^{\prime} \mathbf{E}_1}{\sqrt{\left( \mathbf{E}_2^{\prime} \mathbf{E}_1 \right)^2 + \mathbf{E}_2^{\prime} \mathbf{E}_2 \left( \mathbf{E}_1^{\prime} \mathbf{E}_1 - \frac{\left( \mathbf{E}_2^{\prime} \mathbf{E}_1 \right)^2}{\mathbf{E}_2^{\prime} \mathbf{E}_2} \right)}} \\
&amp;= \frac{\mathbf{E}_1^{\prime} \mathbf{E}_2}{\sqrt{(\mathbf{E}_1^{\prime} \mathbf{E}_1) (\mathbf{E}_2^{\prime} \mathbf{E}_2)}} = r(\mathbf{E}_1, \mathbf{E}_2).
\end{array}
\]</span>
Esto establece la relación entre el coeficiente de correlación parcial y el estadístico <span class="math inline">\(t\)</span>-ratio.</p>

<!-- # Chap 1 -->
<!-- # Chap 2 -->
<!-- # Chap 3 -->
<!-- # Chap 4 -->
<!-- # Chap 5 -->
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="C4MLRANOVA.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="interpretación-de-resultados-de-regresión.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
