<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Variable Selection | Regression Modeling with Actuarial and Financial Applications</title>
  <meta name="description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Variable Selection | Regression Modeling with Actuarial and Financial Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Variable Selection | Regression Modeling with Actuarial and Financial Applications" />
  
  <meta name="twitter:description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  

<meta name="author" content="Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="C4MLRANOVA.html"/>
<link rel="next" href="interpreting-regression-results.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript"  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleSolution(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}      
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>



<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Regression Modeling With Actuarial and Financial Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#forward"><i class="fa fa-check"></i>Forward</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-is-this-book-for"><i class="fa fa-check"></i>Who Is This Book For?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-is-this-book-about"><i class="fa fa-check"></i>What Is This Book About?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-does-this-book-deliver-its-message"><i class="fa fa-check"></i>How Does This Book Deliver Its Message?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html"><i class="fa fa-check"></i><b>1</b> Regression and the Normal Distribution</a>
<ul>
<li class="chapter" data-level="1.1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec11"><i class="fa fa-check"></i><b>1.1</b> What is Regression Analysis?</a></li>
<li class="chapter" data-level="1.2" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec12"><i class="fa fa-check"></i><b>1.2</b> Fitting Data to a Normal Distribution</a></li>
<li class="chapter" data-level="1.3" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec13"><i class="fa fa-check"></i><b>1.3</b> Power Transforms</a></li>
<li class="chapter" data-level="1.4" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec14"><i class="fa fa-check"></i><b>1.4</b> Sampling and the Role of Normality</a></li>
<li class="chapter" data-level="1.5" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec15"><i class="fa fa-check"></i><b>1.5</b> Regression and Sampling Designs</a></li>
<li class="chapter" data-level="1.6" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec16"><i class="fa fa-check"></i><b>1.6</b> Actuarial Applications of Regression</a></li>
<li class="chapter" data-level="1.7" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec17"><i class="fa fa-check"></i><b>1.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="1.8" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec18"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li>
<li class="chapter" data-level="1.9" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec19"><i class="fa fa-check"></i><b>1.9</b> Technical Supplement - Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="C2BasicLR.html"><a href="C2BasicLR.html"><i class="fa fa-check"></i><b>2</b> Basic Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec21"><i class="fa fa-check"></i><b>2.1</b> Correlations and Least Squares</a></li>
<li class="chapter" data-level="2.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec22"><i class="fa fa-check"></i><b>2.2</b> Basic Linear Regression Model</a></li>
<li class="chapter" data-level="2.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec23"><i class="fa fa-check"></i><b>2.3</b> Is the Model Useful? Some Basic Summary Measures</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec231"><i class="fa fa-check"></i><b>2.3.1</b> Partitioning the Variability</a></li>
<li class="chapter" data-level="2.3.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec232"><i class="fa fa-check"></i><b>2.3.2</b> The Size of a Typical Deviation: <em>s</em></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec24"><i class="fa fa-check"></i><b>2.4</b> Properties of Regression Coefficient Estimators</a></li>
<li class="chapter" data-level="2.5" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec25"><i class="fa fa-check"></i><b>2.5</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec251"><i class="fa fa-check"></i><b>2.5.1</b> Is the Explanatory Variable Important?: The <em>t</em>-Test</a></li>
<li class="chapter" data-level="2.5.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec252"><i class="fa fa-check"></i><b>2.5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="2.5.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec253"><i class="fa fa-check"></i><b>2.5.3</b> Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec26"><i class="fa fa-check"></i><b>2.6</b> Building a Better Model: Residual Analysis</a></li>
<li class="chapter" data-level="2.7" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec27"><i class="fa fa-check"></i><b>2.7</b> Application: Capital Asset Pricing Model</a></li>
<li class="chapter" data-level="2.8" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec28"><i class="fa fa-check"></i><b>2.8</b> Illustrative Regression Computer Output</a></li>
<li class="chapter" data-level="2.9" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec29"><i class="fa fa-check"></i><b>2.9</b> Further Reading and References</a></li>
<li class="chapter" data-level="2.10" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec210"><i class="fa fa-check"></i><b>2.10</b> Exercises</a></li>
<li class="chapter" data-level="2.11" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec211"><i class="fa fa-check"></i><b>2.11</b> Technical Supplement - Elements of Matrix Algebra</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2111"><i class="fa fa-check"></i><b>2.11.1</b> Basic Definitions</a></li>
<li class="chapter" data-level="2.11.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2112"><i class="fa fa-check"></i><b>2.11.2</b> Some Special Matrices</a></li>
<li class="chapter" data-level="2.11.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2113"><i class="fa fa-check"></i><b>2.11.3</b> Basic Operations</a></li>
<li class="chapter" data-level="2.11.4" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2114"><i class="fa fa-check"></i><b>2.11.4</b> Random Matrices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html"><i class="fa fa-check"></i><b>3</b> Multiple Linear Regression - I</a>
<ul>
<li class="chapter" data-level="3.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec31"><i class="fa fa-check"></i><b>3.1</b> Method of Least Squares</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec311"><i class="fa fa-check"></i><b>3.1.1</b> Least Squares Method</a></li>
<li class="chapter" data-level="3.1.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec312"><i class="fa fa-check"></i><b>3.1.2</b> General Case with <em>k</em> Explanatory Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec32"><i class="fa fa-check"></i><b>3.2</b> Linear Regression Model and Properties of Estimators</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec321"><i class="fa fa-check"></i><b>3.2.1</b> Regression Function</a></li>
<li class="chapter" data-level="3.2.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec322"><i class="fa fa-check"></i><b>3.2.2</b> Regression Coefficient Interpretation</a></li>
<li class="chapter" data-level="3.2.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec323"><i class="fa fa-check"></i><b>3.2.3</b> Model Assumptions</a></li>
<li class="chapter" data-level="3.2.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec324"><i class="fa fa-check"></i><b>3.2.4</b> Properties of Regression Coefficient Estimators</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec33"><i class="fa fa-check"></i><b>3.3</b> Estimation and Goodness of Fit</a></li>
<li class="chapter" data-level="3.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec34"><i class="fa fa-check"></i><b>3.4</b> Statistical Inference for a Single Coefficient</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec341"><i class="fa fa-check"></i><b>3.4.1</b> The <em>t</em>-Test</a></li>
<li class="chapter" data-level="3.4.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec342"><i class="fa fa-check"></i><b>3.4.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="3.4.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec343"><i class="fa fa-check"></i><b>3.4.3</b> Added Variable Plots</a></li>
<li class="chapter" data-level="3.4.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec344"><i class="fa fa-check"></i><b>3.4.4</b> Partial Correlation Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec35"><i class="fa fa-check"></i><b>3.5</b> Some Special Explanatory Variables</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec351"><i class="fa fa-check"></i><b>3.5.1</b> Binary Variables</a></li>
<li class="chapter" data-level="3.5.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec352"><i class="fa fa-check"></i><b>3.5.2</b> Transforming Explanatory Variables</a></li>
<li class="chapter" data-level="3.5.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec353"><i class="fa fa-check"></i><b>3.5.3</b> Interaction Terms</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec36"><i class="fa fa-check"></i><b>3.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="3.7" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec37"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression - II</a>
<ul>
<li class="chapter" data-level="4.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec41"><i class="fa fa-check"></i><b>4.1</b> The Role of Binary Variables</a></li>
<li class="chapter" data-level="4.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec42"><i class="fa fa-check"></i><b>4.2</b> Statistical Inference for Several Coefficients</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec421"><i class="fa fa-check"></i><b>4.2.1</b> Sets of Regression Coefficients</a></li>
<li class="chapter" data-level="4.2.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec422"><i class="fa fa-check"></i><b>4.2.2</b> The General Linear Hypothesis</a></li>
<li class="chapter" data-level="4.2.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec423"><i class="fa fa-check"></i><b>4.2.3</b> Estimating and Predicting Several Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec43"><i class="fa fa-check"></i><b>4.3</b> One Factor ANOVA Model</a></li>
<li class="chapter" data-level="4.4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec44"><i class="fa fa-check"></i><b>4.4</b> Combining Categorical and Continuous Explanatory Variables</a></li>
<li class="chapter" data-level="4.5" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec45"><i class="fa fa-check"></i><b>4.5</b> Further Reading and References</a></li>
<li class="chapter" data-level="4.6" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec46"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec47"><i class="fa fa-check"></i><b>4.7</b> Technical Supplement - Matrix Expressions</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec471"><i class="fa fa-check"></i><b>4.7.1</b> Expressing Models with Categorical Variables in Matrix Form</a></li>
<li class="chapter" data-level="4.7.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec472"><i class="fa fa-check"></i><b>4.7.2</b> Calculating Least Squares Recursively</a></li>
<li class="chapter" data-level="4.7.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec473"><i class="fa fa-check"></i><b>4.7.3</b> General Linear Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="C5VarSelect.html"><a href="C5VarSelect.html"><i class="fa fa-check"></i><b>5</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="5.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec51"><i class="fa fa-check"></i><b>5.1</b> An Iterative Approach to Data Analysis and Modeling</a></li>
<li class="chapter" data-level="5.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec52"><i class="fa fa-check"></i><b>5.2</b> Automatic Variable Selection Procedures</a></li>
<li class="chapter" data-level="5.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec53"><i class="fa fa-check"></i><b>5.3</b> Residual Analysis</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec531"><i class="fa fa-check"></i><b>5.3.1</b> Residuals</a></li>
<li class="chapter" data-level="5.3.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec532"><i class="fa fa-check"></i><b>5.3.2</b> Using Residuals to Identify Outliers</a></li>
<li class="chapter" data-level="5.3.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec533"><i class="fa fa-check"></i><b>5.3.3</b> Using Residuals to Select Explanatory Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec54"><i class="fa fa-check"></i><b>5.4</b> Influential Points</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec541"><i class="fa fa-check"></i><b>5.4.1</b> Leverage</a></li>
<li class="chapter" data-level="5.4.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec542"><i class="fa fa-check"></i><b>5.4.2</b> Cook’s Distance</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec55"><i class="fa fa-check"></i><b>5.5</b> Collinearity</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec551"><i class="fa fa-check"></i><b>5.5.1</b> What is Collinearity?</a></li>
<li class="chapter" data-level="5.5.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec552"><i class="fa fa-check"></i><b>5.5.2</b> Variance Inflation Factors</a></li>
<li class="chapter" data-level="5.5.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec553"><i class="fa fa-check"></i><b>5.5.3</b> Collinearity and Leverage</a></li>
<li class="chapter" data-level="5.5.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec554"><i class="fa fa-check"></i><b>5.5.4</b> Suppressor Variables</a></li>
<li class="chapter" data-level="5.5.5" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec555"><i class="fa fa-check"></i><b>5.5.5</b> Orthogonal Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec56"><i class="fa fa-check"></i><b>5.6</b> Selection Criteria</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec561"><i class="fa fa-check"></i><b>5.6.1</b> Goodness of Fit</a></li>
<li class="chapter" data-level="5.6.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec562"><i class="fa fa-check"></i><b>5.6.2</b> Model Validation</a></li>
<li class="chapter" data-level="5.6.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec563"><i class="fa fa-check"></i><b>5.6.3</b> Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec57"><i class="fa fa-check"></i><b>5.7</b> Heteroscedasticity</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec571"><i class="fa fa-check"></i><b>5.7.1</b> Detecting Heteroscedasticity</a></li>
<li class="chapter" data-level="5.7.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec572"><i class="fa fa-check"></i><b>5.7.2</b> Heteroscedasticity-Consistent Standard Errors</a></li>
<li class="chapter" data-level="5.7.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec573"><i class="fa fa-check"></i><b>5.7.3</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="5.7.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec574"><i class="fa fa-check"></i><b>5.7.4</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec58"><i class="fa fa-check"></i><b>5.8</b> Further Reading and References</a></li>
<li class="chapter" data-level="5.9" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec59"><i class="fa fa-check"></i><b>5.9</b> Exercises</a></li>
<li class="chapter" data-level="5.10" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec510"><i class="fa fa-check"></i><b>5.10</b> Technical Supplements for Chapter 5</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5101"><i class="fa fa-check"></i><b>5.10.1</b> Projection Matrix</a></li>
<li class="chapter" data-level="5.10.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5102"><i class="fa fa-check"></i><b>5.10.2</b> Leave One Out Statistics</a></li>
<li class="chapter" data-level="5.10.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5103"><i class="fa fa-check"></i><b>5.10.3</b> Omitting Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html"><i class="fa fa-check"></i><b>6</b> Interpreting Regression Results</a>
<ul>
<li class="chapter" data-level="6.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec61"><i class="fa fa-check"></i><b>6.1</b> What the Modeling Process Tells Us</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec611"><i class="fa fa-check"></i><b>6.1.1</b> Interpreting Individual Effects</a></li>
<li class="chapter" data-level="6.1.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec612"><i class="fa fa-check"></i><b>6.1.2</b> Other Interpretations</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec62"><i class="fa fa-check"></i><b>6.2</b> The Importance of Variable Selection</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec621"><i class="fa fa-check"></i><b>6.2.1</b> Overfitting the Model</a></li>
<li class="chapter" data-level="6.2.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec622"><i class="fa fa-check"></i><b>6.2.2</b> Underfitting the Model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec63"><i class="fa fa-check"></i><b>6.3</b> The Importance of Data Collection</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec631"><i class="fa fa-check"></i><b>6.3.1</b> Sampling Frame Error and Adverse Selection</a></li>
<li class="chapter" data-level="6.3.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec632"><i class="fa fa-check"></i><b>6.3.2</b> Limited Sampling Regions</a></li>
<li class="chapter" data-level="6.3.3" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec633"><i class="fa fa-check"></i><b>6.3.3</b> Limited Dependent Variables, Censoring and Truncation</a></li>
<li class="chapter" data-level="6.3.4" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec634"><i class="fa fa-check"></i><b>6.3.4</b> Omitted and Endogenous Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec635"><i class="fa fa-check"></i><b>6.3.5</b> Missing Data</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec64"><i class="fa fa-check"></i><b>6.4</b> Missing Data Models</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec641"><i class="fa fa-check"></i><b>6.4.1</b> Missing at Random</a></li>
<li class="chapter" data-level="6.4.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec642"><i class="fa fa-check"></i><b>6.4.2</b> Non-Ignorable Missing Data</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec65"><i class="fa fa-check"></i><b>6.5</b> Application: Risk Managers’ Cost Effectiveness</a></li>
<li class="chapter" data-level="6.6" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec66"><i class="fa fa-check"></i><b>6.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="6.7" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec67"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
<li class="chapter" data-level="6.8" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec68"><i class="fa fa-check"></i><b>6.8</b> Technical Supplements for Chapter 6</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec681"><i class="fa fa-check"></i><b>6.8.1</b> Effects of Model Misspecification</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="C7Trends.html"><a href="C7Trends.html"><i class="fa fa-check"></i><b>7</b> Modeling Trends</a>
<ul>
<li class="chapter" data-level="7.1" data-path="C7Trends.html"><a href="C7Trends.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#time-series-and-stochastic-processes"><i class="fa fa-check"></i>Time Series and Stochastic Processes</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#time-series-versus-causal-models"><i class="fa fa-check"></i>Time Series versus Causal Models</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="C7Trends.html"><a href="C7Trends.html#S7:Trends"><i class="fa fa-check"></i><b>7.2</b> Fitting Trends in Time</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#understanding-patterns-over-time"><i class="fa fa-check"></i>Understanding Patterns over Time</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#fitting-trends-in-time"><i class="fa fa-check"></i>Fitting Trends in Time</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#fitting-seasonal-trends"><i class="fa fa-check"></i>Fitting Seasonal Trends</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#reliability-of-time-series-forecasts"><i class="fa fa-check"></i>Reliability of Time Series Forecasts</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="C7Trends.html"><a href="C7Trends.html#S7:RandomWalk"><i class="fa fa-check"></i><b>7.3</b> Stationarity and Random Walk Models</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#white-noise"><i class="fa fa-check"></i>White Noise</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#random-walk"><i class="fa fa-check"></i>Random Walk</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="C7Trends.html"><a href="C7Trends.html#inference-using-random-walk-models"><i class="fa fa-check"></i><b>7.4</b> Inference using Random Walk Models</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#model-properties"><i class="fa fa-check"></i>Model Properties</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#forecasting"><i class="fa fa-check"></i>Forecasting</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#identifying-stationarity"><i class="fa fa-check"></i>Identifying Stationarity</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#identifying-random-walks"><i class="fa fa-check"></i>Identifying Random Walks</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#random-walk-versus-linear-trend-in-time-models"><i class="fa fa-check"></i>Random Walk versus Linear Trend in Time Models</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="C7Trends.html"><a href="C7Trends.html#filtering-to-achieve-stationarity"><i class="fa fa-check"></i><b>7.5</b> Filtering to Achieve Stationarity</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#transformations"><i class="fa fa-check"></i>Transformations</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="C7Trends.html"><a href="C7Trends.html#forecast-evaluation"><i class="fa fa-check"></i><b>7.6</b> Forecast Evaluation</a></li>
<li class="chapter" data-level="7.7" data-path="C7Trends.html"><a href="C7Trends.html#further-reading-and-references"><i class="fa fa-check"></i><b>7.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="7.8" data-path="C7Trends.html"><a href="C7Trends.html#exercises"><i class="fa fa-check"></i><b>7.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="C8AR.html"><a href="C8AR.html"><i class="fa fa-check"></i><b>8</b> Autocorrelations and Autoregressive Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="C8AR.html"><a href="C8AR.html#S8:Autocorrs"><i class="fa fa-check"></i><b>8.1</b> Autocorrelations</a>
<ul>
<li class="chapter" data-level="" data-path="C8AR.html"><a href="C8AR.html#application-inflation-bond-returns"><i class="fa fa-check"></i>Application: Inflation Bond Returns</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="C8AR.html"><a href="C8AR.html#autoregressive-models-of-order-one"><i class="fa fa-check"></i><b>8.2</b> Autoregressive Models of Order One</a></li>
<li class="chapter" data-level="8.3" data-path="C8AR.html"><a href="C8AR.html#S8:Estimation"><i class="fa fa-check"></i><b>8.3</b> Estimation and Diagnostic Checking</a></li>
<li class="chapter" data-level="8.4" data-path="C8AR.html"><a href="C8AR.html#S8:AR1Smooth"><i class="fa fa-check"></i><b>8.4</b> Smoothing and Prediction</a></li>
<li class="chapter" data-level="8.5" data-path="C8AR.html"><a href="C8AR.html#S8:BoxJenkins"><i class="fa fa-check"></i><b>8.5</b> Box-Jenkins Modeling and Forecasting</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="C8AR.html"><a href="C8AR.html#models"><i class="fa fa-check"></i><b>8.5.1</b> Models</a></li>
<li class="chapter" data-level="8.5.2" data-path="C8AR.html"><a href="C8AR.html#forecasting-1"><i class="fa fa-check"></i><b>8.5.2</b> Forecasting</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="C8AR.html"><a href="C8AR.html#application-hong-kong-exchange-rates"><i class="fa fa-check"></i><b>8.6</b> Application: Hong Kong Exchange Rates</a></li>
<li class="chapter" data-level="8.7" data-path="C8AR.html"><a href="C8AR.html#further-reading-and-references-1"><i class="fa fa-check"></i><b>8.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="8.8" data-path="C8AR.html"><a href="C8AR.html#exercises-1"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="C9Forecast.html"><a href="C9Forecast.html"><i class="fa fa-check"></i><b>9</b> Forecasting and Time Series Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="C9Forecast.html"><a href="C9Forecast.html#smoothing-with-moving-averages"><i class="fa fa-check"></i><b>9.1</b> Smoothing with Moving Averages</a></li>
<li class="chapter" data-level="9.2" data-path="C9Forecast.html"><a href="C9Forecast.html#S9:ExponSmooth"><i class="fa fa-check"></i><b>9.2</b> Exponential Smoothing</a></li>
<li class="chapter" data-level="9.3" data-path="C9Forecast.html"><a href="C9Forecast.html#S9:SeasonalTSModels"><i class="fa fa-check"></i><b>9.3</b> Seasonal Time Series Models</a></li>
<li class="chapter" data-level="9.4" data-path="C9Forecast.html"><a href="C9Forecast.html#unit-root-tests"><i class="fa fa-check"></i><b>9.4</b> Unit Root Tests</a></li>
<li class="chapter" data-level="9.5" data-path="C9Forecast.html"><a href="C9Forecast.html#archgarch-models"><i class="fa fa-check"></i><b>9.5</b> ARCH/GARCH Models</a></li>
<li class="chapter" data-level="9.6" data-path="C9Forecast.html"><a href="C9Forecast.html#further-reading-and-references-2"><i class="fa fa-check"></i><b>9.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="C10Panel.html"><a href="C10Panel.html"><i class="fa fa-check"></i><b>10</b> Longitudinal and Panel Data Models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="C10Panel.html"><a href="C10Panel.html#S10:Intro"><i class="fa fa-check"></i><b>10.1</b> What are Longitudinal and Panel Data?</a></li>
<li class="chapter" data-level="10.2" data-path="C10Panel.html"><a href="C10Panel.html#S10:Visual"><i class="fa fa-check"></i><b>10.2</b> Visualizing Longitudinal and Panel Data</a></li>
<li class="chapter" data-level="10.3" data-path="C10Panel.html"><a href="C10Panel.html#S10:FEModels"><i class="fa fa-check"></i><b>10.3</b> Basic Fixed Effects Models</a></li>
<li class="chapter" data-level="10.4" data-path="C10Panel.html"><a href="C10Panel.html#S10:FEModels2"><i class="fa fa-check"></i><b>10.4</b> Extended Fixed Effects Models</a></li>
<li class="chapter" data-level="10.5" data-path="C10Panel.html"><a href="C10Panel.html#S10:REModels"><i class="fa fa-check"></i><b>10.5</b> Random Effects Models</a></li>
<li class="chapter" data-level="10.6" data-path="C10Panel.html"><a href="C10Panel.html#S10:References"><i class="fa fa-check"></i><b>10.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="C11Binary.html"><a href="C11Binary.html"><i class="fa fa-check"></i><b>11</b> Categorical Dependent Variables</a>
<ul>
<li class="chapter" data-level="11.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec111"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables</a></li>
<li class="chapter" data-level="11.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec112"><i class="fa fa-check"></i><b>11.2</b> Logistic and Probit Regression Models</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1121"><i class="fa fa-check"></i><b>11.2.1</b> Using Nonlinear Functions of Explanatory Variables</a></li>
<li class="chapter" data-level="11.2.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1122"><i class="fa fa-check"></i><b>11.2.2</b> Threshold Interpretation</a></li>
<li class="chapter" data-level="11.2.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1123"><i class="fa fa-check"></i><b>11.2.3</b> Random Utility Interpretation</a></li>
<li class="chapter" data-level="11.2.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1124"><i class="fa fa-check"></i><b>11.2.4</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec113"><i class="fa fa-check"></i><b>11.3</b> Inference for Logistic and Probit Regression Models</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="C11Binary.html"><a href="C11Binary.html#parameter-estimation"><i class="fa fa-check"></i><b>11.3.1</b> Parameter Estimation</a></li>
<li class="chapter" data-level="11.3.2" data-path="C11Binary.html"><a href="C11Binary.html#additional-inference"><i class="fa fa-check"></i><b>11.3.2</b> Additional Inference</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec114"><i class="fa fa-check"></i><b>11.4</b> Application: Medical Expenditures</a></li>
<li class="chapter" data-level="11.5" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec115"><i class="fa fa-check"></i><b>11.5</b> Nominal Dependent Variables</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1151"><i class="fa fa-check"></i><b>11.5.1</b> Generalized Logit</a></li>
<li class="chapter" data-level="11.5.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1152"><i class="fa fa-check"></i><b>11.5.2</b> Multinomial Logit</a></li>
<li class="chapter" data-level="11.5.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1153"><i class="fa fa-check"></i><b>11.5.3</b> Nested Logit</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec116"><i class="fa fa-check"></i><b>11.6</b> Ordinal Dependent Variables</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="C11Binary.html"><a href="C11Binary.html#cumulative-logit"><i class="fa fa-check"></i><b>11.6.1</b> Cumulative Logit</a></li>
<li class="chapter" data-level="11.6.2" data-path="C11Binary.html"><a href="C11Binary.html#cumulative-probit"><i class="fa fa-check"></i><b>11.6.2</b> Cumulative Probit</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec117"><i class="fa fa-check"></i><b>11.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="11.8" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec118"><i class="fa fa-check"></i><b>11.8</b> Exercises</a></li>
<li class="chapter" data-level="11.9" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec119"><i class="fa fa-check"></i><b>11.9</b> Technical Supplements - Likelihood-Based Inference</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1191"><i class="fa fa-check"></i><b>11.9.1</b> Properties of Likelihood Functions</a></li>
<li class="chapter" data-level="11.9.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1192"><i class="fa fa-check"></i><b>11.9.2</b> Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="11.9.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1193"><i class="fa fa-check"></i><b>11.9.3</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="11.9.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1194"><i class="fa fa-check"></i><b>11.9.4</b> Information Criteria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="C12Count.html"><a href="C12Count.html"><i class="fa fa-check"></i><b>12</b> Count Dependent Variables</a>
<ul>
<li class="chapter" data-level="12.1" data-path="C12Count.html"><a href="C12Count.html#S:Sec121"><i class="fa fa-check"></i><b>12.1</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="C12Count.html"><a href="C12Count.html#S:Sec1211"><i class="fa fa-check"></i><b>12.1.1</b> Poisson Distribution</a></li>
<li class="chapter" data-level="12.1.2" data-path="C12Count.html"><a href="C12Count.html#S:Sec1212"><i class="fa fa-check"></i><b>12.1.2</b> Regression Model</a></li>
<li class="chapter" data-level="12.1.3" data-path="C12Count.html"><a href="C12Count.html#S:Sec1213"><i class="fa fa-check"></i><b>12.1.3</b> Estimation</a></li>
<li class="chapter" data-level="12.1.4" data-path="C12Count.html"><a href="C12Count.html#S:Sec1214"><i class="fa fa-check"></i><b>12.1.4</b> Additional Inference</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="C12Count.html"><a href="C12Count.html#S:Sec122"><i class="fa fa-check"></i><b>12.2</b> Application: Singapore Automobile Insurance</a></li>
<li class="chapter" data-level="12.3" data-path="C12Count.html"><a href="C12Count.html#S:Sec123"><i class="fa fa-check"></i><b>12.3</b> Overdispersion and Negative Binomial Models</a></li>
<li class="chapter" data-level="12.4" data-path="C12Count.html"><a href="C12Count.html#S:Sec124"><i class="fa fa-check"></i><b>12.4</b> Other Count Models</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="C12Count.html"><a href="C12Count.html#zero-inflated-models"><i class="fa fa-check"></i><b>12.4.1</b> Zero-Inflated Models</a></li>
<li class="chapter" data-level="12.4.2" data-path="C12Count.html"><a href="C12Count.html#hurdle-models"><i class="fa fa-check"></i><b>12.4.2</b> Hurdle Models</a></li>
<li class="chapter" data-level="12.4.3" data-path="C12Count.html"><a href="C12Count.html#heterogeneity-models"><i class="fa fa-check"></i><b>12.4.3</b> Heterogeneity Models</a></li>
<li class="chapter" data-level="12.4.4" data-path="C12Count.html"><a href="C12Count.html#latent-class-models"><i class="fa fa-check"></i><b>12.4.4</b> Latent Class Models</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="C12Count.html"><a href="C12Count.html#S:Sec125"><i class="fa fa-check"></i><b>12.5</b> Further Reading and References</a></li>
<li class="chapter" data-level="12.6" data-path="C12Count.html"><a href="C12Count.html#S:Sec126"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="C13GLM.html"><a href="C13GLM.html"><i class="fa fa-check"></i><b>13</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec131"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec132"><i class="fa fa-check"></i><b>13.2</b> GLM Model</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1321"><i class="fa fa-check"></i><b>13.2.1</b> Linear Exponential Family of Distributions</a></li>
<li class="chapter" data-level="13.2.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1322"><i class="fa fa-check"></i><b>13.2.2</b> Link Functions</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec133"><i class="fa fa-check"></i><b>13.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1331"><i class="fa fa-check"></i><b>13.3.1</b> Maximum Likelihood Estimation for Canonical Links</a></li>
<li class="chapter" data-level="13.3.2" data-path="C13GLM.html"><a href="C13GLM.html#overdispersion"><i class="fa fa-check"></i><b>13.3.2</b> Overdispersion</a></li>
<li class="chapter" data-level="13.3.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1333"><i class="fa fa-check"></i><b>13.3.3</b> Goodness of Fit Statistics</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec134"><i class="fa fa-check"></i><b>13.4</b> Application: Medical Expenditures</a></li>
<li class="chapter" data-level="13.5" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec135"><i class="fa fa-check"></i><b>13.5</b> Residuals</a></li>
<li class="chapter" data-level="13.6" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec136"><i class="fa fa-check"></i><b>13.6</b> Tweedie Distribution</a></li>
<li class="chapter" data-level="13.7" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec137"><i class="fa fa-check"></i><b>13.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="13.8" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec138"><i class="fa fa-check"></i><b>13.8</b> Exercises</a></li>
<li class="chapter" data-level="13.9" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec139"><i class="fa fa-check"></i><b>13.9</b> Technical Supplements - Exponential Family</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1391"><i class="fa fa-check"></i><b>13.9.1</b> Linear Exponential Family of Distributions</a></li>
<li class="chapter" data-level="13.9.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1392"><i class="fa fa-check"></i><b>13.9.2</b> Moments</a></li>
<li class="chapter" data-level="13.9.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1393"><i class="fa fa-check"></i><b>13.9.3</b> Maximum Likelihood Estimation for General Links</a></li>
<li class="chapter" data-level="13.9.4" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1394"><i class="fa fa-check"></i><b>13.9.4</b> Iterated Reweighted Least Squares</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="C14Survival.html"><a href="C14Survival.html"><i class="fa fa-check"></i><b>14</b> Survival Models</a>
<ul>
<li class="chapter" data-level="14.1" data-path="C14Survival.html"><a href="C14Survival.html#introduction-2"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec142"><i class="fa fa-check"></i><b>14.2</b> Censoring and Truncation</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="C14Survival.html"><a href="C14Survival.html#definitions-and-examples"><i class="fa fa-check"></i><b>14.2.1</b> Definitions and Examples</a></li>
<li class="chapter" data-level="14.2.2" data-path="C14Survival.html"><a href="C14Survival.html#likelihood-inference"><i class="fa fa-check"></i><b>14.2.2</b> Likelihood Inference</a></li>
<li class="chapter" data-level="14.2.3" data-path="C14Survival.html"><a href="C14Survival.html#product-limit-estimator"><i class="fa fa-check"></i><b>14.2.3</b> Product-Limit Estimator</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec143"><i class="fa fa-check"></i><b>14.3</b> Accelerated Failure Time Model</a></li>
<li class="chapter" data-level="14.4" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec144"><i class="fa fa-check"></i><b>14.4</b> Proportional Hazards Model</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec1441"><i class="fa fa-check"></i><b>14.4.1</b> Proportional Hazards</a></li>
<li class="chapter" data-level="14.4.2" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec1442"><i class="fa fa-check"></i><b>14.4.2</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec145"><i class="fa fa-check"></i><b>14.5</b> Recurrent Events</a></li>
<li class="chapter" data-level="14.6" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec146"><i class="fa fa-check"></i><b>14.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="C15Misc.html"><a href="C15Misc.html"><i class="fa fa-check"></i><b>15</b> Miscellaneous Regression Topics</a>
<ul>
<li class="chapter" data-level="15.1" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec151"><i class="fa fa-check"></i><b>15.1</b> Mixed Linear Models</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="C15Misc.html"><a href="C15Misc.html#weighted-least-squares-2"><i class="fa fa-check"></i><b>15.1.1</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="15.1.2" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec1512"><i class="fa fa-check"></i><b>15.1.2</b> Variance Components Estimation</a></li>
<li class="chapter" data-level="15.1.3" data-path="C15Misc.html"><a href="C15Misc.html#best-linear-unbiased-prediction"><i class="fa fa-check"></i><b>15.1.3</b> Best Linear Unbiased Prediction</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="C15Misc.html"><a href="C15Misc.html#bayesian-regression"><i class="fa fa-check"></i><b>15.2</b> Bayesian Regression</a></li>
<li class="chapter" data-level="15.3" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec153"><i class="fa fa-check"></i><b>15.3</b> Density Estimation and Scatterplot Smoothing}</a></li>
<li class="chapter" data-level="15.4" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec154"><i class="fa fa-check"></i><b>15.4</b> Generalized Additive Models</a></li>
<li class="chapter" data-level="15.5" data-path="C15Misc.html"><a href="C15Misc.html#bootstrapping"><i class="fa fa-check"></i><b>15.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="15.6" data-path="C15Misc.html"><a href="C15Misc.html#further-reading-and-references-3"><i class="fa fa-check"></i><b>15.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="C16FreqSev.html"><a href="C16FreqSev.html"><i class="fa fa-check"></i><b>16</b> Frequency-Severity Models</a>
<ul>
<li class="chapter" data-level="16.1" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec161"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec162"><i class="fa fa-check"></i><b>16.2</b> Tobit Model</a></li>
<li class="chapter" data-level="16.3" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec163"><i class="fa fa-check"></i><b>16.3</b> Application: Medical Expenditures</a></li>
<li class="chapter" data-level="16.4" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec164"><i class="fa fa-check"></i><b>16.4</b> Two-Part Model</a></li>
<li class="chapter" data-level="16.5" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec165"><i class="fa fa-check"></i><b>16.5</b> Aggregate Loss Model</a></li>
<li class="chapter" data-level="16.6" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec166"><i class="fa fa-check"></i><b>16.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="16.7" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec167"><i class="fa fa-check"></i><b>16.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="C17Fat.html"><a href="C17Fat.html"><i class="fa fa-check"></i><b>17</b> Fat-Tailed Regression Models</a>
<ul>
<li class="chapter" data-level="17.1" data-path="C17Fat.html"><a href="C17Fat.html#introduction-3"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec172"><i class="fa fa-check"></i><b>17.2</b> Transformations</a></li>
<li class="chapter" data-level="17.3" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec173"><i class="fa fa-check"></i><b>17.3</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec1731"><i class="fa fa-check"></i><b>17.3.1</b> What is “Fat-Tailed?”</a></li>
<li class="chapter" data-level="17.3.2" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec1732"><i class="fa fa-check"></i><b>17.3.2</b> Application: Wisconsin Nursing Homes</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec174"><i class="fa fa-check"></i><b>17.4</b> Generalized Distributions</a>
<ul>
<li class="chapter" data-level="" data-path="C17Fat.html"><a href="C17Fat.html#applicationwisconsin-nursing-homes"><i class="fa fa-check"></i>Application:Wisconsin Nursing Homes</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec175"><i class="fa fa-check"></i><b>17.5</b> Quantile Regression</a></li>
<li class="chapter" data-level="17.6" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec176"><i class="fa fa-check"></i><b>17.6</b> Extreme Value Models</a></li>
<li class="chapter" data-level="17.7" data-path="C17Fat.html"><a href="C17Fat.html#further-reading-and-references-4"><i class="fa fa-check"></i><b>17.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="17.8" data-path="C17Fat.html"><a href="C17Fat.html#exercises-2"><i class="fa fa-check"></i><b>17.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="C18Cred.html"><a href="C18Cred.html"><i class="fa fa-check"></i><b>18</b> Credibility and Bonus-Malus</a>
<ul>
<li class="chapter" data-level="18.1" data-path="C18Cred.html"><a href="C18Cred.html#risk-classification-and-experience-rating"><i class="fa fa-check"></i><b>18.1</b> Risk Classification and Experience Rating</a></li>
<li class="chapter" data-level="18.2" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec182"><i class="fa fa-check"></i><b>18.2</b> Credibility</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec1821"><i class="fa fa-check"></i><b>18.2.1</b> Limited Fluctuation Credibility</a></li>
<li class="chapter" data-level="18.2.2" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec1822"><i class="fa fa-check"></i><b>18.2.2</b> Greatest Accuracy Credibility</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec183"><i class="fa fa-check"></i><b>18.3</b> Credibility and Regression</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="C18Cred.html"><a href="C18Cred.html#one-way-random-effects-model"><i class="fa fa-check"></i><b>18.3.1</b> One-Way Random Effects Model</a></li>
<li class="chapter" data-level="18.3.2" data-path="C18Cred.html"><a href="C18Cred.html#longitudinal-models"><i class="fa fa-check"></i><b>18.3.2</b> Longitudinal Models</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec184"><i class="fa fa-check"></i><b>18.4</b> Bonus-Malus</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="C19Triangles.html"><a href="C19Triangles.html"><i class="fa fa-check"></i><b>19</b> Claims Triangles</a>
<ul>
<li class="chapter" data-level="19.1" data-path="C19Triangles.html"><a href="C19Triangles.html#introduction-4"><i class="fa fa-check"></i><b>19.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1911"><i class="fa fa-check"></i><b>19.1.1</b> Claims Evolution</a></li>
<li class="chapter" data-level="19.1.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1912"><i class="fa fa-check"></i><b>19.1.2</b> Claims Triangles</a></li>
<li class="chapter" data-level="19.1.3" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1913"><i class="fa fa-check"></i><b>19.1.3</b> Chain Ladder Method</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec192"><i class="fa fa-check"></i><b>19.2</b> Regression Using Functions of Time as Explanatory Variables</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1921"><i class="fa fa-check"></i><b>19.2.1</b> Lognormal Model</a></li>
<li class="chapter" data-level="19.2.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1922"><i class="fa fa-check"></i><b>19.2.2</b> Hoerl Curve</a></li>
<li class="chapter" data-level="19.2.3" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1923"><i class="fa fa-check"></i><b>19.2.3</b> Poisson Models</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec193"><i class="fa fa-check"></i><b>19.3</b> Using Past Developments</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1931"><i class="fa fa-check"></i><b>19.3.1</b> Mack Model</a></li>
<li class="chapter" data-level="19.3.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1932"><i class="fa fa-check"></i><b>19.3.2</b> Distributional Models</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="C19Triangles.html"><a href="C19Triangles.html#further-reading-and-references-5"><i class="fa fa-check"></i><b>19.4</b> Further Reading and References</a></li>
<li class="chapter" data-level="19.5" data-path="C19Triangles.html"><a href="C19Triangles.html#exercises-3"><i class="fa fa-check"></i><b>19.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="C20Report.html"><a href="C20Report.html"><i class="fa fa-check"></i><b>20</b> Report Writing: Communicating Data Analysis Results</a>
<ul>
<li class="chapter" data-level="20.1" data-path="C20Report.html"><a href="C20Report.html#S20:Overview"><i class="fa fa-check"></i><b>20.1</b> Overview</a></li>
<li class="chapter" data-level="20.2" data-path="C20Report.html"><a href="C20Report.html#S20:Methods"><i class="fa fa-check"></i><b>20.2</b> Methods for Communicating Data</a>
<ul>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#within-text-data"><i class="fa fa-check"></i>Within Text Data</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#graphs"><i class="fa fa-check"></i>Graphs</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="C20Report.html"><a href="C20Report.html#S20:Organize"><i class="fa fa-check"></i><b>20.3</b> How to Organize</a>
<ul>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#title-and-abstract"><i class="fa fa-check"></i>Title and Abstract</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#introduction-5"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#model-selection-and-interpretation"><i class="fa fa-check"></i>Model Selection and Interpretation</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#references-and-appendix"><i class="fa fa-check"></i>References and Appendix</a></li>
</ul></li>
<li class="chapter" data-level="20.4" data-path="C20Report.html"><a href="C20Report.html#further-suggestions-for-report-writing"><i class="fa fa-check"></i><b>20.4</b> Further Suggestions for Report Writing</a></li>
<li class="chapter" data-level="20.5" data-path="C20Report.html"><a href="C20Report.html#case-study-swedish-automobile-claims"><i class="fa fa-check"></i><b>20.5</b> Case Study: Swedish Automobile Claims</a></li>
<li class="chapter" data-level="20.6" data-path="C20Report.html"><a href="C20Report.html#further-reading-and-references-6"><i class="fa fa-check"></i><b>20.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="20.7" data-path="C20Report.html"><a href="C20Report.html#exercises-4"><i class="fa fa-check"></i><b>20.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="C21Design.html"><a href="C21Design.html"><i class="fa fa-check"></i><b>21</b> Designing Effective Graphs</a>
<ul>
<li class="chapter" data-level="21.1" data-path="C21Design.html"><a href="C21Design.html#S21:Intro"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="C21Design.html"><a href="C21Design.html#S21:GDesign"><i class="fa fa-check"></i><b>21.2</b> Graphic Design Choices Make a Difference</a></li>
<li class="chapter" data-level="21.3" data-path="C21Design.html"><a href="C21Design.html#S21:DesignGuide"><i class="fa fa-check"></i><b>21.3</b> Design Guidelines</a>
<ul>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-one-avoid-chartjunk"><i class="fa fa-check"></i>Guideline One: Avoid Chartjunk</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-two-use-small-multiples-to-promote-comparisons-and-assess-change"><i class="fa fa-check"></i>Guideline Two: Use Small Multiples to Promote Comparisons and Assess Change</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-three-use-complex-graphs-to-portray-complex-patterns"><i class="fa fa-check"></i>Guideline Three: Use Complex Graphs to Portray Complex Patterns</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-four-relate-graph-size-to-information-content"><i class="fa fa-check"></i>Guideline Four: Relate Graph Size to Information Content</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-five-use-graphical-forms-that-promote-comparisons"><i class="fa fa-check"></i>Guideline Five: Use Graphical Forms That Promote Comparisons</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-six-integrate-graphs-and-text"><i class="fa fa-check"></i>Guideline Six: Integrate Graphs and Text</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-seven-demonstrate-an-important-message"><i class="fa fa-check"></i>Guideline Seven: Demonstrate an Important Message</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-eight-know-your-audience"><i class="fa fa-check"></i>Guideline Eight: Know Your Audience</a></li>
</ul></li>
<li class="chapter" data-level="21.4" data-path="C21Design.html"><a href="C21Design.html#S21:EmpiricalFoundations"><i class="fa fa-check"></i><b>21.4</b> Empirical Foundations For Guidelines</a>
<ul>
<li class="chapter" data-level="21.4.1" data-path="C21Design.html"><a href="C21Design.html#graphs-as-units-of-study"><i class="fa fa-check"></i><b>21.4.1</b> Graphs as Units of Study</a></li>
</ul></li>
<li class="chapter" data-level="21.5" data-path="C21Design.html"><a href="C21Design.html#S21:Conclude"><i class="fa fa-check"></i><b>21.5</b> Concluding Remarks</a></li>
<li class="chapter" data-level="21.6" data-path="C21Design.html"><a href="C21Design.html#S21:References"><i class="fa fa-check"></i><b>21.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i><b>22</b> Appendices</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a1.-basic-statistical-inference"><i class="fa fa-check"></i>Appendix A1. Basic Statistical Inference</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#distributions-of-functions-of-random-variables"><i class="fa fa-check"></i>Distributions of Functions of Random Variables</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#estimation-and-prediction"><i class="fa fa-check"></i>Estimation and Prediction</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#testing-hypotheses"><i class="fa fa-check"></i>Testing Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a2.-matrix-algebra"><i class="fa fa-check"></i>Appendix A2. Matrix Algebra</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#basic-definitions"><i class="fa fa-check"></i>Basic Definitions</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#review-of-basic-operations"><i class="fa fa-check"></i>Review of Basic Operations</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#further-definitions"><i class="fa fa-check"></i>Further Definitions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a3.-probability-tables"><i class="fa fa-check"></i>Appendix A3. Probability Tables</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#normal-distribution"><i class="fa fa-check"></i>Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#chi-square-distribution"><i class="fa fa-check"></i>Chi-Square Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#t-distribution"><i class="fa fa-check"></i><em>t</em>-Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#f-distribution"><i class="fa fa-check"></i><em>F</em>-Distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="brief-answers-to-selected-exercises.html"><a href="brief-answers-to-selected-exercises.html"><i class="fa fa-check"></i>Brief Answers to Selected Exercises</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Regression Modeling with Actuarial and Financial Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="C5VarSelect" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Variable Selection<a href="C5VarSelect.html#C5VarSelect" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Chapter Preview</em>. This chapter describes tools and techniques to help you select variables to enter into a linear regression model, beginning with an iterative model selection process. In applications with many potential explanatory variables, automatic variable selection procedures will help you quickly evaluate many models. Nonetheless, automatic procedures have serious limitations, including the inability to account properly for nonlinearities such as the impact of unusual points; this chapter expands upon the Chapter 2 discussion of unusual points. It also describes collinearity, a common feature of regression data where explanatory variables are linearly related to one another. Other topics that impact variable selection, including heteroscedasticity and out-of-sample validation, are also introduced.</p>
<div id="Sec51" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> An Iterative Approach to Data Analysis and Modeling<a href="C5VarSelect.html#Sec51" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In our introduction of basic linear regression in Chapter 2, we examined the data graphically, hypothesized a model structure, and compared the data to a candidate model to formulate an improved model. Box (1980) describes this as an <em>iterative process</em>, which is shown in Figure <a href="C5VarSelect.html#fig:Fig51">5.1</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig51"></span>
<img src="RegressionMarkdown_files/figure-html/Fig51-1.png" alt="The iterative model specification process" width="80%" />
<p class="caption">
Figure 5.1: <strong>The iterative model specification process</strong>
</p>
</div>
<p>This iterative process provides a useful recipe for structuring the task of specifying a model to represent a set of data. The first step, the model formulation stage, is accomplished by examining the data graphically and using prior knowledge of relationships, such as from economic theory or standard industry practice. The second step in the iteration is based on the assumptions of the specified model. These assumptions must be consistent with the data to make valid use of the model. The third step, <em>diagnostic checking</em>, is also known as <em>data and model criticism</em>; the data and model must be consistent with one another before additional inferences can be made. Diagnostic checking is an important part of the model formulation; it can reveal mistakes made in previous steps and provide ways to correct these mistakes.</p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_gpirwexl&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_yfz3do6b%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="5.1 IterativeApproach">
</iframe>
</center>
</div>
<div id="Sec52" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Automatic Variable Selection Procedures<a href="C5VarSelect.html#Sec52" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Business and economics relationships are complicated; there are typically many variables that could serve as useful predictors of the dependent variable. In searching for a suitable relationship, there is a large number of potential models that are based on linear combinations of explanatory variables and an infinite number that can be formed from nonlinear combinations. To search among models based on linear combinations, several automatic procedures are available to select variables to be included in the model. These automatic procedures are easy to use and will suggest one or more models that you can explore in further detail.</p>
<p>To illustrate how large is the potential number of linear models, suppose that there are only four variables, f<span class="math inline">\(x_{1},\)</span> <span class="math inline">\(x_2,\)</span> <span class="math inline">\(x_3\)</span> and <span class="math inline">\(x_4\)</span>, under consideration for fitting a model to <span class="math inline">\(y\)</span>. Without any consideration of multiplication or other nonlinear combinations of explanatory variables, how many possible models are there? Table <a href="C5VarSelect.html#tab:Tab51">5.1</a> shows that the answer is 16.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab51">Table 5.1: </span><strong>Sixteen Possible Models</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Expression
</th>
<th style="text-align:center;">
Combinations
</th>
<th style="text-align:left;">
Models
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 7cm; border-right:1px solid;">
E <span class="math inline">\(y=\beta_0\)</span>
</td>
<td style="text-align:center;width: 3cm; border-right:1px solid;">
</td>
<td style="text-align:left;width: 3cm; width: 7cm; ">
1 model with no independent variables
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 7cm; border-right:1px solid;">
E <span class="math inline">\(y=\beta_0+\beta_1x_i\)</span>
</td>
<td style="text-align:center;width: 3cm; border-right:1px solid;">
<span class="math inline">\(i\)</span> = 1,2,3,4
</td>
<td style="text-align:left;width: 3cm; width: 7cm; ">
4 models with one independent variable
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 7cm; border-right:1px solid;">
E <span class="math inline">\(y = \beta_0 + \beta_1 x_i + \beta_2 x_j\)</span>
</td>
<td style="text-align:center;width: 3cm; border-right:1px solid;">
(<span class="math inline">\(i,j\)</span>) = (1,2),(1,3),(1,4),(2,3),(2,4),(3,4)
</td>
<td style="text-align:left;width: 3cm; width: 7cm; ">
6 models with two independent variables
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 7cm; border-right:1px solid;">
E <span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \beta_2 x_j +\beta_3x_{k}\)</span>
</td>
<td style="text-align:center;width: 3cm; border-right:1px solid;">
(<span class="math inline">\(i,j,k\)</span>) = (1,2,3),(1,2,4),(1,3,4),(2,3,4)
</td>
<td style="text-align:left;width: 3cm; width: 7cm; ">
4 models with three independent variables
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 7cm; border-right:1px solid;">
E <span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 +\beta_3 x_3 + \beta_4 x_4\)</span>
</td>
<td style="text-align:center;width: 3cm; border-right:1px solid;">
</td>
<td style="text-align:left;width: 3cm; width: 7cm; ">
1 model with all independent variables
</td>
</tr>
</tbody>
</table>
<p>If there were only three explanatory variables, then you can use the same logic to verify that there are eight possible models. Extrapolating from these two examples, how many linear models will there be if there are ten explanatory variables? The answer is 1,024, which is quite a few. In general, the answer is <span class="math inline">\(2^k\)</span>, where <span class="math inline">\(k\)</span> is the number of explanatory variables. For example, <span class="math inline">\(2^3\)</span> is 8, <span class="math inline">\(2^4\)</span> is 16, and so on.</p>
<p>In any case, for a moderately large number of explanatory variables, there are many potential models that are based on linear combinations of explanatory variables. We would like a procedure to search quickly through these potential models to give us more time to think about other interesting aspects of model selection. <em>Stepwise regression</em> are procedures that employ <span class="math inline">\(t\)</span>-tests to check the “significance” of explanatory variables entered into, or deleted from, the model.</p>
<p>To begin, in the <em>forward selection</em> version of stepwise regression, variables are added one at a time. In the first stage, out of all the candidate variables the one that is most statistically significant is added to the model. At the next stage, with the first stage variable already included, the next most statistically significant variable is added. This procedure is repeated until all statistically significant variables have been added. Here, statistical significance is typically assessed using a variable’s <span class="math inline">\(t\)</span>-ratio – the cut-off for statistical significance is typically a pre-determined <span class="math inline">\(t\)</span>-value (such as two, corresponding to an approximate 95% significance level).</p>
<p>The <em>backwards selection</em> version works in a similar manner, except that all variables are included in the initial stage and then are dropped one at a time (instead of added).</p>
<p>More generally, an algorithm that adds and deletes variables at each stage is sometimes known as <em>the</em> stepwise regression algorithm.</p>
<div class="blackbox">
<p><em>Stepwise Regression Algorithm.</em> Suppose that the analyst has identified one variable as the response, <span class="math inline">\(y\)</span>, and <span class="math inline">\(k\)</span> potential explanatory variables, <span class="math inline">\(x_1, x_2, \ldots, x_k\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Consider all possible regressions using one explanatory variable. For each of the <span class="math inline">\(k\)</span> regressions, compute <span class="math inline">\(t(b_1)\)</span>, the <span class="math inline">\(t\)</span>-ratio for the slope. Choose that variable with the largest <span class="math inline">\(t\)</span>-ratio. If the <span class="math inline">\(t\)</span>-ratio does not exceed a pre-specified <span class="math inline">\(t\)</span>-value (such as two), then do not choose any variables and halt the procedure.</li>
<li>Add a variable to the model from the previous step. The variable to enter is the one that makes the largest significant contribution. To determine the size of contribution, use the absolute value of the variable’s <span class="math inline">\(t\)</span>-ratio. To enter, the <span class="math inline">\(t\)</span>-ratio must exceed a specified <span class="math inline">\(t\)</span>-value in absolute value.</li>
<li>Delete a variable from the model in the previous step. The variable to be removed is the one that makes the smallest contribution. To determine the size of contribution, use the absolute value of the variable’s <span class="math inline">\(t\)</span>-ratio. To be removed, the <span class="math inline">\(t\)</span>-ratio must be less than a specified <span class="math inline">\(t\)</span>-value in absolute value.</li>
<li>Repeat steps (ii) and (iii) until all possible additions and deletions are performed.</li>
</ol>
</div>
<p>When implementing this routine, some statistical software packages use an <span class="math inline">\(F\)</span>-test in lieu of <span class="math inline">\(t\)</span>-tests. Recall, when only one variable is being considered, that <span class="math inline">\((t\text{-ratio})^2 = F\)</span>-ratio and thus these procedures are equivalent.</p>
<p>This algorithm is useful in that it quickly searches through a number of candidate models. However, there are several drawbacks:</p>
<ol style="list-style-type: decimal">
<li>The procedure “snoops” through a large number of models and may fit the data “too well.”</li>
<li>There is no guarantee that the selected model is the best. The algorithm does not consider models that are based on nonlinear combinations of explanatory variables. It also ignores the presence of outliers and high leverage points.</li>
<li>In addition, the algorithm does not even search all <span class="math inline">\(2^{k}\)</span> possible linear regressions.</li>
<li>The algorithm uses one criterion, a <span class="math inline">\(t\)</span>-ratio, and does not consider other criteria such as <span class="math inline">\(s\)</span>, <span class="math inline">\(R^2\)</span>, <span class="math inline">\(R_a^2\)</span>, and so on.</li>
<li>There is a sequence of significance tests involved. Thus, the significance level that determines the <span class="math inline">\(t\)</span>-value is not meaningful.</li>
<li>By considering each variable separately, the algorithm does not take into account the joint effect of explanatory variables.</li>
<li>Purely automatic procedures may not take into account an investigator’s special knowledge.</li>
</ol>
<p>Many of the criticisms of the basic stepwise regression algorithm can be addressed with modern computing software that is now widely available. We now consider each drawback, in reverse order. To respond to drawback number (7), many statistical software routines have options for forcing variables into a model equation. In this way, if other evidence indicates that one or more variables should be included in the model, then the investigator can force the inclusion of these variables.</p>
<p>For drawback number (6), in Section <a href="C5VarSelect.html#Sec554">5.5.4</a> on <em>suppressor variables</em>, we will provide examples of variables that do not have important individual effects but are important when considered jointly. These combinations of variables may not be detected with the basic algorithm but will be detected with the backwards selection algorithm. Because the backwards procedure starts with all variables, it will detect, and retain, variables that are jointly important.</p>
<p>Drawback number (5) is really a suggestion about the way to use stepwise regression. Bendel and Afifi (1977) suggested using a cut-off smaller than you ordinarily might. For example, in lieu of using <span class="math inline">\(t\)</span>-value = 2 corresponding approximately to a 5% significance level, consider using <span class="math inline">\(t\)</span>-value = 1.645 corresponding approximately to a 10% significance level. In this way, there is less chance of screening out variables that may be important. A lower bound, but still a good choice for exploratory work, is a cut-off as small as <span class="math inline">\(t\)</span>-value = 1. This choice is motivated by an algebraic result: when a variable enters a model, <span class="math inline">\(s\)</span> will decrease if the <span class="math inline">\(t\)</span>-ratio exceeds one in absolute value.</p>
<p>To address drawbacks number (3) and (4), we now introduce the <em>best regressions</em> routine. Best regressions is a useful algorithm that is now widely available in statistical software packages. The best regression algorithm searches over all possible combinations of explanatory variables, unlike stepwise regression, that adds and deletes one variable at a time. For example, suppose that there are four possible explanatory variables, <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math inline">\(x_3\)</span>, and <span class="math inline">\(x_4\)</span>, and the user would like to know what is the best two-variable model. The best regression algorithm searches over all six models of the form <span class="math inline">\(\mathrm{E}~y = \beta_0 + \beta_1 x_i + \beta_2 x_j\)</span>. Typically, a best regression routine recommends one or two models for each <span class="math inline">\(p\)</span> coefficient model, where <em>p</em> is a number that is user-specified. Because it has specified the number of coefficients to enter the model, it does not matter which of the criteria we use: <span class="math inline">\(R^2\)</span>, <span class="math inline">\(R_a^2\)</span>, or <span class="math inline">\(s\)</span>.</p>
<p>The best regression algorithm performs its search by a clever use of the algebraic fact that, when a variable is added to the model, the error sum of squares does not increase. Because of this fact, certain combinations of variables included in the model need not be computed. An important drawback of this algorithm is that it can take a considerable amount of time when the number of variables considered is large.</p>
<p>Users of regression do not always appreciate the depth of drawback number (1), <em>data-snooping</em>. Data-snooping occurs when the analyst fits a great number of models to a data set. We will address the problem of data-snooping in Section <a href="C5VarSelect.html#Sec562">5.6.2</a> on model validation. Here, we illustrate the effect of data-snooping in stepwise regression.</p>
<hr />
<p><strong>Example: Data-Snooping in Stepwise Regression.</strong> The idea of this illustration is due to Rencher and Pun (1980). Consider <span class="math inline">\(n = 100\)</span> observations of <span class="math inline">\(y\)</span> and fifty explanatory variables, <span class="math inline">\(x_1, x_2, \ldots, x_{50}\)</span>. The data we consider here were simulated using independent standard normal random variates. Because the variables were simulated independently, we are working under the null hypothesis of no relation between the response and the explanatory variables, that is, <span class="math inline">\(H_0: \beta_1 = \beta_2 = \ldots = \beta_{50} = 0\)</span>. Indeed, when the model with all fifty explanatory variables was fit, it turns out that <span class="math inline">\(s = 1.142\)</span>, <span class="math inline">\(R^2 = 46.2\%\)</span>, and <span class="math inline">\(F\)</span>-ratio = <span class="math inline">\(\frac{Regression~MS}{Error~MS} = 0.84\)</span>. Using an <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(df_1 = 50\)</span> and <span class="math inline">\(df_2 = 49\)</span>, the 95th percentile is 1.604. In fact, 0.84 is the 27th percentile of this distribution, indicating that the <span class="math inline">\(p\)</span>-value is 0.73. Thus, as expected, the data are in congruence with <span class="math inline">\(H_0\)</span>.</p>
<p>Next, a stepwise regression with <span class="math inline">\(t\)</span>-value = 2 was performed. Two variables were retained by this procedure, yielding a model with <span class="math inline">\(s = 1.05\)</span>, <span class="math inline">\(R^2 = 9.5\%\)</span>, and <span class="math inline">\(F\)</span>-ratio = 5.09. For an <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(df_1 = 2\)</span> and <span class="math inline">\(df_2 = 97\)</span>, the 95th percentile is <span class="math inline">\(F\)</span>-value = 3.09. This indicates that the two variables are statistically significant predictors of <span class="math inline">\(y\)</span>. At first glance, this result is surprising. The data were generated so that <span class="math inline">\(y\)</span> is unrelated to the explanatory variables. However, because <span class="math inline">\(F\)</span>-ratio <span class="math inline">\(&gt;\)</span> <span class="math inline">\(F\)</span>-value, the <span class="math inline">\(F\)</span>-test indicates that two explanatory variables are significantly related to <span class="math inline">\(y\)</span>. The reason is that stepwise regression has performed many hypothesis tests on the data. For example, in Step 1, fifty tests were performed to find significant variables. Recall that a 5% level means that we expect to make roughly one mistake in 20. Thus, with fifty tests, we expect to find <span class="math inline">\(50 \times 0.05 = 2.5\)</span> “significant” variables, even under the null hypothesis of no relationship between <span class="math inline">\(y\)</span> and the explanatory variables.</p>
<p>To continue, a stepwise regression with <span class="math inline">\(t\)</span>-value = 1.645 was performed. Six variables were retained by this procedure, yielding a model with <span class="math inline">\(s = 0.99\)</span>, <span class="math inline">\(R^2 = 22.9\%\)</span>, and <span class="math inline">\(F\)</span>-ratio = 4.61. As before, an <span class="math inline">\(F\)</span>-test indicates a significant relationship between the response and these six explanatory variables.</p>
<p>To summarize, using simulation we constructed a data set so that the explanatory variables have no relationship with the response. However, when using stepwise regression to examine the data, we “found” seemingly significant relationships between the response and certain subsets of the explanatory variables. This example illustrates a general caveat in model selection: when explanatory variables are selected using the data, <span class="math inline">\(t\)</span>-ratios and <span class="math inline">\(F\)</span>-ratios will be too large, thus overstating the importance of variables in the model.</p>
<hr />
<p>Stepwise regression and best regressions are examples of <em>automatic variable selection procedures</em>. In your modeling work, you will find these procedures to be useful because they can quickly search through several candidate models. However, these procedures do ignore nonlinear alternatives as well as the effect of outliers and high leverage points. The main point of the procedures is to mechanize certain routine tasks. This automatic selection approach can be extended and indeed, there are a number of so-called “expert systems” available in the market. For example, algorithms are available that “automatically” handle unusual points such as outliers and high leverage points. A model suggested by automatic variable selection procedures should be subject to the same careful diagnostic checking procedures as a model arrived at by any other means.</p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_kvaj4gct&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_a6b3752b%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="5.2 AutomaticSelectionProcedures">
</iframe>
</center>
</div>
<div id="Sec53" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Residual Analysis<a href="C5VarSelect.html#Sec53" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall the role of a residual in the linear regression model introduced in Section 2.6. A residual is a response minus the corresponding fitted value under the model. Because the model summarizes the linear effect of several explanatory variables, we may think of a residual as a response controlled for values of the explanatory variables. If the model is an adequate representation of the data, then residuals should closely approximate random errors. Random errors are used to represent the natural variation in the model; they represent the result of an unpredictable mechanism. Thus, to the extent that residuals resemble random errors, there should be no discernible patterns in the residuals. Patterns in the residuals indicate the presence of additional information that we hope to incorporate into the model. A lack of patterns in the residuals indicates that the model seems to account for the primary relationships in the data.</p>
<div id="Sec531" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Residuals<a href="C5VarSelect.html#Sec531" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are at least four types of patterns that can be uncovered through the residual analysis. In this section, we discuss the first two: residuals that are unusual and those that are related to other explanatory variables. We then introduce the third type, residuals that display a heteroscedastic pattern, in Section <a href="C5VarSelect.html#Sec57">5.7</a>. In our study of time series data that begins in Chapter 7, we will introduce the fourth type, residuals that display patterns through time.</p>
<p>When examining residuals, it is usually easier to work with a <em>standardized residual</em>, a residual that has been rescaled to be dimensionless. We generally work with standardized residuals because we achieve some carry-over of experience from one data set to another and may thus focus on relationships of interest. By using standardized residuals, we can train ourselves to look at a variety of residual plots and immediately recognize an unusual point when working in standard units.</p>
<p>There are a number of ways of defining a standardized residual. Using <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span> as the <span class="math inline">\(i\)</span>th residual, here are three commonly used definitions:</p>
<p><span class="math display" id="eq:eq51">\[\begin{equation}
\text{(a) }\frac{e_i}{s}, \quad \text{(b) }\frac{e_i}{s\sqrt{1 - h_{ii}}}, \quad \text{(c) }\frac{e_i}{s_{(i)}\sqrt{1 - h_{ii}}}.
\tag{5.1}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(h_{ii}\)</span> is the <span class="math inline">\(i\)</span>th leverage. It is calculated based on values of the explanatory variables and will be defined in Section <a href="C5VarSelect.html#Sec541">5.4.1</a>. Recall that <span class="math inline">\(s\)</span> is the residual standard deviation (defined in equation 3.8). Similarly, define <span class="math inline">\(s_{(i)}\)</span> to be the residual standard deviation when running a regression after having deleted the <span class="math inline">\(i\)</span>th observation.</p>
<p>Now, the first definition in (a) is simple and easy to explain. An easy calculation shows that the sample standard deviation of the residuals is approximately <span class="math inline">\(s\)</span> (one reason that <span class="math inline">\(s\)</span> is often referred to as the residual standard deviation). Thus, it seems reasonable to standardize residuals by dividing by <span class="math inline">\(s\)</span>.</p>
<p>The second choice presented in (b), although more complex, is more precise. The variance of the <span class="math inline">\(i\)</span>th residual is</p>
<p><span class="math display">\[
\text{Var}(e_i) = \sigma^2(1 - h_{ii}).
\]</span></p>
<p>This result will be established in equation <a href="C5VarSelect.html#eq:eq515">(5.15)</a> of Section <a href="C5VarSelect.html#Sec510">5.10</a>. Note that this variance is smaller than the variance of the error term, Var<span class="math inline">\((\varepsilon_i) = \sigma^2\)</span>. Now, we can replace <span class="math inline">\(\sigma\)</span> by its estimate, <span class="math inline">\(s\)</span>. Then, this result leads to using the quantity <span class="math inline">\(s\sqrt{1 - h_{ii}}\)</span> as an estimated standard deviation, or standard error, for <span class="math inline">\(e_i\)</span>. Thus, we define the standard error of <span class="math inline">\(e_i\)</span> to be</p>
<p><span class="math display">\[
\text{se}(e_i) = s \sqrt{1 - h_{ii}}.
\]</span></p>
<p>Following the conventions introduced in Section 2.6, in this text we use <span class="math inline">\(e_i / \text{se}(e_i)\)</span> to be our <em>standardized residual</em>.</p>
<p>The third choice presented in (c) is a modification of (b) and is known as a <em>studentized residual</em>. As emphasized in Section <a href="C5VarSelect.html#Sec532">5.3.2</a>, one important use of residuals is to identify unusually large responses. Now, suppose that the <span class="math inline">\(i\)</span>th response is unusually large and that this is measured through its residual. This unusually large residual will also cause the value of <span class="math inline">\(s\)</span> to be large. Because the large effect appears in both the numerator and denominator, the standardized residual may not detect this unusual response. However, this large response will not inflate <span class="math inline">\(s_{(i)}\)</span> because it is constructed after having deleted the <span class="math inline">\(i\)</span>th observation. Thus, when using studentized residuals, we get a better measure of observations that have unusually large residuals. By omitting this observation from the estimate of <span class="math inline">\(\sigma\)</span>, the size of the observation affects only the numerator <span class="math inline">\(e_i\)</span> and not the denominator <span class="math inline">\(s_{(i)}\)</span>.</p>
<p>As another advantage, studentized residuals follow a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n - (k + 1)\)</span> degrees of freedom, assuming the errors are normally distributed (assumption E5). This knowledge of the precise distribution helps us assess the degree of model fit and is particularly useful in small samples. It is this relationship with the “Student’s” <span class="math inline">\(t\)</span>-distribution that suggests the name “studentized” residuals.</p>
</div>
<div id="Sec532" class="section level3 hasAnchor" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Using Residuals to Identify Outliers<a href="C5VarSelect.html#Sec532" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One important role of residual analysis is to identify outliers. An outlier is an observation that is not well fit by the model; these are observations where the residual is unusually large. A rule of thumb that is used by many statistical packages is that an observation is marked as an outlier if the standardized residual exceeds two in absolute value. To the extent that the distribution of standardized residuals mimics the standard normal curve, we expect about only one in 20 observations, or 95%, to exceed two in absolute value and very few observations to exceed three.</p>
<p>Outliers provide a signal that an observation should be investigated to understand special causes associated with this point. An outlier is an observation that seems unusual with respect to the rest of the data set. It is often the case that the reason for this atypical behavior may be uncovered after additional investigation. Indeed, this may be the primary purpose of the regression analysis of a data set.</p>
<p>Consider a simple example of so-called <em>performance analysis</em>. Suppose we have available a sample of <span class="math inline">\(n\)</span> salespeople and are trying to understand each person’s second-year sales based on their first-year sales. To a certain extent, we expect that higher first-year sales are associated with higher second-year sales. High sales may be due to a salesperson’s natural ability, ambition, good territory, and so on. First-year sales may be thought of as a proxy variable that summarizes these factors. We expect variation in sales performance both cross-sectionally and across years. It is interesting when one salesperson performs unusually well (or poorly) in the second year compared to their first-year performance. Residuals provide a formal mechanism for evaluating second-year sales after controlling for the effects of first-year sales.</p>
<p>There are a number of options available for handling outliers.</p>
<div class="blackbox">
<p><em>Options for Handling Outliers</em></p>
<ul>
<li>Include the observation in the usual summary statistics but comment on its effects. An outlier may be large but not so large as to skew the results of the entire analysis. If no special causes for this unusual observation can be determined, then this observation may simply reflect the variability in the data.</li>
<li>Delete the observation from the data set. The observation may be determined to be unrepresentative of the population from which the sample is drawn. If this is the case, then there may be little information contained in the observation that can be used to make general statements about the population. This option means that we would omit the observation from the regression summary statistics and discuss it in our report as a separate case.</li>
<li>Create a binary variable to indicate the presence of an outlier. If one or several special causes have been identified to explain an outlier, then these causes could be introduced into the modeling procedure formally by introducing a variable to indicate the presence (or absence) of these causes. This approach is similar to point deletion but allows the outlier to be formally included in the model formulation so that, if additional observations arise that are affected by the same causes, then they can be handled on an automatic basis.</li>
</ul>
</div>
</div>
<div id="Sec533" class="section level3 hasAnchor" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Using Residuals to Select Explanatory Variables<a href="C5VarSelect.html#Sec533" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another important role of residual analysis is to help identify additional explanatory variables that may be used to improve the formulation of the model. If we have specified the model correctly, then residuals should resemble random errors and contain no discernible patterns. Thus, when comparing residuals to explanatory variables, we do not expect any relationships. If we do detect a relationship, then this suggests the need to control for this additional variable. This can be accomplished by introducing the additional variable into the regression model.</p>
<p>Relationships between residuals and explanatory variables can be quickly established using correlation statistics. However, if an explanatory variable is already included in the regression model, then the correlation between the residuals and an explanatory variable will be zero (see Section <a href="C5VarSelect.html#Sec5101">5.10.1</a> for the algebraic demonstration). It is a good idea to reinforce this correlation with a scatter plot. Not only will a plot of residuals versus explanatory variables reinforce graphically the correlation statistic, it will also serve to detect potential nonlinear relationships. For example, a quadratic relationship can be detected using a scatter plot, not a correlation statistic.</p>
<p>If you detect a relationship between the residuals from a preliminary model fit and an additional explanatory variable, then introducing this additional variable will not always improve your model specification. The reason is that the additional variable may be linearly related to the variables that are already in the model. If you would like a guarantee that adding an additional variable will improve your model, then construct an added variable plot (see Section 3.4.3).</p>
<p>To summarize, after a preliminary model fit, you should:</p>
<ul>
<li>Calculate summary statistics and display the distribution of (standardized) residuals to identify outliers.</li>
<li>Calculate the correlation between the (standardized) residuals and additional explanatory variables to search for linear relationships.</li>
<li>Create scatter plots between the (standardized) residuals and additional explanatory variables to search for nonlinear relationships.</li>
</ul>
<hr />
<p><strong>Example: Stock Market Liquidity.</strong> An investor’s decision to purchase a stock is generally made with a number of criteria in mind. First, investors usually look for a high expected return. A second criterion is the riskiness of a stock, which can be measured through the variability of the returns. Third, many investors are concerned with the length of time that they are committing their capital with the purchase of a security. Many income stocks, such as utilities, regularly return portions of capital investments in the form of dividends. Other stocks, particularly growth stocks, return nothing until the sale of the security. Thus, the average length of investment in a security is another criterion. Fourth, investors are concerned with the ability to sell the stock at any time convenient to the investor. We refer to this fourth criterion as the <em>liquidity</em> of the stock. The more liquid the stock, the easier it is to sell. To measure liquidity, in this study we use the number of shares traded on an exchange over a specified period of time (called the VOLUME). We are interested in studying the relationship between the volume and other financial characteristics of a stock.</p>
<p>We begin this study with 126 companies whose options were traded on December 3, 1984. The stock data were obtained from Francis Emory Fitch, Inc. for the period from December 3, 1984 to February 28, 1985. For the trading activity variables, we examine:</p>
<ul>
<li>The three months total trading volume (VOLUME, in millions of shares),</li>
<li>The three months total number of transactions (NTRAN), and</li>
<li>The average time between transactions (AVGT, measured in minutes).</li>
</ul>
<p>For the firm size variables, we use:</p>
<ul>
<li>Opening stock price on January 2, 1985 (PRICE),</li>
<li>The number of outstanding shares on December 31, 1984 (SHARE, in millions of shares), and</li>
<li>The market equity value (VALUE, in billions of dollars) obtained by taking the product of PRICE and SHARE.</li>
</ul>
<p>Finally, for the financial leverage, we examine the debt-to-equity ratio (DEB_EQ) obtained from the Compustat Industrial Tape and the Moody’s manual. The data in SHARE are obtained from the Center for Research in Security Prices (CRSP) monthly tape.</p>
<p>After examining some preliminary summary statistics of the data, three companies were deleted because they either had an unusually large volume or high price. They are Teledyne and Capital Cities Communication, whose prices were more than four times the average price of the remaining companies, and American Telephone and Telegraph, whose total volume was more than seven times the average total volume of the remaining companies. Based on additional investigation, the details of which are not presented here, these companies were deleted because they seemed to represent special circumstances that we would not wish to model.</p>
<p>Table <a href="C5VarSelect.html#tab:Tab52">5.2</a> summarizes the descriptive statistics based on the remaining <span class="math inline">\(n = 123\)</span> companies. For example, from Table <a href="C5VarSelect.html#tab:Tab52">5.2</a>, we see that the average time between transactions is about five minutes and this time ranges from a minimum of less than 1 minute to a maximum of about 20 minutes.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab52">Table 5.2: </span><strong>Summary Statistics of the Stock Liquidity Variables</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Median
</th>
<th style="text-align:right;">
Standard Deviation
</th>
<th style="text-align:right;">
Minimum
</th>
<th style="text-align:right;">
Maximum
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
VOLUME
</td>
<td style="text-align:right;width: 1.6cm; ">
13.423
</td>
<td style="text-align:right;width: 1.6cm; ">
11.556
</td>
<td style="text-align:right;width: 1.6cm; ">
10.632
</td>
<td style="text-align:right;width: 1.6cm; ">
0.658
</td>
<td style="text-align:right;width: 1.6cm; ">
64.572
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
AVGT
</td>
<td style="text-align:right;width: 1.6cm; ">
5.441
</td>
<td style="text-align:right;width: 1.6cm; ">
4.284
</td>
<td style="text-align:right;width: 1.6cm; ">
3.853
</td>
<td style="text-align:right;width: 1.6cm; ">
0.590
</td>
<td style="text-align:right;width: 1.6cm; ">
20.772
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
NTRAN
</td>
<td style="text-align:right;width: 1.6cm; ">
6436.000
</td>
<td style="text-align:right;width: 1.6cm; ">
5071.000
</td>
<td style="text-align:right;width: 1.6cm; ">
5310.000
</td>
<td style="text-align:right;width: 1.6cm; ">
999.000
</td>
<td style="text-align:right;width: 1.6cm; ">
36420.000
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
PRICE
</td>
<td style="text-align:right;width: 1.6cm; ">
38.800
</td>
<td style="text-align:right;width: 1.6cm; ">
34.380
</td>
<td style="text-align:right;width: 1.6cm; ">
21.370
</td>
<td style="text-align:right;width: 1.6cm; ">
9.120
</td>
<td style="text-align:right;width: 1.6cm; ">
122.380
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SHARE
</td>
<td style="text-align:right;width: 1.6cm; ">
94.730
</td>
<td style="text-align:right;width: 1.6cm; ">
53.830
</td>
<td style="text-align:right;width: 1.6cm; ">
115.100
</td>
<td style="text-align:right;width: 1.6cm; ">
6.740
</td>
<td style="text-align:right;width: 1.6cm; ">
783.050
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
VALUE
</td>
<td style="text-align:right;width: 1.6cm; ">
4.116
</td>
<td style="text-align:right;width: 1.6cm; ">
2.065
</td>
<td style="text-align:right;width: 1.6cm; ">
8.157
</td>
<td style="text-align:right;width: 1.6cm; ">
0.115
</td>
<td style="text-align:right;width: 1.6cm; ">
75.437
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
DEBEQ
</td>
<td style="text-align:right;width: 1.6cm; ">
2.697
</td>
<td style="text-align:right;width: 1.6cm; ">
1.105
</td>
<td style="text-align:right;width: 1.6cm; ">
6.509
</td>
<td style="text-align:right;width: 1.6cm; ">
0.185
</td>
<td style="text-align:right;width: 1.6cm; ">
53.628
</td>
</tr>
</tbody>
</table>
<p><strong>Source:</strong> Francis Emory Fitch, Inc., Standard &amp; Poor’s Compustat, and University of Chicago’s Center for Research on Security Prices.</p>
<p>Table <a href="C5VarSelect.html#tab:Tab53">5.3</a> reports the correlation coefficients and Figure <a href="C5VarSelect.html#fig:Fig52">5.2</a> provides the corresponding scatterplot matrix. If you have a background in finance, you will find it interesting to note that the financial leverage, measured by DEB_EQ, does not seem to be related to the other variables. From the scatterplot and correlation matrix, we see a strong relationship between VOLUME and the size of the firm as measured by SHARE and VALUE. Further, the three trading activity variables, VOLUME, AVGT, and NTRAN, are all highly related to one another.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab53">Table 5.3: </span><strong>Correlation Matrix of the Stock Liquidity</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
AVGT
</th>
<th style="text-align:right;">
NTRAN
</th>
<th style="text-align:right;">
PRICE
</th>
<th style="text-align:right;">
SHARE
</th>
<th style="text-align:right;">
VALUE
</th>
<th style="text-align:right;">
DEB_EQ
</th>
<th style="text-align:right;">
VOLUME
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
AVGT
</td>
<td style="text-align:right;width: 1.4cm; ">
1.000
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.668
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.128
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.429
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.318
</td>
<td style="text-align:right;width: 1.4cm; ">
0.094
</td>
<td style="text-align:right;">
-0.674
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
NTRAN
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.668
</td>
<td style="text-align:right;width: 1.4cm; ">
1.000
</td>
<td style="text-align:right;width: 1.4cm; ">
0.190
</td>
<td style="text-align:right;width: 1.4cm; ">
0.817
</td>
<td style="text-align:right;width: 1.4cm; ">
0.760
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.092
</td>
<td style="text-align:right;">
0.913
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
PRICE
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.128
</td>
<td style="text-align:right;width: 1.4cm; ">
0.190
</td>
<td style="text-align:right;width: 1.4cm; ">
1.000
</td>
<td style="text-align:right;width: 1.4cm; ">
0.177
</td>
<td style="text-align:right;width: 1.4cm; ">
0.457
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.038
</td>
<td style="text-align:right;">
0.168
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SHARE
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.429
</td>
<td style="text-align:right;width: 1.4cm; ">
0.817
</td>
<td style="text-align:right;width: 1.4cm; ">
0.177
</td>
<td style="text-align:right;width: 1.4cm; ">
1.000
</td>
<td style="text-align:right;width: 1.4cm; ">
0.829
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.077
</td>
<td style="text-align:right;">
0.773
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
VALUE
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.318
</td>
<td style="text-align:right;width: 1.4cm; ">
0.760
</td>
<td style="text-align:right;width: 1.4cm; ">
0.457
</td>
<td style="text-align:right;width: 1.4cm; ">
0.829
</td>
<td style="text-align:right;width: 1.4cm; ">
1.000
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.077
</td>
<td style="text-align:right;">
0.702
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
DEB_EQ
</td>
<td style="text-align:right;width: 1.4cm; ">
0.094
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.092
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.038
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.077
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.077
</td>
<td style="text-align:right;width: 1.4cm; ">
1.000
</td>
<td style="text-align:right;">
-0.052
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
VOLUME
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.674
</td>
<td style="text-align:right;width: 1.4cm; ">
0.913
</td>
<td style="text-align:right;width: 1.4cm; ">
0.168
</td>
<td style="text-align:right;width: 1.4cm; ">
0.773
</td>
<td style="text-align:right;width: 1.4cm; ">
0.702
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.052
</td>
<td style="text-align:right;">
1.000
</td>
</tr>
</tbody>
</table>
<h5 style="text-align: center;">
<a id="displayCode.Tab52.Hide" href="javascript:togglecode('toggleCode.Tab52.Hide','displayCode.Tab52.Hide');"><i><strong>R Code to Produce Tables 5.2 and 5.3</strong></i></a>
</h5>
<div id="toggleCode.Tab52.Hide" style="display: none">
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="C5VarSelect.html#cb48-1" tabindex="-1"></a>liquidity <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;CSVData/Liquidity.csv&quot;</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb48-2"><a href="C5VarSelect.html#cb48-2" tabindex="-1"></a>varLiquid <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;AVGT&quot;</span>, <span class="st">&quot;NTRAN&quot;</span>, <span class="st">&quot;PRICE&quot;</span>, <span class="st">&quot;SHARE&quot;</span>, <span class="st">&quot;VALUE&quot;</span>, <span class="st">&quot;DEBEQ&quot;</span>, <span class="st">&quot;VOLUME&quot;</span>)</span>
<span id="cb48-3"><a href="C5VarSelect.html#cb48-3" tabindex="-1"></a>liquidMat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(liquidity[varLiquid])</span>
<span id="cb48-4"><a href="C5VarSelect.html#cb48-4" tabindex="-1"></a><span class="fu">names</span>(liquidMat)[<span class="fu">names</span>(liquidMat) <span class="sc">==</span> <span class="st">&quot;DEBEQ&quot;</span>] <span class="ot">&lt;-</span> <span class="st">&quot;DEB_EQ&quot;</span></span>
<span id="cb48-5"><a href="C5VarSelect.html#cb48-5" tabindex="-1"></a></span>
<span id="cb48-6"><a href="C5VarSelect.html#cb48-6" tabindex="-1"></a><span class="co">#  TABLE 5.2 SUMMARY STATISTICS</span></span>
<span id="cb48-7"><a href="C5VarSelect.html#cb48-7" tabindex="-1"></a>BookSummStats <span class="ot">&lt;-</span> <span class="cf">function</span>(Xymat){</span>
<span id="cb48-8"><a href="C5VarSelect.html#cb48-8" tabindex="-1"></a>meanSummary <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, mean,  <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb48-9"><a href="C5VarSelect.html#cb48-9" tabindex="-1"></a>sdSummary   <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, sd,    <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb48-10"><a href="C5VarSelect.html#cb48-10" tabindex="-1"></a>minSummary  <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, min,   <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb48-11"><a href="C5VarSelect.html#cb48-11" tabindex="-1"></a>maxSummary  <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, max,   <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb48-12"><a href="C5VarSelect.html#cb48-12" tabindex="-1"></a>medSummary  <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, median,<span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb48-13"><a href="C5VarSelect.html#cb48-13" tabindex="-1"></a>tableMat  <span class="ot">&lt;-</span> <span class="fu">cbind</span>(meanSummary, medSummary, sdSummary, minSummary, maxSummary)</span>
<span id="cb48-14"><a href="C5VarSelect.html#cb48-14" tabindex="-1"></a><span class="fu">return</span>(tableMat)</span>
<span id="cb48-15"><a href="C5VarSelect.html#cb48-15" tabindex="-1"></a>}</span>
<span id="cb48-16"><a href="C5VarSelect.html#cb48-16" tabindex="-1"></a></span>
<span id="cb48-17"><a href="C5VarSelect.html#cb48-17" tabindex="-1"></a>liquidMat1 <span class="ot">&lt;-</span> liquidMat[,<span class="fu">c</span>(<span class="dv">7</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)]</span>
<span id="cb48-18"><a href="C5VarSelect.html#cb48-18" tabindex="-1"></a>tableMat  <span class="ot">&lt;-</span> <span class="fu">BookSummStats</span>(liquidMat1)</span>
<span id="cb48-19"><a href="C5VarSelect.html#cb48-19" tabindex="-1"></a><span class="fu">colnames</span>(tableMat)  <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Mean&quot;</span> , <span class="st">&quot;Median&quot;</span> , <span class="st">&quot;Standard Deviation&quot;</span> , </span>
<span id="cb48-20"><a href="C5VarSelect.html#cb48-20" tabindex="-1"></a>                         <span class="st">&quot;Minimum&quot;</span> , <span class="st">&quot;Maximum&quot;</span>)</span>
<span id="cb48-21"><a href="C5VarSelect.html#cb48-21" tabindex="-1"></a><span class="fu">rownames</span>(tableMat)  <span class="ot">&lt;-</span> varLiquid[<span class="fu">c</span>(<span class="dv">7</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)]</span>
<span id="cb48-22"><a href="C5VarSelect.html#cb48-22" tabindex="-1"></a>tableMat       <span class="ot">&lt;-</span> <span class="fu">round</span>(tableMat, <span class="at">digits =</span> <span class="dv">3</span>)</span>
<span id="cb48-23"><a href="C5VarSelect.html#cb48-23" tabindex="-1"></a>tableMat[<span class="dv">3</span>,]   <span class="ot">&lt;-</span> <span class="fu">round</span>(tableMat[<span class="dv">3</span>,], <span class="at">digits =</span> <span class="dv">0</span>) </span>
<span id="cb48-24"><a href="C5VarSelect.html#cb48-24" tabindex="-1"></a>tableMat[<span class="dv">4</span><span class="sc">:</span><span class="dv">5</span>,] <span class="ot">&lt;-</span> <span class="fu">round</span>(tableMat[<span class="dv">4</span><span class="sc">:</span><span class="dv">5</span>,], <span class="at">digits =</span> <span class="dv">2</span>)</span>
<span id="cb48-25"><a href="C5VarSelect.html#cb48-25" tabindex="-1"></a></span>
<span id="cb48-26"><a href="C5VarSelect.html#cb48-26" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>tableMat, </span>
<span id="cb48-27"><a href="C5VarSelect.html#cb48-27" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Summary Statistics of the Stock Liquidity Variables&#39;</span>, </span>
<span id="cb48-28"><a href="C5VarSelect.html#cb48-28" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>, <span class="at">Digits=</span><span class="dv">3</span>, <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,</span>
<span id="cb48-29"><a href="C5VarSelect.html#cb48-29" tabindex="-1"></a>         <span class="at">ColWidth =</span> ColWidth5) </span></code></pre></div>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="C5VarSelect.html#cb49-1" tabindex="-1"></a>cor_matrix <span class="ot">&lt;-</span> <span class="fu">cor</span>(liquidMat)</span>
<span id="cb49-2"><a href="C5VarSelect.html#cb49-2" tabindex="-1"></a><span class="fu">rownames</span>(cor_matrix) <span class="ot">&lt;-</span> <span class="fu">colnames</span>(cor_matrix) <span class="ot">&lt;-</span> </span>
<span id="cb49-3"><a href="C5VarSelect.html#cb49-3" tabindex="-1"></a>        <span class="fu">c</span>(<span class="st">&quot;AVGT&quot;</span>, <span class="st">&quot;NTRAN&quot;</span>, <span class="st">&quot;PRICE&quot;</span>, <span class="st">&quot;SHARE&quot;</span>, <span class="st">&quot;VALUE&quot;</span>, <span class="st">&quot;DEB_EQ&quot;</span>, <span class="st">&quot;VOLUME&quot;</span>)</span>
<span id="cb49-4"><a href="C5VarSelect.html#cb49-4" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>cor_matrix, </span>
<span id="cb49-5"><a href="C5VarSelect.html#cb49-5" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Correlation Matrix of the Stock Liquidity&#39;</span>, </span>
<span id="cb49-6"><a href="C5VarSelect.html#cb49-6" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>, <span class="at">Digits=</span><span class="dv">3</span>, <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,</span>
<span id="cb49-7"><a href="C5VarSelect.html#cb49-7" tabindex="-1"></a>         <span class="at">ColWidth =</span> ColWidth6) </span></code></pre></div>
</div>
<p>Figure <a href="C5VarSelect.html#fig:Fig52">5.2</a> shows that the variable AVGT is inversely related to VOLUME and NTRAN is inversely related to AVGT. In fact, it turned out the correlation between the average time between transactions and the reciprocal of the number of transactions was <span class="math inline">\(99.98\%!\)</span> This is not so surprising when one thinks about how AVGT might be calculated. For example, on the New York Stock Exchange, the market is open from 10:00 A.M. to 4:00 P.M. For each stock on a particular day, the average time between transactions times the number of transactions is nearly equal to 360 minutes (= 6 hours). Thus, except for rounding errors because transactions are only recorded to the nearest minute, there is a perfect linear relationship between AVGT and the reciprocal of NTRAN.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig52"></span>
<img src="RegressionMarkdown_files/figure-html/Fig52-1.png" alt="Scatterplot matrix for stock liquidity variables. The number of transactions variable (NTRAN) appears to be strongly related to the VOLUME of shares traded, and inversely related to AVGT." width="100%" />
<p class="caption">
Figure 5.2: <strong>Scatterplot matrix for stock liquidity variables.</strong> The number of transactions variable (NTRAN) appears to be strongly related to the VOLUME of shares traded, and inversely related to AVGT.
</p>
</div>
<p>To begin to understand the liquidity measure VOLUME, we first fit a regression model using NTRAN as an explanatory variable. The fitted regression model is:</p>
<p><span class="math display">\[
\small{
\begin{array}{lcc}
\text{VOLUME} &amp;= 1.65 &amp;+ 0.00183 \text{ NTRAN} \\
\text{standard errors} &amp; (0.6173) &amp; (0.000074)
\end{array}
}
\]</span></p>
<p>with <span class="math inline">\(R^2 = 83.4\%\)</span> and <span class="math inline">\(s = 4.35\)</span>. Note that the <span class="math inline">\(t\)</span>-ratio for the slope associated with NTRAN is</p>
<p><span class="math display">\[
t(b_1) = \frac{b_1}{se(b_1)} = \frac{0.00183}{0.000074} = 24.7
\]</span></p>
<p>indicating strong statistical significance. Residuals were computed using this estimated model. To see if the residuals are related to the other explanatory variables, Table <a href="C5VarSelect.html#tab:Tab54">5.4</a> shows correlations.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab54">Table 5.4: </span><strong>First Table of Correlations</strong>
</caption>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Variable
</td>
<td style="text-align:center;width: 3cm; ">
AVGT
</td>
<td style="text-align:center;width: 3cm; ">
PRICE
</td>
<td style="text-align:center;width: 3cm; ">
SHARE
</td>
<td style="text-align:center;width: 3cm; ">
VALUE
</td>
<td style="text-align:center;width: 3cm; ">
DEB_EQ
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
RESID
</td>
<td style="text-align:center;width: 3cm; ">
-0.159
</td>
<td style="text-align:center;width: 3cm; ">
-0.014
</td>
<td style="text-align:center;width: 3cm; ">
0.064
</td>
<td style="text-align:center;width: 3cm; ">
0.018
</td>
<td style="text-align:center;width: 3cm; ">
0.078
</td>
</tr>
</tbody>
</table>
<p><em>Note</em>: The residuals were created from a regression of VOLUME on NTRAN.</p>
<p>The correlation between the residual and AVGT and the scatter plot (not shown here) indicates that there may be some information in the variable AVGT in the residual. Thus, it seems sensible to use AVGT directly in the regression model. Remember that we are interpreting the residual as the value of VOLUME having controlled for the effect of NTRAN.</p>
<p>We next fit a regression model using NTRAN and AVGT as explanatory variables. The fitted regression model is:</p>
<p><span class="math display">\[
\small{
\begin{array}{lccc}
\text{VOLUME} &amp;= 4.41 &amp;- 0.322 \text{ AVGT} &amp;+ 0.00167 \text{ NTRAN} \\
\text{standard errors} &amp; (1.30)&amp; (0.135)&amp; (0.000098)
\end{array}
}
\]</span></p>
<p>with <span class="math inline">\(R^2 = 84.2\%\)</span> and <span class="math inline">\(s = 4.26\)</span>. Based on the <span class="math inline">\(t\)</span>-ratio for AVGT, <span class="math inline">\(t(b_{AVGT}) = \frac{-0.322}{0.135} = -2.39\)</span>, it seems as if AVGT is a useful explanatory variable in the model. Note also that <span class="math inline">\(s\)</span> has decreased, indicating that <span class="math inline">\(R_a^2\)</span> has increased.</p>
<p>Table <a href="C5VarSelect.html#tab:Tab55">5.5</a> provides correlations between the model residuals and other potential explanatory variables and indicates that there does not seem to be much additional information in the explanatory variables. This is reaffirmed by the corresponding table of scatter plots in Figure <a href="C5VarSelect.html#fig:Fig53">5.3</a>. The histograms in Figure <a href="C5VarSelect.html#fig:Fig53">5.3</a> suggest that although the distribution of the residuals is fairly symmetric, the distribution of each explanatory variable is skewed. Because of this, transformations of the explanatory variables were explored. This line of thought provided no real improvements and thus the details are not provided here.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig53"></span>
<img src="RegressionMarkdown_files/figure-html/Fig53-1.png" alt="Scatterplot matrix of the residuals from the regression of VOLUME on NTRAN and AVGT on the vertical axis and the remaining predictor variables on the horizontal axes." width="100%" />
<p class="caption">
Figure 5.3: <strong>Scatterplot matrix of the residuals from the regression of VOLUME on NTRAN and AVGT on the vertical axis and the remaining predictor variables on the horizontal axes.</strong>
</p>
</div>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab55">Table 5.5: </span><strong>Second Table of Correlations</strong>
</caption>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Variable
</td>
<td style="text-align:center;width: 3cm; ">
PRICE
</td>
<td style="text-align:center;width: 3cm; ">
SHARE
</td>
<td style="text-align:center;width: 3cm; ">
VALUE
</td>
<td style="text-align:center;width: 3cm; ">
DEB_EQ
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
RESID
</td>
<td style="text-align:center;width: 3cm; ">
-0.015
</td>
<td style="text-align:center;width: 3cm; ">
0.100
</td>
<td style="text-align:center;width: 3cm; ">
0.074
</td>
<td style="text-align:center;width: 3cm; ">
0.089
</td>
</tr>
</tbody>
</table>
<p><em>Note</em>: The residuals were created from a regression of VOLUME on NTRAN and AVGT.</p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_57lino6f&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_fmjwjz4d%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="5.3 ResidualAnalysis">
</iframe>
</center>
</div>
</div>
<div id="Sec54" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Influential Points<a href="C5VarSelect.html#Sec54" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Not all points are created equal – in this section we will see that specific observations can potentially have a disproportionate effect on the overall regression fit. We will call such points “influential.” This is not too surprising; we have already seen that regression coefficient estimates are <em>weighted</em> sums of responses (see Section 3.2.4). Some observations have heavier weights than others and thus have a greater influence on the regression coefficient estimates. Of course, simply because an observation is influential does not mean that it is incorrect or that its impact on the model is misleading. As analysts, we would simply like to know whether our fitted model is sensitive to mild changes such as the removal of a single point so that we feel comfortable generalizing our results from the sample to a larger population.</p>
<p>To assess influence, we think of observations as being unusual responses, given a set of explanatory variables, or having an unusual set of explanatory variables values. We have already seen in Section <a href="C5VarSelect.html#Sec53">5.3</a> how to assess unusual responses using residuals. This section focuses on unusual sets of explanatory variables values.</p>
<div id="Sec541" class="section level3 hasAnchor" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Leverage<a href="C5VarSelect.html#Sec541" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We introduced this topic in Section 2.6 where we called an observation having an unusual explanatory variable value a “high leverage point.” With more than one explanatory variable, determining whether an observation is a high leverage point is not as straightforward. For example, it is possible for an observation to be “not unusual” for any single variable and yet still be unusual in the space of explanatory variables. Consider the fictitious data set represented in Figure <a href="C5VarSelect.html#fig:Fig54">5.4</a>. Visually, it seems clear that the point marked in the upper right-hand corner is unusual. However, it is not unusual when examining the histogram of either <span class="math inline">\(x_1\)</span> or <span class="math inline">\(x_2\)</span>. It is only unusual when the explanatory variables are considered jointly.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig54"></span>
<img src="RegressionMarkdown_files/figure-html/Fig54-1.png" alt="The ellipsoid represents most of the data. The arrow marks an unusual point." width="60%" />
<p class="caption">
Figure 5.4: <strong>The ellipsoid represents most of the data.</strong> The arrow marks an unusual point.
</p>
</div>
<p>For two explanatory variables, this is apparent when examining the data graphically. Because it is difficult to examine graphically data having more than two explanatory variables, we need a numerical procedure for assessing leverage.</p>
<p>To define the concept of leverage in multiple linear regression, we use some concepts from matrix algebra. Specifically, in Section 3.1, we showed that the vector of least squares regression coefficients could be calculated using
<span class="math inline">\(\mathbf{b} = (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{y}\)</span>.
Thus, we can express the vector of fitted values <span class="math inline">\(\hat{\mathbf{y}} = (\hat{y}_1, \ldots, \hat{y}_n)^{\prime}\)</span> as</p>
<p><span class="math display" id="eq:eq52">\[\begin{equation}
\mathbf{\hat{y}} = \mathbf{Xb} .
\tag{5.2}
\end{equation}\]</span></p>
<p>Similarly, the vector of residuals is the vector of response minus the vector of fitted values, that is, <span class="math inline">\(\mathbf{e} = \mathbf{y - \hat{y}}\)</span>.</p>
<p>From the expression for the regression coefficients <span class="math inline">\(\mathbf{b}\)</span> in equation (3.4), we have</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{X} (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{y}
\]</span></p>
<p>This equation suggests defining</p>
<p><span class="math display">\[
\mathbf{H} = \mathbf{X} (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime}
\]</span></p>
<p>so that</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{Hy}
\]</span></p>
<p>From this, the matrix <span class="math inline">\(\mathbf{H}\)</span> is said to <em>project</em> the vector of responses <span class="math inline">\(\mathbf{y}\)</span> onto the vector of fitted values <span class="math inline">\(\mathbf{\hat{y}}\)</span>. Alternatively, you may think of <span class="math inline">\(\mathbf{H}\)</span> as the matrix that puts the “hat,” or carat, on <span class="math inline">\(\mathbf{y}\)</span>. From the <span class="math inline">\(i\)</span>th row of the vector equation <span class="math inline">\(\mathbf{\hat{y}} = \mathbf{Hy}\)</span>, we have</p>
<p><span class="math display">\[
\hat{y}_i = h_{i1} y_1 + h_{i2} y_2 + \cdots + h_{ii} y_i + \cdots + h_{in} y_n
\]</span></p>
<p>Here, <span class="math inline">\(h_{ij}\)</span> is the number in the <span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column of <span class="math inline">\(\mathbf{H}\)</span>. From this expression, we see that the larger <span class="math inline">\(h_{ii}\)</span> is, the larger the effect that the <span class="math inline">\(i\)</span>th response <span class="math inline">\((y_i)\)</span> has on the corresponding fitted value <span class="math inline">\((\hat{y}_i)\)</span>. Thus, we call <span class="math inline">\(h_{ii}\)</span> the <em>leverage</em> for the <span class="math inline">\(i\)</span>th observation. Because <span class="math inline">\(h_{ii}\)</span> is the <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\(\mathbf{H}\)</span>, a direct expression for <span class="math inline">\(h_{ii}\)</span> is</p>
<p><span class="math display" id="eq:eq53">\[\begin{equation}
h_{ii} = \mathbf{x}_i^{\prime} (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{x}_i
\tag{5.3}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}_i = (x_{i0}, x_{i1}, \ldots, x_{ik})^{\prime}\)</span>. Because the values <span class="math inline">\(h_{ii}\)</span> are calculated based on the explanatory variables, the values of the response variable do not affect the calculation of leverages.</p>
<p>Large leverage values indicate that an observation may exhibit a disproportionate effect on the fit, essentially because it is distant from the other observations (when looking at the space of explanatory variables). How large is large? Some guidelines are available from matrix algebra, where we have that</p>
<p><span class="math display">\[
\frac{1}{n} \le h_{ii} \le 1
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\bar{h} = \frac{1}{n} \sum_{i=1}^{n} h_{ii} = \frac{k+1}{n}.
\]</span></p>
<p>Thus, each leverage is bounded by <span class="math inline">\(n^{-1}\)</span> and <span class="math inline">\(1\)</span>, and the average leverage equals the number of regression coefficients divided by the number of observations. From these and related arguments, we use a widely adopted convention and declare an observation to be a <em>high leverage point</em> if the leverage exceeds three times the average, that is, if</p>
<p><span class="math display">\[
h_{ii} &gt; \frac{3(k+1)}{n}.
\]</span></p>
<p>Having identified high leverage points, as with outliers it is important for the analyst to search for special causes that may have produced these unusual points. To illustrate, in Section 2.7 we identified the 1987 market crash as the reason behind the high leverage point. Further, high leverage points are often due to clerical errors in coding the data, which may or may not be easy to rectify. In general, the options for dealing with high leverage points are similar to those available for dealing with outliers.</p>
<div class="blackbox">
<p><em>Options for Handling High Leverage Points</em></p>
<ol style="list-style-type: decimal">
<li>Include the observation in the summary statistics but comment on its effect. For example, an observation may barely exceed a cut-off and its effect may not be important in the overall analysis.</li>
<li>Delete the observation from the data set. Again, the basic rationale for this action is that the observation is deemed not representative of some larger population. An intermediate course of action between (1) and (2) is to present the analysis both with and without the high leverage point. In this way, the impact of the point is fully demonstrated and the reader of your analysis may decide which option is more appropriate.</li>
<li>Choose another variable to represent the information. In some instances, another explanatory variable will be available to serve as a replacement. For example, in an apartment rents example, we could use the number of bedrooms to replace a square footage variable as a measure of apartment size. Although an apartment’s square footage may be unusually large causing it to be a high leverage point, it may have one, two, or three bedrooms, depending on the sample examined.</li>
<li>Use a nonlinear transformation of an explanatory variable. To illustrate, with our Stock Liquidity example in Section <a href="C5VarSelect.html#Sec553">5.5.3</a>, we can transform the debt-to-equity DEB_EQ continuous variable into a variable that indicates the presence of “high” debt-to-equity. For example, we might code DE_IND = 1 if DEB_EQ &gt; 5 and DE_IND = 0 if DEB_EQ ≤ 5. With this recoding, we still retain information on the financial leverage of a company without allowing the large values of DEB_EQ to drive the regression fit.</li>
</ol>
</div>
<p>Some analysts use “robust” estimation methodologies as an alternative to least squares estimation. The basic idea of these techniques is to reduce the effect of any particular observation. These techniques are useful in reducing the effect of both outliers and high leverage points. This tactic may be viewed as intermediate between one extreme procedure, ignoring the effect of unusual points, and another extreme, giving unusual points full credibility by deleting them from the data set. The word <em>robust</em> is meant to suggest that these estimation methodologies are “healthy” even when attacked by an occasional bad observation (a germ). We have seen that this is not true for least squares estimation.</p>
</div>
<div id="Sec542" class="section level3 hasAnchor" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> Cook’s Distance<a href="C5VarSelect.html#Sec542" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To quantify the influence of a point, a measure that considers both the response and explanatory variables is <em>Cook’s Distance</em>. This distance, <span class="math inline">\(D_i\)</span>, is defined as</p>
<p><span class="math display" id="eq:eq54">\[\begin{equation}
\begin{array}{ll}
D_i &amp;= \frac{\sum_{j=1}^{n} (\hat{y}_j - \hat{y}_{j(i)})^2}{(k+1) s^2} \tag{5.4} \\
&amp;= \left( \frac{e_i}{se(e_i)} \right)^2 \frac{h_{ii}}{(k+1)(1 - h_{ii})}.
\end{array}
\end{equation}\]</span></p>
<p>The first expression provides a definition. Here, <span class="math inline">\(\hat{y}_{j(i)}\)</span> is the prediction of the <span class="math inline">\(j\)</span>th observation, computed leaving the <span class="math inline">\(i\)</span>th observation out of the regression fit. To measure the impact of the <span class="math inline">\(i\)</span>th observation, we compare the fitted values with and without the <span class="math inline">\(i\)</span>th observation. Each difference is then squared and summed over all observations to summarize the impact.</p>
<p>The second equation provides another interpretation of the distance <span class="math inline">\(D_i\)</span>. The first part, <span class="math inline">\(\left( \frac{e_i}{se(e_i)} \right)^2\)</span>, is the square of the <span class="math inline">\(i\)</span>th standardized residual. The second part, <span class="math inline">\(\frac{h_{ii}}{(k+1)(1 - h_{ii})}\)</span>, is attributable solely to the leverage. Thus, the distance <span class="math inline">\(D_i\)</span> is composed of a measure for outliers times a measure for leverage. In this way, Cook’s distance accounts for both the response and explanatory variables. Section <a href="C5VarSelect.html#Sec5103">5.10.3</a> establishes the validity of equation <a href="C5VarSelect.html#eq:eq54">(5.4)</a>.</p>
<p>To get an idea of the expected size of <span class="math inline">\(D_i\)</span> for a point that is not unusual, recall that we expect the standardized residuals to be about one and the leverage <span class="math inline">\(h_{ii}\)</span> to be about <span class="math inline">\(\frac{k+1}{n}\)</span>. Thus, we anticipate that <span class="math inline">\(D_i\)</span> should be about <span class="math inline">\(\frac{1}{n}\)</span>. Another rule of thumb is to compare <span class="math inline">\(D_i\)</span> to an <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(df_1 = k+1\)</span> and <span class="math inline">\(df_2 = n - (k+1)\)</span> degrees of freedom. Values of <span class="math inline">\(D_i\)</span> that are large compared to this distribution merit attention.</p>
<hr />
<p><strong>Example: Outliers and High Leverage Points - Continued.</strong> To illustrate, we return to our example in Section 2.6. In this example, we considered 19 “good,” or base, points plus each of the three types of unusual points, labeled A, B, and C. Table <a href="C5VarSelect.html#tab:Tab56">5.6</a> summarizes the calculations.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab56">Table 5.6: </span><strong>Measures of Three Types of Unusual Points</strong>
</caption>
<thead>
<tr>
<th style="text-align:center;">
Observation
</th>
<th style="text-align:center;">
Standardized Residual <span class="math inline">\(e / se(e)\)</span>
</th>
<th style="text-align:center;">
Leverage <span class="math inline">\(h\)</span>
</th>
<th style="text-align:center;">
Cook’s Distance <span class="math inline">\(D\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
A
</td>
<td style="text-align:center;width: 1.8cm; ">
4.00
</td>
<td style="text-align:center;width: 1.8cm; ">
0.067
</td>
<td style="text-align:center;width: 1.8cm; ">
0.577
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
B
</td>
<td style="text-align:center;width: 1.8cm; ">
0.77
</td>
<td style="text-align:center;width: 1.8cm; ">
0.550
</td>
<td style="text-align:center;width: 1.8cm; ">
0.363
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
C
</td>
<td style="text-align:center;width: 1.8cm; ">
-4.01
</td>
<td style="text-align:center;width: 1.8cm; ">
0.550
</td>
<td style="text-align:center;width: 1.8cm; ">
9.832
</td>
</tr>
</tbody>
</table>
<p>As noted in Section 2.6, from the standardized residual column we see that both points A and C are outliers. To judge the size of the leverages, because there are <span class="math inline">\(n=20\)</span> points, the leverages are bounded by 0.05 and 1.00 with the average leverage being <span class="math inline">\(\bar{h} = \frac{2}{20} = 0.10\)</span>. Using 0.3 (<span class="math inline">\(= 3 \times \bar{h}\)</span>) as a cut-off, both points B and C are high leverage points. Note that their values are the same. This is because, from Figure 2.7, the values of the explanatory variables are the same and only the response variable has been changed. The column for Cook’s distance captures both types of unusual behavior. Because the typical value of <span class="math inline">\(D_i\)</span> is <span class="math inline">\(\frac{1}{n}\)</span> or 0.05, Cook’s distance provides one statistic to alert us to the fact that each point is unusual in one respect or another. In particular, point C has a very large <span class="math inline">\(D_i\)</span>, reflecting the fact that it is both an outlier and a high leverage point. The 95th percentile of an <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(df_1 = 2\)</span> and <span class="math inline">\(df_2 = 18\)</span> is 3.555. The fact that point C has a value of <span class="math inline">\(D_i\)</span> that well exceeds this cut-off indicates the substantial influence of this point.</p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_v2sfka7q&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_4szpsffk%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="5.4 Influential">
</iframe>
</center>
</div>
</div>
<div id="Sec55" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Collinearity<a href="C5VarSelect.html#Sec55" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="Sec551" class="section level3 hasAnchor" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> What is Collinearity?<a href="C5VarSelect.html#Sec551" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Collinearity</em>, or <em>multicollinearity</em>, occurs when one explanatory variable is, or nearly is, a linear combination of the other explanatory variables. Intuitively, with collinear data it is useful to think of explanatory variables as being highly correlated with one another. If an explanatory variable is collinear, then the question arises as to whether it is redundant, that is, whether the variable provides little additional information over and above the information in the other explanatory variables. The issues are: Is collinearity important? If so, how does it affect our model fit and how do we detect it? To address the first question, consider a somewhat pathological example.</p>
<hr />
<p><strong>Example: Perfectly Correlated Explanatory Variables.</strong> Joe Finance was asked to fit the model <span class="math inline">\(\mathrm{E} ~y = \beta_0 + \beta_1 x_1 + \beta_2 x_2\)</span> to a data set. His resulting fitted model was <span class="math inline">\(\hat{y} = -87 + x_1 + 18 x_2.\)</span> The data set under consideration is:</p>
<p><span class="math display">\[
\begin{array}{l|cccc} \hline
i &amp; 1 &amp; 2 &amp; 3 &amp; 4 \\ \hline
y_i &amp; 23 &amp; 83 &amp; 63 &amp; 103 \\
x_{i1} &amp; 2 &amp; 8  &amp;6 &amp; 10 \\
x_{i2} &amp; 6 &amp; 9 &amp; 8 &amp; 10 \\ \hline
\end{array}
\]</span></p>
<p>Joe checked the fit for each observation. Joe was very happy because he fit the data perfectly! For example, for the third observation the fitted value is
<span class="math inline">\(\hat{y}_3 = -87 + 6 + 18 \times 8 = 63\)</span> which is equal to the third response, <span class="math inline">\(y_3\)</span>. Because the response equals the fitted value, the residual is zero. You may check that this is true of each observation and thus the <span class="math inline">\(R^2\)</span> turned out to be <span class="math inline">\(100\%\)</span>.</p>
<p>However, Jane Actuary came along and fit the model
<span class="math inline">\(\hat{y} = -7 + 9 x_1 + 2 x_2.\)</span> Jane performed the same careful checks that Joe did and also got a perfect fit (<span class="math inline">\(R^2 = 1\)</span>). Who is right?</p>
<p>The answer is both and neither one. There are, in fact, an infinite number of fits. This is due to the perfect relationship
<span class="math inline">\(x_2 = 5 + \frac{x_1}{2}\)</span> between the two explanatory variables.</p>
<hr />
<p>This example illustrates some important facts about collinearity.</p>
<div class="blackbox">
<p><strong>Collinearity Facts</strong></p>
<ul>
<li>Collinearity neither precludes us from getting good fits nor from making predictions of new observations. Note that in the above example we got perfect fits.</li>
<li>Estimates of error variances and, therefore, tests of model adequacy, are still reliable.</li>
<li>In cases of serious collinearity, standard errors of individual regression coefficients are larger than cases where, other things equal, serious collinearity does not exist. With large standard errors, individual regression coefficients may not be meaningful. Further, because a large standard error means that the corresponding <span class="math inline">\(t\)</span>-ratio is small, it is difficult to detect the importance of a variable.</li>
</ul>
</div>
<p>To detect collinearity, begin with a matrix of correlation coefficients of the explanatory variables. This matrix is simple to create, easy to interpret, and quickly captures linear relationships between pairs of variables. A scatterplot matrix provides a visual reinforcement of the summary statistics in the correlation matrix.</p>
</div>
<div id="Sec552" class="section level3 hasAnchor" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> Variance Inflation Factors<a href="C5VarSelect.html#Sec552" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Correlation and scatterplot matrices capture only relationships between pairs of variables. To capture more complex relationships among several variables, we introduce the <em>variance inflation factor (VIF)</em>. To define a <em>VIF</em>, suppose that the set of explanatory variables is labeled <span class="math inline">\(x_1, x_2, \ldots, x_{k}\)</span>. Now, run the regression using <span class="math inline">\(x_j\)</span> as the “response” and the other <span class="math inline">\(x\)</span>’s (<span class="math inline">\(x_1, x_2, \ldots, x_{j-1}, x_{j+1}, \ldots, x_{k}\)</span>) as the explanatory variables. Denote the coefficient of determination from this regression by <span class="math inline">\(R_j^2\)</span>. We interpret <span class="math inline">\(R_j = \sqrt{R_j^2}\)</span> as the multiple correlation coefficient between <span class="math inline">\(x_j\)</span> and linear combinations of the other <span class="math inline">\(x\)</span>’s. From this coefficient of determination, we define the variance inflation factor</p>
<p><span class="math display">\[
VIF_j = \frac{1}{1 - R_j^2}, \text{ for } j = 1, 2, \ldots, k.
\]</span></p>
<p>A larger <span class="math inline">\(R_j^2\)</span> results in a larger <span class="math inline">\(VIF_j\)</span>; this means greater collinearity between <span class="math inline">\(x_j\)</span> and the other <span class="math inline">\(x\)</span>’s. Now, <span class="math inline">\(R_j^2\)</span> alone is enough to capture the linear relationship of interest. However, we use <span class="math inline">\(VIF_j\)</span> in lieu of <span class="math inline">\(R_j^2\)</span> as our measure for collinearity because of the algebraic relationship</p>
<p><span class="math display" id="eq:eq55">\[\begin{equation}
se(b_j) = s \frac{\sqrt{VIF_j}}{s_{x_j} \sqrt{n - 1}}.
\tag{5.5}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(se(b_j)\)</span> and <span class="math inline">\(s\)</span> are standard errors and residual standard deviation from a full regression fit of <span class="math inline">\(y\)</span> on <span class="math inline">\(x_1, \ldots, x_{k}\)</span>. Further, <span class="math inline">\(s_{x_j} = \sqrt{(n - 1)^{-1} \sum_{i=1}^{n} (x_{ij} - \bar{x}_j)^2 }\)</span> is the sample standard deviation of the <span class="math inline">\(j\)</span>th variable <span class="math inline">\(x_j\)</span>. Section <a href="C5VarSelect.html#Sec5103">5.10.3</a> provides a verification of equation <a href="C5VarSelect.html#eq:eq55">(5.5)</a>.</p>
<p>Thus, a larger <span class="math inline">\(VIF_j\)</span> results in a larger standard error associated with the <span class="math inline">\(j\)</span>th slope, <span class="math inline">\(b_j\)</span>. Recall that <span class="math inline">\(se(b_j)\)</span> is <span class="math inline">\(s\)</span> times the square root of the <span class="math inline">\((j+1)\)</span>st diagonal element of <span class="math inline">\((\mathbf{X}^{\prime} \mathbf{X})^{-1}\)</span>. The idea is that when collinearity occurs, the matrix <span class="math inline">\(\mathbf{X}^{\prime} \mathbf{X}\)</span> has properties similar to the number zero. When we attempt to calculate the inverse of <span class="math inline">\(\mathbf{X}^{\prime} \mathbf{X}\)</span>, this is analogous to dividing by zero for scalar numbers. As a rule of thumb, when <span class="math inline">\(VIF_j\)</span> exceeds 10 (which is equivalent to <span class="math inline">\(R_j^2 &gt; 90\%\)</span>), we say that severe collinearity exists. This may signal a need for action. <em>Tolerance</em>, defined as the reciprocal of the variance inflation factor, is another measure of collinearity used by some analysts.</p>
<p>For example, with <span class="math inline">\(k = 2\)</span> explanatory variables in the model, then <span class="math inline">\(R_1^2\)</span> is the squared correlation between the two explanatory variables, say <span class="math inline">\(r_{12}^2\)</span>. Then, from the equation above, we have that</p>
<p><span class="math display">\[
se(b_j) = s \left(s_{x_j} \sqrt{n - 1} \right)^{-1} \left(1 - r_{12}^2 \right)^{-1/2}, \text{ for } j = 1, 2.
\]</span></p>
<p>As the correlation approaches one in absolute value, <span class="math inline">\(|r_{12}| \rightarrow 1\)</span>, then the standard error becomes large meaning that the corresponding <span class="math inline">\(t\)</span>-statistic becomes small. In summary, a high <span class="math inline">\(VIF\)</span> may mean small <span class="math inline">\(t\)</span>-statistics even though variables are important. Further, one can check that the correlation between <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span> is <span class="math inline">\(-r_{12}\)</span>, indicating that the coefficient estimates are highly correlated.</p>
<p><strong>Example: Stock Market Liquidity - Continued.</strong> As an example, consider a regression of VOLUME on PRICE, SHARE, and VALUE. Unlike the explanatory variables considered in Section <a href="C5VarSelect.html#Sec553">5.5.3</a>, these three explanatory variables are not measures of trading activity. From a regression fit, we have <span class="math inline">\(R^2 = 61\%\)</span> and <span class="math inline">\(s = 6.72\)</span>. The statistics associated with the regression coefficients are in Table <a href="C5VarSelect.html#tab:Tab57">5.7</a>.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab57">Table 5.7: </span><strong>Statistics from a regression of VOLUME on PRICE, SHARE and VALUE</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
<span class="math inline">\(x_j\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(s_{x_j}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(b_j\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(se(b_j)\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(t(b_j)\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(VIF_j\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
PRICE
</td>
<td style="text-align:right;width: 1.6cm; ">
21.370
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.022
</td>
<td style="text-align:right;width: 1.6cm; ">
0.035
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.63
</td>
<td style="text-align:right;width: 1.6cm; ">
1.5
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SHARE
</td>
<td style="text-align:right;width: 1.6cm; ">
115.100
</td>
<td style="text-align:right;width: 1.6cm; ">
0.054
</td>
<td style="text-align:right;width: 1.6cm; ">
0.010
</td>
<td style="text-align:right;width: 1.6cm; ">
5.19
</td>
<td style="text-align:right;width: 1.6cm; ">
3.8
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
VALUE
</td>
<td style="text-align:right;width: 1.6cm; ">
8.157
</td>
<td style="text-align:right;width: 1.6cm; ">
0.313
</td>
<td style="text-align:right;width: 1.6cm; ">
0.162
</td>
<td style="text-align:right;width: 1.6cm; ">
1.94
</td>
<td style="text-align:right;width: 1.6cm; ">
4.7
</td>
</tr>
</tbody>
</table>
<p>You may check that the relationship in equation <a href="C5VarSelect.html#eq:eq55">(5.5)</a> is valid for each of the explanatory variables in Table <a href="C5VarSelect.html#tab:Tab57">5.7</a>. Because each <span class="math inline">\(VIF\)</span> statistic is less than ten, there is little reason to suspect severe collinearity. This is interesting because you may recall that there is a perfect relationship between PRICE, SHARE, and VALUE in that we defined the market value to be VALUE = PRICE <span class="math inline">\(\times\)</span> SHARE. However, the relationship is multiplicative, and hence is nonlinear. Because the variables are not linearly related, it is valid to enter all three into the regression model. From a financial perspective, the variable VALUE is important because it measures the worth of a firm. From a statistical perspective, the variable VALUE quantifies the interaction between PRICE and SHARE (interaction variables were introduced in Section 3.5.3).</p>
<hr />
<p>For collinearity, we are only interested in detecting linear trends, so nonlinear relationships between variables are not an issue here. For example, we have seen that it is sometimes useful to retain both an explanatory variable <span class="math inline">\(x\)</span> and its square <span class="math inline">\(x^2\)</span>, despite the fact that there is a perfect (nonlinear) relationship between the two. Still, we must check that nonlinear relationships are not approximately linear over the sampling region. Even though the relationship is theoretically nonlinear, if it is close to linear for our available sample, then problems of collinearity might arise. Figure <a href="C5VarSelect.html#fig:Fig55">5.5</a> illustrates this situation.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig55"></span>
<img src="RegressionMarkdown_files/figure-html/Fig55-1.png" alt="The relationship between \(x_1\) and \(x_2\) is nonlinear. However, over the region sampled, the variables have close to a linear relationship." width="60%" />
<p class="caption">
Figure 5.5: <strong>The relationship between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> is nonlinear.</strong> However, over the region sampled, the variables have close to a linear relationship.
</p>
</div>
<p>What can we do in the presence of collinearity? One option is to center each variable, by subtracting its average and dividing by its standard deviation. For example, create a new variable <span class="math inline">\(x_{ij}^{\ast} = (x_{ij} - \bar{x}_j) / s_{x_j}\)</span>. Occasionally, one variable appears as millions of units and another variable appears as fractions of units. Compared to the first mentioned variable, the second variable is close to a constant column of zeros (in that computers typically retain a finite number of digits). If this is true, then the second variable looks very much like a linear shift of the constant column of ones corresponding to the intercept. This is a problem because, with the least squares operations, we are implicitly squaring numbers that can make these columns appear even more similar.</p>
<p>This problem is simply a computational one and is easy to rectify. Simply recode the variables so that the units are of similar order of magnitude. Some data analysts automatically center all variables to avoid these problems. This is a legitimate approach because regression techniques search for linear relationships; location and scale shifts do not affect linear relationships.</p>
<p>Another option is to simply not explicitly account for collinearity in the analysis but to discuss some of its implications when interpreting the results of the regression analysis. This approach is probably the most commonly adopted one. It is a fact of life that, when dealing with business and economic data, collinearity does tend to exist among variables. Because the data tends to be observational in lieu of experimental in nature, there is little that the analyst can do to avoid this situation.</p>
<p>In the best-case situation, an auxiliary variable that provides similar information and that eases the collinearity problem is available to replace a variable. Similar to our discussion of high leverage points, a transformed version of the explanatory variable may also be a useful substitute. In some situations, such an ideal replacement is not available and we are forced to remove one or more variables. Deciding which variables to remove is a difficult choice. When deciding among variables, often the choice will be dictated by the investigator’s judgement as to which is the most relevant set of variables.</p>
</div>
<div id="Sec553" class="section level3 hasAnchor" number="5.5.3">
<h3><span class="header-section-number">5.5.3</span> Collinearity and Leverage<a href="C5VarSelect.html#Sec553" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Measures of collinearity and leverage share common characteristics and yet are designed to capture different aspects of a data set. Both are useful for data and model criticism; they are applied after a preliminary model fit with the objective of improving model specification. Further, both are calculated using only the explanatory variables; values of the responses do not enter into either calculation.</p>
<p>Our measure of collinearity, the variance inflation factor, is designed to help with model criticism. It is a measure calculated for each explanatory variable, designed to explain the relationship with other explanatory variables.</p>
<p>The leverage statistic is designed to help us with data criticism. It is a measure calculated for each observation to help us explain how unusual an observation is with respect to other observations.</p>
<p>Collinearity may be masked, or induced, by high leverage points, as pointed out by Mason and Gunst (1985) and Hadi (1988). Figures <a href="C5VarSelect.html#fig:Fig56">5.6</a> and <a href="C5VarSelect.html#fig:Fig57">5.7</a> provide illustrations of each case. These simple examples underscore an important point: data criticism and model criticism are not separate exercises.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig56"></span>
<img src="RegressionMarkdown_files/figure-html/Fig56-1.png" alt="With the exception of the marked point, \(x_1\) and \(x_2\) are highly linearly related." width="60%" />
<p class="caption">
Figure 5.6: <strong>With the exception of the marked point, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are highly linearly related.</strong>
</p>
</div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig57"></span>
<img src="RegressionMarkdown_files/figure-html/Fig57-1.png" alt="The highly linear relationship between \(x_1\) and \(x_2\) is primarily due to the marked point." width="60%" />
<p class="caption">
Figure 5.7: <strong>The highly linear relationship between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> is primarily due to the marked point.</strong>
</p>
</div>
<p>The examples in Figures <a href="C5VarSelect.html#fig:Fig56">5.6</a> and <a href="C5VarSelect.html#fig:Fig57">5.7</a> also help us to see one way in which high leverage points may affect standard errors of regression coefficients. Recall, in Section <a href="C5VarSelect.html#Sec541">5.4.1</a>, we saw that high leverage points may affect the model fitted values. In Figures <a href="C5VarSelect.html#fig:Fig56">5.6</a> and <a href="C5VarSelect.html#fig:Fig57">5.7</a>, we see that high leverage points affect collinearity. Thus, from equation <a href="C5VarSelect.html#eq:eq55">(5.5)</a>, we have that high leverage points can also affect our standard errors of regression coefficients.</p>
</div>
<div id="Sec554" class="section level3 hasAnchor" number="5.5.4">
<h3><span class="header-section-number">5.5.4</span> Suppressor Variables<a href="C5VarSelect.html#Sec554" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we have seen, severe collinearity can seriously inflate standard errors of regression coefficients. Because we rely on these standard errors for judging the usefulness of explanatory variables, our model selection procedures and inferences may be deficient in the presence of severe collinearity. Despite these drawbacks, mild collinearity in a data set should not be viewed as a deficiency of the data set; it is simply an attribute of the available explanatory variables.</p>
<p>Even if one explanatory variable is nearly a linear combination of the others, that does not necessarily mean that the information that it provides is redundant. To illustrate, we now consider a <em>suppressor variable</em>, an explanatory variable that increases the importance of other explanatory variables when included in the model.</p>
<p><strong>Example: Suppressor Variable.</strong> Figure <a href="C5VarSelect.html#fig:Fig58">5.8</a> shows a scatterplot matrix of a hypothetical data set of fifty observations. This data set contains a response and two explanatory variables. Table <a href="C5VarSelect.html#tab:Tab58">5.8</a> provides the corresponding matrix of correlation coefficients. Here, we see that the two explanatory variables are highly correlated. Now recall, for regression with one explanatory variable, that the correlation coefficient squared is the coefficient of determination. Thus, using Table <a href="C5VarSelect.html#tab:Tab58">5.8</a>, for a regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x_1\)</span>, the coefficient of determination is <span class="math inline">\((0.188)^2 = 3.5\%\)</span>. Similarly, for a regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x_2\)</span>, the coefficient of determination is <span class="math inline">\((-0.022)^2 = 0.04\%\)</span>. However, for a regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, the coefficient of determination turns out to be a surprisingly high <span class="math inline">\(80.7\%\)</span>. The interpretation is that individually, both <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> have little impact on <span class="math inline">\(y\)</span>. However, when taken jointly, the two explanatory variables have a significant effect on <span class="math inline">\(y\)</span>. Although Table <a href="C5VarSelect.html#tab:Tab58">5.8</a> shows that <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are strongly linearly related, this relationship does not mean that <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> provide the same information. In fact, in this example the two variables complement one another.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig58"></span>
<img src="RegressionMarkdown_files/figure-html/Fig58-1.png" alt="Scatterplot matrix of a response and two explanatory variables for the suppressor variable example" width="80%" />
<p class="caption">
Figure 5.8: <strong>Scatterplot matrix of a response and two explanatory variables for the suppressor variable example</strong>
</p>
</div>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab58">Table 5.8: </span><strong>Correlation Matrix for the Suppressor Example</strong>
</caption>
<tbody>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
</td>
<td style="text-align:center;width: 1.5cm; ">
<span class="math inline">\(x_1\)</span>
</td>
<td style="text-align:center;width: 1.5cm; ">
<span class="math inline">\(x_2\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
<span class="math inline">\(x_2\)</span>
</td>
<td style="text-align:center;width: 1.5cm; ">
0.972
</td>
<td style="text-align:center;width: 1.5cm; ">
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
<span class="math inline">\(y\)</span>
</td>
<td style="text-align:center;width: 1.5cm; ">
0.188
</td>
<td style="text-align:center;width: 1.5cm; ">
-0.022
</td>
</tr>
</tbody>
</table>
</div>
<div id="Sec555" class="section level3 hasAnchor" number="5.5.5">
<h3><span class="header-section-number">5.5.5</span> Orthogonal Variables<a href="C5VarSelect.html#Sec555" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another way to understand the impact of collinearity is to study the case when there are <em>no</em> relationships among sets of explanatory variables. Mathematically, two matrices <span class="math inline">\(\mathbf{X}_1\)</span> and <span class="math inline">\(\mathbf{X}_2\)</span> are said to be <em>orthogonal</em> if <span class="math inline">\(\mathbf{X}_1^{\prime} \mathbf{X}_2 = \mathbf{0}\)</span>. Intuitively, because we generally work with centered variables (with zero averages), this means that each column of <span class="math inline">\(\mathbf{X}_1\)</span> is uncorrelated with each column of <span class="math inline">\(\mathbf{X}_2\)</span>. Although unlikely to occur with observational data in the social sciences, when designing experimental treatments or constructing high degree polynomials, applications of orthogonal variables are regularly used (see for example, Hocking, 2003). For our purposes, we will work with orthogonal variables simply to understand the logical consequences of a total lack of collinearity.</p>
<p>Suppose that <span class="math inline">\(\mathbf{x}_2\)</span> is an explanatory variable that is orthogonal to <span class="math inline">\(\mathbf{X}_1\)</span>, where <span class="math inline">\(\mathbf{X}_1\)</span> is a matrix of explanatory variables that includes the intercept. Then, it is straightforward to check that the addition of <span class="math inline">\(\mathbf{x}_2\)</span> to the regression equation does not change the fit for coefficients corresponding to <span class="math inline">\(\mathbf{X}_1\)</span>. That is, without <span class="math inline">\(\mathbf{x}_2\)</span>, the coefficients corresponding to <span class="math inline">\(\mathbf{X}_1\)</span> would be calculated as <span class="math inline">\(\mathbf{b}_1 = \left(\mathbf{X}_1^{\prime} \mathbf{X}_1 \right)^{-1} \mathbf{X}_1^{\prime} \mathbf{y}\)</span>. Using the orthogonal <span class="math inline">\(\mathbf{x}_2\)</span> as part of the least squares calculation would not change the result for <span class="math inline">\(\mathbf{b}_1\)</span> (see the recursive least squares calculation in Section 4.7.2).</p>
<p>Further, the variance inflation factor for <span class="math inline">\(\mathbf{x}_2\)</span> is 1, indicating that the standard error is unaffected by the other explanatory variables. In the same vein, the reduction in the error sum of squares by adding the orthogonal variable <span class="math inline">\(\mathbf{x}_2\)</span> is due only to that variable, and not its interaction with other variables in <span class="math inline">\(\mathbf{X}_1\)</span>.</p>
<p>Orthogonal variables can be created for observational social science data (as well as other collinear data) using the method of <em>principal components</em>. With this method, one uses a linear transformation of the matrix of explanatory variables of the form, <span class="math inline">\(\mathbf{X}^{\ast} = \mathbf{X} \mathbf{P}\)</span>, so that the resulting matrix <span class="math inline">\(\mathbf{X}^{\ast}\)</span> is composed of orthogonal columns. The transformed regression function is <span class="math inline">\(\mathrm{E~}\mathbf{y} = \mathbf{X} \boldsymbol \beta = \mathbf{X} \mathbf{P} \mathbf{P}^{-1} \boldsymbol \beta = \mathbf{X}^{\ast} \boldsymbol \beta^{\ast}\)</span>, where <span class="math inline">\(\boldsymbol \beta^{\ast} = \mathbf{P}^{-1} \boldsymbol \beta\)</span> is the set of new regression coefficients. Estimation proceeds as before, with the orthogonal set of explanatory variables. By choosing the matrix <span class="math inline">\(\mathbf{P}\)</span> appropriately, each column of <span class="math inline">\(\mathbf{X}^{\ast}\)</span> has an identifiable contribution. Thus, we can readily use variable selection techniques to identify the “principal components” portions of <span class="math inline">\(\mathbf{X}^{\ast}\)</span> to use in the regression equation. Principal components regression is a widely used method in some application areas, such as psychology. It can easily address highly collinear data in a disciplined manner. The main drawback of this technique is that the resulting parameter estimates are difficult to interpret.</p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_gvtaer8i&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_2zg901rf%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="5.5 Collinearity">
</iframe>
</center>
</div>
</div>
<div id="Sec56" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Selection Criteria<a href="C5VarSelect.html#Sec56" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="Sec561" class="section level3 hasAnchor" number="5.6.1">
<h3><span class="header-section-number">5.6.1</span> Goodness of Fit<a href="C5VarSelect.html#Sec561" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How well does the model fit the data? Criteria that measure the proximity of the fitted model and realized data are known as <em>goodness of fit</em> statistics. Specifically, we interpret the fitted value <span class="math inline">\(\hat{y}_i\)</span> to be the best model approximation of the <span class="math inline">\(i\)</span>th observation and compare it to the actual value <span class="math inline">\(y_i\)</span>. In linear regression, we examine the difference through the residual <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span>; small residuals imply a good model fit. We have quantified this through the size of the typical error <span class="math inline">\((s)\)</span>, including the coefficient of determination <span class="math inline">\((R^2)\)</span> and an adjusted version <span class="math inline">\((R_{a}^2)\)</span>.</p>
<p>For nonlinear models, we will need additional measures, and it is helpful to introduce these measures in this simpler linear case. One such measure is <em>Akaike’s Information Criterion</em> that will be defined in terms of likelihood fits in Section 11.9.4. For linear regression, it reduces to</p>
<p><span class="math display" id="eq:eq56">\[\begin{equation}
AIC = n \ln (s^2) + n \ln (2 \pi) + n + 3 + k.
\tag{5.6}
\end{equation}\]</span></p>
<p>For model comparison, the smaller the <span class="math inline">\(AIC\)</span>, the better is the fit. Comparing models with the same number of variables (<span class="math inline">\(k\)</span>) means that selecting a model with small values of <span class="math inline">\(AIC\)</span> leads to the same choice as selecting a model with small values of the residual standard deviation <span class="math inline">\(s\)</span>. Further, a small number of parameters means a small value of <span class="math inline">\(AIC\)</span>, other things being equal. The idea is that this measure balances the fit (<span class="math inline">\(n \ln (s^2)\)</span>) with a penalty for complexity (the number of parameters, <span class="math inline">\(k+2\)</span>). Statistical packages often omit constants such as <span class="math inline">\(n \ln (2 \pi)\)</span> and <span class="math inline">\(n+3\)</span> when reporting <span class="math inline">\(AIC\)</span> because they do not matter when comparing models.</p>
<p>Section 11.9.4 will introduce another measure, the Bayes Information Criterion (<span class="math inline">\(BIC\)</span>), that gives a smaller weight to the penalty for complexity. A third goodness of fit measure that is used in linear regression models is the <span class="math inline">\(C_p\)</span> statistic. To define this statistic, assume that we have available <span class="math inline">\(k\)</span> explanatory variables <span class="math inline">\(x_1, ..., x_{k}\)</span> and run a regression to get <span class="math inline">\(s_{full}^2\)</span> as the mean square error. Now, suppose that we are considering using only <span class="math inline">\(p-1\)</span> explanatory variables so that there are <span class="math inline">\(p\)</span> regression coefficients. With these <span class="math inline">\(p-1\)</span> explanatory variables, we run a regression to get the error sum of squares <span class="math inline">\((Error~SS)_p\)</span>. Thus, we are in the position to define</p>
<p><span class="math display">\[
C_{p} = \frac{(Error~SS)_p}{s_{full}^2} - n + 2p.
\]</span></p>
<p>As a selection criterion, we choose the model with a “small” <span class="math inline">\(C_{p}\)</span> coefficient, where small is taken to be relative to <span class="math inline">\(p\)</span>. In general, models with smaller values of <span class="math inline">\(C_{p}\)</span> are more desirable.</p>
<p>Like the <span class="math inline">\(AIC\)</span> and <span class="math inline">\(BIC\)</span> statistics, the <span class="math inline">\(C_{p}\)</span> statistic strikes a balance between the model fit and complexity. That is, each statistic summarizes the trade-off between model fit and complexity, although with different weights. For most data sets, they recommend the same model and so an analyst can report any or all three statistics. However, for some applications, they lead to different recommended models. In this case, the analyst needs to rely more heavily on non-data driven criteria for model selection (which are always important in any regression application).</p>
</div>
<div id="Sec562" class="section level3 hasAnchor" number="5.6.2">
<h3><span class="header-section-number">5.6.2</span> Model Validation<a href="C5VarSelect.html#Sec562" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Model validation is the process of confirming that our proposed model is appropriate, especially in light of the purposes of the investigation. Recall the iterative model formulation selection process described in Section <a href="C5VarSelect.html#Sec51">5.1</a>. An important criticism of this iterative process is that it is guilty of <em>data-snooping</em>, that is, fitting a great number of models to a single set of data. As we saw in Section <a href="C5VarSelect.html#Sec52">5.2</a> on data-snooping in stepwise regression, by looking at a large number of models we may overfit the data and understate the natural variation in our representation.</p>
<p>We can respond to this criticism by using a technique called <em>out-of-sample validation</em>. The ideal situation is to have available two sets of data, one for model development and one for model validation. We initially develop one, or several, models on a first data set. The models developed from the first set of data are called our <em>candidate</em> models. Then, the relative performance of the candidate models could be measured on a second set of data. In this way, the data used to validate the model is unaffected by the procedures used to formulate the model.</p>
<p>Unfortunately, rarely will two sets of data be available to the investigator. However, we can implement the validation process by splitting the data set into two subsamples. We call these the <em>model development</em> and <em>validation subsamples</em>, respectively. They are also known as <em>training</em> and <em>testing</em> samples, respectively. To see how the process works in the linear regression context, consider the following procedure.</p>
<div class="blackbox">
<p><em>Out-of-Sample Validation Procedure</em></p>
<ol style="list-style-type: decimal">
<li>Begin with a sample size of <span class="math inline">\(n\)</span> and divide it into two subsamples, called the model development and validation subsamples. Let <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> denote the size of each subsample. In cross-sectional regression, do this split using a random sampling mechanism. Use the notation <span class="math inline">\(i=1,...,n_1\)</span> to represent observations from the model development subsample and <span class="math inline">\(i=n_1+1,...,n_1+n_2=n\)</span> for the observations from the validation subsample. Figure <a href="C5VarSelect.html#fig:Fig59">5.9</a> illustrates this procedure.</li>
<li>Using the model development subsample, fit a candidate model to the data set <span class="math inline">\(i=1,...,n_1\)</span>.</li>
<li>Using the model created in Step (ii) and the explanatory variables from the validation subsample, “predict” the dependent variables in the validation subsample, <span class="math inline">\(\hat{y}_i\)</span>, where <span class="math inline">\(i=n_1+1,...,n_1+n_2\)</span>. (To get these predictions, you may need to transform the dependent variables back to the original scale.)</li>
<li>Assess the proximity of the predictions to the held-out data. One measure is the <em>sum of squared prediction errors</em>
<span class="math display" id="eq:eq57">\[\begin{equation}
SSPE = \sum_{i=n_1+1}^{n_1+n_2} (y_i - \hat{y}_i)^2 .
\tag{5.7}
\end{equation}\]</span>
Repeat Steps (ii) through (iv) for each candidate model. Choose the model with the smallest <em>SSPE</em>.</li>
</ol>
</div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig59"></span>
<img src="RegressionMarkdown_files/figure-html/Fig59-1.png" alt="For model validation, a data set of size \(n\) is randomly split into two subsamples" width="80%" />
<p class="caption">
Figure 5.9: <strong>For model validation, a data set of size <span class="math inline">\(n\)</span> is randomly split into two subsamples</strong>
</p>
</div>
<p>There are a number of criticisms of the <em>SSPE</em>. First, it is clear that it takes a considerable amount of time and effort to calculate this statistic for each of several candidate models. However, as with many statistical techniques, this is merely a matter of having specialized statistical software available to perform the steps described above. Second, because the statistic itself is based on a random subset of the sample, its value will vary from analyst to analyst. This objection could be overcome by using the first <span class="math inline">\(n_1\)</span> observations from the sample. In most applications, this is not done in case there is a lurking relationship in the order of the observations. Third, and perhaps most important, is the fact that the choice of the relative subset sizes, <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>, is not clear. Various researchers recommend different proportions for the allocation. Snee (1977) suggests that data-splitting not be done unless the sample size is moderately large, specifically, <span class="math inline">\(n \geq 2(k+1) + 20\)</span>. The guidelines of Picard and Berk (1990) show that the greater the number of parameters to be estimated, the greater the proportion of observations needed for the model development subsample. As a rule of thumb, for data sets with 100 or fewer observations, use about 25-35% of the sample for out-of-sample validation. For data sets with 500 or more observations, use 50% of the sample for out-of-sample validation. Hastie, Tibshirani, and Friedman (2001) remark that a typical split is 50% for development/training, 25% for validation, and the remaining 25% for a third stage for further validation that they call <em>testing</em>.</p>
<p>Because of these criticisms, several variants of the basic out-of-sample validation process are used by analysts. Although there is no theoretically best procedure, it is widely agreed that model validation is an important part of confirming the usefulness of a model.</p>
</div>
<div id="Sec563" class="section level3 hasAnchor" number="5.6.3">
<h3><span class="header-section-number">5.6.3</span> Cross-Validation<a href="C5VarSelect.html#Sec563" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Cross-validation is the technique of model validation that splits the data into two disjoint sets. Section <a href="C5VarSelect.html#Sec562">5.6.2</a> discussed out-of-sample validation where the data was split randomly into two subsets both containing a sizeable percentage of data. Another popular method is <em>leave-one-out</em> cross-validation, where the validation sample consists of a single observation and the development sample is based on the remainder of the data set.</p>
<p>Especially for small sample sizes, an attractive leave-one-out cross-validation statistic is <em>PRESS</em>, the <em>Predicted Residual Sum of Squares</em>. To define the statistic, consider the following procedure where we suppose that a candidate model is available.</p>
<div class="blackbox">
<p><em>PRESS Validation Procedure</em></p>
<ol style="list-style-type: decimal">
<li>From the full sample, omit the <span class="math inline">\(i\)</span>th point and use the remaining <span class="math inline">\(n-1\)</span> observations to compute regression coefficients.</li>
<li>Use the regression coefficients computed in step one and the explanatory variables for the <span class="math inline">\(i\)</span>th observation to compute the predicted response, <span class="math inline">\(\hat{y}_{(i)}\)</span>. This part of the procedure is similar to the calculation of the <em>SSPE</em> statistic with <span class="math inline">\(n_1=n-1\)</span> and <span class="math inline">\(n_2=1\)</span>.</li>
<li>Now, repeat (i) and (ii) for <span class="math inline">\(i=1,...,n\)</span>. Summarizing, define
<span class="math display" id="eq:eq58">\[\begin{equation}
PRESS = \sum_{i=1}^{n} (y_i - \hat{y}_{(i)})^2 .
\tag{5.8}
\end{equation}\]</span>
As with <em>SSPE</em>, this statistic is calculated for each of several competing models. Under this criterion, we choose the model with the smallest <em>PRESS</em>.</li>
</ol>
</div>
<p>Based on this definition, the statistic seems very computationally intensive in that it requires <span class="math inline">\(n\)</span> regression fits to evaluate it. To address this, interested readers will find that Section <a href="C5VarSelect.html#Sec5102">5.10.2</a> establishes</p>
<p><span class="math display" id="eq:eq59">\[\begin{equation}
y_i - \hat{y}_{(i)} = \frac{e_i}{1 - h_{ii}} .
\tag{5.9}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(e_i\)</span> and <span class="math inline">\(h_{ii}\)</span> represent the <span class="math inline">\(i\)</span>th residual and leverage from the regression fit using the complete data set. This yields</p>
<p><span class="math display" id="eq:eq510">\[\begin{equation}
PRESS = \sum_{i=1}^{n} \left( \frac{e_i}{1 - h_{ii}} \right)^2 ,
\tag{5.10}
\end{equation}\]</span></p>
<p>which is a much easier computational formula. Thus, the <em>PRESS</em> statistic is less computationally intensive than <em>SSPE</em>.</p>
<p>Another important advantage of this statistic, when compared to <em>SSPE</em>, is that we do not need to make an arbitrary choice as to our relative subset sizes split. Indeed, because we are performing an “out-of-sample” validation for each observation, it can be argued that this procedure is more efficient, an especially important consideration when the sample size is small (say, less than 50 observations). A disadvantage is that because the model is re-fit for each point deleted, <em>PRESS</em> does not enjoy the appearance of independence between the estimation and prediction aspects, unlike <em>SSPE</em>.</p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_8d44i0b2&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_uy32cwry%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="5.6 SelectionCriteria">
</iframe>
</center>
</div>
</div>
<div id="Sec57" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Heteroscedasticity<a href="C5VarSelect.html#Sec57" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In most regression applications, the goal is to understand determinants of the regression function <span class="math inline">\(\mathrm{E~}y_i = \mathbf{x}_i^{\prime} \boldsymbol \beta = \mu_i\)</span>. Our ability to understand the mean is strongly influenced by the amount of spread from the mean that we quantify using the variance <span class="math inline">\(\mathrm{E}\left(y_i - \mu_i\right)^2\)</span>. In some applications, such as when I weigh myself on a scale, there is relatively little variability; repeated measurements yield almost the same result. In other applications, such as the time it takes me to fly to New York, repeated measurements yield substantial variability and are fraught with inherent uncertainty.</p>
<p>The amount of uncertainty can also vary on a case-by-case basis. We denote the case of “varying variability” with the notation <span class="math inline">\(\sigma_i^2 = \mathrm{E}\left(y_i - \mu_i\right)^2\)</span>. When the variability varies by observation, this is known as <em>heteroscedasticity</em> for “different scatter.” In contrast, the usual assumption of common variability (assumption E3/F3 in Section 3.2) is called <em>homoscedasticity</em>, meaning “same scatter.”</p>
<p>Our estimation strategies depend on the extent of heteroscedasticity. For datasets with only a mild amount of heteroscedasticity, one can use least squares to estimate the regression coefficients, perhaps combined with an adjustment for the standard errors (described in Section <a href="C5VarSelect.html#Sec572">5.7.2</a>). This is because least squares estimators are unbiased even in the presence of heteroscedasticity (see Property 1 in Section 3.2).</p>
<p>However, with heteroscedastic dependent variables, the Gauss-Markov theorem no longer applies and so the least squares estimators are not guaranteed to be optimal. In cases of severe heteroscedasticity, alternative estimators are used, the most common being those based on transformations of the dependent variable, as will be described in Section <a href="C5VarSelect.html#Sec574">5.7.4</a>.</p>
<div id="Sec571" class="section level3 hasAnchor" number="5.7.1">
<h3><span class="header-section-number">5.7.1</span> Detecting Heteroscedasticity<a href="C5VarSelect.html#Sec571" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To decide a strategy for handling potential heteroscedasticity, we must first assess, or detect, its presence.</p>
<p>To detect heteroscedasticity graphically, a good idea is to perform a preliminary regression fit of the data and plot the residuals versus the fitted values. To illustrate, Figure <a href="C5VarSelect.html#fig:Fig510">5.10</a> is a plot of a fictitious data set with one explanatory variable where the scatter increases as the explanatory variable increases. A least squares regression was performed - residuals and fitted values were computed. Figure <a href="C5VarSelect.html#fig:Fig511">5.11</a> is an example of a plot of residuals versus fitted values. The preliminary regression fit removes many of the major patterns in the data and leaves the eye free to concentrate on other patterns that may influence the fit. We plot residuals versus fitted values because the fitted values are an approximation of the expected value of the response and, in many situations, the variability grows with the expected response.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig510"></span>
<img src="RegressionMarkdown_files/figure-html/Fig510-1.png" alt="The shaded area represents the data." width="60%" />
<p class="caption">
Figure 5.10: <strong>The shaded area represents the data.</strong>
</p>
</div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig511"></span>
<img src="RegressionMarkdown_files/figure-html/Fig511-1.png" alt="Residuals plotted versus the fitted values for the data in Figure 5.10." width="60%" />
<p class="caption">
Figure 5.11: <strong>Residuals plotted versus the fitted values for the data in Figure 5.10.</strong>
</p>
</div>
<p>More formal tests of heteroscedasticity are also available in the regression literature. To illustrate, let us consider a test due to Breusch and Pagan (1980). Specifically, this test examines the alternative hypothesis <span class="math inline">\(H_a\)</span>: $ y_i = ^2 + _i^{} $, where <span class="math inline">\(\mathbf{z}_i\)</span> is a known vector of variables and <span class="math inline">\(\boldsymbol \gamma\)</span> is a <span class="math inline">\(p\)</span>-dimensional vector of parameters. Thus, the null hypothesis is <span class="math inline">\(H_0:~ \boldsymbol \gamma = \mathbf{0}\)</span> is equivalent to homoscedasticity, <span class="math inline">\(\mathrm{Var~} y_i = \sigma^2.\)</span></p>
<p><em>Procedure to Test for Heteroscedasticity</em></p>
<ol style="list-style-type: decimal">
<li>Fit a regression model and calculate the model residuals, <span class="math inline">\(e_i\)</span>.</li>
<li>Calculate squared standardized residuals, <span class="math inline">\(e_i^{\ast 2} = e_i^2 / s^2\)</span>.</li>
<li>Fit a regression model of <span class="math inline">\(e_i^{\ast 2}\)</span> on <span class="math inline">\(\mathbf{z}_i\)</span>.</li>
<li>The test statistic is <span class="math inline">\(LM = \frac{\text{Regress~SS}_z}{2}\)</span>, where <span class="math inline">\(Regress~SS_z\)</span> is the regression sum of squares from the model fit in step (iii).</li>
<li>Reject the null hypothesis if <span class="math inline">\(LM\)</span> exceeds a percentile from a chi-square distribution with <span class="math inline">\(p\)</span> degrees of freedom. The percentile is one minus the significance level of the test.</li>
</ol>
<p>Here, we use <span class="math inline">\(LM\)</span> to denote the test statistic because Breusch and Pagan derived it as a Lagrange multiplier statistic; see Breusch and Pagan (1980) for more details.</p>
</div>
<div id="Sec572" class="section level3 hasAnchor" number="5.7.2">
<h3><span class="header-section-number">5.7.2</span> Heteroscedasticity-Consistent Standard Errors<a href="C5VarSelect.html#Sec572" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For data sets with only mild heteroscedasticity, a sensible strategy is to employ least squares estimators of the regression coefficients and to adjust the calculation of standard errors to account for the heteroscedasticity.</p>
<p>From the Section 3.2 on properties, we saw that least squares regression coefficients could be written as <span class="math inline">\(\mathbf{b} = \sum_{i=1}^n \mathbf{w}_i y_i,\)</span>
where <span class="math inline">\(\mathbf{w}_i = \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1} \mathbf{x}_i\)</span>. Thus, with <span class="math inline">\(\sigma_i^2 = \mathrm{Var~} y_i\)</span>, we have
<span class="math display" id="eq:eq511">\[\begin{equation}
\mathrm{Var~}\mathbf{b} = \sum_{i=1}^n \mathbf{w}_i \mathbf{w}_i^{\prime} \sigma_i^2
= \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1} \left( \sum_{i=1}^n \sigma_i^2 \mathbf{x}_i \mathbf{x}_i^{\prime} \right) \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1}.
\tag{5.11}
\end{equation}\]</span>
This quantity is known except for <span class="math inline">\(\sigma_i^2\)</span>. We can compute residuals using the least squares regression coefficients as <span class="math inline">\(e_i = y_i - \mathbf{x}_i^{\prime} \mathbf{b}\)</span>. With these, we may define the <em>empirical</em>, or <em>robust</em>, estimate of the variance-covariance matrix as
<span class="math display">\[
\widehat{\mathrm{Var~}\mathbf{b}} = \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1} \left( \sum_{i=1}^n e_i^2 \mathbf{x}_i \mathbf{x}_i^{\prime} \right) \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1}.
\]</span>
The corresponding “heteroscedasticity-consistent” standard errors are
<span class="math display" id="eq:eq512">\[\begin{equation}
se_r(b_j) = \sqrt{(j+1)^{\text{st}}~ \text{diagonal element of }\widehat{\mathrm{Var~}\mathbf{b}}}.
\tag{5.12}
\end{equation}\]</span>
The logic behind this estimator is that each squared residual, <span class="math inline">\(e_i^2\)</span>, may be a poor estimate of <span class="math inline">\(\sigma_i^2\)</span>. However, our interest is estimating a (weighted) sum of variances in equation <a href="C5VarSelect.html#eq:eq511">(5.11)</a>; estimating the sum is a much easier task than estimating any individual variance estimate.</p>
<p>Robust, or heteroscedasticity-consistent, standard errors are widely available in statistical software packages. Here, you will also see alternative definitions of residuals employed, as in Section <a href="C5VarSelect.html#Sec531">5.3.1</a>. If your statistical package offers options, the robust estimator using studentized residuals is generally preferred.</p>
</div>
<div id="Sec573" class="section level3 hasAnchor" number="5.7.3">
<h3><span class="header-section-number">5.7.3</span> Weighted Least Squares<a href="C5VarSelect.html#Sec573" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The least squares estimators are less useful for datasets with severe heteroscedasticity. One strategy is to use a variation of least squares estimation by <em>weighting</em> observations. The idea is that, when minimizing the sum of squared errors using heteroscedastic data, the expected variability of some observations is smaller than others. Intuitively, it seems reasonable that the smaller the variability of the response, the more reliable that response and the greater weight that it should receive in the minimization procedure. <em>Weighted least squares</em> is a technique that accounts for this “varying variability.”</p>
<p>Specifically, we use Section 3.2.3 assumptions E1, E2, and E4, with E3 replaced by E <span class="math inline">\(\varepsilon_i = 0\)</span> and <span class="math inline">\(\text{Var} \varepsilon_i = \sigma^2 / w_i\)</span>, so that the variability is proportional to a known weight <span class="math inline">\(w_i\)</span>. For example, if unit of analysis <span class="math inline">\(i\)</span> represents a geographical entity such as a state, you might use the number of people in the state as a weight. Or, if <span class="math inline">\(i\)</span> represents a firm, you might use firm assets for the weighting variable. Larger values of <span class="math inline">\(w_i\)</span> indicate a more precise response variable through the smaller variability. In actuarial applications, weights are used to account for an exposure such as the amount of insurance premium, number of employees, size of the payroll, number of insured vehicles, and so forth (further discussion is in Chapter 18).</p>
<p>This model can be readily converted to the “ordinary” least squares problem by multiplying all regression variables by <span class="math inline">\(\sqrt{w_i}\)</span>. That is, if we define <span class="math inline">\(y_i^{\ast} = y_i \times \sqrt{w_i}\)</span> and <span class="math inline">\(x_{ij}^{\ast} = x_{ij} \times \sqrt{w_i}\)</span>, then from assumption E1 we have
<span class="math display">\[
\begin{array}{ll}
y_i^{\ast} &amp; = y_i \times \sqrt{w_i} = \left( \beta_0 x_{i0} + \beta_1 x_{i1} + \ldots + \beta_k x_{ik} + \varepsilon_i \right) \sqrt{w_i} \\
&amp;= \beta_0 x_{i0}^{\ast} + \beta_1 x_{i1}^{\ast} + \ldots + \beta_k x_{ik}^{\ast} + \varepsilon_i^{\ast}
\end{array}
\]</span>
where <span class="math inline">\(\varepsilon_i^{\ast} = \varepsilon_i \times \sqrt{w_i}\)</span> has homoscedastic variance <span class="math inline">\(\sigma^2\)</span>. Thus, with the rescaled variables, all inference can proceed as before.</p>
<p>This work has been automated in statistical packages where the user merely specifies the weights <span class="math inline">\(w_i\)</span> and the package does the rest. In terms of matrix algebra, this procedure can be accomplished by defining an <span class="math inline">\(n \times n\)</span> weight matrix <span class="math inline">\(\mathbf{W} = \text{diag}(w_i)\)</span> so that the <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\(\mathbf{W}\)</span> is <span class="math inline">\(w_i\)</span>. Extending equation (3.14) for example, the weighted least squares estimates can be expressed as
<span class="math display" id="eq:eq513">\[\begin{equation}
\mathbf{b}_{WLS} = \left( \mathbf{X}^{\prime} \mathbf{W} \mathbf{X} \right)^{-1} \mathbf{X}^{\prime} \mathbf{W} \mathbf{y}.
\tag{5.13}
\end{equation}\]</span>
Additional discussions of weighted least squares estimation will be presented in Section 15.1.1.</p>
</div>
<div id="Sec574" class="section level3 hasAnchor" number="5.7.4">
<h3><span class="header-section-number">5.7.4</span> Transformations<a href="C5VarSelect.html#Sec574" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another approach that handles severe heteroscedasticity, introduced in Section 1.3, is to transform the dependent variable, typically with a logarithmic transformation of the form <span class="math inline">\(y^{\ast} = \ln y\)</span>. As we saw in Section 1.3, transformations can serve to “shrink” spread out data and symmetrize a distribution. Through a change of scale, a transformation also changes the variability, potentially altering a heteroscedastic dataset into a homoscedastic one. This is both a strength and limitation of the transformation approach - a transformation simultaneously affects both the distribution and the heteroscedasticity.</p>
<p>Power transformations, such as the logarithmic transform, are most useful when the variability of the data grows with the mean. In this case, the transform will serve to “shrink” the data to a scale that appears to be homoscedastic. Conversely, because transformations are monotonic functions, they will not help with patterns of variability that are non-monotonic. Further, if your data is reasonably symmetric but heteroscedastic, a transformation will not be useful because any choice that mitigates the heteroscedasticity will skew the distribution.</p>
<p>When data are non-positive, it is common to add a constant to each observation so that all observations are positive prior to transformation. For example, the transform <span class="math inline">\(\ln(1+y)\)</span> accommodates the presence of zeros. One can also multiply by a constant so that the approximate original units are retained. For example, the transform <span class="math inline">\(100 \ln(1 + y/100)\)</span> may be applied to percentage data where negative percentages sometimes appear.</p>
<p>Our discussions of transformations have focused on transforming dependent variables. As noted in Section 3.5, transformations of explanatory variables are also possible. This is because the regression assumptions condition on explanatory variables (Section 3.2.3). Some analysts prefer to transform variables to approximate normality, thinking of multivariate normal distributions as a foundation for regression analysis. Others are reluctant to transform explanatory variables because of the difficulties in interpreting resulting models. The approach taken here is to use transforms that can be readily interpretable, such as those introduced in Section 3.5. Other transforms are certainly candidates to include in a selected model but they should provide substantial dividends in terms of fit or predictive power if they are difficult to communicate.</p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_lw0s376j&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_7laqr7q6%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="5.7 Heteroscedasticity">
</iframe>
</center>
</div>
</div>
<div id="Sec58" class="section level2 hasAnchor" number="5.8">
<h2><span class="header-section-number">5.8</span> Further Reading and References<a href="C5VarSelect.html#Sec58" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Long and Ervin (2000) gather compelling evidence for the use of alternative heteroscedasticity-consistent estimators of standard errors that have better finite sample performance than the classic versions. The large sample properties of empirical estimators have been established by Eicker (1967), Huber (1967) and White (1980) in the linear regression case. For the linear regression case, MacKinnon and White (1985) suggest alternatives that provide superior small-sample properties. For small samples, the evidence is based on (1) the biasedness of the estimators, (2) their motivation as jackknife estimators and (3) their performance in simulation studies.</p>
<p>Other measures of collinearity based on matrix algebra concepts involving eigenvalues, such as condition numbers and condition indices, are used by some analysts. See Belseley, Kuh and Welsch (1980) for a solid treatment of collinearity and regression diagnostics. Hocking (2003) provides additional background reading on collinearity and principal components. See Carroll and Ruppert (1988) for further discussions of transformations in regression.</p>
<p>Hastie, Tibshirani and Friedman (2001) give an advanced discussion of model selection issues, focusing on predictive aspects of models in the language of machine learning.</p>
<p><strong>Chapter References</strong></p>
<ul>
<li>Belseley, David A., Edwin Kuh and Roy E. Welsch (1980). <em>Regression Diagnostics: Identifying Influential Data and Sources of Collinearity</em>. Wiley, New York.</li>
<li>Bendel, R. B. and Afifi, A. A. (1977). Comparison of stopping rules in forward “stepwise” regression. <em>Journal of the American Statistical Association</em> 72, 46-53.</li>
<li>Box, George E. P. (1980). Sampling and Bayes inference in scientific modeling and robustness (with discussion). <em>Journal of the Royal Statistical Society</em>, Series A, 143, 383-430.</li>
<li>Breusch, T. S. and A. R. Pagan (1980). The Lagrange multiplier test and its applications to model specification in econometrics. <em>Review of Economic Studies</em>, 47, 239-53.</li>
<li>Carroll, Raymond J. and David Ruppert (1988). <em>Transformation and Weighting in Regression</em>, Chapman-Hall.</li>
<li>Eicker, F. (1967). Limit theorems for regressions with unequal and dependent errors. <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em> 1, LeCam, L. M. and J. Neyman, editors, University of California Press, pp, 59-82.</li>
<li>Hadi, A. S. (1988). Diagnosing collinearity-influential observations. <em>Computational Statistics and Data Analysis</em> 7, 143-159.</li>
<li>Hastie, Trevor, Robert Tibshirani and Jerome Friedman (2001). <em>The Elements of Statistical Learning: Data Mining, Inference and Prediction</em>. Springer-Verlag, New York.</li>
<li>Hocking, Ronald R. (2003). <em>Methods and Applications of Linear Models: Regression and the Analysis of Variance</em>. Wiley, New York.</li>
<li>Huber, P. J. (1967). The behaviour of maximum likelihood estimators under non-standard conditions. <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em> 1, LeCam, L. M. and Neyman, J. editors, University of California Press, pp, 221-33.</li>
<li>Long, J.S. and L.H. Ervin (2000). Using heteroscedasticity consistent standard errors in the linear regression model. <em>American Statistician</em> 54, 217-224.</li>
<li>MacKinnon, J.G. and H. White (1985). Some heteroskedasticity consistent covariance matrix estimators with improved finite sample properties. <em>Journal of Econometrics</em> 29, 53-57.</li>
<li>Mason, R. L. and Gunst, R. F. (1985). Outlier-induced collinearities. <em>Technometrics</em> 27, 401-407.</li>
<li>Picard, R. R. and Berk, K. N. (1990). Data splitting. <em>The American Statistician</em> 44, 140-147.</li>
<li>Rencher, A. C. and Pun, F. C. (1980). Inflation of <span class="math inline">\(R^2\)</span> in best subset regression. <em>Technometrics</em> 22, 49-53.</li>
<li>Snee, R. D. (1977). Validation of regression models. Methods and examples. <em>Technometrics</em> 19, 415-428.</li>
</ul>
</div>
<div id="Sec59" class="section level2 hasAnchor" number="5.9">
<h2><span class="header-section-number">5.9</span> Exercises<a href="C5VarSelect.html#Sec59" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>5.1. You are doing regression with one explanatory variable and so consider the basic linear regression model <span class="math inline">\(y_i = \beta_0 + \beta_1 x_i + \varepsilon_i\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Show that the <span class="math inline">\(i\)</span>th leverage can be simplified to
<span class="math display">\[
h_{ii} = \frac{1}{n} + \frac{(x_i - \overline{x})^2}{(n-1) s_x^2}.
\]</span></li>
<li>Show that <span class="math inline">\(\overline{h} = 2 / n\)</span>.</li>
<li>Suppose that <span class="math inline">\(h_{ii} = 6/n\)</span>. How many standard deviations is <span class="math inline">\(x_i\)</span> away (either above or below) from the mean?</li>
</ol>
<p>5.2. Consider the output of a regression using one explanatory variable on <span class="math inline">\(n=3\)</span> observations. The residuals and leverages are:
<span class="math display">\[
\small{
   \begin{array}{l|ccc}
   \hline
   i &amp; 1 &amp; 2 &amp; 3 \\ \hline
   \text{Residuals } e_i &amp; 3.181 &amp; -6.362 &amp; 3.181 \\
   \text{Leverages } h_{ii} &amp; 0.8333 &amp; 0.3333 &amp; 0.8333 \\
   \hline
   \end{array}
}   
\]</span></p>
<p>Compute the <span class="math inline">\(PRESS\)</span> statistic.</p>
<p>5.3. <strong>National Life Expectancies.</strong> We continue the analysis begun in Exercises 1.7, 2.22, 3.6 and 4.7. The focus of this exercise is variable selection.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Begin with the data from <span class="math inline">\(n=185\)</span> countries throughout the world that have valid (non-missing) life expectancies. Plot the life expectancy versus the gross domestic product and private expenditures on health. From these plots, describe why it is desirable to use logarithmic transforms, lnGDP and lnHEALTH, respectively. Also plot life expectancy versus lnGDP and lnHEALTH to confirm your intuition.</p></li>
<li><p>Use a stepwise regression algorithm to help you select a model. Do not consider the variables RESEARCHERS, SMOKING, and FEMALEBOSS as these have many missing values. For the remaining variables, use only the observations without any missing values. Do this twice, with and without the categorical variable EGION.</p></li>
<li><p>Return to the full data set of <span class="math inline">\(n=185\)</span> countries and run a regression model using FERTILITY, PUBLICEDUCATION, and lnHEALTH as explanatory variables.</p>
<p>c(i). Provide histograms of standardized residuals and leverages. <br>
c(ii). Identify the standardized residual and leverage associated with Lesotho, formerly Basutoland, a kingdom surrounded by South Africa. Is this observation an outlier, high leverage point, or both? <br>
c(iii). Re-run the regression without Lesotho. Cite any differences in the statistical coefficients between this model and the one in part c(i).</p></li>
</ol>
<p>5.4. <strong>Term Life Insurance.</strong> We continue our study of Term Life Insurance Demand from Chapters 3 and 4. Specifically, we examine the 2004 Survey of Consumer Finances (SCF), a nationally representative sample that contains extensive information on assets, liabilities, income, and demographic characteristics of those sampled (potential U.S. customers). We study a random sample of 500 families with positive incomes. From the sample of 500, we initially consider a subsample of <span class="math inline">\(n=275\)</span> families that purchased term life insurance.</p>
<p>Consider a linear regression of LNINCOME, EDUCATION, NUMHH, MARSTAT, AGE, and GENDER on LNFACE}.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Collinearity. Not all of the variables turned out to be statistically significant. To investigate one possible explanation, calculate variance inflation factors.</p>
<p>a(i). Briefly explain the idea of collinearity and a variance inflation factor. <br>
a(ii). What constitutes a large variance inflation factor? <br>
a(iii). If a large variance inflation factor is detected, what possible courses of action do we have to address this aspect of the data? <br>
a(iv). Supplement the variance inflation factor statistics with a table of correlations of explanatory variables. Based on these statistics, is collinearity an issue with this fitted model? Why or why not?</p></li>
<li><p>Unusual Points. Sometimes a poor model fit can be due to unusual points.</p>
<p>b(i). Define the idea of leverage for an observation. <br>
b(ii). For this fitted model, give standard rules of thumb for identifying points with unusual leverage. Identify any unusual points. <br>
b(iii). An analyst is concerned with leverage values for this fitted model and suggests using FACE as the dependent variable instead of LNFACE. Describe how leverage values would change using this alternative dependent variable.</p></li>
<li><p>Residual Analysis. We can learn how to improve model fits from analyses of residuals.</p>
<p>c(i). Provide a plot of residuals versus fitted values. What do we hope to learn from this type of plot? Does this plot display any model inadequacies? <br>
c(ii). Provide a <span class="math inline">\(qq\)</span> plot of residuals. What do we hope to learn from this type of plot? Does this plot display any model inadequacies? <br>
c(iii). Provide a plot of residuals versus leverages. What do we hope to learn from this type of plot? Does this plot display any model inadequacies?</p></li>
<li><p>Stepwise Regression. Run a stepwise regression algorithm. Suppose that this algorithm suggests a model using LNINCOME}, EDUCATION, NUMHH, and GENDER as explanatory variables to predict the dependent variable LNFACE.</p>
<p>d(i). What is the purpose of stepwise regression? <br>
d(ii). Describe two important drawbacks of stepwise regression algorithms.</p></li>
</ol>
</div>
<div id="Sec510" class="section level2 hasAnchor" number="5.10">
<h2><span class="header-section-number">5.10</span> Technical Supplements for Chapter 5<a href="C5VarSelect.html#Sec510" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="Sec5101" class="section level3 hasAnchor" number="5.10.1">
<h3><span class="header-section-number">5.10.1</span> Projection Matrix<a href="C5VarSelect.html#Sec5101" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Hat Matrix.</strong> We define the hat matrix to be <span class="math inline">\(\mathbf{H} = \mathbf{X(X}^{\prime}\mathbf{X)}^{-1} \mathbf{X}^{\prime}\)</span>, so that <span class="math inline">\(\mathbf{\hat{y}} = \mathbf{X b} = \mathbf{Hy}\)</span>. From this, the matrix <span class="math inline">\(\mathbf{H}\)</span> is said to <em>project</em> the vector of responses <span class="math inline">\(\mathbf{y}\)</span> onto the vector of fitted values <span class="math inline">\(\mathbf{\hat{y}}\)</span>.</p>
<p>Because <span class="math inline">\(\mathbf{H}^{\prime} = \mathbf{H}\)</span>, the hat matrix is symmetric. Further, it is also an <em>idempotent</em> matrix due to the property that <span class="math inline">\(\mathbf{HH} = \mathbf{H}\)</span>. To see this, we have that
<span class="math display">\[
\begin{array}{ll}
\mathbf{HH} &amp;= \mathbf{(X(\mathbf{X}^{\prime}X)}^{-1}\mathbf{X}^{\prime}\mathbf{)(X(\mathbf{X}^{\prime}X)}^{-1}\mathbf{X}^{\prime}\mathbf{)} \\
&amp;= \mathbf{X(\mathbf{X}^{\prime}X)}^{-1}\mathbf{(\mathbf{X}^{\prime}X)(\mathbf{X}^{\prime}X)}^{-1}\mathbf{X}^{\prime} = \mathbf{X(\mathbf{X}^{\prime}X)}^{-1}\mathbf{X}^{\prime} = \mathbf{H}.
\end{array}
\]</span>
Similarly, it is easy to check that <span class="math inline">\(\mathbf{I-H}\)</span> is idempotent. Because <span class="math inline">\(\mathbf{H}\)</span> is idempotent, from some results in matrix algebra, it is straightforward to show that
<span class="math display">\[
\sum_{i=1}^{n} h_{ii} = k + 1.
\]</span>
As discussed in Section <a href="C5VarSelect.html#Sec541">5.4.1</a>, we use our bounds and the average leverage, <span class="math inline">\(\bar{h} = (k + 1)/n\)</span>, to help identify observations with unusually high leverage.</p>
<p><strong>Variance of Residuals.</strong> Using the model equation <span class="math inline">\(\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}\)</span>, we can express the vector of residuals as
<span class="math display" id="eq:eq514">\[\begin{equation}
\mathbf{e} = \mathbf{y} - \mathbf{\hat{y}} = \mathbf{y - Hy} = \mathbf{(I-H)(X \boldsymbol{\beta} + \boldsymbol{\varepsilon})} = \mathbf{(I-H) \boldsymbol{\varepsilon}}.
\tag{5.14}
\end{equation}\]</span>
The last equality is due to the fact that <span class="math inline">\(\mathbf{(I-H)X} = \mathbf{X - HX} = \mathbf{X - X} = \mathbf{0}\)</span>. Using <span class="math inline">\(\text{Var~} \boldsymbol{\varepsilon} = \sigma^2 \mathbf{I}\)</span>, we have
<span class="math display">\[
\begin{array}{ll}
\text{Var } \mathbf{e} &amp;= \text{Var }\left[ \mathbf{(I-H)\boldsymbol{\varepsilon}} \right] = \mathbf{(I-H)} \text{Var } \boldsymbol{\varepsilon} \mathbf{(I-H)} \\
&amp;= \sigma^2 \mathbf{(I-H)} \mathbf{I} \mathbf{(I-H)} = \sigma^2 \mathbf{(I-H)}.
\end{array}
\]</span>
The last equality comes from the fact that <span class="math inline">\(\mathbf{I-H}\)</span> is idempotent. Thus, we have that
<span class="math display" id="eq:eq515">\[\begin{equation}
\text{Var } e_i = \sigma^2 (1 - h_{ii}) \text{   and   Cov } (e_i, e_j) = -\sigma^2 h_{ij}.
\tag{5.15}
\end{equation}\]</span>
Thus, although the true errors <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> are uncorrelated, there is a small negative correlation among residuals <span class="math inline">\(\mathbf{e}\)</span>.</p>
<p><strong>Dominance of the Error in the Residual.</strong> Examining the <span class="math inline">\(i\)</span>th row of equation <a href="C5VarSelect.html#eq:eq514">(5.14)</a>, we have that the <span class="math inline">\(i\)</span>th residual
<span class="math display" id="eq:eq516">\[\begin{equation}
e_i = \varepsilon_i - \sum_{j=1}^{n} h_{ij} \varepsilon_j
\tag{5.16}
\end{equation}\]</span>
can be expressed as a linear combination of independent errors. The relation <span class="math inline">\(\mathbf{H} = \mathbf{HH}\)</span> yields
<span class="math display" id="eq:eq517">\[\begin{equation}
h_{ii} = \sum_{j=1}^{n} h_{ij}^2.
\tag{5.17}
\end{equation}\]</span>
Because <span class="math inline">\(h_{ii}\)</span> is, on average, <span class="math inline">\((k + 1)/n\)</span>, this indicates that each <span class="math inline">\(h_{ij}\)</span> is small relative to 1. Thus, when interpreting equation <a href="C5VarSelect.html#eq:eq516">(5.16)</a>, we say that most of the information in <span class="math inline">\(e_i\)</span> is due to <span class="math inline">\(\varepsilon_i\)</span>.</p>
<p><strong>Correlations with Residuals.</strong> First define <span class="math inline">\(\mathbf{x}^j = (x_{1j}, x_{2j}, \dots, x_{nj})^{\prime}\)</span> to be the column representing the <span class="math inline">\(j\)</span>th variable. With this notation, we can partition the matrix of explanatory variables as <span class="math inline">\(\mathbf{X} = \left( \mathbf{x}^{0}, \mathbf{x}^{1}, \dots, \mathbf{x}^{k} \right)\)</span>. Now, examining the <span class="math inline">\(j\)</span>th column of the relation <span class="math inline">\(\mathbf{(I-H)X} = \mathbf{0}\)</span>, we have <span class="math inline">\(\mathbf{(I-H)x}^{j} = \mathbf{0}\)</span>. With <span class="math inline">\(\mathbf{e} = \mathbf{(I-H) \boldsymbol{\varepsilon}}\)</span>, this yields
<span class="math display">\[
\mathbf{e}^{\prime} \mathbf{x}^{j} = \boldsymbol{\varepsilon}^{\prime} \mathbf{(I-H)x}^{j} = 0,
\]</span>
for <span class="math inline">\(j = 0, 1, \ldots, k.\)</span> This result has several implications. If the intercept is in the model, then <span class="math inline">\(\mathbf{x}^{0} = (1, 1, \ldots, 1)^{\prime}\)</span> is a vector of ones. Here, <span class="math inline">\(\mathbf{e}^{\prime} \mathbf{x}^{0} = 0\)</span> means that <span class="math inline">\(\sum_{i=1}^{n} e_i = 0\)</span> or, the average residual is zero. Further, because <span class="math inline">\(\mathbf{e}^{\prime} \mathbf{x}^{j} = 0\)</span>, it is easy to check that the sample correlation between <span class="math inline">\(\mathbf{e}\)</span> and <span class="math inline">\(\mathbf{x}^{j}\)</span> is zero. Along the same line, we also have that <span class="math inline">\(\mathbf{e}^{\prime} \mathbf{\hat{y}} = \mathbf{e}^{\prime} \mathbf{(I-H)Xb} = \mathbf{0}\)</span>. Thus, using the same argument as above, the sample correlation between <span class="math inline">\(\mathbf{e}\)</span> and <span class="math inline">\(\mathbf{\hat{y}}\)</span> is zero.</p>
<p><strong>Multiple Correlation Coefficient.</strong> For an example of a non-zero correlation, consider <span class="math inline">\(r(\mathbf{y, \hat{y}})\)</span>, the sample correlation between <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{\hat{y}}\)</span>. Because <span class="math inline">\(\mathbf{(I-H)x}^{0} = \mathbf{0}\)</span>, we have <span class="math inline">\(\mathbf{x}^{0} = \mathbf{Hx}^{0}\)</span> and thus, <span class="math inline">\(\mathbf{\hat{y}}^{\prime} \mathbf{x}^{0} = \mathbf{y}^{\prime} \mathbf{Hx}^{0} = \mathbf{y^{\prime} x}^{0}\)</span>. Assuming <span class="math inline">\(\mathbf{x}^{0} = (1, 1, \ldots, 1)^{\prime}\)</span>, this means that <span class="math inline">\(\sum_{i=1}^{n} \hat{y}_i = \sum_{i=1}^{n} y_i\)</span>, so that the average fitted value is <span class="math inline">\(\bar{y}\)</span>.</p>
<p><span class="math display">\[
r(\mathbf{y, \hat{y}}) = \frac{\sum_{i=1}^{n} (y_i - \bar{y})(\hat{y}_i - \bar{y})}{(n-1) s_y s_{\hat{y}}}.
\]</span></p>
<p>Recall that <span class="math inline">\((n-1) s_y^2 = \sum_{i=1}^{n} (y_i - \bar{y})^2 = Total ~SS\)</span> and <span class="math inline">\((n-1) s_{\hat{y}}^2 = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 = Regress ~SS\)</span>. Further, with <span class="math inline">\(\mathbf{x}^0 = (1, 1, \ldots, 1)^{\prime}\)</span>,
<span class="math display">\[
\begin{array}{ll}
\sum_{i=1}^{n} (y_i - \bar{y})(\hat{y}_i - \bar{y}) &amp;= (\mathbf{y} - \bar{y} \mathbf{x}^0)^{\prime} (\mathbf{\hat{y}} - \bar{y} \mathbf{x}^0) = \mathbf{y}^{\prime} \mathbf{\hat{y}} - \bar{y}^2 \mathbf{x}^{0 \prime} \mathbf{x}^0 \\
&amp;= \mathbf{y}^{\prime} \mathbf{Xb} - n \bar{y}^2 = Regress ~SS.
\end{array}
\]</span></p>
<p>This yields
<span class="math display" id="eq:eq518">\[\begin{equation}
r(\mathbf{y, \hat{y}}) = \frac{Regress ~SS}{\sqrt{\left( Total ~SS \right) \left( Regress ~SS \right)}} = \sqrt{\frac{Regress ~SS}{Total ~SS}} = \sqrt{R^2}.
\tag{5.18}
\end{equation}\]</span>
That is, the coefficient of determination can be interpreted as the square root of the correlation between the observed and fitted responses.</p>
</div>
<div id="Sec5102" class="section level3 hasAnchor" number="5.10.2">
<h3><span class="header-section-number">5.10.2</span> Leave One Out Statistics<a href="C5VarSelect.html#Sec5102" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Notation.</strong> To test the sensitivity of regression quantities, there are a number of statistics of interest that are based on the notion of “leaving out,” or omitting, an observation. To this end, the subscript notation <span class="math inline">\((i)\)</span> means to <em>leave out</em> the <span class="math inline">\(i\)</span>th observation. For example, omitting the row of explanatory variables <span class="math inline">\(\mathbf{x}_i^{\prime} = (x_{i0}, x_{i1}, \dots, x_{ik})\)</span> from <span class="math inline">\(\mathbf{X}\)</span> yields <span class="math inline">\(\mathbf{X}_{(i)}\)</span>, a <span class="math inline">\((n-1) \times (k+1)\)</span> matrix of explanatory variables. Similarly, <span class="math inline">\(\mathbf{y}_{(i)}\)</span> is a <span class="math inline">\((n-1) \times 1\)</span> vector, based on removing the <span class="math inline">\(i\)</span>th row from <span class="math inline">\(\mathbf{y}\)</span>.</p>
<p><strong>Basic Matrix Result.</strong> Suppose that <span class="math inline">\(\mathbf{A}\)</span> is an invertible, <span class="math inline">\(p \times p\)</span> matrix and <span class="math inline">\(\mathbf{z}\)</span> is a <span class="math inline">\(p \times 1\)</span> vector. The following result from matrix algebra provides an important tool for understanding leave one out statistics in linear regression analysis.
<span class="math display" id="eq:eq519">\[\begin{equation}
\left( \mathbf{A - zz}^{\prime} \right)^{-1} = \mathbf{A}^{-1} + \frac{\mathbf{A}^{-1} \mathbf{zz}^{\prime} \mathbf{A}^{-1}}{1 - \mathbf{z}^{\prime} \mathbf{A}^{-1} \mathbf{z}}.
\tag{5.19}
\end{equation}\]</span>
To check this result, simply multiply <span class="math inline">\(\mathbf{A - zz}^{\prime}\)</span> by the right hand side of the equation to get <span class="math inline">\(\mathbf{I}\)</span>, the identity matrix.</p>
<p><strong>Vector of Regression Coefficients.</strong> Omitting the <span class="math inline">\(i\)</span>th observation, our new vector of regression coefficients is <span class="math inline">\(\mathbf{b}_{(i)} = \left( \mathbf{X}_{(i)}^{\prime} \mathbf{X}_{(i)} \right)^{-1} \mathbf{X}_{(i)}^{\prime} \mathbf{y}_{(i)}.\)</span> An alternative expression for <span class="math inline">\(\mathbf{b}_{(i)}\)</span> that is simpler to compute turns out to be
<span class="math display" id="eq:eq520">\[\begin{equation}
\mathbf{b}_{(i)} = \mathbf{b} - \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}}.S
\tag{5.20}
\end{equation}\]</span>
To verify this, first use the matrix inversion result with <span class="math inline">\(\mathbf{A} = \mathbf{X}^{\prime} \mathbf{X}\)</span> and <span class="math inline">\(\mathbf{z} = \mathbf{x}_i\)</span> to get
<span class="math display">\[
\left( \mathbf{X}_{(i)}^{\prime} \mathbf{X}_{(i)} \right)^{-1} = (\mathbf{X}^{\prime} \mathbf{X} - \mathbf{x}_i \mathbf{x}_i^{\prime})^{-1} = (\mathbf{X}^{\prime} \mathbf{X})^{-1} + \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1}}{1 - h_{ii}},
\]</span>
where, from the leverage result, we have <span class="math inline">\(h_{ii} = \mathbf{x}_i^{\prime} (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{x}_i\)</span>. Multiplying each side by
<span class="math display">\[
\mathbf{X}_{(i)}^{\prime} \mathbf{y}_{(i)} = \mathbf{X}^{\prime} \mathbf{y} - \mathbf{x}_i y_i
\]</span>
yields
<span class="math display">\[
\begin{array}{ll}
\mathbf{b}_{(i)} &amp;= \left( \mathbf{X}_{(i)}^{\prime} \mathbf{X}_{(i)} \right)^{-1} \mathbf{X}_{(i)}^{\prime} \mathbf{y}_{(i)} \\
&amp;= \left( (\mathbf{X}^{\prime} \mathbf{X})^{-1} + \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1}}{1 - h_{ii}} \right) \left( \mathbf{X}^{\prime} \mathbf{y} - \mathbf{x}_i y_i \right) \\
&amp;= \mathbf{b} - \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i y_i + \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \mathbf{b} - \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i y_i}{1 - h_{ii}} \\
&amp;= \mathbf{b} - \frac{\left( 1 - h_{ii} \right) \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i y_i - \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \mathbf{b} - \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i h_{ii} y_i}{1 - h_{ii}} \\
&amp;= \mathbf{b} - \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i y_i - \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \mathbf{b}}{1 - h_{ii}} \\
&amp;= \mathbf{b} - \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}}.
\end{array}
\]</span>
This establishes the result.</p>
<p><strong>Cook’s Distance.</strong> To measure the effect, or <em>influence</em>, of omitting the <span class="math inline">\(i\)</span>th observation, Cook examined the difference between fitted values with and without the observation. We define Cook’s Distance to be
<span class="math display">\[
D_i = \frac{\left( \mathbf{\hat{y} - \hat{y}}_{(i)} \right)^{\prime} \left( \mathbf{\hat{y} - \hat{y}}_{(i)} \right)}{(k+1) s^2}
\]</span>
where <span class="math inline">\(\mathbf{\hat{y}}_{(i)} = \mathbf{Xb}_{(i)}\)</span> is the vector of fitted values calculated omitting the <span class="math inline">\(i\)</span>th point. Using equation <a href="C5VarSelect.html#eq:eq520">(5.20)</a> and <span class="math inline">\(\mathbf{\hat{y}} = \mathbf{Xb}\)</span>, an alternative expression for Cook’s Distance is
<span class="math display">\[
\begin{array}{ll}
D_i &amp;= \frac{\left( \mathbf{b - b}_{(i)} \right)^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right) \left( \mathbf{b - b}_{(i)} \right)}{(k+1) s^2} \\
&amp;= \frac{e_i^2}{(1 - h_{ii})^2} \frac{\mathbf{x}_i^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \left( \mathbf{X}^{\prime} \mathbf{X} \right) \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i}{(k+1) s^2} \\
&amp; = \frac{e_i^2}{(1 - h_{ii})^2} \frac{h_{ii}}{(k+1) s^2} \\
&amp;= \left( \frac{e_i}{s \sqrt{1 - h_{ii}}} \right)^2 \frac{h_{ii}}{(k+1) (1 - h_{ii})}.
\end{array}
\]</span>
This result is not only useful computationally, it also serves to decompose the statistic into the part due to the standardized residual, <span class="math inline">\((e_i/(s \sqrt{1 - h_{ii}}))^2\)</span>, and due to the leverage, <span class="math inline">\(\frac{h_{ii}}{(k+1) (1 - h_{ii})}\)</span>.</p>
<p><strong>Leave One Out Residual.</strong> The leave one out residual is defined by <span class="math inline">\(e_{(i)} = y_i - \mathbf{x}_i^{\prime} \mathbf{b}_{(i)}\)</span>. It is used in computing the <em>PRESS</em> statistic, described in Section <a href="C5VarSelect.html#Sec563">5.6.3</a>. A simple computational expression is <span class="math inline">\(e_{(i)} = \frac{e_i}{1 - h_{ii}}\)</span>. To verify this, use equation <a href="C5VarSelect.html#eq:eq520">(5.20)</a> to get
<span class="math display">\[
e_{(i)} = y_i - \mathbf{x}_i^{\prime} \mathbf{b}_{(i)} = y_i - \mathbf{x}_i^{\prime} \left( \mathbf{b} - \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}} \right)
\]</span>
<span class="math display">\[
= e_i + \frac{\mathbf{x}_i \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}} = e_i + \frac{h_{ii} e_i}{1 - h_{ii}} = \frac{e_i}{1 - h_{ii}}.
\]</span></p>
<p><strong>Leave One Out Variance Estimate.</strong> The leave one out estimate of the variance is defined by
<span class="math display">\[
s_{(i)}^2 = \frac{((n - 1) - (k + 1))^{-1} \sum_{j \ne i} \left( y_j - \mathbf{x}_j^{\prime} \mathbf{b}_{(i)} \right)^2}{(n - 1) - (k + 1)}.
\]</span>
It is used in the definition of the <em>studentized residual</em>, defined in Section <a href="C5VarSelect.html#Sec531">5.3.1</a>. A simple computational expression is given by
<span class="math display" id="eq:eq521">\[\begin{equation}
s_{(i)}^2 = \frac{(n - (k + 1)) s^2 - \frac{e_i^2}{1 - h_{ii}}}{(n - 1) - (k + 1)}.
\tag{5.21}
\end{equation}\]</span>
To see this, first note that from equation <a href="C5VarSelect.html#eq:eq514">(5.14)</a>, we have <span class="math inline">\(\mathbf{He} = \mathbf{H(I - H) \boldsymbol{\varepsilon}} = \mathbf{0}\)</span>, because <span class="math inline">\(\mathbf{H} = \mathbf{HH}\)</span>. In particular, from the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{He} = \mathbf{0}\)</span>, we have <span class="math inline">\(\sum_{j=1}^{n} h_{ij} e_j = 0\)</span>. Now, using equations <a href="C5VarSelect.html#eq:eq517">(5.17)</a> and <a href="C5VarSelect.html#eq:eq520">(5.20)</a>, we have
<span class="math display">\[
\begin{array}{ll}
\sum_{j \ne i} \left( y_j - \mathbf{x}_j^{\prime} \mathbf{b}_{(i)} \right)^2 &amp;= \sum_{j=1}^{n} \left( y_j - \mathbf{x}_j^{\prime} \mathbf{b}_{(i)} \right)^2 - \left( y_i - \mathbf{x}_i^{\prime} \mathbf{b}_{(i)} \right)^2 \\
&amp;= \sum_{j=1}^{n} \left( y_j - \mathbf{x}_j^{\prime} \mathbf{b} + \frac{\mathbf{x}_j^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}} \right) - e_{(i)}^2 \\
&amp;= \sum_{j=1}^{n} \left( e_j + \frac{h_{ij} e_i}{1 - h_{ii}} \right)^2 - \frac{e_i^2}{(1 - h_{ii})^2} \\
&amp;= \sum_{j=1}^{n} e_j^2 + 0 + \frac{e_i^2}{(1 - h_{ii})^2} h_{ii} - \frac{e_i^2}{(1 - h_{ii})^2} \\
&amp;= \sum_{j=1}^{n} e_j^2 - \frac{e_i^2}{1 - h_{ii}} = (n - (k + 1)) s^2 - \frac{e_i^2}{1 - h_{ii}}.
\end{array}
\]</span></p>
<p>This establishes equation <a href="C5VarSelect.html#eq:eq521">(5.21)</a>.</p>
</div>
<div id="Sec5103" class="section level3 hasAnchor" number="5.10.3">
<h3><span class="header-section-number">5.10.3</span> Omitting Variables<a href="C5VarSelect.html#Sec5103" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Notation.</strong> To measure the effect on regression quantities, there are a number of statistics of interest that are based on the notion of omitting an explanatory variable. To this end, the superscript notation <span class="math inline">\((j)\)</span> means to omit the <span class="math inline">\(j\)</span>th variable, where <span class="math inline">\(j=0,1,\ldots,k\)</span>. First, recall that <span class="math inline">\(\mathbf{x}^{j} = (x_{1j}, x_{2j}, \ldots, x_{nj})^{\prime}\)</span> is the column representing the <span class="math inline">\(j\)</span>th variable. Further, define <span class="math inline">\(\mathbf{X}^{(j)}\)</span> to be the <span class="math inline">\(n \times k\)</span> matrix of explanatory variables defined by removing <span class="math inline">\(\mathbf{x}^{j}\)</span> from <span class="math inline">\(\mathbf{X}\)</span>. For example, taking <span class="math inline">\(j=k\)</span>, we often partition <span class="math inline">\(\mathbf{X}\)</span> as <span class="math inline">\(\mathbf{X} = \left( \mathbf{X}^{(k)}: \mathbf{x}^k \right)\)</span>. Employing the results of Section 4.7.2, we will use <span class="math inline">\(\mathbf{X}^{(k)} = \mathbf{X}_1\)</span> and <span class="math inline">\(\mathbf{x}^k = \mathbf{X}_2\)</span>.</p>
<p><strong>Variance Inflation Factor.</strong> We first would like to establish the relationship between the definition of the standard error of <span class="math inline">\(b_j\)</span> given by
<span class="math display">\[
se(b_j) = s \sqrt{(j+1)\text{th  diagonal element  of }(\mathbf{X}^{\prime}\mathbf{X})^{-1}}
\]</span>
and the relationship involving the variance inflation factor,
<span class="math display">\[
se(b_j) = s \frac{\sqrt{VIF_j}}{s_{x_j}\sqrt{n-1}}.
\]</span>
By symmetry of the independent variables, we only need to consider the case where <span class="math inline">\(j=k\)</span>. Thus, we would like to establish
<span class="math display" id="eq:eq522">\[\begin{equation}
(k+1)\text{th diagonal element of }(\mathbf{X}^{\prime}\mathbf{X})^{-1} = \frac{VIF_{k}}{(n-1) s_{x_{k}}^2}.
\tag{5.22}
\end{equation}\]</span>
First consider the reparameterized model in equation (4.22). From equation (4.23), we can express the regression coefficient estimate
<span class="math display">\[
b_{k} = \frac{\mathbf{e}_1^{\prime}\mathbf{y}}{\mathbf{e}_1^{\prime}\mathbf{e}_1}.
\]</span>
From equation (4.23), we have that <span class="math inline">\(\text{Var} \, b_{k} = \sigma^2 (\mathbf{E}_2^{\prime} \mathbf{E}_2)^{-1}\)</span> and thus
<span class="math display" id="eq:eq523">\[\begin{equation}
se(b_{k}) = s (\mathbf{E}_2^{\prime} \mathbf{E}_2)^{-1/2}.
\tag{5.23}
\end{equation}\]</span>
Thus, <span class="math inline">\((\mathbf{E}_2^{\prime} \mathbf{E}_2)^{-1}\)</span> is the <span class="math inline">\((k+1)\)</span>th diagonal element of
<span class="math display">\[
\left(
\begin{bmatrix}
\mathbf{X}_1^{\prime} \\
\mathbf{E}_2^{\prime}
\end{bmatrix}
\begin{bmatrix}
\mathbf{X}_1 &amp; \mathbf{E}_2
\end{bmatrix}
\right)^{-1}
\]</span>
and is also the <span class="math inline">\((k+1)\)</span>th diagonal element of <span class="math inline">\((\mathbf{X}^{\prime} \mathbf{X})^{-1}\)</span>. Alternatively, this can be verified directly using the partitioned matrix inverse in equation (4.19).</p>
<p>Now, suppose that we run a regression using <span class="math inline">\(\mathbf{x}^{k} = \mathbf{X}_2\)</span> as the response vector and <span class="math inline">\(\mathbf{X}^{(k)} = \mathbf{X}_1\)</span> as the matrix of explanatory variables. As noted above equation (4.22), <span class="math inline">\(\mathbf{E}_2\)</span> represents the “residuals” from this regression and thus <span class="math inline">\(\mathbf{E}_2^{\prime} \mathbf{E}_2\)</span> represents the error sum of squares. For this regression, the total sum of squares is
<span class="math display">\[
\sum_{i=1}^{n} (x_{ik} - \bar{x}_{k})^2 = (n-1) s_{x_{k}}^2
\]</span>
and the coefficient of determination is <span class="math inline">\(R_{k}^2\)</span>. Thus,
<span class="math display">\[
\mathbf{E}_2^{\prime} \mathbf{E}_2 = Error ~SS = Total ~SS (1 - R_{k}^2) = \frac{(n-1) s_{x_{k}}^2}{VIF_{k}}.
\]</span>
This establishes the result.</p>
<p><strong>Establishing</strong> <span class="math inline">\(t^2 = F\)</span>. For testing the null hypothesis <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\beta_{k} = 0\)</span>, the material in Section 3.4.1 provides a description of a test based on the <span class="math inline">\(t\)</span>-statistic, <span class="math inline">\(t(b_{k}) = \frac{b_{k}}{se(b_{k})}\)</span>. An alternative test procedure, described in Sections 4.2.2, uses the test statistic
<span class="math display">\[
F-\text{ratio} = \frac{(Error ~SS)_{reduced} - (Error ~SS)_{full}}{p \times (Error~MS)_{full}} = \frac{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right)^2}{s^2 \mathbf{E}_2^{\prime} \mathbf{E}_2}
\]</span>
from equation (4.26). Alternatively, from equations (4.23) and <a href="C5VarSelect.html#eq:eq523">(5.23)</a>, we have
<span class="math display" id="eq:eq524">\[\begin{equation}
t(b_{k}) = \frac{b_{k}}{se(b_{k})} = \frac{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right) / \left( \mathbf{E}_2^{\prime} \mathbf{E}_2 \right)}{s / \sqrt{\mathbf{E}_2^{\prime} \mathbf{E}_2}} = \frac{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right)}{s \sqrt{\mathbf{E}_2^{\prime} \mathbf{E}_2}}.
\tag{5.24}
\end{equation}\]</span></p>
<p>Thus, <span class="math inline">\(t(b_{k})^2 = F\)</span>-ratio.</p>
<p><strong>Partial Correlation Coefficients.</strong> From the full regression model
<span class="math display">\[
\mathbf{y} = \mathbf{X}^{(k)} \boldsymbol{\beta}^{(k)} + \mathbf{x}_{k} \beta_{k} + \boldsymbol{\varepsilon},
\]</span>
consider two separate regressions. A regression using <span class="math inline">\(\mathbf{x}^{k}\)</span> as the response vector and <span class="math inline">\(\mathbf{X}^{(k)}\)</span> as the matrix of explanatory variables yields the residuals <span class="math inline">\(\mathbf{E}_2\)</span>. Similarly, a regression with <span class="math inline">\(\mathbf{y}\)</span> as the response vector and <span class="math inline">\(\mathbf{X}^{(k)}\)</span> as the matrix of explanatory variables yields the residuals
<span class="math display">\[
\mathbf{E}_1 = \mathbf{y} - \mathbf{X}^{(k)} \left( \mathbf{X}^{(k)\prime} \mathbf{X}^{(k)} \right)^{-1} \mathbf{X}^{(k)} \mathbf{y}.
\]</span>
If <span class="math inline">\(x^{0} = (1,1,\ldots,1)^{\prime}\)</span>, then the average of <span class="math inline">\(\mathbf{E}_1\)</span> and <span class="math inline">\(\mathbf{E}_2\)</span> is zero. In this case, the sample correlation between <span class="math inline">\(\mathbf{E}_1\)</span> and <span class="math inline">\(\mathbf{E}_2\)</span> is
<span class="math display">\[
r(\mathbf{E}_1, \mathbf{E}_2) = \frac{\sum_{i=1}^{n} E_{1i} E_{2i}}{\sqrt{\left( \sum_{i=1}^{n} E_{1i}^2 \right) \left( \sum_{i=1}^{n} E_{2i}^2 \right)}} = \frac{\mathbf{E}_1^{\prime} \mathbf{E}_2}{\sqrt{\left( \mathbf{E}_1^{\prime} \mathbf{E}_1 \right) \left( \mathbf{E}_2^{\prime} \mathbf{E}_2 \right)}}.
\]</span>
Because <span class="math inline">\(\mathbf{E}_2\)</span> is a vector of residuals using <span class="math inline">\(\mathbf{X}^{(k)}\)</span> as the matrix of explanatory variables, we have that <span class="math inline">\(\mathbf{E}_2^{\prime} \mathbf{X}^{(k)} = 0\)</span>. Thus, for the numerator, we have
<span class="math display">\[
\mathbf{E}_2^{\prime} \mathbf{E}_1 = \mathbf{E}_2^{\prime} \left( \mathbf{y} - \mathbf{X}^{(k)} \left( \mathbf{X}^{(k)\prime} \mathbf{X}^{(k)} \right)^{-1} \mathbf{X}^{(k)} \mathbf{y} \right) = \mathbf{E}_2^{\prime} \mathbf{y}.
\]</span>
From equations (4.24) and (4.25), we have that
<span class="math display">\[
(n - (k+1)) s^2 = (Error ~SS)_{full} = \mathbf{E}_1^{\prime} \mathbf{E}_1 - \frac{\left( \mathbf{E}_1^{\prime} \mathbf{y} \right)^2}{\mathbf{E}_2^{\prime} \mathbf{E}_2} = \mathbf{E}_1^{\prime} \mathbf{E}_1 - \frac{\left( \mathbf{E}_1^{\prime} \mathbf{E}_2 \right)^2}{\mathbf{E}_2^{\prime} \mathbf{E}_2}.
\]</span>
Thus, from equation <a href="C5VarSelect.html#eq:eq524">(5.24)</a>
<span class="math display">\[
\begin{array}{ll}
\frac{t(b_{k})}{\sqrt{t(b_{k})^2 + n - (k+1)}} &amp;= \frac{\mathbf{E}_2^{\prime} \mathbf{y} / \left(s \sqrt{\mathbf{E}_2^{\prime} \mathbf{E}_2}\right)}{\sqrt{\frac{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right)^2}{s^2 \mathbf{E}_2^{\prime} \mathbf{E}_2} + n - (k+1)}} \\
&amp; = \frac{\mathbf{E}_2^{\prime} \mathbf{y}}{\sqrt{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right)^2 + \mathbf{E}_2^{\prime} \mathbf{E}_2 s^2 \left(n - (k+1) \right)}} \\
&amp; = \frac{\mathbf{E}_2^{\prime} \mathbf{E}_1}{\sqrt{\left( \mathbf{E}_2^{\prime} \mathbf{E}_1 \right)^2 + \mathbf{E}_2^{\prime} \mathbf{E}_2 \left( \mathbf{E}_1^{\prime} \mathbf{E}_1 - \frac{\left( \mathbf{E}_2^{\prime} \mathbf{E}_1 \right)^2}{\mathbf{E}_2^{\prime} \mathbf{E}_2} \right)}} \\
&amp;= \frac{\mathbf{E}_1^{\prime} \mathbf{E}_2}{\sqrt{(\mathbf{E}_1^{\prime} \mathbf{E}_1) (\mathbf{E}_2^{\prime} \mathbf{E}_2)}} = r(\mathbf{E}_1, \mathbf{E}_2).
\end{array}
\]</span>
This establishes the relationship between the partial correlation coefficient and the <span class="math inline">\(t\)</span>-ratio statistic.</p>

<!-- # Chap 1 -->
<!-- # Chap 2 -->
<!-- # Chap 3 -->
<!-- # Chap 4 -->
<!-- # Chap 5 -->
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="C4MLRANOVA.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="interpreting-regression-results.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
