<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Categorical Dependent Variables | Regression Modeling with Actuarial and Financial Applications</title>
  <meta name="description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Categorical Dependent Variables | Regression Modeling with Actuarial and Financial Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Categorical Dependent Variables | Regression Modeling with Actuarial and Financial Applications" />
  
  <meta name="twitter:description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  

<meta name="author" content="Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="C10Panel.html"/>
<link rel="next" href="C12Count.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript"  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleSolution(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}      
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>



<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Regression Modeling With Actuarial and Financial Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#forward"><i class="fa fa-check"></i>Forward</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-is-this-book-for"><i class="fa fa-check"></i>Who Is This Book For?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-is-this-book-about"><i class="fa fa-check"></i>What Is This Book About?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-does-this-book-deliver-its-message"><i class="fa fa-check"></i>How Does This Book Deliver Its Message?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html"><i class="fa fa-check"></i><b>1</b> Regression and the Normal Distribution</a>
<ul>
<li class="chapter" data-level="1.1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec11"><i class="fa fa-check"></i><b>1.1</b> What is Regression Analysis?</a></li>
<li class="chapter" data-level="1.2" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec12"><i class="fa fa-check"></i><b>1.2</b> Fitting Data to a Normal Distribution</a></li>
<li class="chapter" data-level="1.3" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec13"><i class="fa fa-check"></i><b>1.3</b> Power Transforms</a></li>
<li class="chapter" data-level="1.4" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec14"><i class="fa fa-check"></i><b>1.4</b> Sampling and the Role of Normality</a></li>
<li class="chapter" data-level="1.5" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec15"><i class="fa fa-check"></i><b>1.5</b> Regression and Sampling Designs</a></li>
<li class="chapter" data-level="1.6" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec16"><i class="fa fa-check"></i><b>1.6</b> Actuarial Applications of Regression</a></li>
<li class="chapter" data-level="1.7" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec17"><i class="fa fa-check"></i><b>1.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="1.8" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec18"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li>
<li class="chapter" data-level="1.9" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec19"><i class="fa fa-check"></i><b>1.9</b> Technical Supplement - Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="C2BasicLR.html"><a href="C2BasicLR.html"><i class="fa fa-check"></i><b>2</b> Basic Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec21"><i class="fa fa-check"></i><b>2.1</b> Correlations and Least Squares</a></li>
<li class="chapter" data-level="2.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec22"><i class="fa fa-check"></i><b>2.2</b> Basic Linear Regression Model</a></li>
<li class="chapter" data-level="2.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec23"><i class="fa fa-check"></i><b>2.3</b> Is the Model Useful? Some Basic Summary Measures</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec231"><i class="fa fa-check"></i><b>2.3.1</b> Partitioning the Variability</a></li>
<li class="chapter" data-level="2.3.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec232"><i class="fa fa-check"></i><b>2.3.2</b> The Size of a Typical Deviation: <em>s</em></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec24"><i class="fa fa-check"></i><b>2.4</b> Properties of Regression Coefficient Estimators</a></li>
<li class="chapter" data-level="2.5" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec25"><i class="fa fa-check"></i><b>2.5</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec251"><i class="fa fa-check"></i><b>2.5.1</b> Is the Explanatory Variable Important?: The <em>t</em>-Test</a></li>
<li class="chapter" data-level="2.5.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec252"><i class="fa fa-check"></i><b>2.5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="2.5.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec253"><i class="fa fa-check"></i><b>2.5.3</b> Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec26"><i class="fa fa-check"></i><b>2.6</b> Building a Better Model: Residual Analysis</a></li>
<li class="chapter" data-level="2.7" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec27"><i class="fa fa-check"></i><b>2.7</b> Application: Capital Asset Pricing Model</a></li>
<li class="chapter" data-level="2.8" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec28"><i class="fa fa-check"></i><b>2.8</b> Illustrative Regression Computer Output</a></li>
<li class="chapter" data-level="2.9" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec29"><i class="fa fa-check"></i><b>2.9</b> Further Reading and References</a></li>
<li class="chapter" data-level="2.10" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec210"><i class="fa fa-check"></i><b>2.10</b> Exercises</a></li>
<li class="chapter" data-level="2.11" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec211"><i class="fa fa-check"></i><b>2.11</b> Technical Supplement - Elements of Matrix Algebra</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2111"><i class="fa fa-check"></i><b>2.11.1</b> Basic Definitions</a></li>
<li class="chapter" data-level="2.11.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2112"><i class="fa fa-check"></i><b>2.11.2</b> Some Special Matrices</a></li>
<li class="chapter" data-level="2.11.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2113"><i class="fa fa-check"></i><b>2.11.3</b> Basic Operations</a></li>
<li class="chapter" data-level="2.11.4" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2114"><i class="fa fa-check"></i><b>2.11.4</b> Random Matrices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html"><i class="fa fa-check"></i><b>3</b> Multiple Linear Regression - I</a>
<ul>
<li class="chapter" data-level="3.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec31"><i class="fa fa-check"></i><b>3.1</b> Method of Least Squares</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec311"><i class="fa fa-check"></i><b>3.1.1</b> Least Squares Method</a></li>
<li class="chapter" data-level="3.1.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec312"><i class="fa fa-check"></i><b>3.1.2</b> General Case with <em>k</em> Explanatory Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec32"><i class="fa fa-check"></i><b>3.2</b> Linear Regression Model and Properties of Estimators</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec321"><i class="fa fa-check"></i><b>3.2.1</b> Regression Function</a></li>
<li class="chapter" data-level="3.2.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec322"><i class="fa fa-check"></i><b>3.2.2</b> Regression Coefficient Interpretation</a></li>
<li class="chapter" data-level="3.2.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec323"><i class="fa fa-check"></i><b>3.2.3</b> Model Assumptions</a></li>
<li class="chapter" data-level="3.2.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec324"><i class="fa fa-check"></i><b>3.2.4</b> Properties of Regression Coefficient Estimators</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec33"><i class="fa fa-check"></i><b>3.3</b> Estimation and Goodness of Fit</a></li>
<li class="chapter" data-level="3.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec34"><i class="fa fa-check"></i><b>3.4</b> Statistical Inference for a Single Coefficient</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec341"><i class="fa fa-check"></i><b>3.4.1</b> The <em>t</em>-Test</a></li>
<li class="chapter" data-level="3.4.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec342"><i class="fa fa-check"></i><b>3.4.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="3.4.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec343"><i class="fa fa-check"></i><b>3.4.3</b> Added Variable Plots</a></li>
<li class="chapter" data-level="3.4.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec344"><i class="fa fa-check"></i><b>3.4.4</b> Partial Correlation Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec35"><i class="fa fa-check"></i><b>3.5</b> Some Special Explanatory Variables</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec351"><i class="fa fa-check"></i><b>3.5.1</b> Binary Variables</a></li>
<li class="chapter" data-level="3.5.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec352"><i class="fa fa-check"></i><b>3.5.2</b> Transforming Explanatory Variables</a></li>
<li class="chapter" data-level="3.5.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec353"><i class="fa fa-check"></i><b>3.5.3</b> Interaction Terms</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec36"><i class="fa fa-check"></i><b>3.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="3.7" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec37"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression - II</a>
<ul>
<li class="chapter" data-level="4.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec41"><i class="fa fa-check"></i><b>4.1</b> The Role of Binary Variables</a></li>
<li class="chapter" data-level="4.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec42"><i class="fa fa-check"></i><b>4.2</b> Statistical Inference for Several Coefficients</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec421"><i class="fa fa-check"></i><b>4.2.1</b> Sets of Regression Coefficients</a></li>
<li class="chapter" data-level="4.2.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec422"><i class="fa fa-check"></i><b>4.2.2</b> The General Linear Hypothesis</a></li>
<li class="chapter" data-level="4.2.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec423"><i class="fa fa-check"></i><b>4.2.3</b> Estimating and Predicting Several Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec43"><i class="fa fa-check"></i><b>4.3</b> One Factor ANOVA Model</a></li>
<li class="chapter" data-level="4.4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec44"><i class="fa fa-check"></i><b>4.4</b> Combining Categorical and Continuous Explanatory Variables</a></li>
<li class="chapter" data-level="4.5" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec45"><i class="fa fa-check"></i><b>4.5</b> Further Reading and References</a></li>
<li class="chapter" data-level="4.6" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec46"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec47"><i class="fa fa-check"></i><b>4.7</b> Technical Supplement - Matrix Expressions</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec471"><i class="fa fa-check"></i><b>4.7.1</b> Expressing Models with Categorical Variables in Matrix Form</a></li>
<li class="chapter" data-level="4.7.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec472"><i class="fa fa-check"></i><b>4.7.2</b> Calculating Least Squares Recursively</a></li>
<li class="chapter" data-level="4.7.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec473"><i class="fa fa-check"></i><b>4.7.3</b> General Linear Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="C5VarSelect.html"><a href="C5VarSelect.html"><i class="fa fa-check"></i><b>5</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="5.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec51"><i class="fa fa-check"></i><b>5.1</b> An Iterative Approach to Data Analysis and Modeling</a></li>
<li class="chapter" data-level="5.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec52"><i class="fa fa-check"></i><b>5.2</b> Automatic Variable Selection Procedures</a></li>
<li class="chapter" data-level="5.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec53"><i class="fa fa-check"></i><b>5.3</b> Residual Analysis</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec531"><i class="fa fa-check"></i><b>5.3.1</b> Residuals</a></li>
<li class="chapter" data-level="5.3.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec532"><i class="fa fa-check"></i><b>5.3.2</b> Using Residuals to Identify Outliers</a></li>
<li class="chapter" data-level="5.3.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec533"><i class="fa fa-check"></i><b>5.3.3</b> Using Residuals to Select Explanatory Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec54"><i class="fa fa-check"></i><b>5.4</b> Influential Points</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec541"><i class="fa fa-check"></i><b>5.4.1</b> Leverage</a></li>
<li class="chapter" data-level="5.4.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec542"><i class="fa fa-check"></i><b>5.4.2</b> Cook’s Distance</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec55"><i class="fa fa-check"></i><b>5.5</b> Collinearity</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec551"><i class="fa fa-check"></i><b>5.5.1</b> What is Collinearity?</a></li>
<li class="chapter" data-level="5.5.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec552"><i class="fa fa-check"></i><b>5.5.2</b> Variance Inflation Factors</a></li>
<li class="chapter" data-level="5.5.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec553"><i class="fa fa-check"></i><b>5.5.3</b> Collinearity and Leverage</a></li>
<li class="chapter" data-level="5.5.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec554"><i class="fa fa-check"></i><b>5.5.4</b> Suppressor Variables</a></li>
<li class="chapter" data-level="5.5.5" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec555"><i class="fa fa-check"></i><b>5.5.5</b> Orthogonal Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec56"><i class="fa fa-check"></i><b>5.6</b> Selection Criteria</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec561"><i class="fa fa-check"></i><b>5.6.1</b> Goodness of Fit</a></li>
<li class="chapter" data-level="5.6.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec562"><i class="fa fa-check"></i><b>5.6.2</b> Model Validation</a></li>
<li class="chapter" data-level="5.6.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec563"><i class="fa fa-check"></i><b>5.6.3</b> Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec57"><i class="fa fa-check"></i><b>5.7</b> Heteroscedasticity</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec571"><i class="fa fa-check"></i><b>5.7.1</b> Detecting Heteroscedasticity</a></li>
<li class="chapter" data-level="5.7.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec572"><i class="fa fa-check"></i><b>5.7.2</b> Heteroscedasticity-Consistent Standard Errors</a></li>
<li class="chapter" data-level="5.7.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec573"><i class="fa fa-check"></i><b>5.7.3</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="5.7.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec574"><i class="fa fa-check"></i><b>5.7.4</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec58"><i class="fa fa-check"></i><b>5.8</b> Further Reading and References</a></li>
<li class="chapter" data-level="5.9" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec59"><i class="fa fa-check"></i><b>5.9</b> Exercises</a></li>
<li class="chapter" data-level="5.10" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec510"><i class="fa fa-check"></i><b>5.10</b> Technical Supplements for Chapter 5</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5101"><i class="fa fa-check"></i><b>5.10.1</b> Projection Matrix</a></li>
<li class="chapter" data-level="5.10.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5102"><i class="fa fa-check"></i><b>5.10.2</b> Leave One Out Statistics</a></li>
<li class="chapter" data-level="5.10.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5103"><i class="fa fa-check"></i><b>5.10.3</b> Omitting Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html"><i class="fa fa-check"></i><b>6</b> Interpreting Regression Results</a>
<ul>
<li class="chapter" data-level="6.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec61"><i class="fa fa-check"></i><b>6.1</b> What the Modeling Process Tells Us</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec611"><i class="fa fa-check"></i><b>6.1.1</b> Interpreting Individual Effects</a></li>
<li class="chapter" data-level="6.1.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec612"><i class="fa fa-check"></i><b>6.1.2</b> Other Interpretations</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec62"><i class="fa fa-check"></i><b>6.2</b> The Importance of Variable Selection</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec621"><i class="fa fa-check"></i><b>6.2.1</b> Overfitting the Model</a></li>
<li class="chapter" data-level="6.2.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec622"><i class="fa fa-check"></i><b>6.2.2</b> Underfitting the Model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec63"><i class="fa fa-check"></i><b>6.3</b> The Importance of Data Collection</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec631"><i class="fa fa-check"></i><b>6.3.1</b> Sampling Frame Error and Adverse Selection</a></li>
<li class="chapter" data-level="6.3.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec632"><i class="fa fa-check"></i><b>6.3.2</b> Limited Sampling Regions</a></li>
<li class="chapter" data-level="6.3.3" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec633"><i class="fa fa-check"></i><b>6.3.3</b> Limited Dependent Variables, Censoring and Truncation</a></li>
<li class="chapter" data-level="6.3.4" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec634"><i class="fa fa-check"></i><b>6.3.4</b> Omitted and Endogenous Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec635"><i class="fa fa-check"></i><b>6.3.5</b> Missing Data</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec64"><i class="fa fa-check"></i><b>6.4</b> Missing Data Models</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec641"><i class="fa fa-check"></i><b>6.4.1</b> Missing at Random</a></li>
<li class="chapter" data-level="6.4.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec642"><i class="fa fa-check"></i><b>6.4.2</b> Non-Ignorable Missing Data</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec65"><i class="fa fa-check"></i><b>6.5</b> Application: Risk Managers’ Cost Effectiveness</a></li>
<li class="chapter" data-level="6.6" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec66"><i class="fa fa-check"></i><b>6.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="6.7" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec67"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
<li class="chapter" data-level="6.8" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec68"><i class="fa fa-check"></i><b>6.8</b> Technical Supplements for Chapter 6</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec681"><i class="fa fa-check"></i><b>6.8.1</b> Effects of Model Misspecification</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="C7Trends.html"><a href="C7Trends.html"><i class="fa fa-check"></i><b>7</b> Modeling Trends</a>
<ul>
<li class="chapter" data-level="7.1" data-path="C7Trends.html"><a href="C7Trends.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#time-series-and-stochastic-processes"><i class="fa fa-check"></i>Time Series and Stochastic Processes</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#time-series-versus-causal-models"><i class="fa fa-check"></i>Time Series versus Causal Models</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="C7Trends.html"><a href="C7Trends.html#S7:Trends"><i class="fa fa-check"></i><b>7.2</b> Fitting Trends in Time</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#understanding-patterns-over-time"><i class="fa fa-check"></i>Understanding Patterns over Time</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#fitting-trends-in-time"><i class="fa fa-check"></i>Fitting Trends in Time</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#fitting-seasonal-trends"><i class="fa fa-check"></i>Fitting Seasonal Trends</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#reliability-of-time-series-forecasts"><i class="fa fa-check"></i>Reliability of Time Series Forecasts</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="C7Trends.html"><a href="C7Trends.html#S7:RandomWalk"><i class="fa fa-check"></i><b>7.3</b> Stationarity and Random Walk Models</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#white-noise"><i class="fa fa-check"></i>White Noise</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#random-walk"><i class="fa fa-check"></i>Random Walk</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="C7Trends.html"><a href="C7Trends.html#inference-using-random-walk-models"><i class="fa fa-check"></i><b>7.4</b> Inference using Random Walk Models</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#model-properties"><i class="fa fa-check"></i>Model Properties</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#forecasting"><i class="fa fa-check"></i>Forecasting</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#identifying-stationarity"><i class="fa fa-check"></i>Identifying Stationarity</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#identifying-random-walks"><i class="fa fa-check"></i>Identifying Random Walks</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#random-walk-versus-linear-trend-in-time-models"><i class="fa fa-check"></i>Random Walk versus Linear Trend in Time Models</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="C7Trends.html"><a href="C7Trends.html#filtering-to-achieve-stationarity"><i class="fa fa-check"></i><b>7.5</b> Filtering to Achieve Stationarity</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#transformations"><i class="fa fa-check"></i>Transformations</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="C7Trends.html"><a href="C7Trends.html#forecast-evaluation"><i class="fa fa-check"></i><b>7.6</b> Forecast Evaluation</a></li>
<li class="chapter" data-level="7.7" data-path="C7Trends.html"><a href="C7Trends.html#further-reading-and-references"><i class="fa fa-check"></i><b>7.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="7.8" data-path="C7Trends.html"><a href="C7Trends.html#exercises"><i class="fa fa-check"></i><b>7.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="C8AR.html"><a href="C8AR.html"><i class="fa fa-check"></i><b>8</b> Autocorrelations and Autoregressive Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="C8AR.html"><a href="C8AR.html#S8:Autocorrs"><i class="fa fa-check"></i><b>8.1</b> Autocorrelations</a>
<ul>
<li class="chapter" data-level="" data-path="C8AR.html"><a href="C8AR.html#application-inflation-bond-returns"><i class="fa fa-check"></i>Application: Inflation Bond Returns</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="C8AR.html"><a href="C8AR.html#autoregressive-models-of-order-one"><i class="fa fa-check"></i><b>8.2</b> Autoregressive Models of Order One</a></li>
<li class="chapter" data-level="8.3" data-path="C8AR.html"><a href="C8AR.html#S8:Estimation"><i class="fa fa-check"></i><b>8.3</b> Estimation and Diagnostic Checking</a></li>
<li class="chapter" data-level="8.4" data-path="C8AR.html"><a href="C8AR.html#S8:AR1Smooth"><i class="fa fa-check"></i><b>8.4</b> Smoothing and Prediction</a></li>
<li class="chapter" data-level="8.5" data-path="C8AR.html"><a href="C8AR.html#S8:BoxJenkins"><i class="fa fa-check"></i><b>8.5</b> Box-Jenkins Modeling and Forecasting</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="C8AR.html"><a href="C8AR.html#models"><i class="fa fa-check"></i><b>8.5.1</b> Models</a></li>
<li class="chapter" data-level="8.5.2" data-path="C8AR.html"><a href="C8AR.html#forecasting-1"><i class="fa fa-check"></i><b>8.5.2</b> Forecasting</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="C8AR.html"><a href="C8AR.html#application-hong-kong-exchange-rates"><i class="fa fa-check"></i><b>8.6</b> Application: Hong Kong Exchange Rates</a></li>
<li class="chapter" data-level="8.7" data-path="C8AR.html"><a href="C8AR.html#further-reading-and-references-1"><i class="fa fa-check"></i><b>8.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="8.8" data-path="C8AR.html"><a href="C8AR.html#exercises-1"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="C9Forecast.html"><a href="C9Forecast.html"><i class="fa fa-check"></i><b>9</b> Forecasting and Time Series Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="C9Forecast.html"><a href="C9Forecast.html#smoothing-with-moving-averages"><i class="fa fa-check"></i><b>9.1</b> Smoothing with Moving Averages</a></li>
<li class="chapter" data-level="9.2" data-path="C9Forecast.html"><a href="C9Forecast.html#S9:ExponSmooth"><i class="fa fa-check"></i><b>9.2</b> Exponential Smoothing</a></li>
<li class="chapter" data-level="9.3" data-path="C9Forecast.html"><a href="C9Forecast.html#S9:SeasonalTSModels"><i class="fa fa-check"></i><b>9.3</b> Seasonal Time Series Models</a></li>
<li class="chapter" data-level="9.4" data-path="C9Forecast.html"><a href="C9Forecast.html#unit-root-tests"><i class="fa fa-check"></i><b>9.4</b> Unit Root Tests</a></li>
<li class="chapter" data-level="9.5" data-path="C9Forecast.html"><a href="C9Forecast.html#archgarch-models"><i class="fa fa-check"></i><b>9.5</b> ARCH/GARCH Models</a></li>
<li class="chapter" data-level="9.6" data-path="C9Forecast.html"><a href="C9Forecast.html#further-reading-and-references-2"><i class="fa fa-check"></i><b>9.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="C10Panel.html"><a href="C10Panel.html"><i class="fa fa-check"></i><b>10</b> Longitudinal and Panel Data Models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="C10Panel.html"><a href="C10Panel.html#S10:Intro"><i class="fa fa-check"></i><b>10.1</b> What are Longitudinal and Panel Data?</a></li>
<li class="chapter" data-level="10.2" data-path="C10Panel.html"><a href="C10Panel.html#S10:Visual"><i class="fa fa-check"></i><b>10.2</b> Visualizing Longitudinal and Panel Data</a></li>
<li class="chapter" data-level="10.3" data-path="C10Panel.html"><a href="C10Panel.html#S10:FEModels"><i class="fa fa-check"></i><b>10.3</b> Basic Fixed Effects Models</a></li>
<li class="chapter" data-level="10.4" data-path="C10Panel.html"><a href="C10Panel.html#S10:FEModels2"><i class="fa fa-check"></i><b>10.4</b> Extended Fixed Effects Models</a></li>
<li class="chapter" data-level="10.5" data-path="C10Panel.html"><a href="C10Panel.html#S10:REModels"><i class="fa fa-check"></i><b>10.5</b> Random Effects Models</a></li>
<li class="chapter" data-level="10.6" data-path="C10Panel.html"><a href="C10Panel.html#S10:References"><i class="fa fa-check"></i><b>10.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="C11Binary.html"><a href="C11Binary.html"><i class="fa fa-check"></i><b>11</b> Categorical Dependent Variables</a>
<ul>
<li class="chapter" data-level="11.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec111"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables</a></li>
<li class="chapter" data-level="11.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec112"><i class="fa fa-check"></i><b>11.2</b> Logistic and Probit Regression Models</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1121"><i class="fa fa-check"></i><b>11.2.1</b> Using Nonlinear Functions of Explanatory Variables</a></li>
<li class="chapter" data-level="11.2.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1122"><i class="fa fa-check"></i><b>11.2.2</b> Threshold Interpretation</a></li>
<li class="chapter" data-level="11.2.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1123"><i class="fa fa-check"></i><b>11.2.3</b> Random Utility Interpretation</a></li>
<li class="chapter" data-level="11.2.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1124"><i class="fa fa-check"></i><b>11.2.4</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec113"><i class="fa fa-check"></i><b>11.3</b> Inference for Logistic and Probit Regression Models</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="C11Binary.html"><a href="C11Binary.html#parameter-estimation"><i class="fa fa-check"></i><b>11.3.1</b> Parameter Estimation</a></li>
<li class="chapter" data-level="11.3.2" data-path="C11Binary.html"><a href="C11Binary.html#additional-inference"><i class="fa fa-check"></i><b>11.3.2</b> Additional Inference</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec114"><i class="fa fa-check"></i><b>11.4</b> Application: Medical Expenditures</a></li>
<li class="chapter" data-level="11.5" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec115"><i class="fa fa-check"></i><b>11.5</b> Nominal Dependent Variables</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1151"><i class="fa fa-check"></i><b>11.5.1</b> Generalized Logit</a></li>
<li class="chapter" data-level="11.5.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1152"><i class="fa fa-check"></i><b>11.5.2</b> Multinomial Logit</a></li>
<li class="chapter" data-level="11.5.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1153"><i class="fa fa-check"></i><b>11.5.3</b> Nested Logit</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec116"><i class="fa fa-check"></i><b>11.6</b> Ordinal Dependent Variables</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="C11Binary.html"><a href="C11Binary.html#cumulative-logit"><i class="fa fa-check"></i><b>11.6.1</b> Cumulative Logit</a></li>
<li class="chapter" data-level="11.6.2" data-path="C11Binary.html"><a href="C11Binary.html#cumulative-probit"><i class="fa fa-check"></i><b>11.6.2</b> Cumulative Probit</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec117"><i class="fa fa-check"></i><b>11.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="11.8" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec118"><i class="fa fa-check"></i><b>11.8</b> Exercises</a></li>
<li class="chapter" data-level="11.9" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec119"><i class="fa fa-check"></i><b>11.9</b> Technical Supplements - Likelihood-Based Inference</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1191"><i class="fa fa-check"></i><b>11.9.1</b> Properties of Likelihood Functions</a></li>
<li class="chapter" data-level="11.9.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1192"><i class="fa fa-check"></i><b>11.9.2</b> Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="11.9.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1193"><i class="fa fa-check"></i><b>11.9.3</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="11.9.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1194"><i class="fa fa-check"></i><b>11.9.4</b> Information Criteria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="C12Count.html"><a href="C12Count.html"><i class="fa fa-check"></i><b>12</b> Count Dependent Variables</a>
<ul>
<li class="chapter" data-level="12.1" data-path="C12Count.html"><a href="C12Count.html#S:Sec121"><i class="fa fa-check"></i><b>12.1</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="C12Count.html"><a href="C12Count.html#S:Sec1211"><i class="fa fa-check"></i><b>12.1.1</b> Poisson Distribution</a></li>
<li class="chapter" data-level="12.1.2" data-path="C12Count.html"><a href="C12Count.html#S:Sec1212"><i class="fa fa-check"></i><b>12.1.2</b> Regression Model</a></li>
<li class="chapter" data-level="12.1.3" data-path="C12Count.html"><a href="C12Count.html#S:Sec1213"><i class="fa fa-check"></i><b>12.1.3</b> Estimation</a></li>
<li class="chapter" data-level="12.1.4" data-path="C12Count.html"><a href="C12Count.html#S:Sec1214"><i class="fa fa-check"></i><b>12.1.4</b> Additional Inference</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="C12Count.html"><a href="C12Count.html#S:Sec122"><i class="fa fa-check"></i><b>12.2</b> Application: Singapore Automobile Insurance</a></li>
<li class="chapter" data-level="12.3" data-path="C12Count.html"><a href="C12Count.html#S:Sec123"><i class="fa fa-check"></i><b>12.3</b> Overdispersion and Negative Binomial Models</a></li>
<li class="chapter" data-level="12.4" data-path="C12Count.html"><a href="C12Count.html#S:Sec124"><i class="fa fa-check"></i><b>12.4</b> Other Count Models</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="C12Count.html"><a href="C12Count.html#zero-inflated-models"><i class="fa fa-check"></i><b>12.4.1</b> Zero-Inflated Models</a></li>
<li class="chapter" data-level="12.4.2" data-path="C12Count.html"><a href="C12Count.html#hurdle-models"><i class="fa fa-check"></i><b>12.4.2</b> Hurdle Models</a></li>
<li class="chapter" data-level="12.4.3" data-path="C12Count.html"><a href="C12Count.html#heterogeneity-models"><i class="fa fa-check"></i><b>12.4.3</b> Heterogeneity Models</a></li>
<li class="chapter" data-level="12.4.4" data-path="C12Count.html"><a href="C12Count.html#latent-class-models"><i class="fa fa-check"></i><b>12.4.4</b> Latent Class Models</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="C12Count.html"><a href="C12Count.html#S:Sec125"><i class="fa fa-check"></i><b>12.5</b> Further Reading and References</a></li>
<li class="chapter" data-level="12.6" data-path="C12Count.html"><a href="C12Count.html#S:Sec126"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="C13GLM.html"><a href="C13GLM.html"><i class="fa fa-check"></i><b>13</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec131"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec132"><i class="fa fa-check"></i><b>13.2</b> GLM Model</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1321"><i class="fa fa-check"></i><b>13.2.1</b> Linear Exponential Family of Distributions</a></li>
<li class="chapter" data-level="13.2.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1322"><i class="fa fa-check"></i><b>13.2.2</b> Link Functions</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec133"><i class="fa fa-check"></i><b>13.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1331"><i class="fa fa-check"></i><b>13.3.1</b> Maximum Likelihood Estimation for Canonical Links</a></li>
<li class="chapter" data-level="13.3.2" data-path="C13GLM.html"><a href="C13GLM.html#overdispersion"><i class="fa fa-check"></i><b>13.3.2</b> Overdispersion</a></li>
<li class="chapter" data-level="13.3.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1333"><i class="fa fa-check"></i><b>13.3.3</b> Goodness of Fit Statistics</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec134"><i class="fa fa-check"></i><b>13.4</b> Application: Medical Expenditures</a></li>
<li class="chapter" data-level="13.5" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec135"><i class="fa fa-check"></i><b>13.5</b> Residuals</a></li>
<li class="chapter" data-level="13.6" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec136"><i class="fa fa-check"></i><b>13.6</b> Tweedie Distribution</a></li>
<li class="chapter" data-level="13.7" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec137"><i class="fa fa-check"></i><b>13.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="13.8" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec138"><i class="fa fa-check"></i><b>13.8</b> Exercises</a></li>
<li class="chapter" data-level="13.9" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec139"><i class="fa fa-check"></i><b>13.9</b> Technical Supplements - Exponential Family</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1391"><i class="fa fa-check"></i><b>13.9.1</b> Linear Exponential Family of Distributions</a></li>
<li class="chapter" data-level="13.9.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1392"><i class="fa fa-check"></i><b>13.9.2</b> Moments</a></li>
<li class="chapter" data-level="13.9.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1393"><i class="fa fa-check"></i><b>13.9.3</b> Maximum Likelihood Estimation for General Links</a></li>
<li class="chapter" data-level="13.9.4" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1394"><i class="fa fa-check"></i><b>13.9.4</b> Iterated Reweighted Least Squares</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="C14Survival.html"><a href="C14Survival.html"><i class="fa fa-check"></i><b>14</b> Survival Models</a>
<ul>
<li class="chapter" data-level="14.1" data-path="C14Survival.html"><a href="C14Survival.html#introduction-2"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec142"><i class="fa fa-check"></i><b>14.2</b> Censoring and Truncation</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="C14Survival.html"><a href="C14Survival.html#definitions-and-examples"><i class="fa fa-check"></i><b>14.2.1</b> Definitions and Examples</a></li>
<li class="chapter" data-level="14.2.2" data-path="C14Survival.html"><a href="C14Survival.html#likelihood-inference"><i class="fa fa-check"></i><b>14.2.2</b> Likelihood Inference</a></li>
<li class="chapter" data-level="14.2.3" data-path="C14Survival.html"><a href="C14Survival.html#product-limit-estimator"><i class="fa fa-check"></i><b>14.2.3</b> Product-Limit Estimator</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec143"><i class="fa fa-check"></i><b>14.3</b> Accelerated Failure Time Model</a></li>
<li class="chapter" data-level="14.4" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec144"><i class="fa fa-check"></i><b>14.4</b> Proportional Hazards Model</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec1441"><i class="fa fa-check"></i><b>14.4.1</b> Proportional Hazards</a></li>
<li class="chapter" data-level="14.4.2" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec1442"><i class="fa fa-check"></i><b>14.4.2</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec145"><i class="fa fa-check"></i><b>14.5</b> Recurrent Events</a></li>
<li class="chapter" data-level="14.6" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec146"><i class="fa fa-check"></i><b>14.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="C15Misc.html"><a href="C15Misc.html"><i class="fa fa-check"></i><b>15</b> Miscellaneous Regression Topics</a>
<ul>
<li class="chapter" data-level="15.1" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec151"><i class="fa fa-check"></i><b>15.1</b> Mixed Linear Models</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="C15Misc.html"><a href="C15Misc.html#weighted-least-squares-2"><i class="fa fa-check"></i><b>15.1.1</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="15.1.2" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec1512"><i class="fa fa-check"></i><b>15.1.2</b> Variance Components Estimation</a></li>
<li class="chapter" data-level="15.1.3" data-path="C15Misc.html"><a href="C15Misc.html#best-linear-unbiased-prediction"><i class="fa fa-check"></i><b>15.1.3</b> Best Linear Unbiased Prediction</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="C15Misc.html"><a href="C15Misc.html#bayesian-regression"><i class="fa fa-check"></i><b>15.2</b> Bayesian Regression</a></li>
<li class="chapter" data-level="15.3" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec153"><i class="fa fa-check"></i><b>15.3</b> Density Estimation and Scatterplot Smoothing}</a></li>
<li class="chapter" data-level="15.4" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec154"><i class="fa fa-check"></i><b>15.4</b> Generalized Additive Models</a></li>
<li class="chapter" data-level="15.5" data-path="C15Misc.html"><a href="C15Misc.html#bootstrapping"><i class="fa fa-check"></i><b>15.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="15.6" data-path="C15Misc.html"><a href="C15Misc.html#further-reading-and-references-3"><i class="fa fa-check"></i><b>15.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="C16FreqSev.html"><a href="C16FreqSev.html"><i class="fa fa-check"></i><b>16</b> Frequency-Severity Models</a>
<ul>
<li class="chapter" data-level="16.1" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec161"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec162"><i class="fa fa-check"></i><b>16.2</b> Tobit Model</a></li>
<li class="chapter" data-level="16.3" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec163"><i class="fa fa-check"></i><b>16.3</b> Application: Medical Expenditures</a></li>
<li class="chapter" data-level="16.4" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec164"><i class="fa fa-check"></i><b>16.4</b> Two-Part Model</a></li>
<li class="chapter" data-level="16.5" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec165"><i class="fa fa-check"></i><b>16.5</b> Aggregate Loss Model</a></li>
<li class="chapter" data-level="16.6" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec166"><i class="fa fa-check"></i><b>16.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="16.7" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec167"><i class="fa fa-check"></i><b>16.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="C17Fat.html"><a href="C17Fat.html"><i class="fa fa-check"></i><b>17</b> Fat-Tailed Regression Models</a>
<ul>
<li class="chapter" data-level="17.1" data-path="C17Fat.html"><a href="C17Fat.html#introduction-3"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec172"><i class="fa fa-check"></i><b>17.2</b> Transformations</a></li>
<li class="chapter" data-level="17.3" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec173"><i class="fa fa-check"></i><b>17.3</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec1731"><i class="fa fa-check"></i><b>17.3.1</b> What is “Fat-Tailed?”</a></li>
<li class="chapter" data-level="17.3.2" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec1732"><i class="fa fa-check"></i><b>17.3.2</b> Application: Wisconsin Nursing Homes</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec174"><i class="fa fa-check"></i><b>17.4</b> Generalized Distributions</a>
<ul>
<li class="chapter" data-level="" data-path="C17Fat.html"><a href="C17Fat.html#applicationwisconsin-nursing-homes"><i class="fa fa-check"></i>Application:Wisconsin Nursing Homes</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec175"><i class="fa fa-check"></i><b>17.5</b> Quantile Regression</a></li>
<li class="chapter" data-level="17.6" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec176"><i class="fa fa-check"></i><b>17.6</b> Extreme Value Models</a></li>
<li class="chapter" data-level="17.7" data-path="C17Fat.html"><a href="C17Fat.html#further-reading-and-references-4"><i class="fa fa-check"></i><b>17.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="17.8" data-path="C17Fat.html"><a href="C17Fat.html#exercises-2"><i class="fa fa-check"></i><b>17.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="C18Cred.html"><a href="C18Cred.html"><i class="fa fa-check"></i><b>18</b> Credibility and Bonus-Malus</a>
<ul>
<li class="chapter" data-level="18.1" data-path="C18Cred.html"><a href="C18Cred.html#risk-classification-and-experience-rating"><i class="fa fa-check"></i><b>18.1</b> Risk Classification and Experience Rating</a></li>
<li class="chapter" data-level="18.2" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec182"><i class="fa fa-check"></i><b>18.2</b> Credibility</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec1821"><i class="fa fa-check"></i><b>18.2.1</b> Limited Fluctuation Credibility</a></li>
<li class="chapter" data-level="18.2.2" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec1822"><i class="fa fa-check"></i><b>18.2.2</b> Greatest Accuracy Credibility</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec183"><i class="fa fa-check"></i><b>18.3</b> Credibility and Regression</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="C18Cred.html"><a href="C18Cred.html#one-way-random-effects-model"><i class="fa fa-check"></i><b>18.3.1</b> One-Way Random Effects Model</a></li>
<li class="chapter" data-level="18.3.2" data-path="C18Cred.html"><a href="C18Cred.html#longitudinal-models"><i class="fa fa-check"></i><b>18.3.2</b> Longitudinal Models</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec184"><i class="fa fa-check"></i><b>18.4</b> Bonus-Malus</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="C19Triangles.html"><a href="C19Triangles.html"><i class="fa fa-check"></i><b>19</b> Claims Triangles</a>
<ul>
<li class="chapter" data-level="19.1" data-path="C19Triangles.html"><a href="C19Triangles.html#introduction-4"><i class="fa fa-check"></i><b>19.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1911"><i class="fa fa-check"></i><b>19.1.1</b> Claims Evolution</a></li>
<li class="chapter" data-level="19.1.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1912"><i class="fa fa-check"></i><b>19.1.2</b> Claims Triangles</a></li>
<li class="chapter" data-level="19.1.3" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1913"><i class="fa fa-check"></i><b>19.1.3</b> Chain Ladder Method</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec192"><i class="fa fa-check"></i><b>19.2</b> Regression Using Functions of Time as Explanatory Variables</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1921"><i class="fa fa-check"></i><b>19.2.1</b> Lognormal Model</a></li>
<li class="chapter" data-level="19.2.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1922"><i class="fa fa-check"></i><b>19.2.2</b> Hoerl Curve</a></li>
<li class="chapter" data-level="19.2.3" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1923"><i class="fa fa-check"></i><b>19.2.3</b> Poisson Models</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec193"><i class="fa fa-check"></i><b>19.3</b> Using Past Developments</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1931"><i class="fa fa-check"></i><b>19.3.1</b> Mack Model</a></li>
<li class="chapter" data-level="19.3.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1932"><i class="fa fa-check"></i><b>19.3.2</b> Distributional Models</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="C19Triangles.html"><a href="C19Triangles.html#further-reading-and-references-5"><i class="fa fa-check"></i><b>19.4</b> Further Reading and References</a></li>
<li class="chapter" data-level="19.5" data-path="C19Triangles.html"><a href="C19Triangles.html#exercises-3"><i class="fa fa-check"></i><b>19.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="C20Report.html"><a href="C20Report.html"><i class="fa fa-check"></i><b>20</b> Report Writing: Communicating Data Analysis Results</a>
<ul>
<li class="chapter" data-level="20.1" data-path="C20Report.html"><a href="C20Report.html#S20:Overview"><i class="fa fa-check"></i><b>20.1</b> Overview</a></li>
<li class="chapter" data-level="20.2" data-path="C20Report.html"><a href="C20Report.html#S20:Methods"><i class="fa fa-check"></i><b>20.2</b> Methods for Communicating Data</a>
<ul>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#within-text-data"><i class="fa fa-check"></i>Within Text Data</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#graphs"><i class="fa fa-check"></i>Graphs</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="C20Report.html"><a href="C20Report.html#S20:Organize"><i class="fa fa-check"></i><b>20.3</b> How to Organize</a>
<ul>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#title-and-abstract"><i class="fa fa-check"></i>Title and Abstract</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#introduction-5"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#model-selection-and-interpretation"><i class="fa fa-check"></i>Model Selection and Interpretation</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#references-and-appendix"><i class="fa fa-check"></i>References and Appendix</a></li>
</ul></li>
<li class="chapter" data-level="20.4" data-path="C20Report.html"><a href="C20Report.html#further-suggestions-for-report-writing"><i class="fa fa-check"></i><b>20.4</b> Further Suggestions for Report Writing</a></li>
<li class="chapter" data-level="20.5" data-path="C20Report.html"><a href="C20Report.html#case-study-swedish-automobile-claims"><i class="fa fa-check"></i><b>20.5</b> Case Study: Swedish Automobile Claims</a></li>
<li class="chapter" data-level="20.6" data-path="C20Report.html"><a href="C20Report.html#further-reading-and-references-6"><i class="fa fa-check"></i><b>20.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="20.7" data-path="C20Report.html"><a href="C20Report.html#exercises-4"><i class="fa fa-check"></i><b>20.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="C21Design.html"><a href="C21Design.html"><i class="fa fa-check"></i><b>21</b> Designing Effective Graphs</a>
<ul>
<li class="chapter" data-level="21.1" data-path="C21Design.html"><a href="C21Design.html#S21:Intro"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="C21Design.html"><a href="C21Design.html#S21:GDesign"><i class="fa fa-check"></i><b>21.2</b> Graphic Design Choices Make a Difference</a></li>
<li class="chapter" data-level="21.3" data-path="C21Design.html"><a href="C21Design.html#S21:DesignGuide"><i class="fa fa-check"></i><b>21.3</b> Design Guidelines</a>
<ul>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-one-avoid-chartjunk"><i class="fa fa-check"></i>Guideline One: Avoid Chartjunk</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-two-use-small-multiples-to-promote-comparisons-and-assess-change"><i class="fa fa-check"></i>Guideline Two: Use Small Multiples to Promote Comparisons and Assess Change</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-three-use-complex-graphs-to-portray-complex-patterns"><i class="fa fa-check"></i>Guideline Three: Use Complex Graphs to Portray Complex Patterns</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-four-relate-graph-size-to-information-content"><i class="fa fa-check"></i>Guideline Four: Relate Graph Size to Information Content</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-five-use-graphical-forms-that-promote-comparisons"><i class="fa fa-check"></i>Guideline Five: Use Graphical Forms That Promote Comparisons</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-six-integrate-graphs-and-text"><i class="fa fa-check"></i>Guideline Six: Integrate Graphs and Text</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-seven-demonstrate-an-important-message"><i class="fa fa-check"></i>Guideline Seven: Demonstrate an Important Message</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-eight-know-your-audience"><i class="fa fa-check"></i>Guideline Eight: Know Your Audience</a></li>
</ul></li>
<li class="chapter" data-level="21.4" data-path="C21Design.html"><a href="C21Design.html#S21:EmpiricalFoundations"><i class="fa fa-check"></i><b>21.4</b> Empirical Foundations For Guidelines</a>
<ul>
<li class="chapter" data-level="21.4.1" data-path="C21Design.html"><a href="C21Design.html#graphs-as-units-of-study"><i class="fa fa-check"></i><b>21.4.1</b> Graphs as Units of Study</a></li>
</ul></li>
<li class="chapter" data-level="21.5" data-path="C21Design.html"><a href="C21Design.html#S21:Conclude"><i class="fa fa-check"></i><b>21.5</b> Concluding Remarks</a></li>
<li class="chapter" data-level="21.6" data-path="C21Design.html"><a href="C21Design.html#S21:References"><i class="fa fa-check"></i><b>21.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i><b>22</b> Appendices</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a1.-basic-statistical-inference"><i class="fa fa-check"></i>Appendix A1. Basic Statistical Inference</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#distributions-of-functions-of-random-variables"><i class="fa fa-check"></i>Distributions of Functions of Random Variables</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#estimation-and-prediction"><i class="fa fa-check"></i>Estimation and Prediction</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#testing-hypotheses"><i class="fa fa-check"></i>Testing Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a2.-matrix-algebra"><i class="fa fa-check"></i>Appendix A2. Matrix Algebra</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#basic-definitions"><i class="fa fa-check"></i>Basic Definitions</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#review-of-basic-operations"><i class="fa fa-check"></i>Review of Basic Operations</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#further-definitions"><i class="fa fa-check"></i>Further Definitions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a3.-probability-tables"><i class="fa fa-check"></i>Appendix A3. Probability Tables</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#normal-distribution"><i class="fa fa-check"></i>Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#chi-square-distribution"><i class="fa fa-check"></i>Chi-Square Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#t-distribution"><i class="fa fa-check"></i><em>t</em>-Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#f-distribution"><i class="fa fa-check"></i><em>F</em>-Distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="brief-answers-to-selected-exercises.html"><a href="brief-answers-to-selected-exercises.html"><i class="fa fa-check"></i>Brief Answers to Selected Exercises</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Regression Modeling with Actuarial and Financial Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="C11Binary" class="section level1 hasAnchor" number="11">
<h1><span class="header-section-number">Chapter 11</span> Categorical Dependent Variables<a href="C11Binary.html#C11Binary" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Chapter Preview</em>. A model with a categorical
dependent variable allows one to predict whether an observation is a
member of a distinct group, or category. Binary variables represent
an important special case; they can indicate whether or not an event
of interest has occurred. In actuarial and financial applications,
the event may be whether a claim occurs, a person purchases
insurance, a person retires or a firm becomes insolvent. The chapter
introduces logistic regression and probit models of binary dependent
variables. Categorical variables may also represent more than two
groups, known as <em>multicategory</em> outcomes. Multicategory
variables may be unordered or ordered, depending on whether it makes
sense to rank the variable outcomes. For unordered outcomes, known
as <em>nominal</em> variables, the chapter introduces generalized
logits and multinomial logit models. For ordered outcomes, known as
<em>ordinal</em> variables, the chapter introduces cumulative logit
and probit models.</p>
<div id="S:Sec111" class="section level2 hasAnchor" number="11.1">
<h2><span class="header-section-number">11.1</span> Binary Dependent Variables<a href="C11Binary.html#S:Sec111" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have already introduced binary variables as a special type of
discrete variable that can be used to indicate whether or not a
subject has a characteristic of interest, such as sex for a
person or ownership of a captive insurance company for a firm.
Binary variables also describe whether or not an event of interest
has occurred, such as an accident. A model with a binary dependent
variable allows one to predict whether an event has occurred or a
subject has a characteristic of interest.</p>
<hr />
<p><strong>Example: MEPS Expenditures.</strong> Section <a href="C11Binary.html#S:Sec114">11.4</a> will describe an extensive database from the Medical Expenditure Panel Survey (MEPS) on hospitalization
utilization and expenditures. For these data, we will consider
<span class="math display">\[
y_i = \left\{
\begin{array}{ll}
1 &amp; \text{i-th person was hospitalized during the sample period} \\
0 &amp; \text{otherwise}
\end{array}
\right. .
\]</span>
There are <span class="math inline">\(n=2,000\)</span> persons in this sample, distributed as:</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab111">Table 11.1: </span><strong>Hospitalization by Sex</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:center;">
</th>
<th style="text-align:center;">
Male
</th>
<th style="text-align:center;">
Female
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Not hospitalized
</td>
<td style="text-align:center;width: 3cm; ">
<span class="math inline">\(y=0\)</span>
</td>
<td style="text-align:center;width: 3cm; ">
902 (95.3%)
</td>
<td style="text-align:center;width: 3cm; ">
941 (89.3%)
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Hospitalized
</td>
<td style="text-align:center;width: 3cm; ">
<span class="math inline">\(y=1\)</span>
</td>
<td style="text-align:center;width: 3cm; ">
44 (4.7%)
</td>
<td style="text-align:center;width: 3cm; ">
113 (10.7%)
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Total
</td>
<td style="text-align:center;width: 3cm; ">
</td>
<td style="text-align:center;width: 3cm; ">
946
</td>
<td style="text-align:center;width: 3cm; ">
1,054
</td>
</tr>
</tbody>
</table>
<h5 style="text-align: center;">
<a id="displayCode.Tab111.Hide" href="javascript:togglecode('toggleCode.Tab111.Hide','displayCode.Tab111.Hide');"><i><strong>R Code to Produce Table 11.1</strong></i></a>
</h5>
<div id="toggleCode.Tab111.Hide" style="display: none">
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="C11Binary.html#cb93-1" tabindex="-1"></a>Hexpend <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;CSVData/HealthExpend.csv&quot;</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb93-2"><a href="C11Binary.html#cb93-2" tabindex="-1"></a><span class="co">#  Table 11.1</span></span>
<span id="cb93-3"><a href="C11Binary.html#cb93-3" tabindex="-1"></a>POSEXP <span class="ot">=</span> <span class="dv">1</span><span class="sc">*</span>(Hexpend<span class="sc">$</span>EXPENDIP<span class="sc">&gt;</span><span class="dv">0</span>)</span>
<span id="cb93-4"><a href="C11Binary.html#cb93-4" tabindex="-1"></a><span class="co">#table(POSEXP)</span></span>
<span id="cb93-5"><a href="C11Binary.html#cb93-5" tabindex="-1"></a><span class="co">#table(Hexpend$GENDER)</span></span>
<span id="cb93-6"><a href="C11Binary.html#cb93-6" tabindex="-1"></a><span class="co">#Hmisc::summarize(POSEXP, Hexpend$GENDER, mean)</span></span>
<span id="cb93-7"><a href="C11Binary.html#cb93-7" tabindex="-1"></a></span>
<span id="cb93-8"><a href="C11Binary.html#cb93-8" tabindex="-1"></a>row1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Not hospitalized&quot;</span> , <span class="st">&quot;$y=0$&quot;</span> , <span class="st">&quot;902 (95.3%)&quot;</span> , <span class="st">&quot;941 (89.3%)&quot;</span> )</span>
<span id="cb93-9"><a href="C11Binary.html#cb93-9" tabindex="-1"></a>row2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Hospitalized&quot;</span> , <span class="st">&quot;$y=1$ &quot;</span>,  <span class="st">&quot;44 (4.7%)&quot;</span>  , <span class="st">&quot;113 (10.7%)&quot;</span> )</span>
<span id="cb93-10"><a href="C11Binary.html#cb93-10" tabindex="-1"></a>row3 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Total&quot;</span> ,  <span class="st">&quot;&quot;</span>, <span class="st">&quot;946&quot;</span>  , <span class="st">&quot;1,054&quot;</span> )</span>
<span id="cb93-11"><a href="C11Binary.html#cb93-11" tabindex="-1"></a>tableout <span class="ot">&lt;-</span> <span class="fu">rbind</span>(row1, row2, row3)</span>
<span id="cb93-12"><a href="C11Binary.html#cb93-12" tabindex="-1"></a><span class="fu">row.names</span>(tableout) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb93-13"><a href="C11Binary.html#cb93-13" tabindex="-1"></a><span class="fu">colnames</span>(tableout) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>)</span>
<span id="cb93-14"><a href="C11Binary.html#cb93-14" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>tableout, </span>
<span id="cb93-15"><a href="C11Binary.html#cb93-15" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Hospitalization by Sex&#39;</span>, </span>
<span id="cb93-16"><a href="C11Binary.html#cb93-16" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;lccc&#39;</span>, <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb93-17"><a href="C11Binary.html#cb93-17" tabindex="-1"></a>         <span class="at">ColWidth =</span> <span class="st">&quot;3cm&quot;</span>)  <span class="sc">%&gt;%</span></span>
<span id="cb93-18"><a href="C11Binary.html#cb93-18" tabindex="-1"></a>     kableExtra<span class="sc">::</span><span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">width =</span>  <span class="st">&quot;4cm&quot;</span>)</span></code></pre></div>
</div>
<p>Table <a href="C11Binary.html#tab:Tab111">11.1</a> suggests that sex has an important influence on whether someone becomes hospitalized.</p>
<p>Like the linear regression techniques introduced in prior chapters, we are
interested in using characteristics of a person, such as their age, sex,
education, income and prior health status, to help explain the
dependent variable <span class="math inline">\(y\)</span>. Unlike the prior chapters, now the dependent
variable is discrete and not even approximately normally distributed. In
limited circumstances, linear regression can be used with binary dependent
variables — this application is known as a <em>linear probability model</em>.</p>
<div id="linear-probability-models" class="section level4 unnumbered hasAnchor">
<h4>Linear probability models<a href="C11Binary.html#linear-probability-models" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To introduce some of the complexities encountered with binary
dependent variables, denote the probability that the response equals
1 by <span class="math inline">\(\pi_i= \mathrm{Pr}(y_i=1)\)</span>. A binary random
variable has a <em>Bernoulli distribution</em>. Thus, we may interpret
the mean response as the probability that the response equals
one, that is, <span class="math inline">\(\mathrm{E~}y_i=0\times \mathrm{Pr}(y_i=0) + 1 \times \mathrm{Pr}(y_i=1) = \pi_i\)</span>. Further, the variance is related to the
mean through the expression <span class="math inline">\(\mathrm{Var}~y_i = \pi_i(1-\pi_i)\)</span>.</p>
<p>We begin by considering a linear model of the form
<span class="math display">\[
y_i = \mathbf{x}_i^{\mathbf{\prime}} \boldsymbol \beta +
\varepsilon_i,
\]</span></p>
<p>known as a linear probability model. Assuming
<span class="math inline">\(\mathrm{E~}\varepsilon_i=0\)</span>, we have that
<span class="math inline">\(\mathrm{E~}y_i=\mathbf{x}_i^{\mathbf{\prime }} \boldsymbol \beta =\pi_i\)</span>. Because <span class="math inline">\(y_i\)</span> has a Bernoulli distribution,
<span class="math inline">\(\mathrm{Var}~y_i=\mathbf{x}_i^{\mathbf{\prime}} \boldsymbol \beta(1-\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta)\)</span>. Linear
probability models are used because of the ease of parameter
interpretations. For large data sets, the computational simplicity
of ordinary least squares estimators is attractive when compared to
some complex alternative nonlinear models introduced later in this
chapter. As described in Chapter 3, ordinary least squares
estimators for <span class="math inline">\(\boldsymbol \beta\)</span> have desirable properties. It is
straightforward to check that the estimators are consistent and
asymptotically normal under mild conditions on the explanatory
variables {<span class="math inline">\(\mathbf{x}_i\)</span>}. However, linear probability models
have several drawbacks that are serious for many applications.</p>
<div class="blackbox">
<p><em>Drawbacks of the Linear Probability Model</em></p>
<ul>
<li><p><em>Fitted values can be poor.</em> The expected response is a probability and thus must vary between 0
and 1. However, the linear combination,
<span class="math inline">\(\mathbf{x}_i^{\mathbf{\prime}} \boldsymbol \beta\)</span>, can vary between
negative and positive infinity. This mismatch implies, for example,
that fitted values may be unreasonable.</p></li>
<li><p><em>Heteroscedasticity.</em> Linear models assume homoscedasticity (constant variance), yet the
variance of the response depends on the mean that varies over
observations. The problem of varying variability is known as
heteroscedasticity.</p></li>
<li><p><em>Residual analysis is meaningless.</em> The response must be either a 0 or 1 although the regression models
typically regard the distribution of the error term as continuous. This
mismatch implies, for example, that the usual residual analysis in
regression modeling is meaningless.</p></li>
</ul>
</div>
<p>To handle the heteroscedasticity problem, a (two-stage) weighted
least squares procedure is possible. In the first stage,
one uses ordinary least squares to compute estimates of <span class="math inline">\(\boldsymbol \beta\)</span>. With this estimate, an estimated variance for each subject
can be computed using the
relation <span class="math inline">\(\mathrm{Var}~y_i=\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta (1-\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta)\)</span>. At the second
stage, a weighted least squares is performed using the inverse of
the estimated variances as weights to arrive at new estimates of
<span class="math inline">\(\boldsymbol \beta\)</span>. It is possible to iterate this procedure,
although studies have shown that there are few advantages in doing
so (see Carroll and Ruppert, 1988). Alternatively, one can use
ordinary least squares estimators of <span class="math inline">\(\boldsymbol \beta\)</span> with
standard errors that are robust to heteroscedasticity (see Section
5.7.2).</p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_t4lkf66k&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_43a5f2kq%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="11.1 BinaryDepVar">
</iframe>
</center>
</div>
</div>
<div id="S:Sec112" class="section level2 hasAnchor" number="11.2">
<h2><span class="header-section-number">11.2</span> Logistic and Probit Regression Models<a href="C11Binary.html#S:Sec112" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="S:Sec1121" class="section level3 hasAnchor" number="11.2.1">
<h3><span class="header-section-number">11.2.1</span> Using Nonlinear Functions of Explanatory Variables<a href="C11Binary.html#S:Sec1121" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To circumvent the drawbacks of linear probability models, we consider
alternative models in which we express the expectation of the response as a
function of explanatory variables,
<span class="math inline">\(\pi_i=\mathrm{\pi }(\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta)\)</span> <span class="math inline">\(=\Pr (y_i=1|\mathbf{x}_i)\)</span>. We
focus on two special cases of the function <span class="math inline">\(\mathrm{\pi }(\cdot)\)</span>:</p>
<ul>
<li><p><span class="math inline">\(\mathrm{\pi }(z)=\frac{1}{1+\exp (-z)}=\frac{e^{z}}{1+e^{z}}\)</span>, the logit case, and</p></li>
<li><p><span class="math inline">\(\mathrm{\pi }(z)=\mathrm{\Phi }(z)\)</span>, the probit case.</p></li>
</ul>
<p>Here, <span class="math inline">\(\mathrm{\Phi }(\cdot)\)</span> is the standard normal
distribution function. The choice of the identity function (a
special kind of linear function), <span class="math inline">\(\mathrm{\pi }(z)=z\)</span>, yields the
linear probability model. In contrast, <span class="math inline">\(\mathrm{\pi}\)</span> is nonlinear
for both the logit and probit cases. These two functions are similar
in that they are almost linearly related over the interval <span class="math inline">\(0.1 \le p \le 0.9\)</span>. Thus, to a large extent, the function choice is
dependent on the preferences of the analyst. Figure
<a href="C11Binary.html#fig:Fig111">11.1</a> compares the logit and probit functions
showing that it will be difficult to distinguish between the two
specifications with most data sets.</p>
<p>The inverse of the function, <span class="math inline">\(\mathrm{\pi }^{-1}\)</span>, specifies the
form of the probability that is linear in the explanatory variables,
that is, <span class="math inline">\(\mathrm{\pi }^{-1}(\pi_i)= \mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta\)</span>. In Chapter 13, we
refer to this inverse as the <em>link function</em>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig111"></span>
<img src="RegressionMarkdown_files/figure-html/Fig111-1.png" alt="Comparison of Logit and Probit (Standard Normal) Distribution" width="60%" />
<p class="caption">
Figure 11.1: <strong>Comparison of Logit and Probit (Standard Normal) Distribution</strong>
</p>
</div>
<hr />
<p><strong>Example: Credit Scoring.</strong> Banks, credit bureaus, and other financial institutions develop “credit scores” for individuals that are used to predict the likelihood that the borrower will repay current and future debts. Individuals who do not meet stipulated repayment schedules in a loan agreement are said to be in “default.” A credit score is then a predicted probability of being in default, with the credit application providing the explanatory variables used in developing the credit score. The choice of explanatory variables depends on the purpose of the application; credit scoring is used for issuing credit cards for making small consumer purchases as well as mortgage applications for multimillion dollar houses. In <a href="C11Binary.html#Tab112">Table 11.2</a>, Hand and Henley (1997) provide a list of typical characteristics that are used in credit scoring.</p>
<p><a id=Tab112></a></p>
<p><span id="Tab112">Table 11.2</span>. <strong>Characteristics Used in Some Credit Scoring Procedures</strong></p>
<p><span class="math display">\[
\small{
\begin{array}{ll}
   \hline
\textbf{Characteristics} &amp; \textbf{Potential Values} \\
\hline
\text{Time at present address} &amp; \text{0-1, 1-2, 3-4, 5+ years}\\
\text{Home status} &amp; \text{Owner, tenant, other }\\
\text{Postal Code} &amp; \text{Band A, B, C, D, E} \\
\text{Telephone} &amp; \text{Yes, no} \\
\text{Applicant&#39;s annual income} &amp;  \text{£ (0-10000),}  \text{£ (10,000-20,000)}  \text{£ (20,000+)} \\
\text{Credit card} &amp; \text{Yes, no} \\
\text{Type of bank account} &amp; \text{Check and/or savings, none} \\
\text{Age }&amp; \text{18-25, 26-40, 41-55, 55+ years} \\
\text{County Court judgements} &amp; \text{Number} \\
\text{Type of occupation} &amp; \text{Coded} \\
\text{Purpose of loan} &amp; \text{Coded} \\
\text{Marital status} &amp; \text{Married, divorced, single, widow, other} \\
\text{Time with bank} &amp; \text{Years} \\
\text{Time with employer} &amp; \text{Years }\\
\hline
\textit{Source}: \text{Hand and Henley (1997)} \\
\end{array}
}
\]</span></p>
<p>With credit application information and default experience, a
logistic regression model can be used to fit the probability of
default with credit scores resulting from fitted values. Wiginton
(1980) provides an early application of logistic regression to
consumer credit scoring. At that time, other statistical methods
known as discriminant analysis were at the cutting edge of
quantitative scoring methodologies. In their review article, Hand
and Henley (1997) discuss other competitors to logistic regression
including machine learning systems and neural networks. As noted by
Hand and Henley, there is no uniformly “best” method. Regression
techniques are important in their own right due to their widespread
usage and because they can provide a platform for learning about
newer methods.</p>
<p>Credit scores provide estimates of the likelihood of defaulting on
loans but issuers of credit are also interested in the amount and
timing of debt repayment. For example, a “good” risk may repay a
credit balance so promptly that little profit is earned by the
lender. Further, a “poor” mortgage risk may default on a loan so
late in the duration of the contract that a sufficient profit was
earned by the lender. See Gourieroux and Jasiak (2007) for a broad
discussion of how credit modeling can be used to assess the
riskiness and profitability of loans.</p>
</div>
<div id="S:Sec1122" class="section level3 hasAnchor" number="11.2.2">
<h3><span class="header-section-number">11.2.2</span> Threshold Interpretation<a href="C11Binary.html#S:Sec1122" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Both the logit and probit cases can be interpreted as follows.
Suppose that there exists an <em>underlying</em> linear model,
<span class="math inline">\(y_i^{\ast} = \mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta  + \varepsilon_i^{\ast}\)</span>. Here, we do not observe the response
<span class="math inline">\(y_i^{\ast}\)</span> yet interpret it to be the “propensity” to possess a
characteristic. For example, we might think about the financial
strength of an insurance company as a measure of its propensity to
become insolvent (no longer capable of meeting its financial
obligations). Under the threshold interpretation, we do not observe
the propensity but we do observe when the propensity crosses a
threshold. It is customary to assume that this threshold is 0, for
simplicity. Thus, we observe
<span class="math display">\[
y_i=\left\{
\begin{array}{ll}
0 &amp; y_i^{\ast} \le 0 \\
1 &amp; y_i^{\ast}&gt;0
\end{array}
\right. .
\]</span>
To see how the logit case is derived from the threshold model,
assume a logistic distribution function for the disturbances, so that
<span class="math display">\[
\mathrm{\Pr }(\varepsilon_i^{\ast} \le a)=\frac{1}{1+\exp (-a)}.
\]</span>
Like the normal distribution, one can verify by calculating the density that the logistic distribution
is symmetric about zero. Thus, <span class="math inline">\(-\varepsilon_i^{\ast}\)</span> has the same distribution as <span class="math inline">\(\varepsilon_i^{\ast}\)</span> and so
<span class="math display">\[
\pi_i=\Pr (y_i=1|\mathbf{x}_i)=\mathrm{\Pr }(y_i^{\ast}&gt;0)=\mathrm{
\Pr }(\varepsilon_i^{\ast} \le \mathbf{x}_i^{\mathbf{\prime}}\mathbf{
\beta })=\frac{1}{1+\exp (-\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta)}
=\mathrm{\pi }(\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta).
\]</span>
This establishes the threshold interpretation for the logit case.
The development for the probit case is similar and is omitted.</p>
</div>
<div id="S:Sec1123" class="section level3 hasAnchor" number="11.2.3">
<h3><span class="header-section-number">11.2.3</span> Random Utility Interpretation<a href="C11Binary.html#S:Sec1123" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Both the logit and probit cases are also justified by appealing
to the following “random utility” interpretation of the model. In
some economic applications, individuals select one of two choices.
Here, preferences among choices are indexed by an unobserved utility
function; individuals select the choice that provides the greater
utility.</p>
<p>For the <span class="math inline">\(i\)</span>th subject, we use the notation <span class="math inline">\(u_i\)</span> for this utility function.
We model the utility (<span class="math inline">\(U\)</span>) as a function of an underlying value (<span class="math inline">\(V\)</span>) plus random
noise (<span class="math inline">\(\varepsilon\)</span>), that is, <span class="math inline">\(U_{ij}=u_i(V_{ij}+\varepsilon_{ij})\)</span>, where <span class="math inline">\(j\)</span> may
be 1 or 2, corresponding to the choice. To illustrate, we assume
that the individual chooses the category corresponding to <span class="math inline">\(j=1\)</span> if
<span class="math inline">\(U_{i1}&gt;U_{i2}\)</span> and denote this choice as <span class="math inline">\(y_i=1\)</span>. Assuming that
<span class="math inline">\(u_i\)</span> is a strictly increasing function, we have
<span class="math display">\[\begin{eqnarray*}
\Pr (y_i &amp;=&amp;1)=\mathrm{\Pr }(U_{i2}&lt;U_{i1})=\mathrm{\Pr }\left(
u_i(V_{i2}+\varepsilon_{i2})&lt;u_i(V_{i1}+\varepsilon_{i1})\right) \\
&amp;=&amp;\mathrm{\Pr }(\varepsilon_{i2}-\varepsilon_{i1}&lt;V_{i1}-V_{i2}).
\end{eqnarray*}\]</span></p>
<p>To parameterize the problem, assume that the value <span class="math inline">\(V\)</span> is an unknown
linear combination of explanatory variables. Specifically, we take
<span class="math inline">\(V_{i2}=0\)</span> and <span class="math inline">\(V_{i1}=\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta\)</span>. We may take the difference in the errors,
<span class="math inline">\(\varepsilon_{i2}-\varepsilon_{i1}\)</span>, as normal or logistic,
corresponding to the probit and logit cases, respectively. The
logistic distribution is satisfied if the errors are assumed to have
an <em>extreme-value</em>, or <em>Gumbel</em>, distribution (see, for
example, Amemiya, 1985).</p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_4hr9nmh7&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_wk8qt9wk%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="11.2 LogitRegression">
</iframe>
</center>
</div>
<div id="S:Sec1124" class="section level3 hasAnchor" number="11.2.4">
<h3><span class="header-section-number">11.2.4</span> Logistic Regression<a href="C11Binary.html#S:Sec1124" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An advantage of the logit case is that it permits closed-form expressions,
unlike the normal distribution function. <em>Logistic regression</em> is
another phrase used to describe the logit case.</p>
<p>Using <span class="math inline">\(p=\mathrm{\pi }(z)= \left( 1+ \mathrm{e}^{-z}\right)^{-1}\)</span>,
the inverse of <span class="math inline">\(\mathrm{\pi }\)</span> is calculated as <span class="math inline">\(z=\mathrm{\pi }^{-1}(p)=\ln(p/(1-p))\)</span>. To simplify future presentations, we define
<span class="math display">\[
\mathrm{logit}(p)=\ln \left( \frac{p}{1-p}\right)
\]</span>
to be the <em>logit function</em>. With a logistic regression model,
we represent the linear combination of explanatory variables as the
logit of the success probability, that is,
<span class="math inline">\(\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta=\mathrm{logit}(\pi_i)\)</span>.</p>
<div id="odds-interpretation" class="section level4 unnumbered hasAnchor">
<h4>Odds interpretation<a href="C11Binary.html#odds-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When the response <span class="math inline">\(y\)</span> is binary, knowing only <span class="math inline">\(p=\Pr(y=1)\)</span>
summarizes the entire distribution. In some applications, a simple
transformation of <span class="math inline">\(p\)</span> has an important interpretation. The lead
example of this is the <em>odds</em>, given by <span class="math inline">\(p/(1-p)\)</span>. For example,
suppose that <span class="math inline">\(y\)</span> indicates whether or not a horse wins a race and
<span class="math inline">\(p\)</span> is the probability of
the horse winning. If <span class="math inline">\(p=0.25\)</span>, then the odds of the horse winning is $
0.25/(1.00-0.25)=0.3333$. We might say that the odds of winning are 0.3333
to 1, or one to three. Equivalently, we say that the probability of not
winning is <span class="math inline">\(1-p=0.75\)</span> so that the odds of the horse not winning is $
0.75/(1-0.75)=3$ and the odds against the horse are three to one.</p>
<p>Odds have a useful interpretation from a betting standpoint. Suppose that we are playing a fair game and that we place a bet of 1 with one to three odds. If the horse wins, then we get our 1 back plus winnings of 3. If the horse loses, then we lose our bet of 1. It is a fair game in the sense that the expected value of the game is zero because we win 3 with probability <span class="math inline">\(p=0.25\)</span> and lose 1 with probability <span class="math inline">\(1-p=0.75\)</span>. From an economic standpoint, the odds provide the important numbers (bet of 1 and winnings of 3), not the probabilities. Of course, if we know <span class="math inline">\(p\)</span>, then we can always calculate the odds. Similarly, if we know the odds, we can always calculate the probability <span class="math inline">\(p\)</span>.</p>
<p>The logit is the logarithmic odds function, also known as the <em>log odds</em>.</p>
</div>
<div id="odds-ratio-interpretation" class="section level4 unnumbered hasAnchor">
<h4>Odds ratio interpretation<a href="C11Binary.html#odds-ratio-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To interpret the regression coefficients in the logistic regression model, <span class="math inline">\(\boldsymbol \beta=(\beta_0,\ldots ,\beta_{k})^{\prime}\)</span>, we begin by
assuming that <span class="math inline">\(j\)</span>th explanatory variable, <span class="math inline">\(x_{ij}\)</span>, is either 0 or
1. Then, with the notation <span class="math inline">\(\mathbf{x}_i=(x_{i0},...,x_{ij},\ldots ,x_{ik})^{\prime}\)</span>, we may interpret</p>
<p><span class="math display">\[\begin{eqnarray*}
\beta_j &amp;=&amp;(x_{i0},...,1,\ldots ,x_{ik})^{\prime}\boldsymbol \beta
-(x_{i0},...,0,\ldots ,x_{ik})^{\prime}\boldsymbol \beta \\
&amp;=&amp;\ln \left( \frac{\Pr (y_i=1|x_{ij}=1)}{1-\Pr
(y_i=1|x_{ij}=1)}\right) -\ln \left( \frac{\Pr
(y_i=1|x_{ij}=0)}{1-\Pr (y_i=1|x_{ij}=0)}\right)
\end{eqnarray*}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
e^{\beta_j}=\frac{\Pr (y_i=1|x_{ij}=1)/\left( 1-\Pr
(y_i=1|x_{ij}=1)\right) }{\Pr (y_i=1|x_{ij}=0)/\left( 1-\Pr
(y_i=1|x_{ij}=0)\right) }.
\]</span>
This shows that <span class="math inline">\(e^{\beta_j}\)</span> can be expressed as the ratio of two
odds, known as the <em>odds ratio</em>. That is, the numerator of this
expression is the odds when <span class="math inline">\(x_{ij}=1,\)</span> whereas the denominator is
the odds when <span class="math inline">\(x_{ij}=0\)</span>. Thus, we can say that the odds when
<span class="math inline">\(x_{ij}=1\)</span> are <span class="math inline">\(\exp (\beta_j)\)</span> times as large as the odds when
<span class="math inline">\(x_{ij}=0\)</span>. To illustrate, suppose <span class="math inline">\(\beta_j=0.693\)</span>, so that <span class="math inline">\(\exp (\beta _j)=2\)</span>. From this, we say that the odds (for <span class="math inline">\(y=1\)</span>) are twice
as great for <span class="math inline">\(x_{ij}=1\)</span> as for <span class="math inline">\(x_{ij}=0\)</span>.</p>
<p>Similarly, assuming that <span class="math inline">\(j\)</span>th explanatory variable is continuous
(differentiable), we have
<span class="math display" id="eq:eq111">\[\begin{eqnarray}
\beta_j &amp;=&amp;\frac{\partial }{\partial x_{ij}}\mathbf{x}_i^{\prime}
\boldsymbol \beta =\frac{\partial }{\partial x_{ij}}\ln \left(
\frac{\Pr (y_i=1|x_{ij})}{1-\Pr (y_i=1|x_{ij})}\right)   \nonumber \\
&amp;=&amp;\frac{\frac{\partial }{\partial x_{ij}}\Pr (y_i=1|x_{ij})/\left(
1-\Pr (y_i=1|x_{ij})\right) }{\Pr (y_i=1|x_{ij})/\left( 1-\Pr
(y_i=1|x_{ij})\right) }.
\tag{11.1}
\end{eqnarray}\]</span>
Thus, we may interpret <span class="math inline">\(\beta_j\)</span> as the proportional change in the
odds ratio, known as an <em>elasticity</em> in
economics.</p>
<hr />
<p><strong>Example: MEPS Expenditures - Continued.</strong> Table
<a href="C11Binary.html#tab:Tab111">11.1</a> shows that the percentage of females who
were hospitalized is <span class="math inline">\(10.7\%\)</span>; alternatively, the odds of females
being hospitalized is <span class="math inline">\(0.107/(1-0.107)=0.120\)</span>. For males, the
percentage is <span class="math inline">\(4.7\%\)</span> so that the odds were <span class="math inline">\(0.0493\)</span>. The odds ratio
is <span class="math inline">\(0.120/0.0493=2.434\)</span>; females are more than twice as likely to be
hospitalized as males.</p>
<p>From a logistic regression fit (described in Section
<a href="C11Binary.html#S:Sec114">11.4</a>), the coefficient associated with sex is <span class="math inline">\(0.733\)</span>.
Based on this model, we say that females are <span class="math inline">\(\exp (0.733)=2.081\)</span> times as likely as males to be hospitalized. The
regression estimate of the odds ratio controls for additional
variables (such as age and education) compared to the
basic calculation based on raw frequencies.</p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_xxxtzddg&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_yl2uckd5%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="11.3 LogitRegressionModel">
</iframe>
</center>
</div>
</div>
</div>
<div id="S:Sec113" class="section level2 hasAnchor" number="11.3">
<h2><span class="header-section-number">11.3</span> Inference for Logistic and Probit Regression Models<a href="C11Binary.html#S:Sec113" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="parameter-estimation" class="section level3 hasAnchor" number="11.3.1">
<h3><span class="header-section-number">11.3.1</span> Parameter Estimation<a href="C11Binary.html#parameter-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The customary method of estimation for logistic and probit models is
<em>maximum likelihood</em>, described in further detail in Section
<a href="C11Binary.html#S:Sec119">11.9</a>. To provide intuition, we outline the
ideas in the context of binary dependent variable regression models.</p>
<p>The <em>likelihood</em> is the observed value of the probability
function. For a single observation, the likelihood is
<span class="math display">\[
\left\{
\begin{array}{ll}
1-\pi_i &amp; \mathrm{if}\ y_i=0 \\
\pi_i &amp; \mathrm{if}\ y_i=1
\end{array}
\right. .
\]</span>
The objective of maximum likelihood estimation is to find the
parameter values that produce the largest likelihood. Finding the
maximum of the logarithmic function yields the same solution as
finding the maximum of the corresponding function. Because it is
generally computationally simpler, we consider the logarithmic (or
log-) likelihood, written as
<span class="math display" id="eq:eq112">\[\begin{equation}
\left\{
\begin{array}{ll}
\ln \left( 1-\pi_i\right) &amp; \mathrm{if}\ y_i=0 \\
\ln \pi_i &amp; \mathrm{if}\ y_i=1
\end{array}
\right. .
\tag{11.2}
\end{equation}\]</span>
More compactly, the log-likelihood of a single observation is
<span class="math display">\[
y_i\ln \mathrm{\pi }(\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol
\beta) + (1-y_i) \ln \left( 1-\mathrm{\pi }(\mathbf{x}_i^{\mathbf{\prime}}
\boldsymbol \beta)\right) ,
\]</span>
where
<span class="math inline">\(\pi_i=\mathrm{\pi }(\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta )\)</span>. Assuming independence among observations, the likelihood of the data
set is a product of likelihoods of each observation. Taking logarithms, the
log-likelihood of the data set is the sum of log-likelihoods of single
observations.</p>
<div class="blackbox">
<p>The log-likelihood of the data set is
<span class="math display" id="eq:eq113">\[\begin{equation}
L(\boldsymbol \beta)=\sum\limits_{i=1}^{n}\left\{ y_i\ln \mathrm{\pi
}( \mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta) + (1-y_i) \ln
\left( 1- \mathrm{\pi }(\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol
\beta)\right) \right\} .
\tag{11.3}
\end{equation}\]</span>
The log-likelihood is viewed as a function of the parameters, with
the data held fixed. In contrast, the joint probability mass
function is viewed as a function of the realized data, with the
parameters held fixed.</p>
</div>
<p>The <em>method of maximum likelihood</em> involves finding the values
of <span class="math inline">\(\boldsymbol \beta\)</span> that maximize the log-likelihood. The
customary method of finding the maximum is taking partial
derivatives with respect to the parameters of interest and finding
roots of the resulting equations. In this case, taking partial
derivatives with respect to <span class="math inline">\(\boldsymbol \beta\)</span> yields the
<em>score equations</em></p>
<p><span class="math display" id="eq:eq114">\[\begin{equation}
\frac{\partial }{\partial \boldsymbol \beta}L(\boldsymbol \beta
)=\sum\limits_{i=1}^{n}\mathbf{x}_i\left( y_i-\mathrm{\pi
}(\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta)\right)
\frac{\mathrm{\pi }^{\prime}(
\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta)}{\mathrm{\pi
}(\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta)(1-\mathrm{\pi
}(\mathbf{x}_i^{ \mathbf{\prime}}\boldsymbol \beta))}=\mathbf{0},
\tag{11.4}
\end{equation}\]</span>
where <span class="math inline">\(\pi^{\prime}\)</span> is the derivative of <span class="math inline">\(\pi\)</span>. The solution of these equations, denoted as <span class="math inline">\(\mathbf{b}_{MLE}\)</span>, is
the maximum likelihood estimator. For the logit
function the score equations reduce to</p>
<p><span class="math display" id="eq:eq115">\[\begin{equation}
\frac{\partial }{\partial \boldsymbol \beta}L(\boldsymbol \beta
)=\sum\limits_{i=1}^{n}\mathbf{x}_i\left( y_i-\mathrm{\pi }(\mathbf{x}
_i^{\mathbf{\prime}}\boldsymbol \beta)\right) =\mathbf{0},
\tag{11.5}
\end{equation}\]</span>
where <span class="math inline">\(\mathrm{\pi }(z)=1/(1+\exp (-z))\)</span>.</p>
</div>
<div id="additional-inference" class="section level3 hasAnchor" number="11.3.2">
<h3><span class="header-section-number">11.3.2</span> Additional Inference<a href="C11Binary.html#additional-inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An estimator of the large sample variance of <span class="math inline">\(\boldsymbol \beta\)</span> may
be calculated taking partial derivatives of the score equations.
Specifically, the term
<span class="math display">\[
\mathbf{I}(\boldsymbol \beta) = - \mathrm{E} \left( \frac{\partial^2}
{\partial \boldsymbol \beta ~ \partial \boldsymbol \beta
^{\prime}}L(\boldsymbol \beta) \right)
\]</span>
is the <em>information matrix</em>. As a special case, using the logit
function and equation <a href="C11Binary.html#eq:eq115">(11.5)</a>, straightforward
calculations show that the information matrix is
<span class="math display">\[
\mathbf{I}(\boldsymbol \beta) = \sum\limits_{i=1}^{n} \sigma_i^2
\mathbf{x}_i \mathbf{x}_i^{\prime}
\]</span>
where <span class="math inline">\(\sigma_i^2 = \mathrm{\pi} (\mathbf{x}_i^{\prime} \boldsymbol \beta) (1 - \mathrm{\pi}(\mathbf{x}_i^{\prime} \boldsymbol \beta))\)</span>.
The square root of the <span class="math inline">\((j+1)st\)</span> diagonal element of this matrix
evaluated at <span class="math inline">\(\boldsymbol \beta = \mathbf{b}_{MLE}\)</span> yields the
standard error for <span class="math inline">\(b_{j,MLE}\)</span>, denoted as <span class="math inline">\(se(b_{j,MLE})\)</span>.</p>
<p>To assess the overall model fit, it is customary to cite
<em>likelihood ratio test statistics</em> in nonlinear regression
models. To test the overall model adequacy <span class="math inline">\(H_0:\boldsymbol \beta=\mathbf{0}\)</span>, we use the statistic
<span class="math display">\[
LRT=2\times (L(\mathbf{b}_{MLE})-L_0),
\]</span>
where <span class="math inline">\(L_0\)</span> is the maximized log-likelihood with only an intercept
term. Under the null hypothesis <span class="math inline">\(H_0\)</span>, this statistic has a
chi-square distribution with <span class="math inline">\(k\)</span> degrees of freedom. Section
<a href="C11Binary.html#S:Sec1193">11.9.3</a> describes likelihood ratio test statistics in
greater technical detail.</p>
<p>As described in Section <a href="C11Binary.html#S:Sec119">11.9</a>, measures of
goodness of fit can be difficult to interpret in nonlinear models.
One measure is the so-called <span class="math inline">\(max-scaled~R^2\)</span>, defined as
<span class="math inline">\(R_{ms}^2=R^2/R_{max}^2\)</span>, where
<span class="math display">\[
R^2=1-\left( \frac{\exp (L_0/n)}{\exp
(L(\mathbf{b}_{MLE})/n)}\right)
\]</span>
and <span class="math inline">\(R_{max }^2 = 1 - \exp(L_0/n)^2\)</span>. Here, <span class="math inline">\(L_0/n\)</span> represents the
average value of this log-likelihood.</p>
<p>Another measure is a “<em>pseudo-</em><span class="math inline">\(R^2\)</span>”
<span class="math display">\[
\frac{L( \mathbf{b}_{MLE}) - L_0}{L_{max}-L_0},
\]</span>
where <span class="math inline">\(L_0\)</span> and <span class="math inline">\(L_{max }\)</span> is the log-likelihood based on only an
intercept and on the maximum achievable, respectively. Like the
coefficient of determination, the pseudo-<span class="math inline">\(R^2\)</span> takes on values
between zero and one, with larger values indicating a better fit to
the data. Other versions of pseudo-<span class="math inline">\(R^2\)</span>’s are available in the
literature, see, for example, Cameron and Trivedi (1998). An
advantage of this pseudo-<span class="math inline">\(R^2\)</span> measure is its link to hypothesis
testing of regression coefficients.</p>
<hr />
<p><strong>Example: Job Security.</strong> Valletta (1999) studied declining job security using the Panel Survey of
Income Dynamics (PSID) database. We consider here one of the
regression models presented by Valletta, based on a sample of male
heads of households that consists of <span class="math inline">\(n=24,168\)</span> observations over the
years 1976-1992, inclusive. The PSID survey records reasons why men
left their most recent employment, including plant closures,
“quit”and changed jobs for other reasons. However, Valletta focused on dismissals (“laid off” or “fired”) because involuntary separations are associated with job insecurity.</p>
<p><a href="C11Binary.html#Tab113">Table 11.3</a> presents a probit regression model
run by Valletta (1999), using dismissals as the dependent variable.
In addition to the explanatory variables listed in <a href="C11Binary.html#Tab113">Table 11.3</a>, other variables controlled for consisted
of education, marital status, number of children, race, years of
full-time work experience and its square, union membership,
government employment, logarithmic wage, the U.S. employment rate
and location as measured through the Metropolitan Statistical Area
residence. In <a href="C11Binary.html#Tab113">Table 11.3</a>, tenure is years
employed at the current firm. Further, sector employment was
measured by examining the Consumer Price Survey employment in 387
sectors of the economy, based on 43 industry categories and nine
regions of the country.</p>
<p>On the one hand, the tenure coefficient reveals that more
experienced workers are less likely to be dismissed. On the other
hand, the coefficient associated with the interaction between tenure
and time trend reveals an increasing dismissal rate for experienced
workers.</p>
<p>The interpretation of the sector employment coefficients is also of
interest. With an average tenure of about 7.8 years in the sample,
we see the low tenure men are relatively unaffected by changes in
sector employment. However, for more experienced men, there is an
increasing probability of dismissal associated with sectors of the
economy where growth declines.</p>
<p><a id=Tab113></a></p>
<p><span id="Tab113">Table 11.3</span>. <strong>Dismissal Probit Regression Estimates</strong></p>
<p><span class="math display">\[
\small{
\begin{array}{lrr}
\hline
\textbf{Variable} &amp; \textbf{Parameter} &amp; \textbf{Standard} \\
&amp; \textbf{Estimate} &amp; \textbf{Error} \\ \hline
\text{Tenure} &amp; -0.084 &amp; 0.010 \\
\text{Time Trend} &amp; -0.002 &amp; 0.005 \\
\text{Tenure*(Time Trend)} &amp; 0.003 &amp; 0.001 \\
\text{Change in Logarithmic Sector Employment} &amp; 0.094 &amp; 0.057 \\
\text{Tenure*( Change in Logarithmic Sector Employment)} &amp; -0.020  &amp; 0.009 \\ \hline
\text{-2 Log Likelihood }&amp; 7,027.8 &amp;  \\
\text{Pseudo}-R^2 &amp; 0.097 &amp;  \\ \hline
\end{array}
}
\]</span></p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_hb9m8ewx&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_t4kdv3bm%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="11.4 LogRegressionInference">
</iframe>
</center>
</div>
</div>
<div id="S:Sec114" class="section level2 hasAnchor" number="11.4">
<h2><span class="header-section-number">11.4</span> Application: Medical Expenditures<a href="C11Binary.html#S:Sec114" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section considers data from the Medical Expenditure Panel
Survey (MEPS), conducted by the U.S. Agency of Health Research and
Quality. MEPS is a probability survey that provides nationally
representative estimates of health care use, expenditures, sources
of payment, and insurance coverage for the U.S. civilian population.
This survey collects detailed information on individuals and each
medical care episode by type of services including physician office
visits, hospital emergency room visits, hospital outpatient visits,
hospital inpatient stays, all other medical provider visits, and use
of prescribed medicines. This detailed information allows one to
develop models of health care utilization to predict future
expenditures. We consider MEPS data from the first panel of 2003 and
take a random sample of <span class="math inline">\(n=2,000\)</span> individuals between ages 18 and
65.</p>
<div id="dependent-variable" class="section level4 unnumbered hasAnchor">
<h4>Dependent Variable<a href="C11Binary.html#dependent-variable" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Our dependent variable is an indicator of positive expenditures for
inpatient admissions. For MEPS, inpatient admissions include persons
who were admitted to a hospital and stayed overnight. In contrast,
outpatient events include hospital outpatient department visits,
office-based provider visits and emergency room visits excluding
dental services. (Dental services, compared to other types of health
care services, are more predictable and occur on a more regular
basis.) Hospital stays with the same date of admission and
discharge, known as “zero-night stays,” were included in
outpatient counts and expenditures. Payments associated with
emergency room visits that immediately preceded an inpatient stay
were included in the inpatient expenditures. Prescribed medicines
that can be linked to hospital admissions were included in inpatient
expenditures (not in outpatient utilization).</p>
</div>
<div id="explanatory-variables" class="section level4 unnumbered hasAnchor">
<h4>Explanatory Variables<a href="C11Binary.html#explanatory-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Explanatory variables that can help explain health care utilization are
categorized as demographic, geographic, health status, education and
economic factors. Demographic factors include age, sex and ethnicity. As
persons age, the rate at which their health deteriorates increases with age;
as a result, age has an increasing impact on the demand for health care. Sex
and ethnicity can be treated as proxies for inherited health and social
habits in maintaining health. For a geographic factor, we use region to
proxy the accessibility of health care services and the overall economic or
regional impact on residents’ health care behavior.</p>
<p>The demand for medical services is thought to be influenced by individuals’
health status and education. In MEPS, self-rated physical health, mental
health and any functional or activity related limitations during the sample
period are used as proxies for health status. Education tends to have
ambiguous impact on the demand for health care services. One theory is that
more educated persons are more aware of health risks, thus being more active
in maintaining their health; as a result, educated persons may be less prone
to severe diseases leading to hospital admissions. Another theory is that
less educated persons have greater exposure to health risks and, through
exposure, develop a greater tolerance for certain types of risks. In MEPS,
education is proxied by degrees received and categorized into three
different levels: lower than high school, high school, and college or above
education.</p>
<p>Economic covariates include income and insurance coverage. A measure of
income in MEPS is income relative to the poverty line. This approach is
appropriate because it summarizes effects of different levels of income on
health care utilization in constant dollars. Insurance coverage is also an
important variable in explaining health care utilization. One issue with
health insurance coverage is that it reduces the out-of-pocket prices paid
by insureds and thus induces moral hazard. Research associated with the Rand
Health Insurance Experiment empirically suggested that cost sharing effects
from insurance coverage will affect primarily the number of medical contacts
rather than the intensity of each contact. This motivated our introduction
of a binary variable that takes the value of 1 if a person had any public or
private health insurance for at least one month, and 0 otherwise.</p>
</div>
<div id="summary-statistics" class="section level4 unnumbered hasAnchor">
<h4>Summary Statistics<a href="C11Binary.html#summary-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><a href="C11Binary.html#Tab114">Table 11.4</a> describes these explanatory variables
and provides summary statistics that suggest their effects on the
probability of positive inpatient expenditures. For example, we see
that females had a higher overall utilization than males.
Specifically, 10.7% of females had a positive expenditure during
the year compared to only 4.7% for males. Similarly, utilizations
vary by other covariates, suggesting their importance as predictors
of expenditures.</p>
<p><a id=Tab114></a></p>
<p><span id="Tab114">Table 11.4</span>. <strong>Percent of Positive Expenditures by Explanatory Variable</strong></p>
<p><span class="math display">\[
\scriptsize{
\begin{array}{lllrr}
\hline
\textbf{Category} &amp; \textbf{Variable} &amp;  \textbf{Description} &amp; \textbf{Percent} &amp; \textbf{Percent} \\
&amp;  &amp;  &amp; \textbf{of data} &amp; \textbf{Positive} \\
&amp;  &amp;  &amp;  &amp; \textbf{Expend} \\ \hline
\text{Demography} &amp; AGE &amp; \text{Age in years between } \\
&amp;  &amp; \text{    18 to 65 (mean: 39.0)} \\
&amp; GENDER &amp; 1 \text{if female} &amp; 52.7 &amp; 10.7  \\
&amp;  &amp; \text{0 if male} &amp; 47.3  &amp;  4.7\\
\text{Ethnicity} &amp; ASIAN &amp; \text{1 if Asian} &amp; 4.3 &amp; 4.7  \\
&amp; BLACK &amp; \text{1 if Black} &amp; 14.8 &amp; 10.5 \\
&amp; NATIVE &amp; \text{1 if Native} &amp; 1.1 &amp; 13.6  \\
&amp; WHITE &amp; \text{Reference leve}l &amp; 79.9  &amp;  7.5  \\
\text{Region} &amp; NORTHEAST &amp; 1 \text{if Northeast }&amp; 14.3 &amp; 10.1 \\
&amp; MIDWEST &amp; 1 \text{if Midwest} &amp; 19.7 &amp;  8.7 \\
&amp; SOUTH &amp; 1 \text{if South} &amp; 38.2  &amp;  8.4 \\
&amp; WEST &amp; \text{Reference level} &amp;27.9  &amp;  5.4 \\
\hline \text{Education} &amp; \text{COLLEGE }&amp; 1 \text{if college or higher degree} &amp; 27.2  &amp; 6.8 \\
&amp; HIGHSCHOOL &amp; 1 \text{if high school degree} &amp; 43.3   &amp; 7.9\\
&amp; \text{Reference level is lower } &amp; &amp; 29.5 &amp; 8.8\\
&amp; \text{    than high school degree} &amp;  &amp; \\ \hline
\text{Self-rated} &amp; POOR &amp; \text{1 if poor} &amp; 3.8 &amp; 36.0 \\
\ \ \text{physical}&amp; FAIR &amp; \text{1 if fair} &amp; 9.9 &amp; 8.1 \\
\ \ \text{health} &amp; GOOD &amp; \text{1 if good} &amp; 29.9  &amp;  8.2 \\
&amp; VGOOD &amp; \text{1 if very good} &amp; 31.1  &amp;  6.3 \\
&amp;  \text{Reference level } &amp; &amp; 25.4  &amp;  5.1 \\
&amp;  ~~~\text{   is excellent health} &amp;   &amp; \\
\text{Self-rated} &amp; MNHPOOR &amp; \text{1 if poor or fair} &amp; 7.5 &amp; 16.8  \\
\ \ \text{mental health} &amp;  &amp; \text{0 if good to excellent mental health} &amp; 92.6 &amp;  7.1 \\
\text{Any activity} &amp; ANYLIMIT &amp; \text{1 if any functional/activity limitation}&amp;
22.3  &amp; 14.6  \\
\ \ \text{limitation} &amp;  &amp; \text{0 if otherwise} &amp; 77.7 &amp; 5.9
\\
\hline \text{Income} &amp; HINCOME  &amp; \text{1 if high income} &amp; 31.6 &amp; 5.4 \\
\ \ \text{compared to} &amp; MINCOME &amp; \text{1 if middle income} &amp; 29.9 &amp; 7.0 \\
\ \ \text{poverty line} &amp; LINCOME &amp; \text{1 if low income} &amp; 15.8 &amp; 8.3 \\
&amp; NPOOR &amp; \text{1 if near poor} &amp; 5.8 &amp; 9.5
\\
&amp; \text{Reference level } &amp; 17.0 &amp; 13.0 \\
&amp; ~~~\text{  is poor/negative} &amp;  &amp; \\ \hline
\text{Insurance} &amp; INSURE &amp; \text{1 if covered by public/private health} &amp; 77.8 &amp;  9.2 \\
\ \ \text{coverage} &amp;  &amp; \ \ \text{insurance in any month of 2003} &amp;  &amp;
\\
&amp;  &amp; \text{0 if have no health insurance in 2003} &amp; 22.3 &amp; 3.1
\\ \hline
Total &amp;  &amp;  &amp; 100.0 &amp; 7.9 \\ \hline
\end{array}
}
\]</span></p>
<h5 style="text-align: center;">
<a id="displayCode.Tab114.Hide" href="javascript:togglecode('toggleCode.Tab114.Hide','displayCode.Tab114.Hide');"><i><strong>R Code to Produce Table 11.4</strong></i></a>
</h5>
<div id="toggleCode.Tab114.Hide" style="display: none">
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="C11Binary.html#cb94-1" tabindex="-1"></a><span class="co">#  CREATE A SHORT FUNCTION TO SAVE WORK</span></span>
<span id="cb94-2"><a href="C11Binary.html#cb94-2" tabindex="-1"></a>fun1 <span class="ot">&lt;-</span> <span class="cf">function</span>(y){</span>
<span id="cb94-3"><a href="C11Binary.html#cb94-3" tabindex="-1"></a>  <span class="fu">options</span>(<span class="at">digits=</span><span class="dv">3</span>)</span>
<span id="cb94-4"><a href="C11Binary.html#cb94-4" tabindex="-1"></a>  temp0 <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">table</span>(y)<span class="sc">/</span><span class="fu">length</span>(y),<span class="fu">summarize</span>(POSEXP, y, mean)) </span>
<span id="cb94-5"><a href="C11Binary.html#cb94-5" tabindex="-1"></a>  temp <span class="ot">&lt;-</span> temp0[<span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>),] </span>
<span id="cb94-6"><a href="C11Binary.html#cb94-6" tabindex="-1"></a>  temp[,<span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>)] <span class="ot">&lt;-</span> <span class="fu">round</span>(temp[,<span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>)], <span class="at">digits=</span><span class="dv">3</span>)</span>
<span id="cb94-7"><a href="C11Binary.html#cb94-7" tabindex="-1"></a><span class="fu">return</span>(<span class="fu">cbind</span>(temp[,<span class="dv">3</span>],temp[,<span class="dv">2</span>],temp[,<span class="dv">4</span>]))}</span>
<span id="cb94-8"><a href="C11Binary.html#cb94-8" tabindex="-1"></a></span>
<span id="cb94-9"><a href="C11Binary.html#cb94-9" tabindex="-1"></a>var1 <span class="ot">&lt;-</span> <span class="fu">fun1</span>(Hexpend<span class="sc">$</span>GENDER)</span>
<span id="cb94-10"><a href="C11Binary.html#cb94-10" tabindex="-1"></a>var2 <span class="ot">&lt;-</span> <span class="fu">fun1</span>(Hexpend<span class="sc">$</span>RACE)</span>
<span id="cb94-11"><a href="C11Binary.html#cb94-11" tabindex="-1"></a>var3 <span class="ot">&lt;-</span> <span class="fu">fun1</span>(Hexpend<span class="sc">$</span>REGION)</span>
<span id="cb94-12"><a href="C11Binary.html#cb94-12" tabindex="-1"></a>var4 <span class="ot">&lt;-</span> <span class="fu">fun1</span>(Hexpend<span class="sc">$</span>EDUC)</span>
<span id="cb94-13"><a href="C11Binary.html#cb94-13" tabindex="-1"></a>var5 <span class="ot">&lt;-</span> <span class="fu">fun1</span>(Hexpend<span class="sc">$</span>PHSTAT)</span>
<span id="cb94-14"><a href="C11Binary.html#cb94-14" tabindex="-1"></a><span class="co">#var6 &lt;- fun1(Hexpend$MPOOR)</span></span>
<span id="cb94-15"><a href="C11Binary.html#cb94-15" tabindex="-1"></a>var7 <span class="ot">&lt;-</span> <span class="fu">fun1</span>(Hexpend<span class="sc">$</span>ANYLIMIT)</span>
<span id="cb94-16"><a href="C11Binary.html#cb94-16" tabindex="-1"></a>var8 <span class="ot">&lt;-</span> <span class="fu">fun1</span>(Hexpend<span class="sc">$</span>INCOME)</span>
<span id="cb94-17"><a href="C11Binary.html#cb94-17" tabindex="-1"></a>var9 <span class="ot">&lt;-</span> <span class="fu">fun1</span>(Hexpend<span class="sc">$</span>insure)</span>
<span id="cb94-18"><a href="C11Binary.html#cb94-18" tabindex="-1"></a></span>
<span id="cb94-19"><a href="C11Binary.html#cb94-19" tabindex="-1"></a>tableout <span class="ot">&lt;-</span> <span class="fu">rbind</span>(var1, var2, var3, var4,</span>
<span id="cb94-20"><a href="C11Binary.html#cb94-20" tabindex="-1"></a>                  var5, var7, var8, var9)</span>
<span id="cb94-21"><a href="C11Binary.html#cb94-21" tabindex="-1"></a><span class="co">#tableout</span></span></code></pre></div>
</div>
<p><a href="C11Binary.html#Tab115">Table 11.5</a> summarizes the fit of several
binary regression models. Fits are reported under the “Full Model”
column for all variables using the logit function. The <span class="math inline">\(t\)</span>-ratios
for many of the explanatory variables exceed two in absolute value,
suggesting that they are useful predictors. From an inspection of
these <span class="math inline">\(t\)</span>-ratios, one might consider a more parsimonious model by
removing statistically insignificant variables. <a href="C11Binary.html#Tab115">Table 11.5</a> shows a “Reduced Model,” where the age
and mental health status variables have been removed. To assess
their joint significance, we can compute a likelihood ratio test
statistic as twice the change in the log-likelihood. This turns out
to be only <span class="math inline">\(2\times \left( -488.78-(-488.69)\right) =0.36.\)</span>
Comparing this to a chi-square distribution with <span class="math inline">\(df=2\)</span> degrees of
freedom results in a <span class="math inline">\(p\)</span>-value<span class="math inline">\(=0.835\)</span>, indicating that the
additional parameters for age and mental health status are not
statistically significant. <a href="C11Binary.html#Tab115">Table 11.5</a> also
provides probit model fits. Here, we see that the results are
similar to the logit model fits, according to sign of the
coefficients and their significance, suggesting that for this
application there is little difference in the two specifications.</p>
<h5 style="text-align: center;">
<a id="displayCode.Table11.1Silly" href="javascript:togglecode('toggleCode.Table11.1Silly','displayCode.Table11.1Silly');"><i><strong></strong></i></a>
</h5>
<div id="toggleCode.Table11.1Silly" style="display: none">
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="C11Binary.html#cb95-1" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="dv">2</span>, <span class="at">caption =</span> <span class="st">&quot;Silly. Create a table just to update the counter...&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-154">Table 11.2: </span>Silly. Create a table just to update the counter…</caption>
<thead>
<tr class="header">
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="C11Binary.html#cb96-1" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="dv">2</span>, <span class="at">caption =</span> <span class="st">&quot;Silly.&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-155">Table 11.3: </span>Silly.</caption>
<thead>
<tr class="header">
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="C11Binary.html#cb97-1" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="dv">2</span>, <span class="at">caption =</span> <span class="st">&quot;Silly. &quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-156">Table 11.4: </span>Silly.</caption>
<thead>
<tr class="header">
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
</tr>
</tbody>
</table>
</div>
<p><a id=Tab115></a></p>
<p><span id="Tab115">Table 11.5</span>. <strong>Comparison of Binary Regression Models</strong></p>
<p><span class="math display">\[
\scriptsize{
\begin{array}{l|rr|rr|rr}
\hline &amp; \text{Logistic} &amp; &amp; \text{Logistic} &amp;&amp;\text{Probit} \\
\hline &amp; \text{Full Model} &amp; &amp;\text{Reduced Model} &amp;&amp; \text{Reduced Model} \\
&amp; \text{Parameter} &amp;  &amp; \text{Parameter} &amp;  &amp; \text{Parameter} &amp;  \\
\text{Effect} &amp; \text{Estimate} &amp; t\text{-ratio}o &amp; \text{Estimate} &amp; t\text{-rati}o &amp;
\text{Estimate} &amp; t\text{-ratio} \\ \hline
Intercept &amp;     -4.239 &amp;     -8.982 &amp;     -4.278 &amp;    -10.094 &amp;     -2.281 &amp;    -11.432 \\
       AGE &amp;     -0.001 &amp;     -0.180 &amp;            &amp;            &amp;            &amp;            \\
    GENDER &amp;      0.733 &amp;      3.812 &amp;      0.732 &amp;      3.806 &amp;      0.395 &amp;      4.178 \\
     ASIAN &amp;     -0.219 &amp;     -0.411 &amp;     -0.219 &amp;     -0.412 &amp;     -0.108 &amp;     -0.427 \\
     BLACK &amp;     -0.001 &amp;     -0.003 &amp;      0.004 &amp;      0.019 &amp;      0.009 &amp;      0.073 \\
    NATIVE &amp;      0.610 &amp;      0.926 &amp;      0.612 &amp;      0.930 &amp;      0.285 &amp;      0.780 \\
NORTHEAST &amp;      0.609 &amp;      2.112 &amp;      0.604 &amp;      2.098 &amp;      0.281 &amp;      1.950 \\
   MIDWEST &amp;      0.524 &amp;      1.904 &amp;      0.517 &amp;      1.883 &amp;      0.237 &amp;      1.754 \\
     SOUTH &amp;      0.339 &amp;      1.376 &amp;      0.328 &amp;      1.342 &amp;      0.130 &amp;      1.085 \\ \hline
   COLLEGE &amp;      0.068 &amp;      0.255 &amp;      0.070 &amp;      0.263 &amp;      0.049 &amp;      0.362 \\
HIGHSCHOOL &amp;      0.004 &amp;      0.017 &amp;      0.009 &amp;      0.041 &amp;
0.003 &amp;      0.030 \\ \hline
      POOR &amp;      1.712 &amp;      4.385 &amp;      1.652 &amp;      4.575 &amp;      0.939 &amp;      4.805 \\
      FAIR &amp;      0.136 &amp;      0.375 &amp;      0.109 &amp;      0.306 &amp;      0.079 &amp;      0.450 \\
      GOOD &amp;      0.376 &amp;      1.429 &amp;      0.368 &amp;      1.405 &amp;      0.182 &amp;      1.412 \\
     VGOOD &amp;      0.178 &amp;      0.667 &amp;      0.174 &amp;      0.655 &amp;      0.094 &amp;      0.728 \\
   MNHPOOR &amp;     -0.113 &amp;     -0.369 &amp;            &amp;            &amp;            &amp;            \\
  ANYLIMIT &amp;      0.564 &amp;      2.680 &amp;      0.545 &amp;      2.704 &amp;      0.311 &amp;      3.022 \\ \hline
   HINCOME &amp;     -0.921 &amp;     -3.101 &amp;     -0.919 &amp;     -3.162 &amp;     -0.470 &amp;     -3.224 \\
   MINCOME &amp;     -0.609 &amp;     -2.315 &amp;     -0.604 &amp;     -2.317 &amp;     -0.314 &amp;     -2.345 \\
   LINCOME &amp;     -0.411 &amp;     -1.453 &amp;     -0.408 &amp;     -1.449 &amp;     -0.241 &amp;     -1.633 \\
     NPOOR &amp;     -0.201 &amp;     -0.528 &amp;     -0.204 &amp;     -0.534 &amp;     -0.146 &amp;     -0.721 \\
    INSURE &amp;      1.234 &amp;      4.047 &amp;      1.227 &amp;      4.031 &amp;      0.579 &amp;      4.147 \\
\hline Log-Likelihood &amp;  -488.69 &amp;&amp; -488.78 &amp;&amp; -486.98 \\
\textit{AIC} &amp;  1,021.38 &amp;&amp; 1,017.56 &amp;&amp; 1,013.96 \\
\hline
\end{array}
}
\]</span></p>
<h5 style="text-align: center;">
<a id="displayCode.Tab115.Hide" href="javascript:togglecode('toggleCode.Tab115.Hide','displayCode.Tab115.Hide');"><i><strong>R Code to Produce Table 11.5</strong></i></a>
</h5>
<div id="toggleCode.Tab115.Hide" style="display: none">
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="C11Binary.html#cb98-1" tabindex="-1"></a><span class="co">#Hexpend &lt;- read.csv(&quot;CSVData/HealthExpend.csv&quot;, header=TRUE)</span></span>
<span id="cb98-2"><a href="C11Binary.html#cb98-2" tabindex="-1"></a><span class="co">#  Table 11.5</span></span>
<span id="cb98-3"><a href="C11Binary.html#cb98-3" tabindex="-1"></a>Hexpend<span class="sc">$</span>POSEXP <span class="ot">=</span> <span class="dv">1</span><span class="sc">*</span>(Hexpend<span class="sc">$</span>EXPENDIP<span class="sc">&gt;</span><span class="dv">0</span>)</span>
<span id="cb98-4"><a href="C11Binary.html#cb98-4" tabindex="-1"></a></span>
<span id="cb98-5"><a href="C11Binary.html#cb98-5" tabindex="-1"></a><span class="co">#  CHANGE REFERENCE LEVELS TO AGREE WITH BOOK (DONE IN SAS)</span></span>
<span id="cb98-6"><a href="C11Binary.html#cb98-6" tabindex="-1"></a><span class="fu">attach</span>(Hexpend)</span>
<span id="cb98-7"><a href="C11Binary.html#cb98-7" tabindex="-1"></a>RACE <span class="ot">=</span> <span class="fu">relevel</span>(<span class="fu">factor</span>(RACE),<span class="at">ref=</span><span class="st">&quot;WHITE&quot;</span>)</span>
<span id="cb98-8"><a href="C11Binary.html#cb98-8" tabindex="-1"></a>REGION <span class="ot">=</span> <span class="fu">relevel</span>(<span class="fu">factor</span>(REGION),<span class="at">ref=</span><span class="st">&quot;WEST&quot;</span>)</span>
<span id="cb98-9"><a href="C11Binary.html#cb98-9" tabindex="-1"></a>EDUC <span class="ot">=</span> <span class="fu">relevel</span>(<span class="fu">factor</span>(EDUC),<span class="at">ref=</span><span class="st">&quot;LHIGHSC&quot;</span>)</span>
<span id="cb98-10"><a href="C11Binary.html#cb98-10" tabindex="-1"></a>PHSTAT <span class="ot">=</span> <span class="fu">relevel</span>(<span class="fu">factor</span>(PHSTAT),<span class="at">ref=</span><span class="st">&quot;EXCE&quot;</span>)</span>
<span id="cb98-11"><a href="C11Binary.html#cb98-11" tabindex="-1"></a>INCOME <span class="ot">=</span> <span class="fu">relevel</span>(<span class="fu">factor</span>(INCOME),<span class="at">ref=</span><span class="st">&quot;POOR&quot;</span>)</span>
<span id="cb98-12"><a href="C11Binary.html#cb98-12" tabindex="-1"></a></span>
<span id="cb98-13"><a href="C11Binary.html#cb98-13" tabindex="-1"></a><span class="co"># FULL LOGIT MODEL;</span></span>
<span id="cb98-14"><a href="C11Binary.html#cb98-14" tabindex="-1"></a>PosExpglmFull <span class="ot">=</span> <span class="fu">glm</span>(POSEXP<span class="sc">~</span>AGE<span class="sc">+</span>GENDER</span>
<span id="cb98-15"><a href="C11Binary.html#cb98-15" tabindex="-1"></a> <span class="sc">+</span><span class="fu">factor</span>(RACE)<span class="sc">+</span> <span class="fu">factor</span>(REGION)<span class="sc">+</span><span class="fu">factor</span>(EDUC)</span>
<span id="cb98-16"><a href="C11Binary.html#cb98-16" tabindex="-1"></a> <span class="sc">+</span><span class="fu">factor</span>(PHSTAT)<span class="sc">+</span>ANYLIMIT<span class="sc">+</span><span class="fu">factor</span>(INCOME)</span>
<span id="cb98-17"><a href="C11Binary.html#cb98-17" tabindex="-1"></a> <span class="sc">+</span>insure, <span class="at">family=</span><span class="fu">binomial</span>(<span class="at">link=</span>logit))</span>
<span id="cb98-18"><a href="C11Binary.html#cb98-18" tabindex="-1"></a><span class="co">#summary(PosExpglmFull)</span></span>
<span id="cb98-19"><a href="C11Binary.html#cb98-19" tabindex="-1"></a>Table1 <span class="ot">&lt;-</span> <span class="fu">summary</span>(PosExpglmFull)</span>
<span id="cb98-20"><a href="C11Binary.html#cb98-20" tabindex="-1"></a>Table1Red <span class="ot">&lt;-</span> <span class="fu">round</span>(Table1<span class="sc">$</span>coefficients[,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>)], <span class="at">digits=</span><span class="dv">3</span>)</span>
<span id="cb98-21"><a href="C11Binary.html#cb98-21" tabindex="-1"></a></span>
<span id="cb98-22"><a href="C11Binary.html#cb98-22" tabindex="-1"></a></span>
<span id="cb98-23"><a href="C11Binary.html#cb98-23" tabindex="-1"></a><span class="co"># REDUCED LOGIT MODEL;</span></span>
<span id="cb98-24"><a href="C11Binary.html#cb98-24" tabindex="-1"></a>PosExpglmRed <span class="ot">=</span> <span class="fu">glm</span>(POSEXP<span class="sc">~</span>GENDER <span class="sc">+</span> <span class="fu">factor</span>(RACE) <span class="sc">+</span> </span>
<span id="cb98-25"><a href="C11Binary.html#cb98-25" tabindex="-1"></a>        <span class="sc">+</span><span class="fu">factor</span>(REGION)<span class="sc">+</span><span class="fu">factor</span>(EDUC)</span>
<span id="cb98-26"><a href="C11Binary.html#cb98-26" tabindex="-1"></a> <span class="sc">+</span><span class="fu">factor</span>(PHSTAT)<span class="sc">+</span>ANYLIMIT<span class="sc">+</span><span class="fu">factor</span>(INCOME)</span>
<span id="cb98-27"><a href="C11Binary.html#cb98-27" tabindex="-1"></a> <span class="sc">+</span>insure, <span class="at">family=</span><span class="fu">binomial</span>(<span class="at">link=</span>logit))</span>
<span id="cb98-28"><a href="C11Binary.html#cb98-28" tabindex="-1"></a><span class="co">#summary(PosExpglmRed)</span></span>
<span id="cb98-29"><a href="C11Binary.html#cb98-29" tabindex="-1"></a>Table2 <span class="ot">&lt;-</span> <span class="fu">summary</span>(PosExpglmRed)</span>
<span id="cb98-30"><a href="C11Binary.html#cb98-30" tabindex="-1"></a>Table2Red <span class="ot">&lt;-</span> <span class="fu">round</span>(Table2<span class="sc">$</span>coefficients[,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>)], <span class="at">digits =</span> <span class="dv">3</span>) </span>
<span id="cb98-31"><a href="C11Binary.html#cb98-31" tabindex="-1"></a><span class="co">#anova(PosExpglmRed,PosExpglmFull, test=&quot;Chisq&quot;)</span></span>
<span id="cb98-32"><a href="C11Binary.html#cb98-32" tabindex="-1"></a></span>
<span id="cb98-33"><a href="C11Binary.html#cb98-33" tabindex="-1"></a><span class="co"># REDUCED PROBIT MODEL;</span></span>
<span id="cb98-34"><a href="C11Binary.html#cb98-34" tabindex="-1"></a>PosExpglmRedProbit <span class="ot">=</span> <span class="fu">glm</span>(POSEXP<span class="sc">~</span>GENDER<span class="sc">+</span> <span class="fu">factor</span>(RACE)<span class="sc">+</span></span>
<span id="cb98-35"><a href="C11Binary.html#cb98-35" tabindex="-1"></a>        <span class="sc">+</span><span class="fu">factor</span>(REGION)<span class="sc">+</span><span class="fu">factor</span>(EDUC)</span>
<span id="cb98-36"><a href="C11Binary.html#cb98-36" tabindex="-1"></a> <span class="sc">+</span><span class="fu">factor</span>(PHSTAT)<span class="sc">+</span><span class="fu">factor</span>(ANYLIMIT)<span class="sc">+</span><span class="fu">factor</span>(INCOME)</span>
<span id="cb98-37"><a href="C11Binary.html#cb98-37" tabindex="-1"></a> <span class="sc">+</span><span class="fu">factor</span>(insure), <span class="fu">binomial</span>(<span class="at">link=</span>probit))</span>
<span id="cb98-38"><a href="C11Binary.html#cb98-38" tabindex="-1"></a><span class="co">#summary(PosExpglmRedProbit)</span></span>
<span id="cb98-39"><a href="C11Binary.html#cb98-39" tabindex="-1"></a>Table3 <span class="ot">&lt;-</span> <span class="fu">summary</span>(PosExpglmRedProbit)</span>
<span id="cb98-40"><a href="C11Binary.html#cb98-40" tabindex="-1"></a>Table3Red <span class="ot">&lt;-</span> <span class="fu">round</span>(Table3<span class="sc">$</span>coefficients[,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>)], <span class="at">digits =</span> <span class="dv">3</span>)</span>
<span id="cb98-41"><a href="C11Binary.html#cb98-41" tabindex="-1"></a><span class="fu">detach</span>(Hexpend)</span>
<span id="cb98-42"><a href="C11Binary.html#cb98-42" tabindex="-1"></a><span class="fu">row.names</span>(Table2Red) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb98-43"><a href="C11Binary.html#cb98-43" tabindex="-1"></a><span class="fu">row.names</span>(Table3Red) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb98-44"><a href="C11Binary.html#cb98-44" tabindex="-1"></a>Table23Join <span class="ot">&lt;-</span> <span class="fu">cbind</span>(Table2Red,Table3Red)</span>
<span id="cb98-45"><a href="C11Binary.html#cb98-45" tabindex="-1"></a>Table23JoinA <span class="ot">&lt;-</span> <span class="fu">rbind</span>(Table23Join[<span class="dv">1</span>,], <span class="fu">c</span>(<span class="st">&quot;&quot;</span>,<span class="st">&quot;&quot;</span>,<span class="st">&quot;&quot;</span>,<span class="st">&quot;&quot;</span>),</span>
<span id="cb98-46"><a href="C11Binary.html#cb98-46" tabindex="-1"></a>                      Table23Join[<span class="dv">2</span><span class="sc">:</span><span class="dv">14</span>,], <span class="fu">c</span>(<span class="st">&quot;&quot;</span>,<span class="st">&quot;&quot;</span>,<span class="st">&quot;&quot;</span>,<span class="st">&quot;&quot;</span>),</span>
<span id="cb98-47"><a href="C11Binary.html#cb98-47" tabindex="-1"></a>                      Table23Join[<span class="dv">15</span><span class="sc">:</span><span class="dv">20</span>,])</span>
<span id="cb98-48"><a href="C11Binary.html#cb98-48" tabindex="-1"></a>Table123 <span class="ot">&lt;-</span> <span class="fu">cbind</span>(Table1Red, Table23JoinA)</span>
<span id="cb98-49"><a href="C11Binary.html#cb98-49" tabindex="-1"></a>rowLogLik <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">c</span>(<span class="fu">logLik</span>(PosExpglmFull) , <span class="fu">logLik</span>(PosExpglmRed) , <span class="fu">logLik</span>(PosExpglmRedProbit) ), <span class="at">digits =</span> <span class="dv">2</span>)</span>
<span id="cb98-50"><a href="C11Binary.html#cb98-50" tabindex="-1"></a>rowAIC <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">c</span>(<span class="fu">AIC</span>(PosExpglmFull) , <span class="fu">AIC</span>(PosExpglmRed) , <span class="fu">AIC</span>(PosExpglmRedProbit) ), <span class="at">digits =</span> <span class="dv">2</span>)</span>
<span id="cb98-51"><a href="C11Binary.html#cb98-51" tabindex="-1"></a>rowSumStats <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">c</span>(rowLogLik[<span class="dv">1</span>], <span class="st">&quot;&quot;</span>,rowLogLik[<span class="dv">2</span>], <span class="st">&quot;&quot;</span>,rowLogLik[<span class="dv">3</span>], <span class="st">&quot;&quot;</span>),</span>
<span id="cb98-52"><a href="C11Binary.html#cb98-52" tabindex="-1"></a>                     <span class="fu">c</span>(rowAIC[<span class="dv">1</span>], <span class="st">&quot;&quot;</span>,rowAIC[<span class="dv">2</span>], <span class="st">&quot;&quot;</span>,rowAIC[<span class="dv">3</span>], <span class="st">&quot;&quot;</span>) )</span>
<span id="cb98-53"><a href="C11Binary.html#cb98-53" tabindex="-1"></a><span class="fu">row.names</span>(rowSumStats) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Log-Likelihood&quot;</span>, <span class="st">&quot;$AIC$&quot;</span>)</span>
<span id="cb98-54"><a href="C11Binary.html#cb98-54" tabindex="-1"></a>Table123A <span class="ot">&lt;-</span> <span class="fu">rbind</span>(Table123, rowSumStats)</span>
<span id="cb98-55"><a href="C11Binary.html#cb98-55" tabindex="-1"></a><span class="fu">colnames</span>(Table123A) <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;Parameter Estimates&quot;</span>, <span class="st">&quot;$t$-Ratio&quot;</span>),<span class="dv">3</span>)</span>
<span id="cb98-56"><a href="C11Binary.html#cb98-56" tabindex="-1"></a>  </span>
<span id="cb98-57"><a href="C11Binary.html#cb98-57" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>Table123A, </span>
<span id="cb98-58"><a href="C11Binary.html#cb98-58" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Comparison of Binary Regression Models&#39;</span>, </span>
<span id="cb98-59"><a href="C11Binary.html#cb98-59" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>, <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,</span>
<span id="cb98-60"><a href="C11Binary.html#cb98-60" tabindex="-1"></a>         <span class="at">ColWidth =</span> ColWidth6)  <span class="sc">%&gt;%</span></span>
<span id="cb98-61"><a href="C11Binary.html#cb98-61" tabindex="-1"></a>  <span class="fu">add_header_above</span>(<span class="fu">c</span>(<span class="st">&quot; &quot;</span><span class="ot">=</span><span class="dv">1</span>, <span class="st">&quot;Logistic Full Model&quot;</span> <span class="ot">=</span> <span class="dv">2</span>, <span class="st">&quot;Logistic Reduced Model&quot;</span> <span class="ot">=</span> <span class="dv">2</span>, <span class="st">&quot;Probit Reduced Model&quot;</span><span class="ot">=</span><span class="dv">2</span>))</span></code></pre></div>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab115">Table 11.5: </span><strong>Comparison of Binary Regression Models</strong>
</caption>
<thead>
<tr>
<th style="empty-cells: hide;" colspan="1">
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Logistic Full Model
</div>
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Logistic Reduced Model
</div>
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Probit Reduced Model
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Parameter Estimates
</th>
<th style="text-align:right;">
<span class="math inline">\(t\)</span>-Ratio
</th>
<th style="text-align:right;">
Parameter Estimates
</th>
<th style="text-align:right;">
<span class="math inline">\(t\)</span>-Ratio
</th>
<th style="text-align:right;">
Parameter Estimates
</th>
<th style="text-align:right;">
<span class="math inline">\(t\)</span>-Ratio
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
(Intercept)
</td>
<td style="text-align:right;width: 1.4cm; ">
-4.239
</td>
<td style="text-align:right;width: 1.4cm; ">
-8.981
</td>
<td style="text-align:right;width: 1.4cm; ">
-4.274
</td>
<td style="text-align:right;width: 1.4cm; ">
-10.082
</td>
<td style="text-align:right;width: 1.4cm; ">
-2.279
</td>
<td style="text-align:right;width: 1.4cm; ">
-11.346
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
AGE
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.001
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.17
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
GENDER
</td>
<td style="text-align:right;width: 1.4cm; ">
0.734
</td>
<td style="text-align:right;width: 1.4cm; ">
3.815
</td>
<td style="text-align:right;width: 1.4cm; ">
0.735
</td>
<td style="text-align:right;width: 1.4cm; ">
3.817
</td>
<td style="text-align:right;width: 1.4cm; ">
0.395
</td>
<td style="text-align:right;width: 1.4cm; ">
4.197
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(RACE)ASIAN
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.222
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.417
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.223
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.418
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.108
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.432
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(RACE)BLACK
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.004
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.017
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.002
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.008
</td>
<td style="text-align:right;width: 1.4cm; ">
0.008
</td>
<td style="text-align:right;width: 1.4cm; ">
0.062
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(RACE)NATIV
</td>
<td style="text-align:right;width: 1.4cm; ">
0.602
</td>
<td style="text-align:right;width: 1.4cm; ">
0.913
</td>
<td style="text-align:right;width: 1.4cm; ">
0.607
</td>
<td style="text-align:right;width: 1.4cm; ">
0.92
</td>
<td style="text-align:right;width: 1.4cm; ">
0.283
</td>
<td style="text-align:right;width: 1.4cm; ">
0.781
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(RACE)OTHER
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.215
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.32
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.214
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.319
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.05
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.148
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(REGION)MIDWEST
</td>
<td style="text-align:right;width: 1.4cm; ">
0.518
</td>
<td style="text-align:right;width: 1.4cm; ">
1.887
</td>
<td style="text-align:right;width: 1.4cm; ">
0.517
</td>
<td style="text-align:right;width: 1.4cm; ">
1.882
</td>
<td style="text-align:right;width: 1.4cm; ">
0.236
</td>
<td style="text-align:right;width: 1.4cm; ">
1.747
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(REGION)NORTHEAST
</td>
<td style="text-align:right;width: 1.4cm; ">
0.605
</td>
<td style="text-align:right;width: 1.4cm; ">
2.099
</td>
<td style="text-align:right;width: 1.4cm; ">
0.603
</td>
<td style="text-align:right;width: 1.4cm; ">
2.092
</td>
<td style="text-align:right;width: 1.4cm; ">
0.28
</td>
<td style="text-align:right;width: 1.4cm; ">
1.941
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(REGION)SOUTH
</td>
<td style="text-align:right;width: 1.4cm; ">
0.332
</td>
<td style="text-align:right;width: 1.4cm; ">
1.355
</td>
<td style="text-align:right;width: 1.4cm; ">
0.33
</td>
<td style="text-align:right;width: 1.4cm; ">
1.348
</td>
<td style="text-align:right;width: 1.4cm; ">
0.13
</td>
<td style="text-align:right;width: 1.4cm; ">
1.081
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(EDUC)COLLEGE
</td>
<td style="text-align:right;width: 1.4cm; ">
0.068
</td>
<td style="text-align:right;width: 1.4cm; ">
0.255
</td>
<td style="text-align:right;width: 1.4cm; ">
0.068
</td>
<td style="text-align:right;width: 1.4cm; ">
0.253
</td>
<td style="text-align:right;width: 1.4cm; ">
0.048
</td>
<td style="text-align:right;width: 1.4cm; ">
0.357
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(EDUC)HIGHSCH
</td>
<td style="text-align:right;width: 1.4cm; ">
0.005
</td>
<td style="text-align:right;width: 1.4cm; ">
0.025
</td>
<td style="text-align:right;width: 1.4cm; ">
0.005
</td>
<td style="text-align:right;width: 1.4cm; ">
0.024
</td>
<td style="text-align:right;width: 1.4cm; ">
0.003
</td>
<td style="text-align:right;width: 1.4cm; ">
0.024
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(PHSTAT)FAIR
</td>
<td style="text-align:right;width: 1.4cm; ">
0.115
</td>
<td style="text-align:right;width: 1.4cm; ">
0.321
</td>
<td style="text-align:right;width: 1.4cm; ">
0.108
</td>
<td style="text-align:right;width: 1.4cm; ">
0.303
</td>
<td style="text-align:right;width: 1.4cm; ">
0.078
</td>
<td style="text-align:right;width: 1.4cm; ">
0.445
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(PHSTAT)GOOD
</td>
<td style="text-align:right;width: 1.4cm; ">
0.371
</td>
<td style="text-align:right;width: 1.4cm; ">
1.409
</td>
<td style="text-align:right;width: 1.4cm; ">
0.366
</td>
<td style="text-align:right;width: 1.4cm; ">
1.399
</td>
<td style="text-align:right;width: 1.4cm; ">
0.181
</td>
<td style="text-align:right;width: 1.4cm; ">
1.409
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(PHSTAT)POOR
</td>
<td style="text-align:right;width: 1.4cm; ">
1.668
</td>
<td style="text-align:right;width: 1.4cm; ">
4.524
</td>
<td style="text-align:right;width: 1.4cm; ">
1.656
</td>
<td style="text-align:right;width: 1.4cm; ">
4.583
</td>
<td style="text-align:right;width: 1.4cm; ">
0.939
</td>
<td style="text-align:right;width: 1.4cm; ">
4.739
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(PHSTAT)VGOO
</td>
<td style="text-align:right;width: 1.4cm; ">
0.175
</td>
<td style="text-align:right;width: 1.4cm; ">
0.654
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
ANYLIMIT
</td>
<td style="text-align:right;width: 1.4cm; ">
0.554
</td>
<td style="text-align:right;width: 1.4cm; ">
2.651
</td>
<td style="text-align:right;width: 1.4cm; ">
0.171
</td>
<td style="text-align:right;width: 1.4cm; ">
0.644
</td>
<td style="text-align:right;width: 1.4cm; ">
0.093
</td>
<td style="text-align:right;width: 1.4cm; ">
0.723
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(INCOME)HINCOME
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.911
</td>
<td style="text-align:right;width: 1.4cm; ">
-3.082
</td>
<td style="text-align:right;width: 1.4cm; ">
0.545
</td>
<td style="text-align:right;width: 1.4cm; ">
2.702
</td>
<td style="text-align:right;width: 1.4cm; ">
0.311
</td>
<td style="text-align:right;width: 1.4cm; ">
3.008
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(INCOME)LINCOME
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.404
</td>
<td style="text-align:right;width: 1.4cm; ">
-1.431
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.92
</td>
<td style="text-align:right;width: 1.4cm; ">
-3.165
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.471
</td>
<td style="text-align:right;width: 1.4cm; ">
-3.187
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(INCOME)MINCOME
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.6
</td>
<td style="text-align:right;width: 1.4cm; ">
-2.288
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.408
</td>
<td style="text-align:right;width: 1.4cm; ">
-1.448
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.241
</td>
<td style="text-align:right;width: 1.4cm; ">
-1.635
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
factor(INCOME)NPOOR
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.199
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.522
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.604
</td>
<td style="text-align:right;width: 1.4cm; ">
-2.317
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.315
</td>
<td style="text-align:right;width: 1.4cm; ">
-2.326
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
insure
</td>
<td style="text-align:right;width: 1.4cm; ">
1.232
</td>
<td style="text-align:right;width: 1.4cm; ">
4.041
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.199
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.522
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.146
</td>
<td style="text-align:right;width: 1.4cm; ">
-0.722
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Log-Likelihood
</td>
<td style="text-align:right;width: 1.4cm; ">
-488.71
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
<td style="text-align:right;width: 1.4cm; ">
-488.72
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
<td style="text-align:right;width: 1.4cm; ">
-486.97
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
<span class="math inline">\(AIC\)</span>
</td>
<td style="text-align:right;width: 1.4cm; ">
1021.42
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
<td style="text-align:right;width: 1.4cm; ">
1019.45
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
<td style="text-align:right;width: 1.4cm; ">
1015.94
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="S:Sec115" class="section level2 hasAnchor" number="11.5">
<h2><span class="header-section-number">11.5</span> Nominal Dependent Variables<a href="C11Binary.html#S:Sec115" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now consider a response that is an unordered categorical
variable, also known as a <em>nominal</em> dependent variable. We
assume that the dependent variable <span class="math inline">\(y\)</span> may take on values <span class="math inline">\(1, 2, \ldots , c,\)</span> corresponding to <span class="math inline">\(c\)</span> categories. When <span class="math inline">\(c&gt;2\)</span>, we refer
to the data as “multicategory,” also known as <em>polychotomous</em>
or <em>polytomous</em>.</p>
<p>In many applications, the response categories correspond to an
attribute possessed or choices made by individuals, households or
firms. Some applications include:</p>
<ul>
<li>employment choice, such as Valletta (1999)</li>
<li>mode of transportation, such as the classic work by McFadden (1978)</li>
<li>type of health insurance, as in Browne and Frees (2007).</li>
</ul>
<p>For an observation from subject <span class="math inline">\(i\)</span>, denote the probability of
choosing the <span class="math inline">\(j\)</span>th category as <span class="math inline">\(\pi_{ij}= \mathrm{Pr}(y_i = j)\)</span>, so
that <span class="math inline">\(\pi_{i1}+\cdots+\pi_{ic}=1\)</span>. In general, we will model these
probabilities as a (known) function of parameters and use maximum
likelihood estimation for statistical inference. Let <span class="math inline">\(y_{ij}\)</span> be a
binary variable that is 1 if <span class="math inline">\(y_i=j\)</span>. Extending equation
<a href="C11Binary.html#eq:eq112">(11.2)</a> to <span class="math inline">\(c\)</span> categories, the likelihood for the
<span class="math inline">\(i\)</span>th subject is:</p>
<p><span class="math display">\[
\prod_{j=1}^c \left( \pi_{i,j} \right)^{y_{i,j}} =\left\{
\begin{array}{cc}
\pi_{i,1} &amp; \mathrm {if}~ y_i = 1  \\
  \pi_{i,2} &amp; \mathrm {if}~  y_i = 2  \\
  \vdots &amp; \vdots   \\
   \pi_{i,c} &amp; \mathrm {if}~  y_i = c  \\
\end{array}
\right. .
\]</span>
Thus, assuming independence among observations, the total
log-likelihood is</p>
<p><span class="math display">\[
L = \sum_{i=1}^n \sum_{j=1}^c y_{i,j}~ \mathrm{ln}~ \pi_{i,j} .
\]</span>
With this framework, standard maximum likelihood estimation is
available (Section <a href="C11Binary.html#S:Sec119">11.9</a>). Thus, our main
task is to specify an appropriate form for <span class="math inline">\(\pi\)</span>.</p>
<div id="S:Sec1151" class="section level3 hasAnchor" number="11.5.1">
<h3><span class="header-section-number">11.5.1</span> Generalized Logit<a href="C11Binary.html#S:Sec1151" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Like standard linear regression, generalized logit models employ
linear combinations of explanatory variables of the form:
<span class="math display" id="eq:eq116">\[\begin{equation}
V_{i,j} = \mathbf{x}_i^{\prime} \boldsymbol \beta_j .
\tag{11.6}
\end{equation}\]</span>
Because the dependent variables are not numerical, we cannot model
the response <span class="math inline">\(y\)</span> as a linear combination of explanatory variables
plus an error. Instead we use the probabilities
<span class="math display" id="eq:eq117">\[\begin{equation}
\mathrm{Pr} \left(y_i = j \right) = \pi_{i,j} = \frac {\exp
(V_{i,j})}{\sum_{k=1}^c  \exp(V_{i,k})} .
\tag{11.7}
\end{equation}\]</span>
Note here that <span class="math inline">\(\boldsymbol \beta_j\)</span> is the corresponding vector of
parameters that may depend on the alternative <span class="math inline">\(j\)</span> whereas
the explanatory variables <span class="math inline">\(\mathbf{x}_i\)</span> do not. So that
probabilities sum to one, a convenient normalization for this model
is <span class="math inline">\(\boldsymbol \beta_c =\mathbf{0}\)</span>. With this normalization and
the special case of <span class="math inline">\(c = 2\)</span>, the generalized logit reduces to the
logit model introduced in Section <a href="C11Binary.html#S:Sec112">11.2</a>.</p>
<div id="parameter-interpretations" class="section level4 unnumbered hasAnchor">
<h4>Parameter interpretations<a href="C11Binary.html#parameter-interpretations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We now describe an interpretation of coefficients in generalized
logit models, similar to the logistic model. From equations
<a href="C11Binary.html#eq:eq116">(11.6)</a> and <a href="C11Binary.html#eq:eq117">(11.7)</a>,
we have
<span class="math display">\[
\mathrm{ln}~ \frac{\mathrm{Pr} \left(y_i = j \right)} {\mathrm{Pr}
\left(y_i = c \right)} = V_{i,j} - V_{i,c} =\mathbf{x}_i^{\prime}
\boldsymbol \beta_j .
\]</span>
The left-hand side of this equation is interpreted to be the
logarithmic odds of choosing choice <span class="math inline">\(j\)</span> compared to choice <span class="math inline">\(c\)</span>.
Thus, we may interpret <span class="math inline">\(\boldsymbol \beta_j\)</span> as the proportional
change in the odds ratio.</p>
<p>Generalized logits have an interesting <em>nested</em> structure that
we will explore briefly in Section <a href="C11Binary.html#S:Sec1153">11.5.3</a>. That is,
it is easy to check that, conditional on not choosing the first
category, the form of Pr(<span class="math inline">\(y_i = j| y_i \neq 1\)</span>) has a generalized
logit form in equation <a href="C11Binary.html#eq:eq117">(11.7)</a>. Further,
if <span class="math inline">\(j\)</span> and <span class="math inline">\(h\)</span> are different alternatives, we note that</p>
<p><span class="math display">\[\begin{eqnarray*}
\mathrm{Pr}(y_i = j| y_i=j ~\mathrm{or}~ y_i=h)
&amp;=&amp;\frac{\mathrm{Pr}(y_i = j)}{\mathrm{Pr}(y_i = j)+\mathrm{Pr}(y_i
= h)}
=\frac{\mathrm{exp}(V_{i,j})}{\mathrm{exp}(V_{i,j})+\mathrm{exp}(V_{i,h})}
\\
&amp;=&amp;\frac{1}{1+\mathrm{exp}(\mathbf{x}_i^{\prime}(\boldsymbol \beta
_h - \boldsymbol \beta_j))} . \end{eqnarray*}\]</span></p>
<p>This has a logit form that was introduced in Section
<a href="C11Binary.html#S:Sec112">11.2</a>.</p>
<p><strong>Special Case - Intercept only model.</strong> To develop intuition,
we now consider the model with only intercepts. Thus, let
<span class="math inline">\(\mathbf{x}_i = 1\)</span> and <span class="math inline">\(\boldsymbol \beta_j = \beta_{0,j} = \alpha_j\)</span>. With the convention <span class="math inline">\(\alpha_c=0\)</span>, we have
<span class="math display">\[
\mathrm{Pr} \left(y_i = j \right) = \pi_{i,j} = \frac
{e^{\alpha_j}}{e^{\alpha_1}+e^{\alpha_2}+\cdots+e^{\alpha_{c-1}}+1}
\]</span>
and
<span class="math display">\[
\mathrm{ln}~ \frac{\mathrm{Pr} \left(y_i = j \right)} {\mathrm{Pr}
\left(y_i = c \right)} = \alpha_j.
\]</span>
From the second relation, we may interpret the <span class="math inline">\(j\)</span>th intercept
<span class="math inline">\(\alpha_j\)</span> to be the logarithmic odds of choosing alternative <span class="math inline">\(j\)</span>
compared to alternative <span class="math inline">\(c\)</span>.</p>
<hr />
<p><strong>Example: Job Security - Continued.</strong> This is a continuation
of the Section <a href="C11Binary.html#S:Sec112">11.2</a> example on the determinants
of job turnover, based on the work of Valletta (1999). The first
analysis of this data considered only the binary dependent variable
dismissal as this outcome is the main source of job insecurity.
Valetta (1999) also presented results from a generalized logit
model, his primary motivation being that the economic theory
describing turnover implies that other reasons for leaving a job may
affect dismissal probabilities.</p>
<p>For the generalized logit model, the response variable has <span class="math inline">\(c = 5\)</span>
categories: dismissal, left job because of plant closures, “quit,”
changed jobs for other reasons and no change in employment. The
“no change in employment” category is the omitted one in <a href="C11Binary.html#Tab116">Table 11.6</a>. The explanatory variables of the
generalized logit are the same as the probit regression; the
estimates summarized in Table <a href="C11Binary.html#tab:Tab111">11.1</a> are
reproduced here for convenience.</p>
<p><a href="C11Binary.html#Tab116">Table 11.6</a> shows that turnover declines as
tenure increases. To illustrate, consider a typical man in the 1992
sample where we have time = 16 and focus on dismissal probabilities.
For this value of time, the coefficient associated with tenure for
dismissal is -0.221 + 16 (0.008) = -0.093 (due to the interaction
term). From this, we interpret an additional year of tenure to imply
that the dismissal probability is exp(-0.093) = 91% of what it
would be otherwise, representing a decline of 9%.</p>
<p><a href="C11Binary.html#Tab116">Table 11.6</a> also shows that the generalized
coefficients associated with dismissal are similar to the probit
fits.</p>
<p>The standard errors are also qualitatively similar, although higher
for the generalized logits when compared to the probit model. In
particular, we again see that the coefficient associated with the
interaction between tenure and time trend reveals an increasing
dismissal rate for experienced workers. The same is true for the
rate of quitting.</p>
<p><a id=Tab116></a></p>
<p><span id="Tab116">Table 11.6</span>. <strong>Turnover Generalized Logit and Probit Regression Estimates</strong></p>
<p><span class="math display">\[
\small{
\begin{array}{lcrrrr}
\hline
&amp; \textbf{Probit} &amp; &amp;\textbf{Generalized} &amp;\textbf{Logit} &amp;\textbf{Model} \\
&amp; \text{Regression} &amp; &amp; \text{Plant} &amp; \text{Other} &amp; \\
\text{Variable} &amp; \text{Model} &amp; \text{Dismissal} &amp; \text{closed} &amp; \text{reason} &amp; \text{Quit} \\
&amp; \text{(Dismissal)}\\
\hline
\text{Tenure  }&amp; -0.084  &amp;  -0.221  &amp;  -0.086   &amp; -0.068  &amp;  -0.127 \\
   &amp;     (0.010)  &amp; (0.025)  &amp; (0.019)  &amp; (0.020)  &amp; (0.012) \\
\text{Time Trend } &amp;  -0.002  &amp;  -0.008  &amp;  -0.024  &amp;  0.011  &amp;   -0.022 \\
            &amp;  (0.005)  &amp; (0.011)  &amp; (0.016)  &amp; (0.013) &amp;  (0.007)
            \\
\text{Tenure (Time Trend) } &amp;    0.003  &amp;   0.008   &amp;  0.004  &amp;   -0.005
&amp; 0.006 \\
  &amp;    (0.001)  &amp; (0.002) &amp;  (0.001) &amp;  (0.002) &amp;  (0.001) \\
\text{Change in Logarithmic}   &amp;  0.094  &amp;   0.286   &amp;
0.459  &amp; -0.022  &amp; 0.333 \\
~~\text{Sector Employment}    &amp;  (0.057)  &amp; (0.123)  &amp; (0.189)  &amp; (0.158)  &amp; (0.082) \\
\text{Tenure  (Change in Logarithmic}&amp; -0.020
&amp; -0.061 &amp;  -0.053  &amp;  -0.005   &amp; -0.027 \\
~~\text{Sector Employment)} &amp;    (0.009)  &amp;  (0.023)  &amp;  (0.025)  &amp; (0.025)  &amp; (0.012) \\
\hline
\end{array}
}
\]</span>
<em>Notes</em>: <em>Standard errors in parentheses. Omitted category is no change in employment for the generalized logit. Other variables controlled for
consist of education, marital status, number of children, race,
years of full-time work experience and its square, union membership,
government employment, logarithmic wage, the U.S. employment rate
and location.</em></p>
</div>
</div>
<div id="S:Sec1152" class="section level3 hasAnchor" number="11.5.2">
<h3><span class="header-section-number">11.5.2</span> Multinomial Logit<a href="C11Binary.html#S:Sec1152" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to equation <a href="C11Binary.html#eq:eq116">(11.6)</a>, an alternative
linear combination of explanatory variables is
<span class="math display" id="eq:eq118">\[\begin{equation}
V_{i,j} = \mathbf{x}_{i,j}^{\prime} \boldsymbol \beta,
\tag{11.8}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{x}_{i,j}\)</span> is a vector of explanatory variables that
depends on the <span class="math inline">\(j\)</span>th alternative whereas the parameters
<span class="math inline">\(\boldsymbol \beta\)</span> do not. Using the expressions in equations
<a href="C11Binary.html#eq:eq117">(11.7)</a> and <a href="C11Binary.html#eq:eq118">(11.8)</a>
forms the basis of the <em>multinomial logit</em> model, also known as
the <em>conditional logit</em> model (McFadden, 1974). With this
specification, the total log-likelihood is
<span class="math display">\[
L = \sum_{i=1}^n \sum_{j=1}^c y_{i,j}~ \mathrm{ln}~ \pi_{i,j} =
\sum_{i=1}^n \left[ \sum_{j=1}^c y_{i,j} \mathbf{x}_{i,j}^{\prime}
\boldsymbol \beta \ - \mathrm{ln} \left(\sum_{k=1}^c
\mathrm{exp}(\mathbf{x}_{i,k}^{\prime} \boldsymbol \beta)  \right)
\right].
\]</span>
This straightforward expression for the likelihood enables maximum
likelihood inference to be easily performed.</p>
<p>The generalized logit model is a special case of the multinomial
logit model. To see this, consider explanatory variables
<span class="math inline">\(\mathbf{x}_i\)</span> and parameters <span class="math inline">\(\boldsymbol \beta_j\)</span>, each of
dimension <span class="math inline">\(k\times 1\)</span>. Define
<span class="math display">\[
\mathbf{x}_{i,j} = \left(
\begin{array}{c}
\mathbf{0} \\ \vdots \\ \mathbf{0} \\ \mathbf{x}_i \\ \mathbf{0} \\
\vdots \\ \mathbf{0} \\
\end{array}\right) ~~~ \mathrm{and}~~~
\boldsymbol \beta = \left(
\begin{array}{c}
\boldsymbol \beta_1 \\ \boldsymbol \beta_2 \\
\vdots \\
\boldsymbol \beta_c \\
\end{array} \right).
\]</span>
Specifically, <span class="math inline">\(\mathbf{x}_{i,j}\)</span> is defined as <span class="math inline">\(j-1\)</span> zero vectors
(each of dimension <span class="math inline">\(k\times 1\)</span>), followed by <span class="math inline">\(\mathbf{x}_i\)</span> and
then followed by <span class="math inline">\(c-j\)</span> zero vectors. With this specification, we
have <span class="math inline">\(\mathbf{x}_{i,j}^{\prime} \boldsymbol \beta =\mathbf{x}_i^{\prime} \boldsymbol \beta_j\)</span>. Thus, a statistical
package that performs multinomial logit estimation can also perform
generalized logit estimation through the appropriate coding of
explanatory variables and parameters. Another consequence of this
connection is that some authors use the descriptor multinomial logit
when referring to the generalized logit model.</p>
<p>Moreover, through similar coding schemes, multinomial logit models
can also handle linear combinations of the form:
<span class="math display">\[
V_i = \mathbf{x}_{i,1,j}^{\prime} \boldsymbol \beta +
\mathbf{x}_{i,2}^{\prime} \boldsymbol \beta_j .
\]</span>
Here, <span class="math inline">\(\mathbf{x}_{i,1,j}\)</span> are explanatory variables that depend on
the alternative whereas <span class="math inline">\(\mathbf{x}_{i,2}\)</span> do not. Similarly,
<span class="math inline">\(\boldsymbol \beta_j\)</span> are parameters that depend on the alternative
whereas <span class="math inline">\(\boldsymbol \beta\)</span> do not. This type of linear combination
is the basis of a <em>mixed logit model</em>. As with conditional
logits, it is customary to choose one set of parameters as the
baseline and specify <span class="math inline">\(\boldsymbol \beta_c = \mathbf{0}\)</span> to avoid
redundancies.</p>
<p>To interpret parameters for the multinomial logit model, we may
compare alternatives <span class="math inline">\(h\)</span> and <span class="math inline">\(k\)</span> using equations
<a href="C11Binary.html#eq:eq117">(11.7)</a> and <a href="C11Binary.html#eq:eq118">(11.8)</a>,
to get
<span class="math display">\[
\mathrm{ln}~ \frac{\mathrm{Pr} \left(y_i = h \right)} {\mathrm{Pr}
\left(y_i = k \right)} = (\mathbf{x}_{i,h}-\mathbf{x}_{i,k})
^{\prime} \boldsymbol \beta  .
\]</span>
Thus, we may interpret <span class="math inline">\(\beta_j\)</span> as the proportional change in the
odds ratio, where the change is the value of the <span class="math inline">\(j\)</span>th explanatory
variable, moving from the <span class="math inline">\(k\)</span>th to the <span class="math inline">\(h\)</span>th alternative.</p>
<p>With equation <a href="C11Binary.html#eq:eq117">(11.7)</a>, note that
<span class="math inline">\(\pi_{i,1} / \pi_{i,2} = \mathrm{exp}(V_{i,1}) /\mathrm{exp}(V_{i,2})\)</span>. This ratio does not depend on the underlying values of the other alternatives, <span class="math inline">\(V_{i,j}\)</span>, for <span class="math inline">\(j=3, \ldots, c\)</span>. This feature, called the <em>independence of irrelevant alternatives</em>, can be a drawback of the multinomial logit model for some applications.</p>
<hr />
<p><strong>Example: Choice of Health Insurance.</strong> To illustrate, Browne and Frees (2007) examined <span class="math inline">\(c=4\)</span> health insurance choices, consisting of:</p>
<ul>
<li><span class="math inline">\(y=1\)</span> - an individual covered by group insurance,</li>
<li><span class="math inline">\(y=2\)</span> - an individual covered by private, non-group insurance,</li>
<li><span class="math inline">\(y=3\)</span> - an individual covered by government, but not private insurance or</li>
<li><span class="math inline">\(y=4\)</span> - an individual not covered by health insurance.</li>
</ul>
<p>Their data on health insurance coverage came from the March
supplement of the Current Population Survey (CPS), conducted by the
Bureau of Labor Statistics. Browne and Frees (2007) analyzed
approximately 10,800 single person households per year, covering
1988-1995, yielding <span class="math inline">\(n=86,475\)</span> observations. They examined whether
underwriting restrictions, laws passed to prohibit insurers from
discrimination, facilitate or discourage consumption of health
insurance. They focused on disability laws that prohibited insurers from
using physical impairment (disability) as an underwriting criterion.</p>
<p>Table <a href="C11Binary.html#tab:Tab117">11.7</a> suggests that disability laws have
little effect on the average health insurance purchasing behavior.
To illustrate, for individuals surveyed with disability laws in
effect, 57.6% purchased group health compared to 59.3% of those
where restrictions were not in effect. Similarly, 19.9% were
uninsured when disability restrictions were in effect compared to
20.1% when they were not. In terms of odds, when disability
restrictions were in effect, the odds of purchasing group health
insurance compared to becoming uninsured are 57.6/19.9 = 2.895. When
disability restrictions were not in effect, the odds are 2.946. The
odds ratio, 2.895/2.946 = 0.983, indicates that there is little
change in the odds when comparing whether or not disability
restrictions were in effect.</p>
<h5 style="text-align: center;">
<a id="displayCode.Table11.2Silly" href="javascript:togglecode('toggleCode.Table11.2Silly','displayCode.Table11.2Silly');"><i><strong></strong></i></a>
</h5>
<div id="toggleCode.Table11.2Silly" style="display: none">
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="C11Binary.html#cb99-1" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="dv">2</span>, <span class="at">caption =</span> <span class="st">&quot;Silly. Create a table just to update the counter...&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-157">Table 11.6: </span>Silly. Create a table just to update the counter…</caption>
<thead>
<tr class="header">
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
</tr>
</tbody>
</table>
</div>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab117">Table 11.7: </span><strong>Percentages of Health Coverage by Law Variable</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Disability Law in Effect
</th>
<th style="text-align:center;">
Number
</th>
<th style="text-align:center;">
Uninsured
</th>
<th style="text-align:center;">
Non-group
</th>
<th style="text-align:left;">
Government
</th>
<th style="text-align:center;">
Group
</th>
<th style="text-align:center;">
Odds-Comparing Group to Uninsured
</th>
<th style="text-align:center;">
Odds Ratio
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
No
</td>
<td style="text-align:center;width: 1.3cm; ">
82246
</td>
<td style="text-align:center;width: 1.3cm; ">
20.1
</td>
<td style="text-align:center;width: 1.3cm; ">
12.2
</td>
<td style="text-align:left;width: 1.3cm; ">
8.4
</td>
<td style="text-align:center;width: 1.3cm; ">
59.3
</td>
<td style="text-align:center;width: 1.3cm; ">
2.946
</td>
<td style="text-align:center;width: 1.3cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Yes
</td>
<td style="text-align:center;width: 1.3cm; ">
4229
</td>
<td style="text-align:center;width: 1.3cm; ">
19.9
</td>
<td style="text-align:center;width: 1.3cm; ">
10.1
</td>
<td style="text-align:left;width: 1.3cm; ">
12.5
</td>
<td style="text-align:center;width: 1.3cm; ">
57.6
</td>
<td style="text-align:center;width: 1.3cm; ">
2.895
</td>
<td style="text-align:center;width: 1.3cm; ">
0.983
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Total
</td>
<td style="text-align:center;width: 1.3cm; ">
86475
</td>
<td style="text-align:center;width: 1.3cm; ">
20.1
</td>
<td style="text-align:center;width: 1.3cm; ">
12.1
</td>
<td style="text-align:left;width: 1.3cm; ">
8.6
</td>
<td style="text-align:center;width: 1.3cm; ">
59.2
</td>
<td style="text-align:center;width: 1.3cm; ">
</td>
<td style="text-align:center;width: 1.3cm; ">
</td>
</tr>
</tbody>
</table>
<p>In contrast, Table <a href="C11Binary.html#tab:Tab118">11.8</a> suggests disability laws
may have important effects on the average health insurance
purchasing behavior of selected subgroups of the sample. Table
<a href="C11Binary.html#tab:Tab118">11.8</a> shows the percent uninsured and odds of
purchasing group insurance (compared to being uninsured) for
selected subgroups. To illustrate, for disabled individuals, the
odds of purchasing group insurance are 1.329 times higher when
disability restrictions are in effect. Table <a href="C11Binary.html#tab:Tab117">11.7</a> suggests that disability restrictions have
no effect; this may be true when looking at the entire sample.
However, by examining subgroups, Table <a href="C11Binary.html#tab:Tab118">11.8</a>
shows that we may see important effects associated with legal
underwriting restrictions that are not evident when looking at
averages over the whole sample.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab118">Table 11.8: </span><strong>Odds of Health Coverage by Law and Physical Impairment</strong>
</caption>
<thead>
<tr>
<th style="text-align:right;">
Selected Subgroups
</th>
<th style="text-align:right;">
Disability Law in Effect
</th>
<th style="text-align:right;">
Number
</th>
<th style="text-align:right;">
Percentage Group
</th>
<th style="text-align:right;">
Percentage Uninsured
</th>
<th style="text-align:right;">
Odds-Comparing Group to Uninsured
</th>
<th style="text-align:right;">
Odds Ratio
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;width: 2.5cm; border-right:1px solid;">
Nondisabled
</td>
<td style="text-align:right;width: 1.4cm; ">
No
</td>
<td style="text-align:right;width: 1.4cm; ">
72150
</td>
<td style="text-align:right;width: 1.4cm; ">
64.2
</td>
<td style="text-align:right;width: 1.4cm; ">
20.5
</td>
<td style="text-align:right;width: 1.4cm; ">
3.134
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
</tr>
<tr>
<td style="text-align:right;width: 2.5cm; border-right:1px solid;">
Nondisabled
</td>
<td style="text-align:right;width: 1.4cm; ">
Yes
</td>
<td style="text-align:right;width: 1.4cm; ">
3649
</td>
<td style="text-align:right;width: 1.4cm; ">
63.4
</td>
<td style="text-align:right;width: 1.4cm; ">
21.2
</td>
<td style="text-align:right;width: 1.4cm; ">
2.985
</td>
<td style="text-align:right;width: 1.4cm; ">
0.952
</td>
</tr>
<tr>
<td style="text-align:right;width: 2.5cm; border-right:1px solid;">
Disabled
</td>
<td style="text-align:right;width: 1.4cm; ">
No
</td>
<td style="text-align:right;width: 1.4cm; ">
10096
</td>
<td style="text-align:right;width: 1.4cm; ">
24.5
</td>
<td style="text-align:right;width: 1.4cm; ">
17.6
</td>
<td style="text-align:right;width: 1.4cm; ">
1.391
</td>
<td style="text-align:right;width: 1.4cm; ">
</td>
</tr>
<tr>
<td style="text-align:right;width: 2.5cm; border-right:1px solid;">
Disabled
</td>
<td style="text-align:right;width: 1.4cm; ">
Yes
</td>
<td style="text-align:right;width: 1.4cm; ">
580
</td>
<td style="text-align:right;width: 1.4cm; ">
21
</td>
<td style="text-align:right;width: 1.4cm; ">
11.4
</td>
<td style="text-align:right;width: 1.4cm; ">
1.848
</td>
<td style="text-align:right;width: 1.4cm; ">
1.329
</td>
</tr>
</tbody>
</table>
<p>There are many ways of picking subgroups of interest. With a large
dataset of <span class="math inline">\(n=86,475\)</span> observations, one could probably pick
subgroups to confirm almost any hypothesis. Further, there is a
concern that the CPS data may not provide a representative sample of
state populations. Thus, it is customary to use regression
techniques to “control” for explanatory variables, such as
physical impairment.</p>
<p>Table <a href="C11Binary.html#tab:Tab119">11.9</a> reports the main results from a
multinomial logit model with many control variables included. A
dummy variable for each of 50 states was included (the District of
Columbia is a “state” in this data set, so we need <span class="math inline">\(51-1=50\)</span> dummy
variables). These variables were suggested in the literature and are
further described in Browne and Frees (2007). They include an
individual’s gender, marital status, race, education, whether or not
self-employed and whether an individual worked full-time, part-time
or not at all.</p>
<p>In Table <a href="C11Binary.html#tab:Tab119">11.9</a>, “Law” refers to the binary
variable that is 1 if a legal restriction was in effect and
“Disabled” is a binary variable that is 1 if an
individual is physically impaired. Thus, the interaction
“Law*Disabled” reports the effect of a legal restriction on a
physically impaired individual. The interpretation is similar to
Table <a href="C11Binary.html#tab:Tab118">11.8</a>. Specifically, we interpret the
coefficient 1.419 to mean that disabled individuals are 41.9% more
likely to purchase group health insurance compared to purchasing no
insurance, when the disability underwriting restriction is in
effect. Similarly, non-disabled individuals are 21.2% (<span class="math inline">\(=1/0.825 - 1\)</span>) less likely to purchase group health insurance compared to
purchasing no insurance, when the disability underwriting
restriction is in effect. This result suggests that the non-disabled
are more likely to be uninsured as a result of prohibitions on the
use of disability status as an underwriting criteria. Overall, the
results are statistically significant, confirming that this legal
restriction does have an impact on the consumption of health
insurance.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab119">Table 11.9: </span><strong>Odds Ratios from Multinomial Logit Regression Model</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Group versus Uninsured
</th>
<th style="text-align:right;">
Nongroup versus Uninsured
</th>
<th style="text-align:right;">
Government versus Uninsured
</th>
<th style="text-align:right;">
Group versus Nongroup
</th>
<th style="text-align:right;">
Group versus Government
</th>
<th style="text-align:right;">
Nongroup versus Governement
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 3.5cm; ">
Law <span class="math inline">\(\times\)</span> Nondisabled
</td>
<td style="text-align:right;width: 1.4cm; ">
0.825
</td>
<td style="text-align:right;width: 1.4cm; ">
1.053
</td>
<td style="text-align:right;width: 1.4cm; ">
1.010
</td>
<td style="text-align:right;width: 1.4cm; ">
0.784
</td>
<td style="text-align:right;width: 1.4cm; ">
0.818
</td>
<td style="text-align:right;width: 1.4cm; ">
1.043
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 3.5cm; ">
<span class="math inline">\(p\)</span>-Value
</td>
<td style="text-align:right;width: 1.4cm; ">
0.001
</td>
<td style="text-align:right;width: 1.4cm; ">
0.452
</td>
<td style="text-align:right;width: 1.4cm; ">
0.900
</td>
<td style="text-align:right;width: 1.4cm; ">
0.001
</td>
<td style="text-align:right;width: 1.4cm; ">
0.023
</td>
<td style="text-align:right;width: 1.4cm; ">
0.677
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 3.5cm; ">
Law <span class="math inline">\(\times\)</span> Disabled
</td>
<td style="text-align:right;width: 1.4cm; ">
1.419
</td>
<td style="text-align:right;width: 1.4cm; ">
0.953
</td>
<td style="text-align:right;width: 1.4cm; ">
1.664
</td>
<td style="text-align:right;width: 1.4cm; ">
1.490
</td>
<td style="text-align:right;width: 1.4cm; ">
0.854
</td>
<td style="text-align:right;width: 1.4cm; ">
0.573
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 3.5cm; ">
<span class="math inline">\(p\)</span>-Value
</td>
<td style="text-align:right;width: 1.4cm; ">
0.062
</td>
<td style="text-align:right;width: 1.4cm; ">
0.789
</td>
<td style="text-align:right;width: 1.4cm; ">
0.001
</td>
<td style="text-align:right;width: 1.4cm; ">
0.079
</td>
<td style="text-align:right;width: 1.4cm; ">
0.441
</td>
<td style="text-align:right;width: 1.4cm; ">
0.001
</td>
</tr>
</tbody>
</table>
<p><em>Notes: The regression includes 150 (<span class="math inline">\(=50 \times 3\)</span>) state-specific effects, several continuous variables (age, education and income, as well as higher order terms) and categorical variables (such as race and year).</em></p>
</div>
<div id="S:Sec1153" class="section level3 hasAnchor" number="11.5.3">
<h3><span class="header-section-number">11.5.3</span> Nested Logit<a href="C11Binary.html#S:Sec1153" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To mitigate the problem of independence of irrelevant alternatives
in multinomial logits, we now introduce a type of hierarchical model
known as a <em>nested logit</em> model. To interpret the nested logit
model, in the first stage one chooses an alternative (say the first
alternative) with probability
<span class="math display" id="eq:eq119">\[\begin{equation}
\pi_{i,1} = \mathrm{Pr}(y_i = 1) =
\frac{\mathrm{exp}(V_{i,1})}{\mathrm{exp}(V_{i,1})+ \left[
\sum_{k=2}^c \mathrm{exp}(V_{i,k}/ \rho) \right]^{\rho}}  .
\tag{11.9}
\end{equation}\]</span>
Then, conditional on not choosing the first alternative, the
probability of choosing any one of the other alternatives follows a
multinomial logit model with probabilities
<span class="math display" id="eq:eq1110">\[\begin{equation}
\frac{\pi_{i,j}}{1-\pi_{i,1}} = \mathrm{Pr}(y_i = j | y_i \neq 1) =
\frac{\mathrm{exp}(V_{i,j}/ \rho)}{\sum_{k=2}^c
\mathrm{exp}(V_{i,k}/ \rho) }, ~~~j=2, \ldots, c .
\tag{11.10}
\end{equation}\]</span>
In equations <a href="C11Binary.html#eq:eq119">(11.9)</a> and <a href="C11Binary.html#eq:eq1110">(11.10)</a>,
the parameter <span class="math inline">\(\rho\)</span> measures the association among the choices <span class="math inline">\(j = 2, \ldots, c\)</span>. The value of <span class="math inline">\(\rho=1\)</span> reduces to the multinomial logit model that we interpret to mean independence of irrelevant
alternatives. We also interpret Prob(<span class="math inline">\(y_i = 1\)</span>) to be a weighted
average of values from the first choice and the others. Conditional
on not choosing the first category, the form of <span class="math inline">\(\mathrm{Pr}(y_i = j| y_i \neq 1)\)</span> in equation <a href="C11Binary.html#eq:eq1110">(11.10)</a> has the same
form as the multinomial logit.</p>
<p>The advantage of the nested logit is that it generalizes the
multinomial logit model in a way such that we no longer have the
problem of independence of irrelevant alternatives. A disadvantage,
pointed out by McFadden (1981), is that only one choice is observed;
thus, we do not know which category belongs in the first stage of
the nesting without additional theory regarding choice behavior.
Nonetheless, the nested logit generalizes the multinomial logit by
allowing alternative “dependence” structures. That is, one may
view the nested logit as a robust alternative to the multinomial
logit and examine each one of the categories in the first stage of
the nesting.</p>
</div>
</div>
<div id="S:Sec116" class="section level2 hasAnchor" number="11.6">
<h2><span class="header-section-number">11.6</span> Ordinal Dependent Variables<a href="C11Binary.html#S:Sec116" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now consider a response that is an ordered categorical variable,
also known as an <em>ordinal</em> dependent variable. To illustrate,
any type of survey response where you score your impression on a
seven point scale ranging from “very dissatisfied” to “very
satisfied” is an example of an ordinal variable.</p>
<hr />
<p><strong>Example: Health Plan Choice.</strong> Pauly and Herring (2007) examined <span class="math inline">\(c=4\)</span> choices of health
care plan types, consisting of:</p>
<ul>
<li><span class="math inline">\(y=1\)</span> - a health maintenance organization (HMO),</li>
<li><span class="math inline">\(y=2\)</span> - a point of service (POS) plan,</li>
<li><span class="math inline">\(y=3\)</span> - a preferred provider organization (PPO) or</li>
<li><span class="math inline">\(y=4\)</span> - a fee for service (FFS) plan.</li>
</ul>
<p>A FFS plan is the least restrictive, allowing enrollees to see
health care providers (such as primary care physicians) for a fee
reflecting the cost of services rendered. The PPO plan is the next
least restrictive; this plan generally uses FFS payments but
enrollees generally must choose from a list of “preferred
providers.” Pauly and Herring (2007) took POS and HMO plans to be
the third and fourth least restrictive, respectively. An HMO often
uses capitation (a flat rate per person) to reimburse providers,
restricting enrollees to a network of providers. In contrast, a POS
plan gives enrollees the option to see providers outside of the HMO
network (for an additional fee).</p>
<div id="cumulative-logit" class="section level3 hasAnchor" number="11.6.1">
<h3><span class="header-section-number">11.6.1</span> Cumulative Logit<a href="C11Binary.html#cumulative-logit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Models of ordinal dependent variables are based on cumulative
probabilities of the form
<span class="math display">\[
\mathrm{Pr} ( y  \le j ) = \pi_1 + \cdots + \pi_j, ~ ~ j=1, \ldots,
c .
\]</span>
In this section, we use <em>cumulative logits</em>
<span class="math display" id="eq:eq1111">\[\begin{equation}
\mathrm{logit}\left(\mathrm{Pr} ( y  \le j ) \right) = \mathrm{ln}
\left(\frac{\Pr ( y  \le j )}{1-\Pr ( y  \le j )}
\right)
= \mathrm{ln} \left(\frac{\pi_1 + \cdots + \pi_j}{\pi_{j+1} + \cdots
+ \pi_c}
\right) .
\tag{11.11}
\end{equation}\]</span></p>
<p>The simplest cumulative logit model is
<span class="math display">\[
\mathrm{logit}\left(\Pr ( y  \le j ) \right) = \alpha_j
\]</span>
that does not use any explanatory variables. The “cut-point”
parameters <span class="math inline">\(\alpha_j\)</span> are nondecreasing so that <span class="math inline">\(\alpha_1 \le \alpha_2 \le \ldots \le \alpha_c,\)</span> reflecting the cumulative
nature of the distribution function <span class="math inline">\(\mathrm{Pr} ( y \le j )\)</span>.</p>
<p>The <em>proportional odds model</em> incorporates explanatory
variables. With this model, cumulative logits are expressed as
<span class="math display" id="eq:eq1112">\[\begin{equation}
\mathrm{logit}\left(\Pr ( y  \le j ) \right)  = \alpha_j +
\mathbf{x}_i^{\prime} \boldsymbol \beta .
\tag{11.12}
\end{equation}\]</span>
This model provides parameter interpretations similar to those for
logistic regression described in Section
<a href="C11Binary.html#S:Sec114">11.4</a>. For example, if the variable <span class="math inline">\(x_1\)</span> is
continuous, then as in equation <a href="C11Binary.html#eq:eq111">(11.1)</a> we
have
<span class="math display">\[
\beta_1 = \frac{\partial }{\partial x_{i1}}\left( \alpha_j +
\mathbf{x}_i^{\prime}\boldsymbol \beta \right) =
\frac{\frac{\partial }{\partial x_{i1}}\Pr (y_i  \le
j|\mathbf{x}_i)/\left( 1-\Pr (y_i \le j|\mathbf{x}_i)\right) }{\Pr
(y_i \le j|\mathbf{x}_i)/\left( 1-\Pr (y_i \le
j|\mathbf{x}_i)\right) }.
\]</span>
Thus, we may interpret <span class="math inline">\(\beta_1\)</span> as the proportional change in the
cumulative odds ratio.</p>
<hr />
<p><strong>Example: Health Plan Choice - Continued.</strong> Pauly and Herring
used data from the 1996-1997 and 1998-1999 Community Tracking
Study’s Household Surveys (CTS-HS) to study the demand for health
insurance. This is a nationally representative survey containing
over 60,000 individuals per period. As one measure of demand, Pauly
and Herring examined health plan choice, reasoning that individuals
that chose (through employment or association membership) less
restrictive plans sought greater protection for health care. (They
also looked at other measures, including the number of restrictions
placed on plans and the amount of cost-sharing.)
<a href="C11Binary.html#Tab1110">Table 11.10</a> provides determinants of health plan choice
based on <span class="math inline">\(n=34,486\)</span> individuals who had group health insurance, aged
18-64 without public insurance. Pauly and Herring also compared
these results to those who had individual health insurance to
understand the differences in determinants between these two
markets.</p>
<p><a id=Tab1110></a></p>
<p><span id="Tab1110">Table 11.10</span>. <strong>Cumulative Logit Model of Health Plan Choice</strong></p>
<p><span class="math display">\[
\small{
\begin{array}{llll}
\hline \textbf{Variable}  &amp; \textbf{Odds Ratio} &amp; \textbf{Variable}  &amp; \textbf{Odds Ratio} \\
\hline \text{Age} &amp; 0.992^{***} &amp; \text{Hispanic} &amp; 1.735^{***} \\
\text{Female} &amp; 1.064^{***} &amp; \text{Risk taker} &amp; 0.967 \\
\text{Family size} &amp; 0.985 &amp; \text{Smoker} &amp; 1.055^{***} \\
\text{Family income} &amp; 0.963^{***} &amp; \text{Fair/poor health} &amp; 1.056 \\
\text{Education} &amp; 1.006 &amp; \alpha_1 &amp; 0.769^{***} \\
\text{Asian} &amp; 1.180^{***} &amp; \alpha_2 &amp; 1.406^{***} \\
\text{African-American} &amp; 1.643^{***} &amp; \alpha_3 &amp; 12.089^{***} \\
\text{Maximum-rescaled } R^2 &amp; 0.102 &amp; \\
\hline
\end{array}
}
\]</span>
<em>Notes: Source: Pauly and Herring (2007).</em> <span class="math inline">\(^{***}\)</span> <em>indicates that the associated <span class="math inline">\(p\)</span>-values are less than 0.01. For race, Caucasian is the omitted variable.</em></p>
<p>To interpret the odds ratios in <a href="C11Binary.html#Tab1110">Table 11.10</a>, we
first note that the cut-point estimates, corresponding to <span class="math inline">\(\alpha_1,\)</span> <span class="math inline">\(\alpha_2\)</span> and <span class="math inline">\(\alpha_3\)</span>,
increase as choices become less restrictive, as anticipated. For gender, we see that the
estimated odds for females are 1.064 times that of males in the
direction of choosing a less restrictive health plan. Controlling
for other variables, females are more likely to choose less
restrictive plans than males. Similarly, younger, less wealthy,
non-Caucasian and smokers are more likely to choose less restrictive
plans. Coefficients associated with family size, education, risk
taking and self reported health were not statistically significant
in this fitted model.</p>
</div>
<div id="cumulative-probit" class="section level3 hasAnchor" number="11.6.2">
<h3><span class="header-section-number">11.6.2</span> Cumulative Probit<a href="C11Binary.html#cumulative-probit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As in Section <a href="C11Binary.html#S:Sec1122">11.2.2</a> for logistic regression,
cumulative logit models have a threshold interpretation.
Specifically, let <span class="math inline">\(y_i^{\ast}\)</span> be a latent, unobserved, random
variable upon which we base the observed dependent variable as
<span class="math display">\[
y_i=\left\{
\begin{array}{cc}
1 &amp; y_i^{\ast}  \le \alpha_1 \\
2 &amp; \alpha_1 &lt; y_i^{\ast}  \le \alpha_2 \\
\vdots &amp; \vdots \\
c-1 &amp; \alpha_{c-2} &lt; y_i^{\ast}  \le \alpha_{c-1} \\
c &amp; \alpha_{c-1} &lt; y_i^{\ast}\\
\end{array}
\right. .
\]</span>
If <span class="math inline">\(y_i^{\ast} - \mathbf{x}_i^{\prime}\boldsymbol \beta\)</span> has a
logistic distribution, then
<span class="math display">\[
\Pr(y_i^{\ast} - \mathbf{x}_i^{\prime}\boldsymbol \beta  \le
a)=\frac{1}{1+\exp (-a)}
\]</span>
and thus
<span class="math display">\[
\Pr(y_i  \le j ) = \Pr(y_i^{\ast}  \le \alpha_j) =\frac{1}{1+\exp
\left( -(\alpha_j - \mathbf{x}_i^{\prime}\boldsymbol \beta)
\right)}.
\]</span>
Applying the logit transform to both sides yields equation
<a href="C11Binary.html#eq:eq1112">(11.12)</a>.</p>
<p>Alternatively, assume that <span class="math inline">\(y_i^{\ast} - \mathbf{x}_i^{\prime}\boldsymbol \beta\)</span> has a standard normal
distribution. Then,
<span class="math display">\[
\Pr(y_i  \le j ) = \Pr(y_i^{\ast}  \le \alpha_j) =\Phi \left(
\alpha_j - \mathbf{x}_i^{\prime}\boldsymbol \beta \right).
\]</span>
This is the <em>cumulative probit</em> model. As with binary variable
models, the cumulative probit gives results that are similar to the
cumulative logit model.</p>
</div>
</div>
<div id="S:Sec117" class="section level2 hasAnchor" number="11.7">
<h2><span class="header-section-number">11.7</span> Further Reading and References<a href="C11Binary.html#S:Sec117" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Regression models of binary variables are used extensively. For more
detailed introductions, see Hosmer and Lemshow (1989) or Agresti
(1996). You may also wish to examine more rigorous treatments such
as those in Agresti (1990) and Cameron and Trivedi (1998). The work
by Agresti (1990, 1996) discuss multicategory dependent variables,
as does the advanced econometrics treatment in Amemiya (1985).</p>
<p><strong>Chapter References</strong></p>
<ul>
<li>Agresti, Alan (1990). <em>Categorical Data Analysis</em>. Wiley, New York.</li>
<li>Agresti, Alan (1996). <em>An Introduction to Categorical Data Analysis</em>. Wiley, New York.</li>
<li>Amemiya, Takeshi (1985). <em>Advanced Econometrics</em>. Harvard University Press, Cambridge, Massachusetts.</li>
<li>Browne, Mark J. and Edward W. Frees (2007). Prohibitions on health insurance underwriting. Working paper.</li>
<li>Cameron, A. Colin and Pravin K. Trivedi (1998). <em>Regression Analysis of Count Data</em>. Cambridge University Press, Cambridge.</li>
<li>Carroll, Raymond J. and David Ruppert (1988). <em>Transformation and Weighting in Regression</em>. Chapman-Hall.</li>
<li>Gourieroux, Christian and Joann Jasiak (2007). <em>The Econometrics of Individual Risk</em>. Princeton University Press, Princeton.</li>
<li>Hand, D.J. and W. E. Henley (1997). Statistical classification methods in consumer credit scoring: A review. <em>Journal of the Royal Statistical Society A</em>, 160(3), 523-541.</li>
<li>Hosmer, David W. and Stanley Lemeshow (1989). <em>Applied Logistic Regression</em>. Wiley, New York.</li>
<li>Pauly, Mark V. and Bradley Herring (2007). The demand for health insurance in the group setting: Can you always get what you want? <em>Journal of Risk and Insurance</em> 74, 115-140.</li>
<li>Smith, Richard M. and Phyllis Schumacher (2006). Academic attributes of college freshmen that lead to success in actuarial studies in a business college. <em>Journal of Education for Business</em> 81(5), 256-260.</li>
<li>Valletta, R. G. (1999). Declining job security. <em>Journal of Labor Economics</em> 17, S170-S197.</li>
<li>Wiginton, John C. (1980). A note on the comparison of logit and discriminant models of consumer credit behavior. <em>Journal of Financial and Quantitative Analysis</em> 15(3), 757-770.</li>
</ul>
</div>
<div id="S:Sec118" class="section level2 hasAnchor" number="11.8">
<h2><span class="header-section-number">11.8</span> Exercises<a href="C11Binary.html#S:Sec118" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>11.1 <strong>Similarity of Logit and Probit</strong>. Suppose that the random
variable <span class="math inline">\(y^{\ast}\)</span> has a logit distribution function, <span class="math inline">\(\Pr(y^{\ast}  \le y) = \mathrm{F}(y) = e^y/(1+e^y).\)</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>Calculate the corresponding probability density function.</p></li>
<li><p>Use the probability density function to compute the mean
(<span class="math inline">\(\mu_y)\)</span>.</p></li>
<li><p>Compute the corresponding standard deviation (<span class="math inline">\(\sigma_y\)</span>).</p></li>
<li><p>Define the rescaled random variable <span class="math inline">\(y^{\ast \ast} =\frac{y^{\ast}-\mu_y}{\sigma_y}.\)</span> Determine the probability density function for <span class="math inline">\(y^{\ast \ast}\)</span>.</p></li>
<li><p>Plot the probability density function in part (d). Overlay this
plot with a plot of a standard normal probability density function.
(This provides a density function version of the distribution
function plots in Figure 11.1.)</p></li>
</ol>
<p>11.2 <strong>Threshold interpretation of the probit regression model</strong>.
Consider an underlying linear model, <span class="math inline">\(y_i^{\ast }=\mathbf{x}_i^{\mathbf{ \prime }}\boldsymbol \beta+\epsilon_i^{\ast }\)</span>, where
<span class="math inline">\(\epsilon_i^{\ast }\)</span> is normally distributed with mean zero and variance
<span class="math inline">\(\sigma ^{2}\)</span>. Define <span class="math inline">\(y_i=\mathrm{I}(y_i^{\ast }&gt;0),\)</span> where I(<span class="math inline">\(\cdot\)</span>) is
the indicator function. Show that <span class="math inline">\(\pi_i=\Pr (y_i=1|\mathbf{x}_i)\)</span>
<span class="math inline">\(=\mathrm{\Phi }(\mathbf{x}_i^{\mathbf{\prime }}\mathbf{\beta /\sigma })\)</span>, where
<span class="math inline">\(\mathrm{\Phi }(\cdot)\)</span> is the standard normal distribution function.</p>
<p>11.3 <strong>Random utility interpretation of the logistic regression
model</strong>.
Under the random utility interpretation, an individual with utility $
U_{ij}=u_i(V_{ij}+_{ij})$, where <span class="math inline">\(j\)</span> may be 1 or 2, selects
category corresponding to <span class="math inline">\(j=1\)</span> with probability
<span class="math display">\[\begin{eqnarray*}
\pi_i &amp;=&amp; \Pr (y_i =1)=\mathrm{\Pr }(U_{i2}&lt;U_{i1}) \\
&amp;=&amp;\mathrm{\Pr }(\epsilon _{i2}-\epsilon _{i1}&lt;V_{i1}-V_{i2}).
\end{eqnarray*}\]</span>
As in Section 11.2.3, we take <span class="math inline">\(V_{i2}=0\)</span> and
<span class="math inline">\(V_{i1}=\mathbf{x}_i^{\mathbf{\prime}}\boldsymbol \beta\)</span>. Further
suppose that the errors are from an extreme value distribution of
the form
<span class="math display">\[
\Pr (\epsilon_{ij}&lt;a)=\exp (-e^{-a}).
\]</span>
Show that the choice probability <span class="math inline">\(\pi_i\)</span> has a logit form. That is,
show
<span class="math display">\[
\pi_i=\frac{1}{1+\exp (-\mathbf{x}_i^{\mathbf{\prime }}\boldsymbol
\beta)}.
\]</span></p>
<p>11.4 <strong>Two Populations.</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>Begin with one population and assume that <span class="math inline">\(y_1, \ldots, y_n\)</span> is
an i.i.d. sample from a Bernoulli distribution with mean <span class="math inline">\(\pi\)</span>. Show
that the maximum likelihood estimator of <span class="math inline">\(\pi\)</span> is <span class="math inline">\(\overline{y}\)</span>.</p></li>
<li><p>Now consider two populations. Suppose that <span class="math inline">\(y_1, \ldots, y_{n_1}\)</span>
is an i.i.d. sample from a Bernoulli distribution with mean <span class="math inline">\(\pi_1\)</span>
and <span class="math inline">\(y_{n_1+1}, \ldots, y_{n_1+n_2}\)</span> is an i.i.d. sample from a
Bernoulli distribution with mean <span class="math inline">\(\pi_2\)</span>, where the samples are
independent of one another.</p>
<p>b(i). Show that the maximum likelihood estimator of <span class="math inline">\(\pi_2 - \pi_1\)</span>
is <span class="math inline">\(\overline{y}_2 - \overline{y}_1\)</span>.</p>
<p>b(ii). Determine the variance of the estimator in part b(i).</p></li>
<li><p>Now express the two population problem in a regression context
using one explanatory variable. Specifically, suppose that <span class="math inline">\(x_i\)</span>
only takes on the values 0 and 1. Out of the <span class="math inline">\(n\)</span> observations, <span class="math inline">\(n_1\)</span>
take on the value <span class="math inline">\(x=0\)</span>. These <span class="math inline">\(n_1\)</span> observations have an average
<span class="math inline">\(y\)</span> value of <span class="math inline">\(\overline{y}_1\)</span>. The remaining <span class="math inline">\(n_2 =n-n_1\)</span>
observations have value <span class="math inline">\(x=1\)</span> and an average <span class="math inline">\(y\)</span> value of
<span class="math inline">\(\overline{y}_2\)</span>. Using the logit case, let <span class="math inline">\(b_{0,MLE}\)</span> and
<span class="math inline">\(b_{1,MLE}\)</span> represent the maximum likelihood estimators of <span class="math inline">\(\beta_0\)</span>
and <span class="math inline">\(\beta_1\)</span>, respectively.</p>
<p>c(i). Show that the maximum likelihood estimators satisfy the
equations
<span class="math display">\[
\overline{y}_1 = \mathrm{\pi}\left(b_{0,MLE}\right)
\]</span>
and
<span class="math display">\[
\overline{y}_2 = \mathrm{\pi}\left(b_{0,MLE}+b_{1,MLE}\right).
\]</span></p>
<p>c(ii). Use part c(i) to show that the maximum likelihood estimator
for <span class="math inline">\(\beta_1\)</span> is
<span class="math inline">\(\mathrm{\pi}^{-1}(\overline{y}_2)-\mathrm{\pi}^{-1}(\overline{y}_1)\)</span>.</p>
<p>c(iii). With the notation <span class="math inline">\(\pi_1 = \mathrm{\pi}(\beta_0)\)</span> and <span class="math inline">\(\pi_2 = \mathrm{\pi}(\beta_0 +\beta_1)\)</span>, confirm that the information
matrix can be expressed as
<span class="math display">\[
\mathbf{I}(\beta_0, \beta_1)  = n_1  \pi_1 (1-\pi_1) \left(
  \begin{array}{cc}
1 &amp; 0 \\
0 &amp; 0 \\
  \end{array}
\right) + n_2 \pi_2 (1-\pi_2) \left(
  \begin{array}{cc}
1 &amp; 1 \\
1 &amp; 1 \\
  \end{array}
\right).
\]</span></p>
<p>c(iv). Use the information matrix to determine the large sample
variance of the maximum likelihood estimator for <span class="math inline">\(\beta_1\)</span>.</p></li>
</ol>
<p>11.5 <strong>Fitted Values</strong>. Let <span class="math inline">\(\widehat{y}_i = \mathrm{\pi }\left( \mathbf{x}_i^{\prime} \mathbf{b}_{MLE})\right)\)</span> denote the <span class="math inline">\(i\)</span>th fitted value for the
logit function. Assume that an intercept is used in the model so
that one of the explanatory variables <span class="math inline">\(x\)</span> is a constant equal to
one. Show that the average response is equal to the average fitted
value, that is, show
<span class="math inline">\(\overline{y} = n^{-1} \sum_{i=1}^n \widehat{y}_i\)</span>.</p>
<p>11.6 Beginning with the score equations <a href="C11Binary.html#eq:eq114">(11.4)</a>, verify the expression for the logit case in equation <a href="C11Binary.html#eq:eq115">(11.5)</a>.</p>
<p>11.7 <strong>Information Matrix</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>Beginning with the score function for the logit case in equation
<a href="C11Binary.html#eq:eq115">(11.5)</a>, show that the information matrix can be
expressed as
<span class="math display">\[
\mathbf{I}(\boldsymbol \beta) = \sum\limits_{i=1}^{n}
\sigma_i^2 \mathbf{x}_i\mathbf{x}_i^{\mathbf{\prime }},
\]</span>
where <span class="math inline">\(\sigma_i^2 = \mathrm{\pi}(\mathbf{x}_i^{\prime} \boldsymbol \beta)(1-\mathrm{\pi}(\mathbf{x}_i^{\prime}\boldsymbol \beta))\)</span>.</p></li>
<li><p>Beginning with the general score function in equation
<a href="C11Binary.html#eq:eq114">(11.4)</a>, determine the information matrix.</p></li>
</ol>
<p>11.8 <strong>Automobile injury insurance claims.</strong> Refer to the description in
Exercise 1.5.</p>
<p>We consider <span class="math inline">\(n=1,340\)</span> bodily injury liability claims from a single
state using a 2002 survey conducted by the Insurance Research
Council (IRC). The IRC is a division of the American Institute for
Chartered Property Casualty Underwriters and the Insurance Institute
of America. The survey asked participating companies to report
claims closed with payment during a designated two week period. In
this assignment, we are interested in understanding the
characteristics of the claimants who choose to be presented by an
attorney when settling their claim. Variable descriptions are given
Table <a href="C11Binary.html#tab:Tab1111">11.11</a>.</p>
<h5 style="text-align: center;">
<a id="displayCode.Table11.11Silly" href="javascript:togglecode('toggleCode.Table11.11Silly','displayCode.Table11.11Silly');"><i><strong></strong></i></a>
</h5>
<div id="toggleCode.Table11.11Silly" style="display: none">
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="C11Binary.html#cb100-1" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="dv">2</span>, <span class="at">caption =</span> <span class="st">&quot;Silly. Create a table just to update the counter...&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-158">Table 11.10: </span>Silly. Create a table just to update the counter…</caption>
<thead>
<tr class="header">
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
</tr>
</tbody>
</table>
</div>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab1111">Table 11.11: </span><strong>Bodily Injury Claims</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
<strong>Variable</strong>
</th>
<th style="text-align:left;">
<span class="math inline">\(\textbf{Description}\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
ATTORNEY
</td>
<td style="text-align:left;width: 1.8cm; width: 10cm; ">
whether the claimant is represented by an attorney (=1 if yes and =2 if no)
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
CLMAGE
</td>
<td style="text-align:left;width: 1.8cm; width: 10cm; ">
claimant’s age
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
CLMSEX
</td>
<td style="text-align:left;width: 1.8cm; width: 10cm; ">
claimant’s gender (=1 if male and =2 if female)
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
MARITAL
</td>
<td style="text-align:left;width: 1.8cm; width: 10cm; ">
claimant’s marital status), (=1 if married, =2 if single, =3 if widowed, and =4 if divorced/separated)
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SEATBELT
</td>
<td style="text-align:left;width: 1.8cm; width: 10cm; ">
whether or not the claimant was wearing a seatbelt/child restraint (=1 if yes, =2 if no, and =3 if not applicable)
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
CLMINSUR
</td>
<td style="text-align:left;width: 1.8cm; width: 10cm; ">
whether or not the driver of the claimant’s vehicle was uninsured (=1 if yes, =2 if no, and =3 if not applicable)
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
LOSS
</td>
<td style="text-align:left;width: 1.8cm; width: 10cm; ">
the claimant’s total economic loss (in thousands).
</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li><p><em>Summary Statistics</em>.</p>
<ol style="list-style-type: lower-roman">
<li><p>Calculate histograms and summary statistics of continuous explanatory variables CLMAGE and LOSS. Based on these results, create a logarithm version of LOSS, say lnLOSS.</p></li>
<li><p>Examine the means of CLMAGE, LOSS and lnLOSS by level of ATTORNEY. Do these statistics suggest that the continuous variables differ by ATTORNEY?</p></li>
<li><p>Create tables of counts (or percentages) of ATTORNEY by level of CLMSEX, MARITAL, SEATBELT, and CLMINSUR. Do these statistics suggest that the categorial variables differ by ATTORNEY?</p></li>
<li><p>Identify the number of missing values for each explanatory variable.</p></li>
</ol></li>
<li><p><em>Logistic Regression Models.</em></p>
<ol style="list-style-type: lower-roman">
<li><p>Run a logistic regression model using only the explanatory
variable CLMSEX. Is it an important factor in determining the use of
an attorney? Provide an interpretation in terms of the odds of using
an attorney.</p></li>
<li><p>Run a logistic regression model using the explanatory variables
CLMAGE, CLMSEX, MARITAL, SEATBELT, and CLMINSUR. Which variables
appear to be statistically significant?</p></li>
<li><p>For the model in part (ii), who uses attorneys more, men or
women? Provide an interpretation in terms of the odds of using an
attorney for the variable CLMSEX.</p></li>
<li><p>Run a logistic regression model using the explanatory variables
CLMAGE, CLMSEX, MARITAL, SEATBELT, CLMINSUR, LOSS and lnLOSS. Decide
which of the two loss measures is more important and re-run the
model using only one of these variables. In this model, is the
measure of losses a statistically significant variable?</p></li>
<li><p>Run your model in part (iv) but omitting the variable CLMAGE.
Describe differences between this model fit and that in part (iv),
focusing on statistically significant variables and number of
observations used in the model fit.</p></li>
<li><p>Consider a single male claimant who is age 32. Assume that the
claimant was wearing a seat belt, that the driver was insured and
the total economic loss is $5,000. For the model in part (iv), what
is the estimate of the probability of using an attorney?</p></li>
</ol></li>
<li><p><em>Probit Regression.</em> Repeat part b(v) using probit
regression models but interpret only the sign of the regression
coefficients.</p></li>
</ol>
<p>11.9 <strong>Hong Kong Horse Racing.</strong>
The race track is a fascinating example of financial market dynamics
at work. Let’s go to the track and make a wager. Suppose that, from
a field of 10 horses, we simply want to pick a winner. In the
context of regression, we will let <span class="math inline">\(y\)</span> be the response variable
indicating whether a horse wins (<span class="math inline">\(y\)</span> = 1) or not (<span class="math inline">\(y\)</span> = 0). From
racing forms, newspapers and so on, there are many explanatory
variables that are publicly available that might help us predict the
outcome for <span class="math inline">\(y\)</span>. Some candidate variables may include the age of the
horse, recent track performance of the horse and jockey, pedigree of
the horse, and so on. These variables are assessed by the investors
present at the race, the betting crowd. Like many financial markets,
it turns out that one of the most useful explanatory variable is the
crowd’s overall assessment of the horse’s abilities. These
assessments are not made based on a survey of the crowd, but rather
based on the wagers placed. Information about the crowd’s wagers is
available on a large sign at the race called the <em>tote
board</em>. The tote board provides the odds of each horse winning a
race. <a href="C11Binary.html#Tab1112">Table 11.12</a> is a hypothetical tote board for a
race of 10 horses.</p>
<p><a id=Tab1112></a></p>
<p><span id="Tab1112">Table 11.12</span>. <strong>Hypothetical Tote Board</strong></p>
<p><span class="math display">\[
\scriptsize{
\begin{array}{l|cccccccccc}
\hline
\text{Horse} &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 \\
\text{Posted Odds} &amp; 1-1 &amp; 79-1 &amp; 7-1 &amp; 3-1 &amp; 15-1 &amp; 7-1 &amp; 49-1 &amp; 49-1 &amp;
19-1 &amp; 79-1 \\
\hline
\end{array}
}
\]</span></p>
<p>The odds that appear on the tote board have been adjusted to provide
a “track take.” That is, for every dollar that has been wagered,
$<span class="math inline">\(T\)</span> goes to the track for sponsoring the race and $(1-<span class="math inline">\(T\)</span>) goes
to the winning bettors. Typical track takes are in the neighborhood
of twenty percent, or <span class="math inline">\(T\)</span>=0.20.</p>
<p>We can readily convert the odds on the tote board to the crowd’s
assessment of the probabilities of winning. To illustrate this,
<a href="C11Binary.html#Tab1113">Table 11.13</a> shows hypothetical bets to win which
resulted in the displayed information on the hypothetical tote board
in <a href="C11Binary.html#Tab1112">Table 11.12</a>.</p>
<p><a id=Tab1113></a></p>
<p><span id="Tab1113">Table 11.13</span>. <strong>Hypothetical Bets</strong></p>
<p><span class="math display">\[
\scriptsize{
\begin{array}{l|cccccccccc}
\hline \text{Horse} &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 &amp; Total\\
\hline \text{Bets} to Win &amp; 8,000 &amp; 200 &amp; 2,000 &amp; 4,000 &amp; 1,000 &amp; 3,000 &amp;
400 &amp; 400 &amp; 800 &amp; 200 &amp; 20,000 \\
\text{Probability} &amp; 0.40 &amp; 0.01 &amp; 0.10 &amp; 0.20 &amp; 0.05 &amp; 0.15 &amp; 0.02 &amp; 0.02
&amp; 0.04 &amp; 0.02 &amp; 1.000 \\
\text{Posted Odds} &amp; 1-1 &amp; 79-1 &amp; 7-1 &amp; 3-1 &amp; 15-1 &amp; 7-1 &amp; 49-1 &amp; 49-1 &amp;
19-1 &amp;
79-1 \\
\hline
\end{array}
}
\]</span></p>
<p>For this hypothetical race, 20,000 was bet to win. Because 8,000
of this 20,000 was bet on the first horse, interpret the ratio
8000/20000 = 0.40 as the crowd’s assessment of the probability to
win. The theoretical odds are calculated as 0.4/(1-0.4) = 2/3, or a
0.67 bet wins 1. However, the theoretical odds assume a fair game
with no track take. To adjust for the fact that only (1-<span class="math inline">\(T\)</span>) are
available to the winner, the posted odds for this horse would be
0.4/(1-<span class="math inline">\(T\)</span>-0.4) = 1, if <span class="math inline">\(T\)</span>=0.20. For this case, it now takes a 1
bet to win 1. We then have the relationship <span class="math inline">\(adjusted~odds = x/(1-T-x)\)</span>, where <span class="math inline">\(x\)</span> is the crowd’s assessment of the probability
of winning.</p>
<p>Before the start of the race, the tote board provides us with
adjusted odds that can readily be converted into <span class="math inline">\(x\)</span>, the crowd’s
assessment of winning. We use this measure to help us to predict
<span class="math inline">\(y\)</span>, the event of the horse actually winning the race.</p>
<p>We consider data from 925 races run in Hong Kong from September,
1981 through September, 1989. In each race, there were ten horses,
one of whom was randomly selected to be in the sample. In the data,
use FINISH = <span class="math inline">\(y\)</span> to be the indicator of a horse winning a race and
WIN = <span class="math inline">\(x\)</span> to be the crowd’s a priori probability assessment of a
horse winning a race.</p>
<ol style="list-style-type: lower-alpha">
<li><p>A statistically naive colleague would like to double the sample
size by picking two horses from each race instead of randomly
selecting one horse from a field of 10.</p>
<ol style="list-style-type: lower-roman">
<li><p>Describe the relationship between the dependent variables of the
two horses selected.</p></li>
<li><p>Say how this violates the regression model assumptions.</p></li>
</ol></li>
<li><p>Calculate the average FINISH and summary statistics for WIN. Note
that the standard deviation of FINISH is higher than that of WIN,
even though the sample means are about the same. For the variable
FINISH, what is the relationship between the sample mean and
standard deviation?</p></li>
<li><p>Calculate summary statistics of WIN by level of FINISH. Note that
the sample mean is larger for horses that won (FINISH = 1) than for
those that lost (FINISH = 0). Interpret this result.</p></li>
<li><p>Estimate a linear probability model, using WIN to predict FINISH.</p>
<ol style="list-style-type: lower-roman">
<li><p>Is WIN a statistically significant predictor of FINISH?</p></li>
<li><p>How well does this model fit the data using the usual goodness
of fit statistic?</p></li>
<li><p>For this estimated model, is it possible for the fitted values
to lie outside the interval [0, 1]? Note, by definition, that the
x-variable WIN must lie within the interval [0, 1].</p></li>
</ol></li>
<li><p>Estimate a logistic regression model, using WIN to predict
FINISH. Is WIN a statistically significant predictor of FINISH?</p></li>
<li><p>Compare the fitted values from the models in parts (d) and (e)</p>
<ol style="list-style-type: lower-roman">
<li><p>For each model, provide fitted values at WIN = 0, 0.01, 0.05,
0.10 and 1.0.</p></li>
<li><p>Plot fitted values from the linear probability model versus
fitted values from the logistic regression model.</p></li>
</ol></li>
<li><p>Interpret WIN as the crowd’s prior probability assessment of the
probability of a horse winning a race. The fitted values, FINISH, is
your new estimate of the probability of a horse winning a race,
based on the crowd’s assessment.</p>
<ol style="list-style-type: lower-roman">
<li><p>Plot the difference FINISH - WIN versus WIN.</p></li>
<li><p>Discuss a betting strategy that you might employ based on the
difference, FINISH - WIN.</p></li>
</ol></li>
</ol>
<p>11.10 <strong>Demand for Term Life Insurance.</strong> We
continue our study of Term Life Insurance Demand from Chapters 3 and
4. Specifically, we examine the 2004 Survey of Consumer Finances
(SCF), a nationally representative sample that contains extensive
information on assets, liabilities, income, and demographic
characteristics of those sampled (potential U.S. customers). We now
return to the original sample of <span class="math inline">\(n=500\)</span> families with positive
incomes and study whether or not a family purchases term life
insurance. From our sample, it turns out that 225 did not
(FACEPOS=0), whereas 275 did purchase term life insurance
(FACEPOS=1).</p>
<ol style="list-style-type: lower-alpha">
<li><p>Summary Statistics. Provide a table of means of explanatory
variables by level of the dependent variable FACEPOS. Interpret what
we learn from this table.</p></li>
<li><p>Linear Probability Model. Fit a linear probability model using
FACEPOS as the dependent variable and LINCOME, EDUCATION, AGE and
GENDER as continuous explanatory variables, together with the factor
MARSTAT.</p>
<p>b(i). Briefly define a linear probability model.</p>
<p>b(ii). Comment on the quality of the fitted model.</p>
<p>b(iii). What are the three main drawbacks of the linear probability
model?</p></li>
<li><p>Logistic Regression Model. Fit a logistic regression model using
the same set of explanatory variables.</p>
<p>c(i). Identify which variables appear to be statistically
significant. In your identification, describe the basis for your
conclusions.</p>
<p>c(ii). Which measure summarizes the goodness of fit?</p></li>
<li><p>Reduced Logistic Regression Model. Define MARSTAT1 to be a binary
variable that indicates MARSTAT=1. Fit a second logistic regression
model using LINCOME, EDUCATION and MARSTAT1.</p>
<p>d(i). Compare these two models, using a likelihood ratio test. State
your null and alternative hypotheses, decision making criterion and
your decision-making rule.</p>
<p>d(ii). Who is more likely to purchase term life insurance, married
or “non” married? Provide an interpretation in terms of the odds
of purchasing term life insurance for the variable MARSTAT1.</p>
<p>d(iii). Consider a married male who is age 54. Assume that this
person has 13 years of education, annual wages of $70,000 and is
living in a household composed of four people. For this model, what
is the estimate of the probability of purchasing term life
insurance?</p></li>
</ol>
<p>11.11 <strong>Success in Actuarial Studies</strong>. Much like the medical
and legal fields, members of the actuarial profession face
interesting problems and are generally well compensated for their
efforts in resolving these problems. Also like the medical and legal
professions, the educational barriers to becoming an actuary are
challenging, limiting entrance into the field.</p>
<p>To advise students on whether they have the potential to meet the
demands of this intellectually challenging field, Smith and
Schumacher (2006) studied attributes of students in a business
college. Specifically, they examined <span class="math inline">\(n=185\)</span> freshman at Bryant
University in Rhode Island who had begun their college careers in
1995-2001. The dependent variable of interest was whether they
graduated with an actuarial concentration, for these students the
first step to becoming a professional actuary. Of these, 77
graduated with an actuarial concentration and the other 108 dropped
the concentration (at Bryant, most transferred to other
concentrations although some left the university).</p>
<p>Smith and Schumacher (2006) reported the effects of four early
assessment mechanisms as well as GENDER, a control variable. The
assessment mechanisms were: PLACE%, performance on a mathematics
placement exam administered just prior to the freshman year, MSAT
and VSAT, mathematics (M) and verbal (V) portions of the Scholastic
Aptitude Test (SAT) and RANK, high school rank given as a proportion
(with closer to one being better). Table <a href="C11Binary.html#tab:Tab1114">11.14</a> shows
that students who eventually graduated with an actuarial
concentration performed higher on these early assessment mechanisms
than actuarial dropouts.</p>
<p>A logistic regression was fit to the data with the results reported
in Table <a href="C11Binary.html#tab:Tab1114">11.14</a>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>To get a sense of which variables are statistically significance,
calculate <span class="math inline">\(t\)</span>-ratios for each variable. For each variable, state
whether or not it is statistically significant.</p></li>
<li><p>To get a sense of the relative impact of the assessment
mechanisms, use the coefficients in Table <a href="C11Binary.html#tab:Tab1114">11.14</a> to
compute estimated success probabilities for the following
combination of variables. In your calculations, assume that
GENDER=1.</p>
<p>b(i). Assume PLACE% =0.80, MSAT = 680, VSAT=570 and RANK=0.90.</p>
<p>b(ii). Assume PLACE% =0.60, MSAT = 680, VSAT=570 and RANK=0.90.</p>
<p>b(iii). Assume PLACE% =0.80, MSAT = 620, VSAT=570 and RANK=0.90.</p>
<p>b(iv). Assume PLACE% =0.80, MSAT = 680, VSAT=540 and RANK=0.90.</p>
<p>b(v). Assume PLACE% =0.80, MSAT = 680, VSAT=570 and RANK=0.70.</p></li>
</ol>
<h5 style="text-align: center;">
<a id="displayCode.Table14.1Silly" href="javascript:togglecode('toggleCode.Table14.1Silly','displayCode.Table14.1Silly');"><i><strong></strong></i></a>
</h5>
<div id="toggleCode.Table14.1Silly" style="display: none">
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="C11Binary.html#cb101-1" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="dv">2</span>, <span class="at">caption =</span> <span class="st">&quot;Silly. Create a table just to update the counter...&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-159">Table 11.12: </span>Silly. Create a table just to update the counter…</caption>
<thead>
<tr class="header">
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="C11Binary.html#cb102-1" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="dv">2</span>, <span class="at">caption =</span> <span class="st">&quot;Silly.&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-160">Table 11.13: </span>Silly.</caption>
<thead>
<tr class="header">
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
</tr>
</tbody>
</table>
</div>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab1114">Table 11.14: </span><strong>Summary Statistics and Logistic
Regression Fits For Predicting Actuarial Graduation</strong>
</caption>
<thead>
<tr>
<th style="empty-cells: hide;" colspan="1">
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Average for Actuarial
</div>
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Logistic Regression
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:right;">
Graduates
</th>
<th style="text-align:right;">
Dropout
</th>
<th style="text-align:right;">
Estimate
</th>
<th style="text-align:right;">
Std. Error
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Intercept
</td>
<td style="text-align:right;width: 1.6cm; ">
<ul>
<li></td>
<td style="text-align:right;width: 1.6cm; ">
<ul>
<li></td>
<td style="text-align:right;width: 1.6cm; ">
-12.094
</td>
<td style="text-align:right;width: 1.6cm; ">
2.575
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
GENDER
</td>
<td style="text-align:right;width: 1.6cm; ">
<ul>
<li></td>
<td style="text-align:right;width: 1.6cm; ">
<ul>
<li></td>
<td style="text-align:right;width: 1.6cm; ">
0.256
</td>
<td style="text-align:right;width: 1.6cm; ">
0.407
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
PLACE%
</td>
<td style="text-align:right;width: 1.6cm; ">
0.83
</td>
<td style="text-align:right;width: 1.6cm; ">
0.64
</td>
<td style="text-align:right;width: 1.6cm; ">
4.336
</td>
<td style="text-align:right;width: 1.6cm; ">
1.657
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
MSAT
</td>
<td style="text-align:right;width: 1.6cm; ">
679.25
</td>
<td style="text-align:right;width: 1.6cm; ">
624.25
</td>
<td style="text-align:right;width: 1.6cm; ">
0.008
</td>
<td style="text-align:right;width: 1.6cm; ">
0.004
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
VSAT
</td>
<td style="text-align:right;width: 1.6cm; ">
572.2
</td>
<td style="text-align:right;width: 1.6cm; ">
544.25
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.002
</td>
<td style="text-align:right;width: 1.6cm; ">
0.003
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
RANK
</td>
<td style="text-align:right;width: 1.6cm; ">
0.88
</td>
<td style="text-align:right;width: 1.6cm; ">
0.76
</td>
<td style="text-align:right;width: 1.6cm; ">
4.442
</td>
<td style="text-align:right;width: 1.6cm; ">
1.836
</td>
</tr>
</tbody>
</table></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p>11.12 <strong>Case-Control.</strong> Consider the following “case-control” sample selection method for
binary dependent variables. Intuitively, if we are working with a
problem where the event of interest is rare, we want to make sure
that we sample a sufficient number of events so that our estimation
procedures are reliable.</p>
<p>Suppose that we have a large database consisting of <span class="math inline">\(\{y_i, \mathbf{x}_i\}\)</span>, <span class="math inline">\(i=1,\ldots, N\)</span> observations. (For insurance
company records, <span class="math inline">\(N\)</span> could easily be ten million or more.) We want
to make sure to get plenty of <span class="math inline">\(y_i = 1\)</span> (corresponding to claims or
“cases”) in our sample, plus a sample of <span class="math inline">\(y_i = 0\)</span> (corresponding
to non-claims or “controls”). Thus, we split the data set into two
subsets. For the first subset consisting of observations with <span class="math inline">\(y_i = 1\)</span>, we take a random sample with probability <span class="math inline">\(\tau_1\)</span>. Similarly,
for the second subset consisting of observations with <span class="math inline">\(y_i = 0\)</span>, we
take a random sample with probability <span class="math inline">\(\tau_0\)</span>. For example, in
practice we might use <span class="math inline">\(\tau_1=1\)</span> and <span class="math inline">\(\tau_0 = 0.10\)</span>, corresponding
to taking all of the claims and a 10% sample of non-claims - thus,
<span class="math inline">\(\tau_1\)</span> and <span class="math inline">\(\tau_1\)</span> are considered known to the analyst.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Let <span class="math inline">\(\{r_i = 1\}\)</span> denote the event that the observation is
selected to be part of the analysis. Determine <span class="math inline">\(\Pr(y_i = 1, r_i = 1)\)</span>, <span class="math inline">\(\Pr(y_i = 0, r_i = 1)\)</span> and <span class="math inline">\(\Pr(r_i = 1)\)</span>
in terms of <span class="math inline">\(\tau_0\)</span>, <span class="math inline">\(\tau_1\)</span> and <span class="math inline">\(\pi_i = \Pr(y_i=1)\)</span>.</p></li>
<li><p>Using the calculations in part (a), determine the conditional
probability <span class="math inline">\(\Pr(y_i=1 | r_i=1)\)</span>.</p></li>
<li><p>Now assume that <span class="math inline">\(\pi_i\)</span> has a logistic form (<span class="math inline">\(\pi(z) = \exp(z)/(1+\exp(z))\)</span> and <span class="math inline">\(\pi_i= \pi(\mathbf{x}_i^{\prime}\boldsymbol \beta ))\)</span>. Re-write your answer
part (b) using this logistic form.</p></li>
<li><p>Write the likelihood of the observed <span class="math inline">\(y_i\)</span>’s (conditional on <span class="math inline">\(r_i = 1, i=1, \ldots, n\)</span>). Show how we can interpret this as the usual
logistic regression likelihood with the exception that the intercept
has changed. Specify the new intercept in terms of the original
intercept, <span class="math inline">\(\tau_0\)</span> and <span class="math inline">\(\tau_1\)</span>.</p></li>
</ol>
</div>
<div id="S:Sec119" class="section level2 hasAnchor" number="11.9">
<h2><span class="header-section-number">11.9</span> Technical Supplements - Likelihood-Based Inference<a href="C11Binary.html#S:Sec119" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Begin with random variables <span class="math inline">\(\left( y_1, \ldots, y_n \right) ^{\prime} = \mathbf y\)</span> whose joint distribution is known up to a
vector of parameters <span class="math inline">\(\boldsymbol \theta\)</span>. In regression
applications, <span class="math inline">\(\boldsymbol \theta\)</span> consists of the regression
coefficients, <span class="math inline">\(\boldsymbol \beta\)</span>, and possibly a scale parameter
<span class="math inline">\(\sigma^2\)</span> as well as additional parameters. This joint probability
density function is denoted as <span class="math inline">\(\mathrm{f}(\mathbf{y};\boldsymbol \theta)\)</span>. The function may also be a probability mass function for
discrete random variables or a mixture distribution for random
variables that have discrete and continuous components. In each
case, we can use the same notation,
<span class="math inline">\(\mathrm{f}(\mathbf{y};\boldsymbol \theta),\)</span> and call it the
<em>likelihood function</em>. The likelihood is a function of the
parameters with the data (<span class="math inline">\(\mathbf{y}\)</span>) fixed rather than a function
of the data with the parameters (<span class="math inline">\(\boldsymbol \theta\)</span>) fixed.</p>
<p>It is customary to work with the logarithmic version of the
likelihood function and thus we define the <em>log-likelihood
function</em> to be
<span class="math display">\[
L(\boldsymbol \theta) = L(\mathbf{y};\boldsymbol \theta ) = \ln
\mathrm{f}(\mathbf{y};\boldsymbol \theta),
\]</span>
evaluated at a realization of <span class="math inline">\(\mathbf{y}\)</span>. In part, this is because
we often work with the important special case where the random
variables <span class="math inline">\(y_1, \ldots, y_n\)</span> are independent. In this case, the
joint density function can be expressed as a product of the marginal
density functions and, by taking logarithms, we can work with sums.
Even when not dealing with independent random variables, as with
time series data, it is often computationally more convenient to
work with log-likelihoods than the original likelihood function.</p>
<div id="S:Sec1191" class="section level3 hasAnchor" number="11.9.1">
<h3><span class="header-section-number">11.9.1</span> Properties of Likelihood Functions<a href="C11Binary.html#S:Sec1191" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two basic properties of likelihood functions are:
<span class="math display" id="eq:eq11A1">\[\begin{equation}
\mathrm{E} \left( \frac{ \partial}{\partial \boldsymbol \theta}
L(\boldsymbol \theta) \right) = \mathbf 0
\tag{11.13}
\end{equation}\]</span>
and
<span class="math display" id="eq:eq11A2">\[\begin{equation}
\mathrm{E} \left( \frac{ \partial^2}{\partial \boldsymbol \theta
\partial \boldsymbol \theta^{\prime}} L(\boldsymbol \theta) \right)
+ \mathrm{E} \left( \frac{ \partial L(\boldsymbol \theta)}{\partial
\boldsymbol \theta} \frac{ \partial L(\boldsymbol \theta)}{\partial
\boldsymbol \theta^{\prime}}
\right) = \mathbf 0.
\tag{11.14}
\end{equation}\]</span></p>
<p>The derivative of the log-likelihood function, <span class="math inline">\(\partial L(\boldsymbol \theta)/\partial \boldsymbol \theta\)</span>, is called the
<em>score function</em>. Equation <a href="C11Binary.html#eq:eq11A1">(11.13)</a> shows that the
score function has mean zero. To see this, under suitable regularity
conditions, we have
<span class="math display">\[\begin{eqnarray*}
\mathrm{E} \left( \frac{ \partial}{\partial \boldsymbol \theta}
L(\boldsymbol \theta) \right) &amp;=&amp; \mathrm{E} \left( \frac{
\frac{\partial}{\partial \boldsymbol \theta}
\mathrm{f}(\mathbf{y};\boldsymbol \theta
)}{\mathrm{f}(\mathbf{y};\boldsymbol \theta )}  \right) = \int
\frac{\partial}{\partial \boldsymbol \theta}
\mathrm{f}(\mathbf{y};\boldsymbol \theta ) d \mathbf y =
\frac{\partial}{\partial \boldsymbol \theta} \int
\mathrm{f}(\mathbf{y};\boldsymbol \theta ) d \mathbf y \\
&amp;=&amp; \frac{\partial}{\partial \boldsymbol \theta} 1 = \mathbf 0.
\end{eqnarray*}\]</span>
For convenience, this demonstration assumes a density for f(<span class="math inline">\(\cdot\)</span>);
extensions to mass and mixtures distributions are straightforward.
The proof of equation <a href="C11Binary.html#eq:eq11A2">(11.14)</a> is similar and is
omitted. To establish equation <a href="C11Binary.html#eq:eq11A1">(11.13)</a>, we implicitly
used “suitable regularity conditions” to allow the interchange of
the derivative and integral sign. To be more precise, an analyst
working with a specific type of distribution can use this
information to check that the interchange of the derivative and
integral sign is valid.</p>
<p>Using equation <a href="C11Binary.html#eq:eq11A2">(11.14)</a>, we can define the <em>information matrix</em>
<span class="math display" id="eq:eq11A3">\[\begin{equation}
\mathbf{I}(\boldsymbol \theta) = \mathrm{E} \left( \frac{ \partial
L(\boldsymbol \theta)}{\partial \boldsymbol \theta} \frac{ \partial
L(\boldsymbol \theta)}{\partial \boldsymbol \theta^{\prime}}
\right) = -\mathrm{E} \left( \frac{ \partial^2}{\partial \boldsymbol \theta
\partial \boldsymbol \theta^{\prime}} L(\boldsymbol \theta) \right).
\tag{11.15}
\end{equation}\]</span>
This quantity is used extensively in the study of large sample
properties of likelihood functions.</p>
<p>The information matrix appears in the large sample distribution of
the score function. Specifically, under broad conditions, we have
that <span class="math inline">\(\partial L(\boldsymbol \theta)/\partial \boldsymbol \theta\)</span>
has a large sample normal distribution with mean <strong>0</strong> and
variance <span class="math inline">\(\mathbf{I}(\boldsymbol \theta)\)</span>. To illustrate, suppose
that the random variables are independent so that the score function
can be written as
<span class="math display">\[
\frac{ \partial}{\partial \boldsymbol \theta} L(\boldsymbol \theta)
=\frac{ \partial}{\partial \boldsymbol \theta} \ln \prod_{i=1}^n
\mathrm{f}(y_i;\boldsymbol \theta ) =\sum_{i=1}^n \frac{
\partial}{\partial \boldsymbol \theta}
\ln \mathrm{f}(y_i;\boldsymbol \theta ).
\]</span>
The score function is the sum of mean zero random variables because
of equation <a href="C11Binary.html#eq:eq11A1">(11.13)</a>; central limit theorems are widely
available to ensure that sums of independent random variables have
large sample normal distributions (see Section 1.4 for an example).
Further, if the random variables are identical, then from equation
<a href="C11Binary.html#eq:eq11A3">(11.15)</a> we can see that the second moment of
<span class="math inline">\(\partial \ln \mathrm{f}(y_i;\boldsymbol \theta ) /\partial \boldsymbol \theta\)</span> is the information matrix, yielding the result.</p>
</div>
<div id="S:Sec1192" class="section level3 hasAnchor" number="11.9.2">
<h3><span class="header-section-number">11.9.2</span> Maximum Likelihood Estimators<a href="C11Binary.html#S:Sec1192" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Maximum likelihood estimators are values of the parameters
<span class="math inline">\(\boldsymbol \theta\)</span> that are “most likely” to have been produced
by the data. The value of <span class="math inline">\(\boldsymbol \theta\)</span>, say <span class="math inline">\(\boldsymbol \theta_{MLE}\)</span>, that maximizes <span class="math inline">\(\mathrm{f}(\mathbf{y};\boldsymbol \theta)\)</span> is called the* maximum likelihood estimator*. Because
<span class="math inline">\(\ln(\cdot)\)</span> is a one-to-one function, we can also determine
<span class="math inline">\(\boldsymbol \theta_{MLE}\)</span> by maximizing the log-likelihood
function, <span class="math inline">\(L(\boldsymbol \theta)\)</span>.</p>
<p>Under broad conditions, we have that <span class="math inline">\(\boldsymbol \theta_{MLE}\)</span> has
a large sample normal distribution with mean <span class="math inline">\(\boldsymbol \theta\)</span>
and variance <span class="math inline">\(\left( \mathbf{I}(\boldsymbol \theta) \right)^{-1}\)</span>.
This is a critical result upon which much of estimation and
hypothesis testing is based. To underscore this result, we examine
the special case of “normal-based” regression.</p>
<hr />
<p><strong>Special Case. Regression with normal distributions.</strong>
Suppose that <span class="math inline">\(y_1, \ldots, y_n\)</span> are independent and normally
distributed, with mean <span class="math inline">\(\mathrm{E~}y_i = \mu_i = \mathbf{x}_i^{\prime} \boldsymbol \beta\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.
The parameters can be summarized as <span class="math inline">\(\boldsymbol \theta = \left( \boldsymbol \beta^{\prime}, \sigma^2 \right)^{\prime}.\)</span> Recall from
equation (1.1) that the normal probability density function is
<span class="math display">\[
\mathrm{f}(y; \mu_i, \sigma^2)=\frac{1}{\sigma \sqrt{2\pi }}\exp \left( -\frac{1}{2\sigma^2
}\left( y-\mu_i \right)^2\right) .
\]</span>
With this, the two components of the score function are
<span class="math display">\[\begin{eqnarray*}
\frac{ \partial}{\partial \boldsymbol \beta} L(\boldsymbol \theta)
&amp;=&amp; \sum_{i=1}^n \frac{
\partial}{\partial \boldsymbol \beta}
\ln \mathrm{f}(y_i; \mathbf{x}_i^{\prime} \boldsymbol \beta,
\sigma^2) =-\frac{1}{2\sigma^2} \sum_{i=1}^n \frac{
\partial}{\partial \boldsymbol \beta}
\left(y_i-\mathbf{x}_i^{\prime} \boldsymbol \beta  \right)^2 \\ &amp;=&amp;
-\frac{(-2)}{2 \sigma^2} \sum_{i=1}^n
\left(y_i-\mathbf{x}_i^{\prime} \boldsymbol \beta  \right)
\mathbf{x}_i
\end{eqnarray*}\]</span>
and
<span class="math display">\[\begin{eqnarray*}
\frac{ \partial}{\partial \sigma^2} L(\boldsymbol \theta) &amp;=&amp;
\sum_{i=1}^n \frac{
\partial}{\partial  \sigma^2}
\ln \mathrm{f}(y_i; \mathbf{x}_i^{\prime} \boldsymbol \beta,
\sigma^2)  = -\frac{n}{2 \sigma^2} + \frac {1}{2 \sigma
^4}\sum_{i=1}^n \left(y_i-\mathbf{x}_i^{\prime} \boldsymbol \beta
\right)^2 .
\end{eqnarray*}\]</span>
Setting these equations to zero and solving yields the maximum
likelihood estimators
<span class="math display">\[
\boldsymbol \beta_{MLE} = \left(\sum_{i=1}^n \mathbf{x}_i
\mathbf{x}_i^{\prime}\right)^{-1} \sum_{i=1}^n \mathbf{x}_i y_i =
\mathbf{b}
\]</span>
and
<span class="math display">\[
\sigma^2_{MLE} = \frac{1}{n} \sum_{i=1}^n \left(
y_i - \mathbf{x}_i^{\prime} \mathbf{b} \right)^2 = \frac{n-(k+1)}{n} s^2.
\]</span>
Thus, the maximum likelihood estimator of <span class="math inline">\(\boldsymbol \beta\)</span> is
equal to the usual least squares estimator. The maximum likelihood
estimator of <span class="math inline">\(\sigma^2\)</span> is a scalar multiple of the usual least
squares estimator. The least squares estimators <span class="math inline">\(s^2\)</span> is unbiased
whereas as <span class="math inline">\(\sigma^2_{MLE}\)</span> is only approximately unbiased in large
samples.</p>
<p>The information matrix is</p>
<p><span class="math display">\[
\mathbf{I}(\boldsymbol \theta) = -\mathrm{E~} \left(
  \begin{array}{cc}
   \frac{ \partial^2}{\partial
\boldsymbol \beta ~\partial \boldsymbol \beta^{\prime}} L(\boldsymbol
\theta) &amp; \frac{ \partial^2}{\partial
\boldsymbol \beta ~\partial \sigma^2} L(\boldsymbol \theta) \\
   \frac{ \partial^2}{\partial \sigma^2 \partial
\boldsymbol \beta^{\prime} } L(\boldsymbol \theta) &amp; \frac{
\partial^2}{\partial
\sigma^2 \partial \sigma^2} L(\boldsymbol \theta)\\
  \end{array}
  \right)=
  \left(
  \begin{array}{cc}
   \frac{ 1}{\sigma^2} \sum_{i=1}^n \mathbf{x}_i
\mathbf{x}_i^{\prime} &amp; 0 \\
   0 &amp; \frac{n}{2 \sigma^4}\\
  \end{array}
  \right).
\]</span>
Thus, <span class="math inline">\(\boldsymbol \beta_{MLE} = \mathbf{b}\)</span> has a large sample
normal distribution with mean <span class="math inline">\(\boldsymbol \beta\)</span> and
variance-covariance matrix <span class="math inline">\(\sigma^2 \left(\sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^{\prime} \right)^{-1}\)</span>, as seen previously. Moreover,
<span class="math inline">\(\sigma^2_{MLE}\)</span> has a large sample normal distribution with mean
<span class="math inline">\(\sigma^2\)</span> and variance <span class="math inline">\(2 \sigma^4 /n.\)</span></p>
<hr />
<p>Maximum likelihood is a general estimation technique that can be
applied in many statistical settings, not just regression and time
series applications. It can be applied broadly and
enjoys certain optimality properties. We have already cited the
result that maximum likelihood estimators typically have a large
sample normal distribution. Moreover, maximum likelihood estimators
are the most efficient in the following sense. Suppose that
<span class="math inline">\(\widehat{\boldsymbol \theta}\)</span> is an alternative unbiased estimator.
The Cramer-Rao theorem states, under mild regularity conditions, for
all vectors <span class="math inline">\(\mathbf c\)</span>, that <span class="math inline">\(\mathrm{Var~} \mathbf c^{\prime} \boldsymbol \theta_{MLE} \le \mathrm{Var~} \mathbf c^{\prime} \widehat{\boldsymbol \theta}\)</span>, for sufficiently large <span class="math inline">\(n\)</span>.</p>
<p>We also note that <span class="math inline">\(2 \left( L(\boldsymbol \theta_{MLE}) - L(\boldsymbol \theta) \right)\)</span> has a chi-square distribution with
degrees of freedom equal to the dimension of <span class="math inline">\(\boldsymbol \theta\)</span> .</p>
<p>In a few applications, such as the regression case with a normal
distribution, maximum likelihood estimators can be computed
analytically as a closed-form expression. Typically, this can be
done by finding roots of the first derivative of the function.
However, in general, maximum likelihood estimators can not be
calculated with closed form expressions and are determined
iteratively. Two general procedures are widely used:
1. <em>Newton-Raphson</em> uses the iterative algorithm
<span class="math display" id="eq:eq11A4">\[\begin{equation}
\boldsymbol \theta_{NEW} = \boldsymbol \theta_{OLD} - \left. \left\{
\left( \frac{ \partial^2 L}{\partial \boldsymbol \theta
\partial \boldsymbol \theta^{\prime}} \right)^{-1}
\frac{\partial L}{\partial \boldsymbol \theta } \right\} \right|_
{\boldsymbol \theta = \boldsymbol \theta_{OLD}} .
\tag{11.16}
\end{equation}\]</span>
2. <em>Fisher scoring</em> uses the iterative algorithm
<span class="math display" id="eq:eq11A5">\[\begin{equation}
\boldsymbol \theta_{NEW} = \boldsymbol \theta_{OLD} +
\mathbf{I}(\boldsymbol \theta_{OLD})^{-1} \left. \left\{
\frac{\partial L}{\partial \boldsymbol \theta } \right\} \right|_
{\boldsymbol \theta = \boldsymbol \theta_{OLD}} .
\tag{11.17}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{I}(\boldsymbol \theta)\)</span> is the information matrix.</p>
</div>
<div id="S:Sec1193" class="section level3 hasAnchor" number="11.9.3">
<h3><span class="header-section-number">11.9.3</span> Hypothesis Tests<a href="C11Binary.html#S:Sec1193" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We consider testing the null hypothesis <span class="math inline">\(H_0: h(\boldsymbol \theta) = \mathbf{d}\)</span>, where <span class="math inline">\(\mathbf{d}\)</span> is a known vector of dimension <span class="math inline">\(r \times 1\)</span> and h(<span class="math inline">\(\cdot\)</span>) is known and differentiable. This testing
framework encompasses the general linear hypothesis introduced in
Chapter 4 as a special case.</p>
<p>There are three general approaches for testing hypotheses, called
the <em>likelihood ratio</em>, <em>Wald</em> and <em>Rao</em> tests. The
Wald approach evaluates a function of the likelihood at <span class="math inline">\(\boldsymbol \theta_{MLE}\)</span>. The likelihood ratio approach uses <span class="math inline">\(\boldsymbol \theta_{MLE}\)</span> and <span class="math inline">\(\boldsymbol \theta_{Reduced}\)</span>. Here, <span class="math inline">\(\boldsymbol \theta_{Reduced}\)</span> is the value of <span class="math inline">\(\boldsymbol \theta\)</span> that
maximizes <span class="math inline">\(L(\boldsymbol \theta_{Reduced})\)</span> under the constraint
that <span class="math inline">\(h(\boldsymbol \theta) = \mathbf{d}\)</span>. The Rao approach also
uses <span class="math inline">\(\boldsymbol \theta_{Reduced}\)</span> but determines it by maximizing
<span class="math inline">\(L(\boldsymbol \theta) - \boldsymbol \lambda^{\prime}(h(\boldsymbol \theta) -\mathbf{d})\)</span>, where <span class="math inline">\(\boldsymbol \lambda\)</span> is a vector of
Lagrange multipliers. Hence, Rao’s test is also called the
<em>Lagrange multiplier test</em>.</p>
<p>The test statistics associated with the three approaches are:</p>
<ul>
<li><span class="math inline">\(LRT = 2 \times \left\{L(\boldsymbol \theta_{MLE})-L(\boldsymbol \theta_{Reduced}) \right\}\)</span></li>
<li>Wald: <span class="math inline">\(TS_W(\boldsymbol \theta_{MLE})\)</span>, where
<span class="math display">\[
TS_W(\boldsymbol \theta)=(h(\boldsymbol \theta)
-\mathbf{d})^{\prime} \left\{ \frac{\partial}{\partial \boldsymbol
\theta} h(\boldsymbol \theta)^{\prime} \left(-\mathbf{I}(\boldsymbol
\theta) \right)^{-1} \frac{\partial}{\partial \boldsymbol \theta}
h(\boldsymbol \theta) \right\}^{-1} (h(\boldsymbol \theta)
-\mathbf{d}),
\]</span>
and</li>
<li>Rao: <span class="math inline">\(TS_R(\boldsymbol \theta_{Reduced})\)</span>, where
<span class="math display">\[
TS_R(\boldsymbol \theta)
  = \frac{\partial}{\partial \boldsymbol \theta} L(\boldsymbol \theta)
  \left(-\mathbf{I}(\boldsymbol \theta) \right)^{-1} \frac{\partial}{\partial \boldsymbol \theta}
  L(\boldsymbol \theta)^{\prime}.
\]</span></li>
</ul>
<p>Under broad conditions, all three test statistics have large sample
chi-square distributions with <span class="math inline">\(r\)</span> degrees of freedom under <span class="math inline">\(H_0\)</span>.
All three methods work well when the number of parameters is finite
dimensional and the null hypothesis specifies that <span class="math inline">\(\boldsymbol \theta\)</span> is on the interior of the parameter space.</p>
<p>The main advantage of the Wald statistic is that it only requires
computation of <span class="math inline">\(\boldsymbol \theta_{MLE}\)</span> and not <span class="math inline">\(\boldsymbol \theta_{Reduced}\)</span>. In contrast, the main advantage of the Rao
statistic is that it only requires computation of <span class="math inline">\(\boldsymbol \theta_{Reduced}\)</span> and not <span class="math inline">\(\boldsymbol \theta_{MLE}\)</span>. In many
applications, computation of <span class="math inline">\(\boldsymbol \theta_{MLE}\)</span> is onerous.
The likelihood ratio test is a direct extension of the partial
<span class="math inline">\(F\)</span>-test introduced in Chapter 4 - it allows one to directly compare
nested models, a helpful technique in applications.</p>
</div>
<div id="S:Sec1194" class="section level3 hasAnchor" number="11.9.4">
<h3><span class="header-section-number">11.9.4</span> Information Criteria<a href="C11Binary.html#S:Sec1194" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Likelihood ratio tests are useful for choosing between two models
that are <em>nested</em>, that is, where one model is a subset of the
other. How do we compare models when they are not nested? One way is
to use the following information criteria.</p>
<p>The distance between two probability distributions given by
probability density functions <span class="math inline">\(g\)</span> and <span class="math inline">\(f_{\boldsymbol \theta}\)</span> can
be summarized by
<span class="math display">\[
\mathrm{KL}(g,f_{\boldsymbol \theta}) = \mathrm{E}_g \ln
\frac{g(y)}{f_{\boldsymbol \theta}(y)} .
\]</span>
This is the <em>Kullback-Leibler distance</em>. Here, we have indexed
<span class="math inline">\(f\)</span> by a vector of parameters <span class="math inline">\(\boldsymbol \theta\)</span>. If we let the
density function <span class="math inline">\(g\)</span> be fixed at a hypothesized value, say
<span class="math inline">\(f_{{\boldsymbol \theta}_0}\)</span>, then minimizing
<span class="math inline">\(\mathrm{KL}(f_{{\boldsymbol \theta}_0},f_{\boldsymbol \theta})\)</span> is
equivalent to maximizing the log-likelihood.</p>
<p>However, maximizing the likelihood does not impose sufficient
structure on the problem because we know that we can always make the
likelihood greater by introducing additional parameters. Thus,
Akaike in 1974 showed that a reasonable alternative is to minimize
<span class="math display">\[
AIC = -2 \times L(\boldsymbol \theta_{MLE}) + 2 \times
(number~of~parameters),
\]</span>
known as <em>Akaike’s Information Criterion</em>. Here, the additional
term <span class="math inline">\(2 \times\)</span> <span class="math inline">\((number~of~parameters)\)</span> is a penalty for
the complexity of the model. With this penalty, one cannot improve
upon the fit simply by introducing additional parameters. This
statistic can be used when comparing several alternative models that
are not necessarily nested. One picks the model that minimizes
<span class="math inline">\(AIC\)</span>. If the models under consideration have the same number of
parameters, this is equivalent to choosing the model that maximizes
the log-likelihood.</p>
<p>We remark that this definition is not uniformly adopted in the
literature. For example, in time series analysis, the <span class="math inline">\(AIC\)</span> is
rescaled by the number of parameters. Other versions that provide
finite sample corrections are also available in the literature.</p>
<p>Schwarz in 1978 derived an alternative criterion using Bayesian
methods. His measure is known as the <em>Bayesian Information
Criterion</em>, defined as
<span class="math display">\[
BIC = -2 \times L(\boldsymbol \theta_{MLE}) + (number~of~parameters)
\times \ln (number~of~observations),
\]</span>
This measure gives greater weight to the number of parameters. That
is, other things being equal, <span class="math inline">\(BIC\)</span> will suggest a more parsimonious
model than <span class="math inline">\(AIC\)</span>.</p>
<p>Like the adjusted coefficient of determination <span class="math inline">\(R^2_a\)</span> that we have
introduced in the regression literature, both <span class="math inline">\(AIC\)</span> and <span class="math inline">\(BIC\)</span>
provide measures of fit with a penalty for model complexity. In
normal linear regression models, Section 5.6 pointed out that
minimizing <span class="math inline">\(AIC\)</span> is equivalent to minimizing <span class="math inline">\(n \ln s^2 + k\)</span>.
Another linear regression statistic that balances the goodness of
fit and complexity of the model is Mallows <span class="math inline">\(C_p\)</span> statistic. For <span class="math inline">\(p\)</span>
candidate variables in the model, this is defined as <span class="math inline">\(C_p = (Error~SS)_p/s^2 - (n-2p).\)</span> See, for example, Cameron and Trivedi
(1998) for references and further discussion of information
criteria.</p>

<!-- # Chap 1 -->
<!-- # Chap 2 -->
<!-- # Chap 3 -->
<!-- # Chap 4 -->
<!-- # Chap 5 -->
<!-- # Chap 6 -->
<!-- # Chap 7 -->
<!-- # Chap 8 -->
<!-- # Chap 9 -->
<!-- # Chap 10 -->
<!-- # Chap 11 -->
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="C10Panel.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="C12Count.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
