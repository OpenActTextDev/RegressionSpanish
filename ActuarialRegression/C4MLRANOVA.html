<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Multiple Linear Regression - II | Regression Modeling with Actuarial and Financial Applications</title>
  <meta name="description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Multiple Linear Regression - II | Regression Modeling with Actuarial and Financial Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Multiple Linear Regression - II | Regression Modeling with Actuarial and Financial Applications" />
  
  <meta name="twitter:description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  

<meta name="author" content="Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="C3BasicMLR.html"/>
<link rel="next" href="C5VarSelect.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript"  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleSolution(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}      
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>



<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Regression Modeling With Actuarial and Financial Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#forward"><i class="fa fa-check"></i>Forward</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-is-this-book-for"><i class="fa fa-check"></i>Who Is This Book For?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-is-this-book-about"><i class="fa fa-check"></i>What Is This Book About?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-does-this-book-deliver-its-message"><i class="fa fa-check"></i>How Does This Book Deliver Its Message?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html"><i class="fa fa-check"></i><b>1</b> Regression and the Normal Distribution</a>
<ul>
<li class="chapter" data-level="1.1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec11"><i class="fa fa-check"></i><b>1.1</b> What is Regression Analysis?</a></li>
<li class="chapter" data-level="1.2" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec12"><i class="fa fa-check"></i><b>1.2</b> Fitting Data to a Normal Distribution</a></li>
<li class="chapter" data-level="1.3" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec13"><i class="fa fa-check"></i><b>1.3</b> Power Transforms</a></li>
<li class="chapter" data-level="1.4" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec14"><i class="fa fa-check"></i><b>1.4</b> Sampling and the Role of Normality</a></li>
<li class="chapter" data-level="1.5" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec15"><i class="fa fa-check"></i><b>1.5</b> Regression and Sampling Designs</a></li>
<li class="chapter" data-level="1.6" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec16"><i class="fa fa-check"></i><b>1.6</b> Actuarial Applications of Regression</a></li>
<li class="chapter" data-level="1.7" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec17"><i class="fa fa-check"></i><b>1.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="1.8" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec18"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li>
<li class="chapter" data-level="1.9" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec19"><i class="fa fa-check"></i><b>1.9</b> Technical Supplement - Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="C2BasicLR.html"><a href="C2BasicLR.html"><i class="fa fa-check"></i><b>2</b> Basic Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec21"><i class="fa fa-check"></i><b>2.1</b> Correlations and Least Squares</a></li>
<li class="chapter" data-level="2.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec22"><i class="fa fa-check"></i><b>2.2</b> Basic Linear Regression Model</a></li>
<li class="chapter" data-level="2.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec23"><i class="fa fa-check"></i><b>2.3</b> Is the Model Useful? Some Basic Summary Measures</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec231"><i class="fa fa-check"></i><b>2.3.1</b> Partitioning the Variability</a></li>
<li class="chapter" data-level="2.3.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec232"><i class="fa fa-check"></i><b>2.3.2</b> The Size of a Typical Deviation: <em>s</em></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec24"><i class="fa fa-check"></i><b>2.4</b> Properties of Regression Coefficient Estimators</a></li>
<li class="chapter" data-level="2.5" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec25"><i class="fa fa-check"></i><b>2.5</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec251"><i class="fa fa-check"></i><b>2.5.1</b> Is the Explanatory Variable Important?: The <em>t</em>-Test</a></li>
<li class="chapter" data-level="2.5.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec252"><i class="fa fa-check"></i><b>2.5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="2.5.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec253"><i class="fa fa-check"></i><b>2.5.3</b> Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec26"><i class="fa fa-check"></i><b>2.6</b> Building a Better Model: Residual Analysis</a></li>
<li class="chapter" data-level="2.7" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec27"><i class="fa fa-check"></i><b>2.7</b> Application: Capital Asset Pricing Model</a></li>
<li class="chapter" data-level="2.8" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec28"><i class="fa fa-check"></i><b>2.8</b> Illustrative Regression Computer Output</a></li>
<li class="chapter" data-level="2.9" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec29"><i class="fa fa-check"></i><b>2.9</b> Further Reading and References</a></li>
<li class="chapter" data-level="2.10" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec210"><i class="fa fa-check"></i><b>2.10</b> Exercises</a></li>
<li class="chapter" data-level="2.11" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec211"><i class="fa fa-check"></i><b>2.11</b> Technical Supplement - Elements of Matrix Algebra</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2111"><i class="fa fa-check"></i><b>2.11.1</b> Basic Definitions</a></li>
<li class="chapter" data-level="2.11.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2112"><i class="fa fa-check"></i><b>2.11.2</b> Some Special Matrices</a></li>
<li class="chapter" data-level="2.11.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2113"><i class="fa fa-check"></i><b>2.11.3</b> Basic Operations</a></li>
<li class="chapter" data-level="2.11.4" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2114"><i class="fa fa-check"></i><b>2.11.4</b> Random Matrices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html"><i class="fa fa-check"></i><b>3</b> Multiple Linear Regression - I</a>
<ul>
<li class="chapter" data-level="3.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec31"><i class="fa fa-check"></i><b>3.1</b> Method of Least Squares</a></li>
<li class="chapter" data-level="3.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec32"><i class="fa fa-check"></i><b>3.2</b> Linear Regression Model and Properties of Estimators</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec321"><i class="fa fa-check"></i><b>3.2.1</b> Regression Function</a></li>
<li class="chapter" data-level="3.2.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec322"><i class="fa fa-check"></i><b>3.2.2</b> Regression Coefficient Interpretation</a></li>
<li class="chapter" data-level="3.2.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec323"><i class="fa fa-check"></i><b>3.2.3</b> Model Assumptions</a></li>
<li class="chapter" data-level="3.2.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec324"><i class="fa fa-check"></i><b>3.2.4</b> Properties of Regression Coefficient Estimators</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec33"><i class="fa fa-check"></i><b>3.3</b> Estimation and Goodness of Fit</a></li>
<li class="chapter" data-level="3.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec34"><i class="fa fa-check"></i><b>3.4</b> Statistical Inference for a Single Coefficient</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec341"><i class="fa fa-check"></i><b>3.4.1</b> The <em>t</em>-Test</a></li>
<li class="chapter" data-level="3.4.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec342"><i class="fa fa-check"></i><b>3.4.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="3.4.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec343"><i class="fa fa-check"></i><b>3.4.3</b> Added Variable Plots</a></li>
<li class="chapter" data-level="3.4.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec344"><i class="fa fa-check"></i><b>3.4.4</b> Partial Correlation Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec35"><i class="fa fa-check"></i><b>3.5</b> Some Special Explanatory Variables</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec351"><i class="fa fa-check"></i><b>3.5.1</b> Binary Variables</a></li>
<li class="chapter" data-level="3.5.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec352"><i class="fa fa-check"></i><b>3.5.2</b> Transforming Explanatory Variables</a></li>
<li class="chapter" data-level="3.5.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec353"><i class="fa fa-check"></i><b>3.5.3</b> Interaction Terms</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec36"><i class="fa fa-check"></i><b>3.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="3.7" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec37"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression - II</a>
<ul>
<li class="chapter" data-level="4.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec41"><i class="fa fa-check"></i><b>4.1</b> The Role of Binary Variables</a></li>
<li class="chapter" data-level="4.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec42"><i class="fa fa-check"></i><b>4.2</b> Statistical Inference for Several Coefficients</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec421"><i class="fa fa-check"></i><b>4.2.1</b> Sets of Regression Coefficients</a></li>
<li class="chapter" data-level="4.2.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec422"><i class="fa fa-check"></i><b>4.2.2</b> The General Linear Hypothesis</a></li>
<li class="chapter" data-level="4.2.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec423"><i class="fa fa-check"></i><b>4.2.3</b> Estimating and Predicting Several Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec43"><i class="fa fa-check"></i><b>4.3</b> One Factor ANOVA Model</a></li>
<li class="chapter" data-level="4.4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec44"><i class="fa fa-check"></i><b>4.4</b> Combining Categorical and Continuous Explanatory Variables</a></li>
<li class="chapter" data-level="4.5" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec45"><i class="fa fa-check"></i><b>4.5</b> Further Reading and References</a></li>
<li class="chapter" data-level="4.6" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec46"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec47"><i class="fa fa-check"></i><b>4.7</b> Technical Supplement - Matrix Expressions</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec471"><i class="fa fa-check"></i><b>4.7.1</b> Expressing Models with Categorical Variables in Matrix Form</a></li>
<li class="chapter" data-level="4.7.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec472"><i class="fa fa-check"></i><b>4.7.2</b> Calculating Least Squares Recursively</a></li>
<li class="chapter" data-level="4.7.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec473"><i class="fa fa-check"></i><b>4.7.3</b> General Linear Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="C5VarSelect.html"><a href="C5VarSelect.html"><i class="fa fa-check"></i><b>5</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="5.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec51"><i class="fa fa-check"></i><b>5.1</b> An Iterative Approach to Data Analysis and Modeling</a></li>
<li class="chapter" data-level="5.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec52"><i class="fa fa-check"></i><b>5.2</b> Automatic Variable Selection Procedures</a></li>
<li class="chapter" data-level="5.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec53"><i class="fa fa-check"></i><b>5.3</b> Residual Analysis</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec531"><i class="fa fa-check"></i><b>5.3.1</b> Residuals</a></li>
<li class="chapter" data-level="5.3.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec532"><i class="fa fa-check"></i><b>5.3.2</b> Using Residuals to Identify Outliers</a></li>
<li class="chapter" data-level="5.3.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec533"><i class="fa fa-check"></i><b>5.3.3</b> Using Residuals to Select Explanatory Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec54"><i class="fa fa-check"></i><b>5.4</b> Influential Points</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec541"><i class="fa fa-check"></i><b>5.4.1</b> Leverage</a></li>
<li class="chapter" data-level="5.4.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec542"><i class="fa fa-check"></i><b>5.4.2</b> Cook’s Distance</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec55"><i class="fa fa-check"></i><b>5.5</b> Collinearity</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec551"><i class="fa fa-check"></i><b>5.5.1</b> What is Collinearity?</a></li>
<li class="chapter" data-level="5.5.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec552"><i class="fa fa-check"></i><b>5.5.2</b> Variance Inflation Factors</a></li>
<li class="chapter" data-level="5.5.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec553"><i class="fa fa-check"></i><b>5.5.3</b> Collinearity and Leverage</a></li>
<li class="chapter" data-level="5.5.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec554"><i class="fa fa-check"></i><b>5.5.4</b> Suppressor Variables</a></li>
<li class="chapter" data-level="5.5.5" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec555"><i class="fa fa-check"></i><b>5.5.5</b> Orthogonal Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec56"><i class="fa fa-check"></i><b>5.6</b> Selection Criteria</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec561"><i class="fa fa-check"></i><b>5.6.1</b> Goodness of Fit</a></li>
<li class="chapter" data-level="5.6.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec562"><i class="fa fa-check"></i><b>5.6.2</b> Model Validation</a></li>
<li class="chapter" data-level="5.6.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec563"><i class="fa fa-check"></i><b>5.6.3</b> Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec57"><i class="fa fa-check"></i><b>5.7</b> Heteroscedasticity</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec571"><i class="fa fa-check"></i><b>5.7.1</b> Detecting Heteroscedasticity</a></li>
<li class="chapter" data-level="5.7.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec572"><i class="fa fa-check"></i><b>5.7.2</b> Heteroscedasticity-Consistent Standard Errors</a></li>
<li class="chapter" data-level="5.7.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec573"><i class="fa fa-check"></i><b>5.7.3</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="5.7.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec574"><i class="fa fa-check"></i><b>5.7.4</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec58"><i class="fa fa-check"></i><b>5.8</b> Further Reading and References</a></li>
<li class="chapter" data-level="5.9" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec59"><i class="fa fa-check"></i><b>5.9</b> Exercises</a></li>
<li class="chapter" data-level="5.10" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec510"><i class="fa fa-check"></i><b>5.10</b> Technical Supplements for Chapter 5</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5101"><i class="fa fa-check"></i><b>5.10.1</b> Projection Matrix</a></li>
<li class="chapter" data-level="5.10.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5102"><i class="fa fa-check"></i><b>5.10.2</b> Leave One Out Statistics</a></li>
<li class="chapter" data-level="5.10.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5103"><i class="fa fa-check"></i><b>5.10.3</b> Omitting Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html"><i class="fa fa-check"></i><b>6</b> Interpreting Regression Results</a>
<ul>
<li class="chapter" data-level="6.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec61"><i class="fa fa-check"></i><b>6.1</b> What the Modeling Process Tells Us</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec611"><i class="fa fa-check"></i><b>6.1.1</b> Interpreting Individual Effects</a></li>
<li class="chapter" data-level="6.1.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec612"><i class="fa fa-check"></i><b>6.1.2</b> Other Interpretations</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec62"><i class="fa fa-check"></i><b>6.2</b> The Importance of Variable Selection</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec621"><i class="fa fa-check"></i><b>6.2.1</b> Overfitting the Model</a></li>
<li class="chapter" data-level="6.2.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec622"><i class="fa fa-check"></i><b>6.2.2</b> Underfitting the Model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec63"><i class="fa fa-check"></i><b>6.3</b> The Importance of Data Collection</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec631"><i class="fa fa-check"></i><b>6.3.1</b> Sampling Frame Error and Adverse Selection</a></li>
<li class="chapter" data-level="6.3.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec632"><i class="fa fa-check"></i><b>6.3.2</b> Limited Sampling Regions</a></li>
<li class="chapter" data-level="6.3.3" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec633"><i class="fa fa-check"></i><b>6.3.3</b> Limited Dependent Variables, Censoring and Truncation</a></li>
<li class="chapter" data-level="6.3.4" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec634"><i class="fa fa-check"></i><b>6.3.4</b> Omitted and Endogenous Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec635"><i class="fa fa-check"></i><b>6.3.5</b> Missing Data</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec64"><i class="fa fa-check"></i><b>6.4</b> Missing Data Models</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec641"><i class="fa fa-check"></i><b>6.4.1</b> Missing at Random</a></li>
<li class="chapter" data-level="6.4.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec642"><i class="fa fa-check"></i><b>6.4.2</b> Non-Ignorable Missing Data</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec65"><i class="fa fa-check"></i><b>6.5</b> Application: Risk Managers’ Cost Effectiveness</a></li>
<li class="chapter" data-level="6.6" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec66"><i class="fa fa-check"></i><b>6.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="6.7" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec67"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
<li class="chapter" data-level="6.8" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec68"><i class="fa fa-check"></i><b>6.8</b> Technical Supplements for Chapter 6</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec681"><i class="fa fa-check"></i><b>6.8.1</b> Effects of Model Misspecification</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="C7Trends.html"><a href="C7Trends.html"><i class="fa fa-check"></i><b>7</b> Modeling Trends</a>
<ul>
<li class="chapter" data-level="7.1" data-path="C7Trends.html"><a href="C7Trends.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#time-series-and-stochastic-processes"><i class="fa fa-check"></i>Time Series and Stochastic Processes</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#time-series-versus-causal-models"><i class="fa fa-check"></i>Time Series versus Causal Models</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="C7Trends.html"><a href="C7Trends.html#S7:Trends"><i class="fa fa-check"></i><b>7.2</b> Fitting Trends in Time</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#understanding-patterns-over-time"><i class="fa fa-check"></i>Understanding Patterns over Time</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#fitting-trends-in-time"><i class="fa fa-check"></i>Fitting Trends in Time</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#fitting-seasonal-trends"><i class="fa fa-check"></i>Fitting Seasonal Trends</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#reliability-of-time-series-forecasts"><i class="fa fa-check"></i>Reliability of Time Series Forecasts</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="C7Trends.html"><a href="C7Trends.html#S7:RandomWalk"><i class="fa fa-check"></i><b>7.3</b> Stationarity and Random Walk Models</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#white-noise"><i class="fa fa-check"></i>White Noise</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#random-walk"><i class="fa fa-check"></i>Random Walk</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="C7Trends.html"><a href="C7Trends.html#inference-using-random-walk-models"><i class="fa fa-check"></i><b>7.4</b> Inference using Random Walk Models</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#model-properties"><i class="fa fa-check"></i>Model Properties</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#forecasting"><i class="fa fa-check"></i>Forecasting</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#identifying-stationarity"><i class="fa fa-check"></i>Identifying Stationarity</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#identifying-random-walks"><i class="fa fa-check"></i>Identifying Random Walks</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#random-walk-versus-linear-trend-in-time-models"><i class="fa fa-check"></i>Random Walk versus Linear Trend in Time Models</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="C7Trends.html"><a href="C7Trends.html#filtering-to-achieve-stationarity"><i class="fa fa-check"></i><b>7.5</b> Filtering to Achieve Stationarity</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#transformations"><i class="fa fa-check"></i>Transformations</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="C7Trends.html"><a href="C7Trends.html#forecast-evaluation"><i class="fa fa-check"></i><b>7.6</b> Forecast Evaluation</a></li>
<li class="chapter" data-level="7.7" data-path="C7Trends.html"><a href="C7Trends.html#further-reading-and-references"><i class="fa fa-check"></i><b>7.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="7.8" data-path="C7Trends.html"><a href="C7Trends.html#exercises"><i class="fa fa-check"></i><b>7.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="C8AR.html"><a href="C8AR.html"><i class="fa fa-check"></i><b>8</b> Autocorrelations and Autoregressive Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="C8AR.html"><a href="C8AR.html#S8:Autocorrs"><i class="fa fa-check"></i><b>8.1</b> Autocorrelations</a>
<ul>
<li class="chapter" data-level="" data-path="C8AR.html"><a href="C8AR.html#application-inflation-bond-returns"><i class="fa fa-check"></i>Application: Inflation Bond Returns</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="C8AR.html"><a href="C8AR.html#autoregressive-models-of-order-one"><i class="fa fa-check"></i><b>8.2</b> Autoregressive Models of Order One</a></li>
<li class="chapter" data-level="8.3" data-path="C8AR.html"><a href="C8AR.html#S8:Estimation"><i class="fa fa-check"></i><b>8.3</b> Estimation and Diagnostic Checking</a></li>
<li class="chapter" data-level="8.4" data-path="C8AR.html"><a href="C8AR.html#S8:AR1Smooth"><i class="fa fa-check"></i><b>8.4</b> Smoothing and Prediction</a></li>
<li class="chapter" data-level="8.5" data-path="C8AR.html"><a href="C8AR.html#S8:BoxJenkins"><i class="fa fa-check"></i><b>8.5</b> Box-Jenkins Modeling and Forecasting</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="C8AR.html"><a href="C8AR.html#models"><i class="fa fa-check"></i><b>8.5.1</b> Models</a></li>
<li class="chapter" data-level="8.5.2" data-path="C8AR.html"><a href="C8AR.html#forecasting-1"><i class="fa fa-check"></i><b>8.5.2</b> Forecasting</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="C8AR.html"><a href="C8AR.html#application-hong-kong-exchange-rates"><i class="fa fa-check"></i><b>8.6</b> Application: Hong Kong Exchange Rates</a></li>
<li class="chapter" data-level="8.7" data-path="C8AR.html"><a href="C8AR.html#further-reading-and-references-1"><i class="fa fa-check"></i><b>8.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="8.8" data-path="C8AR.html"><a href="C8AR.html#exercises-1"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="C9Forecast.html"><a href="C9Forecast.html"><i class="fa fa-check"></i><b>9</b> Forecasting and Time Series Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="C9Forecast.html"><a href="C9Forecast.html#smoothing-with-moving-averages"><i class="fa fa-check"></i><b>9.1</b> Smoothing with Moving Averages</a></li>
<li class="chapter" data-level="9.2" data-path="C9Forecast.html"><a href="C9Forecast.html#S9:ExponSmooth"><i class="fa fa-check"></i><b>9.2</b> Exponential Smoothing</a></li>
<li class="chapter" data-level="9.3" data-path="C9Forecast.html"><a href="C9Forecast.html#S9:SeasonalTSModels"><i class="fa fa-check"></i><b>9.3</b> Seasonal Time Series Models</a></li>
<li class="chapter" data-level="9.4" data-path="C9Forecast.html"><a href="C9Forecast.html#unit-root-tests"><i class="fa fa-check"></i><b>9.4</b> Unit Root Tests</a></li>
<li class="chapter" data-level="9.5" data-path="C9Forecast.html"><a href="C9Forecast.html#archgarch-models"><i class="fa fa-check"></i><b>9.5</b> ARCH/GARCH Models</a></li>
<li class="chapter" data-level="9.6" data-path="C9Forecast.html"><a href="C9Forecast.html#further-reading-and-references-2"><i class="fa fa-check"></i><b>9.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="C10Panel.html"><a href="C10Panel.html"><i class="fa fa-check"></i><b>10</b> Longitudinal and Panel Data Models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="C10Panel.html"><a href="C10Panel.html#S10:Intro"><i class="fa fa-check"></i><b>10.1</b> What are Longitudinal and Panel Data?</a></li>
<li class="chapter" data-level="10.2" data-path="C10Panel.html"><a href="C10Panel.html#S10:Visual"><i class="fa fa-check"></i><b>10.2</b> Visualizing Longitudinal and Panel Data</a></li>
<li class="chapter" data-level="10.3" data-path="C10Panel.html"><a href="C10Panel.html#S10:FEModels"><i class="fa fa-check"></i><b>10.3</b> Basic Fixed Effects Models</a></li>
<li class="chapter" data-level="10.4" data-path="C10Panel.html"><a href="C10Panel.html#S10:FEModels2"><i class="fa fa-check"></i><b>10.4</b> Extended Fixed Effects Models</a></li>
<li class="chapter" data-level="10.5" data-path="C10Panel.html"><a href="C10Panel.html#S10:REModels"><i class="fa fa-check"></i><b>10.5</b> Random Effects Models</a></li>
<li class="chapter" data-level="10.6" data-path="C10Panel.html"><a href="C10Panel.html#S10:References"><i class="fa fa-check"></i><b>10.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="C11Binary.html"><a href="C11Binary.html"><i class="fa fa-check"></i><b>11</b> Categorical Dependent Variables</a>
<ul>
<li class="chapter" data-level="11.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec111"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables</a></li>
<li class="chapter" data-level="11.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec112"><i class="fa fa-check"></i><b>11.2</b> Logistic and Probit Regression Models</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1121"><i class="fa fa-check"></i><b>11.2.1</b> Using Nonlinear Functions of Explanatory Variables</a></li>
<li class="chapter" data-level="11.2.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1122"><i class="fa fa-check"></i><b>11.2.2</b> Threshold Interpretation</a></li>
<li class="chapter" data-level="11.2.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1123"><i class="fa fa-check"></i><b>11.2.3</b> Random Utility Interpretation</a></li>
<li class="chapter" data-level="11.2.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1124"><i class="fa fa-check"></i><b>11.2.4</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec113"><i class="fa fa-check"></i><b>11.3</b> Inference for Logistic and Probit Regression Models</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="C11Binary.html"><a href="C11Binary.html#parameter-estimation"><i class="fa fa-check"></i><b>11.3.1</b> Parameter Estimation</a></li>
<li class="chapter" data-level="11.3.2" data-path="C11Binary.html"><a href="C11Binary.html#additional-inference"><i class="fa fa-check"></i><b>11.3.2</b> Additional Inference</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec114"><i class="fa fa-check"></i><b>11.4</b> Application: Medical Expenditures</a></li>
<li class="chapter" data-level="11.5" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec115"><i class="fa fa-check"></i><b>11.5</b> Nominal Dependent Variables</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1151"><i class="fa fa-check"></i><b>11.5.1</b> Generalized Logit</a></li>
<li class="chapter" data-level="11.5.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1152"><i class="fa fa-check"></i><b>11.5.2</b> Multinomial Logit</a></li>
<li class="chapter" data-level="11.5.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1153"><i class="fa fa-check"></i><b>11.5.3</b> Nested Logit</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec116"><i class="fa fa-check"></i><b>11.6</b> Ordinal Dependent Variables</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="C11Binary.html"><a href="C11Binary.html#cumulative-logit"><i class="fa fa-check"></i><b>11.6.1</b> Cumulative Logit</a></li>
<li class="chapter" data-level="11.6.2" data-path="C11Binary.html"><a href="C11Binary.html#cumulative-probit"><i class="fa fa-check"></i><b>11.6.2</b> Cumulative Probit</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec117"><i class="fa fa-check"></i><b>11.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="11.8" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec118"><i class="fa fa-check"></i><b>11.8</b> Exercises</a></li>
<li class="chapter" data-level="11.9" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec119"><i class="fa fa-check"></i><b>11.9</b> Technical Supplements - Likelihood-Based Inference</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1191"><i class="fa fa-check"></i><b>11.9.1</b> Properties of Likelihood Functions</a></li>
<li class="chapter" data-level="11.9.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1192"><i class="fa fa-check"></i><b>11.9.2</b> Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="11.9.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1193"><i class="fa fa-check"></i><b>11.9.3</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="11.9.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1194"><i class="fa fa-check"></i><b>11.9.4</b> Information Criteria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="C12Count.html"><a href="C12Count.html"><i class="fa fa-check"></i><b>12</b> Count Dependent Variables</a>
<ul>
<li class="chapter" data-level="12.1" data-path="C12Count.html"><a href="C12Count.html#S:Sec121"><i class="fa fa-check"></i><b>12.1</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="C12Count.html"><a href="C12Count.html#S:Sec1211"><i class="fa fa-check"></i><b>12.1.1</b> Poisson Distribution</a></li>
<li class="chapter" data-level="12.1.2" data-path="C12Count.html"><a href="C12Count.html#S:Sec1212"><i class="fa fa-check"></i><b>12.1.2</b> Regression Model</a></li>
<li class="chapter" data-level="12.1.3" data-path="C12Count.html"><a href="C12Count.html#S:Sec1213"><i class="fa fa-check"></i><b>12.1.3</b> Estimation</a></li>
<li class="chapter" data-level="12.1.4" data-path="C12Count.html"><a href="C12Count.html#S:Sec1214"><i class="fa fa-check"></i><b>12.1.4</b> Additional Inference</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="C12Count.html"><a href="C12Count.html#S:Sec122"><i class="fa fa-check"></i><b>12.2</b> Application: Singapore Automobile Insurance</a></li>
<li class="chapter" data-level="12.3" data-path="C12Count.html"><a href="C12Count.html#S:Sec123"><i class="fa fa-check"></i><b>12.3</b> Overdispersion and Negative Binomial Models</a></li>
<li class="chapter" data-level="12.4" data-path="C12Count.html"><a href="C12Count.html#S:Sec124"><i class="fa fa-check"></i><b>12.4</b> Other Count Models</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="C12Count.html"><a href="C12Count.html#zero-inflated-models"><i class="fa fa-check"></i><b>12.4.1</b> Zero-Inflated Models</a></li>
<li class="chapter" data-level="12.4.2" data-path="C12Count.html"><a href="C12Count.html#hurdle-models"><i class="fa fa-check"></i><b>12.4.2</b> Hurdle Models</a></li>
<li class="chapter" data-level="12.4.3" data-path="C12Count.html"><a href="C12Count.html#heterogeneity-models"><i class="fa fa-check"></i><b>12.4.3</b> Heterogeneity Models</a></li>
<li class="chapter" data-level="12.4.4" data-path="C12Count.html"><a href="C12Count.html#latent-class-models"><i class="fa fa-check"></i><b>12.4.4</b> Latent Class Models</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="C12Count.html"><a href="C12Count.html#S:Sec125"><i class="fa fa-check"></i><b>12.5</b> Further Reading and References</a></li>
<li class="chapter" data-level="12.6" data-path="C12Count.html"><a href="C12Count.html#S:Sec126"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="C13GLM.html"><a href="C13GLM.html"><i class="fa fa-check"></i><b>13</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec131"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec132"><i class="fa fa-check"></i><b>13.2</b> GLM Model</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1321"><i class="fa fa-check"></i><b>13.2.1</b> Linear Exponential Family of Distributions</a></li>
<li class="chapter" data-level="13.2.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1322"><i class="fa fa-check"></i><b>13.2.2</b> Link Functions</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec133"><i class="fa fa-check"></i><b>13.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1331"><i class="fa fa-check"></i><b>13.3.1</b> Maximum Likelihood Estimation for Canonical Links</a></li>
<li class="chapter" data-level="13.3.2" data-path="C13GLM.html"><a href="C13GLM.html#overdispersion"><i class="fa fa-check"></i><b>13.3.2</b> Overdispersion</a></li>
<li class="chapter" data-level="13.3.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1333"><i class="fa fa-check"></i><b>13.3.3</b> Goodness of Fit Statistics</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec134"><i class="fa fa-check"></i><b>13.4</b> Application: Medical Expenditures</a></li>
<li class="chapter" data-level="13.5" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec135"><i class="fa fa-check"></i><b>13.5</b> Residuals</a></li>
<li class="chapter" data-level="13.6" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec136"><i class="fa fa-check"></i><b>13.6</b> Tweedie Distribution</a></li>
<li class="chapter" data-level="13.7" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec137"><i class="fa fa-check"></i><b>13.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="13.8" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec138"><i class="fa fa-check"></i><b>13.8</b> Exercises</a></li>
<li class="chapter" data-level="13.9" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec139"><i class="fa fa-check"></i><b>13.9</b> Technical Supplements - Exponential Family</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1391"><i class="fa fa-check"></i><b>13.9.1</b> Linear Exponential Family of Distributions</a></li>
<li class="chapter" data-level="13.9.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1392"><i class="fa fa-check"></i><b>13.9.2</b> Moments</a></li>
<li class="chapter" data-level="13.9.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1393"><i class="fa fa-check"></i><b>13.9.3</b> Maximum Likelihood Estimation for General Links</a></li>
<li class="chapter" data-level="13.9.4" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1394"><i class="fa fa-check"></i><b>13.9.4</b> Iterated Reweighted Least Squares</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="C14Survival.html"><a href="C14Survival.html"><i class="fa fa-check"></i><b>14</b> Survival Models</a>
<ul>
<li class="chapter" data-level="14.1" data-path="C14Survival.html"><a href="C14Survival.html#introduction-2"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec142"><i class="fa fa-check"></i><b>14.2</b> Censoring and Truncation</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="C14Survival.html"><a href="C14Survival.html#definitions-and-examples"><i class="fa fa-check"></i><b>14.2.1</b> Definitions and Examples</a></li>
<li class="chapter" data-level="14.2.2" data-path="C14Survival.html"><a href="C14Survival.html#likelihood-inference"><i class="fa fa-check"></i><b>14.2.2</b> Likelihood Inference</a></li>
<li class="chapter" data-level="14.2.3" data-path="C14Survival.html"><a href="C14Survival.html#product-limit-estimator"><i class="fa fa-check"></i><b>14.2.3</b> Product-Limit Estimator</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec143"><i class="fa fa-check"></i><b>14.3</b> Accelerated Failure Time Model</a></li>
<li class="chapter" data-level="14.4" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec144"><i class="fa fa-check"></i><b>14.4</b> Proportional Hazards Model</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec1441"><i class="fa fa-check"></i><b>14.4.1</b> Proportional Hazards</a></li>
<li class="chapter" data-level="14.4.2" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec1442"><i class="fa fa-check"></i><b>14.4.2</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec145"><i class="fa fa-check"></i><b>14.5</b> Recurrent Events</a></li>
<li class="chapter" data-level="14.6" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec146"><i class="fa fa-check"></i><b>14.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="C15Misc.html"><a href="C15Misc.html"><i class="fa fa-check"></i><b>15</b> Miscellaneous Regression Topics</a>
<ul>
<li class="chapter" data-level="15.1" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec151"><i class="fa fa-check"></i><b>15.1</b> Mixed Linear Models</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="C15Misc.html"><a href="C15Misc.html#weighted-least-squares-2"><i class="fa fa-check"></i><b>15.1.1</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="15.1.2" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec1512"><i class="fa fa-check"></i><b>15.1.2</b> Variance Components Estimation</a></li>
<li class="chapter" data-level="15.1.3" data-path="C15Misc.html"><a href="C15Misc.html#best-linear-unbiased-prediction"><i class="fa fa-check"></i><b>15.1.3</b> Best Linear Unbiased Prediction</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="C15Misc.html"><a href="C15Misc.html#bayesian-regression"><i class="fa fa-check"></i><b>15.2</b> Bayesian Regression</a></li>
<li class="chapter" data-level="15.3" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec153"><i class="fa fa-check"></i><b>15.3</b> Density Estimation and Scatterplot Smoothing}</a></li>
<li class="chapter" data-level="15.4" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec154"><i class="fa fa-check"></i><b>15.4</b> Generalized Additive Models</a></li>
<li class="chapter" data-level="15.5" data-path="C15Misc.html"><a href="C15Misc.html#bootstrapping"><i class="fa fa-check"></i><b>15.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="15.6" data-path="C15Misc.html"><a href="C15Misc.html#further-reading-and-references-3"><i class="fa fa-check"></i><b>15.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="C16FreqSev.html"><a href="C16FreqSev.html"><i class="fa fa-check"></i><b>16</b> Frequency-Severity Models</a>
<ul>
<li class="chapter" data-level="16.1" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec161"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec162"><i class="fa fa-check"></i><b>16.2</b> Tobit Model</a></li>
<li class="chapter" data-level="16.3" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec163"><i class="fa fa-check"></i><b>16.3</b> Application: Medical Expenditures</a></li>
<li class="chapter" data-level="16.4" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec164"><i class="fa fa-check"></i><b>16.4</b> Two-Part Model</a></li>
<li class="chapter" data-level="16.5" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec165"><i class="fa fa-check"></i><b>16.5</b> Aggregate Loss Model</a></li>
<li class="chapter" data-level="16.6" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec166"><i class="fa fa-check"></i><b>16.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="16.7" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec167"><i class="fa fa-check"></i><b>16.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="C17Fat.html"><a href="C17Fat.html"><i class="fa fa-check"></i><b>17</b> Fat-Tailed Regression Models</a>
<ul>
<li class="chapter" data-level="17.1" data-path="C17Fat.html"><a href="C17Fat.html#introduction-3"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec172"><i class="fa fa-check"></i><b>17.2</b> Transformations</a></li>
<li class="chapter" data-level="17.3" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec173"><i class="fa fa-check"></i><b>17.3</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec1731"><i class="fa fa-check"></i><b>17.3.1</b> What is “Fat-Tailed?”</a></li>
<li class="chapter" data-level="17.3.2" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec1732"><i class="fa fa-check"></i><b>17.3.2</b> Application: Wisconsin Nursing Homes</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec174"><i class="fa fa-check"></i><b>17.4</b> Generalized Distributions</a>
<ul>
<li class="chapter" data-level="" data-path="C17Fat.html"><a href="C17Fat.html#application-wisconsin-nursing-homes"><i class="fa fa-check"></i>Application: Wisconsin Nursing Homes</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec175"><i class="fa fa-check"></i><b>17.5</b> Quantile Regression</a></li>
<li class="chapter" data-level="17.6" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec176"><i class="fa fa-check"></i><b>17.6</b> Extreme Value Models</a></li>
<li class="chapter" data-level="17.7" data-path="C17Fat.html"><a href="C17Fat.html#further-reading-and-references-4"><i class="fa fa-check"></i><b>17.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="17.8" data-path="C17Fat.html"><a href="C17Fat.html#exercises-2"><i class="fa fa-check"></i><b>17.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="C18Cred.html"><a href="C18Cred.html"><i class="fa fa-check"></i><b>18</b> Credibility and Bonus-Malus</a>
<ul>
<li class="chapter" data-level="18.1" data-path="C18Cred.html"><a href="C18Cred.html#risk-classification-and-experience-rating"><i class="fa fa-check"></i><b>18.1</b> Risk Classification and Experience Rating</a></li>
<li class="chapter" data-level="18.2" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec182"><i class="fa fa-check"></i><b>18.2</b> Credibility</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec1821"><i class="fa fa-check"></i><b>18.2.1</b> Limited Fluctuation Credibility</a></li>
<li class="chapter" data-level="18.2.2" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec1822"><i class="fa fa-check"></i><b>18.2.2</b> Greatest Accuracy Credibility</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec183"><i class="fa fa-check"></i><b>18.3</b> Credibility and Regression</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="C18Cred.html"><a href="C18Cred.html#one-way-random-effects-model"><i class="fa fa-check"></i><b>18.3.1</b> One-Way Random Effects Model</a></li>
<li class="chapter" data-level="18.3.2" data-path="C18Cred.html"><a href="C18Cred.html#longitudinal-models"><i class="fa fa-check"></i><b>18.3.2</b> Longitudinal Models</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec184"><i class="fa fa-check"></i><b>18.4</b> Bonus-Malus</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="C19Triangles.html"><a href="C19Triangles.html"><i class="fa fa-check"></i><b>19</b> Claims Triangles</a>
<ul>
<li class="chapter" data-level="19.1" data-path="C19Triangles.html"><a href="C19Triangles.html#introduction-4"><i class="fa fa-check"></i><b>19.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1911"><i class="fa fa-check"></i><b>19.1.1</b> Claims Evolution</a></li>
<li class="chapter" data-level="19.1.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1912"><i class="fa fa-check"></i><b>19.1.2</b> Claims Triangles</a></li>
<li class="chapter" data-level="19.1.3" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1913"><i class="fa fa-check"></i><b>19.1.3</b> Chain Ladder Method</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec192"><i class="fa fa-check"></i><b>19.2</b> Regression Using Functions of Time as Explanatory Variables</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1921"><i class="fa fa-check"></i><b>19.2.1</b> Lognormal Model</a></li>
<li class="chapter" data-level="19.2.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1922"><i class="fa fa-check"></i><b>19.2.2</b> Hoerl Curve</a></li>
<li class="chapter" data-level="19.2.3" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1923"><i class="fa fa-check"></i><b>19.2.3</b> Poisson Models</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec193"><i class="fa fa-check"></i><b>19.3</b> Using Past Developments</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1931"><i class="fa fa-check"></i><b>19.3.1</b> Mack Model</a></li>
<li class="chapter" data-level="19.3.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1932"><i class="fa fa-check"></i><b>19.3.2</b> Distributional Models</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="C19Triangles.html"><a href="C19Triangles.html#further-reading-and-references-5"><i class="fa fa-check"></i><b>19.4</b> Further Reading and References</a></li>
<li class="chapter" data-level="19.5" data-path="C19Triangles.html"><a href="C19Triangles.html#exercises-3"><i class="fa fa-check"></i><b>19.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="C20Report.html"><a href="C20Report.html"><i class="fa fa-check"></i><b>20</b> Report Writing: Communicating Data Analysis Results</a>
<ul>
<li class="chapter" data-level="20.1" data-path="C20Report.html"><a href="C20Report.html#S20:Overview"><i class="fa fa-check"></i><b>20.1</b> Overview</a></li>
<li class="chapter" data-level="20.2" data-path="C20Report.html"><a href="C20Report.html#S20:Methods"><i class="fa fa-check"></i><b>20.2</b> Methods for Communicating Data</a>
<ul>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#within-text-data"><i class="fa fa-check"></i>Within Text Data</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#graphs"><i class="fa fa-check"></i>Graphs</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="C20Report.html"><a href="C20Report.html#S20:Organize"><i class="fa fa-check"></i><b>20.3</b> How to Organize</a>
<ul>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#title-and-abstract"><i class="fa fa-check"></i>Title and Abstract</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#introduction-5"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#model-selection-and-interpretation"><i class="fa fa-check"></i>Model Selection and Interpretation</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#references-and-appendix"><i class="fa fa-check"></i>References and Appendix</a></li>
</ul></li>
<li class="chapter" data-level="20.4" data-path="C20Report.html"><a href="C20Report.html#further-suggestions-for-report-writing"><i class="fa fa-check"></i><b>20.4</b> Further Suggestions for Report Writing</a></li>
<li class="chapter" data-level="20.5" data-path="C20Report.html"><a href="C20Report.html#case-study-swedish-automobile-claims"><i class="fa fa-check"></i><b>20.5</b> Case Study: Swedish Automobile Claims</a></li>
<li class="chapter" data-level="20.6" data-path="C20Report.html"><a href="C20Report.html#further-reading-and-references-6"><i class="fa fa-check"></i><b>20.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="20.7" data-path="C20Report.html"><a href="C20Report.html#exercise"><i class="fa fa-check"></i><b>20.7</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="C21Design.html"><a href="C21Design.html"><i class="fa fa-check"></i><b>21</b> Designing Effective Graphs</a>
<ul>
<li class="chapter" data-level="21.1" data-path="C21Design.html"><a href="C21Design.html#S21:Intro"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="C21Design.html"><a href="C21Design.html#S21:GDesign"><i class="fa fa-check"></i><b>21.2</b> Graphic Design Choices Make a Difference</a></li>
<li class="chapter" data-level="21.3" data-path="C21Design.html"><a href="C21Design.html#S21:DesignGuide"><i class="fa fa-check"></i><b>21.3</b> Design Guidelines</a>
<ul>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-one-avoid-chartjunk"><i class="fa fa-check"></i>Guideline One: Avoid Chartjunk</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-two-use-small-multiples-to-promote-comparisons-and-assess-change"><i class="fa fa-check"></i>Guideline Two: Use Small Multiples to Promote Comparisons and Assess Change</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-three-use-complex-graphs-to-portray-complex-patterns"><i class="fa fa-check"></i>Guideline Three: Use Complex Graphs to Portray Complex Patterns</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-four-relate-graph-size-to-information-content"><i class="fa fa-check"></i>Guideline Four: Relate Graph Size to Information Content</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-five-use-graphical-forms-that-promote-comparisons"><i class="fa fa-check"></i>Guideline Five: Use Graphical Forms That Promote Comparisons</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-six-integrate-graphs-and-text"><i class="fa fa-check"></i>Guideline Six: Integrate Graphs and Text</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-seven-demonstrate-an-important-message"><i class="fa fa-check"></i>Guideline Seven: Demonstrate an Important Message</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-eight-know-your-audience"><i class="fa fa-check"></i>Guideline Eight: Know Your Audience</a></li>
</ul></li>
<li class="chapter" data-level="21.4" data-path="C21Design.html"><a href="C21Design.html#S21:EmpiricalFoundations"><i class="fa fa-check"></i><b>21.4</b> Empirical Foundations For Guidelines</a>
<ul>
<li class="chapter" data-level="21.4.1" data-path="C21Design.html"><a href="C21Design.html#graphs-as-units-of-study"><i class="fa fa-check"></i><b>21.4.1</b> Graphs as Units of Study</a></li>
</ul></li>
<li class="chapter" data-level="21.5" data-path="C21Design.html"><a href="C21Design.html#S21:Conclude"><i class="fa fa-check"></i><b>21.5</b> Concluding Remarks</a></li>
<li class="chapter" data-level="21.6" data-path="C21Design.html"><a href="C21Design.html#S21:References"><i class="fa fa-check"></i><b>21.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i><b>22</b> Appendices</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a1.-basic-statistical-inference"><i class="fa fa-check"></i>Appendix A1. Basic Statistical Inference</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#distributions-of-functions-of-random-variables"><i class="fa fa-check"></i>Distributions of Functions of Random Variables</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#estimation-and-prediction"><i class="fa fa-check"></i>Estimation and Prediction</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#testing-hypotheses"><i class="fa fa-check"></i>Testing Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a2.-matrix-algebra"><i class="fa fa-check"></i>Appendix A2. Matrix Algebra</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#basic-definitions"><i class="fa fa-check"></i>Basic Definitions</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#review-of-basic-operations"><i class="fa fa-check"></i>Review of Basic Operations</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#further-definitions"><i class="fa fa-check"></i>Further Definitions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a3.-probability-tables"><i class="fa fa-check"></i>Appendix A3. Probability Tables</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#normal-distribution"><i class="fa fa-check"></i>Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#chi-square-distribution"><i class="fa fa-check"></i>Chi-Square Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#t-distribution"><i class="fa fa-check"></i><em>t</em>-Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#f-distribution"><i class="fa fa-check"></i><em>F</em>-Distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="brief-answers-to-selected-exercises.html"><a href="brief-answers-to-selected-exercises.html"><i class="fa fa-check"></i>Brief Answers to Selected Exercises</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Regression Modeling with Actuarial and Financial Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="C4MLRANOVA" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Multiple Linear Regression - II<a href="C4MLRANOVA.html#C4MLRANOVA" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Chapter Preview</em>. This chapter extends the discussion of multiple linear regression by introducing statistical inference for handling several coefficients simultaneously. To motivate this extension, this chapter considers coefficients associated with <em>categorical variables</em>. These variables allow us to group observations into distinct categories. This chapter shows how to incorporate categorical variables into regression functions using binary variables, thus considerably widening the scope of potential applications for regression analysis. Statistical inference for several coefficients allows analysts to make decisions about categorical variables, as well as other important applications. Categorical explanatory variables also provide the basis for an <em>ANOVA</em> model, a special type of regression model that permits easier analysis and interpretation.</p>
<div id="Sec41" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> The Role of Binary Variables<a href="C4MLRANOVA.html#Sec41" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Categorical variables</em> provide labels for observations to denote membership in distinct groups, or categories. A binary variable is a special case of a categorical variable. To illustrate, a binary variable may tell us whether or not someone has health insurance. A categorical variable could tell us whether someone has:</p>
<ul>
<li>private group insurance (offered by employers and associations),</li>
<li>private individual health insurance (through insurance companies),</li>
<li>public insurance (such as Medicare or Medicaid),</li>
<li>no health insurance.</li>
</ul>
<p>For categorical variables, there may or may not be an ordering of the groups. In health insurance, it is difficult to order these four categories and say which is “larger,” private group, private individual, public, or no health insurance. In contrast, for education, we might group individuals into “low,” “intermediate,” and “high” years of education. In this case, there is an ordering among groups based on the level of educational achievement. As we will see, this ordering may or may not provide information about the dependent variable. <em>Factor</em> is another term used for an unordered categorical explanatory variable.</p>
<p>For ordered categorical variables, analysts typically assign a numerical score to each outcome and treat the variable as if it were continuous. For example, if we had three levels of education, we might employ ranks and use</p>
<p><span class="math display">\[
\small{
\text{EDUCATION} = \begin{cases}
      1 &amp; \text{for low education} \\
      2 &amp; \text{for intermediate education} \\
      3 &amp; \text{for high education.}
   \end{cases}
}
\]</span></p>
<p>An alternative would be to use a numerical score that approximates an underlying value of the category. For example, we might use</p>
<p><span class="math display">\[
\small{
\text{EDUCATION} = \begin{cases}
      6 &amp; \text{for low education} \\
      10 &amp; \text{for intermediate education} \\
      14 &amp; \text{for high education.}
   \end{cases}
}
\]</span></p>
<p>This gives the approximate number of years of schooling that individuals in each category completed.</p>
<p>The assignment of numerical scores and treating the variable as continuous has important implications for the regression modeling interpretation. Recall that the regression coefficient is the marginal change in the expected response; in this case, the <span class="math inline">\(\beta\)</span> for education assesses the increase in <span class="math inline">\(\mathrm{E }~y\)</span> per unit change in EDUCATION. If we record EDUCATION as a rank in a regression model, then the <span class="math inline">\(\beta\)</span> for education corresponds to the increase in <span class="math inline">\(\mathrm{E }~y\)</span> moving from EDUCATION=1 to EDUCATION=2 (from low to intermediate); this increase is the same as moving from EDUCATION=2 to EDUCATION=3 (from intermediate to high). Do we want to model this increase as the same? This is an assumption that the analyst makes with this coding of EDUCATION; it may or may not be valid but certainly needs to be recognized.</p>
<p>Because of this interpretation of coefficients, analysts rarely use ranks or other numerical scores to summarize <em>unordered</em> categorical variables. The most direct way of handling factors in regression is through the use of binary variables. A categorical variable with <span class="math inline">\(c\)</span> levels can be represented using <span class="math inline">\(c\)</span> binary variables, one for each category. For example, suppose that we were uncertain about the direction of the education effect and so decide to treat it as a factor. Then, we could code <span class="math inline">\(c=3\)</span> binary variables: (1) a variable to indicate low education, (2) one to indicate intermediate education, and (3) one to indicate high education. These binary variables are often known as <em>dummy variables</em>. In regression analysis with an intercept term, we use only <span class="math inline">\(c-1\)</span> of these binary variables; the remaining variable enters implicitly through the intercept term. By identifying a variable as a factor, most statistical software packages will automatically create binary variables for you.</p>
<p>Through the use of binary variables, we do not make use of the ordering of categories within a factor. Because no assumption is made regarding the ordering of the categories, for the model fit it does not matter which variable is dropped with regard to the fit of the model. However, it does matter for the interpretation of the regression coefficients. Consider the following example.</p>
<hr />
<p><strong>Example: Term Life Insurance - Continued.</strong> We now return to the marital status of respondents from the Survey of Consumer Finances (SCF). Recall that marital status is not measured continuously but rather takes on values that fall into distinct groups that we treat as unordered. In Chapter 3, we grouped survey respondents according to whether or not they are “single,” where being single includes never married, separated, divorced, widowed, and not married but living with a partner. We now supplement this by considering the categorical variable, MARSTAT, which represents the marital status of the survey respondent. This may be:</p>
<ul>
<li>1, for married</li>
<li>2, for living with a partner</li>
<li>0, for other (SCF further breaks down this category into separated, divorced, widowed, never married, inapplicable, persons age 17 or less, and no further persons).</li>
</ul>
<p>As before, the dependent variable is <span class="math inline">\(y =\)</span> LNFACE, the amount that the company will pay in the event of the death of the named insured (in logarithmic dollars). Table <a href="C4MLRANOVA.html#tab:Tab41">4.1</a> summarizes the dependent variable by the level of the categorical variable. This table shows that the marital status “married” is the most prevalent in the sample and that those married choose to have the most life insurance coverage. Figure <a href="C4MLRANOVA.html#fig:Fig41">4.1</a> gives a more complete picture of the distribution of LNFACE for each of the three types of marital status. The table and figure also suggest that those living together have less life insurance coverage than the other two categories.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab41">Table 4.1: </span><strong>Summary Statistics of Logarithmic Face By Marital Status</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:center;">
MARSTAT
</th>
<th style="text-align:right;">
Number
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Standard Deviation
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Other
</td>
<td style="text-align:center;width: 1.8cm; ">
0
</td>
<td style="text-align:right;width: 1.8cm; ">
57
</td>
<td style="text-align:right;width: 1.8cm; ">
10.958
</td>
<td style="text-align:right;">
1.566
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Married
</td>
<td style="text-align:center;width: 1.8cm; ">
1
</td>
<td style="text-align:right;width: 1.8cm; ">
208
</td>
<td style="text-align:right;width: 1.8cm; ">
12.329
</td>
<td style="text-align:right;">
1.822
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Living together
</td>
<td style="text-align:center;width: 1.8cm; ">
2
</td>
<td style="text-align:right;width: 1.8cm; ">
10
</td>
<td style="text-align:right;width: 1.8cm; ">
10.825
</td>
<td style="text-align:right;">
2.001
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Total
</td>
<td style="text-align:center;width: 1.8cm; ">
</td>
<td style="text-align:right;width: 1.8cm; ">
275
</td>
<td style="text-align:right;width: 1.8cm; ">
11.990
</td>
<td style="text-align:right;">
1.871
</td>
</tr>
</tbody>
</table>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig41"></span>
<img src="RegressionMarkdown_files/figure-html/Fig41-1.png" alt="Box Plots of Logarithmic Face, by Level of Marital Status" width="60%" />
<p class="caption">
Figure 4.1: <strong>Box Plots of Logarithmic Face, by Level of Marital Status</strong>
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Fig41.Hide" href="javascript:togglecode('toggleCode.Fig41.Hide','displayCode.Fig41.Hide');"><i><strong>R Code to Produce Table 4.1 and Figure 4.1</strong></i></a>
</h5>
<div id="toggleCode.Fig41.Hide" style="display: none">
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="C4MLRANOVA.html#cb43-1" tabindex="-1"></a>tableout <span class="ot">&lt;-</span> </span>
<span id="cb43-2"><a href="C4MLRANOVA.html#cb43-2" tabindex="-1"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb43-3"><a href="C4MLRANOVA.html#cb43-3" tabindex="-1"></a>    MARSTAT <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="st">&quot;&quot;</span>),</span>
<span id="cb43-4"><a href="C4MLRANOVA.html#cb43-4" tabindex="-1"></a>    Number <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">57</span>, <span class="dv">208</span>, <span class="dv">10</span>, <span class="dv">275</span>),</span>
<span id="cb43-5"><a href="C4MLRANOVA.html#cb43-5" tabindex="-1"></a>    Mean <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">10.958</span>, <span class="fl">12.329</span>, <span class="fl">10.825</span>, <span class="fl">11.990</span>),</span>
<span id="cb43-6"><a href="C4MLRANOVA.html#cb43-6" tabindex="-1"></a>    Standard_Deviation <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1.566</span>, <span class="fl">1.822</span>, <span class="fl">2.001</span>, <span class="fl">1.871</span>)</span>
<span id="cb43-7"><a href="C4MLRANOVA.html#cb43-7" tabindex="-1"></a>    )</span>
<span id="cb43-8"><a href="C4MLRANOVA.html#cb43-8" tabindex="-1"></a></span>
<span id="cb43-9"><a href="C4MLRANOVA.html#cb43-9" tabindex="-1"></a><span class="fu">colnames</span>(tableout) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;MARSTAT&quot;</span>, <span class="st">&quot;Number&quot;</span>, <span class="st">&quot;Mean&quot;</span>, <span class="st">&quot;Standard Deviation&quot;</span>)</span>
<span id="cb43-10"><a href="C4MLRANOVA.html#cb43-10" tabindex="-1"></a><span class="fu">rownames</span>(tableout) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Other&quot;</span>, <span class="st">&quot;Married&quot;</span>, <span class="st">&quot;Living together&quot;</span>, <span class="st">&quot;Total&quot;</span>)</span>
<span id="cb43-11"><a href="C4MLRANOVA.html#cb43-11" tabindex="-1"></a></span>
<span id="cb43-12"><a href="C4MLRANOVA.html#cb43-12" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>tableout , </span>
<span id="cb43-13"><a href="C4MLRANOVA.html#cb43-13" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Summary Statistics of Logarithmic Face By Marital Status&#39;</span>, </span>
<span id="cb43-14"><a href="C4MLRANOVA.html#cb43-14" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;crrr&#39;</span>, <span class="at">Digits=</span><span class="dv">3</span>, <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb43-15"><a href="C4MLRANOVA.html#cb43-15" tabindex="-1"></a>         <span class="at">ColWidth =</span> ColWidth4) <span class="sc">%&gt;%</span></span>
<span id="cb43-16"><a href="C4MLRANOVA.html#cb43-16" tabindex="-1"></a>         kableExtra<span class="sc">::</span><span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">width =</span>  <span class="st">&quot;4cm&quot;</span>) </span></code></pre></div>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="C4MLRANOVA.html#cb44-1" tabindex="-1"></a><span class="fu">library</span>(HH)</span>
<span id="cb44-2"><a href="C4MLRANOVA.html#cb44-2" tabindex="-1"></a>Term   <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;CSVData/TermLife.csv&quot;</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb44-3"><a href="C4MLRANOVA.html#cb44-3" tabindex="-1"></a>Term2  <span class="ot">&lt;-</span> <span class="fu">subset</span>(Term, FACE <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb44-4"><a href="C4MLRANOVA.html#cb44-4" tabindex="-1"></a>LNFACE <span class="ot">&lt;-</span> <span class="fu">log</span>(Term2<span class="sc">$</span>FACE)</span>
<span id="cb44-5"><a href="C4MLRANOVA.html#cb44-5" tabindex="-1"></a>LNINCOME <span class="ot">&lt;-</span> <span class="fu">log</span>(Term2<span class="sc">$</span>INCOME)</span>
<span id="cb44-6"><a href="C4MLRANOVA.html#cb44-6" tabindex="-1"></a>MAR0   <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">*</span>(Term2<span class="sc">$</span>MARSTAT <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb44-7"><a href="C4MLRANOVA.html#cb44-7" tabindex="-1"></a></span>
<span id="cb44-8"><a href="C4MLRANOVA.html#cb44-8" tabindex="-1"></a><span class="co">#  FIGURE 4.1</span></span>
<span id="cb44-9"><a href="C4MLRANOVA.html#cb44-9" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">4.1</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">cex=</span><span class="fl">1.1</span>)</span>
<span id="cb44-10"><a href="C4MLRANOVA.html#cb44-10" tabindex="-1"></a><span class="fu">boxplot</span>(LNFACE <span class="sc">~</span> MARSTAT, <span class="at">data=</span>Term2, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>,</span>
<span id="cb44-11"><a href="C4MLRANOVA.html#cb44-11" tabindex="-1"></a>   <span class="at">xlab=</span><span class="st">&quot;Marital Status&quot;</span>)</span>
<span id="cb44-12"><a href="C4MLRANOVA.html#cb44-12" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;LNFACE&quot;</span>, <span class="at">side=</span><span class="dv">2</span>, <span class="at">at=</span><span class="fl">17.2</span>, <span class="at">las=</span><span class="dv">1</span>, <span class="at">cex=</span><span class="fl">1.1</span>, <span class="at">adj=</span>.<span class="dv">4</span>)</span></code></pre></div>
</div>
<p>Are the continuous and categorical variables jointly important determinants of response? To answer this, a regression was run using LNFACE as the response and five explanatory variables: three continuous and two binary (for marital status). Recall that our three continuous explanatory variables are: LNINCOME (logarithmic annual income), the number of years of EDUCATION of the survey respondent, and the number of household members, NUMHH.</p>
<p>For the binary variables, first define MAR0 to be the binary variable that is one if MARSTAT=0 and zero otherwise. Similarly, define MAR1 and MAR2 to be binary variables that indicate MARSTAT=1 and MARSTAT=2, respectively. There is a perfect linear dependency among these three binary variables in that MAR0 + MAR1 + MAR2 = 1 for any survey respondent. Thus, we need only two of the three. However, there is <em>not</em> a perfect dependency among any two of the three. It turns out that cor(MAR0, MAR1) = -0.90, cor(MAR0, MAR2) = -0.10, and cor(MAR1, MAR2) = -0.34.</p>
<p>A regression model was run using LNINCOME, EDUCATION, NUMHH, MAR0, and MAR2 as explanatory variables. The fitted regression equation turns out to be:</p>
<p><span class="math display">\[
\small{
\widehat{y} = 3.395 + 0.452 \text{ LNINCOME} + 0.205 \text{ EDUCATION} + 0.248 \text{ NUMHH} - 0.557 \text{ MAR0} - 0.789 \text{ MAR2}.
}
\]</span></p>
<p>To interpret the regression coefficients associated with marital status, consider a respondent who is married. In this case, MAR0=0, MAR1=1, and MAR2=0, so that:</p>
<p><span class="math display">\[
\small{
\widehat{y}_m = 3.395 + 0.452 \text{ LNINCOME} + 0.205 \text{ EDUCATION} + 0.248 \text{ NUMHH}.
}
\]</span></p>
<p>Similarly, if the respondent is coded as living together, then <code>MAR0=0</code>, MAR1=0, and MAR2=1, and:</p>
<p><span class="math display">\[
\small{
\widehat{y}_{lt} = 3.395 + 0.452 \text{ LNINCOME} + 0.205 \text{ EDUCATION} + 0.248 \text{ NUMHH} - 0.789.
}
\]</span></p>
<p>The difference between <span class="math inline">\(\widehat{y}_m\)</span> and <span class="math inline">\(\widehat{y}_{lt}\)</span> is <span class="math inline">\(0.789.\)</span> Thus, we may interpret the regression coefficient associated with MAR2, <span class="math inline">\(-0.789\)</span>, to be the difference in fitted values for someone living together compared to a similar person who is married (the omitted category).</p>
<p>Similarly, we can interpret <span class="math inline">\(-0.557\)</span> to be the difference between the “other” category and the married category, holding other explanatory variables fixed. For the difference in fitted values between the “other” and the “living together” categories, we may use <span class="math inline">\(-0.557 - (-0.789) = 0.232.\)</span></p>
<p>Although the regression was run using MAR0 and MAR2, any two out of the three would produce the same ANOVA Table (Table <a href="C4MLRANOVA.html#tab:Tab42">4.2</a>). However, the choice of binary variables does impact the regression coefficients. Table <a href="C4MLRANOVA.html#tab:Tab43">4.3</a> shows three models, omitting MAR1, MAR2, and MAR0, respectively. For each fit, the coefficients associated with the continuous variables remain the same. As we have seen, the binary variable interpretations are with respect to the omitted category, known as the <em>reference level</em>. Although they change from model to model, their overall interpretation remains the same. That is, if we would like to estimate the difference in coverage between the “other” and the “living together” category, the estimate would be <span class="math inline">\(0.232\)</span>, regardless of the model.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab42">Table 4.2: </span><strong>Term Life with Marital Status ANOVA Table</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Source
</th>
<th style="text-align:right;">
Sum of Squares
</th>
<th style="text-align:right;">
<span class="math inline">\(df\)</span>
</th>
<th style="text-align:right;">
Mean Square
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Regression
</td>
<td style="text-align:right;width: 1.8cm; ">
343.28
</td>
<td style="text-align:right;width: 1.8cm; ">
5
</td>
<td style="text-align:right;width: 1.8cm; ">
68.66
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Error
</td>
<td style="text-align:right;width: 1.8cm; ">
615.62
</td>
<td style="text-align:right;width: 1.8cm; ">
269
</td>
<td style="text-align:right;width: 1.8cm; ">
2.29
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Total
</td>
<td style="text-align:right;width: 1.8cm; ">
948.90
</td>
<td style="text-align:right;width: 1.8cm; ">
274
</td>
<td style="text-align:right;width: 1.8cm; ">
</td>
</tr>
</tbody>
</table>
<p>Although the three models in Table <a href="C4MLRANOVA.html#tab:Tab43">4.3</a> are
the same except for different choices of parameters, they do appear
different. In particular, the <span class="math inline">\(t\)</span>-ratios differ and give different
appearances of statistical significance. For example, both of the
<span class="math inline">\(t\)</span>-ratios associated with marital status in Model 2 are less than 2
in absolute value, suggesting that marital status is unimportant. In
contrast, both Models 1 and 3 have at least one marital status
binary that exceeds 2 in absolute value, suggesting statistical
significance. Thus, you can influence the <em>appearance</em> of
statistical significance by altering the choice of the reference
level. To assess the overall importance of marital status (not just
each binary variable), Section <a href="C4MLRANOVA.html#Sec42">4.2</a> will introduce
tests of sets of regression coefficients.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab43">Table 4.3: </span><strong>Term Life Regression Coefficients with Marital Status</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Model 1 Coefficient
</th>
<th style="text-align:right;">
Model 1 <span class="math inline">\(t\)</span>-Ratio
</th>
<th style="text-align:right;">
Model 2 Coefficient
</th>
<th style="text-align:right;">
Model 2 <span class="math inline">\(t\)</span>-Ratio
</th>
<th style="text-align:right;">
Model 3 Coefficient
</th>
<th style="text-align:right;">
Model 3 <span class="math inline">\(t\)</span>-Ratio
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
LNINCOME
</td>
<td style="text-align:right;width: 1.8cm; ">
0.452
</td>
<td style="text-align:right;width: 1.8cm; border-right:1px solid;">
5.74
</td>
<td style="text-align:right;width: 1.8cm; ">
0.452
</td>
<td style="text-align:right;width: 1.8cm; border-right:1px solid;">
5.74
</td>
<td style="text-align:right;width: 1.8cm; ">
0.452
</td>
<td style="text-align:right;width: 1.8cm; ">
5.74
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
EDUCATION
</td>
<td style="text-align:right;width: 1.8cm; ">
0.205
</td>
<td style="text-align:right;width: 1.8cm; border-right:1px solid;">
5.3
</td>
<td style="text-align:right;width: 1.8cm; ">
0.205
</td>
<td style="text-align:right;width: 1.8cm; border-right:1px solid;">
5.3
</td>
<td style="text-align:right;width: 1.8cm; ">
0.205
</td>
<td style="text-align:right;width: 1.8cm; ">
5.3
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
NUMHH
</td>
<td style="text-align:right;width: 1.8cm; ">
0.248
</td>
<td style="text-align:right;width: 1.8cm; border-right:1px solid;">
3.57
</td>
<td style="text-align:right;width: 1.8cm; ">
0.248
</td>
<td style="text-align:right;width: 1.8cm; border-right:1px solid;">
3.57
</td>
<td style="text-align:right;width: 1.8cm; ">
0.248
</td>
<td style="text-align:right;width: 1.8cm; ">
3.57
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
Intercept
</td>
<td style="text-align:right;width: 1.8cm; ">
3.395
</td>
<td style="text-align:right;width: 1.8cm; border-right:1px solid;">
3.77
</td>
<td style="text-align:right;width: 1.8cm; ">
3.395
</td>
<td style="text-align:right;width: 1.8cm; border-right:1px solid;">
2.74
</td>
<td style="text-align:right;width: 1.8cm; ">
2.838
</td>
<td style="text-align:right;width: 1.8cm; ">
3.34
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
MAR0
</td>
<td style="text-align:right;width: 1.8cm; ">
-0.557
</td>
<td style="text-align:right;width: 1.8cm; border-right:1px solid;">
-2.15
</td>
<td style="text-align:right;width: 1.8cm; ">
0.232
</td>
<td style="text-align:right;width: 1.8cm; border-right:1px solid;">
0.44
</td>
<td style="text-align:right;width: 1.8cm; ">
</td>
<td style="text-align:right;width: 1.8cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
MAR1
</td>
<td style="text-align:right;width: 1.8cm; ">
</td>
<td style="text-align:right;width: 1.8cm; border-right:1px solid;">
</td>
<td style="text-align:right;width: 1.8cm; ">
0.789
</td>
<td style="text-align:right;width: 1.8cm; border-right:1px solid;">
1.59
</td>
<td style="text-align:right;width: 1.8cm; ">
0.557
</td>
<td style="text-align:right;width: 1.8cm; ">
2.15
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
MAR2
</td>
<td style="text-align:right;width: 1.8cm; ">
-0.789
</td>
<td style="text-align:right;width: 1.8cm; border-right:1px solid;">
-1.59
</td>
<td style="text-align:right;width: 1.8cm; ">
</td>
<td style="text-align:right;width: 1.8cm; border-right:1px solid;">
</td>
<td style="text-align:right;width: 1.8cm; ">
-0.232
</td>
<td style="text-align:right;width: 1.8cm; ">
-0.44
</td>
</tr>
</tbody>
</table>
<hr />
<p><strong>Example: How does Cost-Sharing in Insurance Plans affect Expenditures in Healthcare?</strong> In one of many studies that resulted from the Rand Health Insurance Experiment (HIE) introduced in Section 1.5, Keeler and Rolph (1988) investigated the effects of cost-sharing in insurance plans. For this study, 14 health insurance plans were grouped by the co-insurance rate (the percentage paid as out-of-pocket expenditures that varied by 0, 25, 50 and 95%). One of the 95% plans limited annual out-of-pocket outpatient expenditures to 150 per person (450 per family), providing in effect an individual outpatient deductible. This plan was analyzed as a separate group so that there were <span class="math inline">\(c=5\)</span> categories of insurance plans.</p>
<p>In most insurance studies, individuals choose insurance plans making it difficult to assess cost-sharing effects because of adverse selection. Adverse selection can arise because individuals in poor chronic health are more likely to choose plans with less cost sharing, thus giving the appearance that less coverage leads to greater expenditures. In the Rand HIE, individuals were randomly assigned to plans, thus removing this potential source of bias.</p>
<p>Keeler and Rolph (1988) organized an individual’s expenditures into episodes of treatment; each episode contains spending associated with a given bout of illness, chronic condition or procedure. Episodes were classified as hospital, dental or outpatient; this classification was based primarily on diagnoses, not by location of services. Thus, for example, outpatient services preceding or following a hospitalization, as well as related drugs and tests, were included as part of a hospital episode.</p>
<p>For simplicity, here we report only results for hospital episodes. Although families were randomly assigned to plans, Keeler and Rolph (1988) used regression methods to control for participant attributes and isolate the effects of plan cost-sharing. Table <a href="C4MLRANOVA.html#tab:Tab44">4.4</a> summarizes the regression coefficients, based on a sample of <span class="math inline">\(n=1,967\)</span> episode expenditures. In this regression, logarithmic expenditure was the dependent variable.</p>
<p>The cost-sharing categorical variable was decomposed into five binary variables so that no functional form was imposed on the response to insurance. These variables are “Co-ins25,” “Co-ins50,” and “Co-ins95,” for coinsurance rates 25, 50 and 95%, respectively, and “Indiv Deductible” for the plan with individual deductibles. The omitted variable is the free insurance plan with 0% coinsurance. The HIE was conducted in six cities; a categorical variable to control for the location was represented with five binary variables, Dayton, Fitchburg, Franklin, Charleston and Georgetown, with Seattle being the omitted variable. A categorical factor with <span class="math inline">\(c=6\)</span> levels was used for age and sex; binary variables in the model consisted of “Age 0-2,” “Age 3-5,” “Age 6-17,” “Woman age 18-65,” and “Man age 46-65,” the omitted category was “Man age 18-45.” Other control variables included a health status scale, socioeconomic status, number of medical visits in the year prior to the experiment on a logarithmic scale and race.</p>
<p>Table <a href="C4MLRANOVA.html#tab:Tab44">4.4</a> summarizes the effects of the variables. As noted by Keeler and Rolph, there were large differences by site and age although the regression only served to summarize <span class="math inline">\(R^2=11\%\)</span> of the variability. For the cost-sharing variables, only “Co-ins95” was statistically significant, and this only at the 5% level, not the 1% level.</p>
<p>The paper of Keeler and Rolph (1988) examines other types of episode expenditures, as well as the frequency of expenditures. They concluded that cost-sharing of health insurance plans has little effect on the amount of expenditures per episode although there are important differences in the frequency of episodes. This is because an episode of treatment is composed of two decisions. The amount of treatment is made jointly between the patient and the physician and is largely unaffected by the type of health insurance plan. The decision to seek health care treatment is made by the patient; this decision-making process is more susceptible to economic incentives in cost-sharing aspects of health insurance plans.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab44">Table 4.4: </span><strong>Coefficients of Episode Expenditures from the Rand HIE</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:center;">
Regression Coefficient
</th>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:center;">
Regression Coefficient
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; ">
Intercept
</td>
<td style="text-align:center;width: 3.5cm; border-right:1px solid;">
7.95
</td>
<td style="text-align:left;width: 3.5cm; ">
</td>
<td style="text-align:center;width: 3.5cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; ">
Dayton
</td>
<td style="text-align:center;width: 3.5cm; border-right:1px solid;">
0.13*
</td>
<td style="text-align:left;width: 3.5cm; ">
Co-ins25
</td>
<td style="text-align:center;width: 3.5cm; ">
0.07
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; ">
Fitchburg
</td>
<td style="text-align:center;width: 3.5cm; border-right:1px solid;">
0.12
</td>
<td style="text-align:left;width: 3.5cm; ">
Co-ins50
</td>
<td style="text-align:center;width: 3.5cm; ">
0.02
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; ">
Franklin
</td>
<td style="text-align:center;width: 3.5cm; border-right:1px solid;">
-0.01
</td>
<td style="text-align:left;width: 3.5cm; ">
Co-ins95
</td>
<td style="text-align:center;width: 3.5cm; ">
-0.13*
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; ">
Charleston
</td>
<td style="text-align:center;width: 3.5cm; border-right:1px solid;">
0.20*
</td>
<td style="text-align:left;width: 3.5cm; ">
Indiv Deductible
</td>
<td style="text-align:center;width: 3.5cm; ">
-0.03
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; ">
Georgetown
</td>
<td style="text-align:center;width: 3.5cm; border-right:1px solid;">
-0.18*
</td>
<td style="text-align:left;width: 3.5cm; ">
</td>
<td style="text-align:center;width: 3.5cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; ">
Health scale
</td>
<td style="text-align:center;width: 3.5cm; border-right:1px solid;">
-0.02*
</td>
<td style="text-align:left;width: 3.5cm; ">
Age 0-2
</td>
<td style="text-align:center;width: 3.5cm; ">
-0.63**
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; ">
Socioeconomic status
</td>
<td style="text-align:center;width: 3.5cm; border-right:1px solid;">
0.03
</td>
<td style="text-align:left;width: 3.5cm; ">
Age 3-5
</td>
<td style="text-align:center;width: 3.5cm; ">
-0.64**
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; ">
Medical visits
</td>
<td style="text-align:center;width: 3.5cm; border-right:1px solid;">
-0.03
</td>
<td style="text-align:left;width: 3.5cm; ">
Age 6-17
</td>
<td style="text-align:center;width: 3.5cm; ">
-0.30**
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; ">
Examination
</td>
<td style="text-align:center;width: 3.5cm; border-right:1px solid;">
-0.10*
</td>
<td style="text-align:left;width: 3.5cm; ">
Woman age 18-65
</td>
<td style="text-align:center;width: 3.5cm; ">
0.11
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; ">
Black
</td>
<td style="text-align:center;width: 3.5cm; border-right:1px solid;">
0.14*
</td>
<td style="text-align:left;width: 3.5cm; ">
Man age 46-65
</td>
<td style="text-align:center;width: 3.5cm; ">
0.26
</td>
</tr>
</tbody>
</table>
<p><em>Note</em>: * significant at 5%, ** significant at 1%</p>
<p><em>Source</em>: Keeler and Rolph (1988)</p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_0fwsarjm&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_5pv7wf31%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="4.1 BinaryVariables">
</iframe>
</center>
</div>
<div id="Sec42" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Statistical Inference for Several Coefficients<a href="C4MLRANOVA.html#Sec42" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It can be useful to examine several regression coefficients at the same time. For example, when assessing the effect of a categorical variable with <span class="math inline">\(c\)</span> levels, we need to say something jointly about the <span class="math inline">\(c-1\)</span> binary variables that enter the regression equation. To do this, Section <a href="C4MLRANOVA.html#Sec421">4.2.1</a> introduces a method for handling linear combinations of regression coefficients. Section <a href="C4MLRANOVA.html#Sec422">4.2.2</a> shows how to test several linear combinations, and Section <a href="C4MLRANOVA.html#Sec423">4.2.3</a> presents other inference applications.</p>
<div id="Sec421" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Sets of Regression Coefficients<a href="C4MLRANOVA.html#Sec421" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that our regression coefficients are specified by <span class="math inline">\(\boldsymbol{\beta} = \left( \beta_0, \beta_1, \ldots, \beta_k \right)^{\prime},\)</span> a <span class="math inline">\((k+1) \times 1\)</span> vector. It will be convenient to express linear combinations of the regression coefficients using the notation <span class="math inline">\(\mathbf{C} \boldsymbol{\beta},\)</span> where <span class="math inline">\(\mathbf{C}\)</span> is a <span class="math inline">\(p \times (k+1)\)</span> matrix that is user-specified and depends on the application. Some applications involve estimating <span class="math inline">\(\mathbf{C} \boldsymbol{\beta}\)</span>. Others involve testing whether <span class="math inline">\(\mathbf{C} \boldsymbol{\beta}\)</span> equals a specific known value (denoted as <span class="math inline">\(\mathbf{d}\)</span>). We call <span class="math inline">\(H_0:\mathbf{C \boldsymbol{\beta} = d}\)</span> the <em>general linear hypothesis</em>. To demonstrate the broad variety of applications in which sets of regression coefficients can be used, we now present a series of special cases.</p>
<p><strong>Special Case 1: One Regression Coefficient</strong>. In Section 3.4, we investigated the importance of a single coefficient, say <span class="math inline">\(\beta_j.\)</span> We may express this coefficient as <span class="math inline">\(\mathbf{C} \boldsymbol{\beta}\)</span> by choosing <span class="math inline">\(p=1\)</span> and <span class="math inline">\(\mathbf{C}\)</span> to be a <span class="math inline">\(1 \times (k+1)\)</span> vector with a one in the <span class="math inline">\((j+1)\)</span>st column and zeros otherwise. These choices result in</p>
<p><span class="math display">\[
\mathbf{C \boldsymbol{\beta} =} \left( 0~\ldots~0~1~0~\ldots~0\right) \left( \begin{array}{c} \beta_0 \\ \vdots  \\ \beta_k \end{array} \right) = \beta_j.
\]</span></p>
<p><strong>Special Case 2: Regression Function</strong>. Here, we choose <span class="math inline">\(p=1\)</span> and <span class="math inline">\(\mathbf{C}\)</span> to be a <span class="math inline">\(1 \times (k+1)\)</span> vector representing the transpose of a set of explanatory variables. These choices result in</p>
<p><span class="math display">\[
\mathbf{C \boldsymbol{\beta} =} \left( x_0, x_1, \ldots, x_k \right) \left( \begin{array}{c} \beta_0 \\ \vdots  \\ \beta_k \end{array} \right) = \beta_0 x_0 + \beta_1 x_1 + \ldots + \beta_k x_k = \mathrm{E}~y,
\]</span>
the regression function.</p>
<p><strong>Special Case 3: Linear Combination of Regression Coefficients</strong>. When <span class="math inline">\(p=1\)</span>, we use the convention that lower-case bold letters are vectors and let <span class="math inline">\(\mathbf{C = c^{\prime}} = \left( c_0, \ldots, c_k \right)^{\prime}\)</span>. In this case, <span class="math inline">\(\mathbf{C} \boldsymbol{\beta}\)</span> is a generic linear combination of regression coefficients</p>
<p><span class="math display">\[
\mathbf{C \boldsymbol{\beta} = \mathbf{c}^{\prime} \boldsymbol{\beta} = c_0 \beta_0 + \ldots + c_k \beta_k}.
\]</span></p>
<p><strong>Special Case 4: Testing Equality of Regression Coefficients</strong>. Suppose that the interest is in testing <span class="math inline">\(H_0: \beta_1 = \beta_2.\)</span> For this purpose, let <span class="math inline">\(p=1\)</span>, <span class="math inline">\(\mathbf{c}^{\prime} = \left( 0, 1, -1, 0, \ldots, 0\right),\)</span> and <span class="math inline">\(\mathbf{d} = 0\)</span>. With these choices, we have</p>
<p><span class="math display">\[
\mathbf{C \boldsymbol{\beta} = c^{\prime} \boldsymbol{\beta} =} \left( 0, 1, -1, 0, \ldots, 0\right) \left( \begin{array}{c} \beta_0 \\ \vdots  \\ \beta_k \end{array} \right) = \beta_1 - \beta_2 = 0,
\]</span></p>
<p>so that the general linear hypothesis reduces to <span class="math inline">\(H_0: \beta_1 = \beta_2.\)</span></p>
<p><strong>Special Case 5: Adequacy of the Model</strong>. It is customary in regression analysis to present a test of whether or not <em>any</em> of the explanatory variables are useful for explaining the response. Formally, this is a test of the null hypothesis <span class="math inline">\(H_0:\beta_1=\beta_2=\ldots=\beta_k=0\)</span>. Note that, as a convention, one does not test whether or not the intercept is zero. To test this using the general linear hypothesis, we choose <span class="math inline">\(p=k\)</span>, <span class="math inline">\(\mathbf{d}=\left( 0~\ldots~0\right)^{\prime}\)</span> to be a <span class="math inline">\(k \times 1\)</span> vector of zeros and <span class="math inline">\(\mathbf{C}\)</span> to be a <span class="math inline">\(k \times (k+1)\)</span> matrix such that</p>
<p><span class="math display">\[
\small{
\mathbf{C \boldsymbol{\beta} =}\left(
\begin{array}{ccccc}
0 &amp; 1 &amp; 0 &amp; \cdots  &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; \cdots  &amp; 0 \\
\vdots  &amp; \vdots  &amp; \vdots  &amp; \ddots  &amp; \vdots  \\
0 &amp; 0 &amp; 0 &amp; \cdots  &amp; 1
\end{array}
\right) \left(
\begin{array}{c}
\beta_0 \\
\vdots  \\
\beta_k
\end{array}
\right) =\left(
\begin{array}{c}
\beta_1 \\
\vdots  \\
\beta_k
\end{array}
\right) =\left(
\begin{array}{c}
0 \\
\vdots  \\
0
\end{array}
\right) =\mathbf{d}.
}
\]</span></p>
<p><strong>Special Case 6: Testing Portions of the Model</strong>. Suppose that we are interested in comparing a <em>full</em> regression function</p>
<p><span class="math display">\[
\mathrm{E~}y = \beta_0 + \beta_1 x_1 +\ldots + \beta_k x_k + \beta_{k+1} x_{k+1} + \ldots + \beta_{k+p} x_{k+p}
\]</span>
to a <em>reduced</em> regression function,</p>
<p><span class="math display">\[
\mathrm{E~}y = \beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k.
\]</span>
Beginning with the full regression, we see that if the null hypothesis <span class="math inline">\(H_0:\beta_{k+1} = \ldots = \beta_{k+p} = 0\)</span> holds, then we arrive at the reduced regression. To illustrate, the variables <span class="math inline">\(x_{k+1}, \ldots, x_{k+p}\)</span> may refer to several binary variables representing a categorical variable and our interest is in whether the categorical variable is important. To test the importance of the categorical variable, we want to see whether the binary variables <span class="math inline">\(x_{k+1}, \ldots, x_{k+p}\)</span> <em>jointly</em> affect the dependent variables.</p>
<p>To test this using the general linear hypothesis, we choose <span class="math inline">\(\mathbf{d}\)</span> and <span class="math inline">\(\mathbf{C}\)</span> such that</p>
<p><span class="math display">\[
\small{
\mathbf{C \boldsymbol{\beta} =}\left(
\begin{array}{ccccccc}
0 &amp; \cdots  &amp; 0 &amp; 1 &amp; 0 &amp; \cdots  &amp; 0 \\
0 &amp; \cdots  &amp; 0 &amp; 0 &amp; 1 &amp; \cdots  &amp; 0 \\
\vdots  &amp; \vdots  &amp; \vdots  &amp; \vdots  &amp; \vdots  &amp; \ddots  &amp; \vdots  \\
0 &amp; \cdots  &amp; 0 &amp; 0 &amp; 0 &amp; \cdots  &amp; 1
\end{array}
\right) \left(
\begin{array}{c}
\beta_0 \\
\vdots  \\
\beta_k \\
\beta_{k+1} \\
\vdots  \\
\beta_{k+p}
\end{array}
\right) =\left(
\begin{array}{c}
\beta_{k+1} \\
\vdots  \\
\beta_{k+p}
\end{array}
\right) =\left(
\begin{array}{c}
0 \\
\vdots  \\
0
\end{array}
\right) =\mathbf{d}.
}
\]</span></p>
<p>From a list of <span class="math inline">\(k+p\)</span> variables <span class="math inline">\(x_1, \ldots, x_{k+p}\)</span>, you may drop any <span class="math inline">\(p\)</span> that you deem appropriate. The additional variables do not need to be the last <span class="math inline">\(p\)</span> in the regression specification. Dropping <span class="math inline">\(x_{k+1}, \ldots, x_{k+p}\)</span> is for notational convenience only.</p>
<div class="blackboxvideo">
<p><strong>Video: Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_mvopry50&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_2f2fe0t8%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="4.2 SetRegrCoefficients">
</iframe>
</center>
</div>
<div id="Sec422" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> The General Linear Hypothesis<a href="C4MLRANOVA.html#Sec422" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To recap, the general linear hypothesis can be stated as <span class="math inline">\(H_0:\mathbf{C \boldsymbol{\beta} =d}\)</span>. Here, <span class="math inline">\(\mathbf{C}\)</span> is a <span class="math inline">\(p \times (k+1)\)</span> matrix, <span class="math inline">\(\mathbf{d}\)</span> is a <span class="math inline">\(p \times 1\)</span> vector, and both <span class="math inline">\(\mathbf{C}\)</span> and <span class="math inline">\(\mathbf{d}\)</span> are user-specified and depend on the application at hand. Although <span class="math inline">\(k+1\)</span> is the number of regression coefficients, <span class="math inline">\(p\)</span> is the number of restrictions under <span class="math inline">\(H_0\)</span> on these coefficients. (For those readers with knowledge of advanced matrix algebra, <span class="math inline">\(p\)</span> is the rank of <span class="math inline">\(\mathbf{C}\)</span>.) This null hypothesis is tested against the alternative <span class="math inline">\(H_a:\mathbf{C \boldsymbol{\beta} \neq d}\)</span>. This may be obvious, but we do require <span class="math inline">\(p \leq k+1\)</span> because we cannot test more constraints than free parameters.</p>
<p>To understand the basis for the testing procedure, we first recall some of the basic properties of the regression coefficient estimators described in Section 3.3. Now, however, our goal is to understand properties of the linear combinations of regression coefficients specified by <span class="math inline">\(\mathbf{C \boldsymbol{\beta}}\)</span>. A natural estimator of this quantity is <span class="math inline">\(\mathbf{Cb}\)</span>. It is easy to see that <span class="math inline">\(\mathbf{Cb}\)</span> is an unbiased estimator of <span class="math inline">\(\mathbf{C \boldsymbol{\beta}}\)</span>, because <span class="math inline">\(\mathrm{E~}\mathbf{Cb=C}\mathrm{E~}\mathbf{b=C \boldsymbol{\beta}}\)</span>. Moreover, the variance is <span class="math inline">\(\mathrm{Var}\left( \mathbf{Cb}\right) \mathbf{=C}\mathrm{Var}\left( \mathbf{b}\right) \mathbf{C}^{\prime}=\sigma^2 \mathbf{C}\left( \mathbf{X^{\prime}X}\right)^{-1} \mathbf{C}^{\prime}\)</span>. To assess the difference between <span class="math inline">\(\mathbf{d}\)</span>, the hypothesized value of <span class="math inline">\(\mathbf{C \boldsymbol{\beta}}\)</span>, and its estimated value, <span class="math inline">\(\mathbf{Cb}\)</span>, we use the following statistic:</p>
<p><span class="math display" id="eq:eq41">\[
F-\text{ratio}=\frac{(\mathbf{Cb-d)}^{\prime}\left( \mathbf{C}\left( \mathbf{X^{\prime}X} \right)^{-1} \mathbf{C}^{\prime}\right)^{-1}(\mathbf{Cb-d)}}{ps_{full}^2}.
\tag{4.1}
\]</span></p>
<p>Here, <span class="math inline">\(s_{full}^2\)</span> is the mean square error from the full regression model. Using the theory of linear models, it can be checked that the statistic <span class="math inline">\(F\)</span>-ratio has an <span class="math inline">\(F\)</span>-distribution with numerator degrees of freedom <span class="math inline">\(df_1=p\)</span> and denominator degrees of freedom <span class="math inline">\(df_2=n-(k+1)\)</span>. Both the statistic and the theoretical distribution are named for R. A. Fisher, a renowned scientist and statistician who did much to advance statistics as a science in the early half of the twentieth century.</p>
<p>Like the normal and the <span class="math inline">\(t\)</span>-distribution, the <span class="math inline">\(F\)</span>-distribution is a continuous distribution. The <span class="math inline">\(F\)</span>-distribution is the sampling distribution for the <span class="math inline">\(F\)</span>-ratio and is proportional to the ratio of two sums of squares, each of which is positive or zero. Thus, unlike the normal distribution and the <span class="math inline">\(t\)</span>-distribution, the <span class="math inline">\(F\)</span>-distribution takes on only nonnegative values. Recall that the <span class="math inline">\(t\)</span>-distribution is indexed by a single degree of freedom parameter. The <span class="math inline">\(F\)</span>-distribution is indexed by two degrees of freedom parameters: one for the numerator, <span class="math inline">\(df_1\)</span>, and one for the denominator, <span class="math inline">\(df_2\)</span>. Appendix A3.4 provides additional details.</p>
<p>The test statistic in equation <a href="C4MLRANOVA.html#eq:eq41">(4.1)</a> is complex in form. Fortunately, there is an alternative that is simpler to implement and to interpret; this alternative is based on the <em>extra sum of squares principle</em>.</p>
<div class="blackbox">
<p><em>Procedure for Testing the General Linear Hypothesis</em></p>
<ol style="list-style-type: decimal">
<li>Run the full regression and get the error sum of squares and mean square error, which we label as <span class="math inline">\((Error~SS)_{full}\)</span> and <span class="math inline">\(s_{full}^2\)</span>, respectively.</li>
<li>Consider the model assuming the null hypothesis is true. Run a regression with this model and get the error sum of squares, which we label <span class="math inline">\((Error~SS)_{reduced}\)</span>.</li>
<li>Calculate
<span class="math display" id="eq:eq42">\[
F-\text{ratio}=\frac{(Error~SS)_{reduced}-(Error~SS)_{full}}{ps_{full}^2}.
\tag{4.2}
\]</span></li>
<li>Reject the null hypothesis in favor of the alternative if the <span class="math inline">\(F\)</span>-ratio exceeds an <span class="math inline">\(F\)</span>-value. The <span class="math inline">\(F\)</span>-value is a percentile from the <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(df_1=p\)</span> and <span class="math inline">\(df_2=n-(k+1)\)</span> degrees of freedom. The percentile is one minus the significance level of the test. Following our notation with the <span class="math inline">\(t\)</span>-distribution, we denote this percentile as <span class="math inline">\(F_{p,n-(k+1),1-\alpha}\)</span>, where <span class="math inline">\(\alpha\)</span> is the significance level.</li>
</ol>
</div>
<p>This procedure is commonly known as an <span class="math inline">\(F\)</span>-test.</p>
<p>Section <a href="C4MLRANOVA.html#Sec472">4.7.2</a> provides the mathematical underpinnings. To understand the extra-sum-of-squares principle, recall that the error sum of squares for the full model is determined to be the minimum value of</p>
<p><span class="math display">\[
SS(b_0^{\ast}, \ldots, b_k^{\ast}) = \sum_{i=1}^{n} \left( y_i - \left( b_0^{\ast} + \ldots + b_k^{\ast} x_{i,k} \right) \right)^2.
\]</span></p>
<p>Here, <span class="math inline">\(SS(b_0^{\ast}, \ldots, b_k^{\ast})\)</span> is a function of <span class="math inline">\(b_0^{\ast}, \ldots, b_k^{\ast}\)</span> and <span class="math inline">\((Error~SS)_{full}\)</span> is the minimum over all possible values of <span class="math inline">\(b_0^{\ast}, \ldots, b_k^{\ast}\)</span>. Similarly, <span class="math inline">\((Error~SS)_{reduced}\)</span> is the minimum error sum of squares under the constraints in the null hypothesis. Because there are fewer possibilities under the null hypothesis, we have that</p>
<p><span class="math display" id="eq:eq43">\[
(Error~SS)_{full} \leq (Error~SS)_{reduced}.
\tag{4.3}
\]</span></p>
<p>To illustrate, consider our first special case where <span class="math inline">\(H_0 : \beta_j = 0\)</span>. In this case, the difference between the full and reduced models amounts to dropping a variable. A consequence of equation <a href="C4MLRANOVA.html#eq:eq43">(4.3)</a> is that, when adding variables to a regression model, the error sum of squares never goes up (and, in fact, usually goes down). Thus, adding variables to a regression model increases <span class="math inline">\(R^2\)</span>, the coefficient of determination.</p>
<p>How large a decrease in the error sum of squares is statistically significant? Intuitively, one can view the <span class="math inline">\(F\)</span>-ratio as the difference in the error sum of squares divided by the number of constraints, <span class="math inline">\(\frac{(Error~SS)_{reduced}-(Error~SS)_{full}}{p}\)</span>, and then rescaled by the best estimate of the variance term, the <span class="math inline">\(s^2\)</span>, from the full model. Under the null hypothesis, this statistic follows an <span class="math inline">\(F\)</span>-distribution and we may compare the test statistic to this distribution to see if it is unusually large.</p>
<p>Using the relationship <span class="math inline">\(Regression~SS = Total~SS - Error~SS\)</span>, we can re-express the difference in the error sum of squares as</p>
<p><span class="math display">\[
(Error~SS)_{reduced} - (Error~SS)_{full} = (Regression~SS)_{full} - (Regression~SS)_{reduced}.
\]</span></p>
<p>This difference is known as a <em>Type III Sum of Squares</em>. When testing the importance of a set of explanatory variables, <span class="math inline">\(x_{k+1}, \ldots, x_{k+p}\)</span>, in the presence of <span class="math inline">\(x_1, \ldots, x_k\)</span>, you will find that many statistical software packages compute this quantity directly in a single regression run. The advantage of this is it allows the analyst to perform an <span class="math inline">\(F\)</span>-test using a single regression run, instead of two regression runs as in our four-step procedure described above.</p>
<hr />
<p><strong>Example: Term Life Insurance - Continued.</strong> Before discussing the logic and the implications of the <span class="math inline">\(F\)</span>-test, let us illustrate its use. In the Term Life Insurance example, suppose that we wish to understand the impact of marital status. Table <a href="C4MLRANOVA.html#tab:Tab43">4.3</a> presented a mixed message in terms of <span class="math inline">\(t\)</span>-ratios; sometimes they were statistically significant and sometimes not. It would be helpful to have a formal test to give a definitive answer, at least in terms of statistical significance. Specifically, we consider a regression model using LNINCOME, EDUCATION, NUMHH, MAR0, and MAR2 as explanatory variables. The model equation is</p>
<p><span class="math display">\[
\small{
\begin{array}{ll}
y &amp;= \beta_0 + \beta_1 \text{LNINCOME} + \beta_2 \text{EDUCATION} + \beta_3 \text{NUMHH} \\
&amp; \ \ \ \ + \beta_4 \text{MAR0} + \beta_5 \text{MAR2}.
\end{array}
}
\]</span></p>
<p>Our goal is to test <span class="math inline">\(H_0: \beta_4 = \beta_5 = 0\)</span>.</p>
<ol style="list-style-type: lower-roman">
<li>We begin by running a regression model with all <span class="math inline">\(k+p=5\)</span> variables. The results were reported in Table <a href="C4MLRANOVA.html#tab:Tab42">4.2</a>, where we saw that <span class="math inline">\((Error~SS)_{full} = 615.62\)</span> and <span class="math inline">\(s_{full}^2 = (1.513)^2 = 2.289\)</span>.</li>
<li>The next step is to run the reduced model without MAR0 and MAR2. This was done in Table 3.3 of Chapter 3, where we saw that <span class="math inline">\((Error~SS)_{reduced} = 630.43\)</span>.</li>
<li>We then calculate the test statistic
<span class="math display">\[
F-\text{ratio} = \frac{(Error~SS)_{reduced} - (Error~SS)_{full}}{ps_{full}^2} = \frac{630.43 - 615.62}{2 \times 2.289} = 3.235.
\]</span></li>
<li>The fourth step compares the test statistic to an <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(df_1=p=2\)</span> and <span class="math inline">\(df_2 = n-(k+p+1) = 269\)</span> degrees of freedom. Using a 5% level of significance, it turns out that the 95th percentile is <span class="math inline">\(F-\text{value} \approx 3.029\)</span>. The corresponding <span class="math inline">\(p\)</span>-value is <span class="math inline">\(\Pr(F &gt; 3.235) = 0.0409\)</span>. At the 5% significance level, we reject the null hypothesis <span class="math inline">\(H_0: \beta_4 = \beta_5 = 0\)</span>. This suggests that it is important to use marital status to understand term life insurance coverage, even in the presence of income, education, and number of household members.</li>
</ol>
<hr />
<div id="some-special-cases" class="section level4 unnumbered hasAnchor">
<h4>Some Special Cases<a href="C4MLRANOVA.html#some-special-cases" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The general linear hypothesis test is available when you can express
one model as a subset of another. For this reason, it is useful to
think of it as a device for comparing “smaller” to “larger”
models. However, the smaller model must be a subset of the larger
model. For example, the general linear hypothesis test cannot be
used to compare the regression functions <span class="math inline">\(\mathrm{E~}y = \beta_0 + \beta_7 x_7\)</span> versus <span class="math inline">\(\mathrm{E~}y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4\)</span>. This is because the former,
smaller function is not a subset of the latter, larger function.</p>
<p>The general linear hypothesis can be used in many instances,
although its use is not always necessary. For example, suppose that
we wish to test <span class="math inline">\(H_0:\beta_k=0\)</span>. We have already seen that this null
hypothesis can be examined using the <span class="math inline">\(t\)</span>-ratio test. In this special
case, it turns out that <span class="math inline">\((t-\textrm{ratio})^2=F-\textrm{ratio}\)</span>.
Thus, these tests are equivalent for testing <span class="math inline">\(H_0:\beta_k=0\)</span> versus
<span class="math inline">\(H_a:\beta_k \neq 0\)</span>. The <span class="math inline">\(F\)</span>-test has the advantage that it works
for more than one predictor whereas the <span class="math inline">\(t\)</span>-test has the advantage
that one can consider one-sided alternatives. Thus, both tests are
considered useful.</p>
<p>Dividing the numerator and denominator of equation <a href="C4MLRANOVA.html#eq:eq44">(4.4)</a> by <span class="math inline">\(Total~SS\)</span>, the test statistic can also be written as:</p>
<p><span class="math display" id="eq:eq44">\[
F-\textrm{ratio}=\frac{\left( R_{full}^2-R_{reduced}^2\right) /p}{\left( 1-R_{full}^2\right) / (n-(k+1))}.
\tag{4.4}
\]</span></p>
<p>The interpretation of this expression is that the <span class="math inline">\(F\)</span>-ratio measures
the drop in the coefficient of determination, <span class="math inline">\(R^2\)</span>.</p>
<p>The expression in equation <a href="C4MLRANOVA.html#eq:eq44">(4.4)</a> is particularly useful for testing the adequacy of the model, our Special Case 5. In this case, <span class="math inline">\(p=k\)</span>, and the regression sum of squares under the reduced model is zero. Thus, we have</p>
<p><span class="math display">\[
\small{
F-\textrm{ratio}=\frac{\left( (Regression~SS)_{full}\right) /k}{s_{full}^2}
=\frac{(Regression~MS)_{full}}{(Error~SS)_{full}}.
}
\]</span></p>
<p>This test statistic is a regular feature of the ANOVA table for many
statistical packages.</p>
<p>For example, in our Term Life Insurance example, testing the
adequacy of the model means evaluating <span class="math inline">\(H_0: \beta_1 = \beta_2 =\beta_3 =\beta_4 =\beta_5 = 0\)</span>. From Table
<a href="C4MLRANOVA.html#tab:Tab42">4.2</a>, the <span class="math inline">\(F\)</span>-ratio is 68.66 / 2.29 = 29.98.
With <span class="math inline">\(df_1=5\)</span> and <span class="math inline">\(df_2 = 269\)</span>, we have that the <span class="math inline">\(F\)</span>-value is
approximately 2.248 and the corresponding <span class="math inline">\(p\)</span>-value is <span class="math inline">\(\Pr(F &gt; 29.98) \approx 0\)</span>. This leads us to reject strongly the notion that
the explanatory variables are not useful in understanding term life
insurance coverage, reaffirming what we learned in the graphical and
correlation analysis. Any other result would be surprising.</p>
<p>For another expression, dividing by <span class="math inline">\(Total~SS\)</span>, we may write</p>
<p><span class="math display">\[
F-\textrm{ratio}=\frac{R^2}{1-R^2}\frac{n-(k+1)}{k}.
\]</span></p>
<p>Because both <span class="math inline">\(F\)</span>-ratio and <span class="math inline">\(R^2\)</span> are measures of model fit, it seems
intuitively plausible that they be related in some fashion. A
consequence of this relationship is the fact that as <span class="math inline">\(R^2\)</span>
increases, so does the <span class="math inline">\(F\)</span>-ratio and vice versa. The <span class="math inline">\(F\)</span>-ratio is
used because its sampling distribution is known under a null
hypothesis so we can make statements about statistical significance.
The <span class="math inline">\(R^2\)</span> measure is used because of the easy interpretations
associated with it.</p>
<div class="blackboxvideo">
<p><strong>Video: Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_qhubtvyu&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_xrshg1ms%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="4.3 GenLinearHyp">
</iframe>
</center>
</div>
</div>
<div id="Sec423" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Estimating and Predicting Several Coefficients<a href="C4MLRANOVA.html#Sec423" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="estimating-linear-combinations-of-regression-coefficients" class="section level4 unnumbered hasAnchor">
<h4>Estimating Linear Combinations of Regression Coefficients<a href="C4MLRANOVA.html#estimating-linear-combinations-of-regression-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In some applications, the main interest is to estimate a linear
combination of regression coefficients. To illustrate, recall in
Section 3.5 that we developed a regression function for an
individual’s charitable contributions (<span class="math inline">\(y\)</span>) in terms of their wages
(<span class="math inline">\(x\)</span>). In this function, there was an abrupt change in the function
at <span class="math inline">\(x=97,500\)</span>. To model this, we defined the binary variable <span class="math inline">\(z\)</span> to
be zero if <span class="math inline">\(x&lt;97,500\)</span> and to be one if <span class="math inline">\(x \geq 97,500\)</span>, and the
regression function <span class="math inline">\(\mathrm{E~}y = \beta_0 + \beta_1 x + \beta_2 z(x - 97,500)\)</span>. Thus, the marginal expected change in contributions
per dollar wage change for wages in excess of <span class="math inline">\(97,500\)</span> is <span class="math inline">\(\frac{\partial \left( \mathrm{E~}y\right)}{\partial x} = \beta_1 + \beta_2\)</span>.</p>
<p>To estimate <span class="math inline">\(\beta_1 + \beta_2\)</span>, a reasonable estimator is <span class="math inline">\(b_1 + b_2\)</span>
which is readily available from standard regression software. In
addition, we would also like to compute standard errors for
<span class="math inline">\(b_1 + b_2\)</span> to be used, for example, in determining a confidence
interval for <span class="math inline">\(\beta_1 + \beta_2\)</span>. However, <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span> are
typically correlated so that the calculation of the standard error
of <span class="math inline">\(b_1 + b_2\)</span> requires estimation of the covariance between <span class="math inline">\(b_1\)</span> and
<span class="math inline">\(b_2\)</span>.</p>
<p>Estimating <span class="math inline">\(\beta_1 + \beta_2\)</span> is an example of our Special Case 3
that considers linear combinations of regression coefficients of the
form <span class="math inline">\(\mathbf{c}^{\prime} \boldsymbol \beta = c_0 \beta_0 + c_1 \beta_1 + \ldots + c_k \beta_k\)</span>. For our charitable contribution’s
example, we would choose <span class="math inline">\(c_1 = c_2 = 1\)</span> and other <span class="math inline">\(c\)</span>’s equal to zero.</p>
<p>To estimate <span class="math inline">\(\mathbf{c}^{\prime} \boldsymbol \beta\)</span>, we replace the
vector of parameters by the vector of estimators and use <span class="math inline">\(\mathbf{c}^{\prime} \mathbf{b}\)</span>. To assess the reliability of this estimator, as in Section
4.2.2, we have that <span class="math inline">\(\mathrm{Var}\left( \mathbf{c}^{\prime} \mathbf{b}\right) = \sigma^2 \mathbf{c}^{\prime}(\mathbf{X^{\prime} X})^{-1} \mathbf{c}\)</span>. Thus, we
may define the estimated standard deviation, or standard error, of
<span class="math inline">\(\mathbf{c}^{\prime} \mathbf{b}\)</span> to be</p>
<p><span class="math display">\[
se\left( \mathbf{c}^{\prime} \mathbf{b} \right) = s \sqrt{\mathbf{c}^{\prime} (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{c}}.
\]</span></p>
<p>With this quantity, a <span class="math inline">\(100(1 - \alpha) \%\)</span> confidence interval for
<span class="math inline">\(\mathbf{c}^{\prime} \boldsymbol \beta\)</span> is</p>
<p><span class="math display" id="eq:eq45">\[
\mathbf{c}^{\prime} \mathbf{b} \pm t_{n - (k + 1), 1 - \alpha / 2} ~ se(\mathbf{c}^{\prime} \mathbf{b}).
\tag{4.5}
\]</span></p>
<p>The confidence interval in equation <a href="C4MLRANOVA.html#eq:eq45">(4.5)</a>
is valid under Assumptions F1-F5. If we choose <span class="math inline">\(\mathbf{c}\)</span> to have
a “1” in the <span class="math inline">\((j + 1)^{\text{st}}\)</span> row and zeros otherwise, then
<span class="math inline">\(\mathbf{c}^{\prime} \boldsymbol \beta = \beta_j\)</span>,
<span class="math inline">\(\mathbf{c}^{\prime} \mathbf{b} = b_j\)</span>, and</p>
<p><span class="math display">\[
se(b_j) = s \sqrt{(j + 1)^{\text{st}}~ \textit{diagonal element of } (\mathbf{X}^{\prime} \mathbf{X})^{-1}}.
\]</span></p>
<p>Thus, equation <a href="C4MLRANOVA.html#eq:eq45">(4.5)</a> provides a
theoretical basis for the individual regression coefficient
confidence intervals introduced in Section 3.4’s equation (3.10) and
generalizes it to arbitrary linear combinations of regression
coefficients.</p>
<p>Another important application of equation <a href="C4MLRANOVA.html#eq:eq45">(4.5)</a> is the choice of <span class="math inline">\(\mathbf{c}\)</span>
corresponding to a set of explanatory variables of interest, say,
<span class="math inline">\(\mathbf{x}_{\ast} = \left( 1, x_{\ast 1}, x_{\ast 2}, \ldots, x_{\ast k} \right)^{\prime}\)</span>. These may correspond to an observation within the
data set or to a point outside the available data. The parameter of
interest, <span class="math inline">\(\mathbf{c}^{\prime} \boldsymbol \beta = \mathbf{x}_{\ast}^{\prime} \boldsymbol \beta\)</span>, is the expected
response or the regression function at that point. Then,
<span class="math inline">\(\mathbf{x}_{\ast}^{\prime} \mathbf{b}\)</span> provides a point estimator
and equation <a href="C4MLRANOVA.html#eq:eq45">(4.5)</a> provides the
corresponding confidence interval.</p>
</div>
<div id="prediction-intervals" class="section level4 unnumbered hasAnchor">
<h4>Prediction Intervals<a href="C4MLRANOVA.html#prediction-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Prediction is an inferential goal that is closely related to
estimating the regression function at a point. Suppose that, when
considering charitable contributions, we know an individual’s wages
(and thus whether wages are in excess of <span class="math inline">\(97,500\)</span>) and wish to predict
the amount of charitable contributions. In general, we assume that
the set of explanatory variables <span class="math inline">\(\mathbf{x}_{\ast}\)</span> is known and
wish to predict the corresponding response <span class="math inline">\(y_{\ast}\)</span>. This new
response follows the assumptions as described in Section 3.2.
Specifically, the expected response is <span class="math inline">\(\mathrm{E~}y_{\ast} = \mathbf{x}_{\ast}^{\prime} \boldsymbol \beta\)</span>, <span class="math inline">\(\mathbf{x}_{\ast}\)</span>
is nonstochastic, <span class="math inline">\(\mathrm{Var~}y_{\ast} = \sigma^2\)</span>, <span class="math inline">\(y_{\ast}\)</span> is
independent of <span class="math inline">\(\{y_1, \ldots, y_{n}\}\)</span> and is normally distributed.
Under these assumptions, a <span class="math inline">\(100(1 - \alpha)\%\)</span> prediction interval
for <span class="math inline">\(y_{\ast}\)</span> is</p>
<p><span class="math display" id="eq:eq46">\[
\mathbf{x}_{\ast}^{\prime} \mathbf{b} \pm t_{n - (k + 1), 1 - \alpha / 2} ~ s
\sqrt{1 + \mathbf{x}_{\ast}^{\prime} (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{x}_{\ast}}.
\tag{4.6}
\]</span></p>
<p>Equation <a href="C4MLRANOVA.html#eq:eq46">(4.6)</a> generalizes the prediction interval introduced in Section 2.4.</p>
<div class="blackboxvideo">
<p><strong>Video: Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_x5sklaay&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_cxhdwa67%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="4.4 EstPredictionCoeff">
</iframe>
</center>
</div>
</div>
</div>
<div id="Sec43" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> One Factor ANOVA Model<a href="C4MLRANOVA.html#Sec43" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Section <a href="C4MLRANOVA.html#Sec41">4.1</a> showed how to incorporate unordered
categorical variables, or factors, into a linear regression model
through the use of binary variables. Factors are important in social
science research; they can be used to classify people by gender,
ethnicity, marital status and so on, or classify firms by geographic
region, organizational structure and so forth. Within studies of
insurance, factors are used by insurers to categorize policyholders
according to a “risk classification system.” Here, the idea is to
create groups of policyholders with similar risk characteristics
that will have similar claims experience. These groups form the
basis of insurance pricing, so that each policyholder is
charged an amount that is appropriate to their risk category. This
process is sometimes known as “segmentation.”</p>
<p>Although factors may be represented as binary variables in a linear
regression model, we study one factor models as a separate unit
because:</p>
<ul>
<li>The method of least squares is much simpler, obviating the need to take inverses of high-dimensional matrices.</li>
<li>The resulting interpretations of coefficients are more straightforward.</li>
</ul>
<p>The one factor model is still a special case of the linear
regression model. Hence, no additional statistical theory is needed
to establish its statistical inference capabilities.</p>
<p>To establish notation for the one factor ANOVA model, we now
consider the following example.</p>
<hr />
<p><strong>Example: Automobile Insurance Claims.</strong> We examine claims experience
from a large midwestern (US) property and casualty insurer for
private passenger automobile insurance. The dependent variable is
the amount paid on a closed claim, in (US) dollars (claims that were
not closed by year end are handled separately). Insurers categorize
policyholders according to a risk classification system. This
insurer’s risk classification system is based on:</p>
<ul>
<li>Automobile operator characteristics (age, gender, marital
status and whether the primary or occasional driver of a car).</li>
<li>Vehicle characteristics (city versus farm usage, if the vehicle is used to commute to school or work, used for business or pleasure, and if commuting, the
approximate distance of the commute).</li>
</ul>
<p>These factors are summarized by the risk class categorical variable
CLASS. Table <a href="C4MLRANOVA.html#tab:Tab45">4.5</a> shows 18 risk classes - further
classification information is not given here to protect proprietary
interests of the insurer.</p>
<p>Table <a href="C4MLRANOVA.html#tab:Tab45">4.5</a> summarizes the results from <span class="math inline">\(n=6,773\)</span>
claims for drivers aged 50 and above. We can see that the median
claim varies from a low of $707.40 (CLASS F7) to a high of
1,231.25 (CLASS C72). The distribution of claims turns out to be
skewed, so we consider <span class="math inline">\(y\)</span> = logarithmic claims. The table presents
means, medians and standard deviations. Because the distribution of
logarithmic claims is less skewed, means are close to medians.
Figure <a href="C4MLRANOVA.html#fig:Fig42">4.2</a> shows the distribution of logarithmic
claims by risk class.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab45">Table 4.5: </span><strong>Automobile Claims Summary Statistics by Risk Class</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
1
</th>
<th style="text-align:right;">
2
</th>
<th style="text-align:right;">
3
</th>
<th style="text-align:right;">
4
</th>
<th style="text-align:right;">
5
</th>
<th style="text-align:right;">
6
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Class
</td>
<td style="text-align:right;width: 1.8cm; ">
C1
</td>
<td style="text-align:right;width: 1.8cm; ">
C11
</td>
<td style="text-align:right;width: 1.8cm; ">
C1A
</td>
<td style="text-align:right;width: 1.8cm; ">
C1B
</td>
<td style="text-align:right;width: 1.8cm; ">
C1C
</td>
<td style="text-align:right;width: 1.8cm; ">
C2
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Number
</td>
<td style="text-align:right;width: 1.8cm; ">
726
</td>
<td style="text-align:right;width: 1.8cm; ">
1151
</td>
<td style="text-align:right;width: 1.8cm; ">
77
</td>
<td style="text-align:right;width: 1.8cm; ">
424
</td>
<td style="text-align:right;width: 1.8cm; ">
38
</td>
<td style="text-align:right;width: 1.8cm; ">
61
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Median (dollars)
</td>
<td style="text-align:right;width: 1.8cm; ">
948.86
</td>
<td style="text-align:right;width: 1.8cm; ">
1,013.81
</td>
<td style="text-align:right;width: 1.8cm; ">
925.48
</td>
<td style="text-align:right;width: 1.8cm; ">
1,026.73
</td>
<td style="text-align:right;width: 1.8cm; ">
1,001.73
</td>
<td style="text-align:right;width: 1.8cm; ">
851.20
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Median (in log dollars)
</td>
<td style="text-align:right;width: 1.8cm; ">
6.855
</td>
<td style="text-align:right;width: 1.8cm; ">
6.921
</td>
<td style="text-align:right;width: 1.8cm; ">
6.830
</td>
<td style="text-align:right;width: 1.8cm; ">
6.934
</td>
<td style="text-align:right;width: 1.8cm; ">
6.909
</td>
<td style="text-align:right;width: 1.8cm; ">
6.747
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Mean (in log dollars)
</td>
<td style="text-align:right;width: 1.8cm; ">
6.941
</td>
<td style="text-align:right;width: 1.8cm; ">
6.952
</td>
<td style="text-align:right;width: 1.8cm; ">
6.866
</td>
<td style="text-align:right;width: 1.8cm; ">
6.998
</td>
<td style="text-align:right;width: 1.8cm; ">
6.786
</td>
<td style="text-align:right;width: 1.8cm; ">
6.801
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; border-bottom: 2px solid black;">
Std. dev. (in log dollars)
</td>
<td style="text-align:right;width: 1.8cm; border-bottom: 2px solid black;">
1.064
</td>
<td style="text-align:right;width: 1.8cm; border-bottom: 2px solid black;">
1.074
</td>
<td style="text-align:right;width: 1.8cm; border-bottom: 2px solid black;">
1.072
</td>
<td style="text-align:right;width: 1.8cm; border-bottom: 2px solid black;">
1.068
</td>
<td style="text-align:right;width: 1.8cm; border-bottom: 2px solid black;">
1.110
</td>
<td style="text-align:right;width: 1.8cm; border-bottom: 2px solid black;">
0.948
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Class
</td>
<td style="text-align:right;width: 1.8cm; ">
C6
</td>
<td style="text-align:right;width: 1.8cm; ">
C7
</td>
<td style="text-align:right;width: 1.8cm; ">
C71
</td>
<td style="text-align:right;width: 1.8cm; ">
C72
</td>
<td style="text-align:right;width: 1.8cm; ">
C7A
</td>
<td style="text-align:right;width: 1.8cm; ">
C7B
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Number
</td>
<td style="text-align:right;width: 1.8cm; ">
911
</td>
<td style="text-align:right;width: 1.8cm; ">
913
</td>
<td style="text-align:right;width: 1.8cm; ">
1129
</td>
<td style="text-align:right;width: 1.8cm; ">
85
</td>
<td style="text-align:right;width: 1.8cm; ">
113
</td>
<td style="text-align:right;width: 1.8cm; ">
686
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Median (dollars)
</td>
<td style="text-align:right;width: 1.8cm; ">
1,011.24
</td>
<td style="text-align:right;width: 1.8cm; ">
957.68
</td>
<td style="text-align:right;width: 1.8cm; ">
960.40
</td>
<td style="text-align:right;width: 1.8cm; ">
1,231.25
</td>
<td style="text-align:right;width: 1.8cm; ">
1,139.93
</td>
<td style="text-align:right;width: 1.8cm; ">
1,113.13
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Median (in log dollars)
</td>
<td style="text-align:right;width: 1.8cm; ">
6.919
</td>
<td style="text-align:right;width: 1.8cm; ">
6.865
</td>
<td style="text-align:right;width: 1.8cm; ">
6.867
</td>
<td style="text-align:right;width: 1.8cm; ">
7.116
</td>
<td style="text-align:right;width: 1.8cm; ">
7.039
</td>
<td style="text-align:right;width: 1.8cm; ">
7.015
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Mean (in log dollars)
</td>
<td style="text-align:right;width: 1.8cm; ">
6.926
</td>
<td style="text-align:right;width: 1.8cm; ">
6.901
</td>
<td style="text-align:right;width: 1.8cm; ">
6.954
</td>
<td style="text-align:right;width: 1.8cm; ">
7.183
</td>
<td style="text-align:right;width: 1.8cm; ">
7.064
</td>
<td style="text-align:right;width: 1.8cm; ">
7.072
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; border-bottom: 2px solid black;">
Std. dev. (in log dollars)
</td>
<td style="text-align:right;width: 1.8cm; border-bottom: 2px solid black;">
1.115
</td>
<td style="text-align:right;width: 1.8cm; border-bottom: 2px solid black;">
1.058
</td>
<td style="text-align:right;width: 1.8cm; border-bottom: 2px solid black;">
1.038
</td>
<td style="text-align:right;width: 1.8cm; border-bottom: 2px solid black;">
0.988
</td>
<td style="text-align:right;width: 1.8cm; border-bottom: 2px solid black;">
1.021
</td>
<td style="text-align:right;width: 1.8cm; border-bottom: 2px solid black;">
1.103
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Class
</td>
<td style="text-align:right;width: 1.8cm; ">
C7C
</td>
<td style="text-align:right;width: 1.8cm; ">
F1
</td>
<td style="text-align:right;width: 1.8cm; ">
F11
</td>
<td style="text-align:right;width: 1.8cm; ">
F6
</td>
<td style="text-align:right;width: 1.8cm; ">
F7
</td>
<td style="text-align:right;width: 1.8cm; ">
F71
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Number
</td>
<td style="text-align:right;width: 1.8cm; ">
81
</td>
<td style="text-align:right;width: 1.8cm; ">
29
</td>
<td style="text-align:right;width: 1.8cm; ">
40
</td>
<td style="text-align:right;width: 1.8cm; ">
157
</td>
<td style="text-align:right;width: 1.8cm; ">
59
</td>
<td style="text-align:right;width: 1.8cm; ">
93
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Median (dollars)
</td>
<td style="text-align:right;width: 1.8cm; ">
1,200.00
</td>
<td style="text-align:right;width: 1.8cm; ">
1,078.04
</td>
<td style="text-align:right;width: 1.8cm; ">
774.79
</td>
<td style="text-align:right;width: 1.8cm; ">
1,105.04
</td>
<td style="text-align:right;width: 1.8cm; ">
707.40
</td>
<td style="text-align:right;width: 1.8cm; ">
1,118.73
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Median (in log dollars)
</td>
<td style="text-align:right;width: 1.8cm; ">
7.090
</td>
<td style="text-align:right;width: 1.8cm; ">
6.983
</td>
<td style="text-align:right;width: 1.8cm; ">
6.652
</td>
<td style="text-align:right;width: 1.8cm; ">
7.008
</td>
<td style="text-align:right;width: 1.8cm; ">
6.562
</td>
<td style="text-align:right;width: 1.8cm; ">
7.020
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Mean (in log dollars)
</td>
<td style="text-align:right;width: 1.8cm; ">
7.244
</td>
<td style="text-align:right;width: 1.8cm; ">
7.004
</td>
<td style="text-align:right;width: 1.8cm; ">
6.804
</td>
<td style="text-align:right;width: 1.8cm; ">
6.910
</td>
<td style="text-align:right;width: 1.8cm; ">
6.577
</td>
<td style="text-align:right;width: 1.8cm; ">
6.935
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
Std. dev. (in log dollars)
</td>
<td style="text-align:right;width: 1.8cm; ">
0.944
</td>
<td style="text-align:right;width: 1.8cm; ">
0.996
</td>
<td style="text-align:right;width: 1.8cm; ">
1.212
</td>
<td style="text-align:right;width: 1.8cm; ">
1.193
</td>
<td style="text-align:right;width: 1.8cm; ">
0.897
</td>
<td style="text-align:right;width: 1.8cm; ">
0.983
</td>
</tr>
</tbody>
</table>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig42"></span>
<img src="RegressionMarkdown_files/figure-html/Fig42-1.png" alt="Box Plots of Logarithmic Claims by Risk Class" width="100%" />
<p class="caption">
Figure 4.2: <strong>Box Plots of Logarithmic Claims by Risk Class</strong>
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Fig42.Hide" href="javascript:togglecode('toggleCode.Fig42.Hide','displayCode.Fig42.Hide');"><i><strong>R Code to Produce Table 4.5 and Figure 4.2</strong></i></a>
</h5>
<div id="toggleCode.Fig42.Hide" style="display: none">
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="C4MLRANOVA.html#cb45-1" tabindex="-1"></a>AutoC   <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;CSVData/AutoClaims.csv&quot;</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb45-2"><a href="C4MLRANOVA.html#cb45-2" tabindex="-1"></a><span class="co">#  CREATE A TABLE OF MEANS AND STANDARD DEVIATIONS</span></span>
<span id="cb45-3"><a href="C4MLRANOVA.html#cb45-3" tabindex="-1"></a><span class="fu">library</span>(Hmisc)</span>
<span id="cb45-4"><a href="C4MLRANOVA.html#cb45-4" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">summarize</span>(<span class="fu">log</span>(AutoC<span class="sc">$</span>PAID), AutoC<span class="sc">$</span>CLASS, length ) </span>
<span id="cb45-5"><a href="C4MLRANOVA.html#cb45-5" tabindex="-1"></a>t2 <span class="ot">&lt;-</span> <span class="fu">summarize</span>(AutoC<span class="sc">$</span>PAID, AutoC<span class="sc">$</span>CLASS, median) </span>
<span id="cb45-6"><a href="C4MLRANOVA.html#cb45-6" tabindex="-1"></a>t3 <span class="ot">&lt;-</span> <span class="fu">summarize</span>(<span class="fu">log</span>(AutoC<span class="sc">$</span>PAID), AutoC<span class="sc">$</span>CLASS, median) </span>
<span id="cb45-7"><a href="C4MLRANOVA.html#cb45-7" tabindex="-1"></a>t4 <span class="ot">&lt;-</span> <span class="fu">summarize</span>(<span class="fu">log</span>(AutoC<span class="sc">$</span>PAID), AutoC<span class="sc">$</span>CLASS, mean) </span>
<span id="cb45-8"><a href="C4MLRANOVA.html#cb45-8" tabindex="-1"></a>t5 <span class="ot">&lt;-</span> <span class="fu">summarize</span>(<span class="fu">log</span>(AutoC<span class="sc">$</span>PAID), AutoC<span class="sc">$</span>CLASS, sd) </span>
<span id="cb45-9"><a href="C4MLRANOVA.html#cb45-9" tabindex="-1"></a>tablemat <span class="ot">&lt;-</span> <span class="fu">cbind</span>(t1,</span>
<span id="cb45-10"><a href="C4MLRANOVA.html#cb45-10" tabindex="-1"></a>                  <span class="fu">format</span>(<span class="fu">round</span>(t2[<span class="dv">2</span>], <span class="at">digits =</span> <span class="dv">2</span>), <span class="at">big.mark =</span> <span class="st">&#39;,&#39;</span>),</span>
<span id="cb45-11"><a href="C4MLRANOVA.html#cb45-11" tabindex="-1"></a>                  <span class="fu">round</span>(t3[<span class="dv">2</span>], <span class="at">digits =</span> <span class="dv">3</span>),</span>
<span id="cb45-12"><a href="C4MLRANOVA.html#cb45-12" tabindex="-1"></a>                  <span class="fu">round</span>(t4[<span class="dv">2</span>], <span class="at">digits =</span> <span class="dv">3</span>),</span>
<span id="cb45-13"><a href="C4MLRANOVA.html#cb45-13" tabindex="-1"></a>                  <span class="fu">round</span>(t5[<span class="dv">2</span>], <span class="at">digits =</span> <span class="dv">3</span>))</span>
<span id="cb45-14"><a href="C4MLRANOVA.html#cb45-14" tabindex="-1"></a>block1 <span class="ot">&lt;-</span>  <span class="fu">t</span>(tablemat[<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,])</span>
<span id="cb45-15"><a href="C4MLRANOVA.html#cb45-15" tabindex="-1"></a>block2 <span class="ot">&lt;-</span> <span class="fu">t</span>(tablemat[<span class="dv">7</span><span class="sc">:</span><span class="dv">12</span>,])</span>
<span id="cb45-16"><a href="C4MLRANOVA.html#cb45-16" tabindex="-1"></a>block3 <span class="ot">&lt;-</span> <span class="fu">t</span>(tablemat[<span class="dv">13</span><span class="sc">:</span><span class="dv">18</span>,])</span>
<span id="cb45-17"><a href="C4MLRANOVA.html#cb45-17" tabindex="-1"></a></span>
<span id="cb45-18"><a href="C4MLRANOVA.html#cb45-18" tabindex="-1"></a><span class="co">#tableout[3,] &lt;- format(round(tableout[3,], digits=0), big.mark = &#39;,&#39;)</span></span>
<span id="cb45-19"><a href="C4MLRANOVA.html#cb45-19" tabindex="-1"></a></span>
<span id="cb45-20"><a href="C4MLRANOVA.html#cb45-20" tabindex="-1"></a>bigblock <span class="ot">&lt;-</span> <span class="fu">rbind</span>(block1, block2, block3)</span>
<span id="cb45-21"><a href="C4MLRANOVA.html#cb45-21" tabindex="-1"></a>temprow <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Class&quot;</span>, <span class="st">&quot;Number&quot;</span>, <span class="st">&quot;Median (dollars)&quot;</span>, <span class="st">&quot;Median (in log dollars)&quot;</span> ,<span class="st">&quot;Mean (in log dollars)&quot;</span>, <span class="st">&quot;Std. dev. (in log dollars)&quot;</span>) </span>
<span id="cb45-22"><a href="C4MLRANOVA.html#cb45-22" tabindex="-1"></a>bigblock1 <span class="ot">&lt;-</span> <span class="fu">cbind</span> (<span class="fu">c</span>(temprow,temprow, temprow), bigblock)</span>
<span id="cb45-23"><a href="C4MLRANOVA.html#cb45-23" tabindex="-1"></a><span class="fu">row.names</span>(bigblock1) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb45-24"><a href="C4MLRANOVA.html#cb45-24" tabindex="-1"></a></span>
<span id="cb45-25"><a href="C4MLRANOVA.html#cb45-25" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>bigblock1 , </span>
<span id="cb45-26"><a href="C4MLRANOVA.html#cb45-26" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Automobile Claims Summary Statistics by Risk Class&#39;</span>, </span>
<span id="cb45-27"><a href="C4MLRANOVA.html#cb45-27" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;lrrrrrr&#39;</span>, <span class="at">Digits=</span><span class="dv">3</span>, <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,</span>
<span id="cb45-28"><a href="C4MLRANOVA.html#cb45-28" tabindex="-1"></a>         <span class="at">ColWidth =</span> ColWidth4) <span class="sc">%&gt;%</span></span>
<span id="cb45-29"><a href="C4MLRANOVA.html#cb45-29" tabindex="-1"></a>       kableExtra<span class="sc">::</span><span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">width =</span>  <span class="st">&quot;4cm&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb45-30"><a href="C4MLRANOVA.html#cb45-30" tabindex="-1"></a>     kableExtra<span class="sc">::</span><span class="fu">row_spec</span>(<span class="dv">6</span>, <span class="at">extra_css =</span> <span class="st">&quot;border-bottom: 2px solid black;&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb45-31"><a href="C4MLRANOVA.html#cb45-31" tabindex="-1"></a>     kableExtra<span class="sc">::</span><span class="fu">row_spec</span>(<span class="dv">12</span>, <span class="at">extra_css =</span> <span class="st">&quot;border-bottom: 2px solid black;&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="C4MLRANOVA.html#cb46-1" tabindex="-1"></a><span class="fu">library</span>(HH)</span>
<span id="cb46-2"><a href="C4MLRANOVA.html#cb46-2" tabindex="-1"></a>AutoC   <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;CSVData/AutoClaims.csv&quot;</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb46-3"><a href="C4MLRANOVA.html#cb46-3" tabindex="-1"></a><span class="co">#  AUTO CLAIMS</span></span>
<span id="cb46-4"><a href="C4MLRANOVA.html#cb46-4" tabindex="-1"></a><span class="co">#  FIGURE 4.2</span></span>
<span id="cb46-5"><a href="C4MLRANOVA.html#cb46-5" tabindex="-1"></a></span>
<span id="cb46-6"><a href="C4MLRANOVA.html#cb46-6" tabindex="-1"></a><span class="fu">par</span>(<span class="at">cex=</span><span class="fl">0.6</span>)</span>
<span id="cb46-7"><a href="C4MLRANOVA.html#cb46-7" tabindex="-1"></a><span class="fu">boxplot</span>(<span class="fu">log</span>(PAID) <span class="sc">~</span> CLASS,<span class="at">cex=</span>.<span class="dv">6</span>, <span class="at">cex.labels=</span><span class="dv">2</span>, <span class="at">data =</span> AutoC,</span>
<span id="cb46-8"><a href="C4MLRANOVA.html#cb46-8" tabindex="-1"></a>        <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
</div>
<hr />
<p>This section focuses on the risk class (CLASS) as the explanatory variable. We use the notation <span class="math inline">\(y_{ij}\)</span> to mean the <span class="math inline">\(i\)</span>th observation of the <span class="math inline">\(j\)</span>th risk class. For the <span class="math inline">\(j\)</span>th risk class, we assume there are <span class="math inline">\(n_j\)</span> observations. There are <span class="math inline">\(n=n_1+n_2+\ldots +n_c\)</span> observations. The data are:</p>
<p><span class="math display">\[
\begin{array}{cccccc}
\text{Data for risk class }1 &amp; \ \ \ \
&amp; y_{11} &amp; y_{21} &amp; \ldots  &amp; y_{n_1,1} \\
\text{Data for risk class }2 &amp;  &amp; y_{12} &amp; y_{22} &amp; \ldots  &amp; y_{n_2,1} \\
. &amp;  &amp; . &amp; . &amp; \ldots &amp; . \\
\text{Data for risk class } c &amp;  &amp; y_{1c} &amp; y_{2c} &amp; \ldots  &amp; y_{n_c,c}
\end{array}
\]</span></p>
<p>where <span class="math inline">\(c=18\)</span> is the number of levels of the CLASS factor. Because each level of a factor can be arranged in a single row (or column), another term for this type of data is a “one way classification.” Thus, a <em>one way model</em> is another term for a one factor model.</p>
<p>An important summary measure of each level of the factor is the sample average. Let
<span class="math display">\[
\overline{y}_j=\frac{1}{n_j}\sum_{i=1}^{n_j}y_{ij}
\]</span>
denote the average from the <span class="math inline">\(j\)</span>th CLASS.</p>
<div id="model-assumptions-and-analysis" class="section level4 unnumbered hasAnchor">
<h4>Model Assumptions and Analysis<a href="C4MLRANOVA.html#model-assumptions-and-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The one factor ANOVA model equation is
<span class="math display" id="eq:eq47">\[
y_{ij}=\mu_j+ \varepsilon_{ij}\ \ \ \ \ \ i=1,\ldots ,n_j,\ \ \ \ \
j=1,\ldots ,c.
\tag{4.7}
\]</span>
As with regression models, the random deviations <span class="math inline">\(\{\varepsilon_{ij} \}\)</span> are assumed to be zero mean with constant variance (Assumption E3) and independent of one another (Assumption E4). Because we assume the expected value of each deviation is zero, we have <span class="math inline">\(\text{E}~y_{ij}=\mu_j\)</span>. Thus, we interpret <span class="math inline">\(\mu_j\)</span> to be the expected value of the response <span class="math inline">\(y_{ij}\)</span>, that is, the mean <span class="math inline">\(\mu\)</span> varies by the <span class="math inline">\(j\)</span>th factor level.</p>
<p>To estimate the parameters <span class="math inline">\(\{\mu_j\}\)</span>, as with regression we use the <em>method of least squares</em>, introduced in Section 2.1. That is, let <span class="math inline">\(\mu^{\ast}_j\)</span> be a “candidate” estimate of <span class="math inline">\(\mu_j\)</span>. The quantity
<span class="math display">\[
SS(\mu^{\ast}_1, \ldots , \mu^{\ast}_{c}) = \sum_{j=1}^{c} \sum_{i=1}^{n_j} (y_{ij}-\mu^{\ast}_j)^2
\]</span>
represents the sum of squared deviations of the responses from these candidate estimates. From straightforward algebra, the value of <span class="math inline">\(\mu^{\ast}_j\)</span> that minimizes this sum of squares is <span class="math inline">\(\bar{y}_j\)</span>. Thus, <span class="math inline">\(\bar{y}_j\)</span> is the <em>least squares estimate</em> of <span class="math inline">\(\mu_j\)</span>.</p>
<p>To understand the reliability of the estimates, we can partition the variability as in the regression case, presented in Sections 2.3.1 and 3.3. The minimum sum of squared deviations is called the <em>error sum of squares</em> and is defined to be
<span class="math display">\[
Error ~SS = SS(\bar{y}_1, \ldots, \bar{y}_{c}) = \sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij}-\bar{y}_j \right)^2.
\]</span>
The total variation in the data set is summarized by the <em>total sum of squares</em>,
<span class="math display">\[
Total ~SS=\sum_{j=1}^{c}\sum_{i=1}^{n_j}(y_{ij}-\bar{y})^2.
\]</span>
The difference, called the <em>factor sum of squares</em>, can be expressed as:
<span class="math display">\[
\begin{array}{ll}
Factor~ SS  &amp; = Total ~SS - Error ~SS \\
&amp; = \sum_{j=1}^{c}\sum_{i=1}^{n_j}(y_{ij}-\bar{y})^2-\sum_{j=1}^{c}\sum_{i=1}^{n_j}(y_{ij}-\bar{y}_j)^2 = \sum_{j=1}^{c}\sum_{i=1}^{n_j}(\bar{y}_j-\bar{y})^2 \\
&amp; = \sum_{j=1}^{c}n_j(\bar{y}_j-\bar{y})^2.
\end{array}
\]</span>
The last two equalities follow from algebra manipulation. The <span class="math inline">\(Factor ~SS\)</span> plays the same role as the <span class="math inline">\(Regression ~SS\)</span> in Chapters 2 and 3. The variability decomposition is summarized in Table <a href="C4MLRANOVA.html#tab:Tab46">4.6</a>.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab46">Table 4.6: </span><strong>ANOVA Table for One Factor Model</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Source
</th>
<th style="text-align:right;">
Sum of Squares
</th>
<th style="text-align:center;">
<span class="math inline">\(df\)</span>
</th>
<th style="text-align:right;">
Mean Square
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Factor
</td>
<td style="text-align:right;width: 1.8cm; ">
<span class="math inline">\(Factor ~SS\)</span>
</td>
<td style="text-align:center;width: 1.8cm; ">
<span class="math inline">\(c-1\)</span>
</td>
<td style="text-align:right;width: 1.8cm; ">
<span class="math inline">\(Factor ~MS\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Error
</td>
<td style="text-align:right;width: 1.8cm; ">
<span class="math inline">\(Error ~SS\)</span>
</td>
<td style="text-align:center;width: 1.8cm; ">
<span class="math inline">\(n-c\)</span>
</td>
<td style="text-align:right;width: 1.8cm; ">
<span class="math inline">\(Error ~MS\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Total
</td>
<td style="text-align:right;width: 1.8cm; ">
<span class="math inline">\(Total ~SS\)</span>
</td>
<td style="text-align:center;width: 1.8cm; ">
<span class="math inline">\(n-1\)</span>
</td>
<td style="text-align:right;width: 1.8cm; ">
</td>
</tr>
</tbody>
</table>
<p>The conventions for this table are the same as in the regression case. That is, the mean square (MS) column is defined by the sum of squares (SS) column divided by the degrees of freedom (<em>df</em>) column. Thus, <span class="math inline">\(Factor~MS \equiv (Factor~SS)/(c-1)\)</span> and <span class="math inline">\(Error~MS \equiv (Error~SS)/(n-c)\)</span>. We use
<span class="math display">\[
s^2 = \text{Error MS} = \frac{1}{n-c} \sum_{j=1}^{c}\sum_{i=1}^{n_j} e_{ij}^2
\]</span>
to be our estimate of <span class="math inline">\(\sigma^2\)</span>, where <span class="math inline">\(e_{ij} = y_{ij} - \bar{y}_j\)</span> is the residual.</p>
<p>With this value for <span class="math inline">\(s\)</span>, it can be shown that the interval estimate
for <span class="math inline">\(\mu_j\)</span> is
<span class="math display" id="eq:eq48">\[
\bar{y}_j \pm t_{n-c,1-\alpha /2}\frac{s}{\sqrt{n_j}}.
\tag{4.8}
\]</span></p>
<p>Here, the <em>t</em>-value <span class="math inline">\(t_{n-c,1-\alpha /2}\)</span> is a percentile from the <em>t</em>-distribution with <span class="math inline">\(df=n-c\)</span> degrees of freedom.</p>
<hr />
<p><strong>Example: Automobile Claims - Continued.</strong> To illustrate, the
ANOVA table summarizing the fit for the automobile claims data
appears in Table <a href="C4MLRANOVA.html#tab:Tab47">4.7</a>. Here, we see that the mean
square error is <span class="math inline">\(s^2 = 1.14.\)</span></p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab47">Table 4.7: </span><strong>ANOVA Table for Logarithmic Automobile Claims</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Source
</th>
<th style="text-align:right;">
Sum of Squares
</th>
<th style="text-align:right;">
<span class="math inline">\(df\)</span>
</th>
<th style="text-align:right;">
Mean Square
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
CLASS
</td>
<td style="text-align:right;width: 1.8cm; ">
39.2
</td>
<td style="text-align:right;width: 1.8cm; ">
17
</td>
<td style="text-align:right;width: 1.8cm; ">
2.31
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Error
</td>
<td style="text-align:right;width: 1.8cm; ">
7729.0
</td>
<td style="text-align:right;width: 1.8cm; ">
6755
</td>
<td style="text-align:right;width: 1.8cm; ">
1.14
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Total
</td>
<td style="text-align:right;width: 1.8cm; ">
7768.2
</td>
<td style="text-align:right;width: 1.8cm; ">
6772
</td>
<td style="text-align:right;width: 1.8cm; ">
</td>
</tr>
</tbody>
</table>
<p>In automobile ratemaking, one uses the average claims to help set
prices for insurance coverages. As an example, for CLASS C72 the
average logarithmic claim is 7.183. From equation <a href="C4MLRANOVA.html#eq:eq48">(4.8)</a>, a 95% confidence interval is
<span class="math display">\[
7.183 \pm (1.96) \frac{\sqrt{1.14}}{\sqrt{85}} = 7.183 \pm 0.227 = (6.956 ,7.410).
\]</span>
Note that these estimates are in natural logarithmic units. In dollars, our point estimate is <span class="math inline">\(e^{7.183} = 1,316.85\)</span> and our 95% confidence interval is <span class="math inline">\((e^{6.956} , e^{7.410}) \text{ or } (\$1,049.43, \$1,652.43)\)</span>.</p>
<hr />
<p>An important feature of the one factor ANOVA decomposition and
estimation is the ease of computation. Although the sum of squares
appear complex, it is important to note that <em>no matrix calculations are required</em>. Rather, all of the calculations can be
done through averages and sums of squares. This has been an important
consideration historically, before the age of readily available
desktop computing. Moreover, insurers may segment their portfolios
into hundreds or even thousands of risk classes instead of the 18
used in our Automobile Claims data. Thus, even today it can be
helpful to identify a categorical variable as a factor and let your
statistical software use ANOVA estimation techniques. Further, ANOVA
estimation also provides for direct interpretation of the results.</p>
</div>
<div id="link-with-regression" class="section level4 unnumbered hasAnchor">
<h4>Link with Regression<a href="C4MLRANOVA.html#link-with-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This subsection shows how a one factor ANOVA model can be rewritten
as a regression model. To this end, we have seen that both the
regression model and one factor ANOVA model use a linear error
structure with Assumptions E3 and E4 for identically and
independently distributed errors. Similarly, both use the normality
assumption E5 for selected inference results (such as confidence
intervals). Both employ non-stochastic explanatory variables as in
Assumption E2. Both have an additive (mean zero) error term, so the
main apparent difference is in the expected response, <span class="math inline">\(\mathrm{E }~y\)</span>.</p>
<p>For the linear regression model, <span class="math inline">\(\mathrm{E }~y\)</span> is a linear combination of
explanatory variables (Assumption F1). For the one factor ANOVA
model, <span class="math inline">\(E[y_j] = \mu_j\)</span> is a mean that depends on the level of the
factor. To equate these two approaches, for the ANOVA factor with
<span class="math inline">\(c\)</span> levels, we define <span class="math inline">\(c\)</span> binary variables, <span class="math inline">\(x_1, x_2, \ldots, x_c\)</span>. Here, <span class="math inline">\(x_j\)</span> indicates whether or not an observation falls in
the <span class="math inline">\(j\)</span>th level. With these variables, we can rewrite our one factor
ANOVA model as
<span class="math display" id="eq:eq49">\[
y = \mu_1 x_1 + \mu_2 x_2 + \ldots + \mu_c x_c + \varepsilon.
\tag{4.9}
\]</span>
Thus, we have re-written the one factor ANOVA expected response as a regression function, although using a no-intercept form (as in equation (3.5)).</p>
<p>The one factor ANOVA is a special case of our usual regression
model, using binary variables from the factor as explanatory
variables in the regression function. As we have seen, no matrix
calculations are needed for least squares estimation. However, one
can always use the matrix procedures developed in Chapter 3. Section
4.7.1 shows how our usual matrix expression for
regression coefficients (<span class="math inline">\(\mathbf{b} = \left(\mathbf{X}^{\prime}\mathbf{X}\right)^{-1}\mathbf{X}^{\prime}\mathbf{y}\)</span>) reduces to the simple estimates <span class="math inline">\(\bar{y}_j\)</span> when using only one categorical variable.</p>
</div>
<div id="reparameterization" class="section level4 unnumbered hasAnchor">
<h4>Reparameterization<a href="C4MLRANOVA.html#reparameterization" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To include an intercept term, define <span class="math inline">\(\tau_j = \mu_j - \mu\)</span>, where
<span class="math inline">\(\mu\)</span> is an, as yet, unspecified parameter. Because each observation
must fall into one of the <span class="math inline">\(c\)</span> categories, we have <span class="math inline">\(x_1 + x_2 + \ldots + x_{c} = 1\)</span> for each observation. Thus, using <span class="math inline">\(\mu_j = \tau_j + \mu\)</span> in equation <a href="C4MLRANOVA.html#eq:eq49">(4.9)</a>, we have
<span class="math display" id="eq:eq410">\[
y = \mu + \tau_1 x_1 + \tau_2 x_2 + \ldots + \tau_{c} x_{c} + \varepsilon,
\tag{4.10}
\]</span>
we have re-written the model into what appears to be our usual regression format.</p>
<p>We use the <span class="math inline">\(\tau\)</span> in lieu of <span class="math inline">\(\beta\)</span> for historical reasons. ANOVA
models were invented by R.A. Fisher in connection with agricultural
experiments. Here, the typical set-up is to apply several
<em>treatments</em> to plots of land in order to quantify crop yield
responses. Thus, the Greek “t”, <span class="math inline">\(\tau\)</span>, suggests the word
treatment, another term used to describe levels of the factor of
interest.</p>
<p>A simpler version of equation <a href="C4MLRANOVA.html#eq:eq410">(4.10)</a>
<span class="math display">\[
y_{ij} = \mu + \tau_j + \varepsilon_{ij}.
\]</span>
can be given when we identify the factor level. That is, if we know an
observation falls in the <span class="math inline">\(j\)</span>th level, then only <span class="math inline">\(x_j\)</span> is one and the
other <span class="math inline">\(x\)</span>’s are 0. Thus, a simpler expression for equation <a href="C4MLRANOVA.html#eq:eq410">(4.10)</a>
<span class="math display">\[
y_{ij} = \mu + \tau_j + \varepsilon_{ij}.
\]</span></p>
<p>Comparing equations <a href="C4MLRANOVA.html#eq:eq49">(4.9)</a> and <a href="C4MLRANOVA.html#eq:eq410">(4.10)</a>, we see that the number of parameters has increased by one. That is, in equation <a href="C4MLRANOVA.html#eq:eq49">(4.9)</a> there are <span class="math inline">\(c\)</span> parameters, <span class="math inline">\(\mu_1, \ldots, \mu_c\)</span>, even though in equation <a href="C4MLRANOVA.html#eq:eq410">(4.10)</a>
there are <span class="math inline">\(c + 1\)</span> parameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau_1, \ldots, \tau_c\)</span>. The model in equation
<a href="C4MLRANOVA.html#eq:eq410">(4.10)</a> is said to be <em>overparameterized</em>. It is possible to estimate
this model directly, using the general theory of linear models,
summarized in Section <a href="C4MLRANOVA.html#Sec473">4.7.3</a>. In this theory, regression coefficients need not be identifiable. Alternatively, one
can make these two expressions equivalent by <em>restricting</em> the
movement of the parameters in
equation <a href="C4MLRANOVA.html#eq:eq410">(4.10)</a>.
We now present two ways of imposing restrictions.</p>
<p>The first type of restriction, usually done in the regression context, is to
require one of the <span class="math inline">\(\tau\)</span>’s to be zero. This amounts to <em>dropping</em>
one of the explanatory variables. For example, we might use
<span class="math display" id="eq:eq411">\[
y = \mu + \tau_1 x_1 + \tau_2 x_2 + \ldots + \tau_{c-1} x_{c-1} + \varepsilon,
\tag{4.11}
\]</span>
dropping <span class="math inline">\(x_c\)</span>. With this formulation, it is easy to fit the model
in equation <a href="C4MLRANOVA.html#eq:eq411">(4.11)</a> using regression statistical software routines because one only needs to run the regression with <span class="math inline">\(c-1\)</span> explanatory variables. However, one needs to be careful with the interpretation of parameters. To equate the models in equations <a href="C4MLRANOVA.html#eq:eq49">(4.9)</a> and <a href="C4MLRANOVA.html#eq:eq410">(4.10)</a>,
we need to define <span class="math inline">\(\mu \equiv \mu_c\)</span> and <span class="math inline">\(\tau_j = \mu_j - \mu_c\)</span> for <span class="math inline">\(j=1,2,\ldots,c-1\)</span>. That is, the regression intercept term is the mean level of the
category dropped, and each regression coefficient is the difference
between a mean level and the mean level dropped. It is not necessary
to drop the last level <span class="math inline">\(c\)</span>, and indeed, one could drop any level.
However, the interpretation of the parameters does depend on the variable dropped. With this restriction, the fitted values are <span class="math inline">\(\hat{\mu} = \hat{\mu}_c = \bar{y}_c\)</span> and <span class="math inline">\(\hat{\tau}_j = \hat{\mu}_j - \hat{\mu}_c = \bar{y}_j - \bar{y}_c.\)</span> Recall that the caret (<span class="math inline">\(\hat{\cdot}\)</span>), or “hat,” stands for an estimated, or fitted, value.</p>
<p>The second type of restriction is to
interpret <span class="math inline">\(\mu\)</span> as a mean for the entire population. To this end,
the usual requirement is <span class="math inline">\(\mu \equiv \frac{1}{n} \sum_{j=1}^c n_j \mu_j,\)</span>
that is, <span class="math inline">\(\mu\)</span> is a weighted average of means. With this
definition, we interpret <span class="math inline">\(\tau_j = \mu_j - \mu\)</span> as treatment
differences between a mean level and the population mean. Another
way of expressing this restriction is <span class="math inline">\(\sum_{j=1}^{c} n_j \tau_j = 0,\)</span>
that is, the (weighted) sum of treatment differences is zero. The
disadvantage of this restriction is that it is not readily
implementable with a regression routine and a special routine is
needed. The advantage is that there is a symmetry in the definitions
of the parameters. There is no need to worry about which variable is
being dropped from the equation, an important consideration. With
this restriction, the fitted values are
<span class="math display">\[
\hat{\mu} = \frac{1}{n} \sum_{j=1}^{c} n_j \hat{\mu}_j = \frac{1}{n} \sum_{j=1}^{c} n_j \bar{y}_j = \bar{y}
\]</span>
and
<span class="math display">\[
\hat{\tau}_j = \hat{\mu}_j - \hat{\mu} = \bar{y}_j - \bar{y}.
\]</span></p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_0xirmuez&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_tha8c614%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="4.5 ANOVAModel">
</iframe>
</center>
</div>
</div>
<div id="Sec44" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Combining Categorical and Continuous Explanatory Variables<a href="C4MLRANOVA.html#Sec44" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are several ways to combine categorical and continuous
explanatory variables. We initially present the case of only one
categorical and one continuous variable. We then briefly present the
general case, called the <em>general linear model</em>. When
combining categorical and continuous variable models, we use the
terminology <em>factor</em> for the categorical variable and
<em>covariate</em> for the continuous variable.</p>
<div id="combining-a-factor-and-covariate" class="section level4 unnumbered hasAnchor">
<h4>Combining a Factor and Covariate<a href="C4MLRANOVA.html#combining-a-factor-and-covariate" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let us begin with the simplest models that use a factor and a
covariate. In Section <a href="C4MLRANOVA.html#Sec43">4.3</a>, we introduced the one
factor model <span class="math inline">\(y_{ij} = \mu_j + \varepsilon_{ij}\)</span>. In Chapter 2, we
introduced basic linear regression in terms of one continuous
variable, or covariate, using <span class="math inline">\(y_{ij} = \beta_0 + \beta_1 x_{ij} + \varepsilon_{ij}\)</span>. Table <a href="C4MLRANOVA.html#tab:Tab48">4.8</a> summarizes
different approaches that could be used to represent combinations of
a factor and covariate.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab48">Table 4.8: </span><strong>Several Models that Represent Combinations of One Factor and One Covariate</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model Description
</th>
<th style="text-align:right;">
Notation
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 9cm; border-right:1px solid;">
One factor ANOVA (no covariate model)
</td>
<td style="text-align:right;width: 1.8cm; ">
<span class="math inline">\(y_{ij} = \mu_j + \varepsilon_{ij}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;width: 9cm; border-right:1px solid;">
Regression with constant intercept and slope (no factor model)
</td>
<td style="text-align:right;width: 1.8cm; ">
<span class="math inline">\(y_{ij} = \beta_0 + \beta_1 x_{ij} + \varepsilon_{ij}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;width: 9cm; border-right:1px solid;">
Regression with variable intercept and constant slope (analysis of covariance model)
</td>
<td style="text-align:right;width: 1.8cm; ">
<span class="math inline">\(y_{ij} = \beta_{0j} + \beta_1 x_{ij} + \varepsilon_{ij}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;width: 9cm; border-right:1px solid;">
Regression with constant intercept and variable slope
</td>
<td style="text-align:right;width: 1.8cm; ">
<span class="math inline">\(y_{ij} = \beta_0 + \beta_{1j} x_{ij} + \varepsilon_{ij}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;width: 9cm; border-right:1px solid;">
Regression with variable intercept and slope
</td>
<td style="text-align:right;width: 1.8cm; ">
<span class="math inline">\(y_{ij} = \beta_{0j} + \beta_{1j} x_{ij} + \varepsilon_{ij}\)</span>
</td>
</tr>
</tbody>
</table>
<p>We can interpret the regression with variable intercept and constant
slope to be an additive model, because we are adding the factor
effect, <span class="math inline">\(\beta_{0j}\)</span>, to the covariate effect, <span class="math inline">\(\beta_1 x_{ij}\)</span>. Note
that one could also use the notation, <span class="math inline">\(\mu_j\)</span>, in lieu of <span class="math inline">\(\beta_{0j}\)</span> to suggest the presence of a factor effect. This is also
known as an <em>analysis of covariance (ANCOVA) model</em>. The
regression with variable intercept and slope can be thought of as an
<em>interaction model</em>. Here, both the intercept, <span class="math inline">\(\beta_{0j}\)</span>,
and slope, <span class="math inline">\(\beta_{1j}\)</span>, may vary by level of the factor. In this
sense, we interpret the factor and covariate to be “interacting.”
The model with constant intercept and variable slope is typically
not used in practice; it is included here for completeness. With
this model, the factor and covariate interact only through the
variable slope. Figures <a href="C4MLRANOVA.html#fig:Fig43">4.3</a>,
<a href="C4MLRANOVA.html#fig:Fig44">4.4</a>, and <a href="C4MLRANOVA.html#fig:Fig45">4.5</a> illustrate the expected responses of these models.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig43"></span>
<img src="RegressionMarkdown_files/figure-html/Fig43-1.png" alt="Plot of the expected response versus the covariate for the regression model with variable intercept and constant slope." width="60%" />
<p class="caption">
Figure 4.3: <strong>Plot of the expected response versus the covariate for the regression model with variable intercept and constant slope.</strong>
</p>
</div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig44"></span>
<img src="RegressionMarkdown_files/figure-html/Fig44-1.png" alt="Plot of the expected response versus the covariate for the regression model with constant intercept and variable slope." width="60%" />
<p class="caption">
Figure 4.4: <strong>Plot of the expected response versus the covariate for the regression model with constant intercept and variable slope.</strong>
</p>
</div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig45"></span>
<img src="RegressionMarkdown_files/figure-html/Fig45-1.png" alt="Plot of the expected response versus the covariate for the regression model with variable intercept and variable slope." width="60%" />
<p class="caption">
Figure 4.5: <strong>Plot of the expected response versus the covariate for the regression model with variable intercept and variable slope.</strong>
</p>
</div>
<p>For each model presented in Table <a href="C4MLRANOVA.html#tab:Tab48">4.8</a>,
parameter estimates can be calculated using the method of least
squares. As usual, this means writing the expected response, <span class="math inline">\(E[y_{ij}]\)</span>, as a function of known variables and unknown parameters.
For the regression model with variable intercept and constant slope,
the least squares estimates can be expressed compactly as:</p>
<p><span class="math display">\[
b_1 = \frac{\sum_{j=1}^{c}\sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j) (y_{ij} - \bar{y}_j)}{\sum_{j=1}^{c}\sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2}
\]</span></p>
<p>and <span class="math inline">\(b_{0j} = \bar{y}_j - b_1 \bar{x}_j\)</span>. Similarly, the least
squares estimates for the regression model with variable intercept
and slope can be expressed as:</p>
<p><span class="math display">\[
b_{1j} = \frac{\sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j) (y_{ij} - \bar{y}_j)}{\sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2}
\]</span></p>
<p>and <span class="math inline">\(b_{0j} = \bar{y}_j - b_{1j} \bar{x}_j\)</span>. With these parameter estimates, fitted values may be calculated.</p>
<p>For each model, fitted values are defined to be the expected response with
the unknown parameters replaced by their least squares estimates.
For example, for the regression model with variable intercept and
constant slope the fitted values are <span class="math inline">\(\hat{y}_{ij} = b_{0j} + b_1 x_{ij}.\)</span></p>
<hr />
<p><strong>Example: Wisconsin Hospital Costs.</strong> We now study the impact of various predictors on hospital charges in the state of Wisconsin. Identifying predictors of
hospital charges can provide direction for hospitals, government,
insurers, and consumers in controlling these variables, which in turn
leads to better control of hospital costs. The data for the year
1989 were obtained from the Office of Health Care Information,
Wisconsin’s Department of Health and Human Services. Cross-sectional
data are used, which detail the 20 diagnosis-related group (DRG)
discharge costs for hospitals in the state of Wisconsin, broken down
into nine major health service areas and three types of payer (Fee
for service, HMO, and other). Even though there are 540 potential
DRG, area, and payer combinations (<span class="math inline">\(20 \times 9 \times 3 = 540\)</span>), only
526 combinations were actually realized in the 1989 data set. Other
predictor variables included the logarithm of the total number of
discharges (NO DSCHG) and total number of hospital beds (NUM BEDS)
for each combination. The response variable is the logarithm of
total hospital charges per number of discharges (CHGNUM). To
streamline the presentation, we now consider only costs associated
with three diagnostic related groups (DRGs): DRG #209, DRG #391,
and DRG #430.</p>
<p>The covariate, <span class="math inline">\(x\)</span>, is the natural logarithm of the number of
discharges. In ideal settings, hospitals with more patients enjoy
lower costs due to economies of scale. In non-ideal settings,
hospitals may not have excess capacity and thus, hospitals with more
patients have higher costs. One purpose of this analysis is to
investigate the relationship between hospital costs and hospital
utilization.</p>
<p>Recall that our measure of hospital charges is the logarithm of
costs per discharge (<span class="math inline">\(y\)</span>). The scatter plot in Figure
<a href="C4MLRANOVA.html#fig:Fig46">4.6</a> gives a preliminary idea of the relationship
between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>. We note that there appears to be a negative
relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>.</p>
<p>The negative relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> suggested by Figure
<a href="C4MLRANOVA.html#fig:Fig46">4.6</a> is misleading and is induced by an
<em>omitted variable</em>, the category of the cost (DRG). To see
the joint effect of the categorical variable DRG and the continuous
variable <span class="math inline">\(x\)</span>, in Figure <a href="C4MLRANOVA.html#fig:Fig47">4.7</a> is a plot of <span class="math inline">\(y\)</span> versus
<span class="math inline">\(x\)</span> where the plotting symbols are codes for the level of the
categorical variable. From this plot, we see that the level of cost
varies by level of the factor DRG. Moreover, for each level of DRG,
the slope between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> is either zero or positive. The slopes
are not negative, as suggested by Figure <a href="C4MLRANOVA.html#fig:Fig46">4.6</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig46"></span>
<img src="RegressionMarkdown_files/figure-html/Fig46-1.png" alt="Plot of natural logarithm of cost per discharge versus natural logarithm of the number of discharges. This plot suggests a misleading negative relationship." width="60%" />
<p class="caption">
Figure 4.6: <strong>Plot of natural logarithm of cost per discharge versus natural logarithm of the number of discharges.</strong> This plot suggests a misleading negative relationship.
</p>
</div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig47"></span>
<img src="RegressionMarkdown_files/figure-html/Fig47-1.png" alt="Letter plot of natural logarithm of cost per discharge versus natural logarithm of the number of discharges by DRG. Here, A is for DRG #209, B is for DRG #391, and C is for DRG #430." width="60%" />
<p class="caption">
Figure 4.7: <strong>Letter plot of natural logarithm of cost per discharge versus natural logarithm of the number of discharges by DRG.</strong> Here, A is for DRG #209, B is for DRG #391, and C is for DRG #430.
</p>
</div>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab49">Table 4.9: </span><strong>Wisconsin Hospital Cost Models Goodness of Fit</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model Description
</th>
<th style="text-align:center;">
Model degrees of freedom
</th>
<th style="text-align:center;">
Error degrees of freedom
</th>
<th style="text-align:right;">
Error Sum of Squares
</th>
<th style="text-align:right;">
R-squared (%)
</th>
<th style="text-align:right;">
Mean Square
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 6cm; border-right:1px solid;">
One factor ANOVA
</td>
<td style="text-align:center;width: 1.8cm; ">
2
</td>
<td style="text-align:center;width: 1.8cm; ">
76
</td>
<td style="text-align:right;width: 1.8cm; ">
9.396
</td>
<td style="text-align:right;width: 1.8cm; ">
93.3
</td>
<td style="text-align:right;width: 1.8cm; ">
0.124
</td>
</tr>
<tr>
<td style="text-align:left;width: 6cm; border-right:1px solid;">
Regression with constant intercept and slope
</td>
<td style="text-align:center;width: 1.8cm; ">
1
</td>
<td style="text-align:center;width: 1.8cm; ">
77
</td>
<td style="text-align:right;width: 1.8cm; ">
115.059
</td>
<td style="text-align:right;width: 1.8cm; ">
18.2
</td>
<td style="text-align:right;width: 1.8cm; ">
1.222
</td>
</tr>
<tr>
<td style="text-align:left;width: 6cm; border-right:1px solid;">
Regression with variable intercept and constant slope
</td>
<td style="text-align:center;width: 1.8cm; ">
3
</td>
<td style="text-align:center;width: 1.8cm; ">
75
</td>
<td style="text-align:right;width: 1.8cm; ">
7.482
</td>
<td style="text-align:right;width: 1.8cm; ">
94.7
</td>
<td style="text-align:right;width: 1.8cm; ">
0.100
</td>
</tr>
<tr>
<td style="text-align:left;width: 6cm; border-right:1px solid;">
Regression with constant intercept and variable slope
</td>
<td style="text-align:center;width: 1.8cm; ">
3
</td>
<td style="text-align:center;width: 1.8cm; ">
75
</td>
<td style="text-align:right;width: 1.8cm; ">
14.048
</td>
<td style="text-align:right;width: 1.8cm; ">
90.0
</td>
<td style="text-align:right;width: 1.8cm; ">
0.187
</td>
</tr>
<tr>
<td style="text-align:left;width: 6cm; border-right:1px solid;">
Regression with variable intercept and slope
</td>
<td style="text-align:center;width: 1.8cm; ">
5
</td>
<td style="text-align:center;width: 1.8cm; ">
73
</td>
<td style="text-align:right;width: 1.8cm; ">
5.458
</td>
<td style="text-align:right;width: 1.8cm; ">
96.1
</td>
<td style="text-align:right;width: 1.8cm; ">
0.075
</td>
</tr>
</tbody>
</table>
<p>Each of the five models defined in Table <a href="C4MLRANOVA.html#tab:Tab48">4.8</a>
was fit to this subset of the Hospital case study. The summary
statistics are in Table <a href="C4MLRANOVA.html#tab:Tab49">4.9</a>. For this data set, there
are <span class="math inline">\(n = 79\)</span> observations and <span class="math inline">\(c = 3\)</span> levels of the DRG factor. For each
model, the model degrees of freedom is the number of model
parameters minus one. The error degrees of freedom is the number of
observations minus the number of model parameters.</p>
<p>Using binary variables, each of the models in Table <a href="C4MLRANOVA.html#tab:Tab48">4.8</a> can be written in a regression format. As we have seen in Section <a href="C4MLRANOVA.html#Sec42">4.2</a>, when a model can be written as a subset of another, larger model, we have formal testing procedures available to decide which model is more appropriate. To illustrate this testing procedure with our DRG example, from Table <a href="C4MLRANOVA.html#tab:Tab49">4.9</a> and the associated plots, it seems clear that the DRG factor is important. Further, a <span class="math inline">\(t\)</span>-test, not presented here, shows that the covariate <span class="math inline">\(x\)</span> is important. Thus, let’s compare the full model <span class="math inline">\(E[y_{ij}] = \beta_{0,j} + \beta_{1,j}x\)</span> to the reduced model <span class="math inline">\(E[y_{ij}] = \beta_{0,j} + \beta_1x\)</span>. In other words, is there a different slope for each DRG?</p>
<p>Using the notation from Section <a href="C4MLRANOVA.html#Sec42">4.2</a>, we call the variable intercept and slope the full model. Under the null hypothesis, <span class="math inline">\(H_0: \beta_{1,1} = \beta_{1,2} = \beta_{1,3}\)</span>, we get the variable intercept, constant slope model. Thus, using the <span class="math inline">\(F\)</span>-ratio in equation <a href="C4MLRANOVA.html#eq:eq42">(4.2)</a>, we have</p>
<p><span class="math display">\[
F\text{-ratio} = \frac{(Error~SS)_{reduced} - (Error~SS)_{full}}{ps_{full}^2} = \frac{7.482 - 5.458}{2 \times 0.075} = 13.535.
\]</span></p>
<p>The 95th percentile from the <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(df_1 = p = 2\)</span> and <span class="math inline">\(df_2 = (df)_{full} = 73\)</span> is approximately 3.13. Thus, this test leads us to reject the null hypothesis and declare the alternative, the regression model with variable intercept and variable slope, to be valid.</p>
<hr />
</div>
<div id="combining-two-factors" class="section level4 unnumbered hasAnchor">
<h4>Combining Two Factors<a href="C4MLRANOVA.html#combining-two-factors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We have seen how to combine covariates as well as a covariate and factor, both additively and with interactions. In the same fashion, suppose that we have two factors, say sex (two levels, male/female) and age (three levels, young/middle/old). Let the corresponding binary variables be <span class="math inline">\(x_1\)</span> to indicate whether the observation represents a female, <span class="math inline">\(x_2\)</span> to indicate whether the observation represents a young person, and <span class="math inline">\(x_3\)</span> to indicate whether the observation represents a middle-aged person.</p>
<p>An <em>additive model</em> for these two factors may use the regression function</p>
<p><span class="math display">\[
\mathrm{E }~y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3.
\]</span></p>
<p>As we have seen, this model is simple to interpret. For example, we can interpret <span class="math inline">\(\beta_1\)</span> to be the sex effect, holding age constant.</p>
<p>We can also incorporate two interaction terms, <span class="math inline">\(x_1 x_2\)</span> and <span class="math inline">\(x_1 x_3\)</span>. Using all five explanatory variables yields the regression function</p>
<p><span class="math display" id="eq:eq412">\[
\mathrm{E }~y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3.
\tag{4.12}
\]</span></p>
<p>Here, the variables <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span> are known as the <em>main effects</em>. Table <a href="C4MLRANOVA.html#tab:Tab410">4.10</a> helps interpret this equation. Specifically, there are six types of people that we could encounter: males and females who are young, middle-aged, or old. We have six parameters in equation <a href="C4MLRANOVA.html#eq:eq412">(4.12)</a>. Table <a href="C4MLRANOVA.html#tab:Tab410">4.10</a> provides the link between the parameters and the types of people. By using the interaction terms, we do not impose any prior specifications on the additive effects of each factor. From Table <a href="C4MLRANOVA.html#tab:Tab410">4.10</a>, we see that the interpretation of the regression coefficients in equation <a href="C4MLRANOVA.html#eq:eq412">(4.12)</a> is not straightforward. However, using the additive model with interaction terms is equivalent to creating a new categorical variable with six levels, one for each type of person. If the interaction terms are critical in your study, you may wish to create a new factor that incorporates the interaction terms simply for ease of interpretation.</p>
<table style="width:100%;">
<caption><span id="tab:Tab410">Table 4.10: </span><strong>Regression Function for a Two Factor Model with Interactions</strong></caption>
<colgroup>
<col width="8%" />
<col width="8%" />
<col width="7%" />
<col width="7%" />
<col width="7%" />
<col width="7%" />
<col width="7%" />
<col width="47%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Sex</th>
<th align="left">Age</th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right"><span class="math inline">\(x_2\)</span></th>
<th align="right"><span class="math inline">\(x_3\)</span></th>
<th align="right"><span class="math inline">\(x_4\)</span></th>
<th align="right"><span class="math inline">\(x_5\)</span></th>
<th align="left">Regression Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Male</td>
<td align="left">Young</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left"><span class="math inline">\(\beta_0 + \beta_2\)</span></td>
</tr>
<tr class="even">
<td align="left">Male</td>
<td align="left">Middle</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left"><span class="math inline">\(\beta_0 + \beta_3\)</span></td>
</tr>
<tr class="odd">
<td align="left">Male</td>
<td align="left">Old</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left"><span class="math inline">\(\beta_0\)</span></td>
</tr>
<tr class="even">
<td align="left">Female</td>
<td align="left">Young</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left"><span class="math inline">\(\beta_0 + \beta_1 + \beta_2 + \beta_4\)</span></td>
</tr>
<tr class="odd">
<td align="left">Female</td>
<td align="left">Middle</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left"><span class="math inline">\(\beta_0 + \beta_1 + \beta_3 + \beta_5\)</span></td>
</tr>
<tr class="even">
<td align="left">Female</td>
<td align="left">Old</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left"><span class="math inline">\(\beta_0 + \beta_1\)</span></td>
</tr>
</tbody>
</table>
<p>Extensions to more than two factors follow in a similar fashion. For example, suppose that you are examining the behavior of firms with headquarters in ten geographic regions, two organizational structures (profit versus non-profit) with four years of data. If you decide to treat each variable as a factor and want to model all interaction terms, then this is equivalent to a factor with <span class="math inline">\(10 \times 2 \times 4 = 80\)</span> levels. Models with interaction terms can have a substantial number of parameters and the analyst must be prudent when specifying interactions to be considered.</p>
</div>
<div id="general-linear-model" class="section level4 unnumbered hasAnchor">
<h4>General Linear Model<a href="C4MLRANOVA.html#general-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The general linear model extends the linear regression model in two ways. First, explanatory variables may be continuous, categorical, or a combination. The only restriction is that they enter linearly such that the resulting regression function</p>
<p><span class="math display" id="eq:eq413">\[
\mathrm{E}~y = \beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k
\tag{4.13}
\]</span></p>
<p>is a linear combination of coefficients. As we have seen, we can square continuous variables or take other nonlinear transforms (such as logarithms) as well as use binary variables to represent categorical variables, so this “restriction,” as the name suggests, allows for a broad class of general functions to represent data.</p>
<p>The second extension is that the explanatory variables may be linear combinations of one another in the general linear model. Because of this, in the general linear model case, the parameter estimates need not be unique. However, an important feature of the general linear model is that the resulting fitted values turn out to be unique, using the method of least squares.</p>
<p>For example, in Section <a href="C4MLRANOVA.html#Sec43">4.3</a> we saw that the one factor ANOVA model could be expressed as a regression model with <span class="math inline">\(c\)</span> indicator variables. However, if we had attempted to estimate the model in equation <a href="C4MLRANOVA.html#eq:eq410">(4.10)</a>, the method of least squares would not have arrived at a unique set of regression coefficient estimates. The reason is that, in equation <a href="C4MLRANOVA.html#eq:eq410">(4.10)</a>, each explanatory variable can be expressed as a linear combination of the others. For example, observe that <span class="math inline">\(x_c = 1 - (x_1 + x_2 + \ldots + x_{c-1})\)</span>.</p>
<p>The fact that parameter estimates are not unique is a drawback, but not an overwhelming one. The assumption that the explanatory variables are not linear combinations of one another means that we can compute unique estimates of the regression coefficients using the method of least squares. In terms of matrices, because the explanatory variables are not linear combinations of one another, the matrix <span class="math inline">\(\mathbf{X}^{\prime}\mathbf{X}\)</span> is not invertible.</p>
<p>Specifically, suppose that we are considering the regression function in equation <a href="C4MLRANOVA.html#eq:eq413">(4.13)</a> and, using the method of least squares, our regression coefficient estimates are <span class="math inline">\(b_0^{o}, b_1^{o}, \ldots, b_k^{o}\)</span>. This set of regression coefficient estimates minimizes our error sum of squares, but there may be other sets of coefficients that also minimize the error sum of squares. The fitted values are computed as <span class="math inline">\(\hat{y}_i = b_0^{o} + b_1^{o} x_{i1} + \ldots + b_k^{o} x_{ik}\)</span>. It can be shown that the resulting fitted values are unique, in the sense that any set of coefficients that minimize the error sum of squares produce the same fitted values (see Section <a href="C4MLRANOVA.html#Sec473">4.7.3</a>).</p>
<p>Thus, for a set of data and a specified general linear model, fitted values are unique. Because residuals are computed as observed responses minus fitted values, we have that the residuals are unique. Because residuals are unique, we have the error sums of squares are unique. Thus, it seems reasonable, and is true, that we can use the general test of hypotheses described in Section <a href="C4MLRANOVA.html#Sec42">4.2</a> to decide whether collections of explanatory variables are important.</p>
<p>To summarize, for general linear models, parameter estimates may not be unique and thus not meaningful. An important part of regression models is the interpretation of regression coefficients. This interpretation is not necessarily available in the general linear model context. However, for general linear models, we may still discuss the importance of an individual variable or collection of variables through partial <em>F</em>-tests. Further, fitted values, and the corresponding exercise of prediction, works in the general linear model context. The advantage of the general linear model context is that we need not worry about the type of restrictions to impose on the parameters. Although not the subject of this text, this advantage is particularly important in complicated experimental designs used in the life sciences. The reader will find that general linear model estimation routines are widely available in statistical software packages available on the market today.</p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_ype7ok99&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_o0tfj2ef%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="4.7 CombineCatCont">
</iframe>
</center>
</div>
</div>
<div id="Sec45" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Further Reading and References<a href="C4MLRANOVA.html#Sec45" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are several good linear model books that focus on categorical variables and analysis of variable techniques. Hocking (2003) and Searle (1987) are good examples.</p>
<p><strong>Chapter References</strong></p>
<ul>
<li>Hocking, Ronald R. (2003). <em>Methods and Applications of Linear Models: Regression and the Analysis of Variance</em> John Wiley and Sons, New York.</li>
<li>Keeler, Emmett B., and John E. Rolph (1988). The demand for episodes of treatment in the Health Insurance Experiment. <em>Journal of Health Economics</em> 7: 337-367.</li>
<li>Searle, Shayle R. (1987). <em>Linear Models for Unbalanced Data</em>. John Wiley &amp; Sons, New York.</li>
</ul>
</div>
<div id="Sec46" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Exercises<a href="C4MLRANOVA.html#Sec46" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>4.1. In this exercise, we consider relating two statistics that summarize how well a regression model fits, the <span class="math inline">\(F\)</span>-ratio and <span class="math inline">\(R^2\)</span>, the coefficient of determination. (Here, the <span class="math inline">\(F\)</span>-ratio is the statistic used to test model adequacy, not a partial <span class="math inline">\(F\)</span> statistic.)</p>
<ol style="list-style-type: lower-alpha">
<li>Write down <span class="math inline">\(R^2\)</span> in terms of <span class="math inline">\(Error ~SS\)</span> and <span class="math inline">\(Regression ~SS\)</span>.</li>
<li>Write down <span class="math inline">\(F\)</span>-ratio in terms of <span class="math inline">\(Error ~SS\)</span>, <span class="math inline">\(Regression ~SS\)</span>, <span class="math inline">\(k\)</span>, and <span class="math inline">\(n\)</span>.</li>
<li>Establish the algebraic relationship
<span class="math display">\[
F\text{-ratio} = \frac{R^2}{1-R^2} \frac{n-(k+1)}{k}.
\]</span></li>
<li>Suppose that <span class="math inline">\(n = 40\)</span>, <span class="math inline">\(k = 5\)</span>, and <span class="math inline">\(R^2 = 0.20\)</span>. Calculate the <span class="math inline">\(F\)</span>-ratio. Perform the usual test of model adequacy to determine whether or not the five explanatory variables jointly significantly affect the response variable.</li>
<li>Suppose that <span class="math inline">\(n = 400\)</span> (not 40), <span class="math inline">\(k = 5\)</span>, and <span class="math inline">\(R^2 = 0.20\)</span>. Calculate the <span class="math inline">\(F\)</span>-ratio. Perform the usual test of model adequacy to determine whether or not the five explanatory variables jointly significantly affect the response variable.</li>
</ol>
<p>4.2. <strong>Hospital Costs</strong>. This exercise considers hospital expenditures data provided by the US Agency for Healthcare Research and Quality (AHRQ) and described in Exercise 1.4.</p>
<ol style="list-style-type: lower-alpha">
<li>Produce a scatter plot, correlation, and a linear regression of <span class="math inline">\(LNTOTCHG\)</span> on <span class="math inline">\(AGE\)</span>. Is <span class="math inline">\(AGE\)</span> a significant predictor of <span class="math inline">\(LNTOTCHG\)</span>?</li>
<li>You are concerned that newborns follow a different pattern than other ages. Create a binary variable that indicates whether or not <span class="math inline">\(AGE\)</span> equals zero. Run a regression using this binary variable and <span class="math inline">\(AGE\)</span> as explanatory variables. Is the binary variable statistically significant?</li>
<li>Now examine the gender effect, using the binary variable <span class="math inline">\(FEMALE\)</span> that is one if the patient is female and zero otherwise. Run a regression using <span class="math inline">\(AGE\)</span> and <span class="math inline">\(FEMALE\)</span> as explanatory variables. Run a second regression including these two variables with an interaction term. Comment on whether the gender effect is important in either model.</li>
<li>Now consider the type of admission, <span class="math inline">\(APRDRG\)</span>, an acronym for “all patient refined diagnostic related group.” This is a categorical explanatory variable that provides information on the type of hospital admission. There are several hundred levels of this category. For example, level 640 represents admission for a normal newborn, with neonatal weight greater than or equal to 2.5 kilograms. As another example, level 225 represents admission resulting in an appendectomy. <br>
<strong>d(i)</strong>. Run a one-factor ANOVA model, using <span class="math inline">\(APRDRG\)</span> to predict <span class="math inline">\(LNTOTCHG\)</span>. Examine the <span class="math inline">\(R^2\)</span> from this model and compare it to the coefficient of determination of the linear regression model of <span class="math inline">\(LNTOTCHG\)</span> on <span class="math inline">\(AGE\)</span>. Based on this comparison, which model do you think is preferred?<br>
<strong>d(ii)</strong>. For the one-factor model in part d(i), provide a 95% confidence interval for the mean of <span class="math inline">\(LNTOTCHG\)</span> for level 225 corresponding to an appendectomy. Convert your final answer from logarithmic dollars to dollars via exponentiation.<br>
<strong>d(iii)</strong>. Run a regression model of <span class="math inline">\(APRDRG\)</span>, <span class="math inline">\(FEMALE\)</span>, and <span class="math inline">\(AGE\)</span> on <span class="math inline">\(LNTOTCHG\)</span>. State whether <span class="math inline">\(AGE\)</span> is a statistically significant predictor of <span class="math inline">\(LNTOTCHG\)</span>. State whether <span class="math inline">\(FEMALE\)</span> is a statistically significant predictor of <span class="math inline">\(LNTOTCHG\)</span>.</li>
</ol>
<p>4.3. <strong>Nursing Home Utilization</strong>. This exercise considers nursing home data provided by the Wisconsin Department of Health and Family Services (DHFS) and described in Exercises 1.2, 2.10, and 2.20.</p>
<p>In addition to the size variables, we also have information on several binary variables. The variable <span class="math inline">\(URBAN\)</span> is used to indicate the facility’s location. It is one if the facility is located in an urban environment and zero otherwise. The variable <span class="math inline">\(MCERT\)</span> indicates whether the facility is Medicare-certified. Most, but not all, nursing homes are certified to provide Medicare-funded care. There are three organizational structures for nursing homes. They are government (state, counties, municipalities), for-profit businesses, and tax-exempt organizations. Periodically, facilities may change ownership and, less frequently, ownership type. We create two binary variables <span class="math inline">\(PRO\)</span> and <span class="math inline">\(TAXEXEMPT\)</span> to denote for-profit business and tax-exempt organizations, respectively. Some nursing homes opt not to purchase private insurance coverage for their employees. Instead, these facilities directly provide insurance and pension benefits to their employees; this is referred to as “self funding of insurance.” We use binary variable <span class="math inline">\(SELFFUNDINS\)</span> to denote it.</p>
<p>You decide to examine the relationship between <span class="math inline">\(LOGTPY(y)\)</span> and the explanatory variables. Use cost report year 2001 data, and do the following analysis.</p>
<ol style="list-style-type: lower-alpha">
<li>There are three levels of organizational structures, but we only use two binary variables (<span class="math inline">\(PRO\)</span> and <span class="math inline">\(TAXEXEMPT\)</span>). Explain why.</li>
<li>Run a one-way analysis of variance using <span class="math inline">\(TAXEXEMPT\)</span> as the factor. Decide whether or not tax-exempt is an important factor in determining <span class="math inline">\(LOGTPY\)</span>. State your null hypothesis, alternative hypothesis, and all components of the decision-making rule. Use a 5% level of significance.</li>
<li>Run a one-way analysis of variance using <span class="math inline">\(MCERT\)</span> as the factor. Decide whether or not <span class="math inline">\(MCERT\)</span> is an important factor in determining <span class="math inline">\(LOGTPY\)</span>. <br>
<strong>c(i)</strong>. Provide a point estimate of <span class="math inline">\(LOGTPY\)</span> for a nursing facility that is not Medicare-Certified.<br>
<strong>c(ii)</strong>. Provide a 95% confidence interval for your point estimate in part (i).<br></li>
<li>Run a regression model using the binary variables, <span class="math inline">\(URBAN\)</span>, <span class="math inline">\(PRO\)</span>, <span class="math inline">\(TAXEXEMPT\)</span>, <span class="math inline">\(SELFFUNDINS\)</span>, and <span class="math inline">\(MCERT\)</span>. Find <span class="math inline">\(R^2\)</span>. Which variables are statistically significant?</li>
<li>Run a regression model using all explanatory variables, <span class="math inline">\(LOGNUMBED\)</span>, <span class="math inline">\(LOGSQRFOOT\)</span>, <span class="math inline">\(URBAN\)</span>, <span class="math inline">\(PRO\)</span>, <span class="math inline">\(TAXEXEMPT\)</span>, <span class="math inline">\(SELFFUNDINS\)</span>, and <span class="math inline">\(MCERT\)</span>. Find <span class="math inline">\(R^2\)</span>. Which variables are statistically significant?<br>
<strong>e(i)</strong>. Calculate the partial correlation between <span class="math inline">\(LOGTPY\)</span> and <span class="math inline">\(LOGSQRFOOT\)</span>. Compare this to the correlation between <span class="math inline">\(LOGTPY\)</span> and <span class="math inline">\(LOGSQRFOOT\)</span>. Explain why the partial correlation is small.<br>
<strong>e(ii)</strong>. Compare the low level of the <span class="math inline">\(t\)</span>-ratios (for testing the importance of individual regression coefficients) and the high level of the <span class="math inline">\(F\)</span>-ratio (for testing model adequacy). Describe the seeming inconsistency and provide an explanation for this inconsistency.</li>
</ol>
<p>4.4. <strong>Automobile Insurance Claims</strong>. Refer to Exercise 1.3.</p>
<ol style="list-style-type: lower-alpha">
<li>Run a regression of <span class="math inline">\(LNPAID\)</span> on <span class="math inline">\(AGE\)</span>. Is <span class="math inline">\(AGE\)</span> a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. Also comment on the goodness of fit of this variable.</li>
<li>Consider using class as a single explanatory variable. Use the one factor to estimate the model and respond to the following questions.<br>
<strong>b(i)</strong>. What is the point estimate of claims in class C7, drivers 50-69, driving to work or school, less than 30 miles per week with annual mileage under 7500, in natural logarithmic units?<br>
<strong>b(ii)</strong>. Determine the corresponding 95% confidence interval of expected claims, in natural logarithmic units.<br>
<strong>b(iii)</strong>. Convert the 95% confidence interval of expected claims that you determined in part b(ii) to dollars.<br></li>
<li>Run a regression of <span class="math inline">\(LNPAID\)</span> on <span class="math inline">\(AGE\)</span>, <span class="math inline">\(GENDER\)</span>, and the categorical variables <span class="math inline">\(STATE\ CODE\)</span> and <span class="math inline">\(CLASS\)</span>.<br>
<strong>c(i)</strong>. Is <span class="math inline">\(GENDER\)</span> a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule.<br>
<strong>c(ii)</strong>. Is <span class="math inline">\(CLASS\)</span> a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule.<br>
<strong>c(iii)</strong>. Use the model to provide a point estimate of claims in dollars (not log dollars) for a male age 60 in STATE 2 in CLASS C7.<br>
<strong>c(iv)</strong>. Write down the coefficient associated with CLASS C7 and interpret this coefficient.</li>
</ol>
<p>4.5. <strong>Wisconsin Lottery Sales</strong>. This exercise considers State of Wisconsin lottery sales data that were described in Section 2.1 and examined in Exercise 3.4.</p>
<p><strong>Part 1:</strong> You decide to examine the relationship between SALES (<span class="math inline">\(y\)</span>) and all eight explanatory variables (<span class="math inline">\(PERPERHH\)</span>, <span class="math inline">\(MEDSCHYR\)</span>, <span class="math inline">\(MEDHVL\)</span>, <span class="math inline">\(PRCRENT\)</span>, <span class="math inline">\(PRC55P\)</span>, <span class="math inline">\(HHMEDAGE\)</span>, <span class="math inline">\(MEDINC\)</span>, and <span class="math inline">\(POP\)</span>).</p>
<ol style="list-style-type: lower-alpha">
<li><p>Fit a regression model of SALES on all eight explanatory variables.</p></li>
<li><p>Find <span class="math inline">\(R^2\)</span>.<br>
<strong>b(i)</strong>. Use it to calculate the correlation coefficient between the observed and fitted values.<br>
<strong>b(ii)</strong>. You want to use <span class="math inline">\(R^2\)</span> to test the adequacy of the model in part (a). Use a formal test of hypothesis. State your null and alternative hypothesis, decision-making criterion, and your decision-making rules.<br></p></li>
<li><p>Test whether <span class="math inline">\(POP\)</span>, <span class="math inline">\(MEDSCHYR\)</span>, and <span class="math inline">\(MEDHVL\)</span> are jointly important explanatory variables for understanding SALES.</p>
<p><strong>Part 2:</strong> After the preliminary analysis in Part 1, you decide to examine the relationship between SALES (<span class="math inline">\(y\)</span>) and <span class="math inline">\(POP\)</span>, <span class="math inline">\(MEDSCHYR\)</span>, and <span class="math inline">\(MEDHVL\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Fit a regression model of SALES on these three explanatory variables.</p></li>
<li><p>Has the coefficient of determination decreased from the eight-variable regression model to the three-variable model? Does this mean that the model is not improved or does it provide little information? Explain your response.</p></li>
<li><p>To state formally whether one should use the three or eight variable model, use a partial <span class="math inline">\(F\)</span>-test. State your null and alternative hypotheses, decision-making criterion, and your decision-making rules.</p></li>
</ol></li>
</ol>
<p>4.6. <strong>Insurance Company Expenses</strong>. This exercise considers insurance company data from the NAIC and described in Exercises 1.6 and 3.5.</p>
<ol style="list-style-type: lower-alpha">
<li>Are the quadratic terms important?</li>
</ol>
<p>Consider a linear model of <span class="math inline">\(LNEXPENSES\)</span> on twelve explanatory variables. For the explanatory variables, include <span class="math inline">\(ASSETS\)</span>, <span class="math inline">\(GROUP\)</span>, both versions of losses and gross premiums, as well as the two BLS variables. Also include the square of each of the two loss and the two gross premium variables.</p>
<p>Test whether the four squared terms are jointly statistically significant, using a partial <span class="math inline">\(F\)</span>-test. State your null and alternative hypotheses, decision-making criterion, and your decision-making rules.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Are the interaction terms with <span class="math inline">\(GROUP\)</span> important?</li>
</ol>
<p>Omit the two BLS variables, so that now there are eleven variables: <span class="math inline">\(ASSETS\)</span>, <span class="math inline">\(GROUP\)</span>, both versions of losses and gross premiums, as well as interactions of <span class="math inline">\(GROUP\)</span> with <span class="math inline">\(ASSETS\)</span> and both versions of losses and gross premiums.</p>
<p>Test whether the five interaction terms are jointly statistically significant, using a partial <span class="math inline">\(F\)</span>-test. State your null and alternative hypotheses, decision-making criterion, and your decision-making rules.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>You are examining a company that is not in the sample with values <span class="math inline">\(LONGLOSS = 0.025\)</span>, <span class="math inline">\(SHORTLOSS = 0.040\)</span>, <span class="math inline">\(GPWPERSONAL = 0.050\)</span>, <span class="math inline">\(GPWCOMM = 0.120\)</span>, <span class="math inline">\(ASSETS = 0.400\)</span>, <span class="math inline">\(CASH = 0.350\)</span>, and <span class="math inline">\(GROUP = 1\)</span>.</li>
</ol>
<p>Use the eleven variable interaction model in part (b) to produce a 95% prediction interval for this company.</p>
<p>4.7. <strong>National Life Expectancies</strong>. We continue the analysis begun in Exercises 1.7, 2.22, and 3.6.</p>
<ol style="list-style-type: lower-alpha">
<li>Consider the regression using three explanatory variables, <span class="math inline">\(FERTILITY\)</span>, <span class="math inline">\(PUBLICEDUCATION\)</span>, and <span class="math inline">\(\ln{HEALTH}\)</span> that you did in Exercise 3.6. Test whether <span class="math inline">\(PUBLICEDUCATION\)</span> and <span class="math inline">\(\ln{HEALTH}\)</span> are jointly statistically significant, using a partial <span class="math inline">\(F\)</span>-test. State your null and alternative hypotheses, decision-making criterion, and your decision-making rules. (Hint: Use the coefficient of determination form for calculating the test statistic.) Provide an approximate <span class="math inline">\(p\)</span>-value for the test.</li>
<li>We now introduce the <span class="math inline">\(REGION\)</span> variable, summarized in Table <a href="C4MLRANOVA.html#tab:Tab411">4.11</a>. A boxplot of life expectancies versus <span class="math inline">\(REGION\)</span> is given in Figure <a href="C4MLRANOVA.html#fig:Fig48">4.8</a>. Describe what we learn from the Table and boxplot about the effect of <span class="math inline">\(REGION\)</span> on <span class="math inline">\(LIFEEXP\)</span>.</li>
</ol>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig48"></span>
<img src="RegressionMarkdown_files/figure-html/Fig48-1.png" alt="Boxplots of LIFEEXP by REGION" width="60%" />
<p class="caption">
Figure 4.8: <strong>Boxplots of LIFEEXP by REGION</strong>
</p>
</div>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab411">Table 4.11: </span><strong>Average Life Expectancy by Region</strong>
</caption>
<thead>
<tr>
<th style="text-align:center;">
Region
</th>
<th style="text-align:center;">
Description of the Region
</th>
<th style="text-align:center;">
Number
</th>
<th style="text-align:center;">
Mean
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
1
</td>
<td style="text-align:center;width: 1.8cm; width: 5cm; ">
Arab States
</td>
<td style="text-align:center;">
13
</td>
<td style="text-align:center;">
71.9
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
2
</td>
<td style="text-align:center;width: 1.8cm; width: 5cm; ">
East Asia and the Pacific
</td>
<td style="text-align:center;">
17
</td>
<td style="text-align:center;">
69.1
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
3
</td>
<td style="text-align:center;width: 1.8cm; width: 5cm; ">
Latin American and the Carribean
</td>
<td style="text-align:center;">
25
</td>
<td style="text-align:center;">
72.8
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
4
</td>
<td style="text-align:center;width: 1.8cm; width: 5cm; ">
South Asia
</td>
<td style="text-align:center;">
7
</td>
<td style="text-align:center;">
65.1
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
5
</td>
<td style="text-align:center;width: 1.8cm; width: 5cm; ">
Southern Europe
</td>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
67.4
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
6
</td>
<td style="text-align:center;width: 1.8cm; width: 5cm; ">
Sub-Saharan Africa
</td>
<td style="text-align:center;">
38
</td>
<td style="text-align:center;">
52.2
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
7
</td>
<td style="text-align:center;width: 1.8cm; width: 5cm; ">
Central and Eastern Europe
</td>
<td style="text-align:center;">
24
</td>
<td style="text-align:center;">
71.6
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
8
</td>
<td style="text-align:center;width: 1.8cm; width: 5cm; ">
High Income OECD
</td>
<td style="text-align:center;">
23
</td>
<td style="text-align:center;">
79.6
</td>
</tr>
<tr>
<td style="text-align:center;width: 2.5cm; border-right:1px solid;">
</td>
<td style="text-align:center;width: 1.8cm; width: 5cm; ">
All
</td>
<td style="text-align:center;">
150
</td>
<td style="text-align:center;">
67.4
</td>
</tr>
</tbody>
</table>
<p>Table 4.11 summarizes average life expectancy by region.</p>
<table>
<thead>
<tr class="header">
<th>REGION</th>
<th>Region Description</th>
<th>Number</th>
<th>Mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Arab States</td>
<td>13</td>
<td>71.9</td>
</tr>
<tr class="even">
<td>2</td>
<td>East Asia and the Pacific</td>
<td>17</td>
<td>69.1</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Latin American and the Carribean</td>
<td>25</td>
<td>72.8</td>
</tr>
<tr class="even">
<td>4</td>
<td>South Asia</td>
<td>7</td>
<td>65.1</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Southern Europe</td>
<td>3</td>
<td>67.4</td>
</tr>
<tr class="even">
<td>6</td>
<td>Sub-Saharan Africa</td>
<td>38</td>
<td>52.2</td>
</tr>
<tr class="odd">
<td>7</td>
<td>Central and Eastern Europe</td>
<td>24</td>
<td>71.6</td>
</tr>
<tr class="even">
<td>8</td>
<td>High Income OECD</td>
<td>23</td>
<td>79.6</td>
</tr>
<tr class="odd">
<td></td>
<td>All</td>
<td>150</td>
<td>67.4</td>
</tr>
</tbody>
</table>
<ol start="3" style="list-style-type: lower-alpha">
<li>Fit a regression model using only the factor, <span class="math inline">\(REGION\)</span>. Is <span class="math inline">\(REGION\)</span> a statistically significant determinant of <span class="math inline">\(LIFEEXP\)</span>? State your null and alternative hypotheses, decision-making criterion, and your decision-making rules.</li>
<li>Fit a regression model using three explanatory variables, <span class="math inline">\(FERTILITY\)</span>, <span class="math inline">\(PUBLICEDUCATION\)</span>, and <span class="math inline">\(\ln{HEALTH}\)</span>, as well as the categorical variable <span class="math inline">\(REGION\)</span>.<br>
<strong>d(i)</strong>. You are examining a country not in the sample with values <span class="math inline">\(FERTILITY = 2.0\)</span>, <span class="math inline">\(PUBLICEDUCATION = 5.0\)</span>, and <span class="math inline">\(\ln{HEALTH} = 1.0\)</span>. Produce two predicted life expectancy values by assuming that the country is from (1) an Arab state and (2) Sub-Saharan Africa.<br>
<strong>d(ii)</strong>. Provide a 95% confidence interval for the difference in life expectancies between an Arab state and a country from Sub-Saharan Africa.<br>
<strong>d(iii)</strong>. Provide the (usual ordinary least squares) point estimate for the difference in life expectancies between a country from Sub-Saharan Africa and a high-income OECD (Organization for Economic Co-operation and Development) country.</li>
</ol>
</div>
<div id="Sec47" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Technical Supplement - Matrix Expressions<a href="C4MLRANOVA.html#Sec47" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="Sec471" class="section level3 hasAnchor" number="4.7.1">
<h3><span class="header-section-number">4.7.1</span> Expressing Models with Categorical Variables in Matrix Form<a href="C4MLRANOVA.html#Sec471" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Chapter 3 showed how to write the regression model equation in the form <span class="math inline">\(\mathbf{y = X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}\)</span> where <span class="math inline">\(\mathbf{X}\)</span> is a matrix of explanatory variables. This form permits straightforward calculation of regression coefficients, <span class="math inline">\(\mathbf{b} = \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{y}\)</span>. This section shows how the model and calculations reduce to simpler expressions when the explanatory variables are categorical.</p>
<p><strong>One Categorical Variable Model.</strong> Consider the model with one categorical variable introduced in Section <a href="C4MLRANOVA.html#Sec43">4.3</a> with <span class="math inline">\(c\)</span> levels of the categorical variable. From equation <a href="C4MLRANOVA.html#eq:eq49">(4.9)</a>, this model can be written as</p>
<p><span class="math display">\[
\mathbf{y} =
\begin{bmatrix}
y_{1,1} \\
\cdot \\
\cdot \\
\cdot \\
y_{n_1,1} \\
\cdot \\
\cdot \\
\cdot \\
y_{1,c} \\
\cdot \\
\cdot \\
\cdot \\
y_{n_{c},c}
\end{bmatrix}
=
\begin{bmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
\cdot &amp; \cdot &amp; \cdots &amp; \cdot \\
\cdot &amp; \cdot &amp; \cdots &amp; \cdot \\
\cdot &amp; \cdot &amp; \cdots &amp; \cdot \\
1 &amp; 0 &amp; \cdots &amp; \cdots \\
\cdot &amp; \cdot &amp; \cdots &amp; \cdot \\
\cdot &amp; \cdot &amp; \cdots &amp; \cdot \\
\cdot &amp; \cdot &amp; \cdots &amp; \cdot \\
0 &amp; 0 &amp; \cdots &amp; 1 \\
\cdot &amp; \cdot &amp; \cdots &amp; \cdot \\
\cdot &amp; \cdot &amp; \cdots &amp; \cdot \\
\cdot &amp; \cdot &amp; \cdots &amp; \cdot \\
0 &amp; 0 &amp; \cdots &amp; 1 \\
\end{bmatrix}
\begin{bmatrix}
\mu_1 \\
\cdot \\
\cdot \\
\cdot \\
\mu_c
\end{bmatrix}
+
\begin{bmatrix}
\varepsilon_{1,1} \\
\cdot \\
\cdot \\
\cdot \\
\varepsilon_{n_1,1} \\
\cdot \\
\cdot \\
\cdot \\
\varepsilon_{1,c} \\
\cdot \\
\cdot \\
\cdot \\
\varepsilon_{n_{c},c}
\end{bmatrix}
= \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}
\]</span></p>
<p>To make the notation more compact, we write <span class="math inline">\(\mathbf{0}\)</span> and <span class="math inline">\(\mathbf{1}\)</span> for a column of zeros and ones, respectively. With this convention, another way to express this as</p>
<p><span class="math display" id="eq:eq414">\[
\mathbf{y} =
\begin{bmatrix}
\mathbf{1}_1 &amp; \mathbf{0}_1 &amp; \cdots &amp; \mathbf{0}_1 \\
\mathbf{0}_2 &amp; \mathbf{1}_2 &amp; \cdots &amp; \mathbf{0}_2 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathbf{0}_c &amp; \mathbf{0}_c &amp; \cdots &amp; \mathbf{1}_c
\end{bmatrix}
\begin{bmatrix}
\mu_1 \\
\mu_2 \\
\vdots \\
\mu_c
\end{bmatrix}
+ \boldsymbol{\varepsilon} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}
\tag{4.14}
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{0}_1\)</span> and <span class="math inline">\(\mathbf{1}_1\)</span> stand for vector columns of length <span class="math inline">\(n_1\)</span> of zeros and ones, respectively, and similarly for <span class="math inline">\(\mathbf{0}_2, \mathbf{1}_2, \ldots, \mathbf{0}_c, \mathbf{1}_c\)</span>.</p>
<p>Equation <a href="C4MLRANOVA.html#eq:eq414">(4.14)</a> allows us to apply the machinery developed for the regression model to the model with one categorical variable. As an intermediate calculation, we have</p>
<p><span class="math display" id="eq:eq415">\[
\begin{array}{ll}
(\mathbf{X}^{\prime} \mathbf{X})^{-1} &amp;= \left(
\begin{bmatrix}
\mathbf{1}_1 &amp; \mathbf{0}_2 &amp; \cdots &amp; \mathbf{0}_c \\
\mathbf{0}_1 &amp; \mathbf{1}_2 &amp; \cdots &amp; \mathbf{0}_c \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathbf{0}_1 &amp; \mathbf{0}_2 &amp; \cdots &amp; \mathbf{1}_c
\end{bmatrix}^{\prime}
\begin{bmatrix}
\mathbf{1}_1 &amp; \mathbf{0}_1 &amp; \cdots &amp; \mathbf{0}_1 \\
\mathbf{0}_2 &amp; \mathbf{1}_2 &amp; \cdots &amp; \mathbf{0}_2 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathbf{0}_c &amp; \mathbf{0}_c &amp; \cdots &amp; \mathbf{1}_c
\end{bmatrix}
\right)^{-1}\\
&amp;=
\begin{bmatrix}
n_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; n_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; n_c
\end{bmatrix}^{-1}
=
\begin{bmatrix}
\frac{1}{n_1} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \frac{1}{n_2} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \frac{1}{n_c}
\end{bmatrix} .
\tag{4.15}
\end{array}
\]</span></p>
<p>Thus, the parameter estimates are</p>
<p><span class="math display">\[
\mathbf{b} =
\begin{bmatrix}
\hat{\mu}_1 \\
\vdots \\
\hat{\mu}_c
\end{bmatrix}
=
(\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{y} =
\begin{bmatrix}
\frac{1}{n_1} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \frac{1}{n_2} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \frac{1}{n_c}
\end{bmatrix}
\begin{bmatrix}
\mathbf{1}_1 &amp; \mathbf{0}_2 &amp; \cdots &amp; \mathbf{0}_c \\
\mathbf{0}_1 &amp; \mathbf{1}_2 &amp; \cdots &amp; \mathbf{0}_c \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathbf{0}_1 &amp; \mathbf{0}_2 &amp; \cdots &amp; \mathbf{1}_c
\end{bmatrix}^{\prime}
\begin{bmatrix}
y_{1,1} \\
\vdots \\
y_{n_1,1} \\
\vdots \\
y_{1,c} \\
\vdots \\
y_{n_{c},c}
\end{bmatrix}
\]</span></p>
<p><span class="math display" id="eq:eq416">\[
=
\begin{bmatrix}
\frac{1}{n_1} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \frac{1}{n_2} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \frac{1}{n_c}
\end{bmatrix}
\begin{bmatrix}
\sum_{i=1}^{n_1} y_{i1} \\
\vdots \\
\sum_{i=1}^{n_c} y_{ic}
\end{bmatrix}
=
\begin{bmatrix}
\bar{y}_1 \\
\vdots \\
\bar{y}_c
\end{bmatrix}
\tag{4.16}
\]</span></p>
<p>We have seen that the least squares estimate of <span class="math inline">\(\mu_j\)</span>,
<span class="math inline">\(\bar{y}_j\)</span>, can been obtained directly from equation
<a href="C4MLRANOVA.html#eq:eq49">(4.9)</a>. By rewriting the model in matrix regression
notation, we can appeal to linear regression model results and need
not prove properties of models with categorical variables from first
principles. That is, because this model is in regression format, we
immediately have all the properties of the regression model.</p>
<p>Telling your software package that a variable is categorical can
mean more efficient calculations, as in the calculations of the
least squares regression estimates in equation
<a href="C4MLRANOVA.html#eq:eq416">(4.16)</a>. Further, calculation of other
quantities can also be done more directly. As another example, we
have that the standard error of <span class="math inline">\(\hat{\mu}_j\)</span> is</p>
<p><span class="math display">\[
se(\hat{\mu}_j) = s ~ \sqrt{j \text{th} \textit{ diagonal element of }  
\mathbf{(X}^{\prime}\mathbf{X)}^{-1}} = s/\sqrt{n_j}.
\]</span></p>
<div id="one-categorical-and-one-continuous-variable-model" class="section level4 unnumbered hasAnchor">
<h4>One Categorical and One Continuous Variable Model<a href="C4MLRANOVA.html#one-categorical-and-one-continuous-variable-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As another illustration, we consider the variable intercept and constant slope model. This model is summarized as <span class="math inline">\(\mathbf{y} = \mathbf{X} \boldsymbol \beta + \boldsymbol \varepsilon\)</span> where</p>
<p><span class="math display" id="eq:eq417">\[
\mathbf{X} =
\begin{bmatrix}
\mathbf{1}_1 &amp; \mathbf{0}_1 &amp; \cdot \cdot \cdot  &amp; \mathbf{0}_1 &amp; \mathbf{x}_1 \\
\mathbf{0}_2 &amp; \mathbf{1}_2 &amp; \cdot \cdot \cdot  &amp; \mathbf{0}_2 &amp; \mathbf{x}_2 \\
\cdot  &amp; \cdot  &amp; \cdot \cdot \cdot  &amp; \cdot  &amp; \cdot  \\
\cdot  &amp; \cdot  &amp; \cdot \cdot \cdot  &amp; \cdot  &amp; \cdot  \\
\cdot  &amp; \cdot  &amp; \cdot \cdot \cdot  &amp; \cdot  &amp; \cdot  \\
\mathbf{0}_{c} &amp; \mathbf{0}_{c} &amp; \cdot \cdot \cdot  &amp; \mathbf{1}_c &amp; \mathbf{x}_{c}
\end{bmatrix}
\text{   and   }
\boldsymbol \beta =
\begin{bmatrix}
\beta_{01} \\
\beta_{02} \\
\cdot  \\
\cdot  \\
\cdot  \\
\beta_{0c} \\
\beta_1
\end{bmatrix}.
\tag{4.17}
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{0}_j\)</span> and <span class="math inline">\(\mathbf{1}_j\)</span> stand for vector columns of length <span class="math inline">\(n_j\)</span> of zeros and ones, respectively, and <span class="math inline">\(\mathbf{x}_j = (x_{1j}, x_{2j}, \ldots, x_{n_j, j})^{\prime}\)</span> is the column of the continuous variable at the <span class="math inline">\(j\)</span>th level. Straightforward matrix algebra techniques provide the least squares estimates.</p>
</div>
</div>
<div id="Sec472" class="section level3 hasAnchor" number="4.7.2">
<h3><span class="header-section-number">4.7.2</span> Calculating Least Squares Recursively<a href="C4MLRANOVA.html#Sec472" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When computing regression coefficients using least squares, <span class="math inline">\(\mathbf{b} = \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{y}\)</span>, for some applications, the dimension of <span class="math inline">\(\mathbf{X}^{\prime} \mathbf{X}\)</span> can be large, causing computational difficulties. Fortunately, for some problems, the computations can be partitioned into smaller problems that can be solved recursively.</p>
<div class="blackbox">
<p><em>Recursive Least Squares Calculation.</em> Suppose that the regression function can be written as</p>
<p><span class="math display" id="eq:eq418">\[
\mathrm{E}~\mathbf{y} = \mathbf{X} \boldsymbol \beta = \left(
\mathbf{X}_1 : \mathbf{X}_2
\right) \left(
\begin{array}{c}
\boldsymbol \beta_1 \\
\boldsymbol \beta_2 \\
\end{array}
\right),
\tag{4.18}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}_1\)</span> has dimensions <span class="math inline">\(n \times k_1\)</span>, <span class="math inline">\(\mathbf{X}_2\)</span> has dimensions <span class="math inline">\(n \times k_2\)</span>, <span class="math inline">\(k_1 + k_2 = k\)</span>, <span class="math inline">\(\boldsymbol \beta_1\)</span> has dimensions <span class="math inline">\(k_1 \times 1\)</span>, and <span class="math inline">\(\boldsymbol \beta_2\)</span> has dimensions <span class="math inline">\(k_2 \times 1\)</span>. Define <span class="math inline">\(\mathbf{Q}_1 = \mathbf{I} - \mathbf{X}_1 \left(\mathbf{X}_1^{\prime} \mathbf{X}_1\right)^{-1} \mathbf{X}_1^{\prime}\)</span>. Then, the least squares estimator can be computed as:</p>
<p><span class="math display" id="eq:eq419">\[
\mathbf{b} = \left(
\begin{array}{c}
\mathbf{b}_1 \\
\mathbf{b}_2 \\
\end{array}
\right)
=
\left(
\begin{array}{c}
(\mathbf{X}_1^{\prime} \mathbf{X}_1)^{-1} \mathbf{X}_1^{\prime}
\left(
\mathbf{y} - \mathbf{X}_2 \mathbf{b}_2
\right) \\
\left(\mathbf{X}_2^{\prime} \mathbf{Q}_1 \mathbf{X}_2\right)^{-1} \mathbf{X}_2^{\prime} \mathbf{Q}_1 \mathbf{y} \\
\end{array}
\right).
\tag{4.19}
\]</span></p>
</div>
<p>Equation <a href="C4MLRANOVA.html#eq:eq419">(4.19)</a> provides the first step in the recursion. It can easily be iterated to allow for a more detailed decomposition of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p><strong>Special Case: One Categorical and One Continuous Variable Model.</strong> To illustrate the relevance of equation <a href="C4MLRANOVA.html#eq:eq419">(4.19)</a>, consider the model summarized in equation <a href="C4MLRANOVA.html#eq:eq417">(4.17)</a>. Here, the dimension of <span class="math inline">\(\mathbf{X}\)</span> is <span class="math inline">\(n \times (c+1)\)</span> and so the dimension of <span class="math inline">\(\mathbf{X}^{\prime} \mathbf{X}\)</span> is <span class="math inline">\((c+1) \times (c+1)\)</span>. Taking the inverse of this matrix could be difficult if <span class="math inline">\(c\)</span> is large. To apply equation <a href="C4MLRANOVA.html#eq:eq419">(4.19)</a>, we define</p>
<p><span class="math display">\[
\mathbf{X}_1 =
\begin{bmatrix}
\mathbf{1}_1 &amp; \mathbf{0}_1 &amp; \cdot \cdot \cdot  &amp; \mathbf{0}_1 \\
\mathbf{0}_2 &amp; \mathbf{1}_2 &amp; \cdot \cdot \cdot  &amp; \mathbf{0}_2 \\
\cdot  &amp; \cdot  &amp; \cdot \cdot \cdot  &amp; \cdot  \\
\cdot  &amp; \cdot  &amp; \cdot \cdot \cdot  &amp; \cdot  \\
\cdot  &amp; \cdot  &amp; \cdot \cdot \cdot  &amp; \cdot  \\
\mathbf{0}_c &amp; \mathbf{0}_c &amp; \cdot \cdot \cdot  &amp; \mathbf{1}_c
\end{bmatrix}
\text{   and   }
\mathbf{X}_2 =
\begin{bmatrix}
\mathbf{x}_1 \\
\mathbf{x}_2 \\
\cdot  \\
\cdot  \\
\cdot  \\
\mathbf{x}_c
\end{bmatrix}.
\]</span></p>
<p>In this case, we have seen in equation <a href="C4MLRANOVA.html#eq:eq415">(4.15)</a> how it is straightforward to compute <span class="math inline">\(\left(\mathbf{X}_1^{\prime} \mathbf{X}_1\right)^{-1}\)</span> without requiring matrix inversion. This makes calculating <span class="math inline">\(\mathbf{Q}_1\)</span> straightforward. With this, we can compute <span class="math inline">\(\mathbf{X}_2^{\prime} \mathbf{Q}_1 \mathbf{X}_2\)</span> and, because it is a scalar, immediately get its inverse. This provides <span class="math inline">\(\mathbf{b}_2\)</span> which is then used to calculate <span class="math inline">\(\mathbf{b}_1\)</span>. Although this procedure is not as direct as <span class="math inline">\(\mathbf{b} = \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{y}\)</span>, it can be computationally efficient.</p>
<div id="partitioned-matrix-results" class="section level4 unnumbered hasAnchor">
<h4>Partitioned Matrix Results<a href="C4MLRANOVA.html#partitioned-matrix-results" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To establish equation <a href="C4MLRANOVA.html#eq:eq419">(4.19)</a>, we use results standard in matrix algebra regarding the inverses of partitioned matrices.</p>
<p><strong>Partitioned Matrix Results.</strong> Suppose that we can partition the <span class="math inline">\((p+q) \times (p+q)\)</span> matrix <span class="math inline">\(\mathbf{B}\)</span> as</p>
<p><span class="math display">\[
\mathbf{B} =
\begin{bmatrix}
\mathbf{B}_{11} &amp; \mathbf{B}_{12} \\
\mathbf{B}_{12}^{\prime} &amp; \mathbf{B}_{22}
\end{bmatrix},
\]</span></p>
<p>where <span class="math inline">\(\mathbf{B}_{11}\)</span> is a <span class="math inline">\(p \times p\)</span> invertible matrix, <span class="math inline">\(\mathbf{B}_{22}\)</span> is a <span class="math inline">\(q \times q\)</span> invertible matrix, and <span class="math inline">\(\mathbf{B}_{12}\)</span> is a <span class="math inline">\(p \times q\)</span> matrix. Then</p>
<p><span class="math display" id="eq:eq420">\[
\mathbf{B}^{-1} =
\begin{bmatrix}
\mathbf{C}_{11}^{-1} &amp;
- \mathbf{B}_{11}^{-1} \mathbf{B}_{12} \mathbf{C}_{22}^{-1} \\
- \mathbf{C}_{22}^{-1} \mathbf{B}_{12}^{\prime} \mathbf{B}_{11}^{-1} &amp;
\mathbf{C}_{22}^{-1}
\end{bmatrix},
\tag{4.20}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{C}_{11} = \mathbf{B}_{11} - \mathbf{B}_{12} \mathbf{B}_{22}^{-1} \mathbf{B}_{12}^{\prime}\)</span> and <span class="math inline">\(\mathbf{C}_{22} = \mathbf{B}_{22} - \mathbf{B}_{12}^{\prime} \mathbf{B}_{11}^{-1} \mathbf{B}_{12}.\)</span></p>
<p>To verify equation <a href="C4MLRANOVA.html#eq:eq420">(4.20)</a>, multiply <span class="math inline">\(\mathbf{B}^{-1}\)</span> by <span class="math inline">\(\mathbf{B}\)</span> to obtain the identity matrix <span class="math inline">\(\mathbf{I}\)</span>. Further,</p>
<p><span class="math display" id="eq:eq421">\[
\mathbf{C}_{11}^{-1} = \mathbf{B}_{11}^{-1} + \mathbf{B}_{11}^{-1} \mathbf{B}_{12} \mathbf{C}_{22}^{-1} \mathbf{B}_{12}^{\prime} \mathbf{B}_{11}^{-1}.
\tag{4.21}
\]</span></p>
<p>Now, we first write the least squares estimator as
<span class="math display">\[
\begin{array}{ll}
\mathbf{b} &amp;= \left( \mathbf{X}^{\prime}\mathbf{X}\right)^{-1}
\mathbf{X}^{\prime} \mathbf{y} = \left( \left(
  \begin{array}{c}
    \mathbf{X}_1^{\prime} \\
    \mathbf{X}_2^{\prime} \\
  \end{array}
\right)
\left( \mathbf{X}_1 : \mathbf{X}_2 \right)\right)^{-1}
\left(
  \begin{array}{c}
    \mathbf{X}_1^{\prime} \\
    \mathbf{X}_2^{\prime} \\
  \end{array}
\right) \mathbf{y} \\
&amp; = \left(
  \begin{array}{cc}
    \mathbf{X}_1^{\prime} \mathbf{X}_1 &amp; \mathbf{X}_1^{\prime} \mathbf{X}_2 \\
    \mathbf{X}_2^{\prime} \mathbf{X}_1 &amp; \mathbf{X}_2^{\prime} \mathbf{X}_2 \\
  \end{array}
\right)^{-1}
\left(
  \begin{array}{c}
    \mathbf{X}_1^{\prime} \mathbf{y} \\
    \mathbf{X}_2^{\prime} \mathbf{y} \\
  \end{array}
\right) = \left(
  \begin{array}{c}
    \mathbf{b}_1 \\
    \mathbf{b}_2 \\
  \end{array}
\right).
\end{array}
\]</span>
To apply the partitioned matrix results, we define
<span class="math display">\[
\mathbf{Q}_j = \mathbf{I} - \mathbf{X}_j
\left(\mathbf{X}_j^{\prime}\mathbf{X}_j \right)^{-1}
\mathbf{X}_j^{\prime},
\]</span>
<span class="math inline">\(j=1,2,\)</span> and <span class="math inline">\(\mathbf{B}_{j,k} = \mathbf{X}_j^{\prime}\mathbf{X}_k\)</span> for
<span class="math inline">\(j,k=1,2.\)</span> This means that <span class="math inline">\(\mathbf{C}_{11}=\mathbf{X}_1^{\prime}\mathbf{X}_1 - \mathbf{X}_1^{\prime}\mathbf{X}_2 (\mathbf{X}_2^{\prime}\mathbf{X}_2)^{-1} \mathbf{X}_2^{\prime}\mathbf{X}_1^{\prime }\)</span> <span class="math inline">\(= \mathbf{X}_1^{\prime} \mathbf{Q}_2 \mathbf{X}_1\)</span> and similarly <span class="math inline">\(\mathbf{C}_{22} = \mathbf{X}_2^{\prime} \mathbf{Q}_1 \mathbf{X}_2\)</span>. From the second row, we have
<span class="math display">\[
\begin{array}{ll}
\mathbf{b}_2 &amp;= \mathbf{C}_{22}^{-1} \left(
-\mathbf{B}_{12}^{\prime} \mathbf{B}_{11}^{-1}\mathbf{X}_1^{\prime}
\mathbf{y} +
\mathbf{X}_2^{\prime} \mathbf{y} \right) \\
&amp;= \left(\mathbf{X}_2^{\prime} \mathbf{Q}_1 \mathbf{X}_2
\right)^{-1} \left( - \mathbf{X}_2^{\prime} \mathbf{X}_1
(\mathbf{X}_1^{\prime}\mathbf{X}_1)^{-1} \mathbf{X}_1^{\prime}
\mathbf{y} + \mathbf{X}_2^{\prime}\mathbf{y} \right) \\
&amp;= \left(\mathbf{X}_2^{\prime} \mathbf{Q}_1 \mathbf{X}_2
\right)^{-1} \mathbf{X}_2^{\prime} \mathbf{Q}_1\mathbf{y}.
\end{array}
\]</span></p>
<p>From the first row,</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{b}_1 &amp;= \mathbf{C}_{11}^{-1}
\mathbf{X}_1^{\prime}\mathbf{y} -
\mathbf{B}_{11}^{-1}\mathbf{B}_{12}\mathbf{C}_{22}^{-1}
\mathbf{X}_2^{\prime}\mathbf{y} \\
&amp;= \left( \mathbf{B}_{11}^{-1} + \mathbf{B}_{11}^{-1}
\mathbf{B}_{12} \mathbf{C}_{22}^{-1} \mathbf{B}_{21}
\mathbf{B}_{11}^{-1} \right) \mathbf{X}_1^{\prime}\mathbf{y} -
\mathbf{B}_{11}^{-1}\mathbf{B}_{12}\mathbf{C}_{22}^{-1}
\mathbf{X}_2^{\prime}\mathbf{y} \\
&amp;= \mathbf{B}_{11}^{-1}\mathbf{X}_1^{\prime}\mathbf{y} -
\mathbf{B}_{11}^{-1} \mathbf{B}_{12} \mathbf{C}_{22}^{-1} \left(
-\mathbf{B}_{21} \mathbf{B}_{11}^{-1}
\mathbf{X}_1^{\prime}\mathbf{y} + \mathbf{X}_2^{\prime}\mathbf{y}
\right)\\
&amp;= \mathbf{B}_{11}^{-1}\mathbf{X}_1^{\prime}\mathbf{y} -
\mathbf{B}_{11}^{-1} \mathbf{B}_{12} \mathbf{b}_2 \\
&amp;= (\mathbf{X}_1^{\prime}\mathbf{X}_1)^{-1}\mathbf{X}_1^{\prime}\mathbf{y} -
(\mathbf{X}_1^{\prime}\mathbf{X}_1)^{-1} \mathbf{X}_1^{\prime}\mathbf{X}_2 \mathbf{b}_2 \\
&amp;= (\mathbf{X}_1^{\prime}\mathbf{X}_1)^{-1}\mathbf{X}_1^{\prime}
\left( \mathbf{y} - \mathbf{X}_2 \mathbf{b}_2 \right).
\end{array}
\]</span></p>
<p>This establishes equation <a href="C4MLRANOVA.html#eq:eq419">(4.19)</a>.</p>
<p><strong>Reparameterized Model</strong>. For the partitioned regression
function in equation <a href="C4MLRANOVA.html#eq:eq418">(4.18)</a>, define <span class="math inline">\(\mathbf{A}= \left( \mathbf{X}_1^{\prime} \mathbf{X}_1 \right)^{-1} \mathbf{X}_1^{\prime} \mathbf{X}_2\)</span> and <span class="math inline">\(\mathbf{E}_2 = \mathbf{X}_2 - \mathbf{X}_1 \mathbf{A}\)</span>. If one were to run a “multivariate”
regression using <span class="math inline">\(\mathbf{X}_2\)</span> as the response and <span class="math inline">\(\mathbf{X}_1\)</span>
as explanatory variables, then the parameter estimates would be
<span class="math inline">\(\mathbf{A}\)</span> and the residuals <span class="math inline">\(\mathbf{E}_2\)</span>.</p>
<p>With these definitions, use equation <a href="C4MLRANOVA.html#eq:eq418">(4.18)</a> to
define the reparameterized regression model
<span class="math display" id="eq:eq422">\[
\begin{array}{ll}
\mathbf{y} &amp; = \mathbf{X}_1 \boldsymbol \beta_1 + \mathbf{X}_2
\boldsymbol \beta_2 + \boldsymbol \varepsilon = \mathbf{X}_1
\boldsymbol \beta_1 + (\mathbf{E}_2 + \mathbf{X}_1
\mathbf{A})\boldsymbol \beta_2 + \boldsymbol \varepsilon \\
&amp; = \mathbf{X}_1 \boldsymbol \alpha_1 + \mathbf{E}_2 \boldsymbol
\beta_2 + \boldsymbol \varepsilon,
\end{array}
\tag{4.22}
\]</span>
where <span class="math inline">\(\boldsymbol \alpha_1 = \boldsymbol \beta_1 + \mathbf{A}\boldsymbol \beta_2\)</span> is a new vector of parameters. The
reason for introducing this new parameterization is that now the
vector of explanatory variables is <em>orthogonal</em> to the other
explanatory variables, that is, straightforward algebra shows that
<span class="math inline">\(\mathbf{X}_1^{\prime }\mathbf{E}_2=\mathbf{0}\)</span>.</p>
<p>By equation <a href="C4MLRANOVA.html#eq:eq419">(4.19)</a>, the vector of least squares estimates is
<span class="math display" id="eq:eq423">\[
\begin{array}{ll}
\mathbf{a} =
\begin{bmatrix}
\mathbf{a}_1 \\ \mathbf{b}_2
\end{bmatrix}
=
\left(
\begin{bmatrix}
\mathbf{X}_1^{\prime} \\ \mathbf{E}_2^{\prime}
\end{bmatrix}
\begin{bmatrix}
\mathbf{X}_1 &amp; \mathbf{E}_2
\end{bmatrix}
\right) ^{-1}
\begin{bmatrix}
\mathbf{X}_1^{\prime} \\ \mathbf{E}_2^{\prime}
\end{bmatrix}
\mathbf{y}
=
\begin{bmatrix}
\left( \mathbf{X}_1^{\prime} \mathbf{X}_1 \right)^{-1}
\mathbf{X}_1^{
\prime} \mathbf{y} \\
\left( \mathbf{E}_2^{\prime}\mathbf{E}_2\right) ^{-1}
\mathbf{E}_2^{\prime}\mathbf{y}
\end{bmatrix}.
\end{array}
\tag{4.23}
\]</span></p>
<p><strong>Extra Sum of Squares</strong>. Suppose that we wish to consider the
increase in the error sum of squares going from a <em>reduced</em> model
<span class="math display">\[
\mathbf{y} = \mathbf{X}_1 \boldsymbol \beta_1 + \boldsymbol
\varepsilon
\]</span>
to a <em>full</em> model
<span class="math display">\[
\mathbf{y} = \mathbf{X}_1 \boldsymbol \beta_1 + \mathbf{X}_2
\boldsymbol \beta_2 + \boldsymbol \varepsilon.
\]</span>
For the reduced model, the error sum of squares is
<span class="math display" id="eq:eq424">\[
(Error ~SS)_{reduced} = \mathbf{y}^{\prime} \mathbf{y} -
\mathbf{y}^{\prime} \mathbf{X}_1 (\mathbf{X}_1^{\prime}
\mathbf{X}_1)^{-1} \mathbf{X}_1^{\prime} \mathbf{y}.
\tag{4.24}
\]</span>
Using the reparameterized version of the full model, the error sum
of squares is
<span class="math display" id="eq:eq425">\[
\begin{array}{ll}
(Error ~SS)_{full} &amp;= \mathbf{y}^{\prime} \mathbf{y} -
\mathbf{a}^{\prime}
\begin{bmatrix}
\mathbf{X}_1^{\prime} \\ \mathbf{E}_2^{\prime}
\end{bmatrix} \mathbf{y}
= \mathbf{y}^{\prime}\mathbf{y} -
\begin{bmatrix}
\left( \mathbf{X}_1^{\prime} \mathbf{X}_1 \right)^{-1}
\mathbf{X}_1^{\prime} \mathbf{y} \\
\left( \mathbf{E}_2^{\prime}\mathbf{E}_2\right)^{-1}
\mathbf{E}_2^{\prime} \mathbf{y}
\end{bmatrix}^{\prime}
\begin{bmatrix}
\mathbf{X}_1^{\prime} \mathbf{y} \\
\mathbf{E}_2^{\prime}\mathbf{y}
\end{bmatrix} \\
&amp;= \mathbf{y}^{\prime} \mathbf{y} - \mathbf{y}^{\prime}
\mathbf{X}_1 (\mathbf{X}_1^{\prime} \mathbf{X}_1)^{-1}
\mathbf{X}_1^{\prime} \mathbf{y} - \mathbf{y}^{\prime} \mathbf{E}_2
(\mathbf{E}_2^{\prime} \mathbf{E}_2)^{-1} \mathbf{E}_2^{\prime}
\mathbf{y}.
\tag{4.25}
\end{array}
\]</span>
Thus, the reduction in the error sum of squares by adding
<span class="math inline">\(\mathbf{X}_2\)</span> to the model is
<span class="math display" id="eq:eq426">\[
(Error ~SS)_{reduced} - (Error ~SS)_{full}
= \mathbf{y}^{\prime} \mathbf{E}_2 (\mathbf{E}_2^{\prime}
\mathbf{E}_2)^{-1} \mathbf{E}_2^{\prime} \mathbf{y}.
\tag{4.26}
\]</span>
As noted in Section <a href="C4MLRANOVA.html#Sec43">4.3</a>, the quantity <span class="math inline">\((Error ~SS)_{reduced} - (Error ~SS)_{full}\)</span> is called the <em>extra sum of squares</em>, or Type
III sum of squares. It is produced automatically by some statistical
software packages, thus obviating the need to run separate
regressions.</p>
</div>
</div>
<div id="Sec473" class="section level3 hasAnchor" number="4.7.3">
<h3><span class="header-section-number">4.7.3</span> General Linear Model<a href="C4MLRANOVA.html#Sec473" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall the general linear model from Section <a href="C4MLRANOVA.html#Sec44">4.4</a>. That is, we use
<span class="math display">\[
y_i = \beta_0 x_{i0} + \beta_1 x_{i1} + \ldots + \beta_k x_{ik} +
\varepsilon_i,
\]</span>
or, in matrix notation, <span class="math inline">\(\mathbf{y} = \mathbf{X} \boldsymbol \beta + \boldsymbol \varepsilon\)</span>. As before, we use Assumptions F1-F4 (or E1-E4) so that
the disturbance terms are i.i.d. with mean zero and common variance <span class="math inline">\(\sigma^2\)</span>, and the explanatory variables
<span class="math inline">\(\{x_{i0},x_{i1},x_{i2},\ldots,x_{ik}\}\)</span> are non-stochastic.</p>
<p>In the general linear model, we do not require that
<span class="math inline">\(\mathbf{X}^{\prime}\mathbf{X}\)</span> be invertible. As we have seen in
Chapter 4, an important reason for this generalization relates to
handling categorical variables. That is, in order to use categorical
variables, they are generally re-coded using binary variables. For
this re-coding, generally some type of restrictions need to be made
on the set of parameters associated with the indicator variables.
However, it is not always clear what type of restrictions are the
most intuitive. By expressing the model without requiring that <span class="math inline">\(\mathbf{X}^{\prime}\mathbf{X}\)</span> be invertible, the restrictions can be imposed
after the estimation is done, not before.</p>
<p><strong>Normal Equations.</strong> Even when <span class="math inline">\(\mathbf{X}^{\prime}\mathbf{X}\)</span>
is not invertible, solutions to the normal equations still provide
least squares estimates of <span class="math inline">\(\boldsymbol \beta\)</span>. That is, the sum of
squares is
<span class="math display">\[
SS(\mathbf{b}^{\ast}) = \mathbf{(y - Xb}^{\ast}\mathbf{)}^{\prime}\mathbf{(y - Xb}^{\ast}\mathbf{)},
\]</span>
where
<span class="math inline">\(\mathbf{b}^{\ast} = (b_0^{\ast}, b_1^{\ast}, \ldots, b_k^{\ast})^{\prime}\)</span>
is a vector of candidate estimates. Solutions of the normal
equations are those vectors <span class="math inline">\(\mathbf{b}^{\circ }\)</span> that satisfy the
normal equations
<span class="math display" id="eq:eq427">\[
\mathbf{X}^{\prime}\mathbf{Xb}^{\circ } = \mathbf{X}^{\prime}\mathbf{y}.
\tag{4.27}
\]</span></p>
<p>We use the notation <span class="math inline">\(^{\circ }\)</span> to remind ourselves that
<span class="math inline">\(\mathbf{b}^{\circ }\)</span> need not be unique. However, it is a minimizer
of the sum of squares. To see this, consider another candidate vector <span class="math inline">\(\mathbf{b}^{\ast}\)</span> and note that
<span class="math display">\[
SS(\mathbf{b}^{\ast}) = \mathbf{y}^{\prime}\mathbf{y} - 2\mathbf{b}^{\ast \prime }\mathbf{X}^{\prime}\mathbf{y} + \mathbf{b}^{\ast \prime }\mathbf{X}^{\prime}\mathbf{Xb}^{\ast}.
\]</span>
Then, using equation <a href="C4MLRANOVA.html#eq:eq427">(4.27)</a>, we have
<span class="math display">\[
SS(\mathbf{b}^{\ast}) - SS(\mathbf{b}^{\circ }) = -2\mathbf{b}^{\ast \prime }\mathbf{X}^{\prime}\mathbf{y} + \mathbf{b}^{\ast \prime }\mathbf{X}^{\prime}\mathbf{Xb}^{\ast} - (-2\mathbf{b}^{\circ \prime }\mathbf{Xy} + \mathbf{b}^{\circ \prime }\mathbf{X}^{\prime}\mathbf{Xb}^{\circ })
\]</span>
<span class="math display">\[
= -2\mathbf{b}^{\ast \prime }\mathbf{Xb}^{\circ } + \mathbf{b}^{\ast \prime }\mathbf{X}^{\prime}\mathbf{Xb}^{\ast} + \mathbf{b}^{\circ \prime }\mathbf{X}^{\prime}\mathbf{Xb}^{\circ }
\]</span>
<span class="math display">\[
= \mathbf{(b}^{\ast} - \mathbf{b}^{\circ})^{\prime} \mathbf{X}^{\prime} \mathbf{X} (\mathbf{b}^{\ast} - \mathbf{b}^{\circ}) = \mathbf{z}^{\prime} \mathbf{z} \geq 0,
\]</span>
where <span class="math inline">\(\mathbf{z} = \mathbf{X} (\mathbf{b}^{\ast} - \mathbf{b}^{\circ})\)</span>. Thus, any other candidate <span class="math inline">\(\mathbf{b}^{\ast}\)</span> yields a sum of squares at least as large as <span class="math inline">\(SS(\mathbf{b}^{\circ})\)</span>.</p>
<div id="unique-fitted-values" class="section level4 unnumbered hasAnchor">
<h4>Unique Fitted Values<a href="C4MLRANOVA.html#unique-fitted-values" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Despite the fact that there may be (infinitely) many solutions to the normal equations, the resulting fitted values, <span class="math inline">\(\mathbf{\hat{y}} = \mathbf{Xb}^{\circ}\)</span>, are unique. To see this, suppose that <span class="math inline">\(\mathbf{b}_1^{\circ}\)</span> and <span class="math inline">\(\mathbf{b}_2^{\circ}\)</span> are two different solutions of equation <a href="C4MLRANOVA.html#eq:eq427">(4.27)</a>. Let <span class="math inline">\(\mathbf{\hat{y}}_1 = \mathbf{Xb}_1^{\circ}\)</span> and <span class="math inline">\(\mathbf{\hat{y}}_2 = \mathbf{Xb}_2^{\circ}\)</span> denote the vectors of fitted values generated by these estimates. Then,
<span class="math display">\[
\mathbf{(\hat{y}}_1 - \mathbf{\hat{y}}_2)^{\prime} (\mathbf{\hat{y}}_1 - \mathbf{\hat{y}}_2) = \mathbf{(b}_1^{\circ} - \mathbf{b}_2^{\circ})^{\prime} \mathbf{X}^{\prime} \mathbf{X} (\mathbf{b}_1^{\circ} - \mathbf{b}_2^{\circ}) = 0
\]</span>
because <span class="math inline">\(\mathbf{X}^{\prime} \mathbf{X} (\mathbf{b}_1^{\circ} - \mathbf{b}_2^{\circ}) = \mathbf{X}^{\prime} \mathbf{y} - \mathbf{X}^{\prime} \mathbf{y} = \mathbf{0}\)</span>, from equation <a href="C4MLRANOVA.html#eq:eq427">(4.27)</a>. Hence we have that <span class="math inline">\(\mathbf{\hat{y}}_1 = \mathbf{\hat{y}}_2\)</span> for any choice of <span class="math inline">\(\mathbf{b}_1^{\circ}\)</span> and <span class="math inline">\(\mathbf{b}_2^{\circ}\)</span>, thus establishing the uniqueness of the fitted values.</p>
<p>Because the fitted values are unique, the residuals are also unique. Thus, the error sum of squares and estimates of variability (such as <span class="math inline">\(s^2\)</span>) are also unique.</p>
</div>
<div id="generalized-inverses" class="section level4 unnumbered hasAnchor">
<h4>Generalized Inverses<a href="C4MLRANOVA.html#generalized-inverses" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A <em>generalized inverse</em> of a matrix <span class="math inline">\(\mathbf{A}\)</span> is a matrix <span class="math inline">\(\mathbf{B}\)</span> such that <span class="math inline">\(\mathbf{ABA = A}\)</span>. We use the notation <span class="math inline">\(\mathbf{A}^{\mathbf{-}}\)</span> to denote the generalized inverse of <span class="math inline">\(\mathbf{A}\)</span>. In the case that <span class="math inline">\(\mathbf{A}\)</span> is invertible, then <span class="math inline">\(\mathbf{A}^{\mathbf{-}}\)</span> is unique and equals <span class="math inline">\(\mathbf{A}^{\mathbf{-1}}\)</span>. Although there are several definitions of generalized inverses, the above definition suffices for our purposes. See Searle (1987) for further discussion of alternative definitions of generalized inverses.</p>
<p>With this definition, it can be shown that a solution to the equation <span class="math inline">\(\mathbf{Ab = c}\)</span> can be expressed as <span class="math inline">\(\mathbf{b = A}^{-} \mathbf{c}\)</span>. Thus, we can express a least squares estimate of <span class="math inline">\(\boldsymbol \beta\)</span> as <span class="math inline">\(\mathbf{b}^{\circ} = (\mathbf{X}^{\prime} \mathbf{X})^{-} \mathbf{X}^{\prime} \mathbf{y}\)</span>. Statistical software packages can calculate versions of <span class="math inline">\((\mathbf{X}^{\prime} \mathbf{X})^{-}\)</span> and thus generate <span class="math inline">\(\mathbf{b}^{\circ}\)</span>.</p>
</div>
<div id="estimable-functions" class="section level4 unnumbered hasAnchor">
<h4>Estimable Functions<a href="C4MLRANOVA.html#estimable-functions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Above, we saw that each fitted value <span class="math inline">\(\hat{y}_i\)</span> is unique. Because fitted values are simply linear combinations of parameter estimates, it seems reasonable to ask what other linear combinations of parameter estimates are unique. To this end, we say that <span class="math inline">\(\mathbf{C \boldsymbol \beta}\)</span> is an <em>estimable function</em> of parameters if <span class="math inline">\(\mathbf{Cb}^{\circ}\)</span> does not depend (is invariant) on the choice of <span class="math inline">\(\mathbf{b}^{\circ}\)</span>. Because fitted values are invariant to the choice of <span class="math inline">\(\mathbf{b}^{\circ}\)</span>, we have that <span class="math inline">\(\mathbf{C = X}\)</span> produces one type of estimable function. Interestingly, it turns out that all estimable functions are of the form <span class="math inline">\(\mathbf{LXb}^{\circ}\)</span>, that is, <span class="math inline">\(\mathbf{C} = \mathbf{LX}\)</span>. See Searle (1987, page 284) for a demonstration of this. Thus, all estimable functions are linear combinations of fitted values, that is, <span class="math inline">\(\mathbf{LXb}^{\circ} = \mathbf{L\hat{y}}\)</span>.</p>
<p>Estimable functions are unbiased and have a variance that does not depend on the choice of the generalized inverse. That is, it can be shown that <span class="math inline">\(\text{E } \mathbf{Cb}^{\circ} = \mathbf{C \boldsymbol \beta}\)</span> and <span class="math inline">\(\text{Var } \mathbf{Cb}^{\circ} = \sigma^2 \mathbf{C(X}^{\prime} \mathbf{X})^{-} \mathbf{C}^{\prime}\)</span> does not depend on the choice of <span class="math inline">\(\mathbf{(X}^{\prime} \mathbf{X})^{-}\)</span>.</p>
</div>
<div id="testable-hypotheses" class="section level4 unnumbered hasAnchor">
<h4>Testable Hypotheses<a href="C4MLRANOVA.html#testable-hypotheses" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As described in Section <a href="C4MLRANOVA.html#Sec42">4.2</a>, it is often of interest to test <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mathbf{C \boldsymbol \beta} = \mathbf{d}\)</span>, where <span class="math inline">\(\mathbf{d}\)</span> is a specified vector. This hypothesis is said to be <em>testable</em> if <span class="math inline">\(\mathbf{C \boldsymbol \beta}\)</span> is an estimable function, <span class="math inline">\(\mathbf{C}\)</span> is of full row rank, and the rank of <span class="math inline">\(\mathbf{C}\)</span> is less than the rank of <span class="math inline">\(\mathbf{X}\)</span>. For consistency with the notation of Section <a href="C4MLRANOVA.html#Sec42">4.2</a>, let <span class="math inline">\(p\)</span> be the rank of <span class="math inline">\(\mathbf{C}\)</span> and <span class="math inline">\(k + 1\)</span> be the rank of <span class="math inline">\(\mathbf{X}\)</span>. Recall that the rank of a matrix is the smaller of the number of linearly independent rows and linearly independent columns. When we say that <span class="math inline">\(\mathbf{C}\)</span> has full row rank, we mean that there are <span class="math inline">\(p\)</span> rows in <span class="math inline">\(\mathbf{C}\)</span>, so that the number of rows equals the rank.</p>
</div>
<div id="general-linear-hypothesis" class="section level4 unnumbered hasAnchor">
<h4>General Linear Hypothesis<a href="C4MLRANOVA.html#general-linear-hypothesis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As in Section <a href="C4MLRANOVA.html#Sec42">4.2</a>, the test statistic for examining <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mathbf{C \boldsymbol \beta} = \mathbf{d}\)</span> is
<span class="math display">\[
F\text{-ratio} = \frac{\mathbf{(Cb}^{\circ} - \mathbf{d})^{\prime} \mathbf{(C(X}^{\prime} \mathbf{X})^{-} \mathbf{C}^{\prime})^{-1} \mathbf{(Cb}^{\circ} - \mathbf{d})}{ps_{full}^2}.
\]</span>
Note that the statistic <span class="math inline">\(F\)</span>-ratio does not depend on the choice of <span class="math inline">\(\mathbf{b}^{\circ}\)</span> because <span class="math inline">\(\mathbf{C b}^{\circ}\)</span> is invariant to <span class="math inline">\(\mathbf{b}^{\circ}\)</span>. If <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mathbf{C \boldsymbol \beta} = \mathbf{d}\)</span> is a testable hypothesis and the errors <span class="math inline">\(\varepsilon_i\)</span> are i.i.d. <span class="math inline">\(\text{N}(0, \sigma^2)\)</span>, then the <span class="math inline">\(F\)</span>-ratio has an <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(df_1 = p\)</span> and <span class="math inline">\(df_2 = n - (k + 1)\)</span>.</p>
</div>
<div id="one-categorical-variable-model" class="section level4 unnumbered hasAnchor">
<h4>One Categorical Variable Model<a href="C4MLRANOVA.html#one-categorical-variable-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We now illustrate the general linear model by considering an over-parameterized version of the one factor model that appears in equation <a href="C4MLRANOVA.html#eq:eq410">(4.10)</a> using
<span class="math display">\[
y_{ij} = \mu + \tau_j + e_{ij} = \mu + \tau_1 x_{i1} + \tau_2 x_{i2} + \ldots + \tau_c x_{ic} + \varepsilon_{ij}.
\]</span>
At this point, we do not impose additional restrictions in the parameters. As with equation <a href="C4MLRANOVA.html#eq:eq413">(4.13)</a>, this can be written in matrix form as
<span class="math display">\[
\mathbf{y} =
\begin{bmatrix}
\mathbf{1}_1 &amp; \mathbf{1}_1 &amp; \mathbf{0}_1 &amp; \cdots &amp; \mathbf{0}_1 \\
\mathbf{1}_2 &amp; \mathbf{0}_2 &amp; \mathbf{1}_{\mathbf{2}} &amp; \cdots &amp; \mathbf{0}_2 \\
\cdot &amp; \cdot &amp; \cdot &amp; \cdots &amp; \cdot \\
\mathbf{1}_c &amp; \mathbf{0}_c &amp; \mathbf{0}_c &amp; \cdots &amp; \mathbf{1}_c
\end{bmatrix}
\begin{bmatrix}
\mu \\
\tau_1 \\
\cdot \\
\cdot \\
\cdot \\
\tau_c
\end{bmatrix}
+ \boldsymbol \varepsilon = \mathbf{X \boldsymbol \beta + \boldsymbol \varepsilon}.
\]</span>
Thus, the <span class="math inline">\(\mathbf{X}^{\prime} \mathbf{X}\)</span> matrix is
<span class="math display">\[
\mathbf{X}^{\prime} \mathbf{X} =
\begin{bmatrix}
n &amp; n_1 &amp; n_2 &amp; \cdots &amp; n_c \\
n_1 &amp; n_1 &amp; 0 &amp; \cdots &amp; 0 \\
n_2 &amp; 0 &amp; n_2 &amp; \cdots &amp; 0 \\
\cdot &amp; \cdot &amp; \cdot &amp; \cdots &amp; \cdot \\
\cdot &amp; \cdot &amp; \cdot &amp; \cdots &amp; \cdot  \\
\cdot &amp; \cdot &amp; \cdot &amp; \cdots &amp; \cdot \\
n_c &amp; 0 &amp; 0 &amp; \cdots &amp; n_c
\end{bmatrix}.
\]</span></p>
<p>where <span class="math inline">\(n = n_1 + n_2 + \ldots + n_{c}\)</span>. This matrix is not invertible. To see this, note that by adding the last <span class="math inline">\(c\)</span> rows together yields the first row. Thus, the last <span class="math inline">\(c\)</span> rows are an exact linear combination of the first row, meaning that the matrix is not full rank.</p>
<p>The (non-unique) least squares estimates can be expressed as
<span class="math display">\[
\mathbf{b}^{\circ} =
\begin{bmatrix}
\mu^{\circ} \\
\tau_1^{\circ} \\
\cdot \\
\cdot \\
\cdot \\
\tau_c^{\circ}
\end{bmatrix}
= (\mathbf{X}^{\prime} \mathbf{X})^{-} \mathbf{X}^{\prime} \mathbf{y}.
\]</span>
Estimable functions are linear combinations of fitted values. Because fitted values are <span class="math inline">\(\hat{y}_{ij} = \bar{y}_j\)</span>, estimable functions can be expressed as <span class="math inline">\(L = \sum_{j=1}^{c} a_j \bar{y}_j\)</span> where <span class="math inline">\(a_1, \ldots, a_{c}\)</span> are constants. This linear combination of fitted values is an unbiased estimator of <span class="math inline">\(\text{E } L = \sum_{i=1}^{c} a_i (\mu + \tau_i)\)</span>.</p>
<p>Thus, for example, by choosing <span class="math inline">\(a_1 = 1\)</span> and the other <span class="math inline">\(a_i = 0\)</span>, we see that <span class="math inline">\(\mu + \tau_1\)</span> is estimable. As another example, by choosing <span class="math inline">\(a_1 = 1\)</span>, <span class="math inline">\(a_2 = -1\)</span>, and the other <span class="math inline">\(a_i = 0\)</span>, we see that <span class="math inline">\(\tau_1 - \tau_2\)</span> is estimable. It can be shown that <span class="math inline">\(\mu\)</span> is not an estimable parameter without further restrictions on <span class="math inline">\(\tau_1, \ldots, \tau_c\)</span>.</p>

<!-- # Chap 1 -->
<!-- # Chap 2 -->
<!-- # Chap 3 -->
<!-- # Chap 4 -->
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="C3BasicMLR.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="C5VarSelect.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
