<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Basic Statistical Inference | Regression Modeling with Actuarial and Financial Applications</title>
  <meta name="description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Basic Statistical Inference | Regression Modeling with Actuarial and Financial Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Basic Statistical Inference | Regression Modeling with Actuarial and Financial Applications" />
  
  <meta name="twitter:description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  

<meta name="author" content="Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="preface.html"/>
<link rel="next" href="matrix-algebra.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript"  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleSolution(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}      
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>




<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Regression Modeling With Actuarial and Financial Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#forward"><i class="fa fa-check"></i>Forward</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#who-is-this-book-for"><i class="fa fa-check"></i>Who Is This Book For?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#what-is-this-book-about"><i class="fa fa-check"></i>What Is This Book About?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#how-does-this-book-deliver-its-message"><i class="fa fa-check"></i>How Does This Book Deliver Its Message?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html"><i class="fa fa-check"></i><b>1</b> Basic Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html#distributions-of-functions-of-random-variables"><i class="fa fa-check"></i><b>1.1</b> Distributions of Functions of Random Variables</a></li>
<li class="chapter" data-level="1.2" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html#estimation-and-prediction"><i class="fa fa-check"></i><b>1.2</b> Estimation and Prediction</a></li>
<li class="chapter" data-level="1.3" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html#testing-hypotheses"><i class="fa fa-check"></i><b>1.3</b> Testing Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="matrix-algebra.html"><a href="matrix-algebra.html"><i class="fa fa-check"></i><b>2</b> Matrix Algebra</a>
<ul>
<li class="chapter" data-level="2.1" data-path="matrix-algebra.html"><a href="matrix-algebra.html#basic-definitions"><i class="fa fa-check"></i><b>2.1</b> Basic Definitions</a></li>
<li class="chapter" data-level="2.2" data-path="matrix-algebra.html"><a href="matrix-algebra.html#review-of-basic-operations"><i class="fa fa-check"></i><b>2.2</b> Review of Basic Operations</a></li>
<li class="chapter" data-level="2.3" data-path="matrix-algebra.html"><a href="matrix-algebra.html#further-definitions"><i class="fa fa-check"></i><b>2.3</b> Further Definitions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability-tables.html"><a href="probability-tables.html"><i class="fa fa-check"></i><b>3</b> Probability Tables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability-tables.html"><a href="probability-tables.html#normal-distribution"><i class="fa fa-check"></i><b>3.1</b> Normal Distribution</a></li>
<li class="chapter" data-level="3.2" data-path="probability-tables.html"><a href="probability-tables.html#chi-square-distribution"><i class="fa fa-check"></i><b>3.2</b> Chi-Square Distribution</a></li>
<li class="chapter" data-level="3.3" data-path="probability-tables.html"><a href="probability-tables.html#t-distribution"><i class="fa fa-check"></i><b>3.3</b> <em>t</em>-Distribution</a></li>
<li class="chapter" data-level="3.4" data-path="probability-tables.html"><a href="probability-tables.html#f-distribution"><i class="fa fa-check"></i><b>3.4</b> <em>F</em>-Distribution</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTextDev/RegressionSpanish/" target="blank">Spanish Regression on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Regression Modeling with Actuarial and Financial Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="basic-statistical-inference" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Basic Statistical Inference<a href="basic-statistical-inference.html#basic-statistical-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Appendix Preview.</em> This appendix provides
definitions and facts from a course in basic statistical inference
that are needed in one’s study of regression analysis.</p>
<div id="distributions-of-functions-of-random-variables" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Distributions of Functions of Random Variables<a href="basic-statistical-inference.html#distributions-of-functions-of-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Statistics and Sampling Distributions.</strong> A <em>statistic</em> summarizes information in a sample and hence is a function of observations <span class="math inline">\(y_1,\ldots,y_n\)</span>. Because observations are realizations of random variables, the study of distributions of functions of random variables is really the study of the distributions of statistics, known as <em>sampling distributions</em>. Linear combinations of the form <span class="math inline">\(\sum_{i=1}^n a_i y_i\)</span> represent an important type of function. Here, <span class="math inline">\(a_1,\ldots,a_n\)</span> are known constants. To begin, we suppose that <span class="math inline">\(y_1,\ldots,y_n\)</span> are mutually independent random variables with <span class="math inline">\(\mathrm{E~}y_i = \mu_i\)</span> and <span class="math inline">\(\mathrm{ Var~}y_i = \sigma_i^2\)</span>. Then, by the <em>linearity of expectations</em>, we have
<span class="math display">\[
\mathrm{E}\left( \sum_{i=1}^n a_i y_i \right) = \sum_{i=1}^n a_i \mu_i~~~\mathrm{and}~~~\mathrm{Var}\left( \sum_{i=1}^n a_i y_i \right) = \sum_{i=1}^n a_i^2 \sigma_i^2.
\]</span>
An important theorem in mathematical statistics is that, if each random variable is normally distributed, then linear combinations are also normally distributed. That is, we have:</p>
<p><strong>Linearity of Normal Random Variables.</strong> Suppose that <span class="math inline">\(y_1,\ldots,y_n\)</span> are mutually independent random variables with <span class="math inline">\(y_i \sim N(\mu_i,\sigma_i^2)\)</span>. (Read ” <span class="math inline">\(\sim\)</span> ” to mean “is distributed as.”) Then,
<span class="math display">\[
\sum_{i=1}^n a_i y_i \sim N\left( \sum_{i=1}^n a_i \mu_i, \sum_{i=1}^n a_i^2 \sigma_i^2 \right) .
\]</span>
There are several applications of this important property. First, it can be checked that if <span class="math inline">\(y \sim N(\mu ,\sigma^2)\)</span>, then <span class="math inline">\((y - \mu)/\sigma \sim N(0,1)\)</span>. Second, assume that <span class="math inline">\(y_1,\ldots,y_n\)</span> are identically and independently distributed (<em>i.i.d.</em>) as <span class="math inline">\(N(\mu, \sigma^2)\)</span> and take <span class="math inline">\(a_i = n^{-1}\)</span>. Then, we have
<span class="math display">\[
\overline{y} = \frac{1}{n}\sum_{i=1}^n y_i \sim N\left( \mu ,\frac{\sigma^2}{n}\right) .
\]</span>
Equivalently, <span class="math inline">\(\sqrt{n}\left( \overline{y}-\mu \right) /\sigma\)</span> is standard normal.</p>
<p>Thus, the important sample statistic <span class="math inline">\(\overline{y}\)</span> has a normal distribution. Further, the distribution of the sample variance <span class="math inline">\(s_y^2\)</span> can also be calculated. For <span class="math inline">\(y_1,\ldots,y_n\)</span> are i.i.d. <span class="math inline">\(N(\mu ,\sigma^2)\)</span>, we have that <span class="math inline">\(\left( n-1\right) s_y^2 /\sigma^2\sim \chi_{n-1}^2\)</span>, a <span class="math inline">\(\chi^2\)</span> (chi-square) distribution with <span class="math inline">\(n-1\)</span> degrees of freedom. Further, <span class="math inline">\(\overline{y}\)</span> is independent of <span class="math inline">\(s_y^2\)</span>. From these two results, we have that
<span class="math display">\[
\frac{\sqrt{n}}{s_y}\left( \overline{y}-\mu \right) \sim t_{n-1},
\]</span>
a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.</p>
</div>
<div id="estimation-and-prediction" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Estimation and Prediction<a href="basic-statistical-inference.html#estimation-and-prediction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose that <span class="math inline">\(y_1,\ldots,y_n\)</span> are i.i.d. random variables from a distribution that can be summarized by an unknown parameter <span class="math inline">\(\theta\)</span>. We are interested in the quality of an estimate of <span class="math inline">\(\theta\)</span> and denote <span class="math inline">\(\widehat{\theta}\)</span> as this estimator. For example, we consider <span class="math inline">\(\theta = \mu\)</span> with <span class="math inline">\(\widehat{\theta} = \overline{y}\)</span> and <span class="math inline">\(\theta = \sigma^2\)</span> with <span class="math inline">\(\widehat{\theta} = s_y^2\)</span> as our leading examples.</p>
<p><strong>Point Estimation and Unbiasedness.</strong> Because <span class="math inline">\(\widehat{\theta}\)</span> provides a (single) approximation of <span class="math inline">\(\theta\)</span>, it is referred to as a <em>point estimate</em> of <span class="math inline">\(\theta\)</span>. As a statistic, <span class="math inline">\(\widehat{\theta}\)</span> is a function of the observations <span class="math inline">\(y_1,\ldots,y_n\)</span> that varies from one sample to the next. Thus, values of <span class="math inline">\(\widehat{\theta}\)</span> vary from one sample to the next. To examine how close <span class="math inline">\(\widehat{\theta}\)</span> tends to be to <span class="math inline">\(\theta\)</span>, we examine several properties of <span class="math inline">\(\widehat{\theta}\)</span>, in particular, the <em>bias</em> and <em>consistency</em>. A point estimator <span class="math inline">\(\widehat{\theta}\)</span> is said to be an <em>unbiased estimator</em> of <span class="math inline">\(\theta\)</span> if <span class="math inline">\(\mathrm{E~}\widehat{\theta} = \theta\)</span>. For example, since <span class="math inline">\(\mathrm{E~}\overline{y} = \mu\)</span>, <span class="math inline">\(\overline{y}\)</span> is an unbiased estimator of <span class="math inline">\(\mu\)</span>.</p>
<p><strong>Finite Sample versus Large Sample Properties of Estimators.</strong> Biasedness is said to be a <em>finite sample</em> property since it is valid for each sample size <span class="math inline">\(n\)</span>. A <em>limiting</em>, or <em>large sample</em> property is <em>consistency</em>. Consistency is expressed in two ways, <em>weak</em> and <em>strong</em> consistency. An estimator is said to be <em>weakly consistent</em> if
<span class="math display">\[
\lim_{n\rightarrow \infty }\Pr \left( |\widehat{\theta }-\theta |&lt;h\right) = 1,
\]</span>
for each positive <span class="math inline">\(h\)</span>. An estimator is said to be <em>strongly consistent</em> if <span class="math inline">\(\lim_{n\rightarrow \infty }~\widehat{\theta }=\theta\)</span>, with probability one.</p>
<p><strong>Least Squares Estimation Principle.</strong> In this text, two main estimation principles are used, <em>least squares</em> estimation and <em>maximum likelihood</em> estimation. For the least squares procedure, consider independent random variables <span class="math inline">\(y_1,\ldots,y_n\)</span> with means <span class="math inline">\(\mathrm{E~}y_i = \mathrm{g}_i(\theta )\)</span>. Here, <span class="math inline">\(\mathrm{g}_i(.)\)</span> is a known function up to <span class="math inline">\(\theta\)</span>, the unknown parameter. The least squares estimator is that value of <span class="math inline">\(\theta\)</span> that minimizes the sum of squares
<span class="math display">\[
\mathrm{SS}(\theta )=\sum_{i=1}^n\left( y_i-\mathrm{g}_i(\theta )\right)^2.
\]</span></p>
<p><strong>Maximum Likelihood Estimation Principle.</strong> Maximum likelihood estimates are values of the parameter that are “most likely” to have been produced by the data. Consider the independent random variables <span class="math inline">\(y_1,\ldots,y_n\)</span> with probability function <span class="math inline">\(\mathrm{f}_i(a_i,\theta )\)</span>. Here, <span class="math inline">\(\mathrm{f}_i(a_i,\theta )\)</span> is interpreted to be a probability mass function for discrete <span class="math inline">\(y_i\)</span> or a probability density function for continuous <span class="math inline">\(y_i\)</span>, evaluated at <span class="math inline">\(a_i\)</span>, the realization of <span class="math inline">\(y_i\)</span>. The function <span class="math inline">\(\mathrm{f}_i(a_i,\theta )\)</span> is assumed known up to <span class="math inline">\(\theta\)</span>, the unknown parameter. The likelihood of the random variables <span class="math inline">\(y_1,\ldots,y_n\)</span> taking on values <span class="math inline">\(a_1,\ldots,a_n\)</span> is
<span class="math display">\[
\mathrm{L}(\theta )=\prod\limits_{i=1}^n \mathrm{f}_i(a_i,\theta ).
\]</span></p>
<p>The value of <span class="math inline">\(\theta\)</span> that maximizes <span class="math inline">\(\mathrm{L}(\theta )\)</span> is called the <em>maximum likelihood estimator</em>.</p>
<p><strong>Confidence Intervals.</strong> Although point estimates provide a single approximation to parameters, <em>interval estimates</em> provide a range that includes parameters with a certain prespecified level of probability, or <em>confidence</em>. A pair of statistics, <span class="math inline">\(\widehat{\theta }_1\)</span> and <span class="math inline">\(\widehat{\theta }_{2}\)</span>, provide an interval of the form <span class="math inline">\(\left[ \widehat{\theta }_1 &lt; \widehat{\theta }_{2}\right]\)</span>. This interval is a <span class="math inline">\(100(1-\alpha )\%\)</span> confidence interval for <span class="math inline">\(\theta\)</span> if
<span class="math display">\[
\Pr \left( \widehat{\theta }_1 &lt; \theta &lt; \widehat{\theta }_{2}\right) \geq 1-\alpha .
\]</span>
For example, suppose that <span class="math inline">\(y_1,\ldots,y_n\)</span> are i.i.d. <span class="math inline">\(N(\mu ,\sigma^2)\)</span> random variables. Recall that <span class="math inline">\(\sqrt{n}\left( \overline{y}-\mu\right) /s_y\sim t_{n-1}\)</span>. This fact allows us to develop a <span class="math inline">\(100(1-\alpha )\%\)</span> confidence interval for <span class="math inline">\(\mu\)</span> of the form <span class="math inline">\(\overline{y}\pm (t-value)s_y/ \sqrt{n}\)</span>, where <span class="math inline">\(t-value\)</span> is the <span class="math inline">\((1-\alpha /2)^{th}\)</span> percentile from a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.</p>
<p><strong>Prediction Intervals.</strong> Prediction intervals have the same form as confidence intervals. However, a confidence interval provides a range for a parameter whereas a prediction interval provides a range for external values of the observations. Based on observations <span class="math inline">\(y_1,\ldots,y_n\)</span>, we seek to construct statistics <span class="math inline">\(\widehat{\theta }_1\)</span> and <span class="math inline">\(\widehat{\theta }_{2}\)</span> such that
<span class="math display">\[
\Pr \left( \widehat{\theta }_1 &lt; y^{\ast } &lt; \widehat{\theta }_{2}\right) \geq 1-\alpha .
\]</span>
Here, <span class="math inline">\(y^{\ast }\)</span> is an additional observation that is not a part of the sample.</p>
</div>
<div id="testing-hypotheses" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Testing Hypotheses<a href="basic-statistical-inference.html#testing-hypotheses" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Null and Alternative Hypotheses and Test Statistics.</strong> An important statistical procedure involves verifying ideas about parameters. That is, before the data are observed, certain ideas about the parameters are formulated. In this text, we consider a <em>null hypothesis</em> of the form
<span class="math inline">\(H_0:\theta =\theta_0\)</span> versus an <em>alternative hypothesis</em>. We consider both a <em>two-sided alternative</em>, <span class="math inline">\(H_{a}:\theta \neq \theta_0\)</span>, and <em>one-sided alternatives</em>, either <span class="math inline">\(H_{a}:\theta &gt;\theta_0\)</span> or <span class="math inline">\(H_{a}:\theta &lt;\theta_0\)</span>. To choose between these competing hypotheses, we use a test statistic <span class="math inline">\(T_n\)</span> that is typically a point estimate of <span class="math inline">\(\theta\)</span> or a version that is rescaled to conform to a reference distribution under <span class="math inline">\(H_0\)</span>. For example, to test <span class="math inline">\(H_0:\mu =\mu_0\)</span>, we often use <span class="math inline">\(T_n= \overline{y}\)</span> or <span class="math inline">\(T_n=\sqrt{n}\left( \overline{y}-\mu_0\right) /s_y\)</span>. Note that the latter choice has a <span class="math inline">\(t_{n-1}\)</span> distribution, under the assumptions of i.i.d. normal data.</p>
<p><strong>Rejection Regions and Significance Level.</strong> With a statistic in hand, we now establish a criterion for deciding between the two competing hypotheses. This can be done by establishing a <em>rejection</em>, or <em>critical, region</em>. The critical region consists of all possible outcomes of <span class="math inline">\(T_n\)</span> that leads us to reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_{a}\)</span>. In order to specify the critical region, we first quantify the types of errors that can be made in the decision-making procedure. A <em>Type I error</em> consists of rejecting <span class="math inline">\(H_0\)</span> falsely and a <em>Type II error</em> consists of rejecting <span class="math inline">\(H_{a}\)</span> falsely. The probability of a Type I error is called the <em>significance level</em>. Prespecifying the significance level is often enough to determine the critical region. For example, suppose that <span class="math inline">\(y_1,\ldots,y_n\)</span> are i.i.d. <span class="math inline">\(N(\mu ,\sigma^2)\)</span> and we are interested in deciding between <span class="math inline">\(H_0:\mu =\mu_0\)</span> and <span class="math inline">\(H_{a}:\mu &gt; \mu_0\)</span>. Thinking of our test statistic <span class="math inline">\(T_n=\overline{y}\)</span>, we know that we would like to reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\overline{y}\)</span> is larger than <span class="math inline">\(\mu_0\)</span>. The question is how much larger? Specifying a significance level <span class="math inline">\(\alpha\)</span>, we wish to find a critical region of the form <span class="math inline">\(\{\overline{y}&gt;c\}\)</span> for some constant <span class="math inline">\(c\)</span>. To this end, we have
<span class="math display">\[
\begin{array}{ll}
\alpha  &amp;= \Pr \mathrm{(Type~I~error)} = \Pr (\mathrm{Reject~}H_0 \mathrm{~assuming~} H_0:\mu =\mu_0 \mathrm{~is~true)} \\
&amp; = \Pr (\overline{y}&gt;c) = \Pr \left(\sqrt{n}\left( \overline{y}-\mu_0\right)/s_y&gt;\sqrt{n}\left( c-\mu_0 \right)/s_y\right) \\
&amp;= \Pr \left(t_{n-1}&gt;\sqrt{n}\left( c-\mu_0 \right)/s_y\right).
\end{array}
\]</span></p>
<p>With <span class="math inline">\(df=n-1\)</span> degrees of freedom, we have that <span class="math inline">\(t-value = \sqrt{n}\left( c-\mu_0\right)/s_y\)</span> where the <span class="math inline">\(t-value\)</span> is the <span class="math inline">\((1-\alpha)^{th}\)</span> percentile from a <span class="math inline">\(t\)</span>-distribution. Thus, solving for <span class="math inline">\(c\)</span>, our critical region is of the form <span class="math inline">\(\{\overline{y} &gt; \mu_0 + (t-value)/s_y/\sqrt{n}\}\)</span>.</p>
<p><strong>Relationship between Confidence Intervals and Hypothesis Tests.</strong> Similar calculations show, for testing <span class="math inline">\(H_0:\mu = \mu_0\)</span> versus <span class="math inline">\(H_{a}:\theta \neq \theta_0\)</span>, that the critical region is of the form
<span class="math display">\[
\{ \overline{y} &gt; \mu_0 + (t-value)/s_y/\sqrt{n} ~\mathrm{or~} \overline{y} &gt; \mu_0 + (t-value)/s_y/\sqrt{n}\} .
\]</span>
Here, the <span class="math inline">\(t\)</span>-value is a <span class="math inline">\((1-\alpha /2)^{th}\)</span> percentile from a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df=n-1\)</span> degrees of freedom. It is interesting to note that the event of falling in this two-sided critical region is equivalent to the event of <span class="math inline">\(\mu_0\)</span> falling outside the confidence interval <span class="math inline">\(\overline{y}\pm (t-value)s_y/\sqrt{n}\)</span>. This establishes the fact that confidence intervals and hypothesis tests are really reporting the same evidence with different emphasis on interpretation of the statistical inference.</p>
<p><strong><span class="math inline">\(p\)</span>-value.</strong> Another useful concept in hypothesis testing is the <span class="math inline">\(p\)</span>-value, which is short for <em>probability value</em>. For a data set, a <span class="math inline">\(p\)</span>-value is defined to be the smallest significance level for which the null hypothesis would be rejected. The <span class="math inline">\(p\)</span>-value is a useful summary statistic for the data analyst to report since it allows the reader to understand the strength of the deviation from the null hypothesis.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="preface.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="matrix-algebra.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
