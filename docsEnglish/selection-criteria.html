<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Selection Criteria | Regression Modeling with Actuarial and Financial Applications</title>
  <meta name="description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Selection Criteria | Regression Modeling with Actuarial and Financial Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Selection Criteria | Regression Modeling with Actuarial and Financial Applications" />
  
  <meta name="twitter:description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  

<meta name="author" content="Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="C5VarSelect.html"/>
<link rel="next" href="bibliography.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript"  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleSolution(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}      
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>




<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Regression Modeling With Actuarial and Financial Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#forward"><i class="fa fa-check"></i>Forward</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#who-is-this-book-for"><i class="fa fa-check"></i>Who Is This Book For?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#what-is-this-book-about"><i class="fa fa-check"></i>What Is This Book About?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#how-does-this-book-deliver-its-message"><i class="fa fa-check"></i>How Does This Book Deliver Its Message?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="chap-1.html"><a href="chap-1.html"><i class="fa fa-check"></i><b>1</b> Chap 1</a></li>
<li class="chapter" data-level="2" data-path="chap-2.html"><a href="chap-2.html"><i class="fa fa-check"></i><b>2</b> Chap 2</a></li>
<li class="chapter" data-level="3" data-path="chap-3.html"><a href="chap-3.html"><i class="fa fa-check"></i><b>3</b> Chap 3</a></li>
<li class="chapter" data-level="4" data-path="chap-4.html"><a href="chap-4.html"><i class="fa fa-check"></i><b>4</b> Chap 4</a></li>
<li class="chapter" data-level="5" data-path="C5VarSelect.html"><a href="C5VarSelect.html"><i class="fa fa-check"></i><b>5</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="5.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#S5:Iterative"><i class="fa fa-check"></i><b>5.1</b> An Iterative Approach to Data Analysis and Modeling</a></li>
<li class="chapter" data-level="5.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#S5:Automatic"><i class="fa fa-check"></i><b>5.2</b> Automatic Variable Selection Procedures</a></li>
<li class="chapter" data-level="5.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#S5:ResidualAnalysis"><i class="fa fa-check"></i><b>5.3</b> Residual Analysis</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#S5:Residuals"><i class="fa fa-check"></i><b>5.3.1</b> Residuals</a></li>
<li class="chapter" data-level="5.3.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#S5:ResidualsOutliers"><i class="fa fa-check"></i><b>5.3.2</b> Using Residuals to Identify Outliers</a></li>
<li class="chapter" data-level="5.3.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#S5:ResidualsExplanatory"><i class="fa fa-check"></i><b>5.3.3</b> Using Residuals to Select Explanatory Variables</a></li>
<li class="chapter" data-level="5.3.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#understanding-the-liquidity-measure-volume"><i class="fa fa-check"></i><b>5.3.4</b> Understanding the Liquidity Measure VOLUME</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#influential-points"><i class="fa fa-check"></i><b>5.4</b> Influential Points</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#S5:Leverage"><i class="fa fa-check"></i><b>5.4.1</b> Leverage</a></li>
<li class="chapter" data-level="5.4.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#S5:CooksDistance"><i class="fa fa-check"></i><b>5.4.2</b> Cook’s Distance</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="C5VarSelect.html"><a href="C5VarSelect.html#S5:Collinearity"><i class="fa fa-check"></i><b>5.5</b> Collinearity</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#what-is-collinearity"><i class="fa fa-check"></i><b>5.5.1</b> What is Collinearity?</a></li>
<li class="chapter" data-level="5.5.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#S5:VIF"><i class="fa fa-check"></i><b>5.5.2</b> Variance Inflation Factors</a></li>
<li class="chapter" data-level="5.5.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#collinearity-and-leverage"><i class="fa fa-check"></i><b>5.5.3</b> Collinearity and Leverage</a></li>
<li class="chapter" data-level="5.5.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#suppressor-variables"><i class="fa fa-check"></i><b>5.5.4</b> Suppressor Variables</a></li>
<li class="chapter" data-level="5.5.5" data-path="C5VarSelect.html"><a href="C5VarSelect.html#orthogonal-variables"><i class="fa fa-check"></i><b>5.5.5</b> Orthogonal Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="selection-criteria.html"><a href="selection-criteria.html"><i class="fa fa-check"></i><b>6</b> Selection Criteria</a>
<ul>
<li class="chapter" data-level="6.1" data-path="selection-criteria.html"><a href="selection-criteria.html#goodness-of-fit"><i class="fa fa-check"></i><b>6.1</b> Goodness of Fit</a></li>
<li class="chapter" data-level="6.2" data-path="selection-criteria.html"><a href="selection-criteria.html#model-validation"><i class="fa fa-check"></i><b>6.2</b> Model Validation</a></li>
<li class="chapter" data-level="6.3" data-path="selection-criteria.html"><a href="selection-criteria.html#out-of-sample-validation-procedure"><i class="fa fa-check"></i><b>6.3</b> Out-of-Sample Validation Procedure</a></li>
<li class="chapter" data-level="6.4" data-path="selection-criteria.html"><a href="selection-criteria.html#cross-validation"><i class="fa fa-check"></i><b>6.4</b> Cross-Validation</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="selection-criteria.html"><a href="selection-criteria.html#press-validation-procedure"><i class="fa fa-check"></i><b>6.4.1</b> PRESS Validation Procedure</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="selection-criteria.html"><a href="selection-criteria.html#heteroscedasticity"><i class="fa fa-check"></i><b>6.5</b> Heteroscedasticity</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="selection-criteria.html"><a href="selection-criteria.html#detecting-heteroscedasticity"><i class="fa fa-check"></i><b>6.5.1</b> Detecting Heteroscedasticity</a></li>
<li class="chapter" data-level="6.5.2" data-path="selection-criteria.html"><a href="selection-criteria.html#heteroscedasticity-consistent-standard-errors"><i class="fa fa-check"></i><b>6.5.2</b> Heteroscedasticity-Consistent Standard Errors</a></li>
<li class="chapter" data-level="6.5.3" data-path="selection-criteria.html"><a href="selection-criteria.html#weighted-least-squares"><i class="fa fa-check"></i><b>6.5.3</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="6.5.4" data-path="selection-criteria.html"><a href="selection-criteria.html#transformations"><i class="fa fa-check"></i><b>6.5.4</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="selection-criteria.html"><a href="selection-criteria.html#further-reading-and-references"><i class="fa fa-check"></i><b>6.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="6.7" data-path="selection-criteria.html"><a href="selection-criteria.html#chapter-references"><i class="fa fa-check"></i><b>6.7</b> Chapter References</a></li>
<li class="chapter" data-level="6.8" data-path="selection-criteria.html"><a href="selection-criteria.html#exercises"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
<li class="chapter" data-level="6.9" data-path="selection-criteria.html"><a href="selection-criteria.html#S5:TechSupps"><i class="fa fa-check"></i><b>6.9</b> Technical Supplements for Chapter 5</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="selection-criteria.html"><a href="selection-criteria.html#S5:ProjMatrix"><i class="fa fa-check"></i><b>6.9.1</b> Projection Matrix</a></li>
<li class="chapter" data-level="6.9.2" data-path="selection-criteria.html"><a href="selection-criteria.html#S5:LOOStatistics"><i class="fa fa-check"></i><b>6.9.2</b> Leave One Out Statistics</a></li>
<li class="chapter" data-level="6.9.3" data-path="selection-criteria.html"><a href="selection-criteria.html#omitting-variables"><i class="fa fa-check"></i><b>6.9.3</b> Omitting Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTextDev/RegressionSpanish/" target="blank">Spanish Regression on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Regression Modeling with Actuarial and Financial Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="selection-criteria" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Selection Criteria<a href="selection-criteria.html#selection-criteria" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="goodness-of-fit" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Goodness of Fit<a href="selection-criteria.html#goodness-of-fit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>How well does the model fit the data? Criteria that measure the proximity of the fitted model and realized data are known as <em>goodness of fit</em> statistics. Specifically, we interpret the fitted value <span class="math inline">\(\hat{y}_i\)</span> to be the best model approximation of the <span class="math inline">\(i\)</span>th observation and compare it to the actual value <span class="math inline">\(y_i\)</span>. In linear regression, we examine the difference through the residual <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span>; small residuals imply a good model fit. We have quantified this through the size of the typical error <span class="math inline">\((s)\)</span>, including the coefficient of determination <span class="math inline">\((R^2)\)</span> and an adjusted version <span class="math inline">\((R_{a}^2)\)</span>.</p>
<p>For nonlinear models, we will need additional measures, and it is helpful to introduce these measures in this simpler linear case. One such measure is <em>Akaike’s Information Criterion</em> that will be defined in terms of likelihood fits in Section 11.9.4. For linear regression, it reduces to</p>
<p><span class="math display">\[
AIC = n \ln (s^2) + n \ln (2 \pi) + n + 3 + k.
\]</span></p>
<p>For model comparison, the smaller the <span class="math inline">\(AIC\)</span>, the better is the fit. Comparing models with the same number of variables (<span class="math inline">\(k\)</span>) means that selecting a model with small values of <span class="math inline">\(AIC\)</span> leads to the same choice as selecting a model with small values of the residual standard deviation <span class="math inline">\(s\)</span>. Further, a small number of parameters means a small value of <span class="math inline">\(AIC\)</span>, other things being equal. The idea is that this measure balances the fit (<span class="math inline">\(n \ln (s^2)\)</span>) with a penalty for complexity (the number of parameters, <span class="math inline">\(k+2\)</span>). Statistical packages often omit constants such as <span class="math inline">\(n \ln (2 \pi)\)</span> and <span class="math inline">\(n+3\)</span> when reporting <span class="math inline">\(AIC\)</span> because they do not matter when comparing models.</p>
<p>Section 11.9.4 will introduce another measure, the Bayes Information Criterion (<span class="math inline">\(BIC\)</span>), that gives a smaller weight to the penalty for complexity. A third goodness of fit measure that is used in linear regression models is the <span class="math inline">\(C_p\)</span> statistic. To define this statistic, assume that we have available <span class="math inline">\(k\)</span> explanatory variables <span class="math inline">\(x_1, ..., x_{k}\)</span> and run a regression to get <span class="math inline">\(s_{full}^2\)</span> as the mean square error. Now, suppose that we are considering using only <span class="math inline">\(p-1\)</span> explanatory variables so that there are <span class="math inline">\(p\)</span> regression coefficients. With these <span class="math inline">\(p-1\)</span> explanatory variables, we run a regression to get the error sum of squares <span class="math inline">\((Error~SS)_p\)</span>. Thus, we are in the position to define</p>
<p><span class="math display">\[
C_{p} = \frac{(Error~SS)_p}{s_{full}^2} - n + 2p.
\]</span></p>
<p>As a selection criterion, we choose the model with a “small” <span class="math inline">\(C_{p}\)</span> coefficient, where small is taken to be relative to <span class="math inline">\(p\)</span>. In general, models with smaller values of <span class="math inline">\(C_{p}\)</span> are more desirable.</p>
<p>Like the <span class="math inline">\(AIC\)</span> and <span class="math inline">\(BIC\)</span> statistics, the <span class="math inline">\(C_{p}\)</span> statistic strikes a balance between the model fit and complexity. That is, each statistic summarizes the trade-off between model fit and complexity, although with different weights. For most data sets, they recommend the same model and so an analyst can report any or all three statistics. However, for some applications, they lead to different recommended models. In this case, the analyst needs to rely more heavily on non-data driven criteria for model selection (which are always important in any regression application).</p>
</div>
<div id="model-validation" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Model Validation<a href="selection-criteria.html#model-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Model validation is the process of confirming that our proposed model is appropriate, especially in light of the purposes of the investigation. Recall the iterative model formulation selection process described in Section <span class="math inline">\(\ref{S5:Iterative}\)</span>. An important criticism of this iterative process is that it is guilty of <em>data-snooping</em>, that is, fitting a great number of models to a single set of data. As we saw in Section <span class="math inline">\(\ref{S5:Automatic}\)</span> on data-snooping in stepwise regression, by looking at a large number of models we may overfit the data and understate the natural variation in our representation.</p>
<p>We can respond to this criticism by using a technique called <em>out-of-sample validation</em>. The ideal situation is to have available two sets of data, one for model development and one for model validation. We initially develop one, or several, models on a first data set. The models developed from the first set of data are called our <em>candidate</em> models. Then, the relative performance of the candidate models could be measured on a second set of data. In this way, the data used to validate the model is unaffected by the procedures used to formulate the model.</p>
<p>Unfortunately, rarely will two sets of data be available to the investigator. However, we can implement the validation process by splitting the data set into two subsamples. We call these the <em>model development</em> and <em>validation subsamples</em>, respectively. They are also known as <em>training</em> and <em>testing</em> samples, respectively. To see how the process works in the linear regression context, consider the following procedure.</p>
</div>
<div id="out-of-sample-validation-procedure" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Out-of-Sample Validation Procedure<a href="selection-criteria.html#out-of-sample-validation-procedure" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>Begin with a sample size of <span class="math inline">\(n\)</span> and divide it into two subsamples, called the model development and validation subsamples. Let <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> denote the size of each subsample. In cross-sectional regression, do this split using a random sampling mechanism. Use the notation <span class="math inline">\(i=1,...,n_1\)</span> to represent observations from the model development subsample and <span class="math inline">\(i=n_1+1,...,n_1+n_2=n\)</span> for the observations from the validation subsample. Figure <span class="math inline">\(\ref{F5:ModelValidation}\)</span> illustrates this procedure.</p></li>
<li><p>Using the model development subsample, fit a candidate model to the data set <span class="math inline">\(i=1,...,n_1\)</span>.</p></li>
<li><p>Using the model created in Step (ii) and the explanatory variables from the validation subsample, “predict” the dependent variables in the validation subsample, <span class="math inline">\(\hat{y}_i\)</span>, where <span class="math inline">\(i=n_1+1,...,n_1+n_2\)</span>. (To get these predictions, you may need to transform the dependent variables back to the original scale.)</p></li>
<li><p>Assess the proximity of the predictions to the held-out data. One measure is the <em>sum of squared prediction errors</em></p></li>
</ol>
<p><span class="math display">\[
SSPE = \sum_{i=n_1+1}^{n_1+n_2} (y_i - \hat{y}_i)^2
\]</span></p>
<p>Repeat Steps (ii) through (iv) for each candidate model. Choose the model with the smallest <em>SSPE</em>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig59"></span>
<img src="RegressionMarkdown_files/figure-html/Fig59-1.png" alt="For model validation, a data set of size \(n\) is randomly split into two subsamples" width="100%" />
<p class="caption">
Figure 6.1: <strong>For model validation, a data set of size <span class="math inline">\(n\)</span> is randomly split into two subsamples</strong>
</p>
</div>
<p>There are a number of criticisms of the <em>SSPE</em>. First, it is clear that it takes a considerable amount of time and effort to calculate this statistic for each of several candidate models. However, as with many statistical techniques, this is merely a matter of having specialized statistical software available to perform the steps described above. Second, because the statistic itself is based on a random subset of the sample, its value will vary from analyst to analyst. This objection could be overcome by using the first <span class="math inline">\(n_1\)</span> observations from the sample. In most applications, this is not done in case there is a lurking relationship in the order of the observations. Third, and perhaps most important, is the fact that the choice of the relative subset sizes, <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>, is not clear. Various researchers recommend different proportions for the allocation. Snee (1977) suggests that data-splitting not be done unless the sample size is moderately large, specifically, <span class="math inline">\(n \geq 2(k+1) + 20\)</span>. The guidelines of Picard and Berk (1990) show that the greater the number of parameters to be estimated, the greater the proportion of observations needed for the model development subsample. As a rule of thumb, for data sets with 100 or fewer observations, use about 25-35% of the sample for out-of-sample validation. For data sets with 500 or more observations, use 50% of the sample for out-of-sample validation. Hastie, Tibshirani, and Friedman (2001) remark that a typical split is 50% for development/training, 25% for validation, and the remaining 25% for a third stage for further validation that they call <em>testing</em>.</p>
<p>Because of these criticisms, several variants of the basic out-of-sample validation process are used by analysts. Although there is no theoretically best procedure, it is widely agreed that model validation is an important part of confirming the usefulness of a model.</p>
</div>
<div id="cross-validation" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Cross-Validation<a href="selection-criteria.html#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Cross-validation is the technique of model validation that splits the data into two disjoint sets. Section <span class="math inline">\(\ref{S5:ModelValidation}\)</span> discussed out-of-sample validation where the data was split randomly into two subsets both containing a sizeable percentage of data. Another popular method is <em>leave-one-out</em> cross-validation, where the validation sample consists of a single observation and the development sample is based on the remainder of the data set.</p>
<p>Especially for small sample sizes, an attractive leave-one-out cross-validation statistic is <em>PRESS</em>, the <em>Predicted Residual Sum of Squares</em>. To define the statistic, consider the following procedure where we suppose that a candidate model is available.</p>
<div id="press-validation-procedure" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> PRESS Validation Procedure<a href="selection-criteria.html#press-validation-procedure" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><p>From the full sample, omit the <span class="math inline">\(i\)</span>th point and use the remaining <span class="math inline">\(n-1\)</span> observations to compute regression coefficients.</p></li>
<li><p>Use the regression coefficients computed in step one and the explanatory variables for the <span class="math inline">\(i\)</span>th observation to compute the predicted response, <span class="math inline">\(\hat{y}_{(i)}\)</span>. This part of the procedure is similar to the calculation of the <em>SSPE</em> statistic with <span class="math inline">\(n_1=n-1\)</span> and <span class="math inline">\(n_2=1\)</span>.</p></li>
<li><p>Now, repeat (i) and (ii) for <span class="math inline">\(i=1,...,n\)</span>. Summarizing, define</p></li>
</ol>
<p><span class="math display">\[
PRESS = \sum_{i=1}^{n} (y_i - \hat{y}_{(i)})^2
\]</span></p>
<p>As with <em>SSPE</em>, this statistic is calculated for each of several competing models. Under this criterion, we choose the model with the smallest <em>PRESS</em>.</p>
<p>Based on this definition, the statistic seems very computationally intensive in that it requires <span class="math inline">\(n\)</span> regression fits to evaluate it. To address this, interested readers will find that Section <span class="math inline">\(\ref{S5:LOOStatistics}\)</span> establishes</p>
<p><span class="math display">\[
y_i - \hat{y}_{(i)} = \frac{e_i}{1 - h_{ii}}
\]</span></p>
<p>Here, <span class="math inline">\(e_i\)</span> and <span class="math inline">\(h_{ii}\)</span> represent the <span class="math inline">\(i\)</span>th residual and leverage from the regression fit using the complete data set. This yields</p>
<p><span class="math display">\[
PRESS = \sum_{i=1}^{n} \left( \frac{e_i}{1 - h_{ii}} \right)^2
\]</span></p>
<p>which is a much easier computational formula. Thus, the <em>PRESS</em> statistic is less computationally intensive than <em>SSPE</em>.</p>
<p>Another important advantage of this statistic, when compared to <em>SSPE</em>, is that we do not need to make an arbitrary choice as to our relative subset sizes split. Indeed, because we are performing an “out-of-sample” validation for each observation, it can be argued that this procedure is more efficient, an especially important consideration when the sample size is small (say, less than 50 observations). A disadvantage is that because the model is re-fit for each point deleted, <em>PRESS</em> does not enjoy the appearance of independence between the estimation and prediction aspects, unlike <em>SSPE</em>.</p>
</div>
</div>
<div id="heteroscedasticity" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Heteroscedasticity<a href="selection-criteria.html#heteroscedasticity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In most regression applications, the goal is to understand determinants of the regression function <span class="math inline">\(\mathrm{E~}y_i = \mathbf{x}_i^{\prime} \boldsymbol \beta = \mu_i\)</span>. Our ability to understand the mean is strongly influenced by the amount of spread from the mean that we quantify using the variance <span class="math inline">\(\mathrm{E}\left(y_i - \mu_i\right)^2\)</span>. In some applications, such as when I weigh myself on a scale, there is relatively little variability; repeated measurements yield almost the same result. In other applications, such as the time it takes me to fly to New York, repeated measurements yield substantial variability and are fraught with inherent uncertainty.</p>
<p>The amount of uncertainty can also vary on a case-by-case basis. We denote the case of “varying variability” with the notation <span class="math inline">\(\sigma_i^2 = \mathrm{E}\left(y_i - \mu_i\right)^2\)</span>. When the variability varies by observation, this is known as <em>heteroscedasticity</em> for “different scatter.” In contrast, the usual assumption of common variability (assumption E3/F3 in Section 3.2) is called <em>homoscedasticity</em>, meaning “same scatter.”</p>
<p>Our estimation strategies depend on the extent of heteroscedasticity. For datasets with only a mild amount of heteroscedasticity, one can use least squares to estimate the regression coefficients, perhaps combined with an adjustment for the standard errors (described in Section <span class="math inline">\(\ref{S5:HeteroStdErrors}\)</span>). This is because least squares estimators are unbiased even in the presence of heteroscedasticity (see Property 1 in Section 3.2).</p>
<p>However, with heteroscedastic dependent variables, the Gauss-Markov theorem no longer applies and so the least squares estimators are not guaranteed to be optimal. In cases of severe heteroscedasticity, alternative estimators are used, the most common being those based on transformations of the dependent variable, as will be described in Section <span class="math inline">\(\ref{S5:Transformations}\)</span>.</p>
<div id="detecting-heteroscedasticity" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Detecting Heteroscedasticity<a href="selection-criteria.html#detecting-heteroscedasticity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To decide a strategy for handling potential heteroscedasticity, we must first assess, or detect, its presence.</p>
<p>To detect heteroscedasticity graphically, a good idea is to perform a preliminary regression fit of the data and plot the residuals versus the fitted values. To illustrate, Figure <span class="math inline">\(\ref{F5:HeteroRegress}\)</span> is a plot of a fictitious data set with one explanatory variable where the scatter increases as the explanatory variable increases. A least squares regression was performed - residuals and fitted values were computed. Figure <span class="math inline">\(\ref{F5:HeteroResid}\)</span> is an example of a plot of residuals versus fitted values. The preliminary regression fit removes many of the major patterns in the data and leaves the eye free to concentrate on other patterns that may influence the fit. We plot residuals versus fitted values because the fitted values are an approximation of the expected value of the response and, in many situations, the variability grows with the expected response.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig510"></span>
<img src="RegressionMarkdown_files/figure-html/Fig510-1.png" alt="The shaded area represents the data. The line is the true regression line." width="100%" />
<p class="caption">
Figure 6.2: <strong>The shaded area represents the data. The line is the true regression line.</strong>
</p>
</div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig511"></span>
<img src="RegressionMarkdown_files/figure-html/Fig511-1.png" alt="Residuals plotted versus the fitted values for the data in Figure 5.10." width="100%" />
<p class="caption">
Figure 6.3: <strong>Residuals plotted versus the fitted values for the data in Figure 5.10.</strong>
</p>
</div>
<p>More formal tests of heteroscedasticity are also available in the regression literature. To illustrate, let us consider a test due to Breusch and Pagan (1980). Specifically, this test examines the alternative hypothesis <span class="math inline">\(H_a\)</span>: $ y_i = ^2 + _i^{} $, where <span class="math inline">\(\mathbf{z}_i\)</span> is a known vector of variables and <span class="math inline">\(\boldsymbol \gamma\)</span> is a <span class="math inline">\(p\)</span>-dimensional vector of parameters. Thus, the null hypothesis is <span class="math inline">\(H_0:~ \boldsymbol \gamma = \mathbf{0}\)</span> is equivalent to homoscedasticity, <span class="math inline">\(\mathrm{Var~} y_i = \sigma^2.\)</span></p>
<p><em>Procedure to Test for Heteroscedasticity</em></p>
<ol style="list-style-type: decimal">
<li>Fit a regression model and calculate the model residuals, <span class="math inline">\(e_i\)</span>.</li>
<li>Calculate squared standardized residuals, <span class="math inline">\(e_i^{\ast 2} = e_i^2 / s^2\)</span>.</li>
<li>Fit a regression model of <span class="math inline">\(e_i^{\ast 2}\)</span> on <span class="math inline">\(\mathbf{z}_i\)</span>.</li>
<li>The test statistic is <span class="math inline">\(LM = \frac{\text{Regress~SS}_z}{2}\)</span>, where <span class="math inline">\(\text{Regress~SS}_z\)</span> is the regression sum of squares from the model fit in step (iii).</li>
<li>Reject the null hypothesis if <span class="math inline">\(LM\)</span> exceeds a percentile from a chi-square distribution with <span class="math inline">\(p\)</span> degrees of freedom. The percentile is one minus the significance level of the test.</li>
</ol>
<p>Here, we use <span class="math inline">\(LM\)</span> to denote the test statistic because Breusch and Pagan derived it as a Lagrange multiplier statistic; see Breusch and Pagan (1980) for more details.</p>
</div>
<div id="heteroscedasticity-consistent-standard-errors" class="section level3 hasAnchor" number="6.5.2">
<h3><span class="header-section-number">6.5.2</span> Heteroscedasticity-Consistent Standard Errors<a href="selection-criteria.html#heteroscedasticity-consistent-standard-errors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For datasets with only mild heteroscedasticity, a sensible strategy is to employ least squares estimators of the regression coefficients and to adjust the calculation of standard errors to account for the heteroscedasticity.</p>
<p>From the Section 3.2 on properties, we saw that least squares regression coefficients could be written as
<span class="math display">\[
\mathbf{b} = \sum_{i=1}^n \mathbf{w}_i y_i,
\]</span>
where <span class="math inline">\(\mathbf{w}_i = \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1} \mathbf{x}_i\)</span>. Thus, with <span class="math inline">\(\sigma_i^2 = \mathrm{Var~} y_i\)</span>, we have
<span class="math display">\[
\mathrm{Var~}\mathbf{b} = \sum_{i=1}^n \mathbf{w}_i \mathbf{w}_i^{\prime} \sigma_i^2
= \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1} \left( \sum_{i=1}^n \sigma_i^2 \mathbf{x}_i \mathbf{x}_i^{\prime} \right) \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1}.
\]</span>
This quantity is known except for <span class="math inline">\(\sigma_i^2\)</span>. We can compute residuals using the least squares regression coefficients as <span class="math inline">\(e_i = y_i - \mathbf{x}_i^{\prime} \mathbf{b}\)</span>. With these, we may define the <em>empirical</em>, or <em>robust</em>, estimate of the variance-covariance matrix as
<span class="math display">\[
\widehat{\mathrm{Var~}\mathbf{b}} = \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1} \left( \sum_{i=1}^n e_i^2 \mathbf{x}_i \mathbf{x}_i^{\prime} \right) \left( \mathbf{X}^{\prime}\mathbf{X} \right)^{-1}.
\]</span>
The corresponding “heteroscedasticity-consistent” standard errors are
<span class="math display">\[
se_r(b_j) = \sqrt{(j+1)^{\text{st}}~ \text{diagonal~element~of~}\widehat{\mathrm{Var~}\mathbf{b}}}.
\]</span>
The logic behind this estimator is that each squared residual, <span class="math inline">\(e_i^2\)</span>, may be a poor estimate of <span class="math inline">\(\sigma_i^2\)</span>. However, our interest is estimating a (weighted) sum of variances in equation <span class="math inline">\(\eqref{E5:HeterVariances}\)</span>; estimating the sum is a much easier task than estimating any individual variance estimate.</p>
<p>Robust, or heteroscedasticity-consistent, standard errors are widely available in statistical software packages. Here, you will also see alternative definitions of residuals employed, as in Section <span class="math inline">\(\ref{S5:Residuals}\)</span>. If your statistical package offers options, the robust estimator using studentized residuals is generally preferred.</p>
</div>
<div id="weighted-least-squares" class="section level3 hasAnchor" number="6.5.3">
<h3><span class="header-section-number">6.5.3</span> Weighted Least Squares<a href="selection-criteria.html#weighted-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The least squares estimators are less useful for datasets with severe heteroscedasticity. One strategy is to use a variation of least squares estimation by <em>weighting</em> observations. The idea is that, when minimizing the sum of squared errors using heteroscedastic data, the expected variability of some observations is smaller than others. Intuitively, it seems reasonable that the smaller the variability of the response, the more reliable that response and the greater weight that it should receive in the minimization procedure. <em>Weighted least squares</em> is a technique that accounts for this “varying variability.”</p>
<p>Specifically, we use Section 3.2.3 assumptions E1, E2, and E4, with E3 replaced by E <span class="math inline">\(\varepsilon_i = 0\)</span> and <span class="math inline">\(\text{Var} \varepsilon_i = \sigma^2 / w_i\)</span>, so that the variability is proportional to a known weight <span class="math inline">\(w_i\)</span>. For example, if unit of analysis <span class="math inline">\(i\)</span> represents a geographical entity such as a state, you might use the number of people in the state as a weight. Or, if <span class="math inline">\(i\)</span> represents a firm, you might use firm assets for the weighting variable. Larger values of <span class="math inline">\(w_i\)</span> indicate a more precise response variable through the smaller variability. In actuarial applications, weights are used to account for an exposure such as the amount of insurance premium, number of employees, size of the payroll, number of insured vehicles, and so forth (further discussion is in Chapter 18).</p>
<p>This model can be readily converted to the “ordinary” least squares problem by multiplying all regression variables by <span class="math inline">\(\sqrt{w_i}\)</span>. That is, if we define <span class="math inline">\(y_i^{\ast} = y_i \times \sqrt{w_i}\)</span> and <span class="math inline">\(x_{ij}^{\ast} = x_{ij} \times \sqrt{w_i}\)</span>, then from assumption E1 we have
<span class="math display">\[
y_i^{\ast} = y_i \times \sqrt{w_i} = \left( \beta_0 x_{i0} + \beta_1 x_{i1} + \ldots + \beta_k x_{ik} + \varepsilon_i \right) \sqrt{w_i}
= \beta_0 x_{i0}^{\ast} + \beta_1 x_{i1}^{\ast} + \ldots + \beta_k x_{ik}^{\ast} + \varepsilon_i^{\ast}
\]</span>
where <span class="math inline">\(\varepsilon_i^{\ast} = \varepsilon_i \times \sqrt{w_i}\)</span> has homoscedastic variance <span class="math inline">\(\sigma^2\)</span>. Thus, with the rescaled variables, all inference can proceed as before.</p>
<p>This work has been automated in statistical packages where the user merely specifies the weights <span class="math inline">\(w_i\)</span> and the package does the rest. In terms of matrix algebra, this procedure can be accomplished by defining an <span class="math inline">\(n \times n\)</span> weight matrix <span class="math inline">\(\mathbf{W} = \text{diag}(w_i)\)</span> so that the <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\(\mathbf{W}\)</span> is <span class="math inline">\(w_i\)</span>. Extending equation (3.14) for example, the weighted least squares estimates can be expressed as
<span class="math display">\[
\mathbf{b}_{WLS} = \left( \mathbf{X}^{\prime} \mathbf{W} \mathbf{X} \right)^{-1} \mathbf{X}^{\prime} \mathbf{W} \mathbf{y}.
\]</span>
Additional discussions of weighted least squares estimation will be presented in Section 15.1.1.</p>
</div>
<div id="transformations" class="section level3 hasAnchor" number="6.5.4">
<h3><span class="header-section-number">6.5.4</span> Transformations<a href="selection-criteria.html#transformations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another approach that handles severe heteroscedasticity, introduced in Section 1.3, is to transform the dependent variable, typically with a logarithmic transformation of the form <span class="math inline">\(y^{\ast} = \ln y\)</span>. As we saw in Section 1.3, transformations can serve to “shrink” spread out data and symmetrize a distribution. Through a change of scale, a transformation also changes the variability, potentially altering a heteroscedastic dataset into a homoscedastic one. This is both a strength and limitation of the transformation approach - a transformation simultaneously affects both the distribution and the heteroscedasticity.</p>
<p>Power transformations, such as the logarithmic transform, are most useful when the variability of the data grows with the mean. In this case, the transform will serve to “shrink” the data to a scale that appears to be homoscedastic. Conversely, because transformations are monotonic functions, they will not help with patterns of variability that are non-monotonic. Further, if your data is reasonably symmetric but heteroscedastic, a transformation will not be useful because any choice that mitigates the heteroscedasticity will skew the distribution.</p>
<p>When data are non-positive, it is common to add a constant to each observation so that all observations are positive prior to transformation. For example, the transform <span class="math inline">\(\ln(1+y)\)</span> accommodates the presence of zeros. One can also multiply by a constant so that the approximate original units are retained. For example, the transform <span class="math inline">\(100 \ln(1 + y/100)\)</span> may be applied to percentage data where negative percentages sometimes appear.</p>
<p>Our discussions of transformations have focused on transforming dependent variables. As noted in Section 3.5, transformations of explanatory variables are also possible. This is because the regression assumptions condition on explanatory variables (Section 3.2.3). Some analysts prefer to transform variables to approximate normality, thinking of multivariate normal distributions as a foundation for regression analysis. Others are reluctant to transform explanatory variables because of the difficulties in interpreting resulting models. The approach taken here is to use transforms that can be readily interpretable, such as those introduced in Section 3.5. Other transforms are certainly candidates to include in a selected model but they should provide substantial dividends in terms of fit or predictive power if they are difficult to communicate.</p>
</div>
</div>
<div id="further-reading-and-references" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Further Reading and References<a href="selection-criteria.html#further-reading-and-references" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Long and Ervin (2000) gather compelling evidence for the use of alternative heteroscedasticity-consistent estimators of standard errors that have better finite sample performance than the classic versions. The large sample properties of empirical estimators have been established by Eicker (1967), Huber (1967) and White (1980) in the linear regression case. For the linear regression case, MacKinnon and White (1985) suggest alternatives that provide superior small-sample properties. For small samples, the evidence is based on (1) the biasedness of the estimators, (2) their motivation as jackknife estimators and (3) their performance in simulation studies.</p>
<p>Other measures of collinearity based on matrix algebra concepts involving eigenvalues, such as condition numbers and condition indices, are used by some analysts. See Belseley, Kuh and Welsch (1980) for a solid treatment of collinearity and regression diagnostics. Hocking (2003) provides additional background reading on collinearity and principal components. See Carroll and Ruppert (1988) for further discussions of transformations in regression.</p>
<p>Hastie, Tibshirani and Friedman (2001) give an advanced discussion of model selection issues, focusing on predictive aspects of models in the language of machine learning.</p>
</div>
<div id="chapter-references" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Chapter References<a href="selection-criteria.html#chapter-references" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Belseley, David A., Edwin Kuh and Roy E. Welsch (1980). <em>Regression Diagnostics: Identifying Influential Data and Sources of Collinearity</em>. Wiley, New York.</li>
<li>Bendel, R. B. and Afifi, A. A. (1977). Comparison of stopping rules in forward “stepwise” regression. <em>Journal of the American Statistical Association</em> 72, 46-53.</li>
<li>Box, George E. P. (1980). Sampling and Bayes inference in scientific modeling and robustness (with discussion). <em>Journal of the Royal Statistical Society</em>, Series A, 143, 383-430.</li>
<li>Breusch, T. S. and A. R. Pagan (1980). The Lagrange multiplier test and its applications to model specification in econometrics. <em>Review of Economic Studies</em>, 47, 239-53.</li>
<li>Carroll, Raymond J. and David Ruppert (1988). <em>Transformation and Weighting in Regression</em>, Chapman-Hall.</li>
<li>Eicker, F. (1967). Limit theorems for regressions with unequal and dependent errors. <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em> 1, LeCam, L. M. and J. Neyman, editors, University of California Press, pp, 59-82.</li>
<li>Hadi, A. S. (1988). Diagnosing collinearity-influential observations. <em>Computational Statistics and Data Analysis</em> 7, 143-159.</li>
<li>Hastie, Trevor, Robert Tibshirani and Jerome Friedman (2001). <em>The Elements of Statistical Learning: Data Mining, Inference and Prediction</em>. Springer-Verlag, New York.</li>
<li>Hocking, Ronald R. (2003). <em>Methods and Applications of Linear Models: Regression and the Analysis of Variance</em>. Wiley, New York.</li>
<li>Huber, P. J. (1967). The behaviour of maximum likelihood estimators under non-standard conditions. <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em> 1, LeCam, L. M. and Neyman, J. editors, University of California Press, pp, 221-33.</li>
<li>Long, J.S. and L.H. Ervin (2000). Using heteroscedasticity consistent standard errors in the linear regression model. <em>American Statistician</em> 54, 217-224.</li>
<li>MacKinnon, J.G. and H. White (1985). Some heteroskedasticity consistent covariance matrix estimators with improved finite sample properties. <em>Journal of Econometrics</em> 29, 53-57.</li>
<li>Mason, R. L. and Gunst, R. F. (1985). Outlier-induced collinearities. <em>Technometrics</em> 27, 401-407.</li>
<li>Picard, R. R. and Berk, K. N. (1990). Data splitting. <em>The American Statistician</em> 44, 140-147.</li>
<li>Rencher, A. C. and Pun, F. C. (1980). Inflation of <span class="math inline">\(R^2\)</span> in best subset regression. <em>Technometrics</em> 22, 49-53.</li>
<li>Snee, R. D. (1977). Validation of regression models. Methods and examples. <em>Technometrics</em> 19, 415-428.</li>
</ul>
</div>
<div id="exercises" class="section level2 hasAnchor" number="6.8">
<h2><span class="header-section-number">6.8</span> Exercises<a href="selection-criteria.html#exercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>You are doing regression with one explanatory variable and so consider the basic linear regression model <span class="math inline">\(y_i = \beta_0 + \beta_1 x_i + \varepsilon_i\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Show that the <span class="math inline">\(i\)</span>th leverage can be simplified to
<span class="math display">\[
h_{ii} = \frac{1}{n} + \frac{(x_i - \overline{x})^2}{(n-1) s_x^2}.
\]</span></p></li>
<li><p>Show that <span class="math inline">\(\overline{h} = 2 / n\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(h_{ii} = 6/n\)</span>. How many standard deviations is <span class="math inline">\(x_i\)</span> away (either above or below) from the mean?</p></li>
</ol></li>
<li><p>Consider the output of a regression using one explanatory variable on <span class="math inline">\(n=3\)</span> observations. The residuals and leverages are:</p>
<p><span class="math display">\[
\begin{array}{l|ccc}
\hline
i &amp; 1 &amp; 2 &amp; 3 \\
\text{Residuals } e_i &amp; 3.181 &amp; -6.362 &amp; 3.181 \\
\text{Leverages } h_{ii} &amp; 0.8333 &amp; 0.3333 &amp; 0.8333 \\
\hline
\end{array}
\]</span></p>
<p>Compute the <span class="math inline">\(PRESS\)</span> statistic.</p></li>
<li><p><strong>National Life Expectancies.</strong> {#Ex:UNLIFE4} We continue the analysis begun in Exercises 1.{#Ex:UNLIFE}, 2.{#Ex:UNLIFE2}, 3.{#Ex:UNLIFE3} and 4.7. The focus of this exercise is variable selection.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Begin with the data from <span class="math inline">\(n=185\)</span> countries throughout the world that have valid (non-missing) life expectancies. Plot the life expectancy versus the gross domestic product and private expenditures on health. From these plots, describe why it is desirable to use logarithmic transforms, <span class="math inline">\(\text{lnGDP}\)</span> and <span class="math inline">\(\text{lnHEALTH}\)</span>, respectively. Also plot life expectancy versus <span class="math inline">\(\text{lnGDP}\)</span> and <span class="math inline">\(\text{lnHEALTH}\)</span> to confirm your intuition.</p></li>
<li><p>Use a stepwise regression algorithm to help you select a model. Do not consider the variables <span class="math inline">\(\text{RESEARCHERS}\)</span>, <span class="math inline">\(\text{SMOKING}\)</span>, and <span class="math inline">\(\text{FEMALEBOSS}\)</span> as these have many missing values. For the remaining variables, use only the observations without any missing values. Do this twice, with and without the categorical variable <span class="math inline">\(\text{REGION}\)</span>.</p></li>
<li><p>Return to the full data set of <span class="math inline">\(n=185\)</span> countries and run a regression model using <span class="math inline">\(\text{FERTILITY}\)</span>, <span class="math inline">\(\text{PUBLICEDUCATION}\)</span>, and <span class="math inline">\(\text{lnHEALTH}\)</span> as explanatory variables.</p></li>
</ol>
<p>c(i). Provide histograms of standardized residuals and leverages.</p>
<p>c(ii). Identify the standardized residual and leverage associated with Lesotho, formerly Basutoland, a kingdom surrounded by South Africa. Is this observation an outlier, high leverage point, or both?</p>
<p>c(iii). Re-run the regression without Lesotho. Cite any differences in the statistical coefficients between this model and the one in part c(i).</p></li>
<li><p><strong>Term Life Insurance.</strong> We continue our study of Term Life Insurance Demand from Chapters 3 and 4. Specifically, we examine the 2004 Survey of Consumer Finances (SCF), a nationally representative sample that contains extensive information on assets, liabilities, income, and demographic characteristics of those sampled (potential U.S. customers). We study a random sample of 500 families with positive incomes. From the sample of 500, we initially consider a subsample of <span class="math inline">\(n=275\)</span> families that purchased term life insurance.</p>
<p>Consider a linear regression of <span class="math inline">\(\text{LNINCOME}\)</span>, <span class="math inline">\(\text{EDUCATION}\)</span>, <span class="math inline">\(\text{NUMHH}\)</span>, <span class="math inline">\(\text{MARSTAT}\)</span>, <span class="math inline">\(\text{AGE}\)</span>, and <span class="math inline">\(\text{GENDER}\)</span> on <span class="math inline">\(\text{LNFACE}\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Collinearity. Not all of the variables turned out to be statistically significant. To investigate one possible explanation, calculate variance inflation factors.</li>
</ol>
<p>a(i). Briefly explain the idea of collinearity and a variance inflation factor.</p>
<p>a(ii). What constitutes a large variance inflation factor?</p>
<p>a(iii). If a large variance inflation factor is detected, what possible courses of action do we have to address this aspect of the data?</p>
<p>a(iv). Supplement the variance inflation factor statistics with a table of correlations of explanatory variables. Based on these statistics, is collinearity an issue with this fitted model? Why or why not?</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Unusual Points. Sometimes a poor model fit can be due to unusual points.</li>
</ol>
<p>b(i). Define the idea of leverage for an observation.</p>
<p>b(ii). For this fitted model, give standard rules of thumb for identifying points with unusual leverage. Identify any unusual points from the attached summary statistics.</p>
<p>b(iii). An analyst is concerned with leverage values for this fitted model and suggests using <span class="math inline">\(\text{FACE}\)</span> as the dependent variable instead of <span class="math inline">\(\text{LNFACE}\)</span>. Describe how leverage values would change using this alternative dependent variable.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Residual Analysis. We can learn how to improve model fits from analyses of residuals.</li>
</ol>
<p>c(i). Provide a plot of residuals versus fitted values. What do we hope to learn from this type of plot? Does this plot display any model inadequacies?</p>
<p>c(ii). Provide a <span class="math inline">\(qq\)</span> plot of residuals. What do we hope to learn from this type of plot? Does this plot display any model inadequacies?</p>
<p>c(iii). Provide a plot of residuals versus leverages. What do we hope to learn from this type of plot? Does this plot display any model inadequacies?</p>
<ol start="4" style="list-style-type: lower-alpha">
<li>Stepwise Regression. Run a stepwise regression algorithm. Suppose that this algorithm suggests a model using <span class="math inline">\(\text{LNINCOME}\)</span>, <span class="math inline">\(\text{EDUCATION}\)</span>, <span class="math inline">\(\text{NUMHH}\)</span>, and <span class="math inline">\(\text{GENDER}\)</span> as explanatory variables to predict the dependent variable <span class="math inline">\(\text{LNFACE}\)</span>.</li>
</ol>
<p>d(i). What is the purpose of stepwise regression?</p>
<p>d(ii). Describe two important drawbacks of stepwise regression algorithms.</p></li>
</ol>
<p>*National Life Expectancies.** {#Ex:UNLIFE4}</p>
</div>
<div id="S5:TechSupps" class="section level2 hasAnchor" number="6.9">
<h2><span class="header-section-number">6.9</span> Technical Supplements for Chapter 5<a href="selection-criteria.html#S5:TechSupps" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="S5:ProjMatrix" class="section level3 hasAnchor" number="6.9.1">
<h3><span class="header-section-number">6.9.1</span> Projection Matrix<a href="selection-criteria.html#S5:ProjMatrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Hat Matrix.</strong> We define the hat matrix to be <span class="math inline">\(\mathbf{H} = \mathbf{X(X}^{\prime}\mathbf{X)}^{-1} \mathbf{X}^{\prime}\)</span>, so that <span class="math inline">\(\mathbf{\hat{y}} = \mathbf{X b} = \mathbf{Hy}\)</span>. From this, the matrix <span class="math inline">\(\mathbf{H}\)</span> is said to the vector of responses <span class="math inline">\(\mathbf{y}\)</span> onto the vector of fitted values <span class="math inline">\(\mathbf{\hat{y}}\)</span>.</p>
<p>Because <span class="math inline">\(\mathbf{H}^{\prime} = \mathbf{H}\)</span>, the hat matrix is symmetric. Further, it is also an matrix due to the property that <span class="math inline">\(\mathbf{HH} = \mathbf{H}\)</span>. To see this, we have that
<span class="math display">\[
\mathbf{HH} = \mathbf{(X(\mathbf{X}^{\prime}X)}^{-1}\mathbf{X}^{\prime}\mathbf{)(X(\mathbf{X}^{\prime}X)}^{-1}\mathbf{X}^{\prime}\mathbf{)} = \mathbf{X(\mathbf{X}^{\prime}X)}^{-1}\mathbf{(\mathbf{X}^{\prime}X)(\mathbf{X}^{\prime}X)}^{-1}\mathbf{X}^{\prime} = \mathbf{X(\mathbf{X}^{\prime}X)}^{-1}\mathbf{X}^{\prime} = \mathbf{H}.
\]</span>
Similarly, it is easy to check that <span class="math inline">\(\mathbf{I-H}\)</span> is idempotent. Because <span class="math inline">\(\mathbf{H}\)</span> is idempotent, from some results in matrix algebra, it is straightforward to show that
<span class="math display">\[
\sum_{i=1}^{n} h_{ii} = k + 1.
\]</span>
As discussed in Section <span class="math inline">\(\ref{S5:Leverage}\)</span>, we use our bounds and the average leverage, <span class="math inline">\(\bar{h} = (k + 1)/n\)</span>, to help identify observations with unusually high leverage.</p>
<p><strong>Variance of Residuals.</strong> Using the model equation <span class="math inline">\(\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}\)</span>, we can express the vector of residuals as
<span class="math display">\[
\mathbf{e} = \mathbf{y} - \mathbf{\hat{y}} = \mathbf{y - Hy} = \mathbf{(I-H)(X \boldsymbol{\beta} + \boldsymbol{\varepsilon})} = \mathbf{(I-H) \boldsymbol{\varepsilon}}.
\]</span>
The last equality is due to the fact that <span class="math inline">\(\mathbf{(I-H)X} = \mathbf{X - HX} = \mathbf{X - X} = \mathbf{0}\)</span>. Using <span class="math inline">\(\text{Var~} \boldsymbol{\varepsilon} = \sigma^2 \mathbf{I}\)</span>, we have
<span class="math display">\[
\text{Var } \mathbf{e} = \text{Var }\left[ \mathbf{(I-H)\boldsymbol{\varepsilon}} \right] = \mathbf{(I-H)} \text{Var } \boldsymbol{\varepsilon} \mathbf{(I-H)} = \sigma^2 \mathbf{(I-H)} \mathbf{I} \mathbf{(I-H)} = \sigma^2 \mathbf{(I-H)}.
\]</span>
The last equality comes from the fact that <span class="math inline">\(\mathbf{I-H}\)</span> is idempotent. Thus, we have that
<span class="math display">\[
\text{Var } e_i = \sigma^2 (1 - h_{ii}) \text{ \ and \ Cov } (e_i, e_j) = -\sigma^2 h_{ij}.
\]</span>
Thus, although the true errors <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> are uncorrelated, there is a small negative correlation among residuals <span class="math inline">\(\mathbf{e}\)</span>.</p>
<p><strong>Dominance of the Error in the Residual.</strong> Examining the <span class="math inline">\(i\)</span>th row of equation (<span class="math inline">\(\ref{E5:Residuals}\)</span>), we have that the <span class="math inline">\(i\)</span>th residual
<span class="math display">\[
e_i = \varepsilon_i - \sum_{j=1}^{n} h_{ij} \varepsilon_j
\]</span>
can be expressed as a linear combination of independent errors. The relation <span class="math inline">\(\mathbf{H} = \mathbf{HH}\)</span> yields
<span class="math display">\[
h_{ii} = \sum_{j=1}^{n} h_{ij}^2.
\]</span>
Because <span class="math inline">\(h_{ii}\)</span> is, on average, <span class="math inline">\((k + 1)/n\)</span>, this indicates that each <span class="math inline">\(h_{ij}\)</span> is small relative to 1. Thus, when interpreting equation (<span class="math inline">\(\ref{E5:ResidualsErrors}\)</span>), we say that most of the information in <span class="math inline">\(e_i\)</span> is due to <span class="math inline">\(\varepsilon_i\)</span>.</p>
<p><strong>Correlations with Residuals.</strong> First define <span class="math inline">\(\mathbf{x}^j = (x_{1j}, x_{2j}, \dots, x_{nj})^{\prime}\)</span> to be the column representing the <span class="math inline">\(j\)</span>th variable. With this notation, we can partition the matrix of explanatory variables as <span class="math inline">\(\mathbf{X} = \left( \mathbf{x}^{0}, \mathbf{x}^{1}, \dots, \mathbf{x}^{k} \right)\)</span>. Now, examining the <span class="math inline">\(j\)</span>th column of the relation <span class="math inline">\(\mathbf{(I-H)X} = \mathbf{0}\)</span>, we have <span class="math inline">\(\mathbf{(I-H)x}^{j} = \mathbf{0}\)</span>. With <span class="math inline">\(\mathbf{e} = \mathbf{(I-H) \boldsymbol{\varepsilon}}\)</span>, this yields
<span class="math display">\[
\mathbf{e}^{\prime} \mathbf{x}^{j} = \boldsymbol{\varepsilon}^{\prime} \mathbf{(I-H)x}^{j} = 0,
\]</span>
for <span class="math inline">\(j = 0, 1, \ldots, k.\)</span> This result has several implications. If the intercept is in the model, then <span class="math inline">\(\mathbf{x}^{0} = (1, 1, \ldots, 1)^{\prime}\)</span> is a vector of ones. Here, <span class="math inline">\(\mathbf{e}^{\prime} \mathbf{x}^{0} = 0\)</span> means that <span class="math inline">\(\sum_{i=1}^{n} e_i = 0\)</span> or, the average residual is zero. Further, because <span class="math inline">\(\mathbf{e}^{\prime} \mathbf{x}^{j} = 0\)</span>, it is easy to check that the sample correlation between <span class="math inline">\(\mathbf{e}\)</span> and <span class="math inline">\(\mathbf{x}^{j}\)</span> is zero. Along the same line, we also have that <span class="math inline">\(\mathbf{e}^{\prime} \mathbf{\hat{y}} = \mathbf{e}^{\prime} \mathbf{(I-H)Xb} = \mathbf{0}\)</span>. Thus, using the same argument as above, the sample correlation between <span class="math inline">\(\mathbf{e}\)</span> and <span class="math inline">\(\mathbf{\hat{y}}\)</span> is zero.</p>
<p><strong>Multiple Correlation Coefficient.</strong> For an example of a non-zero correlation, consider <span class="math inline">\(r(\mathbf{y, \hat{y}})\)</span>, the sample correlation between <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{\hat{y}}\)</span>. Because <span class="math inline">\(\mathbf{(I-H)x}^{0} = \mathbf{0}\)</span>, we have <span class="math inline">\(\mathbf{x}^{0} = \mathbf{Hx}^{0}\)</span> and thus, <span class="math inline">\(\mathbf{\hat{y}}^{\prime} \mathbf{x}^{0} = \mathbf{y}^{\prime} \mathbf{Hx}^{0} = \mathbf{y^{\prime} x}^{0}\)</span>. Assuming <span class="math inline">\(\mathbf{x}^{0} = (1, 1, \ldots, 1)^{\prime}\)</span>, this means that <span class="math inline">\(\sum_{i=1}^{n} \hat{y}_i = \sum_{i=1}^{n} y_i\)</span>, so that the average fitted value is <span class="math inline">\(\bar{y}\)</span>.</p>
<p><span class="math display">\[
r(\mathbf{y, \hat{y}}) = \frac{\sum_{i=1}^{n} (y_i - \bar{y})(\hat{y}_i - \bar{y})}{(n-1) s_y s_{\hat{y}}}.
\]</span></p>
<p>Recall that <span class="math inline">\((n-1) s_y^2 = \sum_{i=1}^{n} (y_i - \bar{y})^2 = \text{Total SS}\)</span> and <span class="math inline">\((n-1) s_{\hat{y}}^2 = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 = \text{Regress SS}\)</span>. Further, with <span class="math inline">\(\mathbf{x}^0 = (1, 1, \ldots, 1)^{\prime}\)</span>,
<span class="math display">\[
\sum_{i=1}^{n} (y_i - \bar{y})(\hat{y}_i - \bar{y}) = (\mathbf{y} - \bar{y} \mathbf{x}^0)^{\prime} (\mathbf{\hat{y}} - \bar{y} \mathbf{x}^0) = \mathbf{y}^{\prime} \mathbf{\hat{y}} - \bar{y}^2 \mathbf{x}^{0 \prime} \mathbf{x}^0
\]</span>
<span class="math display">\[
= \mathbf{y}^{\prime} \mathbf{Xb} - n \bar{y}^2 = \text{Regress SS}.
\]</span></p>
<p>This yields
<span class="math display">\[
r(\mathbf{y, \hat{y}}) = \frac{\text{Regress SS}}{\sqrt{\left( \text{Total SS} \right) \left( \text{Regress SS} \right)}} = \sqrt{\frac{\text{Regress SS}}{\text{Total SS}}} = \sqrt{R^2}.
\]</span>
That is, the coefficient of determination can be interpreted as the square root of the correlation between the observed and fitted responses.</p>
</div>
<div id="S5:LOOStatistics" class="section level3 hasAnchor" number="6.9.2">
<h3><span class="header-section-number">6.9.2</span> Leave One Out Statistics<a href="selection-criteria.html#S5:LOOStatistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Notation.</strong> To test the sensitivity of regression quantities, there are a number of statistics of interest that are based on the notion of ``leaving out,’’ or omitting, an observation. To this end, the subscript notation <span class="math inline">\((i)\)</span> means to <em>leave out</em> the <span class="math inline">\(i\)</span>th observation. For example, omitting the row of explanatory variables <span class="math inline">\(\mathbf{x}_i^{\prime} = (x_{i0}, x_{i1}, \dots, x_{ik})\)</span> from <span class="math inline">\(\mathbf{X}\)</span> yields <span class="math inline">\(\mathbf{X}_{(i)}\)</span>, a <span class="math inline">\((n-1) \times (k+1)\)</span> matrix of explanatory variables. Similarly, <span class="math inline">\(\mathbf{y}_{(i)}\)</span> is a <span class="math inline">\((n-1) \times 1\)</span> vector, based on removing the <span class="math inline">\(i\)</span>th row from <span class="math inline">\(\mathbf{y}\)</span>.</p>
<p><strong>Basic Matrix Result.</strong> Suppose that <span class="math inline">\(\mathbf{A}\)</span> is an invertible, <span class="math inline">\(p \times p\)</span> matrix and <span class="math inline">\(\mathbf{z}\)</span> is a <span class="math inline">\(p \times 1\)</span> vector. The following result from matrix algebra provides an important tool for understanding leave one out statistics in linear regression analysis.
<span class="math display">\[
\left( \mathbf{A - zz}^{\prime} \right)^{-1} = \mathbf{A}^{-1} + \frac{\mathbf{A}^{-1} \mathbf{zz}^{\prime} \mathbf{A}^{-1}}{1 - \mathbf{z}^{\prime} \mathbf{A}^{-1} \mathbf{z}}.
\]</span>
To check this result, simply multiply <span class="math inline">\(\mathbf{A - zz}^{\prime}\)</span> by the right hand side of the equation to get <span class="math inline">\(\mathbf{I}\)</span>, the identity matrix.</p>
<p><strong>Vector of Regression Coefficients.</strong> Omitting the <span class="math inline">\(i\)</span>th observation, our new vector of regression coefficients is
<span class="math display">\[
\mathbf{b}_{(i)} = \left( \mathbf{X}_{(i)}^{\prime} \mathbf{X}_{(i)} \right)^{-1} \mathbf{X}_{(i)}^{\prime} \mathbf{y}_{(i)}.
\]</span>
An alternative expression for <span class="math inline">\(\mathbf{b}_{(i)}\)</span> that is simpler to compute turns out to be
<span class="math display">\[
\mathbf{b}_{(i)} = \mathbf{b} - \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}}.
\]</span>
To verify this, first use the matrix inversion result with <span class="math inline">\(\mathbf{A} = \mathbf{X}^{\prime} \mathbf{X}\)</span> and <span class="math inline">\(\mathbf{z} = \mathbf{x}_i\)</span> to get
<span class="math display">\[
\left( \mathbf{X}_{(i)}^{\prime} \mathbf{X}_{(i)} \right)^{-1} = (\mathbf{X}^{\prime} \mathbf{X} - \mathbf{x}_i \mathbf{x}_i^{\prime})^{-1} = (\mathbf{X}^{\prime} \mathbf{X})^{-1} + \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1}}{1 - h_{ii}},
\]</span>
where, from the leverage result, we have <span class="math inline">\(h_{ii} = \mathbf{x}_i^{\prime} (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{x}_i\)</span>. Multiplying each side by
<span class="math display">\[
\mathbf{X}_{(i)}^{\prime} \mathbf{y}_{(i)} = \mathbf{X}^{\prime} \mathbf{y} - \mathbf{x}_i y_i
\]</span>
yields
<span class="math display">\[
\mathbf{b}_{(i)} = \left( \mathbf{X}_{(i)}^{\prime} \mathbf{X}_{(i)} \right)^{-1} \mathbf{X}_{(i)}^{\prime} \mathbf{y}_{(i)} = \left( (\mathbf{X}^{\prime} \mathbf{X})^{-1} + \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1}}{1 - h_{ii}} \right) \left( \mathbf{X}^{\prime} \mathbf{y} - \mathbf{x}_i y_i \right)
\]</span>
<span class="math display">\[
= \mathbf{b} - \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i y_i + \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \mathbf{b} - \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i y_i}{1 - h_{ii}}
\]</span>
<span class="math display">\[
= \mathbf{b} - \frac{\left( 1 - h_{ii} \right) \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i y_i - \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \mathbf{b} - \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i h_{ii} y_i}{1 - h_{ii}}
\]</span>
<span class="math display">\[
= \mathbf{b} - \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i y_i - \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i \mathbf{x}_i^{\prime} \mathbf{b}}{1 - h_{ii}} = \mathbf{b} - \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}}.
\]</span>
This establishes the result.</p>
<p><strong>Cook’s Distance.</strong> To measure the effect, or <em>influence</em>, of omitting the <span class="math inline">\(i\)</span>th observation, Cook examined the difference between fitted values with and without the observation. We define Cook’s Distance to be
<span class="math display">\[
D_i = \frac{\left( \mathbf{\hat{y} - \hat{y}}_{(i)} \right)^{\prime} \left( \mathbf{\hat{y} - \hat{y}}_{(i)} \right)}{(k+1) s^2}
\]</span>
where <span class="math inline">\(\mathbf{\hat{y}}_{(i)} = \mathbf{Xb}_{(i)}\)</span> is the vector of fitted values calculated omitting the <span class="math inline">\(i\)</span>th point. Using equation (<span class="math inline">\(\ref{E5:LOORegressionCoeff}\)</span>) and <span class="math inline">\(\mathbf{\hat{y}} = \mathbf{Xb}\)</span>, an alternative expression for Cook’s Distance is
<span class="math display">\[
D_i = \frac{\left( \mathbf{b - b}_{(i)} \right)^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right) \left( \mathbf{b - b}_{(i)} \right)}{(k+1) s^2}
\]</span>
<span class="math display">\[
= \frac{e_i^2}{(1 - h_{ii})^2} \frac{\mathbf{x}_i^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \left( \mathbf{X}^{\prime} \mathbf{X} \right) \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i}{(k+1) s^2}
\]</span>
<span class="math display">\[
= \frac{e_i^2}{(1 - h_{ii})^2} \frac{h_{ii}}{(k+1) s^2} = \left( \frac{e_i^2}{s \sqrt{1 - h_{ii}}} \right)^2 \frac{h_{ii}}{(k+1) (1 - h_{ii})}.
\]</span>
This result is not only useful computationally, it also serves to decompose the statistic into the part due to the standardized residual, <span class="math inline">\(\left( \frac{e_i}{s \sqrt{1 - h_{ii}}} \right)^2\)</span>, and due to the leverage, <span class="math inline">\(\frac{h_{ii}}{(k+1) (1 - h_{ii})}\)</span>.</p>
<p><strong>Leave One Out Residual.</strong> The leave one out residual is defined by <span class="math inline">\(e_{(i)} = y_i - \mathbf{x}_i^{\prime} \mathbf{b}_{(i)}\)</span>. It is used in computing the <em>PRESS</em> statistic, described in Section <span class="math inline">\(\ref{S5:CrossV}\)</span>. A simple computational expression is <span class="math inline">\(e_{(i)} = \frac{e_i}{1 - h_{ii}}\)</span>. To verify this, use equation (<span class="math inline">\(\ref{E5:LOORegressionCoeff}\)</span>) to get
<span class="math display">\[
e_{(i)} = y_i - \mathbf{x}_i^{\prime} \mathbf{b}_{(i)} = y_i - \mathbf{x}_i^{\prime} \left( \mathbf{b} - \frac{\left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}} \right)
\]</span>
<span class="math display">\[
= e_i + \frac{\mathbf{x}_i \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}} = e_i + \frac{h_{ii} e_i}{1 - h_{ii}} = \frac{e_i}{1 - h_{ii}}.
\]</span></p>
<p><strong>Leave One Out Variance Estimate.</strong> The leave one out estimate of the variance is defined by
<span class="math display">\[
s_{(i)}^2 = \frac{((n - 1) - (k + 1))^{-1} \sum_{j \ne i} \left( y_j - \mathbf{x}_j^{\prime} \mathbf{b}_{(i)} \right)^2}{(n - 1) - (k + 1)}.
\]</span>
It is used in the definition of the <em>studentized residual</em>, defined in Section <span class="math inline">\(\ref{S5:Residuals}\)</span>. A simple computational expression is given by
<span class="math display">\[
s_{(i)}^2 = \frac{(n - (k + 1)) s^2 - \frac{e_i^2}{1 - h_{ii}}}{(n - 1) - (k + 1)}.
\]</span>
To see this, first note that from equation (<span class="math inline">\(\ref{E5:Residuals}\)</span>), we have <span class="math inline">\(\mathbf{He} = \mathbf{H(I - H) \boldsymbol{\varepsilon}} = \mathbf{0}\)</span>, because <span class="math inline">\(\mathbf{H} = \mathbf{HH}\)</span>. In particular, from the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{He} = \mathbf{0}\)</span>, we have <span class="math inline">\(\sum_{j=1}^{n} h_{ij} e_j = 0\)</span>. Now, using equations (<span class="math inline">\(\ref{E5:Leverages}\)</span>) and (<span class="math inline">\(\ref{E5:LOORegressionCoeff}\)</span>), we have
<span class="math display">\[
\sum_{j \ne i} \left( y_j - \mathbf{x}_j^{\prime} \mathbf{b}_{(i)} \right)^2 = \sum_{j=1}^{n} \left( y_j - \mathbf{x}_j^{\prime} \mathbf{b}_{(i)} \right)^2 - \left( y_i - \mathbf{x}_i^{\prime} \mathbf{b}_{(i)} \right)^2
\]</span>
<span class="math display">\[
= \sum_{j=1}^{n} \left( y_j - \mathbf{x}_j^{\prime} \mathbf{b} + \frac{\mathbf{x}_j^{\prime} \left( \mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{x}_i e_i}{1 - h_{ii}} \right) - e_{(i)}^2
\]</span>
<span class="math display">\[
= \sum_{j=1}^{n} \left( e_j + \frac{h_{ij} e_i}{1 - h_{ii}} \right)^2 - \frac{e_i^2}{(1 - h_{ii})^2}
\]</span>
<span class="math display">\[
= \sum_{j=1}^{n} e_j^2 + 0 + \frac{e_i^2}{(1 - h_{ii})^2} h_{ii} - \frac{e_i^2}{(1 - h_{ii})^2}
\]</span>
<span class="math display">\[
= \sum_{j=1}^{n} e_j^2 - \frac{e_i^2}{1 - h_{ii}} = (n - (k + 1)) s^2 - \frac{e_i^2}{1 - h_{ii}}.
\]</span></p>
<p>This establishes the result.</p>
</div>
<div id="omitting-variables" class="section level3 hasAnchor" number="6.9.3">
<h3><span class="header-section-number">6.9.3</span> Omitting Variables<a href="selection-criteria.html#omitting-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Notation.</strong> To measure the effect on regression quantities, there are a number of statistics of interest that are based on the notion of omitting an explanatory variable. To this end, the superscript notation <span class="math inline">\((j)\)</span> means to omit the <span class="math inline">\(j\)</span>th variable, where <span class="math inline">\(j=0,1,\ldots,k\)</span>. First, recall that <span class="math inline">\(\mathbf{x}^{j} = (x_{1j}, x_{2j}, \ldots, x_{nj})^{\prime}\)</span> is the column representing the <span class="math inline">\(j\)</span>th variable. Further, define <span class="math inline">\(\mathbf{X}^{(j)}\)</span> to be the <span class="math inline">\(n \times k\)</span> matrix of explanatory variables defined by removing <span class="math inline">\(\mathbf{x}^{j}\)</span> from <span class="math inline">\(\mathbf{X}\)</span>. For example, taking <span class="math inline">\(j=k\)</span>, we often partition <span class="math inline">\(\mathbf{X}\)</span> as <span class="math inline">\(\mathbf{X} = \left( \mathbf{X}^{(k)}: \mathbf{x}^k \right)\)</span>. Employing the results of Section 4.7.2, we will use <span class="math inline">\(\mathbf{X}^{(k)} = \mathbf{X}_1\)</span> and <span class="math inline">\(\mathbf{x}^k = \mathbf{X}_2\)</span>.</p>
<p><strong>Variance Inflation Factor.</strong> We first would like to establish the relationship between the definition of the standard error of <span class="math inline">\(b_j\)</span> given by
<span class="math display">\[
se(b_j) = s \sqrt{(j+1)\text{th \textit{diagonal element} of }(\mathbf{X}^{\prime}\mathbf{X})^{-1}}
\]</span>
and the relationship involving the variance inflation factor,
<span class="math display">\[
se(b_j) = s \frac{\sqrt{VIF_j}}{s_{x_j}\sqrt{n-1}}.
\]</span>
By symmetry of the independent variables, we only need to consider the case where <span class="math inline">\(j=k\)</span>. Thus, we would like to establish
<span class="math display">\[
(k+1)\text{th diagonal element of }(\mathbf{X}^{\prime}\mathbf{X})^{-1} = \frac{VIF_{k}}{(n-1) s_{x_{k}}^2}.
\]</span>
First consider the reparameterized model in equation (4.22). From equation (4.23), we can express the regression coefficient estimate
<span class="math display">\[
b_{k} = \frac{\mathbf{e}_1^{\prime}\mathbf{y}}{\mathbf{e}_1^{\prime}\mathbf{e}_1}.
\]</span>
From equation (4.23), we have that <span class="math inline">\(\text{Var} \, b_{k} = \sigma^2 (\mathbf{E}_2^{\prime} \mathbf{E}_2)^{-1}\)</span> and thus
<span class="math display">\[
se(b_{k}) = s (\mathbf{E}_2^{\prime} \mathbf{E}_2)^{-1/2}.
\]</span>
Thus, <span class="math inline">\((\mathbf{E}_2^{\prime} \mathbf{E}_2)^{-1}\)</span> is the <span class="math inline">\((k+1)\)</span>th diagonal element of
<span class="math display">\[
\left(
\begin{bmatrix}
\mathbf{X}_1^{\prime} \\
\mathbf{E}_2^{\prime}
\end{bmatrix}
\begin{bmatrix}
\mathbf{X}_1 &amp; \mathbf{E}_2
\end{bmatrix}
\right)^{-1}
\]</span>
and is also the <span class="math inline">\((k+1)\)</span>th diagonal element of <span class="math inline">\((\mathbf{X}^{\prime} \mathbf{X})^{-1}\)</span>. Alternatively, this can be verified directly using the partitioned matrix inverse in equation (4.19).</p>
<p>Now, suppose that we run a regression using <span class="math inline">\(\mathbf{x}^{k} = \mathbf{X}_2\)</span> as the response vector and <span class="math inline">\(\mathbf{X}^{(k)} = \mathbf{X}_1\)</span> as the matrix of explanatory variables. As noted above equation (4.22), <span class="math inline">\(\mathbf{E}_2\)</span> represents the <code>residuals'' from this regression and thus $\mathbf{E}_2^{\prime} \mathbf{E}_2$ represents the error sum of squares. For this regression, the total sum of squares is $$ \sum_{i=1}^{n} (x_{ik} - \bar{x}_{k})^2 = (n-1) s_{x_{k}}^2 $$ and the coefficient of determination is $R_{k}^2$. Thus, $$ \mathbf{E}_2^{\prime} \mathbf{E}_2 = \text{</code>Error SS’’} = (1 - R_{k}^2) = .
$$
This establishes the result.</p>
<p><strong>Establishing</strong> <span class="math inline">\(t^2 = F\)</span>. For testing the null hypothesis <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\beta_{k} = 0\)</span>, the material in Section 3.4.1 provides a description of a test based on the <span class="math inline">\(t\)</span>-statistic, <span class="math inline">\(t(b_{k}) = \frac{b_{k}}{se(b_{k})}\)</span>. An alternative test procedure, described in Sections 4.2.2, uses the test statistic
<span class="math display">\[
F-\text{ratio} = \frac{(\text{Error SS})_{\text{reduced}} - (\text{Error SS})_{\text{full}}}{p \times (Error~MS)_{\text{full}}} = \frac{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right)^2}{s^2 \mathbf{E}_2^{\prime} \mathbf{E}_2}
\]</span>
from equation (4.26). Alternatively, from equations (4.23) and (<span class="math inline">\(\ref{E5:StdErrorReparam}\)</span>), we have
<span class="math display">\[
t(b_{k}) = \frac{b_{k}}{se(b_{k})} = \frac{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right) / \left( \mathbf{E}_2^{\prime} \mathbf{E}_2 \right)}{s / \sqrt{\mathbf{E}_2^{\prime} \mathbf{E}_2}} = \frac{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right)}{s \sqrt{\mathbf{E}_2^{\prime} \mathbf{E}_2}}.
\]</span></p>
<p>Thus, <span class="math inline">\(t(b_{k})^2 = F\)</span>-ratio.</p>
<p><strong>Partial Correlation Coefficients.</strong> From the full regression model
<span class="math display">\[
\mathbf{y} = \mathbf{X}^{(k)} \boldsymbol{\beta}^{(k)} + \mathbf{x}_{k} \beta_{k} + \boldsymbol{\varepsilon},
\]</span>
consider two separate regressions. A regression using <span class="math inline">\(\mathbf{x}^{k}\)</span> as the response vector and <span class="math inline">\(\mathbf{X}^{(k)}\)</span> as the matrix of explanatory variables yields the residuals <span class="math inline">\(\mathbf{E}_2\)</span>. Similarly, a regression with <span class="math inline">\(\mathbf{y}\)</span> as the response vector and <span class="math inline">\(\mathbf{X}^{(k)}\)</span> as the matrix of explanatory variables yields the residuals
<span class="math display">\[
\mathbf{E}_1 = \mathbf{y} - \mathbf{X}^{(k)} \left( \mathbf{X}^{(k)\prime} \mathbf{X}^{(k)} \right)^{-1} \mathbf{X}^{(k)} \mathbf{y}.
\]</span>
If <span class="math inline">\(x^{0} = (1,1,\ldots,1)^{\prime}\)</span>, then the average of <span class="math inline">\(\mathbf{E}_1\)</span> and <span class="math inline">\(\mathbf{E}_2\)</span> is zero. In this case, the sample correlation between <span class="math inline">\(\mathbf{E}_1\)</span> and <span class="math inline">\(\mathbf{E}_2\)</span> is
<span class="math display">\[
r(\mathbf{E}_1, \mathbf{E}_2) = \frac{\sum_{i=1}^{n} E_{1i} E_{2i}}{\sqrt{\left( \sum_{i=1}^{n} E_{1i}^2 \right) \left( \sum_{i=1}^{n} E_{2i}^2 \right)}} = \frac{\mathbf{E}_1^{\prime} \mathbf{E}_2}{\sqrt{\left( \mathbf{E}_1^{\prime} \mathbf{E}_1 \right) \left( \mathbf{E}_2^{\prime} \mathbf{E}_2 \right)}}.
\]</span>
Because <span class="math inline">\(\mathbf{E}_2\)</span> is a vector of residuals using <span class="math inline">\(\mathbf{X}^{(k)}\)</span> as the matrix of explanatory variables, we have that <span class="math inline">\(\mathbf{E}_2^{\prime} \mathbf{X}^{(k)} = 0\)</span>. Thus, for the numerator, we have
<span class="math display">\[
\mathbf{E}_2^{\prime} \mathbf{E}_1 = \mathbf{E}_2^{\prime} \left( \mathbf{y} - \mathbf{X}^{(k)} \left( \mathbf{X}^{(k)\prime} \mathbf{X}^{(k)} \right)^{-1} \mathbf{X}^{(k)} \mathbf{y} \right) = \mathbf{E}_2^{\prime} \mathbf{y}.
\]</span>
From equations (4.24) and (4.25), we have that
<span class="math display">\[
(n - (k+1)) s^2 = (\text{Error SS})_{\text{full}} = \mathbf{E}_1^{\prime} \mathbf{E}_1 - \frac{\left( \mathbf{E}_1^{\prime} \mathbf{y} \right)^2}{\mathbf{E}_2^{\prime} \mathbf{E}_2} = \mathbf{E}_1^{\prime} \mathbf{E}_1 - \frac{\left( \mathbf{E}_1^{\prime} \mathbf{E}_2 \right)^2}{\mathbf{E}_2^{\prime} \mathbf{E}_2}.
\]</span>
Thus, from equation (<span class="math inline">\(\ref{E5:tStat}\)</span>)
<span class="math display">\[
\frac{t(b_{k})}{\sqrt{t(b_{k})^2 + n - (k+1)}} = \frac{\mathbf{E}_2^{\prime} \mathbf{y} / \left(s \sqrt{\mathbf{E}_2^{\prime} \mathbf{E}_2}\right)}{\sqrt{\frac{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right)^2}{s^2 \mathbf{E}_2^{\prime} \mathbf{E}_2} + n - (k+1)}}
\]</span>
<span class="math display">\[
= \frac{\mathbf{E}_2^{\prime} \mathbf{y}}{\sqrt{\left( \mathbf{E}_2^{\prime} \mathbf{y} \right)^2 + \mathbf{E}_2^{\prime} \mathbf{E}_2 s^2 \left(n - (k+1) \right)}}
\]</span>
<span class="math display">\[
= \frac{\mathbf{E}_2^{\prime} \mathbf{E}_1}{\sqrt{\left( \mathbf{E}_2^{\prime} \mathbf{E}_1 \right)^2 + \mathbf{E}_2^{\prime} \mathbf{E}_2 \left( \mathbf{E}_1^{\prime} \mathbf{E}_1 - \frac{\left( \mathbf{E}_2^{\prime} \mathbf{E}_1 \right)^2}{\mathbf{E}_2^{\prime} \mathbf{E}_2} \right)}}
\]</span>
<span class="math display">\[
= \frac{\mathbf{E}_1^{\prime} \mathbf{E}_2}{\sqrt{(\mathbf{E}_1^{\prime} \mathbf{E}_1) (\mathbf{E}_2^{\prime} \mathbf{E}_2)}} = r(\mathbf{E}_1, \mathbf{E}_2).
\]</span>
This establishes the relationship between the partial correlation coefficient and the <span class="math inline">\(t\)</span>-ratio statistic.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="C5VarSelect.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliography.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
