<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Regression and the Normal Distribution | Regression Modeling with Actuarial and Financial Applications</title>
  <meta name="description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Regression and the Normal Distribution | Regression Modeling with Actuarial and Financial Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Regression and the Normal Distribution | Regression Modeling with Actuarial and Financial Applications" />
  
  <meta name="twitter:description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  

<meta name="author" content="Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="preface.html"/>
<link rel="next" href="C2BasicLR.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript"  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleSolution(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}      
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>



<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Regression Modeling With Actuarial and Financial Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#forward"><i class="fa fa-check"></i>Forward</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#who-is-this-book-for"><i class="fa fa-check"></i>Who Is This Book For?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#what-is-this-book-about"><i class="fa fa-check"></i>What Is This Book About?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#how-does-this-book-deliver-its-message"><i class="fa fa-check"></i>How Does This Book Deliver Its Message?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html"><i class="fa fa-check"></i><b>1</b> Regression and the Normal Distribution</a>
<ul>
<li class="chapter" data-level="1.1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec11"><i class="fa fa-check"></i><b>1.1</b> What is Regression Analysis?</a></li>
<li class="chapter" data-level="1.2" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec12"><i class="fa fa-check"></i><b>1.2</b> Fitting Data to a Normal Distribution</a></li>
<li class="chapter" data-level="1.3" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec13"><i class="fa fa-check"></i><b>1.3</b> Power Transforms</a></li>
<li class="chapter" data-level="1.4" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec14"><i class="fa fa-check"></i><b>1.4</b> Sampling and the Role of Normality</a></li>
<li class="chapter" data-level="1.5" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec15"><i class="fa fa-check"></i><b>1.5</b> Regression and Sampling Designs</a></li>
<li class="chapter" data-level="1.6" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec16"><i class="fa fa-check"></i><b>1.6</b> Actuarial Applications of Regression</a></li>
<li class="chapter" data-level="1.7" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec17"><i class="fa fa-check"></i><b>1.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="1.8" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec18"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li>
<li class="chapter" data-level="1.9" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec19"><i class="fa fa-check"></i><b>1.9</b> Technical Supplement - Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="C2BasicLR.html"><a href="C2BasicLR.html"><i class="fa fa-check"></i><b>2</b> Basic Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec21"><i class="fa fa-check"></i><b>2.1</b> Correlations and Least Squares</a></li>
<li class="chapter" data-level="2.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec22"><i class="fa fa-check"></i><b>2.2</b> Basic Linear Regression Model</a></li>
<li class="chapter" data-level="2.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec23"><i class="fa fa-check"></i><b>2.3</b> Is the Model Useful? Some Basic Summary Measures</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec231"><i class="fa fa-check"></i><b>2.3.1</b> Partitioning the Variability</a></li>
<li class="chapter" data-level="2.3.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec232"><i class="fa fa-check"></i><b>2.3.2</b> The Size of a Typical Deviation: <em>s</em></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec24"><i class="fa fa-check"></i><b>2.4</b> Properties of Regression Coefficient Estimators</a></li>
<li class="chapter" data-level="2.5" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec25"><i class="fa fa-check"></i><b>2.5</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec251"><i class="fa fa-check"></i><b>2.5.1</b> Is the Explanatory Variable Important?: The <em>t</em>-Test</a></li>
<li class="chapter" data-level="2.5.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec252"><i class="fa fa-check"></i><b>2.5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="2.5.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec253"><i class="fa fa-check"></i><b>2.5.3</b> Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec26"><i class="fa fa-check"></i><b>2.6</b> Building a Better Model: Residual Analysis</a></li>
<li class="chapter" data-level="2.7" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec27"><i class="fa fa-check"></i><b>2.7</b> Application: Capital Asset Pricing Model</a></li>
<li class="chapter" data-level="2.8" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec28"><i class="fa fa-check"></i><b>2.8</b> Illustrative Regression Computer Output</a></li>
<li class="chapter" data-level="2.9" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec29"><i class="fa fa-check"></i><b>2.9</b> Further Reading and References</a></li>
<li class="chapter" data-level="2.10" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec210"><i class="fa fa-check"></i><b>2.10</b> Exercises</a></li>
<li class="chapter" data-level="2.11" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec211"><i class="fa fa-check"></i><b>2.11</b> Technical Supplement - Elements of Matrix Algebra</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2111"><i class="fa fa-check"></i><b>2.11.1</b> Basic Definitions</a></li>
<li class="chapter" data-level="2.11.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2112"><i class="fa fa-check"></i><b>2.11.2</b> Some Special Matrices</a></li>
<li class="chapter" data-level="2.11.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2113"><i class="fa fa-check"></i><b>2.11.3</b> Basic Operations</a></li>
<li class="chapter" data-level="2.11.4" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2114"><i class="fa fa-check"></i><b>2.11.4</b> Random Matrices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html"><i class="fa fa-check"></i><b>3</b> Multiple Linear Regression - I</a>
<ul>
<li class="chapter" data-level="3.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec31"><i class="fa fa-check"></i><b>3.1</b> Method of Least Squares</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec311"><i class="fa fa-check"></i><b>3.1.1</b> Least Squares Method</a></li>
<li class="chapter" data-level="3.1.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec312"><i class="fa fa-check"></i><b>3.1.2</b> General Case with <em>k</em> Explanatory Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec32"><i class="fa fa-check"></i><b>3.2</b> Linear Regression Model and Properties of Estimators</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec321"><i class="fa fa-check"></i><b>3.2.1</b> Regression Function</a></li>
<li class="chapter" data-level="3.2.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec322"><i class="fa fa-check"></i><b>3.2.2</b> Regression Coefficient Interpretation</a></li>
<li class="chapter" data-level="3.2.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec323"><i class="fa fa-check"></i><b>3.2.3</b> Model Assumptions</a></li>
<li class="chapter" data-level="3.2.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec324"><i class="fa fa-check"></i><b>3.2.4</b> Properties of Regression Coefficient Estimators</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec33"><i class="fa fa-check"></i><b>3.3</b> Estimation and Goodness of Fit</a></li>
<li class="chapter" data-level="3.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec34"><i class="fa fa-check"></i><b>3.4</b> Statistical Inference for a Single Coefficient</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec341"><i class="fa fa-check"></i><b>3.4.1</b> The <em>t</em>-Test</a></li>
<li class="chapter" data-level="3.4.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec342"><i class="fa fa-check"></i><b>3.4.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="3.4.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec343"><i class="fa fa-check"></i><b>3.4.3</b> Added Variable Plots</a></li>
<li class="chapter" data-level="3.4.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec344"><i class="fa fa-check"></i><b>3.4.4</b> Partial Correlation Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec35"><i class="fa fa-check"></i><b>3.5</b> Some Special Explanatory Variables</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec351"><i class="fa fa-check"></i><b>3.5.1</b> Binary Variables</a></li>
<li class="chapter" data-level="3.5.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec352"><i class="fa fa-check"></i><b>3.5.2</b> Transforming Explanatory Variables</a></li>
<li class="chapter" data-level="3.5.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec353"><i class="fa fa-check"></i><b>3.5.3</b> Interaction Terms</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec36"><i class="fa fa-check"></i><b>3.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="3.7" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec37"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression - II</a>
<ul>
<li class="chapter" data-level="4.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec41"><i class="fa fa-check"></i><b>4.1</b> The Role of Binary Variables</a></li>
<li class="chapter" data-level="4.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec42"><i class="fa fa-check"></i><b>4.2</b> Statistical Inference for Several Coefficients</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec421"><i class="fa fa-check"></i><b>4.2.1</b> Sets of Regression Coefficients</a></li>
<li class="chapter" data-level="4.2.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec422"><i class="fa fa-check"></i><b>4.2.2</b> The General Linear Hypothesis</a></li>
<li class="chapter" data-level="4.2.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec423"><i class="fa fa-check"></i><b>4.2.3</b> Estimating and Predicting Several Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec43"><i class="fa fa-check"></i><b>4.3</b> One Factor ANOVA Model</a></li>
<li class="chapter" data-level="4.4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec44"><i class="fa fa-check"></i><b>4.4</b> Combining Categorical and Continuous Explanatory Variables</a></li>
<li class="chapter" data-level="4.5" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec45"><i class="fa fa-check"></i><b>4.5</b> Further Reading and References</a></li>
<li class="chapter" data-level="4.6" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec46"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec47"><i class="fa fa-check"></i><b>4.7</b> Technical Supplement - Matrix Expressions</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec471"><i class="fa fa-check"></i><b>4.7.1</b> Expressing Models with Categorical Variables in Matrix Form</a></li>
<li class="chapter" data-level="4.7.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec472"><i class="fa fa-check"></i><b>4.7.2</b> Calculating Least Squares Recursively</a></li>
<li class="chapter" data-level="4.7.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec473"><i class="fa fa-check"></i><b>4.7.3</b> General Linear Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="C5VarSelect.html"><a href="C5VarSelect.html"><i class="fa fa-check"></i><b>5</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="5.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec51"><i class="fa fa-check"></i><b>5.1</b> An Iterative Approach to Data Analysis and Modeling</a></li>
<li class="chapter" data-level="5.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec52"><i class="fa fa-check"></i><b>5.2</b> Automatic Variable Selection Procedures</a></li>
<li class="chapter" data-level="5.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec53"><i class="fa fa-check"></i><b>5.3</b> Residual Analysis</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec531"><i class="fa fa-check"></i><b>5.3.1</b> Residuals</a></li>
<li class="chapter" data-level="5.3.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec532"><i class="fa fa-check"></i><b>5.3.2</b> Using Residuals to Identify Outliers</a></li>
<li class="chapter" data-level="5.3.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec533"><i class="fa fa-check"></i><b>5.3.3</b> Using Residuals to Select Explanatory Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec54"><i class="fa fa-check"></i><b>5.4</b> Influential Points</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec541"><i class="fa fa-check"></i><b>5.4.1</b> Leverage</a></li>
<li class="chapter" data-level="5.4.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec542"><i class="fa fa-check"></i><b>5.4.2</b> Cook’s Distance</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec55"><i class="fa fa-check"></i><b>5.5</b> Collinearity</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec551"><i class="fa fa-check"></i><b>5.5.1</b> What is Collinearity?</a></li>
<li class="chapter" data-level="5.5.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec552"><i class="fa fa-check"></i><b>5.5.2</b> Variance Inflation Factors</a></li>
<li class="chapter" data-level="5.5.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec553"><i class="fa fa-check"></i><b>5.5.3</b> Collinearity and Leverage</a></li>
<li class="chapter" data-level="5.5.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec554"><i class="fa fa-check"></i><b>5.5.4</b> Suppressor Variables</a></li>
<li class="chapter" data-level="5.5.5" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec555"><i class="fa fa-check"></i><b>5.5.5</b> Orthogonal Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec56"><i class="fa fa-check"></i><b>5.6</b> Selection Criteria</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec561"><i class="fa fa-check"></i><b>5.6.1</b> Goodness of Fit</a></li>
<li class="chapter" data-level="5.6.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec562"><i class="fa fa-check"></i><b>5.6.2</b> Model Validation</a></li>
<li class="chapter" data-level="5.6.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec563"><i class="fa fa-check"></i><b>5.6.3</b> Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec57"><i class="fa fa-check"></i><b>5.7</b> Heteroscedasticity</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec571"><i class="fa fa-check"></i><b>5.7.1</b> Detecting Heteroscedasticity</a></li>
<li class="chapter" data-level="5.7.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec572"><i class="fa fa-check"></i><b>5.7.2</b> Heteroscedasticity-Consistent Standard Errors</a></li>
<li class="chapter" data-level="5.7.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec573"><i class="fa fa-check"></i><b>5.7.3</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="5.7.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec574"><i class="fa fa-check"></i><b>5.7.4</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec58"><i class="fa fa-check"></i><b>5.8</b> Further Reading and References</a></li>
<li class="chapter" data-level="5.9" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec59"><i class="fa fa-check"></i><b>5.9</b> Exercises</a></li>
<li class="chapter" data-level="5.10" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec510"><i class="fa fa-check"></i><b>5.10</b> Technical Supplements for Chapter 5</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5101"><i class="fa fa-check"></i><b>5.10.1</b> Projection Matrix</a></li>
<li class="chapter" data-level="5.10.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5102"><i class="fa fa-check"></i><b>5.10.2</b> Leave One Out Statistics</a></li>
<li class="chapter" data-level="5.10.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5103"><i class="fa fa-check"></i><b>5.10.3</b> Omitting Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTextDev/RegressionSpanish/" target="blank">Spanish Regression on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Regression Modeling with Actuarial and Financial Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-and-the-normal-distribution" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Regression and the Normal Distribution<a href="regression-and-the-normal-distribution.html#regression-and-the-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Chapter Preview</em>. Regression analysis is a statistical method that is widely used in many fields of study, with actuarial science being no exception. This chapter provides an introduction to the role of the normal distribution in regression, the use of logarithmic transformations in specifying regression relationships and the sampling basis that is critical for inferring regression results to broad populations of interest.</p>
<div id="Sec11" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> What is Regression Analysis?<a href="regression-and-the-normal-distribution.html#Sec11" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Statistics is about data. As a discipline, it is about the collection, summarization and analysis of data to make statements about the real world. When analysts collect data, they are really collecting information that is quantified, that is, transformed to a numerical scale. There are easy, well-understood rules for reducing
the data, using either numerical or graphical summary measures. These summary measures can then be linked to a theoretical representation, or model, of the data. With a model that is calibrated by data, statements about the world can be made.</p>
<p>Statistical methods have had a major impact on several fields of study.</p>
<ul>
<li>In the area of data collection, the careful design of <em>sample surveys</em> is crucial to market research groups and to the auditing procedures of accounting firms.</li>
<li><em>Experimental design</em> is a second subdiscipline devoted to data collection. The focus of experimental design is on constructing methods of data collection that will extract information in the most efficient way possible. This is especially important in fields such as agriculture and engineering where each observation is expensive, possibly costing millions of dollars.</li>
<li>Other applied statistical methods focus on managing and predicting data. <em>Process control</em> deals with monitoring a process over time and deciding when intervention is most fruitful. Process control helps manage
the quality of goods produced by manufacturers.</li>
<li><em>Forecasting</em> is about extrapolating a process into the future, whether it be sales of a product or movements of an interest rate.</li>
</ul>
<p>Regression analysis is a statistical method used to analyze data. As we will see, the distinguishing feature of this method is the ability to make statements about variables after having controlled for values of known explanatory variables. Important as other methods are, it is regression analysis that has been most influential. To illustrate, an index of business journals, ABI/INFORM, lists over <em>twenty-four thousand</em> articles using
regression techniques over the thirty-year period 1978-2007. And these are only the applications that were considered innovative enough to be published in scholarly reviews!</p>
<p>Regression analysis of data is so pervasive in modern business that it is easy to overlook the fact that the methodology is barely over 120 years old. Scholars attribute the birth of regression to the 1885 presidential address of Sir Francis Galton to the anthropological section of the British Association of the Advancement of Sciences. In that address, described in Stigler (1986), Galton provided a description of regression and linked it to <em>normal curve</em> theory. His discovery arose from his studies of properties of natural selection and inheritance.</p>
<p>To illustrate a data set that can be analyzed using regression methods, Table <a href="regression-and-the-normal-distribution.html#tab:Tab11">1.1</a> displays some data included in Galton’s 1885 paper. This table displays the heights of 928 adult children, classified by an index of their parents’ height. Here, all female heights were multiplied by 1.08, and the index was created by taking the average of the father’s height and rescaled mother’s height. Galton was aware that the parents’ and the adult child’s height could each be adequately approximated by a normal curve. In developing regression
analysis, he provided a single model for the joint distribution of heights.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab11">Table 1.1: </span><strong>Galtons 1885 Regression Data</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
&lt;64.0
</th>
<th style="text-align:right;">
64.5
</th>
<th style="text-align:right;">
65.5
</th>
<th style="text-align:right;">
66.5
</th>
<th style="text-align:right;">
67.5
</th>
<th style="text-align:right;">
68.5
</th>
<th style="text-align:right;">
69.5
</th>
<th style="text-align:right;">
70.5
</th>
<th style="text-align:right;">
71.5
</th>
<th style="text-align:right;">
72.5
</th>
<th style="text-align:right;">
&gt;73.0
</th>
<th style="text-align:right;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
&gt;73.7
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
5
</td>
<td style="text-align:right;width: 0.9cm; ">
3
</td>
<td style="text-align:right;width: 0.9cm; ">
2
</td>
<td style="text-align:right;width: 0.9cm; ">
4
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
0
</td>
<td style="text-align:right;">
14
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
73.2
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
3
</td>
<td style="text-align:right;width: 0.9cm; ">
4
</td>
<td style="text-align:right;width: 0.9cm; ">
3
</td>
<td style="text-align:right;width: 0.9cm; ">
2
</td>
<td style="text-align:right;width: 0.9cm; ">
2
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
3
</td>
<td style="text-align:right;">
17
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
72.2
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
4
</td>
<td style="text-align:right;width: 0.9cm; ">
4
</td>
<td style="text-align:right;width: 0.9cm; ">
11
</td>
<td style="text-align:right;width: 0.9cm; ">
4
</td>
<td style="text-align:right;width: 0.9cm; ">
9
</td>
<td style="text-align:right;width: 0.9cm; ">
7
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
1
</td>
<td style="text-align:right;">
41
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
71.2
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
2
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
11
</td>
<td style="text-align:right;width: 0.9cm; ">
18
</td>
<td style="text-align:right;width: 0.9cm; ">
20
</td>
<td style="text-align:right;width: 0.9cm; ">
7
</td>
<td style="text-align:right;width: 0.9cm; ">
4
</td>
<td style="text-align:right;width: 0.9cm; ">
2
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
0
</td>
<td style="text-align:right;">
64
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
70.2
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
5
</td>
<td style="text-align:right;width: 0.9cm; ">
4
</td>
<td style="text-align:right;width: 0.9cm; ">
19
</td>
<td style="text-align:right;width: 0.9cm; ">
21
</td>
<td style="text-align:right;width: 0.9cm; ">
25
</td>
<td style="text-align:right;width: 0.9cm; ">
14
</td>
<td style="text-align:right;width: 0.9cm; ">
10
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
0
</td>
<td style="text-align:right;">
99
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
69.2
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
2
</td>
<td style="text-align:right;width: 0.9cm; ">
7
</td>
<td style="text-align:right;width: 0.9cm; ">
13
</td>
<td style="text-align:right;width: 0.9cm; ">
38
</td>
<td style="text-align:right;width: 0.9cm; ">
48
</td>
<td style="text-align:right;width: 0.9cm; ">
33
</td>
<td style="text-align:right;width: 0.9cm; ">
18
</td>
<td style="text-align:right;width: 0.9cm; ">
5
</td>
<td style="text-align:right;width: 0.9cm; ">
2
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
0
</td>
<td style="text-align:right;">
167
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
68.2
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
7
</td>
<td style="text-align:right;width: 0.9cm; ">
14
</td>
<td style="text-align:right;width: 0.9cm; ">
28
</td>
<td style="text-align:right;width: 0.9cm; ">
34
</td>
<td style="text-align:right;width: 0.9cm; ">
20
</td>
<td style="text-align:right;width: 0.9cm; ">
12
</td>
<td style="text-align:right;width: 0.9cm; ">
3
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
0
</td>
<td style="text-align:right;">
120
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
67.2
</td>
<td style="text-align:right;width: 0.9cm; ">
2
</td>
<td style="text-align:right;width: 0.9cm; ">
5
</td>
<td style="text-align:right;width: 0.9cm; ">
11
</td>
<td style="text-align:right;width: 0.9cm; ">
17
</td>
<td style="text-align:right;width: 0.9cm; ">
38
</td>
<td style="text-align:right;width: 0.9cm; ">
31
</td>
<td style="text-align:right;width: 0.9cm; ">
27
</td>
<td style="text-align:right;width: 0.9cm; ">
3
</td>
<td style="text-align:right;width: 0.9cm; ">
4
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
0
</td>
<td style="text-align:right;">
138
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
66.2
</td>
<td style="text-align:right;width: 0.9cm; ">
2
</td>
<td style="text-align:right;width: 0.9cm; ">
5
</td>
<td style="text-align:right;width: 0.9cm; ">
11
</td>
<td style="text-align:right;width: 0.9cm; ">
17
</td>
<td style="text-align:right;width: 0.9cm; ">
36
</td>
<td style="text-align:right;width: 0.9cm; ">
25
</td>
<td style="text-align:right;width: 0.9cm; ">
17
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
3
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
0
</td>
<td style="text-align:right;">
117
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
65.2
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
7
</td>
<td style="text-align:right;width: 0.9cm; ">
2
</td>
<td style="text-align:right;width: 0.9cm; ">
15
</td>
<td style="text-align:right;width: 0.9cm; ">
16
</td>
<td style="text-align:right;width: 0.9cm; ">
4
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
0
</td>
<td style="text-align:right;">
48
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
64.2
</td>
<td style="text-align:right;width: 0.9cm; ">
4
</td>
<td style="text-align:right;width: 0.9cm; ">
4
</td>
<td style="text-align:right;width: 0.9cm; ">
5
</td>
<td style="text-align:right;width: 0.9cm; ">
5
</td>
<td style="text-align:right;width: 0.9cm; ">
14
</td>
<td style="text-align:right;width: 0.9cm; ">
11
</td>
<td style="text-align:right;width: 0.9cm; ">
16
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
0
</td>
<td style="text-align:right;">
59
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
63.2
</td>
<td style="text-align:right;width: 0.9cm; ">
2
</td>
<td style="text-align:right;width: 0.9cm; ">
4
</td>
<td style="text-align:right;width: 0.9cm; ">
9
</td>
<td style="text-align:right;width: 0.9cm; ">
3
</td>
<td style="text-align:right;width: 0.9cm; ">
5
</td>
<td style="text-align:right;width: 0.9cm; ">
7
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
0
</td>
<td style="text-align:right;">
32
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
62.2
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
3
</td>
<td style="text-align:right;width: 0.9cm; ">
3
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
0
</td>
<td style="text-align:right;">
7
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
&lt;61.2
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
1
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; ">
0
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
0
</td>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Total
</td>
<td style="text-align:right;width: 0.9cm; ">
14
</td>
<td style="text-align:right;width: 0.9cm; ">
23
</td>
<td style="text-align:right;width: 0.9cm; ">
66
</td>
<td style="text-align:right;width: 0.9cm; ">
78
</td>
<td style="text-align:right;width: 0.9cm; ">
211
</td>
<td style="text-align:right;width: 0.9cm; ">
219
</td>
<td style="text-align:right;width: 0.9cm; ">
183
</td>
<td style="text-align:right;width: 0.9cm; ">
68
</td>
<td style="text-align:right;width: 0.9cm; ">
43
</td>
<td style="text-align:right;width: 0.9cm; ">
19
</td>
<td style="text-align:right;width: 0.9cm; border-right:1px solid;">
4
</td>
<td style="text-align:right;">
928
</td>
</tr>
</tbody>
</table>
<p><em>Source</em>: Stigler (1986)</p>
<p>Table <a href="regression-and-the-normal-distribution.html#tab:Tab11">1.1</a> shows that much of the information concerning the height of an adult child can be attributed to, or ‘explained,’ in terms of the parents’ height. Thus, we use the term <em>explanatory variable</em> for measurements that provide information about a variable of interest. Regression analysis is a method to quantify the relationship between a variable of interest and explanatory variables. The methodology used to study the data in Table <a href="regression-and-the-normal-distribution.html#tab:Tab11">1.1</a> can also be used to study actuarial and other risk management problems, the thesis of this book.</p>
</div>
<div id="Sec12" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Fitting Data to a Normal Distribution<a href="regression-and-the-normal-distribution.html#Sec12" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Historically, the normal distribution had a pivotal role in the development of regression analysis. It continues to play an important role, although we will be interested in extending regression ideas to highly ‘non-normal’ data.</p>
<p>Formally, the normal curve is defined by the function
<span class="math display" id="eq:eq11">\[\begin{equation}
\mathrm{f}(y)=\frac{1}{\sigma \sqrt{2\pi }}\exp \left( -\frac{1}{2\sigma ^{2}%
}\left( y-\mu \right) ^{2}\right) .
\tag{1.1}
\end{equation}\]</span></p>
<p>This curve is a probability density function with the whole real line as its domain. From equation <a href="regression-and-the-normal-distribution.html#eq:eq11">(1.1)</a>, we see that the curve is symmetric about <span class="math inline">\(\mu\)</span> (the mean and median). The degree of peakedness is controlled by the parameter <span class="math inline">\(\sigma ^{2}\)</span>. These two parameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma ^{2}\)</span>, are
known as the <em>location</em> and <em>scale parameters</em>, respectively. Appendix A3.1 provides additional details about this curve, including a graph and tables of its cumulative distribution that we will use throughout the text.</p>
<p>The normal curve is also depicted in Figure <a href="regression-and-the-normal-distribution.html#fig:Fig11">1.1</a>, a display of a now out-of-date German currency note, the ten Deutsche Mark. This note contains the image of German Carl Gauss, an eminent mathematician whose name is often linked with the normal curve (it is sometimes referred to as the <em>Gaussian curve</em>). Gauss
developed the normal curve in connection with the theory of least squares for fitting curves to data in 1809, about the same time as related work by the French scientist Pierre LaPlace. According to Stigler (1986), there was quite a bit of acrimony between these two scientists about the priority of discovery! The normal curve was
first used as an approximation to histograms of data around 1835 by Adolph Quetelet, a Belgian mathematician and social scientist. Like many good things, the normal curve had been around for some time, since about 1720 when Abraham de Moivre derived it for his work on modeling games of chance. The normal curve is popular because it is
easy to use and has proved to be successful in many applications.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig11"></span>
<img src="Chapters/Chapter1/GreyScale256TenDM.jpg" alt="Ten Deutsche Mark. German currency featuring scientist Gauss and the normal curve." width="60%" />
<p class="caption">
Figure 1.1: <strong>Ten Deutsche Mark</strong>. German currency featuring scientist Gauss and the normal curve.
</p>
</div>
<p><strong>Example: Massachusetts Bodily Injury Claims.</strong> For our first look at fitting the normal curve to a set of data, we consider data from Rempala and Derrig (2005). They considered claims arising from automobile bodily injury insurance coverages. These are amounts incurred for outpatient medical treatments that arise from
automobile accidents, typically sprains, broken collarbones and the like. The data consists of a sample of 272 claims from Massachusetts that were closed in 2001 (by ‘closed,’ we mean that the claim is settled and no additional liabilities can arise from the same accident). Rempala and Derrig were interested in developing
procedures for handling mixtures of ‘typical’ claims and others from providers who reported claims fraudulently. For this sample, we consider only those typical claims, ignoring the potentially fraudulent ones.</p>
<p>Table <a href="regression-and-the-normal-distribution.html#tab:Tab12">1.2</a> provides several statistics that summarize different aspects of the distribution. Claim amounts are in units of logarithms of thousands of dollars. The average logarithmic claim is 0.481, corresponding to $1,617.77 (=1000 <span class="math inline">\(\exp(0.481)\)</span>). The smallest and largest claims are -3.101 (45 dollars) and 3.912 (50,000 dollars), respectively.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab12">Table 1.2: </span><strong>Summary Statistics of Massachusetts Automobile Bodily Injury Claims</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Number
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Median
</th>
<th style="text-align:right;">
Standard Deviation
</th>
<th style="text-align:right;">
Minimum
</th>
<th style="text-align:right;">
Maximum
</th>
<th style="text-align:right;">
25th Percentile
</th>
<th style="text-align:right;">
75th Percentile
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Claims
</td>
<td style="text-align:right;width: 1.1cm; ">
272
</td>
<td style="text-align:right;width: 1.1cm; ">
0.481
</td>
<td style="text-align:right;width: 1.1cm; ">
0.793
</td>
<td style="text-align:right;width: 1.1cm; ">
1.101
</td>
<td style="text-align:right;width: 1.1cm; ">
-3.101
</td>
<td style="text-align:right;width: 1.1cm; ">
3.912
</td>
<td style="text-align:right;width: 1.1cm; ">
-0.114
</td>
<td style="text-align:right;width: 1.1cm; ">
1.168
</td>
</tr>
</tbody>
</table>
<p>For completeness, here are a few definitions. The <em>sample</em> is the set of data available for analysis, denoted by <span class="math inline">\(y_1,\ldots,y_n\)</span>. Here, <span class="math inline">\(n\)</span> is the number of observations, <span class="math inline">\(y_1\)</span> represents the first observation, <span class="math inline">\(y_2\)</span> the second, and so on up to <span class="math inline">\(y_n\)</span> for the <span class="math inline">\(nth\)</span> observation. Here are a few important summary statistics.</p>
<div class="blackbox">
<p><strong>Basic Summary Statistics</strong></p>
<ul>
<li>The <em>mean</em> is the average of observations, that is, the sum of the observations divided by the number of units. Using algebraic notation, the mean is
<span class="math display">\[
\overline{y}=\frac{1}{n}\left( y_1 + \cdots + y_n \right) =
\frac{1}{n} \sum_{i=1}^{n} y_i.
\]</span></li>
<li>The <em>median</em> is the middle observation when the observations are ordered by size. That is, it is the observation at which 50% are below it (and 50% are above it).</li>
<li>The <em>standard deviation</em> is a measure of the spread, or scale, of the distribution. It is computed as
<span class="math display">\[
s_y = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}\left(
y_i-\overline{y}\right) ^{2}} .
\]</span></li>
<li>A <em>percentile</em> is a number at which a specified fraction of the observations is below it, when the observations are ordered by size. For example, the 25th percentile is that number so that 25% of observations are below it.</li>
</ul>
</div>
<p>To help visualize the distribution, Figure <a href="regression-and-the-normal-distribution.html#fig:Fig12">1.2</a> displays a <em>histogram</em> of the data. Here, the height of each rectangle shows the relative frequency of observations that fall within the range given by its base. The histogram provides a quick visual impression of the distribution; it shows that the range of the data is approximately (-4,4), the central tendency is slightly greater than zero and that the distribution is roughly symmetric.</p>
<p><strong>Normal Curve Approximation.</strong> Figure <a href="regression-and-the-normal-distribution.html#fig:Fig12">1.2</a> also shows a normal curve superimposed, using <span class="math inline">\(\overline{y}\)</span> for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(s_y^{2}\)</span> for <span class="math inline">\(\sigma ^{2}\)</span>. With the normal curve, only two quantities (<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma ^{2}\)</span>) are required to summarize the
entire distribution. For example, Table <a href="regression-and-the-normal-distribution.html#tab:Tab12">1.2</a> shows that 1.168 is the 75th percentile, which is approximately the 204th (<span class="math inline">\(=0.75\times 272\)</span>) largest observation from the entire sample. From the equation <a href="regression-and-the-normal-distribution.html#eq:eq11">(1.1)</a> normal distribution,
we have that <span class="math inline">\(z=(y-\mu )/\sigma\)</span> is a standard normal, of which 0.675 is the 75th percentile. Thus, <span class="math inline">\(\overline{y}+0.675s_y=\)</span> <span class="math inline">\(0.481+0.675\times 1.101=1.224\)</span> is the 75th percentile using the normal curve approximation.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig12"></span>
<img src="RegressionMarkdown_files/figure-html/Fig12-1.png" alt="Bodily Injury Relative Frequency with Normal Curve Superimposed." width="50%" />
<p class="caption">
Figure 1.2: <strong>Bodily Injury Relative Frequency with Normal Curve Superimposed</strong>.
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Fig12.Hide" href="javascript:togglecode('toggleCode.Fig12.Hide','displayCode.Fig12.Hide');"><i><strong>R Code to Produce Figure 1.2</strong></i></a>
</h5>
<div id="toggleCode.Fig12.Hide" style="display: none">
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="regression-and-the-normal-distribution.html#cb1-1" tabindex="-1"></a>injury <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;CSVData/MassBodilyInjury.csv&quot;</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb1-2"><a href="regression-and-the-normal-distribution.html#cb1-2" tabindex="-1"></a>injury2<span class="ot">&lt;-</span><span class="fu">subset</span>(injury, providerA <span class="sc">!=</span> <span class="dv">0</span> )</span>
<span id="cb1-3"><a href="regression-and-the-normal-distribution.html#cb1-3" tabindex="-1"></a>LOGCLAIMS<span class="ot">&lt;-</span><span class="fu">log</span>(injury2<span class="sc">$</span>claims)</span>
<span id="cb1-4"><a href="regression-and-the-normal-distribution.html#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="regression-and-the-normal-distribution.html#cb1-5" tabindex="-1"></a><span class="co"># FIGURE 1.2 </span></span>
<span id="cb1-6"><a href="regression-and-the-normal-distribution.html#cb1-6" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="fl">0.01</span>)</span>
<span id="cb1-7"><a href="regression-and-the-normal-distribution.html#cb1-7" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x, <span class="at">mean=</span><span class="fu">mean</span>(LOGCLAIMS), <span class="at">sd=</span><span class="fu">sqrt</span>(<span class="fu">var</span>(LOGCLAIMS)))</span>
<span id="cb1-8"><a href="regression-and-the-normal-distribution.html#cb1-8" tabindex="-1"></a><span class="fu">hist</span>(LOGCLAIMS, <span class="at">freq=</span><span class="cn">FALSE</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">las=</span><span class="dv">1</span>)</span>
<span id="cb1-9"><a href="regression-and-the-normal-distribution.html#cb1-9" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Density&quot;</span>, <span class="at">side=</span><span class="dv">2</span>, <span class="at">at=</span>.<span class="dv">35</span>,<span class="at">las=</span><span class="dv">1</span>, <span class="at">adj=</span>.<span class="dv">7</span>,<span class="at">cex=</span><span class="fl">1.4</span>)</span>
<span id="cb1-10"><a href="regression-and-the-normal-distribution.html#cb1-10" tabindex="-1"></a><span class="fu">lines</span>(x,y)</span></code></pre></div>
</div>
<p><strong>Box Plot.</strong> A quick visual inspection of a variable’s
distribution can reveal some surprising features that are hidden by
statistics, numerical summary measures. The <em>box plot</em>, also known as a ‘box and whiskers’ plot, is one
such graphical device. Figure <a href="regression-and-the-normal-distribution.html#fig:Fig13">1.3</a> illustrates a box
plot for the bodily injury claims. Here, the box captures the middle
50% of the data, with the three horizontal lines corresponding to
the 75th, 50th and 25th percentiles, reading from top to bottom.</p>
<p>The horizontal lines above and below the box are the ‘whiskers.’
The upper whisker is 1.5 times the <em>interquartile range</em> (the
difference between the 75th and 25th percentiles) above the 75th
percentile. Similarly, the lower whisker is 1.5 times the
interquartile range below the 25th percentile. Individual
observations outside the whiskers are denoted by small circular
plotting symbols, and are referred to as ‘outliers.’</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig13"></span>
<img src="RegressionMarkdown_files/figure-html/Fig13-1.png" alt="Box plot of bodily injury claims." width="50%" />
<p class="caption">
Figure 1.3: <strong>Box plot of bodily injury claims.</strong>
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Fig13.Hide" href="javascript:togglecode('toggleCode.Fig13.Hide','displayCode.Fig13.Hide');"><i><strong>R Code to Produce Figure 1.3</strong></i></a>
</h5>
<div id="toggleCode.Fig13.Hide" style="display: none">
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="regression-and-the-normal-distribution.html#cb2-1" tabindex="-1"></a><span class="fu">boxplot</span>(LOGCLAIMS, <span class="at">boxwex=</span>.<span class="dv">7</span>, <span class="at">las=</span><span class="dv">1</span>)</span>
<span id="cb2-2"><a href="regression-and-the-normal-distribution.html#cb2-2" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">1</span>, .<span class="dv">57</span>, <span class="st">&quot;median&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>)</span>
<span id="cb2-3"><a href="regression-and-the-normal-distribution.html#cb2-3" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">1.36</span>, <span class="sc">-</span><span class="fl">0.2</span>, <span class="st">&quot;25th percentile&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>)</span>
<span id="cb2-4"><a href="regression-and-the-normal-distribution.html#cb2-4" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">1.36</span>, <span class="fl">1.1</span>, <span class="st">&quot;75th percentile&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>)</span>
<span id="cb2-5"><a href="regression-and-the-normal-distribution.html#cb2-5" tabindex="-1"></a><span class="fu">arrows</span>(<span class="fl">1.05</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="fl">1.05</span>, <span class="sc">-</span><span class="fl">3.3</span>, <span class="at">code=</span><span class="dv">3</span>, <span class="at">angle=</span><span class="dv">20</span>, <span class="at">length=</span><span class="fl">0.1</span>)</span>
<span id="cb2-6"><a href="regression-and-the-normal-distribution.html#cb2-6" tabindex="-1"></a><span class="co">#arrows(1.05, -2, 1.05, -3.3, col=&quot;blue&quot;, code=3, angle=20, length=0.1)</span></span>
<span id="cb2-7"><a href="regression-and-the-normal-distribution.html#cb2-7" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">1.15</span>, <span class="sc">-</span><span class="fl">2.5</span>, <span class="st">&quot;outliers&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>)</span>
<span id="cb2-8"><a href="regression-and-the-normal-distribution.html#cb2-8" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">1.13</span>, <span class="fl">3.9</span>, <span class="st">&quot;outlier&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>)</span></code></pre></div>
</div>
<p>Graphs are powerful tools; they allow analysts to readily visualize
nonlinear relationships that are hard to comprehend when expressed
verbally or by mathematical formula. However, by their very
flexibility, graphs can also readily deceive the analyst. Chapter 21
will underscore this point. For example, Figure
<a href="regression-and-the-normal-distribution.html#fig:Fig14">1.4</a> is a re-drawing of Figure <a href="regression-and-the-normal-distribution.html#fig:Fig12">1.2</a>; the
difference is that Figure <a href="regression-and-the-normal-distribution.html#fig:Fig14">1.4</a> uses more, and
finer, rectangles. This finer analysis reveals the asymmetric nature
of the sample distribution that was not evident in Figure
<a href="regression-and-the-normal-distribution.html#fig:Fig12">1.2</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig14"></span>
<img src="RegressionMarkdown_files/figure-html/Fig14-1.png" alt="Redrawing of Figure 1.2 with an increased number of rectangles." width="50%" />
<p class="caption">
Figure 1.4: <strong>Redrawing of Figure 1.2 with an increased number of rectangles.</strong>
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Fig14.Hide" href="javascript:togglecode('toggleCode.Fig14.Hide','displayCode.Fig14.Hide');"><i><strong>R Code to Produce Figure 1.4</strong></i></a>
</h5>
<div id="toggleCode.Fig14.Hide" style="display: none">
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="regression-and-the-normal-distribution.html#cb3-1" tabindex="-1"></a><span class="fu">hist</span>(LOGCLAIMS, <span class="at">freq=</span><span class="cn">FALSE</span>, <span class="at">nclass=</span><span class="dv">32</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">las=</span><span class="dv">1</span>)</span>
<span id="cb3-2"><a href="regression-and-the-normal-distribution.html#cb3-2" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Density&quot;</span>, <span class="at">side=</span><span class="dv">2</span>, <span class="at">at=</span>.<span class="dv">75</span>,<span class="at">las=</span><span class="dv">1</span>, <span class="at">adj=</span>.<span class="dv">7</span>,<span class="at">cex=</span><span class="fl">1.1</span>)</span>
<span id="cb3-3"><a href="regression-and-the-normal-distribution.html#cb3-3" tabindex="-1"></a><span class="fu">lines</span>(x,y)</span></code></pre></div>
</div>
<p><strong>Quantile-Quantile Plots.</strong> Increasing the number of
rectangles can unmask features that were not previously apparent;
however, there are in general fewer observations per rectangle
meaning that the uncertainty of the relative frequency estimate
increases. This represents a trade-off. Instead of forcing the
analyst to make an arbitrary decision about the number of
rectangles, an alternative is to use a graphical device for
comparing a distribution to another known as a
<em>quantile-quantile</em>, or <em>qq</em>, plot.</p>
<p>Figure <a href="regression-and-the-normal-distribution.html#fig:Fig15">1.5</a> illustrates a <span class="math inline">\(qq\)</span> plot for the bodily
injury data using the normal curve as a reference distribution. For
each point, the vertical axis gives the quantile using the sample
distribution. The horizontal axis gives the corresponding quantity
using the normal curve. For example, earlier we considered the 75th
percentile point. This point appears as (1.168, 0.675) on the graph.
To interpret a <span class="math inline">\(qq\)</span> plot, if the quantile points lie along the
superimposed line, then the sample and the normal reference
distribution have the same shape. (This line is defined by
connecting the 75th and 25th percentiles.)</p>
<p>In Figure <a href="regression-and-the-normal-distribution.html#fig:Fig15">1.5</a>, the small sample percentiles are
consistently smaller than the corresponding values from the standard
normal, indicating that the distribution is skewed to the left. The
difference in values at the ends of the distribution are due to the
outliers noted earlier that could also be interpreted as the sample
distribution having larger tails than the normal reference
distribution.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig15"></span>
<img src="RegressionMarkdown_files/figure-html/Fig15-1.png" alt="A \(qq\) plot of Bodily Injury Claims, using a normal reference distribution." width="50%" />
<p class="caption">
Figure 1.5: <strong>A <span class="math inline">\(qq\)</span> plot of Bodily Injury Claims, using a normal reference distribution.</strong>
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Fig15.Hide" href="javascript:togglecode('toggleCode.Fig15.Hide','displayCode.Fig15.Hide');"><i><strong>R Code to Produce Figure 1.5</strong></i></a>
</h5>
<div id="toggleCode.Fig15.Hide" style="display: none">
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="regression-and-the-normal-distribution.html#cb4-1" tabindex="-1"></a><span class="fu">qqnorm</span>(LOGCLAIMS, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">las=</span><span class="dv">1</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb4-2"><a href="regression-and-the-normal-distribution.html#cb4-2" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Sample Quantiles&quot;</span>, <span class="at">side=</span><span class="dv">2</span>, <span class="at">at=</span><span class="fl">4.5</span>, <span class="at">las=</span><span class="dv">1</span>,<span class="at">cex=</span><span class="fl">1.1</span>,<span class="at">adj=</span>.<span class="dv">4</span>)</span>
<span id="cb4-3"><a href="regression-and-the-normal-distribution.html#cb4-3" tabindex="-1"></a><span class="fu">qqline</span>(LOGCLAIMS)</span></code></pre></div>
</div>
</div>
<div id="Sec13" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Power Transforms<a href="regression-and-the-normal-distribution.html#Sec13" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the Section <a href="regression-and-the-normal-distribution.html#Sec12">1.2</a> example, we considered claims without justifying
the use of the logarithmic scaling. When analyzing variables such as
assets of firms, wages of individuals and housing prices of
households in business and economic applications, it is common to
consider logarithmic instead of the original units. A log transform
retains the original ordering (for example, large wages remain large
on the log wage scale) but serves to ‘pull in’ extreme values of
the distribution.</p>
<p>To illustrate, Figure <a href="regression-and-the-normal-distribution.html#fig:Fig16">1.6</a> shows the bodily injury claims
distribution in (thousands of) dollars. In order to graph the data
meaningfully, the largest observation ($50,000) was removed prior
to making this plot. Even with this observation removed, Figure
<a href="regression-and-the-normal-distribution.html#fig:Fig16">1.6</a> shows that the distribution is heavily lop-sided to
the right, with several large values of claims appearing.</p>
<p>Distributions that are lopsided in one direction or the other are
known as <em>skewed</em>. Figure <a href="regression-and-the-normal-distribution.html#fig:Fig16">1.6</a> is an example of a
distribution skewed to the right, or positively skewed. Here, the
tail of the distribution on the right is longer and there is a
greater concentration of mass to the left. In contrast, a
left-skewed, or negatively skewed distribution, has a longer tail on
the left and a greater concentration of mass to the right. Many
insurance claims distributions are right-skewed (see the text by
Klugman, Panjer and Willmot, 2008, for extensive discussions). As we
saw in Figures <a href="regression-and-the-normal-distribution.html#fig:Fig14">1.4</a> and <a href="regression-and-the-normal-distribution.html#fig:Fig15">1.5</a>, a
logarithmic transformation yields a distribution that is only mildly
skewed to the left.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig16"></span>
<img src="RegressionMarkdown_files/figure-html/Fig16-1.png" alt="Distribution of Bodily Injury Claims. Observations are in (thousands of) dollars with the largest observation omitted.." width="50%" />
<p class="caption">
Figure 1.6: <strong>Distribution of Bodily Injury Claims. Observations are in (thousands of) dollars with the largest observation omitted.</strong>.
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Fig16.Hide" href="javascript:togglecode('toggleCode.Fig16.Hide','displayCode.Fig16.Hide');"><i><strong>R Code to Produce Figure 1.6</strong></i></a>
</h5>
<div id="toggleCode.Fig16.Hide" style="display: none">
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="regression-and-the-normal-distribution.html#cb5-1" tabindex="-1"></a>injury3 <span class="ot">=</span> <span class="fu">subset</span>(injury, claims <span class="sc">&lt;</span> <span class="dv">25</span> )</span>
<span id="cb5-2"><a href="regression-and-the-normal-distribution.html#cb5-2" tabindex="-1"></a>CLAIMS25 <span class="ot">&lt;-</span> injury3<span class="sc">$</span>claims</span>
<span id="cb5-3"><a href="regression-and-the-normal-distribution.html#cb5-3" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">4.2</span>,<span class="dv">4</span>,<span class="fl">1.2</span>,.<span class="dv">2</span>),<span class="at">cex=</span><span class="fl">1.1</span>)</span>
<span id="cb5-4"><a href="regression-and-the-normal-distribution.html#cb5-4" tabindex="-1"></a><span class="fu">hist</span>(CLAIMS25, <span class="at">freq=</span><span class="cn">FALSE</span>,  <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">las=</span><span class="dv">1</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;CLAIMS&quot;</span>)</span>
<span id="cb5-5"><a href="regression-and-the-normal-distribution.html#cb5-5" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Density&quot;</span>, <span class="at">side=</span><span class="dv">2</span>, <span class="at">at=</span>.<span class="dv">28</span>, <span class="at">las=</span><span class="dv">1</span>,<span class="at">cex=</span><span class="fl">1.1</span>)</span></code></pre></div>
</div>
<p>Logarithmic transformations are used extensively in applied
statistics work. One advantage is that they serve to symmetrize
distributions that are skewed. More generally, we consider
<em>power transforms</em>, also known as the <em>Box-Cox family of transforms</em>. Within this family of transforms, in lieu of using the
response <span class="math inline">\(y\)</span>, we use a transformed, or rescaled version, <span class="math inline">\(y^{\lambda}\)</span>. Here, the power <span class="math inline">\(\lambda\)</span> (lambda, a Greek ‘el’) is a number
that may be user specified. Typical values of <span class="math inline">\(\lambda\)</span> that are
used in practice are <span class="math inline">\(\lambda\)</span>=1, 1/2, 0 or -1. When we use
<span class="math inline">\(\lambda =0\)</span>, we mean <span class="math inline">\(\ln (y)\)</span>, that is, the natural logarithmic
transform. More formally, the Box-Cox family can be expressed as
<span class="math display">\[
y^{(\lambda )}=\left\{
\begin{array}{ll}
\frac{y^{\lambda }-1}{\lambda } &amp; \lambda \neq 0 \\
\ln (y) &amp; \lambda =0
\end{array}
\right. .
\]</span></p>
<p>As we will see, because regression estimates are not affected by
location and scale shifts, in practice we do not need to subtract
one nor divide by <span class="math inline">\(\lambda\)</span> when rescaling the response. The
advantage of the above expression is that, if we let <span class="math inline">\(\lambda\)</span>
approach 0, then <span class="math inline">\(y^{(\lambda )}\)</span> approaches <span class="math inline">\(\ln (y)\)</span>, from some
straightforward calculus arguments.</p>
<p>To illustrate the usefulness of transformations, we simulated 500
observations from a chi-square distribution with two degrees of
freedom. Appendix A3.2 introduces this distribution (that we will
encounter again later in studying the behavior of test statistics).
The upper left panel of Figure <a href="regression-and-the-normal-distribution.html#fig:Fig17">1.7</a> shows the original
distribution is heavily skewed to the right. The other panels in
Figure <a href="regression-and-the-normal-distribution.html#fig:Fig17">1.7</a> show the data rescaled using the square
root, logarithmic and negative reciprocal transformations. The
logarithmic transformation, in the lower left panel, provides the
best approximation to symmetry for this example. The negative
reciprocal transformation is based on <span class="math inline">\(\lambda =-1\)</span>, and then
multiplying the rescaled observations by minus one, so that large
observations remain large.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig17"></span>
<img src="RegressionMarkdown_files/figure-html/Fig17-1.png" alt="500 simulated observations from a chi-square distribution. The upper left panel is based on the original distribution. The upper right corresponds to the square root transform, the lower left to the log transform and the lower right to the negative reciprocal transform." width="90%" />
<p class="caption">
Figure 1.7: <strong>500 simulated observations from a chi-square distribution.</strong> The upper left panel is based on the original distribution. The upper right corresponds to the square root transform, the lower left to the log transform and the lower right to the negative reciprocal transform.
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Fig17.Hide" href="javascript:togglecode('toggleCode.Fig17.Hide','displayCode.Fig17.Hide');"><i><strong>R Code to Produce Figure 1.7</strong></i></a>
</h5>
<div id="toggleCode.Fig17.Hide" style="display: none">
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="regression-and-the-normal-distribution.html#cb6-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1237</span>)</span>
<span id="cb6-2"><a href="regression-and-the-normal-distribution.html#cb6-2" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="dv">10000</span><span class="sc">*</span><span class="fu">rchisq</span>(<span class="dv">500</span><span class="sc">*</span><span class="dv">1</span>, <span class="at">df=</span><span class="dv">2</span>)</span>
<span id="cb6-3"><a href="regression-and-the-normal-distribution.html#cb6-3" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(X1)</span>
<span id="cb6-4"><a href="regression-and-the-normal-distribution.html#cb6-4" tabindex="-1"></a>X3 <span class="ot">&lt;-</span> <span class="fu">log</span>(X1)</span>
<span id="cb6-5"><a href="regression-and-the-normal-distribution.html#cb6-5" tabindex="-1"></a>X4 <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">1</span><span class="sc">/</span>X1</span>
<span id="cb6-6"><a href="regression-and-the-normal-distribution.html#cb6-6" tabindex="-1"></a></span>
<span id="cb6-7"><a href="regression-and-the-normal-distribution.html#cb6-7" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="at">cex=</span>.<span class="dv">75</span>, <span class="at">mar=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>,<span class="fl">1.5</span>,<span class="dv">0</span>))</span>
<span id="cb6-8"><a href="regression-and-the-normal-distribution.html#cb6-8" tabindex="-1"></a><span class="fu">hist</span>(X1, <span class="at">freq=</span><span class="cn">FALSE</span>,  <span class="at">nclass=</span><span class="dv">16</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">las=</span><span class="dv">1</span>, <span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">200000</span>),<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,.<span class="dv">00005</span>))</span>
<span id="cb6-9"><a href="regression-and-the-normal-distribution.html#cb6-9" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">2</span>, <span class="at">at=</span><span class="fu">seq</span>(<span class="dv">0</span>,.<span class="dv">00005</span>,.<span class="dv">00001</span>),<span class="at">las=</span><span class="dv">1</span>, <span class="at">cex=</span>.<span class="dv">3</span>, </span>
<span id="cb6-10"><a href="regression-and-the-normal-distribution.html#cb6-10" tabindex="-1"></a>  <span class="at">labels=</span><span class="fu">c</span>(<span class="st">&quot;0&quot;</span>, <span class="st">&quot;0.00001&quot;</span>, <span class="st">&quot;0.00002&quot;</span>,<span class="st">&quot;0.00003&quot;</span>, <span class="st">&quot;0.00004&quot;</span>, <span class="st">&quot;0.00005&quot;</span>))</span>
<span id="cb6-11"><a href="regression-and-the-normal-distribution.html#cb6-11" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Density&quot;</span>, <span class="at">side=</span><span class="dv">2</span>, <span class="at">at=</span>.<span class="dv">000055</span>, <span class="at">las=</span><span class="dv">1</span>, <span class="at">cex=</span>.<span class="dv">75</span>)</span>
<span id="cb6-12"><a href="regression-and-the-normal-distribution.html#cb6-12" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;y&quot;</span>, <span class="at">side=</span><span class="dv">1</span>, <span class="at">cex=</span>.<span class="dv">75</span>, <span class="at">line=</span><span class="dv">2</span>)</span>
<span id="cb6-13"><a href="regression-and-the-normal-distribution.html#cb6-13" tabindex="-1"></a></span>
<span id="cb6-14"><a href="regression-and-the-normal-distribution.html#cb6-14" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">4</span>,<span class="fl">1.5</span>,<span class="fl">0.2</span>))</span>
<span id="cb6-15"><a href="regression-and-the-normal-distribution.html#cb6-15" tabindex="-1"></a><span class="fu">hist</span>(X2, <span class="at">freq=</span><span class="cn">FALSE</span>,  <span class="at">nclass=</span><span class="dv">16</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">las=</span><span class="dv">1</span>,<span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">400</span>), <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,.<span class="dv">008</span>))</span>
<span id="cb6-16"><a href="regression-and-the-normal-distribution.html#cb6-16" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Density&quot;</span>, <span class="at">side=</span><span class="dv">2</span>, <span class="at">at=</span>.<span class="dv">0088</span>, <span class="at">las=</span><span class="dv">1</span>, <span class="at">cex=</span>.<span class="dv">75</span>)</span>
<span id="cb6-17"><a href="regression-and-the-normal-distribution.html#cb6-17" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Square root of y&quot;</span>, <span class="at">side=</span><span class="dv">1</span>, <span class="at">cex=</span>.<span class="dv">75</span>, <span class="at">line=</span><span class="dv">2</span>)</span>
<span id="cb6-18"><a href="regression-and-the-normal-distribution.html#cb6-18" tabindex="-1"></a></span>
<span id="cb6-19"><a href="regression-and-the-normal-distribution.html#cb6-19" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">3.2</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">0</span>))</span>
<span id="cb6-20"><a href="regression-and-the-normal-distribution.html#cb6-20" tabindex="-1"></a><span class="fu">hist</span>(X3,  <span class="at">freq=</span><span class="cn">FALSE</span>,  <span class="at">nclass=</span><span class="dv">16</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">las=</span><span class="dv">1</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,.<span class="dv">4</span>))</span>
<span id="cb6-21"><a href="regression-and-the-normal-distribution.html#cb6-21" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Density&quot;</span>, <span class="at">side=</span><span class="dv">2</span>, <span class="at">at=</span>.<span class="dv">44</span>, <span class="at">las=</span><span class="dv">1</span>, <span class="at">cex=</span>.<span class="dv">75</span>)</span>
<span id="cb6-22"><a href="regression-and-the-normal-distribution.html#cb6-22" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Logarithmic y&quot;</span>, <span class="at">side=</span><span class="dv">1</span>, <span class="at">cex=</span>.<span class="dv">75</span>, <span class="at">line=</span><span class="dv">2</span>)</span>
<span id="cb6-23"><a href="regression-and-the-normal-distribution.html#cb6-23" tabindex="-1"></a></span>
<span id="cb6-24"><a href="regression-and-the-normal-distribution.html#cb6-24" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">3.2</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="fl">0.2</span>))</span>
<span id="cb6-25"><a href="regression-and-the-normal-distribution.html#cb6-25" tabindex="-1"></a><span class="fu">hist</span>(X4, <span class="at">freq=</span><span class="cn">FALSE</span>,  <span class="at">nclass=</span><span class="dv">16</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">las=</span><span class="dv">1</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">100</span>))</span>
<span id="cb6-26"><a href="regression-and-the-normal-distribution.html#cb6-26" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Density&quot;</span>, <span class="at">side=</span><span class="dv">2</span>, <span class="at">at=</span><span class="dv">110</span>, <span class="at">las=</span><span class="dv">1</span>, <span class="at">cex=</span>.<span class="dv">75</span>)</span>
<span id="cb6-27"><a href="regression-and-the-normal-distribution.html#cb6-27" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Negative reciprocal of y&quot;</span>, <span class="at">side=</span><span class="dv">1</span>, <span class="at">cex=</span>.<span class="dv">75</span>, <span class="at">line=</span><span class="dv">2</span>)</span></code></pre></div>
</div>
</div>
<div id="Sec14" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Sampling and the Role of Normality<a href="regression-and-the-normal-distribution.html#Sec14" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A <em>statistic</em> is a summary measure of data, such as a mean,
median or percentile. Collections of statistics are very useful for
analysts, decision-makers and everyday consumers for understanding
massive amounts of data that represent complex situations. To this
point, our focus has been on introducing sensible techniques to
summarize variables; techniques that will be used repeatedly thoughout
this text. However, the true usefulness of the <em>discipline of
statistics</em> is its ability to say something about the unknown, not
merely to summarize information already available. To this end, we
need to make some fairly formal assumptions about the manner in
which the data are observed. As a science, a strong feature of
statistics (as a discipline) is the ability to critique these
assumptions and offer improved alternatives in specific situations.</p>
<p>It is customary to assume that the data are drawn from a larger
population that we are interested in describing. The process of
drawing the data is known as the <em>sampling</em>, or <em>data
generating, process</em>. We denote this sample as <span class="math inline">\(\{y_1,\ldots,y_n\}\)</span>.
So that we may critique, and modify, these sampling assumptions, we
list them below in detail:</p>
<p><span class="math display">\[
\begin{array}{l}
\hline \textbf{Basic Sampling Assumptions} \\ \hline
1. ~\mathrm{E~}y_i=\mu  \\
2. ~\mathrm{Var~}y_i=\sigma ^{2} \\
3. ~\{y_i\} \text{ are independent} \\
4. ~\{y_i\} \text{ are normally distributed}. \\ \hline
\end{array}
\]</span></p>
<p>In this basic set-up, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma ^{2}\)</span> serve as
<em>parameters</em> that describe the location and scale of the parent
population. The goal is to infer something sensible about them based
on statistics such as <span class="math inline">\(\overline{y}\)</span> and <span class="math inline">\(s_y^{2}\)</span>. For the third
assumption, we assume independence among the draws. In a sampling
scheme, this may be guaranteed by taking a simple random sample from
a population. The fourth assumption is not required for many
statistical inference procedures because central limit theorems
provide approximate normality for many statistics of interest.
However, a formal justification of some statistics, such as
<span class="math inline">\(t\)</span>-statistics, requires this additional assumption.</p>
<p>Section <a href="regression-and-the-normal-distribution.html#Sec18">1.8</a> provides an explicit statement of one
version of the central limit theorem, giving conditions under which
<span class="math inline">\(\overline{y}\)</span> is approximately normally distributed. This section
also discusses a related result, known as an <em>Edgeworth approximation</em>, that shows that the quality of the normal
approximation is better for symmetric parent populations when
compared to skewed distributions.</p>
<p>How does this discussion apply to the study of regression analysis?
After all, so far we have focused only on the simple arithmetic
average, <span class="math inline">\(\overline{y}\)</span>. In subsequent chapters, we will emphasize
that <em>linear regression is the study of weighted averages</em>;
specifically, many regression coefficients can be expressed as
weighted averages with appropriately chosen weights. Central limit
and Edgeworth approximation theorems are available for weighted
averages - these results will ensure approximate normality of
regression coefficients. To use normal curve approximations in a
regression context, we will often transform variables to achieve
approximate symmetry.</p>
</div>
<div id="Sec15" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Regression and Sampling Designs<a href="regression-and-the-normal-distribution.html#Sec15" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Approximating normality will be an important issue in practical
applications of linear regression. Parts I and II of this book focus
on linear regression, where we will learn basic regression concepts
and sampling design. Part III will focus on <em>nonlinear</em>
regression, involving binary, count and fat-tailed responses, where
the normal is not the most helpful reference distribution. Ideas
concerning basic concepts and design will also be used in the
nonlinear setting.</p>
<p>In regression analysis, we focus on one measurement of interest and
call this the <em>dependent variable</em>. Other measurements are used
as <em>explanatory variables</em>. A goal is to compare differences in
the dependent variable in terms of differences in the explanatory
variables. As noted in Section <a href="regression-and-the-normal-distribution.html#Sec11">1.1</a>, regression is used
extensively in many scientific fields. <a href="#tab:1.3">Table 1.3</a> lists alternative terms that you may encounter as you read regression applications.</p>
<p><a id=tab:1.3></a></p>
<p>Table 1.3. <strong>Terminology for Regression Variables</strong></p>
<p><span class="math display">\[
{\small
\begin{array}{ll}\hline\hline
y-\text{Variable} &amp; x-\text{Variable} \\\hline
\text{Outcome of interest} &amp; \text{Explanatory variable} \\
\text{Dependent variable} &amp; \text{Independent variable} \\
\text{Endogenous variable} &amp; \text{Exogenous variable} \\
\text{Response} &amp; \text{Treatment} \\
\text{Regressand} &amp; \text{Regressor} \\
\text{Left-hand side variable} &amp; \text{Right-hand side variable} \\
\text{Explained variable } &amp; \text{Predictor variable} \\
\text{Output} &amp; \text{Input} \\
\hline
\end{array}
}
\]</span></p>
<p>In the latter part of the nineteenth century and early part of the
twentieth century, statistics was beginning to make an important
impact on the development of experimental science. Experimental
sciences often use <em>designed studies</em>, where the data are under
the control of an analyst. Designed studies are performed in
laboratory settings, where there are tight physical restrictions on
every variable that a researcher thinks may be important. Designed
studies also occur in larger field experiments, where the mechanisms
for control are different than in laboratory settings. Agriculture
and medicine use designed studies. Data from a designed study are
said to be <em>experimental data</em>.</p>
<p>To illustrate, a classic example is to consider the yield of a crop
such as corn, where each of several parcels of land (the
observations) are assigned various levels of fertilizer. The goal is to
ascertain the effect of fertilizer (the explanatory variable) on the
corn yield (the response variable). Although researchers attempt to
make parcels of land as much alike as possible, differences
inevitably arise. Agricultural researchers use <em>randomization techniques</em> to assign different levels of fertilizer to each parcel
of land. In this way, analysts can explain the variation in corn
yields in terms of the variation of fertilizer levels. Through the
use of randomization techniques, researchers using designed studies
can infer that the treatment has a <em>causal effect</em> on the
response. Chapter 6 discusses causality further.</p>
<hr />
<p><strong>Example: Rand Health Insurance Experiment</strong>. How are medical
care expenditures related to the demand for insurance? Many studies
have established a positive relation between the amount spent on
medical care and the demand for health insurance. Those in poor
health anticipate using more medical services than similarly
positioned people in good or fair health and will seek higher levels
of health insurance to compensate for these anticipated
expenditures. They obtain this additional health insurance by (i)
selecting a more generous health insurance plan from an employer,
(ii) choosing an employer with a more generous health insurance plan
or (iii) paying more for individual health insurance. Thus, it is
difficult to disentangle the cause and effect relationship of
medical care expenditures and the availability of health insurance.</p>
<p>A study reported by Manning et al. (1987) sought to answer this
question using a carefully designed experiment. In this study,
enrolled households from six cities, between November 1974 and
February 1977, were <em>randomly assigned</em> to one of 14 different
insurance plans. These plans varied by the cost-sharing elements,
the co-insurance rate (the percentage paid on out-of-pocket
expenditures which varied by 0, 25, 50 and 95%) as well as the
deductible (5, 10 or 15 percent of family income, up to a maximum of
$1,000). Thus, there was a <em>random</em> assignment to levels of
the treatment, the amount of health insurance. The study found that
more favorable plans resulted in greater total expenditures, even
after controlling for participants’ health status.</p>
<hr />
<p>For actuarial science and other social sciences, designed studies
are the exception rather than the rule. For example, if we want to
study the effects of smoking on mortality, it is highly unlikely
that we could get study participants to agree to be randomly
assigned to smoker/nonsmoker groups for several years just so that
we could observe their mortality patterns! As with the Section
<a href="regression-and-the-normal-distribution.html#Sec11">1.1</a> Galton study, social science researchers generally
work with <em>observational data</em>. Observational data are not
under control of the analyst.</p>
<p>With observational data, we can not infer causal relationships but
we can readily introduce measures of <em>association</em>. To
illustrate, in the Galton data, it is apparent that ‘tall’ parents
are likely to have ‘tall’ children and conversely ‘short’
parents are likely to have ‘short’ children. Chapter 2 will
introduce a correlation and other measures of association. However,
we can not infer causality from the data. For example, there may be
another variable, such as family diet, that is related to both
variables. Good diet in the family could be associated with tall
heights of parents and adult children, whereas poor diet stifles
growth. If this were the case, we would call family diet a
<em>confounding variable</em>.</p>
<p>In designed experiments such as the Rand Health Insurance
Experiment, we can control for the effects of variables such as
health status through random assignment methods. In observational
studies, we use <em>statistical control</em>, rather than experimental
control. To illustrate, for the Galton data, we might split our
observations into two groups, one for ‘good family diet’ and one
for ‘poor family diet,’ and examine the relationship between
parents’ and children’s height for each subgroup. This is the
essence of the regression method, to compare a <span class="math inline">\(y\)</span> and an <span class="math inline">\(x\)</span>,
‘controlling for’ the effects of other explanatory variables.</p>
<p>Of course, to use statistical control and regression methods, one
must record family diet, and any other measures of height that may
confound the effects of parents’ height on the height of their adult
child. The difficulty in designing studies is trying to imagine all
of the variables that could possibly affect a response variable, an
impossible task in most social science problems of interest. To give
some guidance on when ‘enough is enough,’ Chapter 6 will discuss
measures of an explanatory variable’s importance and its impact on
model selection.</p>
</div>
<div id="Sec16" class="section level2 hasAnchor" number="1.6">
<h2><span class="header-section-number">1.6</span> Actuarial Applications of Regression<a href="regression-and-the-normal-distribution.html#Sec16" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This book introduces a statistical method, regression analysis. The
introduction is organized around the traditional triad of
statistical inference:</p>
<ul>
<li>hypothesis testing,</li>
<li>estimation and</li>
<li>prediction.</li>
</ul>
<p>Further, this book shows how this methodology can be used in
applications that are likely to be of interest to actuaries and to
other risk analysts. As such, it is helpful to begin with the three
traditional areas of actuarial applications:</p>
<ul>
<li>pricing,</li>
<li>reserving and</li>
<li>solvency testing.</li>
</ul>
<p>
</p>
<p><strong>Pricing and adverse selection.</strong> Regression analysis can be
used to determine insurance prices for many lines of business. For
example, in private passenger automobile insurance, expected claims
vary by the insured’s gender, age, location (city versus rural),
vehicle purpose (work or pleasure) and a host of other explanatory
variables. Regression can be used to identify the variables that are
important determinants of expected claims.</p>
<p>In competitive markets, insurance companies do not use the same
price for all insureds. If they did, ‘good risks,’ those with
lower than average expected claims, would overpay and leave the
company. In contrast, ‘bad risks,’ those with higher than average
expected claims, would remain with the company. If the company
continued this flat rate pricing policy, premiums would rise (to
compensate for claims by the increasing share of bad risks) and
market share would dwindle as the company loses good risks. This
problem is known as ‘adverse selection.’ Using an appropriate set
of explanatory variables, classification systems can be developed so
that each insured pays their fair share.</p>
<p>
</p>
<p><strong>Reserving and solvency testing.</strong> Both reserving and solvency
testing are concerned with predicting whether liabilities associated
with a group of policies will exceed the capital devoted to meeting
obligations arising from the policies. Reserving involves
determining the appropriate amount of capital to meet these
obligations. Solvency testing is about assessing the adequacy of
capital to fund the obligations for a block of business. In some
practice areas, regression can be used to forecast future
obligations to help determine reserves (see, for example, Chapter
19). Regression can also be used to compare characteristics of
healthy and financially distressed firms for solvency testing (see,
for example, Chapter 14).</p>
<p><strong>Other risk management applications.</strong> Regression analysis is
a quantitative tool that can be applied in a broad variety of
business problems, not just the traditional areas of pricing,
reserving and solvency testing. By becoming familiar with regression
analysis, actuaries will have another quantitative skill that can be
brought to bear on general problems involving the financial security
of people, companies and governmental organizations. To help you
develop insights, this book provides many examples of potential
‘non-actuarial’ applications through featured vignettes labeled as
‘examples’ and illustrative data sets.</p>
<p>To help understand potential regression applications, start by
reviewing the several data sets featured in the Chapter 1 Exercises.
Even if you do not complete the exercises to strengthen your data
summary skills (that require the use of a computer), a review of the
problem descriptions will help you become more familiar with types
of applications in which an actuary might use regression techniques.</p>
</div>
<div id="Sec17" class="section level2 hasAnchor" number="1.7">
<h2><span class="header-section-number">1.7</span> Further Reading and References<a href="regression-and-the-normal-distribution.html#Sec17" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This book introduces regression and time series tools that are most
relevant to actuaries and other financial risk analysts.
Fortunately, there are other sources that provide excellent
introductions to these statistical topics (although not from a risk
management viewpoint). Particularly for analysts that intend to
specialize in statistics, it is helpful to get another perspective.
For regression, I recommend Weisburg (2005) and Faraway (2005). For
time series, Diebold (2004) is a good source. Moreover, Klugman,
Panjer and Willmot (2008) provides a good introduction to actuarial
applications of statistics; this book is intended to complement the
Klugman et al. book by focusing on regression and time series
methods.</p>
<p><strong>Chapter References</strong></p>
<ul>
<li>Beard, Robert E., Teivo Pentik"{a}inen and Erkki Pesonen (1984).
<em>Risk Theory: The Stochastic Basis of Insurance</em> (Third
Edition). Chapman &amp; Hall, New York.</li>
<li>Diebold, Francis. X. (2004). <em>Elements of Forecasting, Third
Edition.</em> Thomson, South-Western, Mason, Ohio.</li>
<li>Faraway, Julian J. (2005). <em>Linear Models in R.</em> Chapman &amp;
Hall/CRC, New York.</li>
<li>Hogg, Robert V. (1972). On statistical education. <em>The
American Statistician</em> 26, 8-11.</li>
<li>Klugman, Stuart A, Harry H. Panjer and Gordon E. Willmot (2008).
<em>Loss Models: From Data to Decisions</em>. John Wiley &amp; Sons,
Hoboken, New Jersey.</li>
<li>Manning, Willard G., Joseph P. Newhouse, Naihua Duan, Emmett B.
Keeler, Arleen Leibowitz and M. Susan Marquis (1987). Health
insurance and the demand for medical care: Evidence from a
randomized experiment. <em>American Economic Review</em> 77, No. 3,
251-277.</li>
<li>Rempala, Grzegorz A. and Richard A. Derrig (2005). Modeling hidden
exposures in claim severity via the EM algorithm. <em>North
American Actuarial Journal</em> 9, No. 2, 108-128.</li>
<li>Singer, Judith D. and Willett, J. B. (1990). Improving the teaching
of applied statistics: Putting the data back into data analysis.
<em>The American Statistician</em> 44, 223-230.</li>
<li>Stigler, Steven M. (1986). <em>The History of Statistics: The
Measurement of Uncertainty before 1900</em>. The Belknap Press of
Harvard University Press, Cambridge, MA.</li>
<li>Weisberg, Sanford (2005). <em>Applied Linear Regression, Third
Edition.</em> John Wiley &amp; Sons, New York.</li>
</ul>
</div>
<div id="Sec18" class="section level2 hasAnchor" number="1.8">
<h2><span class="header-section-number">1.8</span> Exercises<a href="regression-and-the-normal-distribution.html#Sec18" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>1.1 <strong>MEPS health expenditures.</strong> This exercise considers data from the Medical Expenditure Panel Survey (MEPS), conducted by the
U.S. Agency of Health Research and Quality. MEPS is a probability
survey that provides nationally representative estimates of health
care use, expenditures, sources of payment, and insurance coverage
for the U.S. civilian population. This survey collects detailed
information on individuals of each medical care episode by type of
services including physician office visits, hospital emergency room
visits, hospital outpatient visits, hospital inpatient stays, all
other medical provider visits, and use of prescribed medicines. This
detailed information allows one to develop models of health care
utilization to predict future expenditures. You can learn more about
MEPS at <a href="http://www.meps.ahrq.gov/mepsweb/" class="uri">http://www.meps.ahrq.gov/mepsweb/</a>.</p>
<p>We consider MEPS data from the panels 7 and 8 of 2003 that consists
of 18,735 individuals between ages 18 and 65. From this sample, we
took a random sample of 2,000 individuals that appear in the file
‘HealthExpend’. From this sample, there are 157 individuals that
had positive inpatient expenditures. There are also 1,352 that had
positive outpatient expenditures. We will analyze these two samples
separately.</p>
<p>Our dependent variables consist of amounts of expenditures for
inpatient (EXPENDIP) and outpatient (EXPENDOP) visits. For MEPS,
outpatient events include hospital outpatient department visits,
office-based provider visits and emergency room visits excluding
dental services. (Dental services, compared to other types of health
care services, are more predictable and occur in a more regular
basis.) Hospital stays with the same date of admission and
discharge, known as ‘zero-night stays,’ were included in
outpatient counts and expenditures. (Payments associated with
emergency room visits that immediately preceded an inpatient stay
were included in the inpatient expenditures. Prescribed medicines
that can be linked to hospital admissions were included in inpatient
expenditures, not in outpatient utilization.)</p>
<p><strong>Part 1</strong>: Use only the 157 individuals that had positive inpatient
expenditures and do the following analysis.</p>
<ol style="list-style-type: lower-alpha">
<li>Compute descriptive statistics for inpatient (EXPENDIP)
expenditures.
<ul>
<li>a(i). What is the typical (mean and median) expenditure?</li>
<li>a(ii). How does the standard deviation compare to the mean? Do the
data appear to be skewed?</li>
</ul></li>
<li>Compute a box plot, histogram and a (normal) <span class="math inline">\(qq\)</span> plot for
EXPENDIP. Comment on the shape of the distribution.</li>
<li>Transformations.
<ul>
<li>c(i). Take a square root transform of inpatient expenditures.
Summarize the resulting distribution using a histogram and a <span class="math inline">\(qq\)</span>
plot. Does it appear to be approximately normally distributed?</li>
<li>c(ii). Take a (natural) logarithmic transformation of inpatient
expenditures. Summarize the resulting distribution using a histogram
and a <span class="math inline">\(qq\)</span> plot. Does it appear to be approximately normally
distributed?</li>
</ul></li>
</ol>
<p><strong>Part 2</strong>: Use only the 1,352 individuals that had positive outpatient
expenditures.</p>
<ol start="4" style="list-style-type: lower-alpha">
<li>Repeat part (a) and compute histograms for expenditures and
logarithmic expenditures. Comment on the approximate normality for
each histogram.</li>
</ol>
<hr />
<p>1.2 <strong>Nursing Home Utilization.</strong> This exercise considers nursing home data provided by the Wisconsin
Department of Health and Family Services (DHFS). The State of
Wisconsin Medicaid program funds nursing home care for individuals
qualifying on the basis of need and financial status. As part of the
conditions for participation, Medicaid-certified nursing homes must
file an annual cost report to DHFS, summarizing the volume and cost
of care provided to all of its residents, Medicaid-funded and
otherwise. These cost reports are audited by DHFS staff and form the
basis for facility-specific Medicaid daily payment rates for
subsequent periods. The data are publicly available; see
[<a href="http://dhs.wisconsin.gov" class="uri">http://dhs.wisconsin.gov</a>] for more information.</p>
<p>The DHFS is interested in predictive techniques that provide
reliable utilization forecasts to update their Medicaid funding rate
schedule of nursing facilities. In this assignment, we consider the
data in the file ‘WiscNursingHome’ in cost report years 2000 and
2001. There are 362 facilities in 2000 and 355 facilities in 2001.
Typically, utilization of nursing home care is measured in patient
days (‘patient days’ is the number of days each patient was in the
facility, summed over all patients). For this exercise, we define
the outcome variable to be total patient years (TPY), the number of
total patient days in the cost reporting period divided by number of
facility operating days in the cost reporting period (see Rosenberg
et al., 2007, Appendix 1, for further discussion of this choice).
The number of beds (NUMBED) and square footage (SQRFOOT) of the
nursing home both measure the size of the facility. Not
surprisingly, these variables will be important
predictors of TPY.</p>
<p><strong>Part 1:</strong> Use cost report year 2000 data, and do the
following analysis.</p>
<ol style="list-style-type: lower-alpha">
<li>Compute descriptive statistics for TPY, NUMBED, and SQRFOOT.</li>
<li>Summarize the distribution of TPY using a histogram and a <span class="math inline">\(qq\)</span>
plot. Does it appear to be approximately normally distributed?</li>
<li>Transformations. Take a (natural) logarithmic transformation of
TPY (LOGTPY). Summarize the resulting distribution using a histogram
and a <span class="math inline">\(qq\)</span> plot. Does it appear to be approximately normally
distributed?</li>
</ol>
<p><strong>Part 2:</strong> Use cost report year 2001 data and repeat parts (a)
and (c).</p>
<hr />
<p>1.3 <strong>Automobile Insurance Claims.</strong> As an actuarial analyst,
you are working with a large insurance company to help them
understand their claims distribution for their private passenger
automobile policies. You have available claims data for a recent
year, consisting of:</p>
<ul>
<li>STATE CODE: codes 01 through 17 used, with each code randomly
assigned to an actual individual state</li>
<li>CLASS: rating class of operator, based on age, gender, marital
status and use of vehicle</li>
<li>GENDER: operator gender AGE: operator age</li>
<li>PAID: amount paid to settle and close a claim.</li>
</ul>
<p>You are focusing on older drivers, 50 and higher, for which there
are <span class="math inline">\(n = 6,773\)</span> claims available.</p>
<p>Examine the histogram of the amount PAID and comment on the
symmetry. Create a new variable, the (natural) logarithmic claims
paid, LNPAID. Create a histogram and a <span class="math inline">\(qq\)</span> plot of LNPAID. Comment
on the symmetry of this variable.</p>
<hr />
<p>1.4 <strong>Hospital Costs.</strong> Suppose
that you are an employee benefits actuary working with a medium size
company in Wisconsin. This company is considering offering, for the
first time in their industry, hospital insurance coverage to
dependent children of their employees. You have access to company
records and so have available the number, age and gender of the
dependent children but have no other information about hospital
costs from the company. In particular, no firm in this industry has
offered this coverage and so you have little historical industry
experience upon which you can forecast expected claims.</p>
<p>You gather data from the Nationwide Inpatient Sample of the
Healthcare Cost and Utilization Project (NIS-HCUP), a nationwide
survey of hospital costs conducted by the US Agency for Healthcare
Research and Quality (AHRQ). You restrict consideration to Wisconsin
hospitals and analyze a random sample of <span class="math inline">\(n=500\)</span> claims from 2003
data. Although the data comes from hospital records, it is organized
by individual discharge and so you have information about the age
and gender of the patient discharged. Specifically, you consider
patients aged 0-17 years. In a separate project, you will consider
the frequency of hospitalization. For this project, the goal is to
model the severity of hospital charges, by age and gender.</p>
<ol style="list-style-type: lower-alpha">
<li>Examine the distribution of the dependent variable, TOTCHG. Do
this by making a histogram and then a <span class="math inline">\(qq\)</span> plot, comparing the
empirical to a normal distribution.</li>
<li>Take a natural log transformation and call the new variable
LNTOTCHG. Examine the distribution of this transformed variable. To
visualize the logarithmic relationship, plot LNTOTCHG versus TOTCHG.</li>
</ol>
<hr />
<p>1.5 <strong>Automobile injury insurance claims.</strong>
We consider automobile injury claims data using data from the
Insurance Research Council (IRC), a division of the American
Institute for Chartered Property Casualty Underwriters and the
Insurance Institute of America. The data, collected in 2002,
contains information on demographic information about the claimant,
attorney involvement and the economic loss (LOSS, in thousands),
among other variables. We consider here a sample of <span class="math inline">\(n=1,340\)</span> losses
from a single state. The full 2002 study contains over 70,000 closed
claims based on data from thirty-two insurers. The IRC conducted
similar studies in 1977, 1987, 1992 and 1997.</p>
<ol style="list-style-type: lower-alpha">
<li>Compute descriptive statistics for the total economic loss
(LOSS). What is the typical loss?</li>
<li>Compute a histogram and (normal) <span class="math inline">\(qq\)</span> plot for LOSS. Comment on
the shape of the distribution.</li>
<li>Partition the data set into two subsamples, one corresponding to
those claims that involved an ATTORNEY (=1) and the other where an
ATTORNEY was not involved (=2).
<ul>
<li>c(i). For each subsample, compute the typical loss. Does there
appear to be a difference in the typical losses by attorney
involvement?</li>
<li>c(ii) To compare the distributions, compute a boxplot by level of
attorney involvement.</li>
<li>c(iii). For each subsample, compute a histogram and <span class="math inline">\(qq\)</span> plot.
Compare the two distributions to one another.</li>
</ul></li>
</ol>
<hr />
<p>1.6 <strong>Insurance Company Expenses.</strong>
Like other businesses, insurance companies seek to minimize expenses
associated with doing business in order to enhance profitability. To
study expenses, this exercise examines a random sample of 500
insurance companies from the National Association of Insurance
Commissioners (NAIC) database of over 3,000 companies. The NAIC
maintains one of the world’s largest insurance regulatory databases;
we consider here data that are based on 2005 annual reports for all
the property and casualty insurance companies in the United States.
The annual reports are financial statements that use statutory
accounting principles.</p>
<p>Specifically, our dependent variable is EXPENSES, the non-claim
expenses for a company. Although not needed for this exercise,
non-claim expenses are based on three components: unallocated loss
adjustment, underwriting and investment expenses. The unallocated
loss adjustment expense is the expense not directly attributable to
a claim but is indirectly associated with settling claims; it
includes items such as the salaries of claims adjusters, legal fees,
court costs, expert witnesses and investigation costs. Underwriting
expenses consists of policy acquisition costs, such as commissions,
as well as the portion of administrative, general and other expenses
attributable to underwriting operations. Investment expense are
those expenses related to investment activities of the insurer.</p>
<ol style="list-style-type: lower-alpha">
<li>Examine the distribution of the dependent variable, EXPENSES. Do
this by making a histogram and then a <span class="math inline">\(qq\)</span> plot, comparing the
empirical to a normal distribution.</li>
<li>Take a natural log transformation and examine the distribution of
this transformed variable. Has the transformation helped to
symmetrize the distribution?</li>
</ol>
<hr />
<p>1.7 <strong>National Life Expectancies.</strong> Who is doing health care right?
Health care decisions are made at the individual, corporate and
government levels. Virtually every person, corporation and
government have their own perspective on health care; these
different perspectives result in a wide variety of systems for
managing health care. Comparing different health care systems help
us learn about approaches other than our own, which in turn help us
make better decisions in designing improved systems.</p>
<p>Here, we consider health care systems from <span class="math inline">\(n=185\)</span> countries
throughout the world. As a measure of the quality of care, we use
<code>LIFEEXP</code>, the life expectancy at birth. This dependent variable, with
several explanatory variables, are listed in <a href="#tab:1.4">Table 1.4</a>. From this table, you will note that
although there are 185 countries considered in this study, not all
countries provided information for each variable. Data not available
are noted under the column <code>Num Miss.</code> The data are from the
United Nations (UN) Human Development Report.</p>
<ol style="list-style-type: lower-alpha">
<li>Examine the distribution of the dependent variable, LIFEEXP. Do
this by making a histogram and then a <span class="math inline">\(qq\)</span> plot, comparing the
empirical to a normal distribution.</li>
<li>Take a natural log transformation and examine the distribution of
this transformed variable. Has the transformation helped to
symmetrize the distribution?</li>
</ol>
<p><a id=tab:1.4></a></p>
<p>Table 1.4. <strong>Life Expectancy, Economic and Demographic Characteristics of 185 Countries</strong></p>
<p><span class="math display">\[
\small{
\begin{array}{ll|crrrrr}
\hline
&amp;  &amp; Num &amp;        &amp;     &amp; Std  &amp;   Mini- &amp;  Maxi- \\
  Variable     &amp; Description &amp; Miss &amp;    Mean    &amp;    Median   &amp; Dev &amp;    mum &amp;    mum \\\hline
BIRTH &amp; \text{ Births attended  by skilled} &amp;          7 &amp;      78.25 &amp;      92.00 &amp;      26.42 &amp;       6.00 &amp;     100.00 \\
~~ATTEND&amp; ~~ \text{ health personnel} (\%)\\
FEMALE &amp; \text{ Legislators, senior officials} &amp;         87 &amp;      29.07 &amp;      30.00 &amp;      11.71 &amp;       2.00 &amp;      58.00 \\
~~BOSS&amp; ~~ \text{ and managers, % female }\\
FERTILITY &amp; \text{ Total fertility rate,}&amp;          4 &amp;       3.19 &amp;       2.70 &amp;       1.71 &amp;       0.90 &amp;       7.50 \\
&amp; ~~ \text{ births per woman }&amp;          \\
       GDP &amp; \text{ Gross domestic product,} &amp;          7 &amp;     247.55 &amp;      14.20 &amp;   1,055.69 &amp;       0.10 &amp;  12,416.50 \\
       &amp; ~~\text{ in billions of USD} \\
HEALTH&amp; \text{ 2004 Health expenditure} &amp;          5 &amp;     718.01 &amp;     297.50 &amp;   1,037.01 &amp;      15.00 &amp;   6,096.00 \\
~~ EXPEND &amp; ~~ \text{ per capita, PPP in USD} \\
ILLITERATE &amp; \text{ Adult illiteracy rate,}  &amp;         14 &amp;      17.69 &amp;      10.10 &amp;      19.86 &amp;       0.20 &amp;      76.40 \\
  &amp; ~~ \% \text{ aged 15 and older} &amp;      \\
PHYSICIAN &amp; \text{ Physicians,}&amp;          3 &amp;     146.08 &amp;     107.50 &amp;     138.55 &amp;       2.00 &amp;     591.00 \\
&amp; ~~ \text{ per 100,000 people} \\
       POP &amp; \text{ 2005 population,} &amp;          1 &amp;      35.36 &amp;       7.80 &amp;     131.70 &amp;       0.10 &amp;   1,313.00 \\
       &amp; ~~\text{ in millions }\\
PRIVATE &amp; \text{ 2004 Private expenditure}  &amp;          1 &amp;       2.52 &amp;       2.40 &amp;       1.33 &amp;       0.30 &amp;       8.50 \\
~~HEALTH&amp; ~~\text{on health, % of GDP}  \\
PUBLIC &amp; \text{ Public expenditure}  &amp;         28 &amp;       4.69 &amp;       4.60 &amp;       2.05 &amp;       0.60 &amp;      13.40 \\
~~EDUCATION&amp; ~~ \text{ on education, % of GDP} \\
RESEAR &amp; \text{ Researchers in R &amp; D,}  &amp;         95 &amp;   2,034.66 &amp;     848.00 &amp;   4,942.93 &amp;      15.00 &amp;  45,454.00 \\
~~CHERS&amp;~~  \text{ per million people} &amp;         \\
   SMOKING &amp; \text{ Prevalence of smoking,} &amp;         88 &amp;      35.09 &amp;      32.00 &amp;      14.40 &amp;       6.00 &amp;      68.00 \\
    &amp; ~~\text{ (male) % of adults } \\ \hline
   LIFEEXP &amp; \text{ Life expectancy at birth,}&amp;            &amp;      67.05 &amp;      71.00 &amp;      11.08 &amp;      40.50 &amp;      82.30 \\
   &amp; ~~ \text{ in years } \\
\hline
\end{array}
}
\]</span></p>
<p><em>Source:</em> United Nations Human Development Report, available
at <a href="http://hdr.undp.org/en/" class="uri">http://hdr.undp.org/en/</a> .</p>
</div>
<div id="Sec19" class="section level2 hasAnchor" number="1.9">
<h2><span class="header-section-number">1.9</span> Technical Supplement - Central Limit Theorem<a href="regression-and-the-normal-distribution.html#Sec19" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Central limit theorems form the basis for much of the statistical
inference used in regression analysis. Thus, it is helpful to
provide an explicit statement of one version of the central limit
theorem.</p>
<div class="blackbox">
<p><em>Central Limit Theorem.</em> Suppose that <span class="math inline">\(y_1,\ldots,y_n\)</span> are
independently distributed with mean <span class="math inline">\(\mu\)</span>, finite variance <span class="math inline">\(\sigma ^{2}\)</span> and <span class="math inline">\(\mathrm{E}|y|^{3}\)</span> is finite. Then,
<span class="math display">\[
\lim_{n\rightarrow \infty }\Pr \left( \frac{\sqrt{n}}{\sigma }(\overline{y}
-\mu )\leq x\right)
=\Phi \left( x\right) ,
\]</span>
for each <span class="math inline">\(x,\)</span> where <span class="math inline">\(\Phi \left( .\right)\)</span> is the standard normal
distribution function.</p>
</div>
<p>Under the assumptions of this theorem, the re-scaled distribution of
<span class="math inline">\(\overline{y}\)</span> approaches a standard normal as the sample size,
<span class="math inline">\(n\)</span>, increases. We interpret this as meaning that, for ‘large’
sample sizes, the distribution of <span class="math inline">\(\overline{y}\)</span> may be approximated
by a normal distribution. Empirical investigations have shown that
sample sizes of <span class="math inline">\(n=25\)</span> through 50 provide adequate approximations
for most purposes.</p>
<p>When does the central limit theorem not work well? Some insights are
provided by another result from mathematical statistics.</p>
<div class="blackbox">
<p><em>Edgeworth Approximation</em>. Suppose that <span class="math inline">\(y_1,\ldots, y_n\)</span> are
identically and independently distributed with mean <span class="math inline">\(\mu\)</span>, finite variance <span class="math inline">\(\sigma ^{2}\)</span> and <span class="math inline">\(\mathrm{E}|y|^{3}\)</span> is finite. Then,%
<span class="math display">\[
\Pr \left( \frac{\sqrt{n}}{\sigma }(\overline{y}-\mu )\leq x\right)
=\Phi
\left( x\right) +\frac{1}{6}\frac{1}{\sqrt{2\pi }}e^{-x^{2}/2}\frac{\mathrm{E
}(y-\mu )^{3}}{\sigma ^{3}\sqrt{n}}+\frac{h_n}{\sqrt{n}},
\]</span>
for each <span class="math inline">\(x,\)</span> where <span class="math inline">\(h_n\rightarrow 0\)</span> as <span class="math inline">\(n\rightarrow \infty .\)</span></p>
</div>
<p>This result suggests that the distribution of <span class="math inline">\(\bar{y}\)</span> becomes
closer to a normal distribution as the skewness,
<span class="math inline">\(\mathrm{E}(\overline{y} -\mu )^{3}\)</span>, becomes closer to zero. This
is important in insurance applications because many distributions
tend to be skewed. Historically, analysts used the second term on
the right-hand side of the result to provide a ‘correction’ for
the normal curve approximation. See, for example, Beard,
Pentik"{a}inen and Pesonen (1984) for further discussion of
Edgeworth approximations in actuarial science. An alternative (used
in this book) that we saw in Section <a href="regression-and-the-normal-distribution.html#Sec13">1.3</a> is to transform the data, thus achieving approximate symmetry. As suggested by the Edgeworth approximation theorem, if our parent population is close to
symmetric, then the distribution of <span class="math inline">\(\overline{y}\)</span> will be
approximately normal.</p>

<!-- # Chap 1 -->
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="preface.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="C2BasicLR.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
