<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Modeling Trends {c7Trends} | Regression Modeling with Actuarial and Financial Applications</title>
  <meta name="description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Modeling Trends {c7Trends} | Regression Modeling with Actuarial and Financial Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Modeling Trends {c7Trends} | Regression Modeling with Actuarial and Financial Applications" />
  
  <meta name="twitter:description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  

<meta name="author" content="Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap-6.html"/>
<link rel="next" href="appendices.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript"  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleSolution(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}      
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>




<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Regression Modeling With Actuarial and Financial Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#forward"><i class="fa fa-check"></i>Forward</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#who-is-this-book-for"><i class="fa fa-check"></i>Who Is This Book For?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#what-is-this-book-about"><i class="fa fa-check"></i>What Is This Book About?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#how-does-this-book-deliver-its-message"><i class="fa fa-check"></i>How Does This Book Deliver Its Message?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="chap-1.html"><a href="chap-1.html"><i class="fa fa-check"></i><b>1</b> Chap 1</a></li>
<li class="chapter" data-level="2" data-path="chap-2.html"><a href="chap-2.html"><i class="fa fa-check"></i><b>2</b> Chap 2</a></li>
<li class="chapter" data-level="3" data-path="chap-3.html"><a href="chap-3.html"><i class="fa fa-check"></i><b>3</b> Chap 3</a></li>
<li class="chapter" data-level="4" data-path="chap-4.html"><a href="chap-4.html"><i class="fa fa-check"></i><b>4</b> Chap 4</a></li>
<li class="chapter" data-level="5" data-path="chap-5.html"><a href="chap-5.html"><i class="fa fa-check"></i><b>5</b> Chap 5</a></li>
<li class="chapter" data-level="6" data-path="chap-6.html"><a href="chap-6.html"><i class="fa fa-check"></i><b>6</b> Chap 6</a></li>
<li class="chapter" data-level="7" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html"><i class="fa fa-check"></i><b>7</b> Modeling Trends {c7Trends}</a>
<ul>
<li class="chapter" data-level="7.1" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#time-series-and-stochastic-processes"><i class="fa fa-check"></i>Time Series and Stochastic Processes</a></li>
<li class="chapter" data-level="" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#time-series-versus-causal-models"><i class="fa fa-check"></i>Time Series versus Causal Models</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#S7:Trends"><i class="fa fa-check"></i><b>7.2</b> Fitting Trends in Time</a>
<ul>
<li class="chapter" data-level="" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#understanding-patterns-over-time"><i class="fa fa-check"></i>Understanding Patterns over Time</a></li>
<li class="chapter" data-level="" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#fitting-trends-in-time"><i class="fa fa-check"></i>Fitting Trends in Time</a></li>
<li class="chapter" data-level="" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#fitting-seasonal-trends"><i class="fa fa-check"></i>Fitting Seasonal Trends</a></li>
<li class="chapter" data-level="" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#reliability-of-time-series-forecasts"><i class="fa fa-check"></i>Reliability of Time Series Forecasts</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#S7:RandomWalk"><i class="fa fa-check"></i><b>7.3</b> Stationarity and Random Walk Models</a>
<ul>
<li class="chapter" data-level="" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#white-noise"><i class="fa fa-check"></i>White Noise</a></li>
<li class="chapter" data-level="" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#random-walk"><i class="fa fa-check"></i>Random Walk</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#inference-using-random-walk-models"><i class="fa fa-check"></i><b>7.4</b> Inference using Random Walk Models</a>
<ul>
<li class="chapter" data-level="" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#model-properties"><i class="fa fa-check"></i>Model Properties</a></li>
<li class="chapter" data-level="" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#forecasting"><i class="fa fa-check"></i>Forecasting</a></li>
<li class="chapter" data-level="" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#identifying-stationarity"><i class="fa fa-check"></i>Identifying Stationarity</a></li>
<li class="chapter" data-level="" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#identifying-random-walks"><i class="fa fa-check"></i>Identifying Random Walks</a></li>
<li class="chapter" data-level="" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#random-walk-versus-linear-trend-in-time-models"><i class="fa fa-check"></i>Random Walk versus Linear Trend in Time Models</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#filtering-to-achieve-stationarity"><i class="fa fa-check"></i><b>7.5</b> Filtering to Achieve Stationarity</a>
<ul>
<li class="chapter" data-level="" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#transformations"><i class="fa fa-check"></i>Transformations</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#forecast-evaluation"><i class="fa fa-check"></i><b>7.6</b> Forecast Evaluation</a></li>
<li class="chapter" data-level="7.7" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#further-reading-and-references"><i class="fa fa-check"></i><b>7.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="7.8" data-path="modeling-trends-c7trends.html"><a href="modeling-trends-c7trends.html#exercises"><i class="fa fa-check"></i><b>7.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i><b>8</b> Appendices</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a1.-basic-statistical-inference"><i class="fa fa-check"></i>Appendix A1. Basic Statistical Inference</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#distributions-of-functions-of-random-variables"><i class="fa fa-check"></i>Distributions of Functions of Random Variables</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#estimation-and-prediction"><i class="fa fa-check"></i>Estimation and Prediction</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#testing-hypotheses"><i class="fa fa-check"></i>Testing Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a2.-matrix-algebra"><i class="fa fa-check"></i>Appendix A2. Matrix Algebra</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#basic-definitions"><i class="fa fa-check"></i>Basic Definitions</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#review-of-basic-operations"><i class="fa fa-check"></i>Review of Basic Operations</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#further-definitions"><i class="fa fa-check"></i>Further Definitions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a3.-probability-tables"><i class="fa fa-check"></i>Appendix A3. Probability Tables</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#normal-distribution"><i class="fa fa-check"></i>Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#chi-square-distribution"><i class="fa fa-check"></i>Chi-Square Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#t-distribution"><i class="fa fa-check"></i><em>t</em>-Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#f-distribution"><i class="fa fa-check"></i><em>F</em>-Distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTextDev/RegressionSpanish/" target="blank">Spanish Regression on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Regression Modeling with Actuarial and Financial Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modeling-trends-c7trends" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Modeling Trends {c7Trends}<a href="modeling-trends-c7trends.html#modeling-trends-c7trends" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Chapter Preview</em>. This chapter begins our study of
time series data by introducing techniques to account for major
patterns, or trends, in data that evolve over time. The focus is on
how regression techniques developed in earlier chapters can be used
to model trends. Further, new techniques, such differencing data,
allow us to naturally introduce a random walk, an important model of
efficient financial markets.</p>
<div id="introduction" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Introduction<a href="modeling-trends-c7trends.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="time-series-and-stochastic-processes" class="section level3 unnumbered hasAnchor">
<h3>Time Series and Stochastic Processes<a href="modeling-trends-c7trends.html#time-series-and-stochastic-processes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Business firms are not defined by physical structures such as the
solid stone bank building that symbolizes financial security. Nor
are businesses defined by space alien invader toys that they
manufacture for children. Business firms are comprised of several
complex, interrelated processes. A is a series of
actions or operations that lead to a particular end.</p>
<p>Processes are not only the building blocks of businesses, they also
provide the foundations for our everyday lives. We may go to work or
school every day, practice martial arts or study statistics. These
are regular sequences of activities that define us. In this text,
our interest is in modeling , defined to
be ordered collections of random variables, that quantify a process
of interest.</p>
<p>Some processes evolve over time, such as daily trips to work or
school and the quarterly earnings of a firm. We use the term
for measurements of a process that evolves
over time. A
single measurement of a process yields a variable over time, denoted
by <span class="math inline">\(y_1,...,y_T,\)</span> and referred to as a .
In this portion of
the text, we follow common practice and use <span class="math inline">\(T\)</span> to denote the number
of observations available (instead of <span class="math inline">\(n\)</span>). Chapter 10 will describe
another type of longitudinal data where we examine a cross-section
of entities, such as firms, and examine their evolution over time.
This type of data is also known as .</p>
<p>Collections of random variables may be based on orderings other than time.
For example, hurricane claim damages are recorded at the place where the
damage has occurred and thus are ordered spatially. As another example,
evaluation of an oil-drilling project requires taking samples of the earth
at various longitudes, latitudes and depths. This yields observations
ordered by the three dimensions of space but not time. As yet another
example, the study of holes in the ozone layer requires taking atmospheric
measurements. Because the interest is in the trend of ozone depletion, the
measurements are taken at various longitudes, latitudes, heights and time.
Although we consider only processes ordered by time, in other studies of
longitudinal data you may see alternatives orderings. Data that are not
ordered are called .</p>
</div>
<div id="time-series-versus-causal-models" class="section level3 unnumbered hasAnchor">
<h3>Time Series versus Causal Models<a href="modeling-trends-c7trends.html#time-series-versus-causal-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Regression methods can be used to summarize many time series data
sets. However, simply using regression techniques without
establishing an appropriate context can be disastrous. This concept
is reinforced by an example based on Granger and Newbold’s (1974)
work.</p>
<hr />
<p> Let <span class="math inline">\(\{\varepsilon_{x,t}\}\)</span> and <span class="math inline">\(% \{\varepsilon_{y,t}\}\)</span> be two independent sequences, each of which
also has a standard normal distribution. From these, recursively
construct the variables <span class="math inline">\(x_t = 0.5 + x_{t-1} + \varepsilon_{x,t}\)</span>
and <span class="math inline">\(y_t = 0.5 + y_{t-1} + \varepsilon_{y,t}\)</span>, using the initial
conditions <span class="math inline">\(x_0=y_0=0\)</span>. (In Section 7.3, we will identify <span class="math inline">\(x_t\)</span> and
<span class="math inline">\(y_t\)</span> as random walk models.) Figure <span class="math inline">\(\ref{F7:SpurCorr}\)</span> shows a
realization of {<span class="math inline">\(x_t\)</span>} and {<span class="math inline">\(y_t\)</span>}, generated for <span class="math inline">\(T=50\)</span>
observations using simulation. The left-hand panel shows the growth
of each series over time - the increasing nature is due to the
addition of 0.5 at each time point. The right-hand panel shows a
strong relationship between {<span class="math inline">\(x_t\)</span>} and {<span class="math inline">\(y_t\)</span>} - the
correlation between these two series turns out to be 0.92. This is
despite the fact that the two series were generated
Their apparent relationship, said to be
, is because both are related to the growth over
time.</p>
<hr />
<p>In a longitudinal context, regression models of the form%
<span class="math display">\[
y_t = \beta_0 + \beta_1 x_t + \varepsilon_t
\]</span>
are known as . Causal models are regularly
employed in econometrics, where it is assumed that economic theory
provides the information needed to specify the causal relationship
(<span class="math inline">\(x\)</span> “causes” <span class="math inline">\(y\)</span>). In contrast, statistical models can only
validate empirical relationships (“correlation, not causation”).
In the spurious regression example, both variables evolve over time
and so a model of how one variable influences another needs to
account for time patterns of both the left- and right-hand side
variables. Specifying causal models for actuarial applications can
be difficult for this reason - time series patterns in the
explanatory variables may mask or induce a significant relationship
with the dependent variable. In contrast, regression modeling can be
readily applied when explanatory variables are simply functions of
time, the topic of the next section. This is because functions of
time are deterministic and so will not exhibit time series patterns.</p>
<p>Causal models also suffer from the drawback that their applications
are limited for forecasting purposes. This is because in order to
make a forecast of a future realization of the series, for example
<span class="math inline">\(y_{T+2}\)</span>, one needs to have knowledge (or a good forecast) of
<span class="math inline">\(x_{T+2},\)</span> the value of the explanatory variable at time <span class="math inline">\(T+2\)</span>. If
<span class="math inline">\(x\)</span> is a known function of time (as in the next section), then this
is not an issue. Another possibility is to use a lagged value of <span class="math inline">\(x\)</span>
such as <span class="math inline">\(y_t = \beta_0 + \beta_1 x_{t-1} + \varepsilon_t,\)</span> so that
one-step predictors are possible (we can use the equation to predict
<span class="math inline">\(y_{T+1}\)</span> because <span class="math inline">\(x_T\)</span> is known at time <span class="math inline">\(T\)</span>).</p>
</div>
</div>
<div id="S7:Trends" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Fitting Trends in Time<a href="modeling-trends-c7trends.html#S7:Trends" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="understanding-patterns-over-time" class="section level3 unnumbered hasAnchor">
<h3>Understanding Patterns over Time<a href="modeling-trends-c7trends.html#understanding-patterns-over-time" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p> is about predicting future realizations of a time
series. Over the years, analysts have found it convenient to
decompose a series into three types of patterns: trends in time
(<span class="math inline">\(T_t\)</span>), seasonal (<span class="math inline">\(S_t\)</span>), and random, or irregular, patterns
(<span class="math inline">\(\varepsilon_t\)</span>). A series can then be forecast by extrapolating
each of the three patterns. The trend is that part of a series that
corresponds to a long-term, slow evolution of the series. This is
the most important part for long-range forecasts. The seasonal part
of the series corresponds to aspects that repeat itself
periodically, say over a year. The irregular patterns of a series
are short-term movements that are typically harder to
anticipate.</p>
<p>Analysts typically combine these patterns in two ways: in an
additive fashion,
<span class="math display">\[\begin{equation}\label{E7:1}
y_t = T_t + S_t + \varepsilon_t,
\end{equation}\]</span>
or in a multiplicative fashion,
<span class="math display">\[\begin{equation}\label{E7:2}
y_t = T_t \times S_t + \varepsilon_t.
\end{equation}\]</span>
Models without seasonal components can be readily handled by using
<span class="math inline">\(S_t=0\)</span> for the additive model in equation (<span class="math inline">\(\ref{E7:1}\)</span>) and <span class="math inline">\(S_t=1\)</span>
for the multiplicative model in equation (<span class="math inline">\(\ref{E7:2}\)</span>). If the model
is purely multiplicative such that <span class="math inline">\(y_t = T_t \times S_t \times \varepsilon_t\)</span>, then it can be converted to an additive model by
taking logarithms of both sides.</p>
<p>It is instructive to see how these three components can be combined
to form a series of interest. Consider the three components in
Figure <span class="math inline">\(\ref{F7:LinearTrend}\)</span>. Under the additive model, the trend,
seasonal and random variation components are combined to form the
series that appears in the lower right-hand panel. A plot of <span class="math inline">\(y_t\)</span>
versus <span class="math inline">\(t\)</span> is called a . In time series
plots, the convention is to connect adjacent points using a line to
help us detect patterns over time.</p>
<p>When analyzing data, the graph in the lower right-hand panel is the
first type of plot that we will examine. The goal of the analysis is
to go backwards - that is, we wish to decompose the series into the
three components. Each component can then be forecast which will
provide us with forecasts that are reasonable and easy to interpret.</p>
</div>
<div id="fitting-trends-in-time" class="section level3 unnumbered hasAnchor">
<h3>Fitting Trends in Time<a href="modeling-trends-c7trends.html#fitting-trends-in-time" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The simplest type of time trend is a complete lack of trend.
Assuming that the observations are identically and independently
distributed (), then we could use the model
<span class="math display">\[
y_t = \beta_0 + \varepsilon_t.
\]</span>
For example, if you are observing a game of chance such as bets
placed on the roll of two dice, then we typically model this as an
i.i.d. series.</p>
<p>Fitting polynomial functions of time is another type of trend that
is easy to interpret and to fit to the data. We begin with a
straight line for our polynomial function of time, yielding the
,
<span class="math display">\[\begin{equation}\label{E7:3}
y_t = \beta_0 + \beta_1 t + \varepsilon_t.
\end{equation}\]</span>
Similarly, regression techniques can be used to fit other functions
that represent trends in time. Equation (<span class="math inline">\(\ref{E7:3}\)</span>) is easily
extended to handle a ,
<span class="math display">\[
y_t = \beta_0 + \beta_1 t + \beta_2 t^2 + \varepsilon_t,
\]</span>
or a higher-order polynomial.</p>
<hr />
<p> For travelers and firms, exchange rates are an
important part of the monetary economy. The exchange rate that we
consider is the number of Hong Kong dollars that one can purchase
for one US dollar. We have <span class="math inline">\(T=502\)</span> daily observations for the period
April 1, 2005 through May 31, 2007 that were obtained from the
Federal Reserve (H10 report). Figure <span class="math inline">\(\ref{F7:HKFits}\)</span> provides a time
series plot of the Hong Kong exchange rate.</p>
<p>Figure <span class="math inline">\(\ref{F7:HKFits}\)</span> shows a clear quadratic trend in the data. To
handle this trend, we use <span class="math inline">\(t=1,...,502\)</span>, as an explanatory variable
to indicate the time period. The fitted regression equation turns
out to be:
<span class="math display">\[
\begin{tabular}{cccc}
$\widehat{INDEX}_t = $ &amp; $7.797$ &amp; $-3.68\times 10^{-4}t$ &amp;
$+8.269\times
10^{-7}t^2$ \\
{\small $t$-statistics} &amp; {\small (8,531.9)} &amp; {\small (-44.0)} &amp;
{\small (51.2)}
\end{tabular}
.
\]</span>
The coefficient of determination is a healthy <span class="math inline">\(R^2=92.9\%\)</span> and the
standard deviation estimate has dropped from <span class="math inline">\(s_{y}=0.0183\)</span> down to
<span class="math inline">\(s=0.0068\)</span> (our residual standard deviation). Figure <span class="math inline">\(\ref{F7:HKFits}\)</span>
shows the relationship between the data and fitted values through
the time series plot of the exchange rate with the fitted values
superimposed. To apply these regression results to the forecasting
problem, suppose that we wanted to predict the exchange rate for
April 1, 2007, or <span class="math inline">\(t=503\)</span>. Our prediction is
<span class="math display">\[
\widehat{INDEX}_{503} = 7.797 - 3.68 \times 10^{-4}(503) + 8.269
\times 10^{-7}(503)^2 = 7.8208.
\]</span></p>
<p>The overall conclusion is that the regression model using a
quadratic term in time <span class="math inline">\(t\)</span> as an explanatory variable fits the data
well. A close inspection of Figure <span class="math inline">\(\ref{F7:HKFits}\)</span>, however, reveals
that there are patterns in the residuals where the responses are in
some places consistently higher and in other places consistently
lower than the fitted values. These patterns suggest that we can
improve upon the model specification. One way would be to introduce
a higher order polynomial model in time. In Section 7.3, we will
argue that the random walk is an even better model for this data.</p>
<hr />
<p>Other nonlinear functions of time may also be useful. To illustrate,
we might study some measure of interest rates over time (<span class="math inline">\(y_t\)</span>) and
be interested in the effect of a change in the economy (such as the
advent of a war). Define <span class="math inline">\(z_t\)</span> to be a binary variable that is zero
before the change occurs and is one during and after the change.
Consider the model,
<span class="math display">\[\begin{equation} \label{E7:4}
y_t = \beta_0 + \beta_1 z_t + \varepsilon_t.
\end{equation}\]</span>
Thus, using
<span class="math display">\[
\mathrm{E~}y_t = \left\{
\begin{array}{ll}
\beta_0 + \beta_1 &amp; if~z_t=1 \\
\beta_0 &amp; if~z_t = 0
\end{array}
\right. ,
\]</span>
the parameter <span class="math inline">\(\beta_1\)</span> captures the expected change in interest
rates due to the change in the economy. See Figure
<span class="math inline">\(\ref{F7:RegimeSwitch}\)</span>.</p>
<hr />
<p>With the assumption
of normality, we can write the model in equation (<span class="math inline">\(\ref{E7:4}\)</span>) as
<span class="math display">\[
y_t \sim \left\{
\begin{array}{ll}
N(\mu_1, \sigma^2) &amp; t &lt; t_0 \\
N(\mu_2, \sigma^2)&amp; t \geq t_0
\end{array}
\right. ,
\]</span>
where <span class="math inline">\(\mu_1 = \beta_0\)</span>, <span class="math inline">\(\mu_2 = \beta_0 + \beta_1\)</span> and <span class="math inline">\(t_0\)</span> is
the change point. A model generalizes this
concept, primarily by assuming that the change point is not known.
Instead, one assumes there exists a transition mechanism that allows
us to shift from one “regime” to another with a probability that
is typically estimated from the data. In this model, there is a
finite number of states, or “regimes.” Within each regime, a
probabilistic model is specified, such as the (conditionally)
independent normal distribution (<span class="math inline">\(N(\mu_2, \sigma^2)\)</span>). One could
also specify an autoregressive or conditionally autoregressive model
that we will define in Chapter 8. Further, there is a conditional
probability of transiting from one state to another (so-called
“Markov” transition probabilities).</p>
<p>Hardy (2001) introduced regime-switching models to the actuarial
literature where the dependent variable of interest was the
long-term stock market return as measured by monthly returns on (1)
the Standard and Poor’s 500 and the Toronto Stock Exchange 300.
Hardy considered two and three regime models for data over 1956 to
1999, inclusive. Hardy showed how to use the parameter estimates
from the regime-switching model to compute option prices and risk
measures for equity-linked insurance contracts.</p>
<hr />
</div>
<div id="fitting-seasonal-trends" class="section level3 unnumbered hasAnchor">
<h3>Fitting Seasonal Trends<a href="modeling-trends-c7trends.html#fitting-seasonal-trends" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Regular periodic behavior is often found in business and economic
data. Because such periodicity is often tied to the climate, these
trends are called . Seasonal trends can be
modeled using the same techniques as with regular, or aperiodic,
trends. The following example shows how to capture periodic behavior
using seasonal binary variables.
***</p>
<p> On
any given election day, the number of voters that actually turn out
to voting booths depend on a number of factors: the publicity that
an election race has received, the issues that are debated as part
of the race, other issues facing voters on election day, and
nonpolitical factors, such as the weather. Now, potential political
candidates base their projections of campaign financing, and chances
of winning an election, on forecasts of the number of voters who
will actually participate in an election. Decisions as to whether or
not to participate as a candidate must be made well in advance;
generally, so far in advance that well-known factors such as the
weather on election day can not be used in generating forecasts.</p>
<p>We consider here the number of Wisconsin voters who participated in
statewide elections over the period 1920 through 1990. Although the interest
is in forecasting the actual number of voters, we consider voters as a
percentage of the qualified voting public. Dividing by the qualified voting
public controls for the size of the population of voters; this enhances
comparability between the early and latter parts of the series. Because
mortality trends are relatively stable, reliable projections of the
qualified voting public can be readily attained. Forecasts of the percentage
may then be multiplied by projections of the voting public to obtain
forecasts of the actual voter turnout.</p>
<p>To specify a model, we examine Figure <span class="math inline">\(\ref{F7:WiscVotes}\)</span>, a time
series plot of the voter turnout as a percent of the qualified
voting public. This figure displays the low voter turnout in the
early part of the series, followed by larger turnout in the 1950’s
and 1960’s, followed by a smaller turnout in the 1980’s. This
pattern can be modeled using, for example, a quadratic trend in
time. The figure also displays a much larger turnout in presidential
elections years. This periodic, or seasonal, component can be
modeled using a binary variable. A candidate model is
<span class="math display">\[
y_t=\beta_0+\beta_1t+\beta_2t^2+\beta_{3}z_t+\varepsilon _t,
\]</span>
where</p>
<p><span class="math display">\[
z_t=\left\{
\begin{array}{ll}
1 &amp; if~\mathrm{presidential\ election\ year} \\
0 &amp; otherwise
\end{array}
\right. .
\]</span>
Here, <span class="math inline">\(\beta_{3}z_t\)</span> captures the seasonal component in this model.</p>
<p>Regression was used to fit the model. The fitted model provided a
good fit of the data - the coefficient of determination from the fit
was <span class="math inline">\(R^2=89.6\%.\)</span> Figure <span class="math inline">\(\ref{F7:WiscVotes}\)</span> shows a strong
relationship between the fitted and actual values.</p>
<hr />
<p>The voting trend example demonstrates the use of binary variables to
capture seasonal components. Similarly, seasonal effects may be also
be represented using categorical variables, such as
<span class="math display">\[
z_t=\left\{
\begin{array}{ll}
1 &amp; if\text{~spring} \\
2 &amp; if\text{~summer} \\
3 &amp; if\text{~fall} \\
4 &amp; if\text{~winter}%
\end{array}%
\right. .
\]</span></p>
<p>Another way of capturing seasonal effects is through the use of
trigonometric functions. Further discussion of the use of trigonometric
functions to handle seasonal components is in Section 9.3.</p>
<p>Removal of seasonal patterns is known as .
This strategy is appropriate in public policy situations where
interest centers on interpreting the resulting “seasonally adjusted
series.” For example, government agencies typically report
industrial manufacturing revenues in terms of seasonally adjusted
numbers, with the understanding that known holiday and weather
related patterns are accounted for when reporting growth. However,
for most actuarial and risk management applications, the interest is
typically in forecasting the variation of the entire series, not
just the seasonally adjusted portion.</p>
</div>
<div id="reliability-of-time-series-forecasts" class="section level3 unnumbered hasAnchor">
<h3>Reliability of Time Series Forecasts<a href="modeling-trends-c7trends.html#reliability-of-time-series-forecasts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Time series forecasts are sometimes called “naive” forecasts. The
adjective “naive” is somewhat ironic because many time series
forecasting techniques are technical in nature and complex to
compute. However, these forecasts are based on extrapolating a
single series of observations. Thus, they are naive in the sense
that the forecasts ignore other sources of information that may be
available to the forecaster and users of the forecasts. Despite
ignoring this possibly important information, time series forecasts
are useful in that they provide an objective benchmark that other
forecasts and expert opinions can be compared against.</p>
<p>Projections should provide a user with a sense of the reliability of
the forecast. One way of quantifying this is to provide forecasts
under “low-intermediate-high” sets of assumptions. For example, if
we are forecasting the national debt, we might do so under three
scenarios of the future performance of the economy. Alternatively,
we can calculate prediction intervals using many of the models for
forecasting that are discussed in this text. Prediction intervals
provide a measure of reliability that can be interpreted in a
familiar probabilistic sense. Further, by varying the desired level
of confidence, the prediction intervals vary, thus allowing us to
respond to “what if” types of questions.</p>
<p>For example, in Figure 21.10 you will find a comparison of
“low-intermediate-high” projections to prediction intervals for
forecasts of the inflation rate (CPI) used in projecting Social
Security funds. The low-intermediate-high projections are based on a
range of expert opinions and thus reflect variability of the
forecasters. The prediction intervals reflect innovation uncertainty
in the model (assuming that the model is correct). Both ranges give
the user a sense of reliability of the forecasts although in
different ways.</p>
<p>Prediction intervals have the additional advantage in that they
quantify the fact that forecasts become less reliable the further
that we forecast into the future. Even with cross-sectional data, we
saw that the farther away we were from the main part of the data,
the less confident we felt in our predictions. This is also true in
forecasting for longitudinal data. It is important to communicate
this to consumers of forecasts, and prediction intervals are a
convenient way of doing so.</p>
<p>In summary, regression analysis using various functions of time as
explanatory variables is a simple yet powerful tool for forecasting
longitudinal data. It does, however, have drawbacks. Because we are
fitting a curve to the entire data set, there is no guarantee that
the fit for the most recent part of the data will be adequate. That
is, for forecasting, the primary concern is for the most recent part
of the series. We know that regression analysis estimates give the
most weight to observations with unusually large explanatory
variables. To illustrate, using a linear trend in time model, this
means giving the most weight to observations at the end and of the series. Using a model that gives large weight
to observations at the beginning of the series is viewed with
suspicion by forecasters. This drawback of regression analysis
motivates us to introduce additional forecasting tools. (Section 9.1
develops this point further.)</p>
</div>
</div>
<div id="S7:RandomWalk" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Stationarity and Random Walk Models<a href="modeling-trends-c7trends.html#S7:RandomWalk" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A basic concern with processes that evolve over time is the
of the process. For example: “Is it taking me
longer to get to work since they put in the new stop light?” “Have
quarterly earnings improved since the new CEO took over?” We
measure processes to improve or manage their performance and to
forecast the future of the process. Because stability is a
fundamental concern, we will work with a special kind of stability
called .</p>
<div class="blackbox">
<p>  Stationarity is the formal mathematical
concept corresponding to the “stability” of a time series of data.
A series is said to be (weakly) stationary if</p>
</div>
<p>Thus, for example, under weak stationarity $y_{4}=%
y_{8} $ because the means do not depend on time and thus are equal. Further,
<span class="math inline">\(\mathrm{Cov}(y_{4},y_{6})=\mathrm{Cov}(y_{6},y_{8})\)</span>, because <span class="math inline">\(y_{4}\)</span> and <span class="math inline">\(% y_{6}\)</span> are two time units apart, as are <span class="math inline">\(y_{6}\)</span> and <span class="math inline">\(y_{8}\)</span>. As
another implication of the second condition, note that <span class="math inline">\(\sigma^2 = \mathrm{Cov}(y_t, y_t) = \mathrm{Cov}(y_s, y_s) = \sigma^2\)</span>. Thus, a
weakly stationary series has a constant mean as well as a constant
variance (homoscedastic). Another type of stationarity known as
requires that the entire
distribution of <span class="math inline">\(y_t\)</span> be constant over time, not just the mean and
the variance.</p>
<div id="white-noise" class="section level3 unnumbered hasAnchor">
<h3>White Noise<a href="modeling-trends-c7trends.html#white-noise" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The link between longitudinal and cross-sectional models can be
established through the notion of a process. A
white noise process is a stationary process that displays no
apparent patterns through time. More formally, a white noise process
is simply a series that is , identically and
independently distributed. A white noise process is only one type of
stationary process - Chapter 8 will introduce another type, an
autoregressive model.</p>
<p>A special feature of the white noise process is that forecasts do
not depend on how far into the future that we wish to forecast.
Suppose that a series of observations, <span class="math inline">\(y_1,...,y_T\)</span>, has been
identified as a white noise process. Let <span class="math inline">\(\overline{y}\)</span> and <span class="math inline">\(s_y\)</span>
denote the sample average and standard deviation, respectively. A
forecast of an observation in the future, say <span class="math inline">\(y_{T+l}\)</span>, for <span class="math inline">\(l\)</span>
lead time units in the future, is <span class="math inline">\(\overline{y}\)</span>. Further, a
forecast interval is
<span class="math display">\[\begin{equation}\label{E7:ForecastInterval}
\overline{y}\pm \ t_{T-1,1-\alpha/2} ~ s_y \sqrt{1+\frac{1}{T}}.
\end{equation}\]</span>
In time series applications, because the sample size <span class="math inline">\(T\)</span> is
typically relatively large, we use the approximate 95% prediction
interval $ s_y. $ This approximate forecast
interval ignores the parameter uncertainty in using <span class="math inline">\(\overline{y}\)</span><br />
and <span class="math inline">\(s_y\)</span> to estimate the mean <span class="math inline">\(\mathrm{E}~y\)</span> and standard
deviation $$ of the series. Instead, it emphasizes the
uncertainty in future realizations of the series (known as
). Note that this interval does
depend on the choice of <span class="math inline">\(l\)</span>, the number of lead units
that we forecast into the future.</p>
<p>The white noise model is both the least and the most important of
time series models. It is the least important in the sense that the
model assumes that the observations are unrelated to one another, an
unlikely event for most series of interest. It is the most important
because our modeling efforts are directed towards reducing a series
to a white noise process. In time series analysis, the procedure for
reducing a series to a white noise process is called a
. After all patterns have been filtered from the data,
the uncertainty is said to be .</p>
</div>
<div id="random-walk" class="section level3 unnumbered hasAnchor">
<h3>Random Walk<a href="modeling-trends-c7trends.html#random-walk" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now introduce the . For this time series
model, we will show how to filter the data simply by taking
differences.</p>
<p>To illustrate, suppose that you play a simple game based on the roll
of the two dice. To play the game, you must pay $7 each time you
roll the dice. You receive the number of dollars corresponding the
sum of the two dice, <span class="math inline">\(c_t^{\ast}\)</span>. Let <span class="math inline">\(c_t\)</span> denote your winnings on
each roll, so that <span class="math inline">\(c_t = c_t^{\ast} - 7\)</span>. Assuming that the rolls
are independent and come from the same distribution, the series
<span class="math inline">\(\{c_t\}\)</span> is a white noise process.</p>
<p>Assume that you start with initial capital of <span class="math inline">\(y_0 = \$100\)</span>. Let
<span class="math inline">\(y_t\)</span> denote the sum of capital after the <span class="math inline">\(t\)</span>th roll. Note that
<span class="math inline">\(y_t\)</span> is determined recursively by <span class="math inline">\(y_t = y_{t-1} + c_t\)</span>. For
example, because you won $3 on the first roll, <span class="math inline">\(t=1\)</span>, you now have
capital <span class="math inline">\(y_1 = y_0 + c_1\)</span>, or 103 = 100 + 3. Table <span class="math inline">\(\ref{T7:Winnings}\)</span>
shows the results for the first five throws. Figure <span class="math inline">\(\ref{F7:RWDice}\)</span>
is a time series plot of the sums, <span class="math inline">\(y_t\)</span>, for the fifty throws.</p>
<p>The partial sums of a white noise process define a random walk
model. For example, the series <span class="math inline">\(\{y_1, \ldots ,y_{50}\}\)</span> in Figure
<span class="math inline">\(\ref{F7:RWDice}\)</span> is a realization of the random walk model. The
phrase  is used because each observation, <span class="math inline">\(y_t\)</span>,
was created by summing the winnings up to time <span class="math inline">\(t\)</span>. For this
example, winnings, <span class="math inline">\(c_t\)</span>, are a white noise process because the
amount returned, <span class="math inline">\(c_t^{\ast}\)</span>, is i.i.d. In our example, your
winnings from each roll of the dice is represented using a white
noise process. Whether you win on one roll of the dice has no
influence on the outcome of the next, or previous, roll of the dice.
In contrast, your amount of capital at any roll of the dice is
highly related to the amount of capital after the next roll, or
previous roll. Your amount of capital after each roll of the dice is
represented by a random walk model.</p>
</div>
</div>
<div id="inference-using-random-walk-models" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Inference using Random Walk Models<a href="modeling-trends-c7trends.html#inference-using-random-walk-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The random walk is a commonly used time series model. To see how it
can be applied, we first discuss a few model properties. These
properties are then used to forecast and identify a series as a
random walk. Finally, this section compares the random walk to a
competitor, the linear trend in time model.</p>
<div id="model-properties" class="section level3 unnumbered hasAnchor">
<h3>Model Properties<a href="modeling-trends-c7trends.html#model-properties" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To state the properties of the random walk, we first recap some
definitions. Let <span class="math inline">\(c_1,\ldots ,c_T\)</span> be <span class="math inline">\(T\)</span> observations from a white
noise process. A random walk can be expressed recursively as
<span class="math display">\[\begin{equation} \label{E7:5}
y_t = y_{t-1} + c_t.
\end{equation}\]</span>
By repeated substitution, we have
<span class="math display">\[
y_t = c_t + y_{t-1} = c_t + \left( c_{t-1} + y_{t-2}\right) = \ldots
\]</span>
If we use <span class="math inline">\(y_0\)</span> to be the initial level, then we can express the
random
walk as%
<span class="math display">\[\begin{equation}\label{E7:6}
y_t = y_0 + c_1 + \ldots + c_t.
\end{equation}\]</span>
Equation (<span class="math inline">\(\ref{E7:6}\)</span>) shows that a random walk is the partial sum of
a white noise process.</p>
<p>The random walk is a stationary process because the
variability, and possibly the mean, depends on the time point at
which the series is observed. Taking the expectation and variance of
equation (<span class="math inline">\(\ref{E7:6}\)</span>) yields the mean level and variability of the
random walk process:
<span class="math display">\[
\mathrm{E~}y_t = y_0 + t\mu_c\text{ \ \ and \ \ }\mathrm{Var~} y_t =
t \sigma_c^2,
\]</span>
where <span class="math inline">\(\mathrm{E~}c_t = \mu_c\)</span> and <span class="math inline">\(\mathrm{Var~}c_t = \sigma _c^2\)</span>. Hence, as long as there is some variability in the white
noise process (<span class="math inline">\(\sigma_c^2 &gt; 0\)</span>), the random walk is nonstationary
in the variance. Further, if <span class="math inline">\(\mu_c\neq 0\)</span>, then the random walk is
nonstationary in the mean.</p>
</div>
<div id="forecasting" class="section level3 unnumbered hasAnchor">
<h3>Forecasting<a href="modeling-trends-c7trends.html#forecasting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How can we forecast a series of observations, <span class="math inline">\(y_1,...,y_T\)</span>, that
has been identified as a realization of a random walk model? The
technique we use is to forecast the , or
, in the series and then sum the forecast differences
to get the forecast series. This technique is tractable because, by
the definition of a random walk model, the differences can be
represented using a white noise process, a process that we know how
to forecast.</p>
<p>Consider <span class="math inline">\(y_{T+l}\)</span>, the value of the series <span class="math inline">\(l\)</span> lead time units into
the future. Let <span class="math inline">\(c_t=y_t-y_{t-1}\)</span> represent the differences in the
series, so that
<span class="math display">\[\begin{eqnarray*}
y_{T+l} &amp;=&amp;y_{T+l-1}+c_{T+l} = \left( y_{T+l-2} + c_{T+l-1}\right)
+c_{T+l} = \ldots
\\
&amp;=&amp;y_T+c_{T+1}+ \ldots +c_{T+l}.
\end{eqnarray*}\]</span>%
We interpret <span class="math inline">\(y_{T+l}\)</span> to be the current value of the series, <span class="math inline">\(y_T\)</span>,
plus the partial sum of future differences.</p>
<p>To forecast <span class="math inline">\(y_{T+l}\)</span>, because at time <span class="math inline">\(T\)</span> we know <span class="math inline">\(y_T\)</span>, we need
only forecast the changes <span class="math inline">\(\{c_{T+1}, \ldots, c_{T+l}\}\)</span>. Because a
forecast of a future value of a white noise process is just the
average of the process, the forecast of <span class="math inline">\(c_{T+k}\)</span> is <span class="math inline">\(\overline{c}\)</span><br />
for <span class="math inline">\(k=1,2,\ldots,l\)</span>. Putting these together, the forecast of
<span class="math inline">\(y_{T+l}\)</span> is <span class="math inline">\(y_T+l\overline{c}\)</span> . For example, for <span class="math inline">\(l=1\)</span>, we
interpret the forecast of the next value of the series to be the
current value of the series plus the average change of the series.</p>
<p>Using similar ideas, we have that an approximate 95% prediction interval
for <span class="math inline">\(y_{T+l}\)</span> is%
<span class="math display">\[
y_T+l\overline{c}\pm 2s_c\sqrt{l}
\]</span>
where <span class="math inline">\(s_c\)</span> is the standard deviation computed using the changes
<span class="math inline">\(c_2,c_{3},\ldots,c_T\)</span>. Note that the width of the prediction
interval, <span class="math inline">\(4 s_c \sqrt{l}\)</span>, grows as the lead time <span class="math inline">\(l\)</span> grows. This
increasing width simply reflects our diminishing ability to predict
into the future.</p>
<p>As an example, we rolled the dice <span class="math inline">\(T=50\)</span> times and that we would
like to forecast <span class="math inline">\(y_{60}\)</span>, our sum of capital after 60 rolls. At
time 50, it turned out that our sum of money available was
<span class="math inline">\(y_{50}=\$93\)</span>. Starting with <span class="math inline">\(y_0 = \$100\)</span>, the average change was
<span class="math inline">\(\overline{c} = -7/50 = \$-0.14\)</span>, with standard deviation
<span class="math inline">\(s_c=\$2.703\)</span>. Thus, the forecast at time 60 is <span class="math inline">\(93+10(-.14) = 91.6\)</span>. The corresponding 95% prediction interval
is%
<span class="math display">\[
91.6\pm 2\left( 2.703\right) \sqrt{10}=91.6\pm 17.1=\left( 74.5,108.7\right)
.
\]</span></p>
<hr />
<p> Labor force participation rate (<span class="math inline">\(LFPR\)</span>)
forecasts, coupled with forecasts of the population, provide us with
a picture of a nation’s future workforce. This picture provides
insights to the future workings of the overall economy, and thus
<span class="math inline">\(LFPR\)</span> projections are of interest to a number of government
agencies. In the United States, <span class="math inline">\(LFPR\)</span>s are projected by the Social
Security Administration, the Bureau of Labor Statistics, the
Congressional Budget Office and the Office of Management and Budget.
In the context of Social Security, policy-makers use labor force
projections to evaluate proposals for reforming the Social Security
system and to assess its future financial solvency.</p>
<p>A labor force participation rate is the civilian labor force divided
by the civilian noninstitutional population. These data are compiled
by the Bureau of Labor Statistics. For illustration purposes, let us
look at a specific demographic cell and show how to forecast it -
forecasts of other cells may be found in Fullerton (1999) and Frees
(2006). Specifically, we examine 1968-1998 for females, aged 20-44,
living in a household with a spouse present and at least one child
under six years of age. Figure <span class="math inline">\(\ref{F7:LFPR}\)</span> shows the rapid
increase in <span class="math inline">\(LFPR\)</span> for this group over <span class="math inline">\(T=31\)</span> years.</p>
<p>To forecast the <span class="math inline">\(LFPR\)</span> with a random walk, we begin with our most recent
observation, <span class="math inline">\(LFPR_{31}=0.6407\)</span>. We denote the change in the <span class="math inline">\(LFPR\)</span> by $%
c_t $, so that <span class="math inline">\(c_t=LFPR_t-LFPR_{t-1}\)</span>. It turns out that the
average change is <span class="math inline">\(\overline{c}=0.0121\)</span> with standard deviation
<span class="math inline">\(s_c=0.0101\)</span>. Thus, using a random walk model, an approximate 95%
prediction interval for
the <span class="math inline">\(l\)</span>-step forecast is%
<span class="math display">\[
0.6407+0.0121l\pm \ 0.0202\sqrt{l}.
\]</span>
Figure <span class="math inline">\(\ref{F7:LFPRFore}\)</span> illustrates prediction intervals for 1999
through 2002, inclusive.</p>
<hr />
</div>
<div id="identifying-stationarity" class="section level3 unnumbered hasAnchor">
<h3>Identifying Stationarity<a href="modeling-trends-c7trends.html#identifying-stationarity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have seen how to do useful things, like forecasting, with random
walk models. But how do we identify a series as a realization from a
random walk? We know that the random walk is a special kind of
nonstationary model and so the first step is to examine a series and
decide whether or not it is stationary.</p>
<p>Stationarity quantifies the stability of a process. A process that
is strictly stationary has the same distribution over time, so we
should be able to take successive samples of modest size and show
that they have approximately the same distribution. For weak
stationary, the mean and variance are stable over time, so if one
takes successive samples of modest size, then we expect the mean
level and the variance to be roughly similar. To illustrate, when
examining time series plots, if you look at the first five, the next
five, the following five and so forth, successive samples, you
should observe approximately the same levels of averages and
standard deviations.</p>
<p>In quality management applications, this approach is quantified by
looking at . A control chart is a useful
graphical device for detecting the lack of stationarity in a time
series. The basic idea is to superimpose reference lines called
on a time series plot of the data. These
reference lines help us visually detect trends in the data and
identify unusual points. The mechanics behind controls limits are
straightforward. For a given series of observations, calculate the
series mean and standard deviation, <span class="math inline">\(\overline{y}\)</span> and <span class="math inline">\(s_y\)</span>.
Define the
“upper control limit” by <span class="math inline">\(UCL=\overline{y}% +3s_y\)</span> and the “lower control limit” by <span class="math inline">\(LCL=\overline{y}-3s_y\)</span>.
Time series plots with these superimposed control limits are known
as control charts.</p>
<p>Sometimes the adjective is associated with this
type of control chart. This adjective reminds the user that averages
and standard deviations are based on all the available data. In
contrast, when the control chart is used as an ongoing management
tool for detecting whether an industrial process is “out of
control,” a may be more suitable.
Here, prospective merely means using only an early portion of the
process, that is “in control,” to compute the control limits.</p>
<p>A control chart that helps us to examine the stability of the mean
is the <span class="math inline">\(Xbar\)</span> chart. An <span class="math inline">\(Xbar\)</span> chart is created by combining
successive observations of modest size, taking an average over this
group, and then creating a control chart for the group averages. By
taking averages over groups, the variability associated with each
point on the chart is smaller than for a control chart for
individual observations. This allows the data analyst to get a
clearer picture of any patterns that may be evident in the mean of
the series.</p>
<p>A control chart that helps us examine the stability of the variability is
the <span class="math inline">\(R\)</span> chart. As with the <span class="math inline">\(Xbar\)</span> chart, we begin by forming successive
groups of modest size. With the <span class="math inline">\(R\)</span> chart, for each group we compute the
range, which is the largest minus the smallest observation, and then create
a control chart for the group ranges. The range is a measure of variability
that is simple to compute, an important advantage in manufacturing
applications.</p>
</div>
<div id="identifying-random-walks" class="section level3 unnumbered hasAnchor">
<h3>Identifying Random Walks<a href="modeling-trends-c7trends.html#identifying-random-walks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose that you suspect that a series is nonstationary, how do
identify the fact that these are realizations of a random walk
model? Recall that the expected value of a random walk,
<span class="math inline">\(\mathrm{E~}y_t=y_0+t\mu_c\)</span>, suggests that such a series follows a
linear trend in time. The variance of a random walk,
<span class="math inline">\(\mathrm{Var~}y_t=t\sigma_c^2\)</span>, suggests that the variability of a
series gets larger as time <span class="math inline">\(t\)</span> gets large. First, a control chart
can help us to detect these patterns, whether they are of a linear
trend in time, increasing variability, or both.</p>
<p>Second, if the original data follows a random walk model, then the
differenced series follows a white noise process model. If a random walk
model is a candidate model, you should examine the differences of the
series. In this case, the time series plot of the differences should be a
stationary, white noise process that displays no apparent patterns. Control
charts can help us to detect this lack of patterns.</p>
<p>Third, compare the standard deviations of the original series and
the differenced series. We expect the standard deviation of the
original series to be greater than the standard deviation of the
differenced series. Thus, if the series can be represented by a
random walk, we expect a substantial reduction in the standard
deviation when taking differences.</p>
<hr />
<p> In
Figure <span class="math inline">\(\ref{F7:LFPR}\)</span>, the series displays a clear upward trend
whereas the differences show no apparent trends over time. Further,
when computing differences of each series, it turned out
that%
<span class="math display">\[
0.1197=SD(series)&gt;SD(differences)=0.0101.
\]</span>
Thus, it seems reasonable to tentatively use a random walk as a
model of the labor force participation rate series.</p>
<p>In Chapter 8, we will discuss two additional identification devices.
These are scatter plots of the series versus a lagged version of the
series and the corresponding summary statistics called
.</p>
<hr />
</div>
<div id="random-walk-versus-linear-trend-in-time-models" class="section level3 unnumbered hasAnchor">
<h3>Random Walk versus Linear Trend in Time Models<a href="modeling-trends-c7trends.html#random-walk-versus-linear-trend-in-time-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The labor force participation rate example could be represented
using either a random walk or a linear trend in time model. These
two models are more closely related to one another than is evident
at first glance. To see this relationship, recall that the linear
trend in time model can be written as
<span class="math display">\[\begin{equation}\label{E7:7}
y_t = \beta_0 + \beta_1 t + \varepsilon_t,
\end{equation}\]</span>
where <span class="math inline">\(\{\varepsilon_t\}\)</span> is a white noise process. If <span class="math inline">\(\{y_t\}\)</span> is
a random walk, then it can be modeled as a partial sum as in
equation (<span class="math inline">\(\ref{E7:6}\)</span>). We can also decompose the white noise process
into a mean <span class="math inline">\(\mu _c\)</span> plus another white noise process, that is, <span class="math inline">\(c_t = \mu_c + \varepsilon_t\)</span>. Combining these two ideas, a random walk
model can be written as</p>
<p><span class="math display">\[\begin{equation} \label{E7:8}
y_t = y_0 + \mu_c t + u_t
\end{equation}\]</span>
where <span class="math inline">\(u_t = \sum_{j=1}^{t} \varepsilon_j\)</span>. Comparing equations
(<span class="math inline">\(\ref{E7:7}\)</span>) and (<span class="math inline">\(\ref{E7:8}\)</span>), we see that the two models are
similar in that the deterministic portion is a linear function of
time. The difference is in the error component. The error component
for the linear trend in time model is a stationary, white noise
process. The error component for the random walk model is
nonstationary because it is the partial sum of white noise
processes. That is, the error component is also a random walk. Many
introductory treatments of the random walk model focus on the “fair
game” example and ignore the drift term <span class="math inline">\(\mu_c\)</span>. This is
unfortunate because the comparison between the random walk model and
the linear trend in time model is not as clear when the parameter
<span class="math inline">\(\mu_c\)</span> is equal to zero.</p>
</div>
</div>
<div id="filtering-to-achieve-stationarity" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Filtering to Achieve Stationarity<a href="modeling-trends-c7trends.html#filtering-to-achieve-stationarity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A is a procedure for reducing observations to white
noise. In regression, we accomplished this by simply subtracting the
regression function from the observations, that is, <span class="math inline">\(y_i - (\beta_0 + \beta_1 x_{1i} + \ldots + \beta_k x_{ki})=\varepsilon_i\)</span>.
Transformation of the data is another device for filtering that we
introduced in Chapter 1 when analyzing cross-sectional data. We
encountered another example of a filter in Section
<span class="math inline">\(\ref{S7:RandomWalk}\)</span>. There, by taking differences of observations,
we reduced a random walk series to a white noise process.</p>
<p>An important theme of this text is to use an iterative approach for
fitting models to data. In particular, in this chapter we discuss
techniques for reducing a sequence of observations to a stationary
series. By definition, a stationary series is stable and hence is
far easier to forecast than an unstable series. This stage,
sometimes known as the data, generally
accounts for the most important sources of trends in the data. The
next chapter will present models that account for subtler trends in
the data.</p>
<div id="transformations" class="section level3 unnumbered hasAnchor">
<h3>Transformations<a href="modeling-trends-c7trends.html#transformations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When analyzing longitudinal data, transformation is an important
tool used to filter a data set. Specifically, using a logarithmic
transformation tends to shrink “spread out” data. This feature
gives us an alternative method to deal with a process where the
variability appears to grow with time. Recall the first option
discussed is to posit a random walk model and examine differences of
the data. Alternatively, one may take a logarithmic transform that
helps to reduce increasing variance through time.</p>
<p>Further, from the random walk discussion, we know that if both the
series variance and log series variance increase through time, the
differences of the log transform might handle this increasing
variability. Differences of natural logarithms are particularly
pleasing because they can be interpreted as . To see this, define <span class="math inline">\(pchange_t=(y_t/y_{t-1})-1\)</span>. Then,
<span class="math display">\[
\ln y_t-\ln y_{t-1} = \ln \left( \frac{y_t}{y_{t-1}}\right) = \ln
\left( 1+pchange_t\right) \approx pchange_t.
\]</span>
Here we use the Taylor series approximation <span class="math inline">\(\ln (1+x) \approx x\)</span>
that is appropriate for small values of <span class="math inline">\(|x|\)</span>.</p>
<hr />
<p>
An important task of a financial analyst is to quantify costs
associated with future cash flows. We consider here funds invested
in a standard measure of overall market performance, the Standard
and Poor’s (S&amp;P) 500 Composite Index. The goal is to forecast the
performance of the portfolio for discounting of cash flows.</p>
<p>In particular, we examine the S&amp;P Composite Quarterly Index for the
years 1936 to 2007, inclusive. By today’s standards, this period may
not be the most representative because the Depression of the 1930’s
is included. The motivation to analyze these data is from the
Institute of Actuaries “Report of the Maturity Guarantees Working
Party” (1980) who analyzed the series from 1936 to 1977, inclusive.
This paper studied the long term behavior of investment returns from
an actuarial viewpoint. We complement that work by showing how
graphical techniques can suggest a useful transformation for
reducing the data to a stationary process.</p>
<p>The data are shown in Figure <span class="math inline">\(\ref{F7:SandPTS}\)</span>. From the original
index values in the upper left-hand panel, we see that the mean
level and variability increases with time. This pattern clearly
indicates that the series is nonstationary.</p>
<p>From our discussions in Sections <span class="math inline">\(\ref{S7:Trends}\)</span> and
<span class="math inline">\(\ref{S7:RandomWalk}\)</span>, a candidate model that has these properties is
the random walk. However, the time series plot of the differences,
in upper right-hand panel of Figure <span class="math inline">\(\ref{F7:SandPTS}\)</span>, still
indicates a pattern of variability increasing with time. The
differences are not a white noise process so the random walk is not
a suitable model for the S &amp; P 500 Index.</p>
<p>An alternative transformation is to consider logarithmic values of
the series. The time series plot of logged values, presented in
lower left-hand panel of Figure <span class="math inline">\(\ref{F7:SandPTS}\)</span>, indicates the the
mean level of the series increases over time and is not level. Thus,
the logarithmic index is not stationary.</p>
<p>Yet another approach is to examine differences of the logarithmic
series. This is especially desirable when looking at indices, or
breadbaskets, because the
difference of logarithms can be interpreted as proportional changes.
From the final time series plot, in the lower right-hand panel of
Figure <span class="math inline">\(\ref{F7:SandPTS}\)</span>, we see that there are fewer discernible
patterns in the transformed series, the difference of logs. This
transformed series seems to be stationary. It is interesting to note
that there seems to be a higher level of volatility at the beginning
of the series. This type of changing volatility is more difficult to
model and has recently been the subject of considerable attention in
the financial economics literature (see, for example, Hardy, 2003).</p>
<hr />
</div>
</div>
<div id="forecast-evaluation" class="section level2 hasAnchor" number="7.6">
<h2><span class="header-section-number">7.6</span> Forecast Evaluation<a href="modeling-trends-c7trends.html#forecast-evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Judging the accuracy of forecasts is important when modeling time series
data. In this section, we present forecast evaluation techniques that:</p>
<p>In the first five sections of Chapter 7, we presented several techniques for
detecting patterns in residuals from a fitted model. Measures that summarize
the distribution of residuals are called .
As we saw in our study of cross-sectional models, by fitting several
different models to a data set, we introduce the possibility of overfitting
the data. To address this concern, we will use techniques, similar to those introduced in Section 6.5.</p>
<p>To perform an out-of-sample validation of a proposed model, ideally
one would develop the model on a data set and then corroborate the
model’s usefulness on a second, independent data set. Because two
such ideal data sets are rarely available, in practice we can split
a data set into two subsamples, a
and a . For longitudinal data, the
practice is to use the beginning part of the series, the first <span class="math inline">\(T_1\)</span>
observations, to develop one or more candidate models. The latter
part of the series, the last <span class="math inline">\(T_2=T-T_1\)</span> observations, are used to
evaluate the forecasts. For example, we might have ten years of
monthly data so that <span class="math inline">\(T=120\)</span>. It would be reasonable to use the
first eight years of data to develop a model and the last two years
of data for validation, yielding <span class="math inline">\(T_1=96\)</span> and <span class="math inline">\(T_2=24\)</span>.</p>
<p>Thus, observations <span class="math inline">\(y_1,\ldots , y_{T_1}\)</span> are used to develop a
model. From these <span class="math inline">\(T_1\)</span> observations, we can determine the
parameters of the candidate model. Using the fitted model, we can
determine fitted values for the model validation subsample for <span class="math inline">\(t = T_1 + 1,T_1+2, \ldots, T_1+T_2\)</span>. Taking the difference between the
actual and fitted values yield one-step forecast residuals, denoted
by <span class="math inline">\(e_t=y_t-\widehat{y}_t\)</span>. These forecast residuals are the basic
quantities that we will use to evaluate and compare forecasting
techniques.</p>
<p>To compare models, we use a four-step process similar to that described in
Section 6.5, described as follows.</p>
<div class="blackbox">
<p></p>
<p>Repeat Steps 2 through 4 for each of the candidate models. Choose
the model with the smallest set of comparison statistics.</p>
</div>
<p>Out-of-sample validation can be used to compare the accuracy of forecasts
from virtually any forecasting model. As we saw in Section 6.5, we are not
limited to comparisons where one model is a subset of another, where the
competing models use the same units for the response, and so on.</p>
<p>There are several statistics that are commonly used to compare
forecasts.</p>
<div class="blackbox">
<p></p>
<ol style="list-style-type: decimal">
<li><p>The , defined by%
<span class="math display">\[
ME=\frac{1}{T_2}\sum_{t=T_1+1}^{T_1+T_2}e_t.
\]</span>
This statistic measures recent trends that are not anticipated by the model.</p></li>
<li><p>The , defined by
<span class="math display">\[
MPE=\frac{100}{T_2}\sum_{t=T_1+1}^{T_1+T_2}\frac{e_t}{y_t}.
\]</span>
This statistic is also a measure of trend, but examines error relative to
the actual value.</p></li>
<li><p>The , defined by%
<span class="math display">\[
MSE=\frac{1}{T_2}\sum_{t=T_1+1}^{T_1+T_2}e_t^2.
\]</span>
This statistic can detect more patterns than <span class="math inline">\(ME\)</span>. It is the same as
the cross-sectional <span class="math inline">\(SSPE\)</span> statistic, except for the division by
<span class="math inline">\(T_2\)</span>.</p></li>
<li><p>The , defined by
<span class="math display">\[
MAE=\frac{1}{T_2}\sum_{t=T_1+1}^{T_1+T_2}|e_t|.
\]</span>
Like <span class="math inline">\(MSE\)</span>, this statistic can detect more than trend patterns than <span class="math inline">\(ME\)</span>.
The units of <span class="math inline">\(MAE\)</span> are the same as the dependent variable.</p></li>
<li><p>The , defined by
<span class="math display">\[
MAPE=\frac{100}{T_2}\sum_{t=T_1+1}^{T_1+T_2}|\frac{e_t}{y_t}|.
\]</span>
Like <span class="math inline">\(MAE\)</span>, this statistic can detect more than trend patterns. Like <span class="math inline">\(MPE\)</span>,
it examines error relative to the actual value.</p></li>
</ol>
</div>
<hr />
<p>We
can use out-of-sample validation measures to compare two models for
the <span class="math inline">\(LFPR\)</span>s; the linear trend in time model and the random walk
model. For this illustration, we examined the labor rates for years
1968 through 1994, inclusive. This corresponds to <span class="math inline">\(T_1 = 27\)</span>
observations defined in Step 1. Data were subsequently gathered on
rates for 1995 through 1998, inclusive, corresponding to <span class="math inline">\(T_2 = 4\)</span>
for out-of-sample validation. For Step 2, we fit each model using
<span class="math inline">\(t=1,\ldots,27\)</span>, earlier in this chapter. For Step 3, the one-step
forecasts are:</p>
<p><span class="math display">\[
\widehat{y}_t = 0.2574 + 0.0145t
\]</span>
and%
<span class="math display">\[
\widehat{y}_t = y_{t-1} + 0.0132
\]</span>
for the linear trend in time and the random walk models,
respectively. For Step 4, Table <span class="math inline">\(\ref{T7:ForecastComparison}\)</span>
summarizes the forecast comparison statistics. Based on these
statistics, the choice of the model is clearly the random walk.</p>
<hr />
</div>
<div id="further-reading-and-references" class="section level2 hasAnchor" number="7.7">
<h2><span class="header-section-number">7.7</span> Further Reading and References<a href="modeling-trends-c7trends.html#further-reading-and-references" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For many years, actuaries in North America were introduced to time
series analysis from Miller and Wichern (1977), Abraham and Ledolter
(1983) and Pindyck and Rubinfeld (1991). A more recent book-long
introduction is Diebold (2004). Diebold contains a brief
introduction to regime-switching models.</p>
<p>Because of the difficulties regarding their specification and
limited forecasting use, we do not explore causal models further in
this text. For more details on causal models, the interested reader
is referred to Pindyck and Rubinfeld (1991).</p>
<p></p>
<p>\begin{multicols}{2}</p>
<p>“Report of the Maturity Guarantees Working Party” (1980).
107, pp. 103-213.</p>
<p>Abraham, Bovas and Johannes Ledolter (1983). . John Wiley &amp; Sons, New York.</p>
<p>Diebold, Francis X. (2004). , Third
Edition. Thompson South-Western, Mason, OH.</p>
<p>Frees, Edward W. (2006). Forecasting of labor force participation
rates. 22(3), 453-485.</p>
<p>Fullerton, Howard N., Jr. (1999). Labor force projections to 2008:
steady growth and changing composition. , November, pp. 19-32.</p>
<p>Granger, Clive W. J and P. Newbold (1974). Spurious regressions in
econometrics. 2, 111-120.</p>
<p>Hardy, Mary (2001). A regime-switching model of long-term stock
returns. 5(2), 41-53.</p>
<p>Hardy, Mary (2003). . John Wiley &amp; Sons,
New York.</p>
<p>Miller, Robert B. and Dean W. Wichern (1977). . Holt, Rinehart and Winston, New York.</p>
<p>Pindyck, R.S. and D.L. Rubinfeld (1991). Third Edition, McGraw-Hill, New York.</p>
</div>
<div id="exercises" class="section level2 hasAnchor" number="7.8">
<h2><span class="header-section-number">7.8</span> Exercises<a href="modeling-trends-c7trends.html#exercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider a random walk <span class="math inline">\(\{y_t \}\)</span> as the partial sum of a white noise process <span class="math inline">\(\{ c_t \}\)</span> with
mean <span class="math inline">\(\mathrm{E}~c_t= \mu_c\)</span> and variance <span class="math inline">\(\mathrm{Var}~c_t = \sigma_c^2\)</span>. Use equation (<span class="math inline">\(\ref{E7:6}\)</span>) to show</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\mathrm{E}~y_t= y_0 + t \mu_c\)</span>, where <span class="math inline">\(y_0\)</span> is the initial
value and</p></li>
<li><p><span class="math inline">\(\mathrm{Var}~y_t= t \sigma_c^2\)</span>.</p></li>
</ol>
<p>Consider a random walk <span class="math inline">\(\{y_t \}\)</span> as the partial sum of a white noise process <span class="math inline">\(\{ c_t \}\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Show that the <span class="math inline">\(l\)</span>-step forecast error is
<span class="math inline">\(y_{T+l}-\widehat{y_{T+l}} = \sum_{j=1}^l (c_{T+j} - \bar{c} ).\)</span></p></li>
<li><p>Show that the approximate variance of the <span class="math inline">\(l\)</span>-step forecast error
is <span class="math inline">\(l \sigma_c^2.\)</span></p></li>
</ol>
<p>. The exchange rate that we consider is the amount of Euros that one
can purchase for one US dollar. We have <span class="math inline">\(T=699\)</span> daily observations
from the period April 1, 2005 through January 8, 2008. These data
were obtained from the Federal Reserve (H10 report). :
Federal Reserve Bank of New York. Note: The data are based on noon
buying rates in New York from a sample of market participants and
they represent rates set for cable transfers payable in the listed
currencies. These are also the exchange rates required by the
Securities and Exchange Commission for the integrated disclosure
system for foreign private issuers.</p>
<ol style="list-style-type: lower-alpha">
<li>Figure <span class="math inline">\(\ref{Ex:EuroPlot1}\)</span> is a time series plot of the Euro
exchange rate.</li>
</ol>
<p>a(i). Define the concept of a stationary time series.</p>
<p>a(ii). Is the EURO series stationary? Use your definition in part
a(i) to justify your response.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Based on an inspection of Figure <span class="math inline">\(\ref{Ex:EuroPlot1}\)</span> in part (a),
you decide to fit a quadratic trend model of the data. Figure
<span class="math inline">\(\ref{Ex:EuroPlot2}\)</span> superimposes the fitted value on a plot of the
series.</li>
</ol>
<p>b(i). Cite several basic regression statistics that summarize the
quality of the fit.</p>
<p>b(ii). Briefly describe any residual patterns that you observe in
Figure <span class="math inline">\(\ref{Ex:EuroPlot2}\)</span>.</p>
<p>b(iii). Here, TIME varies from <span class="math inline">\(1, 2, \ldots, 699\)</span>. Using this model
calculate the three-step forecast corresponding to TIME = 702.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>To investigate a different approach, DIFFEURO, calculate the
difference of EURO. You decide to model DIFFEURO as a white noise
process.</li>
</ol>
<p>c(i). What is the name for the corresponding model of EURO?</p>
<p>c(ii). The most recent value of EURO is <span class="math inline">\(EURO_{699} = 0.6795\)</span>. Using
the model identified in part c(i), provide a three-step forecast
corresponding to TIME = 702.</p>
<p>c(iii). Using the model identified in part c(i) and the point
forecast in part c(ii), provide the corresponding 95% prediction
interval for <span class="math inline">\(EURO_{702}\)</span>.</p>
<p>\end{exercises}</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chap-6.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendices.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
