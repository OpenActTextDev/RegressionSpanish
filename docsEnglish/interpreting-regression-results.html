<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Interpreting Regression Results | Regression Modeling with Actuarial and Financial Applications</title>
  <meta name="description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Interpreting Regression Results | Regression Modeling with Actuarial and Financial Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Interpreting Regression Results | Regression Modeling with Actuarial and Financial Applications" />
  
  <meta name="twitter:description" content="HTML version of ‘Regression Modeling with Actuarial and Financial Applications’" />
  

<meta name="author" content="Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="C5VarSelect.html"/>
<link rel="next" href="C7Trends.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript"  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleSolution(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}      
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>



<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Regression Modeling With Actuarial and Financial Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#forward"><i class="fa fa-check"></i>Forward</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#who-is-this-book-for"><i class="fa fa-check"></i>Who Is This Book For?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#what-is-this-book-about"><i class="fa fa-check"></i>What Is This Book About?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#how-does-this-book-deliver-its-message"><i class="fa fa-check"></i>How Does This Book Deliver Its Message?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html"><i class="fa fa-check"></i><b>1</b> Regression and the Normal Distribution</a>
<ul>
<li class="chapter" data-level="1.1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec11"><i class="fa fa-check"></i><b>1.1</b> What is Regression Analysis?</a></li>
<li class="chapter" data-level="1.2" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec12"><i class="fa fa-check"></i><b>1.2</b> Fitting Data to a Normal Distribution</a></li>
<li class="chapter" data-level="1.3" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec13"><i class="fa fa-check"></i><b>1.3</b> Power Transforms</a></li>
<li class="chapter" data-level="1.4" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec14"><i class="fa fa-check"></i><b>1.4</b> Sampling and the Role of Normality</a></li>
<li class="chapter" data-level="1.5" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec15"><i class="fa fa-check"></i><b>1.5</b> Regression and Sampling Designs</a></li>
<li class="chapter" data-level="1.6" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec16"><i class="fa fa-check"></i><b>1.6</b> Actuarial Applications of Regression</a></li>
<li class="chapter" data-level="1.7" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec17"><i class="fa fa-check"></i><b>1.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="1.8" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec18"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li>
<li class="chapter" data-level="1.9" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#Sec19"><i class="fa fa-check"></i><b>1.9</b> Technical Supplement - Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="C2BasicLR.html"><a href="C2BasicLR.html"><i class="fa fa-check"></i><b>2</b> Basic Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec21"><i class="fa fa-check"></i><b>2.1</b> Correlations and Least Squares</a></li>
<li class="chapter" data-level="2.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec22"><i class="fa fa-check"></i><b>2.2</b> Basic Linear Regression Model</a></li>
<li class="chapter" data-level="2.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec23"><i class="fa fa-check"></i><b>2.3</b> Is the Model Useful? Some Basic Summary Measures</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec231"><i class="fa fa-check"></i><b>2.3.1</b> Partitioning the Variability</a></li>
<li class="chapter" data-level="2.3.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec232"><i class="fa fa-check"></i><b>2.3.2</b> The Size of a Typical Deviation: <em>s</em></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec24"><i class="fa fa-check"></i><b>2.4</b> Properties of Regression Coefficient Estimators</a></li>
<li class="chapter" data-level="2.5" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec25"><i class="fa fa-check"></i><b>2.5</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec251"><i class="fa fa-check"></i><b>2.5.1</b> Is the Explanatory Variable Important?: The <em>t</em>-Test</a></li>
<li class="chapter" data-level="2.5.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec252"><i class="fa fa-check"></i><b>2.5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="2.5.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec253"><i class="fa fa-check"></i><b>2.5.3</b> Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec26"><i class="fa fa-check"></i><b>2.6</b> Building a Better Model: Residual Analysis</a></li>
<li class="chapter" data-level="2.7" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec27"><i class="fa fa-check"></i><b>2.7</b> Application: Capital Asset Pricing Model</a></li>
<li class="chapter" data-level="2.8" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec28"><i class="fa fa-check"></i><b>2.8</b> Illustrative Regression Computer Output</a></li>
<li class="chapter" data-level="2.9" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec29"><i class="fa fa-check"></i><b>2.9</b> Further Reading and References</a></li>
<li class="chapter" data-level="2.10" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec210"><i class="fa fa-check"></i><b>2.10</b> Exercises</a></li>
<li class="chapter" data-level="2.11" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec211"><i class="fa fa-check"></i><b>2.11</b> Technical Supplement - Elements of Matrix Algebra</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2111"><i class="fa fa-check"></i><b>2.11.1</b> Basic Definitions</a></li>
<li class="chapter" data-level="2.11.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2112"><i class="fa fa-check"></i><b>2.11.2</b> Some Special Matrices</a></li>
<li class="chapter" data-level="2.11.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2113"><i class="fa fa-check"></i><b>2.11.3</b> Basic Operations</a></li>
<li class="chapter" data-level="2.11.4" data-path="C2BasicLR.html"><a href="C2BasicLR.html#Sec2114"><i class="fa fa-check"></i><b>2.11.4</b> Random Matrices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html"><i class="fa fa-check"></i><b>3</b> Multiple Linear Regression - I</a>
<ul>
<li class="chapter" data-level="3.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec31"><i class="fa fa-check"></i><b>3.1</b> Method of Least Squares</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec311"><i class="fa fa-check"></i><b>3.1.1</b> Least Squares Method</a></li>
<li class="chapter" data-level="3.1.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec312"><i class="fa fa-check"></i><b>3.1.2</b> General Case with <em>k</em> Explanatory Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec32"><i class="fa fa-check"></i><b>3.2</b> Linear Regression Model and Properties of Estimators</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec321"><i class="fa fa-check"></i><b>3.2.1</b> Regression Function</a></li>
<li class="chapter" data-level="3.2.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec322"><i class="fa fa-check"></i><b>3.2.2</b> Regression Coefficient Interpretation</a></li>
<li class="chapter" data-level="3.2.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec323"><i class="fa fa-check"></i><b>3.2.3</b> Model Assumptions</a></li>
<li class="chapter" data-level="3.2.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec324"><i class="fa fa-check"></i><b>3.2.4</b> Properties of Regression Coefficient Estimators</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec33"><i class="fa fa-check"></i><b>3.3</b> Estimation and Goodness of Fit</a></li>
<li class="chapter" data-level="3.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec34"><i class="fa fa-check"></i><b>3.4</b> Statistical Inference for a Single Coefficient</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec341"><i class="fa fa-check"></i><b>3.4.1</b> The <em>t</em>-Test</a></li>
<li class="chapter" data-level="3.4.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec342"><i class="fa fa-check"></i><b>3.4.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="3.4.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec343"><i class="fa fa-check"></i><b>3.4.3</b> Added Variable Plots</a></li>
<li class="chapter" data-level="3.4.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec344"><i class="fa fa-check"></i><b>3.4.4</b> Partial Correlation Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec35"><i class="fa fa-check"></i><b>3.5</b> Some Special Explanatory Variables</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec351"><i class="fa fa-check"></i><b>3.5.1</b> Binary Variables</a></li>
<li class="chapter" data-level="3.5.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec352"><i class="fa fa-check"></i><b>3.5.2</b> Transforming Explanatory Variables</a></li>
<li class="chapter" data-level="3.5.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec353"><i class="fa fa-check"></i><b>3.5.3</b> Interaction Terms</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec36"><i class="fa fa-check"></i><b>3.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="3.7" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#Sec37"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression - II</a>
<ul>
<li class="chapter" data-level="4.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec41"><i class="fa fa-check"></i><b>4.1</b> The Role of Binary Variables</a></li>
<li class="chapter" data-level="4.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec42"><i class="fa fa-check"></i><b>4.2</b> Statistical Inference for Several Coefficients</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec421"><i class="fa fa-check"></i><b>4.2.1</b> Sets of Regression Coefficients</a></li>
<li class="chapter" data-level="4.2.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec422"><i class="fa fa-check"></i><b>4.2.2</b> The General Linear Hypothesis</a></li>
<li class="chapter" data-level="4.2.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec423"><i class="fa fa-check"></i><b>4.2.3</b> Estimating and Predicting Several Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec43"><i class="fa fa-check"></i><b>4.3</b> One Factor ANOVA Model</a></li>
<li class="chapter" data-level="4.4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec44"><i class="fa fa-check"></i><b>4.4</b> Combining Categorical and Continuous Explanatory Variables</a></li>
<li class="chapter" data-level="4.5" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec45"><i class="fa fa-check"></i><b>4.5</b> Further Reading and References</a></li>
<li class="chapter" data-level="4.6" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec46"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec47"><i class="fa fa-check"></i><b>4.7</b> Technical Supplement - Matrix Expressions</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec471"><i class="fa fa-check"></i><b>4.7.1</b> Expressing Models with Categorical Variables in Matrix Form</a></li>
<li class="chapter" data-level="4.7.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec472"><i class="fa fa-check"></i><b>4.7.2</b> Calculating Least Squares Recursively</a></li>
<li class="chapter" data-level="4.7.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#Sec473"><i class="fa fa-check"></i><b>4.7.3</b> General Linear Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="C5VarSelect.html"><a href="C5VarSelect.html"><i class="fa fa-check"></i><b>5</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="5.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec51"><i class="fa fa-check"></i><b>5.1</b> An Iterative Approach to Data Analysis and Modeling</a></li>
<li class="chapter" data-level="5.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec52"><i class="fa fa-check"></i><b>5.2</b> Automatic Variable Selection Procedures</a></li>
<li class="chapter" data-level="5.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec53"><i class="fa fa-check"></i><b>5.3</b> Residual Analysis</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec531"><i class="fa fa-check"></i><b>5.3.1</b> Residuals</a></li>
<li class="chapter" data-level="5.3.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec532"><i class="fa fa-check"></i><b>5.3.2</b> Using Residuals to Identify Outliers</a></li>
<li class="chapter" data-level="5.3.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec533"><i class="fa fa-check"></i><b>5.3.3</b> Using Residuals to Select Explanatory Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec54"><i class="fa fa-check"></i><b>5.4</b> Influential Points</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec541"><i class="fa fa-check"></i><b>5.4.1</b> Leverage</a></li>
<li class="chapter" data-level="5.4.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec542"><i class="fa fa-check"></i><b>5.4.2</b> Cook’s Distance</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec55"><i class="fa fa-check"></i><b>5.5</b> Collinearity</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec551"><i class="fa fa-check"></i><b>5.5.1</b> What is Collinearity?</a></li>
<li class="chapter" data-level="5.5.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec552"><i class="fa fa-check"></i><b>5.5.2</b> Variance Inflation Factors</a></li>
<li class="chapter" data-level="5.5.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec553"><i class="fa fa-check"></i><b>5.5.3</b> Collinearity and Leverage</a></li>
<li class="chapter" data-level="5.5.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec554"><i class="fa fa-check"></i><b>5.5.4</b> Suppressor Variables</a></li>
<li class="chapter" data-level="5.5.5" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec555"><i class="fa fa-check"></i><b>5.5.5</b> Orthogonal Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec56"><i class="fa fa-check"></i><b>5.6</b> Selection Criteria</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec561"><i class="fa fa-check"></i><b>5.6.1</b> Goodness of Fit</a></li>
<li class="chapter" data-level="5.6.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec562"><i class="fa fa-check"></i><b>5.6.2</b> Model Validation</a></li>
<li class="chapter" data-level="5.6.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec563"><i class="fa fa-check"></i><b>5.6.3</b> Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec57"><i class="fa fa-check"></i><b>5.7</b> Heteroscedasticity</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec571"><i class="fa fa-check"></i><b>5.7.1</b> Detecting Heteroscedasticity</a></li>
<li class="chapter" data-level="5.7.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec572"><i class="fa fa-check"></i><b>5.7.2</b> Heteroscedasticity-Consistent Standard Errors</a></li>
<li class="chapter" data-level="5.7.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec573"><i class="fa fa-check"></i><b>5.7.3</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="5.7.4" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec574"><i class="fa fa-check"></i><b>5.7.4</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec58"><i class="fa fa-check"></i><b>5.8</b> Further Reading and References</a></li>
<li class="chapter" data-level="5.9" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec59"><i class="fa fa-check"></i><b>5.9</b> Exercises</a></li>
<li class="chapter" data-level="5.10" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec510"><i class="fa fa-check"></i><b>5.10</b> Technical Supplements for Chapter 5</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5101"><i class="fa fa-check"></i><b>5.10.1</b> Projection Matrix</a></li>
<li class="chapter" data-level="5.10.2" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5102"><i class="fa fa-check"></i><b>5.10.2</b> Leave One Out Statistics</a></li>
<li class="chapter" data-level="5.10.3" data-path="C5VarSelect.html"><a href="C5VarSelect.html#Sec5103"><i class="fa fa-check"></i><b>5.10.3</b> Omitting Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html"><i class="fa fa-check"></i><b>6</b> Interpreting Regression Results</a>
<ul>
<li class="chapter" data-level="6.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec61"><i class="fa fa-check"></i><b>6.1</b> What the Modeling Process Tells Us</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec611"><i class="fa fa-check"></i><b>6.1.1</b> Interpreting Individual Effects</a></li>
<li class="chapter" data-level="6.1.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec612"><i class="fa fa-check"></i><b>6.1.2</b> Other Interpretations</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec62"><i class="fa fa-check"></i><b>6.2</b> The Importance of Variable Selection</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec621"><i class="fa fa-check"></i><b>6.2.1</b> Overfitting the Model</a></li>
<li class="chapter" data-level="6.2.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec622"><i class="fa fa-check"></i><b>6.2.2</b> Underfitting the Model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec63"><i class="fa fa-check"></i><b>6.3</b> The Importance of Data Collection</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec631"><i class="fa fa-check"></i><b>6.3.1</b> Sampling Frame Error and Adverse Selection</a></li>
<li class="chapter" data-level="6.3.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec632"><i class="fa fa-check"></i><b>6.3.2</b> Limited Sampling Regions</a></li>
<li class="chapter" data-level="6.3.3" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec633"><i class="fa fa-check"></i><b>6.3.3</b> Limited Dependent Variables, Censoring and Truncation</a></li>
<li class="chapter" data-level="6.3.4" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec634"><i class="fa fa-check"></i><b>6.3.4</b> Omitted and Endogenous Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec635"><i class="fa fa-check"></i><b>6.3.5</b> Missing Data</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec64"><i class="fa fa-check"></i><b>6.4</b> Missing Data Models</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec641"><i class="fa fa-check"></i><b>6.4.1</b> Missing at Random</a></li>
<li class="chapter" data-level="6.4.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec642"><i class="fa fa-check"></i><b>6.4.2</b> Non-Ignorable Missing Data</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec65"><i class="fa fa-check"></i><b>6.5</b> Application: Risk Managers’ Cost Effectiveness</a></li>
<li class="chapter" data-level="6.6" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec66"><i class="fa fa-check"></i><b>6.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="6.7" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec67"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
<li class="chapter" data-level="6.8" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec68"><i class="fa fa-check"></i><b>6.8</b> Technical Supplements for Chapter 6</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#Sec681"><i class="fa fa-check"></i><b>6.8.1</b> Effects of Model Misspecification</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="C7Trends.html"><a href="C7Trends.html"><i class="fa fa-check"></i><b>7</b> Modeling Trends</a>
<ul>
<li class="chapter" data-level="7.1" data-path="C7Trends.html"><a href="C7Trends.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#time-series-and-stochastic-processes"><i class="fa fa-check"></i>Time Series and Stochastic Processes</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#time-series-versus-causal-models"><i class="fa fa-check"></i>Time Series versus Causal Models</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="C7Trends.html"><a href="C7Trends.html#S7:Trends"><i class="fa fa-check"></i><b>7.2</b> Fitting Trends in Time</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#understanding-patterns-over-time"><i class="fa fa-check"></i>Understanding Patterns over Time</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#fitting-trends-in-time"><i class="fa fa-check"></i>Fitting Trends in Time</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#fitting-seasonal-trends"><i class="fa fa-check"></i>Fitting Seasonal Trends</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#reliability-of-time-series-forecasts"><i class="fa fa-check"></i>Reliability of Time Series Forecasts</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="C7Trends.html"><a href="C7Trends.html#S7:RandomWalk"><i class="fa fa-check"></i><b>7.3</b> Stationarity and Random Walk Models</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#white-noise"><i class="fa fa-check"></i>White Noise</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#random-walk"><i class="fa fa-check"></i>Random Walk</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="C7Trends.html"><a href="C7Trends.html#inference-using-random-walk-models"><i class="fa fa-check"></i><b>7.4</b> Inference using Random Walk Models</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#model-properties"><i class="fa fa-check"></i>Model Properties</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#forecasting"><i class="fa fa-check"></i>Forecasting</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#identifying-stationarity"><i class="fa fa-check"></i>Identifying Stationarity</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#identifying-random-walks"><i class="fa fa-check"></i>Identifying Random Walks</a></li>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#random-walk-versus-linear-trend-in-time-models"><i class="fa fa-check"></i>Random Walk versus Linear Trend in Time Models</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="C7Trends.html"><a href="C7Trends.html#filtering-to-achieve-stationarity"><i class="fa fa-check"></i><b>7.5</b> Filtering to Achieve Stationarity</a>
<ul>
<li class="chapter" data-level="" data-path="C7Trends.html"><a href="C7Trends.html#transformations"><i class="fa fa-check"></i>Transformations</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="C7Trends.html"><a href="C7Trends.html#forecast-evaluation"><i class="fa fa-check"></i><b>7.6</b> Forecast Evaluation</a></li>
<li class="chapter" data-level="7.7" data-path="C7Trends.html"><a href="C7Trends.html#further-reading-and-references"><i class="fa fa-check"></i><b>7.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="7.8" data-path="C7Trends.html"><a href="C7Trends.html#exercises"><i class="fa fa-check"></i><b>7.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="C8AR.html"><a href="C8AR.html"><i class="fa fa-check"></i><b>8</b> Autocorrelations and Autoregressive Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="C8AR.html"><a href="C8AR.html#S8:Autocorrs"><i class="fa fa-check"></i><b>8.1</b> Autocorrelations</a>
<ul>
<li class="chapter" data-level="" data-path="C8AR.html"><a href="C8AR.html#application-inflation-bond-returns"><i class="fa fa-check"></i>Application: Inflation Bond Returns</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="C8AR.html"><a href="C8AR.html#autoregressive-models-of-order-one"><i class="fa fa-check"></i><b>8.2</b> Autoregressive Models of Order One</a></li>
<li class="chapter" data-level="8.3" data-path="C8AR.html"><a href="C8AR.html#S8:Estimation"><i class="fa fa-check"></i><b>8.3</b> Estimation and Diagnostic Checking</a></li>
<li class="chapter" data-level="8.4" data-path="C8AR.html"><a href="C8AR.html#S8:AR1Smooth"><i class="fa fa-check"></i><b>8.4</b> Smoothing and Prediction</a></li>
<li class="chapter" data-level="8.5" data-path="C8AR.html"><a href="C8AR.html#S8:BoxJenkins"><i class="fa fa-check"></i><b>8.5</b> Box-Jenkins Modeling and Forecasting</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="C8AR.html"><a href="C8AR.html#models"><i class="fa fa-check"></i><b>8.5.1</b> Models</a></li>
<li class="chapter" data-level="8.5.2" data-path="C8AR.html"><a href="C8AR.html#forecasting-1"><i class="fa fa-check"></i><b>8.5.2</b> Forecasting</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="C8AR.html"><a href="C8AR.html#application-hong-kong-exchange-rates"><i class="fa fa-check"></i><b>8.6</b> Application: Hong Kong Exchange Rates</a></li>
<li class="chapter" data-level="8.7" data-path="C8AR.html"><a href="C8AR.html#further-reading-and-references-1"><i class="fa fa-check"></i><b>8.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="8.8" data-path="C8AR.html"><a href="C8AR.html#exercises-1"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="C9Forecast.html"><a href="C9Forecast.html"><i class="fa fa-check"></i><b>9</b> Forecasting and Time Series Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="C9Forecast.html"><a href="C9Forecast.html#smoothing-with-moving-averages"><i class="fa fa-check"></i><b>9.1</b> Smoothing with Moving Averages</a></li>
<li class="chapter" data-level="9.2" data-path="C9Forecast.html"><a href="C9Forecast.html#S9:ExponSmooth"><i class="fa fa-check"></i><b>9.2</b> Exponential Smoothing</a></li>
<li class="chapter" data-level="9.3" data-path="C9Forecast.html"><a href="C9Forecast.html#S9:SeasonalTSModels"><i class="fa fa-check"></i><b>9.3</b> Seasonal Time Series Models</a></li>
<li class="chapter" data-level="9.4" data-path="C9Forecast.html"><a href="C9Forecast.html#unit-root-tests"><i class="fa fa-check"></i><b>9.4</b> Unit Root Tests</a></li>
<li class="chapter" data-level="9.5" data-path="C9Forecast.html"><a href="C9Forecast.html#archgarch-models"><i class="fa fa-check"></i><b>9.5</b> ARCH/GARCH Models</a></li>
<li class="chapter" data-level="9.6" data-path="C9Forecast.html"><a href="C9Forecast.html#further-reading-and-references-2"><i class="fa fa-check"></i><b>9.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="C10Panel.html"><a href="C10Panel.html"><i class="fa fa-check"></i><b>10</b> Longitudinal and Panel Data Models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="C10Panel.html"><a href="C10Panel.html#S10:Intro"><i class="fa fa-check"></i><b>10.1</b> What are Longitudinal and Panel Data?</a></li>
<li class="chapter" data-level="10.2" data-path="C10Panel.html"><a href="C10Panel.html#S10:Visual"><i class="fa fa-check"></i><b>10.2</b> Visualizing Longitudinal and Panel Data</a></li>
<li class="chapter" data-level="10.3" data-path="C10Panel.html"><a href="C10Panel.html#S10:FEModels"><i class="fa fa-check"></i><b>10.3</b> Basic Fixed Effects Models</a></li>
<li class="chapter" data-level="10.4" data-path="C10Panel.html"><a href="C10Panel.html#S10:FEModels2"><i class="fa fa-check"></i><b>10.4</b> Extended Fixed Effects Models</a></li>
<li class="chapter" data-level="10.5" data-path="C10Panel.html"><a href="C10Panel.html#S10:REModels"><i class="fa fa-check"></i><b>10.5</b> Random Effects Models</a></li>
<li class="chapter" data-level="10.6" data-path="C10Panel.html"><a href="C10Panel.html#S10:References"><i class="fa fa-check"></i><b>10.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="C11Binary.html"><a href="C11Binary.html"><i class="fa fa-check"></i><b>11</b> Categorical Dependent Variables</a>
<ul>
<li class="chapter" data-level="11.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec111"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables</a></li>
<li class="chapter" data-level="11.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec112"><i class="fa fa-check"></i><b>11.2</b> Logistic and Probit Regression Models</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1121"><i class="fa fa-check"></i><b>11.2.1</b> Using Nonlinear Functions of Explanatory Variables</a></li>
<li class="chapter" data-level="11.2.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1122"><i class="fa fa-check"></i><b>11.2.2</b> Threshold Interpretation</a></li>
<li class="chapter" data-level="11.2.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1123"><i class="fa fa-check"></i><b>11.2.3</b> Random Utility Interpretation</a></li>
<li class="chapter" data-level="11.2.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1124"><i class="fa fa-check"></i><b>11.2.4</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec113"><i class="fa fa-check"></i><b>11.3</b> Inference for Logistic and Probit Regression Models</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="C11Binary.html"><a href="C11Binary.html#parameter-estimation"><i class="fa fa-check"></i><b>11.3.1</b> Parameter Estimation</a></li>
<li class="chapter" data-level="11.3.2" data-path="C11Binary.html"><a href="C11Binary.html#additional-inference"><i class="fa fa-check"></i><b>11.3.2</b> Additional Inference</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec114"><i class="fa fa-check"></i><b>11.4</b> Application: Medical Expenditures</a></li>
<li class="chapter" data-level="11.5" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec115"><i class="fa fa-check"></i><b>11.5</b> Nominal Dependent Variables</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1151"><i class="fa fa-check"></i><b>11.5.1</b> Generalized Logit</a></li>
<li class="chapter" data-level="11.5.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1152"><i class="fa fa-check"></i><b>11.5.2</b> Multinomial Logit</a></li>
<li class="chapter" data-level="11.5.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1153"><i class="fa fa-check"></i><b>11.5.3</b> Nested Logit</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec116"><i class="fa fa-check"></i><b>11.6</b> Ordinal Dependent Variables</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="C11Binary.html"><a href="C11Binary.html#cumulative-logit"><i class="fa fa-check"></i><b>11.6.1</b> Cumulative Logit</a></li>
<li class="chapter" data-level="11.6.2" data-path="C11Binary.html"><a href="C11Binary.html#cumulative-probit"><i class="fa fa-check"></i><b>11.6.2</b> Cumulative Probit</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec117"><i class="fa fa-check"></i><b>11.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="11.8" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec118"><i class="fa fa-check"></i><b>11.8</b> Exercises</a></li>
<li class="chapter" data-level="11.9" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec119"><i class="fa fa-check"></i><b>11.9</b> Technical Supplements - Likelihood-Based Inference</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1191"><i class="fa fa-check"></i><b>11.9.1</b> Properties of Likelihood Functions</a></li>
<li class="chapter" data-level="11.9.2" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1192"><i class="fa fa-check"></i><b>11.9.2</b> Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="11.9.3" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1193"><i class="fa fa-check"></i><b>11.9.3</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="11.9.4" data-path="C11Binary.html"><a href="C11Binary.html#S:Sec1194"><i class="fa fa-check"></i><b>11.9.4</b> Information Criteria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="C12Count.html"><a href="C12Count.html"><i class="fa fa-check"></i><b>12</b> Count Dependent Variables</a>
<ul>
<li class="chapter" data-level="12.1" data-path="C12Count.html"><a href="C12Count.html#S:Sec121"><i class="fa fa-check"></i><b>12.1</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="C12Count.html"><a href="C12Count.html#S:Sec1211"><i class="fa fa-check"></i><b>12.1.1</b> Poisson Distribution</a></li>
<li class="chapter" data-level="12.1.2" data-path="C12Count.html"><a href="C12Count.html#S:Sec1212"><i class="fa fa-check"></i><b>12.1.2</b> Regression Model</a></li>
<li class="chapter" data-level="12.1.3" data-path="C12Count.html"><a href="C12Count.html#S:Sec1213"><i class="fa fa-check"></i><b>12.1.3</b> Estimation</a></li>
<li class="chapter" data-level="12.1.4" data-path="C12Count.html"><a href="C12Count.html#S:Sec1214"><i class="fa fa-check"></i><b>12.1.4</b> Additional Inference</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="C12Count.html"><a href="C12Count.html#S:Sec122"><i class="fa fa-check"></i><b>12.2</b> Application: Singapore Automobile Insurance</a></li>
<li class="chapter" data-level="12.3" data-path="C12Count.html"><a href="C12Count.html#S:Sec123"><i class="fa fa-check"></i><b>12.3</b> Overdispersion and Negative Binomial Models</a></li>
<li class="chapter" data-level="12.4" data-path="C12Count.html"><a href="C12Count.html#S:Sec124"><i class="fa fa-check"></i><b>12.4</b> Other Count Models</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="C12Count.html"><a href="C12Count.html#zero-inflated-models"><i class="fa fa-check"></i><b>12.4.1</b> Zero-Inflated Models</a></li>
<li class="chapter" data-level="12.4.2" data-path="C12Count.html"><a href="C12Count.html#hurdle-models"><i class="fa fa-check"></i><b>12.4.2</b> Hurdle Models</a></li>
<li class="chapter" data-level="12.4.3" data-path="C12Count.html"><a href="C12Count.html#heterogeneity-models"><i class="fa fa-check"></i><b>12.4.3</b> Heterogeneity Models</a></li>
<li class="chapter" data-level="12.4.4" data-path="C12Count.html"><a href="C12Count.html#latent-class-models"><i class="fa fa-check"></i><b>12.4.4</b> Latent Class Models</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="C12Count.html"><a href="C12Count.html#S:Sec125"><i class="fa fa-check"></i><b>12.5</b> Further Reading and References</a></li>
<li class="chapter" data-level="12.6" data-path="C12Count.html"><a href="C12Count.html#S:Sec126"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="C13GLM.html"><a href="C13GLM.html"><i class="fa fa-check"></i><b>13</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec131"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec132"><i class="fa fa-check"></i><b>13.2</b> GLM Model</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1321"><i class="fa fa-check"></i><b>13.2.1</b> Linear Exponential Family of Distributions</a></li>
<li class="chapter" data-level="13.2.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1322"><i class="fa fa-check"></i><b>13.2.2</b> Link Functions</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec133"><i class="fa fa-check"></i><b>13.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1331"><i class="fa fa-check"></i><b>13.3.1</b> Maximum Likelihood Estimation for Canonical Links</a></li>
<li class="chapter" data-level="13.3.2" data-path="C13GLM.html"><a href="C13GLM.html#overdispersion"><i class="fa fa-check"></i><b>13.3.2</b> Overdispersion</a></li>
<li class="chapter" data-level="13.3.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1333"><i class="fa fa-check"></i><b>13.3.3</b> Goodness of Fit Statistics</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec134"><i class="fa fa-check"></i><b>13.4</b> Application: Medical Expenditures</a></li>
<li class="chapter" data-level="13.5" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec135"><i class="fa fa-check"></i><b>13.5</b> Residuals</a></li>
<li class="chapter" data-level="13.6" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec136"><i class="fa fa-check"></i><b>13.6</b> Tweedie Distribution</a></li>
<li class="chapter" data-level="13.7" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec137"><i class="fa fa-check"></i><b>13.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="13.8" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec138"><i class="fa fa-check"></i><b>13.8</b> Exercises</a></li>
<li class="chapter" data-level="13.9" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec139"><i class="fa fa-check"></i><b>13.9</b> Technical Supplements - Exponential Family</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1391"><i class="fa fa-check"></i><b>13.9.1</b> Linear Exponential Family of Distributions</a></li>
<li class="chapter" data-level="13.9.2" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1392"><i class="fa fa-check"></i><b>13.9.2</b> Moments</a></li>
<li class="chapter" data-level="13.9.3" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1393"><i class="fa fa-check"></i><b>13.9.3</b> Maximum Likelihood Estimation for General Links</a></li>
<li class="chapter" data-level="13.9.4" data-path="C13GLM.html"><a href="C13GLM.html#S:Sec1394"><i class="fa fa-check"></i><b>13.9.4</b> Iterated Reweighted Least Squares</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="C14Survival.html"><a href="C14Survival.html"><i class="fa fa-check"></i><b>14</b> Survival Models</a>
<ul>
<li class="chapter" data-level="14.1" data-path="C14Survival.html"><a href="C14Survival.html#introduction-2"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec142"><i class="fa fa-check"></i><b>14.2</b> Censoring and Truncation</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="C14Survival.html"><a href="C14Survival.html#definitions-and-examples"><i class="fa fa-check"></i><b>14.2.1</b> Definitions and Examples</a></li>
<li class="chapter" data-level="14.2.2" data-path="C14Survival.html"><a href="C14Survival.html#likelihood-inference"><i class="fa fa-check"></i><b>14.2.2</b> Likelihood Inference</a></li>
<li class="chapter" data-level="14.2.3" data-path="C14Survival.html"><a href="C14Survival.html#product-limit-estimator"><i class="fa fa-check"></i><b>14.2.3</b> Product-Limit Estimator</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec143"><i class="fa fa-check"></i><b>14.3</b> Accelerated Failure Time Model</a></li>
<li class="chapter" data-level="14.4" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec144"><i class="fa fa-check"></i><b>14.4</b> Proportional Hazards Model</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec1441"><i class="fa fa-check"></i><b>14.4.1</b> Proportional Hazards</a></li>
<li class="chapter" data-level="14.4.2" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec1442"><i class="fa fa-check"></i><b>14.4.2</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec145"><i class="fa fa-check"></i><b>14.5</b> Recurrent Events</a></li>
<li class="chapter" data-level="14.6" data-path="C14Survival.html"><a href="C14Survival.html#S:Sec146"><i class="fa fa-check"></i><b>14.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="C15Misc.html"><a href="C15Misc.html"><i class="fa fa-check"></i><b>15</b> Miscellaneous Regression Topics</a>
<ul>
<li class="chapter" data-level="15.1" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec151"><i class="fa fa-check"></i><b>15.1</b> Mixed Linear Models</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="C15Misc.html"><a href="C15Misc.html#weighted-least-squares-2"><i class="fa fa-check"></i><b>15.1.1</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="15.1.2" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec1512"><i class="fa fa-check"></i><b>15.1.2</b> Variance Components Estimation</a></li>
<li class="chapter" data-level="15.1.3" data-path="C15Misc.html"><a href="C15Misc.html#best-linear-unbiased-prediction"><i class="fa fa-check"></i><b>15.1.3</b> Best Linear Unbiased Prediction</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="C15Misc.html"><a href="C15Misc.html#bayesian-regression"><i class="fa fa-check"></i><b>15.2</b> Bayesian Regression</a></li>
<li class="chapter" data-level="15.3" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec153"><i class="fa fa-check"></i><b>15.3</b> Density Estimation and Scatterplot Smoothing}</a></li>
<li class="chapter" data-level="15.4" data-path="C15Misc.html"><a href="C15Misc.html#S:Sec154"><i class="fa fa-check"></i><b>15.4</b> Generalized Additive Models</a></li>
<li class="chapter" data-level="15.5" data-path="C15Misc.html"><a href="C15Misc.html#bootstrapping"><i class="fa fa-check"></i><b>15.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="15.6" data-path="C15Misc.html"><a href="C15Misc.html#further-reading-and-references-3"><i class="fa fa-check"></i><b>15.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="C16FreqSev.html"><a href="C16FreqSev.html"><i class="fa fa-check"></i><b>16</b> Frequency-Severity Models</a>
<ul>
<li class="chapter" data-level="16.1" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec161"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec162"><i class="fa fa-check"></i><b>16.2</b> Tobit Model</a></li>
<li class="chapter" data-level="16.3" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec163"><i class="fa fa-check"></i><b>16.3</b> Application: Medical Expenditures</a></li>
<li class="chapter" data-level="16.4" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec164"><i class="fa fa-check"></i><b>16.4</b> Two-Part Model</a></li>
<li class="chapter" data-level="16.5" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec165"><i class="fa fa-check"></i><b>16.5</b> Aggregate Loss Model</a></li>
<li class="chapter" data-level="16.6" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec166"><i class="fa fa-check"></i><b>16.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="16.7" data-path="C16FreqSev.html"><a href="C16FreqSev.html#S:Sec167"><i class="fa fa-check"></i><b>16.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="C17Fat.html"><a href="C17Fat.html"><i class="fa fa-check"></i><b>17</b> Fat-Tailed Regression Models</a>
<ul>
<li class="chapter" data-level="17.1" data-path="C17Fat.html"><a href="C17Fat.html#introduction-3"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec172"><i class="fa fa-check"></i><b>17.2</b> Transformations</a></li>
<li class="chapter" data-level="17.3" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec173"><i class="fa fa-check"></i><b>17.3</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec1731"><i class="fa fa-check"></i><b>17.3.1</b> What is “Fat-Tailed?”</a></li>
<li class="chapter" data-level="17.3.2" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec1732"><i class="fa fa-check"></i><b>17.3.2</b> Application: Wisconsin Nursing Homes</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec174"><i class="fa fa-check"></i><b>17.4</b> Generalized Distributions</a>
<ul>
<li class="chapter" data-level="" data-path="C17Fat.html"><a href="C17Fat.html#applicationwisconsin-nursing-homes"><i class="fa fa-check"></i>Application:Wisconsin Nursing Homes</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec175"><i class="fa fa-check"></i><b>17.5</b> Quantile Regression</a></li>
<li class="chapter" data-level="17.6" data-path="C17Fat.html"><a href="C17Fat.html#S:Sec176"><i class="fa fa-check"></i><b>17.6</b> Extreme Value Models</a></li>
<li class="chapter" data-level="17.7" data-path="C17Fat.html"><a href="C17Fat.html#further-reading-and-references-4"><i class="fa fa-check"></i><b>17.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="17.8" data-path="C17Fat.html"><a href="C17Fat.html#exercises-2"><i class="fa fa-check"></i><b>17.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="C18Cred.html"><a href="C18Cred.html"><i class="fa fa-check"></i><b>18</b> Credibility and Bonus-Malus</a>
<ul>
<li class="chapter" data-level="18.1" data-path="C18Cred.html"><a href="C18Cred.html#risk-classification-and-experience-rating"><i class="fa fa-check"></i><b>18.1</b> Risk Classification and Experience Rating</a></li>
<li class="chapter" data-level="18.2" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec182"><i class="fa fa-check"></i><b>18.2</b> Credibility</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec1821"><i class="fa fa-check"></i><b>18.2.1</b> Limited Fluctuation Credibility</a></li>
<li class="chapter" data-level="18.2.2" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec1822"><i class="fa fa-check"></i><b>18.2.2</b> Greatest Accuracy Credibility</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec183"><i class="fa fa-check"></i><b>18.3</b> Credibility and Regression</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="C18Cred.html"><a href="C18Cred.html#one-way-random-effects-model"><i class="fa fa-check"></i><b>18.3.1</b> One-Way Random Effects Model</a></li>
<li class="chapter" data-level="18.3.2" data-path="C18Cred.html"><a href="C18Cred.html#longitudinal-models"><i class="fa fa-check"></i><b>18.3.2</b> Longitudinal Models</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="C18Cred.html"><a href="C18Cred.html#S:Sec184"><i class="fa fa-check"></i><b>18.4</b> Bonus-Malus</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="C19Triangles.html"><a href="C19Triangles.html"><i class="fa fa-check"></i><b>19</b> Claims Triangles</a>
<ul>
<li class="chapter" data-level="19.1" data-path="C19Triangles.html"><a href="C19Triangles.html#introduction-4"><i class="fa fa-check"></i><b>19.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1911"><i class="fa fa-check"></i><b>19.1.1</b> Claims Evolution</a></li>
<li class="chapter" data-level="19.1.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1912"><i class="fa fa-check"></i><b>19.1.2</b> Claims Triangles</a></li>
<li class="chapter" data-level="19.1.3" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1913"><i class="fa fa-check"></i><b>19.1.3</b> Chain Ladder Method</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec192"><i class="fa fa-check"></i><b>19.2</b> Regression Using Functions of Time as Explanatory Variables</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1921"><i class="fa fa-check"></i><b>19.2.1</b> Lognormal Model</a></li>
<li class="chapter" data-level="19.2.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1922"><i class="fa fa-check"></i><b>19.2.2</b> Hoerl Curve</a></li>
<li class="chapter" data-level="19.2.3" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1923"><i class="fa fa-check"></i><b>19.2.3</b> Poisson Models</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec193"><i class="fa fa-check"></i><b>19.3</b> Using Past Developments</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1931"><i class="fa fa-check"></i><b>19.3.1</b> Mack Model</a></li>
<li class="chapter" data-level="19.3.2" data-path="C19Triangles.html"><a href="C19Triangles.html#S:Sec1932"><i class="fa fa-check"></i><b>19.3.2</b> Distributional Models</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="C19Triangles.html"><a href="C19Triangles.html#further-reading-and-references-5"><i class="fa fa-check"></i><b>19.4</b> Further Reading and References</a></li>
<li class="chapter" data-level="19.5" data-path="C19Triangles.html"><a href="C19Triangles.html#exercises-3"><i class="fa fa-check"></i><b>19.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="C20Report.html"><a href="C20Report.html"><i class="fa fa-check"></i><b>20</b> Report Writing: Communicating Data Analysis Results</a>
<ul>
<li class="chapter" data-level="20.1" data-path="C20Report.html"><a href="C20Report.html#S20:Overview"><i class="fa fa-check"></i><b>20.1</b> Overview</a></li>
<li class="chapter" data-level="20.2" data-path="C20Report.html"><a href="C20Report.html#S20:Methods"><i class="fa fa-check"></i><b>20.2</b> Methods for Communicating Data</a>
<ul>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#within-text-data"><i class="fa fa-check"></i>Within Text Data</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#graphs"><i class="fa fa-check"></i>Graphs</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="C20Report.html"><a href="C20Report.html#S20:Organize"><i class="fa fa-check"></i><b>20.3</b> How to Organize</a>
<ul>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#title-and-abstract"><i class="fa fa-check"></i>Title and Abstract</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#introduction-5"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#model-selection-and-interpretation"><i class="fa fa-check"></i>Model Selection and Interpretation</a></li>
<li class="chapter" data-level="" data-path="C20Report.html"><a href="C20Report.html#references-and-appendix"><i class="fa fa-check"></i>References and Appendix</a></li>
</ul></li>
<li class="chapter" data-level="20.4" data-path="C20Report.html"><a href="C20Report.html#further-suggestions-for-report-writing"><i class="fa fa-check"></i><b>20.4</b> Further Suggestions for Report Writing</a></li>
<li class="chapter" data-level="20.5" data-path="C20Report.html"><a href="C20Report.html#case-study-swedish-automobile-claims"><i class="fa fa-check"></i><b>20.5</b> Case Study: Swedish Automobile Claims</a></li>
<li class="chapter" data-level="20.6" data-path="C20Report.html"><a href="C20Report.html#further-reading-and-references-6"><i class="fa fa-check"></i><b>20.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="20.7" data-path="C20Report.html"><a href="C20Report.html#exercises-4"><i class="fa fa-check"></i><b>20.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="C21Design.html"><a href="C21Design.html"><i class="fa fa-check"></i><b>21</b> Designing Effective Graphs</a>
<ul>
<li class="chapter" data-level="21.1" data-path="C21Design.html"><a href="C21Design.html#S21:Intro"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="C21Design.html"><a href="C21Design.html#S21:GDesign"><i class="fa fa-check"></i><b>21.2</b> Graphic Design Choices Make a Difference</a></li>
<li class="chapter" data-level="21.3" data-path="C21Design.html"><a href="C21Design.html#S21:DesignGuide"><i class="fa fa-check"></i><b>21.3</b> Design Guidelines</a>
<ul>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-one-avoid-chartjunk"><i class="fa fa-check"></i>Guideline One: Avoid Chartjunk</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-two-use-small-multiples-to-promote-comparisons-and-assess-change"><i class="fa fa-check"></i>Guideline Two: Use Small Multiples to Promote Comparisons and Assess Change</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-three-use-complex-graphs-to-portray-complex-patterns"><i class="fa fa-check"></i>Guideline Three: Use Complex Graphs to Portray Complex Patterns</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-four-relate-graph-size-to-information-content"><i class="fa fa-check"></i>Guideline Four: Relate Graph Size to Information Content</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-five-use-graphical-forms-that-promote-comparisons"><i class="fa fa-check"></i>Guideline Five: Use Graphical Forms That Promote Comparisons</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-six-integrate-graphs-and-text"><i class="fa fa-check"></i>Guideline Six: Integrate Graphs and Text</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-seven-demonstrate-an-important-message"><i class="fa fa-check"></i>Guideline Seven: Demonstrate an Important Message</a></li>
<li class="chapter" data-level="" data-path="C21Design.html"><a href="C21Design.html#guideline-eight-know-your-audience"><i class="fa fa-check"></i>Guideline Eight: Know Your Audience</a></li>
</ul></li>
<li class="chapter" data-level="21.4" data-path="C21Design.html"><a href="C21Design.html#S21:EmpiricalFoundations"><i class="fa fa-check"></i><b>21.4</b> Empirical Foundations For Guidelines</a>
<ul>
<li class="chapter" data-level="21.4.1" data-path="C21Design.html"><a href="C21Design.html#graphs-as-units-of-study"><i class="fa fa-check"></i><b>21.4.1</b> Graphs as Units of Study</a></li>
</ul></li>
<li class="chapter" data-level="21.5" data-path="C21Design.html"><a href="C21Design.html#S21:Conclude"><i class="fa fa-check"></i><b>21.5</b> Concluding Remarks</a></li>
<li class="chapter" data-level="21.6" data-path="C21Design.html"><a href="C21Design.html#S21:References"><i class="fa fa-check"></i><b>21.6</b> Further Reading and References</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i><b>22</b> Appendices</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a1.-basic-statistical-inference"><i class="fa fa-check"></i>Appendix A1. Basic Statistical Inference</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#distributions-of-functions-of-random-variables"><i class="fa fa-check"></i>Distributions of Functions of Random Variables</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#estimation-and-prediction"><i class="fa fa-check"></i>Estimation and Prediction</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#testing-hypotheses"><i class="fa fa-check"></i>Testing Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a2.-matrix-algebra"><i class="fa fa-check"></i>Appendix A2. Matrix Algebra</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#basic-definitions"><i class="fa fa-check"></i>Basic Definitions</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#review-of-basic-operations"><i class="fa fa-check"></i>Review of Basic Operations</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#further-definitions"><i class="fa fa-check"></i>Further Definitions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#appendix-a3.-probability-tables"><i class="fa fa-check"></i>Appendix A3. Probability Tables</a>
<ul>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#normal-distribution"><i class="fa fa-check"></i>Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#chi-square-distribution"><i class="fa fa-check"></i>Chi-Square Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#t-distribution"><i class="fa fa-check"></i><em>t</em>-Distribution</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html#f-distribution"><i class="fa fa-check"></i><em>F</em>-Distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="brief-answers-to-selected-exercises.html"><a href="brief-answers-to-selected-exercises.html"><i class="fa fa-check"></i>Brief Answers to Selected Exercises</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTextDev/RegressionSpanish/" target="blank">Spanish Regression on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Regression Modeling with Actuarial and Financial Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="interpreting-regression-results" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Interpreting Regression Results<a href="interpreting-regression-results.html#interpreting-regression-results" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Chapter Preview</em>. A regression analyst collects data, selects a model and then reports on the findings of the study, in that order. This chapter considers these three topics in <em>reverse</em> order, emphasizing how each stage of the study is influenced by preceding steps. An application, determining a firm’s characteristics that influence its effectiveness in managing risk, illustrates the regression modeling process from start to finish.</p>
<hr />
<p>Studying a problem using a regression modeling process involves a substantial commitment of time and energy. One must first embrace the concept of <em>statistical thinking</em>, a willingness to use data actively as part of a decision making process. Second, one must appreciate the usefulness of a model that is used to approximate a real situation. Having made this substantial commitment, there is a natural tendency to “oversell” the results of statistical methods such as regression analysis. By overselling any set of ideas, consumers eventually become disappointed when the results do not live up to their expectations. This chapter begins in Section <a href="interpreting-regression-results.html#Sec61">6.1</a> by summarizing what we can reasonably expect to learn from regression modeling.</p>
<p>Models are designed to be much simpler than relationships among entities that exist in the real world. A model is merely an approximation of reality. As stated by George Box (1979), “All models are wrong, but some are useful.” Developing the model, the subject of Chapter 5, is part of the art of statistics. Although the principles of variable selection are widely accepted, the application of these principles can vary considerably among analysts. The resulting product has certain aesthetic values and is by no means predetermined. Statistics can be thought of as the art of reasoning with data. Section <a href="interpreting-regression-results.html#Sec62">6.2</a> will underscore the importance of variable selection.</p>
<p>Model formulation and data collection form the first stage of the modeling process. Students of statistics are usually surprised at the difficulty of relating ideas about relationships to available data. These difficulties include a lack of readily available data and the need to use certain data as proxies for ideal information that is not available numerically. Section <a href="interpreting-regression-results.html#Sec63">6.3</a> will describe several types of difficulties that can arise when collecting data. Section <a href="interpreting-regression-results.html#Sec64">6.4</a> will describe some models to alleviate these difficulties.</p>
<div id="Sec61" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> What the Modeling Process Tells Us<a href="interpreting-regression-results.html#Sec61" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Model inference is the final stage of the modeling process. By studying the behavior of models, we hope to learn something about the real world. Models serve to impose an order on reality and provide a basis for understanding reality through the nature of the imposed order. Further, statistical models are based on reasoning with the available data from a sample. Thus, models serve as an important guide for predicting the behavior of observations outside the available sample.</p>
<div id="Sec611" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Interpreting Individual Effects<a href="interpreting-regression-results.html#Sec611" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When interpreting results from multiple regression, the main goal is often to convey the importance of individual variables, or effects, on an outcome of interest. The interpretation depends on whether or not the effects are substantively significant, statistically significant and causal.</p>
<p><strong>Substantive Significance</strong>. Readers of a regression study first want to understand the direction and magnitude of individual effects. Do females have more or less claims than males in a study of insurance claims? If less, by how much? You can give answers to these questions through a table of regression coefficients. Moreover, to give a sense of the reliability of the estimates, you may also wish to include the standard error or a confidence interval, as introduced in Section 3.4.2.</p>
<p>Recall that regression coefficients are estimates of partial derivatives of the regression function</p>
<p><span class="math display">\[
\mathrm{E~}y = \beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k.
\]</span></p>
<p>When interpreting coefficients for continuous explanatory variables, it is helpful to do so in terms of meaningful changes of each <span class="math inline">\(x\)</span>. For example, if population is an explanatory variable, we may talk about the expected change in <span class="math inline">\(y\)</span> per 1,000 or one million change in population. Moreover, when interpreting regression coefficients, comment on their “substantive” significance. For example, suppose that we find a difference in claims between males and females but the estimated difference is only 1% of expected claims. This difference may well be statistically significant but not economically meaningful. Substantive significance refers to importance in the field of inquiry; in actuarial science, this is typically financial or economic significance but could also be non-monetary, such as effects on future life expectancy.</p>
<p><strong>Statistical Significance</strong>. Are the effects due to chance? The hypothesis testing machinery introduced in Section 3.4.1 provides a formal mechanism for answering this question. Tests of hypotheses are useful in that they provide a formal, agreed-upon standard, for deciding whether or not a variable provides an important contribution to an expected response. When interpreting results, typically researchers cite a <span class="math inline">\(t\)</span>-ratio or a <span class="math inline">\(p\)</span>-value to demonstrate statistical significance.</p>
<p>In some situations, it is of interest to comment on variables that are <em>not</em> statistically significant. Effects that are not statistically significant have standard errors that are large relative to the regression coefficients. In Section 5.5.2, we expressed this standard error as</p>
<p><span class="math display" id="eq:eq61">\[\begin{equation}
se(b_{j}) = s \frac{\sqrt{VIF_{j}}}{s_{x_{j}} \sqrt{n-1}}.
\tag{6.1}
\end{equation}\]</span></p>
<p>One possible explanation for a lack of statistical significance is a large variation in the disturbance term. By expressing the standard error in this form, we see that the larger the natural variation, as measured by <span class="math inline">\(s\)</span>, the more difficult it is to reject the null hypothesis of no effect (<span class="math inline">\(H_0\)</span>), other things being equal.</p>
<p>A second possible explanation for the lack of statistical significance is the high collinearity, as measured by <span class="math inline">\(VIF_j\)</span>. A variable may be confounded with other variables such that, from the data being analyzed, it is impossible to distinguish the effects of one variable from another.</p>
<p>A third possible explanation is the sample size. Suppose that a mechanism similar to draws from a stable population is used to observe the explanatory variables. Then, the standard deviation of <span class="math inline">\(x_j\)</span>, <span class="math inline">\(s_{x_j}\)</span>, should be stable as the number of draws increases. Similarly, so should <span class="math inline">\(R_j^2\)</span> and <span class="math inline">\(s^2\)</span>. Then, the standard error <span class="math inline">\(se(b_j)\)</span> should decrease as the sample size, <span class="math inline">\(n\)</span>, increases. Conversely, a smaller sample size means a larger standard error, other things being equal. This means that we may not be able to detect the importance of variables in small or moderate size samples.</p>
<p>Thus, in an ideal world, if you do not detect statistical significance where it was hypothesized (and fully expected), you could: (i) get a more precise measure of <span class="math inline">\(y\)</span>, thus reducing its natural variability, (ii) re-design the sample collection scheme so that the relevant explanatory variables are less redundant and (iii) collect more data. Typically, these options are not available with observational data but it can nonetheless be helpful to point out the next steps in a research program.</p>
<p>Analysts occasionally observe statistically significant relationships that were not anticipated—these could be due to a large sample size. Above we noted that a small sample may not provide enough information to detect meaningful relationships. The flip side of this argument is that, for large samples, we have an opportunity for detecting the importance of variables that might go unnoticed in small or even moderate size samples. Unfortunately, it also means that variables with small parameter coefficients, that contribute little to understanding the variation in the response, can be judged to be significant using our decision-making procedures. This serves to highlight the difference between substantive and statistical significance—particularly for large samples, investigators encounter variables that are <em>statistically significant but practically unimportant</em>. In these cases, it can be prudent for the investigator to omit variables from the model specification when their presence is not in accord with accepted theory, even if they are judged statistically significant.</p>
<p><strong>Causal Effects</strong>. If we change <span class="math inline">\(x\)</span>, would <span class="math inline">\(y\)</span> change? As students of basic sciences, we learned principles involving actions and reactions. Adding mass to a ball in motion increases the force of its impact into a wall. However, in the social sciences, relationships are probabilistic, not deterministic, and hence more subtle. For example, as age (<span class="math inline">\(x\)</span>) increases, the one-year probability of death (<span class="math inline">\(y\)</span>) increases for most human mortality curves. Understanding causality, even probabilistic, is the root of all science and provides the basis for informed decision-making.</p>
<p>It is important to acknowledge that causal processes generally cannot be demonstrated exclusively from the data; the data can only present relevant empirical evidence serving as a link in a chain of reasoning about causal mechanisms. For causality, there are three necessary conditions: (i) statistical association between variables, (ii) appropriate time order and (iii) the elimination of alternative hypotheses or establishment of a formal causal mechanism.</p>
<p>As an example, recall the Section 1.1 Galton study, relating adult children’s height (<span class="math inline">\(y\)</span>) to an index of parents’ height (<span class="math inline">\(x\)</span>). For this study, it was clear that there is a strong statistical association between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. The demographics also make it clear that the parents’ measurements (<span class="math inline">\(x\)</span>) precede the children’s measurements (<span class="math inline">\(y\)</span>). What is uncertain is the causal mechanism. For example, in Section 1.5, we cited the possibility that an omitted variable, family diet, could be influencing both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Evidence and theories from human biology and genetics are needed to establish a formal causal mechanism.</p>
<hr />
<p><strong>Example: Race, Redlining and Automobile Insurance Prices.</strong> In an article with this title, Harrington and Niehaus (1998) investigated whether insurance companies engaged in (racial) discriminatory behavior, often known as <em>redlining</em>. Racial discrimination is illegal and insurance companies may not use race in determining prices. The term redlining refers to the practice of drawing red lines on a map to indicate areas that insurers will not serve, areas typically containing a high proportion of minorities.</p>
<p>To investigate whether or not there exists racial discrimination in insurance pricing, Harrington and Niehaus gathered private passenger premiums and claims data from the Missouri Department of Insurance for the period 1988-1992. Although insurance companies do not keep race/ethnicity information in their premiums and claims data, such information is available at the zip code level from the US Census Bureau. By aggregating premiums and claims up to the zip code level, Harrington and Niehaus were able to assess whether areas with a higher percentage of blacks paid more for insurance (PCTBLACK).</p>
<p>A widely used pricing measure is the loss ratio, defined to be the ratio of claims to premiums. This measures insurers’ profitability; if racial discrimination exists in pricing, one would expect to see a low loss ratio in areas with a high proportion of minorities. Harrington and Niehaus used this as the dependent variable, after taking logarithms to address the skewness in the loss ratio distribution.</p>
<p>Harrington and Niehaus (1998) studied 270 zip codes surrounding six major cities in Missouri where there were large concentrations of minorities. Table <a href="interpreting-regression-results.html#tab:Tab61">6.1</a> reports findings from comprehensive coverage although the authors also investigated collision and liability coverage. In addition to the primary variable of interest, PCTBLACK, a few control variables relating to age distribution (PCT1824 and PCT55UP), marital status (MARRIED), population (ln TOTPOP) and employment (PCTUNEMP) were introduced. Policy size was measured indirectly through an average car value (ln AVCARV).</p>
<p>Table <a href="interpreting-regression-results.html#tab:Tab61">6.1</a> reports that only policy size and population are statistically significant determinants of loss ratios. In fact, the coefficient associated with PCTBLACK has a positive sign, indicating that premiums are lower in areas with high concentrations of minorities (although, not significant). In an efficient insurance market, we would expect prices to be closely aligned with claims and that few broad patterns exist.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;border-bottom: 0;border-bottom: 0;">
<caption style="font-size: initial !important;">
<span id="tab:Tab61">Table 6.1: </span><strong>Loss Ratio Regression Results</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:left;">
Description
</th>
<th style="text-align:center;">
Regression Coefficient
</th>
<th style="text-align:center;">
<span class="math inline">\(t\)</span>-Statistic
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
Intercept
</td>
<td style="text-align:left;width: 3cm; width: 6cm; border-right:1px solid;">
</td>
<td style="text-align:center;width: 3cm; ">
1.98
</td>
<td style="text-align:center;width: 3cm; ">
2.73
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
PCTBLACK
</td>
<td style="text-align:left;width: 3cm; width: 6cm; border-right:1px solid;">
Proportion of population black
</td>
<td style="text-align:center;width: 3cm; ">
0.11
</td>
<td style="text-align:center;width: 3cm; ">
0.63
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
ln TOTPOP
</td>
<td style="text-align:left;width: 3cm; width: 6cm; border-right:1px solid;">
Logarithmic total population
</td>
<td style="text-align:center;width: 3cm; ">
-0.1
</td>
<td style="text-align:center;width: 3cm; ">
-4.43
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
PCT1824
</td>
<td style="text-align:left;width: 3cm; width: 6cm; border-right:1px solid;">
Percent of population between 18 and 24
</td>
<td style="text-align:center;width: 3cm; ">
-0.23
</td>
<td style="text-align:center;width: 3cm; ">
-0.5
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
PCT55UP
</td>
<td style="text-align:left;width: 3cm; width: 6cm; border-right:1px solid;">
Percent of population 55 or older
</td>
<td style="text-align:center;width: 3cm; ">
-0.47
</td>
<td style="text-align:center;width: 3cm; ">
-1.76
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
MARRIED
</td>
<td style="text-align:left;width: 3cm; width: 6cm; border-right:1px solid;">
Percent of population married
</td>
<td style="text-align:center;width: 3cm; ">
-0.32
</td>
<td style="text-align:center;width: 3cm; ">
-0.9
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
PCTUNEMP
</td>
<td style="text-align:left;width: 3cm; width: 6cm; border-right:1px solid;">
Percent of population unemployed
</td>
<td style="text-align:center;width: 3cm; ">
0.11
</td>
<td style="text-align:center;width: 3cm; ">
0.1
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
ln AVCARV
</td>
<td style="text-align:left;width: 3cm; width: 6cm; border-right:1px solid;">
Logarithmic average car value insured
</td>
<td style="text-align:center;width: 3cm; ">
-0.87
</td>
<td style="text-align:center;width: 3cm; ">
-3.26
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;">Source:</span> <sup></sup> Harrington and Niehaus (1998)
</td>
</tr>
</tfoot>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<span style="font-style: italic;"><span class="math inline">\(R_a^2\)</span> </span> <sup></sup> 0.11
</td>
</tr>
</tfoot>
</table>
<p>Certainly, the findings of Harrington and Niehaus (1998) are inconsistent with the hypothesis of racial discrimination in pricing. Establishing a lack of statistical significance is typically more difficult than establishing significance. In the paper by Harrington and Niehaus (1998), there are many alternative model specifications that assess the robustness of their findings to different variable selection procedures and different data subsets. Table <a href="interpreting-regression-results.html#tab:Tab61">6.1</a> reports coefficient estimators and <span class="math inline">\(t\)</span>-ratios calculated using weighted least squares, with population size as weights. The authors also ran (ordinary) least squares, with robust standard errors, achieving similar results.</p>
</div>
<div id="Sec612" class="section level3 hasAnchor" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Other Interpretations<a href="interpreting-regression-results.html#Sec612" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When taken collectively, linear combinations of the regression coefficients can be interpreted as the regression function:</p>
<p><span class="math display">\[
\mathrm{E~}y = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k.
\]</span></p>
<p>When reporting regression results, readers want to know how well the model fits the data. Section 5.6.1 summarized several goodness of fit statistics that are routinely reported in regression investigations.</p>
<p><strong>Regression Function and Pricing.</strong> When evaluating insurance claims data, the regression function represents expected claims and hence forms the basis of the pricing function. (See the example in Chapter 4.) In this case, the shape of the regression function and levels for key combinations of explanatory variables are of interest.</p>
<p><strong>Benchmarking Studies.</strong> In some investigations, the main purpose may be to determine whether a specific observation is “in line” with the others available. For example, in Chapter 20 we will examine CEO salaries. The main purpose of such an analysis could have been to see whether a person’s salary is high or low compared to others in the sample, <em>controlling for</em> characteristics such as industry and years of experience. The residual summarizes the deviation of the response from that expected under the model. If the residual is unusually large or small, then we interpret this to mean that there are unusual circumstances associated with this observation. This analysis does not suggest the nature nor the causes of these circumstances. It merely states that the observation is unusual with respect to others in the sample. For some investigations, such as for litigation concerning compensation packages, this is a powerful statement.</p>
<p><strong>Prediction.</strong> Many actuarial applications concern prediction, where the interest is on describing the distribution of a random variable that is not yet realized. When setting reserves, insurance company actuaries are establishing liabilities for future claims that they predict will be realized, and thus becoming eventual expenses of the company. Prediction, or <em>forecasting</em>, is the main motivation of most analyses of time series data, the subject of Chapters 7-10.</p>
<p>Prediction of a single random variable in the multiple linear regression context was introduced in Section 4.2.3. Here, we assumed that we have available a given set of characteristics, <span class="math inline">\(\mathbf{x}_{\ast}=(1,x_{\ast 1},\ldots,x_{\ast k})^{\prime }\)</span>. According to our model, the new response is</p>
<p><span class="math display">\[
y_{\ast}=\beta_0 + \beta_1 x_{\ast 1} + \cdots + \beta_k x_{\ast k} + \varepsilon_{\ast}.
\]</span></p>
<p>We use as our point predictor</p>
<p><span class="math display">\[
\hat{y}_{\ast}=b_{0} + b_{1} x_{\ast 1} + \cdots + b_{k} x_{\ast k}.
\]</span></p>
<p>As in Section 2.5.3, we can decompose the prediction error into the estimation error plus the random error, as follows:</p>
<p><span class="math display">\[
\begin{array}{ccccc}
\underbrace{y^{\ast}-\widehat{y}^{\ast}} &amp; = &amp;
\underbrace{\beta_0 - b_{0} + (\beta_1 - b_{1})x_{\ast 1} + \cdots + (\beta_k - b_{k})x_{\ast k}} &amp; + &amp; \underbrace{\varepsilon ^{\ast}} \\
{\small \text{prediction error}} &amp; {\small =} &amp;
{\small \text{error in estimating the} } &amp;
{\small +} &amp; {\small \text{additional} }\\
&amp;  &amp; {\small \text{regression function at } x_{\ast 1}, \ldots, x_{\ast k}} &amp; &amp; {\small \text{deviation} }
\end{array}
\]</span></p>
<p>This decomposition allows us to provide a distribution for the prediction error. It is customary to assume approximate normality. With this additional assumption, we summarize this distribution using a prediction interval</p>
<p><span class="math display" id="eq:eq62">\[\begin{equation}
\hat{y}_{\ast} \pm t_{n-(k+1),1-\alpha /2} ~ se(pred),
\tag{6.2}
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display">\[
se(pred) = s \sqrt{1 + \mathbf{x}_{\ast}^{\prime }(\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{x}_{\ast}}.
\]</span></p>
<p>Here, the <span class="math inline">\(t\)</span>-value <span class="math inline">\(t_{n-(k+1),1-\alpha /2}\)</span> is a percentile from the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df=n-(k+1)\)</span> degrees of freedom. This extends equation (2.7).</p>
<p>Communicating the range of likely outcomes is an important goal. When analyzing data, there may be several alternative prediction techniques available. Even within the class of regression models, each of several candidate models will produce a different prediction. It is important to provide a distribution, or range, of potential errors. Naive consumers can easily become disappointed with the results of predictions from regression models. These consumers are told (correctly) that the regression model is optimal, based on certain well-defined criteria, and are then provided with a point prediction, such as <span class="math inline">\(\hat{y}_{\ast}\)</span>. Without knowledge of an interval, the consumer has expectations for the performance of the prediction, usually higher than is warranted by information available in the sample. A prediction interval not only provides a single optimal point prediction but also a range of reliability.</p>
<p>When making the predictions, there is an important assumption that the new observation follows the same model as that used in the sample. Thus, the basic conditions about the distribution of the errors should remain unchanged for new observations. It is also important that the level of the predictor variables, <span class="math inline">\(x_{\ast 1},\ldots,x_{\ast k}\)</span>, be similar to those in the available sample. If one or several of the predictor variables differ dramatically from those in the available sample, then the resulting prediction can perform poorly. For example, it would be imprudent to use the model developed in Sections 2.1 through 2.3 to predict a region’s lottery with a population of <span class="math inline">\(x_{\ast}=400,000\)</span>, over ten times the largest population in our sample. Even though it would be easy to plug <span class="math inline">\(x_{\ast}=400,000\)</span> into our formulas, the result would have little intuitive appeal. Extrapolating relationships beyond the observed data requires expertise with the nature of the data as well as the statistical methodology. In Section <a href="interpreting-regression-results.html#Sec63">6.3</a>, we will identify this problem as a potential bias due to the sampling region.</p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_6qmvxiwh&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_o3txrzcs%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="6.1 ModelTellsUs">
</iframe>
</center>
</div>
</div>
<div id="Sec62" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> The Importance of Variable Selection<a href="interpreting-regression-results.html#Sec62" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>On one hand, choosing a theoretical model to represent precisely real-world events is probably an impossible task. On the other hand, choosing a model to represent approximately the real world is an important practical matter. The closer our model is to the real world, the more accurate are the statements that we make, suggested by the model. Although we cannot get the right model, we may be able to select a useful, or at least adequate, model.</p>
<p>Users of statistics, from the raw beginner to the seasoned expert, will always select an inadequate model from time to time. The key question is: <em>How important is it to select an adequate model</em>? Although not every kind of mistake can be accounted for in advance, there are some guiding principles that are useful to keep in mind when selecting a model.</p>
<div id="Sec621" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Overfitting the Model<a href="interpreting-regression-results.html#Sec621" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This type of mistake occurs when superfluous, or extraneous, variables are added to the specified model. If only a small number of extraneous variables, such as one or two, are added, then this type of error will probably not dramatically skew most of the types of conclusions that might be reached from the fitted model. For example, we know that when we add a variable to the model, the error sum of squares does not increase. If the variable is extraneous, then the error sum of squares will not get appreciably smaller either. In fact, adding an extraneous variable can increase <span class="math inline">\(s^2\)</span> because the denominator is smaller by one degree of freedom. However, for data sets of moderate sample size, the effect is minimal. Adding several extraneous variables can inflate <span class="math inline">\(s^{2}\)</span> appreciably, however. Further, there is the possibility that adding extraneous explanatory variables will induce, or worsen, the presence of collinearity.</p>
<p>A more important point is that, by adding extraneous variables, our regression coefficient estimates remain <em>unbiased</em>. Consider the following example.</p>
<hr />
<p><strong>Example: Regression using One Explanatory Variable.</strong> Assume that the true model of the responses is</p>
<p><span class="math display">\[
y_i = \beta_0 + \varepsilon_i, \quad i = 1, \ldots, n.
\]</span></p>
<p>Under this model, the level of a generic explanatory variable <span class="math inline">\(x\)</span> does not affect the value of the response <span class="math inline">\(y\)</span>. If we were to predict the response at any level of <span class="math inline">\(x\)</span>, the prediction would have expected value <span class="math inline">\(\beta_0\)</span>. However, suppose we mistakenly fit the model</p>
<p><span class="math display">\[
y_i = \beta_0^{\ast} + \beta_1^{\ast}x_i + \varepsilon_i^{\ast}.
\]</span></p>
<p>With this model, the prediction at a generic level <span class="math inline">\(x\)</span> is <span class="math inline">\(b_{0}^{\ast} + b_{1}^{\ast}x\)</span> where <span class="math inline">\(b_{0}^{\ast}\)</span> and <span class="math inline">\(b_{1}^{\ast}\)</span> are the usual least squares estimates of <span class="math inline">\(\beta_0^{\ast}\)</span> and <span class="math inline">\(\beta_1^{\ast}\)</span>, respectively. It is not too hard to confirm that</p>
<p><span class="math display">\[
\text{Bias} = \text{E}(b_{0}^{\ast} + b_{1}^{\ast}x) - \text{E}y = 0,
\]</span></p>
<p>where the expectations are calculated using the true model. Thus, by using a slightly larger model than we should have, we did not pay for it in terms of making a persistent, long-term error such as represented by the bias. The price of making this mistake is that our standard error is slightly higher than it would be if we had chosen the correct model.</p>
<hr />
</div>
<div id="Sec622" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Underfitting the Model<a href="interpreting-regression-results.html#Sec622" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This type of error occurs when important variables are omitted from the model specification; it is more serious than overfitting. Omitting important variables can cause appreciable amounts of bias in our resulting estimates. Further, because of the bias, the resulting estimates of <span class="math inline">\(s^{2}\)</span> are larger than need be. A larger <span class="math inline">\(s\)</span> inflates our prediction intervals and produces inaccurate tests of hypotheses concerning the importance of explanatory variables. To see the effects of underfitting a model, we return to the previous example.</p>
<hr />
<p><strong>Example: Regression using One Explanatory Variable - Continued.</strong> We now reverse the roles of the models described before. Assume that the true model is</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
\]</span></p>
<p>and that we mistakenly fit the model,</p>
<p><span class="math display">\[
y_i = \beta_0^{\ast} + \varepsilon_i^{\ast}.
\]</span></p>
<p>Thus, we have inadvertently omitted the effects of the explanatory variable <span class="math inline">\(x\)</span>. With the fitted model, we would use <span class="math inline">\(\bar{y}\)</span> for our prediction at a generic level of <span class="math inline">\(x\)</span>. From the true model, we have <span class="math inline">\(\bar{y} = \beta_0 + \beta_1 \bar{x} + \bar{\varepsilon}\)</span>. The bias of the prediction at <span class="math inline">\(x\)</span> is</p>
<p><span class="math display">\[
\begin{array}{ll}
\text{Bias} &amp;= \text{E} \bar{y} - \text{E} (\beta_0 + \beta_1 x + \varepsilon) \\
&amp;= \text{E} (\beta_0 + \beta_1 \bar{x} + \bar{\varepsilon}) - (\beta_0 + \beta_1 x) \\
&amp;= \beta_1 (\bar{x} - x).
\end{array}
\]</span></p>
<p>If <span class="math inline">\(\beta_1\)</span> is positive, then we under-predict for large values of <span class="math inline">\(x\)</span>, resulting in a negative bias, and over-predict for small values of <span class="math inline">\(x\)</span> (relative to <span class="math inline">\(\overline{x}\)</span>). Thus, there is a persistent, long-term error in omitting the explanatory variable <span class="math inline">\(x\)</span>. Similarly, one can check that this type of error produces biased regression parameter estimates and an inflated value of <span class="math inline">\(s^{2}\)</span>.</p>
<hr />
<p>Of course, no one wants to overfit or underfit the model. However, data from the social sciences are often messy and it can be hard to know whether or not to include a variable in the model. When selecting variables, analysts are often guided by the principle of parsimony, also known as Occam’s Razor, which states that when there are several possible explanations for a phenomenon, use the simplest. There are several arguments for preferring simpler models:</p>
<ul>
<li>A simpler explanation is easier to interpret.</li>
<li>Simple models, also known as “parsimonious” models, often do well on fitting out-of-sample data.</li>
<li>Extraneous variables can cause problems of collinearity, leading to difficulty in interpreting individual coefficients.</li>
</ul>
<p>The contrasting viewpoint can be summarized in a quote often attributed to Albert Einstein, that states that we should use “the simplest model possible, but no simpler.” This section demonstrates that underfitting a model, by omitting important variables, is typically a more serious error than including extraneous variables that add little to our ability to explain the data. Including extraneous variables decreases the degrees of freedom and increases the estimate of variability, typically of less concern in actuarial applications.</p>
<p>When in doubt, leave the variable in.</p>
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_abnsy04p&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_nvyvpbzk%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="6.2 ImpVarSelection">
</iframe>
</center>
</div>
</div>
<div id="Sec63" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> The Importance of Data Collection<a href="interpreting-regression-results.html#Sec63" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The regression modeling process starts with collecting data. Having
studied the results, and the variable selection process, we can now
discuss the inputs to the process. Not surprisingly, there is a long
list of potential pitfalls that are frequently encountered when
collecting regression data. In this section, we identify the major
potential pitfalls and provide some avenues for avoiding these
pitfalls.</p>
<div id="Sec631" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Sampling Frame Error and Adverse Selection<a href="interpreting-regression-results.html#Sec631" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sampling frame error occurs when the sampling frame, the list from
which the sample is drawn, is not an adequate approximation of the
population of interest. In the end, a sample must be a
representative subset of a larger population, or universe, of
interest. If the sample is not representative, taking a larger
sample does not eliminate bias; you simply repeat the same mistake
over again and again.</p>
<hr />
<p><strong>Example: Literary Digest Poll.</strong> Perhaps the most widely known example of sampling frame error is from the 1936 <em>Literary Digest</em> poll. This poll was conducted to predict the winner of the 1936 U.S. Presidential election. The two leading candidates were Franklin D. Roosevelt, the Democrat, and Alfred Landon, the Republican. <em>Literary Digest</em>, a prominent magazine at the time, conducted a survey of ten million voters. Of those polled, 2.4 million responded, predicting a “landslide” Landon victory by a 57% to 43% margin. However, the actual election resulted in an overwhelming Roosevelt victory, by a 62% to 38% margin. What went wrong?</p>
<p>There were a number of problems with the <em>Literary Digest</em> survey. Perhaps the most important was the sampling frame error. To develop their sampling frame, <em>Literary Digest</em> used addresses from telephone books and membership lists of clubs. In 1936, the United States was in the depths of the Great Depression; telephones and club memberships were a luxury that only upper income individuals could afford. Thus, <em>Literary Digest</em>’s list included an unrepresentative number of upper income individuals. In previous presidential elections conducted by <em>Literary Digest</em>, the rich and poor tended to vote along similar lines and this was not a problem. However, economic problems were top political issues in the 1936 Presidential election. As it turned out, the poor tended to vote for Roosevelt and the rich tended to vote for Landon. As a result, the <em>Literary Digest</em> poll results were grossly mistaken. Taking a large sample, even of size 2.4 million, did not help; the basic mistake was repeated over and over again.</p>
<hr />
<p>Sampling frame bias occurs when the sample is not a representative
subset of the population of interest. When analyzing insurance
company data, this bias can arise due to <em>adverse selection</em>.
In many insurance markets, companies design and price contracts and
policyholders decide whether or not to enter a contractual agreement
(actually, policyholders “apply” for insurance so that insurers
also have a right not to enter into the agreement). Thus, someone is
more likely to enter into an agreement if they believe that the
insurer is underpricing their risk, especially in light of
policyholder characteristics that are not observed by the insurer.
For example, it is well known that mortality experience of a sample
of purchasers of life annuities is not representative of the overall
population; people who purchase annuities tend to be healthy
relative to the overall population. You would not purchase a life
annuity that pays a periodic benefit while living if you were in
poor health and thought that your probability of a long life to be
low. Adverse selection arises because “bad risks,” those with
higher than expected claims, are more likely to enter into contracts
than corresponding “good risks.” Here, the expectation is
developed based on characteristics (explanatory variables) that can
be observed by the insurer.</p>
<p>Of course, there is a large market for annuities and other forms of
insurance in which adverse selection exists. Insurance companies can
price these markets appropriately by redefining their “population
of interest” to be not the general population but rather the
population of potential policyholders. Thus, for example, in pricing
annuities, insurers use annuitant mortality data, not data for the
overall population. In this way, they can avoid potential mismatches
between the population and sample. More generally, the experience of
almost any company differs from the overall population due to
underwriting standards and sales philosophies. Some companies seek
“preferred risks” by offering educational discounts, good driving
bonuses and so forth, whereas others seek high risk insureds. The
company’s sample of insureds will differ from the overall population
and the extent of the difference can be an interesting aspect to
quantify in an analysis.</p>
<p>Sampling frame bias can be particularly important when a company
seeks to market a new product for which it has no experience data.
Identifying a target market and its relation to the overall
population is an important aspect of a market development plan.</p>
</div>
<div id="Sec632" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Limited Sampling Regions<a href="interpreting-regression-results.html#Sec632" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A limited sampling region can give rise to potential bias when we
try to extrapolate outside of the sampling region. To illustrate,
consider Figure <a href="interpreting-regression-results.html#fig:Fig61">6.1</a>. Here, based on the data in
the sampling region, a line may seem to be an appropriate
representation. However, if a quadratic curve is the true expected
response, any forecast that is far from the sampling region will be
seriously biased.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig61"></span>
<img src="RegressionMarkdown_files/figure-html/Fig61-1.png" alt="Extrapolation outside of the sampling region may be biased" width="60%" />
<p class="caption">
Figure 6.1: <strong>Extrapolation outside of the sampling region may be biased</strong>
</p>
</div>
<p>Another pitfall due to a limited sampling region, although not a
bias, that can arise is the difficulty in estimating a regression
coefficient. In Chapter 5, we saw that a smaller spread of a
variable, other things equal, means a less reliable estimate of the
slope coefficient associated with that variable. That is, from
Section 5.5.2 or equation <a href="interpreting-regression-results.html#eq:eq61">(6.1)</a>, we see that the
smaller is the spread of <span class="math inline">\(x_{j}\)</span>, as measured by <span class="math inline">\(s_{x_{j}}\)</span>, the
larger is the standard error of <span class="math inline">\(b_{j},se(b_{j})\)</span>. Taken to the
extreme, where <span class="math inline">\(s_{x_{j}}=0\)</span>, we might have a situation such as
illustrated in Figure <a href="interpreting-regression-results.html#fig:Fig62">6.2</a>. For the extreme
situation illustrated in Figure <a href="interpreting-regression-results.html#fig:Fig62">6.2</a>, there is not
enough variation in <span class="math inline">\(x\)</span> to estimate the corresponding slope
parameter.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig62"></span>
<img src="RegressionMarkdown_files/figure-html/Fig62-1.png" alt="The lack of variation in \(x\) means that we cannot fit a unique line relating \(x\) and \(y\)." width="60%" />
<p class="caption">
Figure 6.2: <strong>The lack of variation in <span class="math inline">\(x\)</span> means that we cannot fit a unique line relating <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</strong>
</p>
</div>
</div>
<div id="Sec633" class="section level3 hasAnchor" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Limited Dependent Variables, Censoring and Truncation<a href="interpreting-regression-results.html#Sec633" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In some applications, the dependent variable is constrained to fall
within certain regions. To see why this is a problem, first recall
that under the linear regression model, the dependent variable
equals the regression function plus a random error. Typically, the
random error is assumed to be approximately normally distributed, so
that the response varies continuously. However, if the outcomes of
the dependent variable are restricted, or limited, then the outcomes
are not purely continuous. This means that our assumption of normal
errors is not strictly correct, and may not even be a good
approximation.</p>
<p>To illustrate, Figure <a href="interpreting-regression-results.html#fig:Fig63">6.3</a> shows a plot of
individual’s income (<span class="math inline">\(x\)</span>) versus amount of insurance purchased
(<span class="math inline">\(y\)</span>). The sample in this plot represents two subsamples, those who
purchased insurance, corresponding to <span class="math inline">\(y&gt;0\)</span>, and those who did not,
corresponding to “price” <span class="math inline">\(y=0\)</span>. Fitting a single line to these
data would misinform users about the effects of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig63"></span>
<img src="RegressionMarkdown_files/figure-html/Fig63-1.png" alt="When individuals do not purchase anything, they are recorded as \(y=0\) sales." width="60%" />
<p class="caption">
Figure 6.3: <strong>When individuals do not purchase anything, they are recorded as <span class="math inline">\(y=0\)</span> sales.</strong>
</p>
</div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig64"></span>
<img src="RegressionMarkdown_files/figure-html/Fig64-1.png" alt="If the responses below the horizontal line at \(y=d\) are omitted, then the fitted regression line can be very different from the true regression line." width="60%" />
<p class="caption">
Figure 6.4: <strong>If the responses below the horizontal line at <span class="math inline">\(y=d\)</span> are omitted, then the fitted regression line can be very different from the true regression line.</strong>
</p>
</div>
<p>If we dealt with only those who purchased insurance then we still
would have an implicit lower bound of zero (if an insurance price
must exceed zero). However, prices need be close to this bound for a
given sampling region and thus not represent an important practical
problem. By including several individuals who did not purchase
insurance (and thus spent $0 on insurance), our sampling region now
clearly includes this lower bound.</p>
<p>There are several ways in which dependent variables can be restricted, or <em>censored</em>. Figure <a href="interpreting-regression-results.html#fig:Fig63">6.3</a> illustrates the case in which the value of <span class="math inline">\(y\)</span> may be no lower than zero. As another example, insurance claims are often restricted to be less than or equal to an upper limit specified in the insurance policy. If censoring is severe, ordinary least squares produces biased results. Specialized approaches, known as <em>censored regression</em> models, are described in Chapter 15 to handle this problem.</p>
<p>Figure <a href="interpreting-regression-results.html#fig:Fig64">6.4</a> illustrates another commonly encountered limitation on the value of the dependent variable. For this illustration, suppose that <span class="math inline">\(y\)</span> represents an insured loss and that <span class="math inline">\(d\)</span> represents the deductible on an insurance policy. In this scenario, it is common practice for insurers to not record losses below <span class="math inline">\(d\)</span> (they are typically not reported by policyholders). In this case, the data are said to be <em>truncated</em>. Not surprisingly, <em>truncated regression models</em> are available to handle this situation. As a rule of thumb, truncated data represent a more serious source of bias than censored data. When data are truncated, we do not have values of dependent variables and thus have less information than when the data are censored. See Chapter 15 for further discussion.</p>
<div class="blackboxvideo">
<p><strong>Video: Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_s3zp1nph&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_4umk9h19%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="6.31 BiasData">
</iframe>
</center>
</div>
<div id="Sec634" class="section level3 hasAnchor" number="6.3.4">
<h3><span class="header-section-number">6.3.4</span> Omitted and Endogenous Variables<a href="interpreting-regression-results.html#Sec634" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Of course, analysts prefer to include all important variables.
However, a common problem is that we may not have the resources nor
the foresight to gather and analyze all the relevant data. Further,
sometimes we are prohibited from including variables. For example,
in insurance rating we are typically precluded from using ethnicity
as a rating variable. Further, there are many mortality and other
decrement tables that are “uni-sex,” that is, blind to gender.</p>
<p>Omitting important variables can affect our ability to fit the
regression function; this can affect in-sample (explanation) as well
as out-of-sample (prediction) performance. If the omitted variable
is uncorrelated with other explanatory variables, then the omission
will not affect estimation of regression coefficients. Typically
this is not the case. The Section 3.4.3 Refrigerator Example
illustrates a serious case where the direction of a statistically
significant result was reversed based on the presence of an
explanatory variable. In this example, we found that a cross-section
of refrigerators displayed a significantly positive correlation
between price and the annual energy cost of operating the
refrigerator. This positive correlation was counter-intuitive
because we would hope that higher prices would mean lower annual
expenditures in operating a refrigerator. However, when we included
several additional variables, in particular, measures of the size of
a refrigerator, we found a significantly negative relationship
between price and energy costs. Again, by omitting these additional
variables, there was an important bias when using regression to
understand the relationship between price and energy costs.</p>
<p>Omitted variables can lead to the presence of endogenous explanatory
variables. An exogenous variable is one that can be taken as
“given” for the purposes at hand. An endogenous variable is one
that fails the exogeneity requirement. An omitted variable can
affect both the <span class="math inline">\(y\)</span> and the <span class="math inline">\(x\)</span> and in this sense induce a
relationship between the two variables. If the relationship between
<span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is due to an omitted variable, it is difficult to
condition on the <span class="math inline">\(x\)</span> when estimating a model for <span class="math inline">\(y\)</span>.</p>
<p>Up to now, the explanatory variables have been treated as
non-stochastic. For many social science applications, it is more
intuitive to consider the <span class="math inline">\(x\)</span>’s to be stochastic, and perform
inference conditional on their realizations. For example, under
common sampling schemes, we can estimate the conditional regression
function
<span class="math display">\[
\mathrm{E~}\left(y|x_1, \ldots, x_k \right) = \beta_0 + \beta_1 x_1
+ \ldots + \beta_k x_k .
\]</span>
This is known as a “sampling-based” model.</p>
<p>In the economics literature, Goldberger (1972) defines a
<em>structural model</em> as a stochastic model representing a causal
relationship, not a relationship that simply captures statistical
associations. Structural models can readily contain endogenous
explanatory variables. To illustrate, we consider an example
relating claims and premiums. For many lines of business, premium
classes are simply nonlinear functions of exogenous factors such as
age, gender and so forth. For other lines of business, premiums
charged are a function of prior claims history. Consider model
equations that relate one’s claims (<span class="math inline">\(y_{it}, t=1, 2\)</span>) to premiums
(<span class="math inline">\(x_{it}, t=1, 2\)</span>):
<span class="math display">\[\begin{eqnarray*}
y_{i2} = \beta_{0,C} + \beta_{1,C} y_{i1} + \beta_{2,C} x_{i2} +
\varepsilon_{i1} \\
x_{i2} = \beta_{0,P} + \beta_{1,P}  y_{i1} + \beta_{2,P}  x_{i1}
+\varepsilon_{i2} .
\end{eqnarray*}\]</span>
In this model, current period (<span class="math inline">\(t=2\)</span>) claims and premiums are
affected by the prior period’s claims and premiums. This is an
example of a <em>structural equations</em> model that requires special
estimation techniques. Our usual estimation procedures are biased!</p>
<hr />
<p><strong>Example: Race, Redlining and Automobile Insurance Prices - Continued.</strong> Although Harrington and Niehaus (1998) did not find
racial discrimination in insurance pricing, their results on access
to insurance were inconclusive. Insurers offer “standard” and
“preferred” risk contracts to applicants that meet restrictive
underwriting standards, as compared to “substandard” risk
contracts where underwriting standards are more relaxed. Expected
claims are lower for standard and preferred risk contracts, and so
premiums are lower, than substandard contracts. Harrington and
Niehaus examined the proportion of applicants offered substandard
contracts, NSSHARE, and found it significantly positively related to
PCTBLACK, the proportion of population black. This suggests evidence
of racial discrimination; they state this to be an inappropriate
interpretation due to omitted variable bias.</p>
<p>Harrington and Niehaus argue that the proportion of applicants
offered substandard contracts should be positively related to
expected claim costs. Further, expected claim costs are strongly
related to PCTBLACK, because minorities in the sample tended to be
lower income. Thus, unobserved variables such as income tend to
drive the positive relationship between NSSHARE and PCTBLACK.
Because the data are analyzed at the zip code level and not at the
individual level, the potential omitted variable bias rendered the
analysis inconclusive.</p>
<hr />
</div>
<div id="Sec635" class="section level3 hasAnchor" number="6.3.5">
<h3><span class="header-section-number">6.3.5</span> Missing Data<a href="interpreting-regression-results.html#Sec635" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the data examples, illustrations, case studies and exercises of
this text, there are many instances where certain data are
unavailable for analysis, or <em>missing</em>. In every instance,
the data were not carelessly lost but were unavailable due to
substantive reasons associated with the data collection. For
example, when we examined stock returns from a cross-section of
companies, we saw that some companies did not have an average five
year earnings per share figure. The reason was simply that they had
not been in existence for five years. As another example, when
examining life expectancies, some countries did not report the total
fertility rate because they lacked administrative resources to
capture this data. Missing data are an inescapable aspect of
analyzing data in the social sciences.</p>
<p>When the reason for the lack of availability of data is unrelated to
actual data values, the data are said to be <em>missing at
random</em>. There are a variety of techniques for handling missing at
random data, none of which are clearly superior to the others. One
“technique” is to simply ignore the problem. Hence, missing at
random is sometimes called the <em>ignorable case</em> of missing
data.</p>
<p>If there are only a few missing data, compared to the total number
available, a widely employed strategy is to delete the observations
corresponding to the missing data. Assuming that the data are
missing at random, little information is lost by deleting a small
portion of the data. Further, with this strategy, we need not make
additional assumptions about the relationships among the data.</p>
<p>If the missing data are primarily from one variable, we can consider
omitting this variable. Here, the motivation is that we lose less
information when omitting this variable as compared to retaining the
variable but losing the observations associated with the missing
data.</p>
<p>Another strategy is to fill in, or <em>impute</em>, missing data.
There are many variations of the imputation strategy. All assume
some type of relationships among the variables in addition to the
regression model assumptions. Although these methods yield
reasonable results, note that any type of filled-in values do not
yield the same inherent variability as the real data. Thus, results
of analyses based on imputed values often reflect less variability
than those with real data.</p>
<hr />
<p><strong>Example: Insurance Company Expenses - Continued.</strong> When
examining company financial information, analysts commonly are
forced to omit substantial amounts of information when using
regression models to search for relationships. To illustrate, Segal
(2002) examined life insurance financial statements from data
provided by the National Association of Insurance Commissioners
(NAIC). He initially considered 733 firm-year observations over the
period 1995-1998. However, 154 observations were excluded because of
inconsistent or negative premiums, benefits and other important
explanatory variables. Small companies representing 131 observations
were also excluded. Small companies consist of fewer than 10
employees and agents, operating costs less than $1 million or fewer
than 1,000 life policies sold. The resulting sample was <span class="math inline">\(n=448\)</span>
observations. The sample restrictions were based on explanatory
variables - this procedure does not necessarily bias results. Segal
argued that his final sample remained representative of the
population of interest. There were about 110 firms in each of
1995-1998. In 1998, aggregate assets of the firms in the sample
represent approximately $650 billion, a third of the life insurance
industry.</p>
<hr />
<div class="blackboxvideo">
<p><strong>Video: Section Summary</strong></p>
</div>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/embedPlaykitJs/uiconf_id/55063162?iframeembed=true&amp;entry_id=1_8gt6z6rb&amp;config%5Bprovider%5D=%7B%22widgetId%22%3A%221_c56k6fc3%22%7D&amp;config%5Bplayback%5D=%7B%22startTime%22%3A0%7D" style="width: 576px;height: 324px;border: 0;" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" title="6.32 BiasOmittedVariables">
</iframe>
</center>
</div>
</div>
<div id="Sec64" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Missing Data Models<a href="interpreting-regression-results.html#Sec64" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To understand the mechanisms that lead to unplanned nonresponse, we
model it stochastically. Let <span class="math inline">\(r_i\)</span> be a binary variable for the
<span class="math inline">\(i\)</span>th observation, with a one indicating that this response is
observed and a zero indicating that the response is missing. Let
<span class="math inline">\(\mathbf{r} = (r_1, \ldots, r_n)^{\prime}\)</span> summarize the data
availability for all subjects. The interest is in whether or not the
responses influence the missing data mechanism. For notation, we use
<span class="math inline">\(\mathbf{Y} = (y_1, \ldots, y_n)^{\prime}\)</span> to be the collection of
all potentially observed responses.</p>
<div id="Sec641" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Missing at Random<a href="interpreting-regression-results.html#Sec641" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the case where <span class="math inline">\(\mathbf{Y}\)</span> does not affect the distribution of
<span class="math inline">\(\mathbf{r}\)</span>, we follow Rubin (1976) and call this case
<em>missing completely at random (MCAR)</em>. Specifically, the
missing data are MCAR if <span class="math inline">\(\mathrm{f}(\mathbf{r} | \mathbf{Y}) = \mathrm{f}(\mathbf{r})\)</span>, where f(.) is a generic probability mass
function. An extension of this idea is in Little (1995), where the
adjective “covariate dependent” is added when <span class="math inline">\(\mathbf{Y}\)</span> does
not affect the distribution of <span class="math inline">\(\mathbf{r}\)</span>, conditional on the
covariates. If the covariates are summarized as <span class="math inline">\(\mathbf{X}\)</span>, then
the condition corresponds to the relation <span class="math inline">\(\mathrm{f}(\mathbf{r} | \mathbf{Y, X}) = \mathrm{f}(\mathbf{r | X})\)</span>. To illustrate this
point, consider an example of Little and Rubin (1987) where
<span class="math inline">\(\mathbf{X}\)</span> corresponds to age and <span class="math inline">\(\mathbf{Y}\)</span> corresponds to
income of all potential observations. If the probability of being
missing does not depend on income, then the missing data are MCAR.
If the probability of being missing varies by age but does not by
income over observations within an age group, then the missing data
are covariate dependent MCAR. Under the latter specification, it is
possible for the missing data to vary by income. For example,
younger people may be less likely to respond to a survey. This shows
that the “missing at random” feature depends on the purpose of the
analysis. Specifically, it is possible that an analysis of the joint
effects of age and income may encounter serious patterns of missing
data whereas an analysis of income controlled for age suffers no
serious bias patterns.</p>
<p>Little and Rubin (1987) advocate modeling the missing data
mechanisms. To illustrate, consider a likelihood approach using a
selection model for the missing data mechanism. Now, partition
<span class="math inline">\(\mathbf{Y}\)</span> into observed and missing components using the notation
<span class="math inline">\(\mathbf{Y} =\{\mathbf{Y}_{obs}, \mathbf{Y}_{miss}\}\)</span>. With the
likelihood approach, we base inference on the observed random
variables. Thus, we use a likelihood proportional to the joint
function <span class="math inline">\(\mathrm{f}(\mathbf{r}, \mathbf{Y}_{obs})\)</span>. We also specify
a <em>selection model</em> by specifying the conditional mass function
<span class="math inline">\(\mathrm{f}(\mathbf{r} | \mathbf{Y})\)</span>.</p>
<p>Suppose that the observed responses and the selection model
distributions are characterized by vectors of parameters
<span class="math inline">\(\boldsymbol \theta\)</span> and <span class="math inline">\(\boldsymbol \psi\)</span> , respectively. Then,
with the relation <span class="math inline">\(\mathrm{f}(\mathbf{r}, \mathbf{Y}_{obs},\boldsymbol \theta, \boldsymbol \psi)\)</span> <span class="math inline">\(= \mathrm{f}(\mathbf{Y}_{obs}, \boldsymbol \theta) \times \mathrm{f}(\mathbf{r} | \mathbf{Y}_{obs}, \boldsymbol \psi)\)</span>, we may
express the log likelihood of the observed random variables as</p>
<p><span class="math display">\[
L(\boldsymbol \theta, \boldsymbol \psi) = \mathrm{ln~}
\mathrm{f}(\mathbf{r}, \mathbf{Y}_{obs}, \boldsymbol \theta,
\boldsymbol \psi) = \mathrm{ln~} \mathrm{f}(\mathbf{Y}_{obs},
\boldsymbol \theta) + \mathrm{ln~} \mathrm{f}(\mathbf{r} |
\mathbf{Y}_{obs}, \boldsymbol \psi).
\]</span>
(See Section 11.9 if you would like a refresher on likelihood
inference.) In the case where the data are MCAR, then
<span class="math inline">\(\mathrm{f}(\mathbf{r} | \mathbf{Y}_{obs}, \boldsymbol \psi) = \mathrm{f}(\mathbf{r} | \boldsymbol \psi)\)</span> does not depend on
<span class="math inline">\(\mathbf{Y}_{obs}\)</span>. Little and Rubin (1987) also consider the case
where the selection mechanism model distribution does not depend on
<span class="math inline">\(\mathbf{Y}_{miss}\)</span> but may depend on <span class="math inline">\(\mathbf{Y}_{obs}\)</span>. In this
case, they call the <em>data missing at random (MAR)</em>.</p>
<p>In both the MAR and MCAR cases, we see that the likelihood may be
maximized over the parameters, separately for each case. In
particular, if one is only interested in the maximum likelihood
estimator of <span class="math inline">\(\boldsymbol \theta\)</span>, then the selection model
mechanism may be “ignored.” Hence, both situations are often
referred to as the <em>ignorable case</em>.</p>
<hr />
<p><strong>Example: Dental Expenditures.</strong> Let <span class="math inline">\(y\)</span> represent a household’s annual dental expenditure and <span class="math inline">\(x\)</span> represent income. Consider the following five selection mechanisms.</p>
<ul>
<li>The household is not selected (missing) with probability without regard to the level of dental expenditure. In this case, the selection mechanism is MCAR.</li>
<li>The household is not selected if the dental expenditure is less than $100. In this case, the selection mechanism depends on the observed and missing response. The selection mechanism cannot be ignored.</li>
<li>The household is not selected if the income is less than $20,000. In this case, the selection mechanism is MCAR, covariate dependent. That is, assuming that the purpose of the analysis is to understand dental expenditures conditional on knowledge of income, stratifying based on income does not seriously bias the analysis.</li>
<li>The probability of a household being selected increases with dental expenditure. For example, suppose the probability of being selected is a linear function of <span class="math inline">\(\exp(\psi y_i)/(1+ \exp(\psi y_i))\)</span>. In this case, the selection mechanism depends on the observed and missing response. The selection mechanism cannot be ignored.</li>
<li>The household is followed over <span class="math inline">\(T\)</span> = 2 periods. In the second period, a household is not selected if the first period expenditure is less than $100. In this case, the selection mechanism is MAR. That is, the selection mechanism is based on an observed response.</li>
</ul>
<hr />
<p>The second and fourth selection mechanisms represent situations
where the selection mechanism must be explicitly modeled; these are
non-ignorable cases. In these situations without explicit
adjustments, procedures that ignore the selection effect may produce
seriously biased results. To illustrate a correction for selection
bias in a simple case, we outline an example due to Little and Rubin
(1987). Section <a href="interpreting-regression-results.html#Sec642">6.4.2</a> describes additional mechanisms.</p>
<hr />
<p><strong>Example: Historical Heights.</strong> Little and Rubin (1987) discuss data due to Wachter and Trusell (1982) on <span class="math inline">\(y\)</span>, the height of men recruited to serve in the
military. The sample is subject to censoring in that minimum height
standards were imposed for admission to the military. Thus, the
selection mechanism is
<span class="math display">\[
r_i = \left\{
              \begin{array}{ll}
                1 &amp; y_i &gt; c_i \\
                0 &amp; \mathrm{otherwise} \\
              \end{array}
            \right. ,
\]</span>
where <span class="math inline">\(c_i\)</span> is the known minimum height standard imposed at the time
of recruitment. The selection mechanism is non-ignorable because it
depends on the individual’s height, <span class="math inline">\(y\)</span>.</p>
<p>For this example, additional information is available to provide
reliable model inference. Specifically, based on other studies of
male heights, we may assume that the population of heights is
normally distributed. Thus, the likelihood of the observables can be
written down and inference may proceed directly. To illustrate,
suppose that <span class="math inline">\(c_i = c\)</span> is constant. Let <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> denote
the mean and standard deviation of <span class="math inline">\(y\)</span>. Further suppose that we have
a random sample of <span class="math inline">\(n + m\)</span> men in which <span class="math inline">\(m\)</span> men fall below the
minimum standard height <span class="math inline">\(c\)</span> and we observe <span class="math inline">\(\mathbf{Y}_{obs} = (y_1, \ldots, y_n)^{\prime}\)</span>. The joint distribution for observables is</p>
<p><span class="math display">\[\begin{eqnarray*}
\mathrm{f}(\mathbf{r}, \mathbf{Y}_{obs}, \mu, \sigma) &amp;=&amp;
\mathrm{f}(\mathbf{Y}_{obs}, \mu, \sigma) \times
\mathrm{f}(\mathbf{r} | \mathbf{Y}_{obs}) \\
&amp;=&amp; \left\{ \prod_{i=1}^n   \mathrm{f}(y_i | y_i &gt; c) \times
\mathrm{Pr}(y_i &gt; c) \right\}
\times \left\{\mathrm{Pr}(y_i \leq c)\right\}^m.
\end{eqnarray*}\]</span>
Now, let <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\Phi\)</span> represent the density and distribution
function for the standard normal distribution. Thus, the
log-likelihood is
<span class="math display">\[\begin{eqnarray*}
L(\mu, \sigma) &amp;=&amp; \mathrm{ln~} \mathrm{f}(\mathbf{r},
\mathbf{Y}_{obs}, \mu, \sigma) \\
&amp;=&amp; \sum_{i=1}^n \mathrm{ln}\left\{ \frac{1}{\sigma} \phi \left(
\frac{y_i-\mu}{\sigma} \right)
\right\}
+  m~ \mathrm{ln}\left\{ \Phi \left(
\frac{c-\mu}{\sigma}\right) \right\} .
\end{eqnarray*}\]</span>
This is easy to maximize in <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. If one ignored the
censoring mechanisms, then one would derive estimates of the
observed data from the “log likelihood,”
<span class="math display">\[
\sum_{i=1}^n \mathrm{ln}\left\{ \frac{1}{\sigma} \phi \left(
\frac{y_i-\mu}{\sigma} \right)
\right\},
\]</span>
yielding different, and biased, results.</p>
<hr />
</div>
<div id="Sec642" class="section level3 hasAnchor" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> Non-Ignorable Missing Data<a href="interpreting-regression-results.html#Sec642" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For non-ignorable missing data, Little (1995) recommends:</p>
<ul>
<li>Avoid missing responses whenever possible by using appropriate follow-up procedures.</li>
<li>Collect covariates that are useful for predicting missing values.</li>
<li>Collect as much information as possible regarding the nature of the missing data mechanism.</li>
</ul>
<p>For the last point, if little is known about the missing data mechanism, then it is difficult to employ a robust statistical procedure to correct for the selection bias.</p>
<p>There are many models of missing data mechanisms. A general overview
appears in Little and Rubin (1987). Little (1995) surveys the
problem of attrition. Rather than survey this developing literature,
we give a widely used model of non-ignorable missing data.</p>
<div id="heckman-two-stage-procedure" class="section level4 unnumbered hasAnchor">
<h4>Heckman Two-Stage Procedure<a href="interpreting-regression-results.html#heckman-two-stage-procedure" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Heckman (1976) assumes that the sampling response mechanism is
governed by the latent (unobserved) variable <span class="math inline">\(r_i^{\ast}\)</span> where
<span class="math display">\[
r_i^{\ast} = \mathbf{z}_i^{\prime} \boldsymbol \gamma + \eta_i.
\]</span>
The variables in <span class="math inline">\(\mathbf{z}_i\)</span> may or may not include the variables
in <span class="math inline">\(\mathbf{x}_i\)</span>. We observe <span class="math inline">\(y_i\)</span> if <span class="math inline">\(r_i^{\ast}&gt;0\)</span>, that is, if
<span class="math inline">\(r_i^{\ast}\)</span> crosses the threshold 0. Thus, we observe
<span class="math display">\[
r_i = \left\{
              \begin{array}{ll}
                1 &amp; r_i^{\ast}&gt;0 \\
                0 &amp; \mathrm{otherwise} \\
              \end{array}
            \right. .
\]</span>
To complete the specification, we assume that
{(<span class="math inline">\(\varepsilon_i,\eta_i\)</span>)} are identically and independently
distributed, and that the joint distribution of
(<span class="math inline">\(\varepsilon_i,\eta_i\)</span>) is bivariate normal with means zero,
variances <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\sigma_{\eta}^2\)</span> and correlation <span class="math inline">\(\rho\)</span>.
Note that if the correlation parameter <span class="math inline">\(\rho\)</span> equals zero, then the
response and selection models are independent. In this case, the
data are MCAR and the usual estimation procedures are unbiased and
asymptotically efficient.</p>
<p>Under these assumptions, basic multivariate normal calculations show
that
<span class="math display">\[
\mathrm{E~}(y_i | r_i^{\ast}&gt;0) = \mathbf{x}_i^{\prime} \boldsymbol
\beta + \beta_{\lambda} \lambda(\mathbf{z}_i^{\prime} \boldsymbol
\gamma),
\]</span>
where <span class="math inline">\(\beta_{\lambda} = \rho \sigma\)</span> and
<span class="math inline">\(\lambda(a)=\phi(a)/\Phi(a)\)</span>. Here, <span class="math inline">\(\lambda(.)\)</span> is the inverse of
the so-called “Mills ratio.” This calculation suggests the
following two-step procedure for estimating the parameters of
interest.</p>
<div class="blackbox">
<p><em>Heckman’s Two-Stage Procedure</em></p>
<ol style="list-style-type: decimal">
<li>Use the data {(<span class="math inline">\(r_i, \mathbf{z}_i\)</span>)} and a probit regression model to estimate <span class="math inline">\(\boldsymbol \gamma\)</span>. Call this estimator <span class="math inline">\(\mathbf{g}_H\)</span>.</li>
<li>Use the estimator from stage (1) to create a new explanatory variable, <span class="math inline">\(x_{i,K+1} = \lambda(\mathbf{z}_i^{\prime}\mathbf{g}_H)\)</span>. Run a regression model using the <span class="math inline">\(K\)</span> explanatory variables <span class="math inline">\(\mathbf{x}_i\)</span> as well as the additional explanatory variable <span class="math inline">\(x_{i,K+1}\)</span>. Use <span class="math inline">\(\mathbf{b}_H\)</span> and <span class="math inline">\(b_{\lambda,H}\)</span> to denote the estimators of <span class="math inline">\(\boldsymbol \beta\)</span> and <span class="math inline">\(\beta_{\lambda}\)</span>, respectively.</li>
</ol>
</div>
<p><br></p>
<p>Chapter 11 will introduce probit regressions. We also note
that the two-step method does not work in absence of covariates to
predict the response and, for practical purposes, requires variables
in <span class="math inline">\(\mathbf{z}\)</span> that are not in <span class="math inline">\(\mathbf{x}\)</span> (see Little and Rubin,
1987).</p>
<p>To test for selection bias, we may test the null hypothesis
<span class="math inline">\(H_0:\beta_{\lambda}=0\)</span> in the second stage due to the relation
<span class="math inline">\(\beta_{\lambda}= \rho \sigma\)</span>. When conducting this test, one
should use heteroscedasticity-corrected standard errors. This is
because the conditional variance <span class="math inline">\(\mathrm{Var}(y_i | r_i^{\ast}&gt;0)\)</span>
depends on the observation <span class="math inline">\(i\)</span>. Specifically, <span class="math inline">\(\mathrm{Var}(y_i | r_i^{\ast}&gt;0) = \sigma^2 (1-\rho^2 \delta_i),\)</span> where <span class="math inline">\(\delta_i= \lambda_i(\lambda_i + \mathbf{z}_i^{\prime} \boldsymbol \gamma)\)</span> and
<span class="math inline">\(\lambda_i = \phi(\mathbf{z}_i^{\prime} \boldsymbol \gamma)/\Phi(\mathbf{z}_i^{\prime} \boldsymbol \gamma).\)</span></p>
<p>This procedure assumes normality for the selection latent variables
to form the augmented variables. Other distribution forms are
available in the literature, including the logistic and uniform
distributions. A deeper criticism, raised by Little (1985), is that
the procedure relies heavily on assumptions that cannot be tested
using the data available. This criticism is analogous to the
historical heights example where we relied heavily on the normal
curve to infer the distribution of heights below the censoring
point. Despite these criticisms, Heckman’s procedure is widely used
in the social sciences.</p>
</div>
<div id="em-algorithm" class="section level4 unnumbered hasAnchor">
<h4>EM algorithm<a href="interpreting-regression-results.html#em-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Section <a href="interpreting-regression-results.html#Sec642">6.4.2</a> has focused on introducing specific
models of non-ignorable nonresponse. General robust models of
nonresponse are not available. Rather, a more appropriate strategy
is to focus on a specific situation, collect as much information as
possible regarding the nature of the selection problem and then
develop a model for this specific selection problem.</p>
<p>The EM algorithm is a computational device for computing model
parameters. Although specific to each model, it has found
applications in a wide variety of models involving missing data.
Computationally, the algorithm iterates between the “E,” for
conditional expectation, and “M,” for maximization, steps. The E
step finds the conditional expectation of the missing data given the
observed data and current values of the estimated parameters. This
is analogous to the time-honored tradition of imputing missing data.
A key innovation of the EM algorithm is that one imputes sufficient
statistics for missing values, not the individual data points. For
the M step, one updates parameter estimates by maximizing an
observed log likelihood. Both the sufficient statistics and the log
likelihood depend on the model specification.</p>
<p>Many introductions of the EM algorithm are available in the
literature. Little and Rubin (1987) provide a detailed treatment.</p>
</div>
</div>
</div>
<div id="Sec65" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Application: Risk Managers’ Cost Effectiveness<a href="interpreting-regression-results.html#Sec65" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section examines data from a survey on the cost effectiveness
of risk management practices. Risk management practices are
activities undertaken by a firm to minimize the potential cost of
future losses, such as the event of a fire in a warehouse or an
accident that injures employees. This section develops a model that
can be used to make statements about cost of managing risks.</p>
<p>An outline of the regression modeling process is as follows. We
begin by providing an introduction to the problem and giving some
brief background on the data. Certain prior theories will lead us to
present a preliminary model fit. Using diagnostic techniques, it
will be evident that several assumptions underpinning this model are
not in accord with the data. This will lead us to go back to the
beginning and start the analysis from scratch. Things that we learn
from a detailed examination of the data will lead us to postulate
some revised models. Finally, to communicate certain aspects of the
new model, we will explore graphical presentations of the
recommended model.</p>
<div id="introduction" class="section level4 unnumbered hasAnchor">
<h4>Introduction<a href="interpreting-regression-results.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The data for this study were provided by Professor Joan Schmit and
are discussed in more detail in the paper, “Cost effectiveness of
risk management practices,” Schmit and Roth (1990). The data are
from a questionnaire that was sent to 374 risk managers of large
U.S.-based organizations. The purpose of the study was to relate
cost effectiveness to management’s philosophy of controlling the
company’s exposure to various property and casualty losses, after
adjusting for company effects such as size and industry type.</p>
<p>First, some caveats. Survey data are often based on samples of
convenience, not probability samples. Just as with all observational
data sets, regression methodology is a useful tool for summarizing
data. However, we must be careful when making inferences based on
this type of data set. For this particular survey, 162 managers
returned completed surveys resulting in a good response rate of
<span class="math inline">\(43\%\)</span>. However, for the variables included in the analysis (defined
below), only 73 forms were completed resulting in a complete
response rate of <span class="math inline">\(20\%\)</span>. Why such a dramatic difference? Managers,
like most people, typically do not mind responding to queries about
their attitudes, or opinions, about various issues. When questioned
about hard facts, in this case company asset size or insurance
premiums, either they considered the information proprietary and
were reluctant to respond even when guaranteed anonymity or they
simply were not willing to take the time to look up the information.
From a surveyor’s standpoint, this is unfortunate because typically
“attitudinal” data are fuzzy (high variance compared to the mean)
as compared to hard financial data. The tradeoff is that the latter
data are often hard to obtain. In fact, for this survey, several
pre-questionnaires were sent to ascertain managers’ willingness to
answer specific questions. From the pre-questionnaires, the
researchers severely reduced the number of financial questions that
they intended to ask.</p>
<p>A measure of risk management cost effectiveness, FIRMCOST, is the
dependent variable. This variable is defined as total property and
casualty premiums and uninsured losses as a percentage of total
assets. It is a proxy for annual expenditures associated with
insurable events, standardized by company size. Here, for the
financial variables, ASSUME is the per occurrence retention amount
as a percentage of total assets, CAP indicates whether the company
owns a captive insurance company, SIZELOG is the logarithm of total
assets and INDCOST is a measure of the firm’s industry risk.
Attitudinal variables include CENTRAL, a measure of the importance
of the local managers in choosing the amount of risk to be retained
and SOPH, a measure of the degree of importance in using analytical
tools, such as regression, in making risk management decisions.</p>
<p>In the paper, the researchers described several weaknesses of the definitions used but argue that these definitions provide useful information, based on the willingness of risk managers to obtain reliable information. The researchers also described several theories concerning relationships that may be confirmed by the data. Specifically, they hypothesized:</p>
<ul>
<li>There exists an inverse relationship between the risk retention (ASSUME) and cost (FIRMCOST). The idea behind this theory is that larger retention amounts should mean lower expenses to a firm, resulting in lower costs.</li>
<li>The use of a captive insurance company (CAP) results in lower costs. Presumably, a captive is used only when cost effective and consequently, this variable should indicate lower costs if used effectively.</li>
<li>There exists an inverse relationship between the measure of centralization (CENTRAL) and cost (FIRMCOST). Presumably, local managers would be able to make more cost-effective decisions because they are more familiar with local circumstances regarding risk management than centrally located managers.</li>
<li>There exists an inverse relationship between the measure of sophistication (SOPH) and cost (FIRMCOST). Presumably, more sophisticated analytical tools help firms to manage risk better, resulting in lower costs.</li>
</ul>
</div>
<div id="preliminary-analysis" class="section level4 unnumbered hasAnchor">
<h4>Preliminary Analysis<a href="interpreting-regression-results.html#preliminary-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To test the theories described above, the regression analysis framework can
be used. To do this, posit the model</p>
<p><span class="math display">\[
\small{
\begin{array}{ll}
\text{FIRMCOST} &amp;=&amp;\beta_0 +\beta_1 \text{ ASSUME}+\beta_{2}\text{ CAP}
+\beta_{3}\text{ SIZELOG}+\beta_4\text{ INDCOST} \\
&amp;&amp;+\beta_5\text{ CENTRAL}+\beta_6\text{ SOPH}+ \varepsilon.
\end{array}
}
\]</span></p>
<p>With this model, each theory can be interpreted in terms of
regression coefficients. For example, <span class="math inline">\(\beta_1\)</span> can be interpreted
as the expected change in cost per unit change in retention level
(ASSUME). Thus, if the first hypothesis is true, we expect <span class="math inline">\(\beta_1\)</span> to be negative. To test this, we can estimate <span class="math inline">\(b_{1}\)</span> and use our
tests of hypotheses machinery to decide if <span class="math inline">\(b_{1}\)</span> is significantly
less than zero. The variables SIZELOG and INDCOST are included in
the model to control for the effects of these variables. These
variables are not directly under a risk manager’s control and thus
are not of primary interest. However, inclusion of these variables
can account for an important part of the variability.</p>
<p>Data from 73 managers was fit using this regression model. Table
<a href="interpreting-regression-results.html#tab:Tab62">6.2</a> summarizes the fitted model.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab62">Table 6.2: </span><strong>Regression Results from a Preliminary Model Fit</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Coefficient
</th>
<th style="text-align:right;">
Standard Error
</th>
<th style="text-align:right;">
<span class="math inline">\(t\)</span>-Statistic
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
(Intercept)
</td>
<td style="text-align:right;width: 3cm; ">
59.765
</td>
<td style="text-align:right;width: 3cm; ">
19.065
</td>
<td style="text-align:right;width: 3cm; ">
3.135
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
ASSUME
</td>
<td style="text-align:right;width: 3cm; ">
-0.300
</td>
<td style="text-align:right;width: 3cm; ">
0.222
</td>
<td style="text-align:right;width: 3cm; ">
-1.353
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
CAP
</td>
<td style="text-align:right;width: 3cm; ">
5.498
</td>
<td style="text-align:right;width: 3cm; ">
3.848
</td>
<td style="text-align:right;width: 3cm; ">
1.429
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SIZELOG
</td>
<td style="text-align:right;width: 3cm; ">
-6.836
</td>
<td style="text-align:right;width: 3cm; ">
1.923
</td>
<td style="text-align:right;width: 3cm; ">
-3.555
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
INDCOST
</td>
<td style="text-align:right;width: 3cm; ">
23.078
</td>
<td style="text-align:right;width: 3cm; ">
8.304
</td>
<td style="text-align:right;width: 3cm; ">
2.779
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
CENTRAL
</td>
<td style="text-align:right;width: 3cm; ">
0.133
</td>
<td style="text-align:right;width: 3cm; ">
1.441
</td>
<td style="text-align:right;width: 3cm; ">
0.092
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SOPH
</td>
<td style="text-align:right;width: 3cm; ">
-0.137
</td>
<td style="text-align:right;width: 3cm; ">
0.347
</td>
<td style="text-align:right;width: 3cm; ">
-0.394
</td>
</tr>
</tbody>
</table>
<p>The adjusted coefficient of determination is <span class="math inline">\(R_{a}^{2}=18.8\%\)</span>, the <span class="math inline">\(F\)</span>-ratio is 3.78 and the residual standard deviation is <span class="math inline">\(s=14.56\)</span>.</p>
<p>On the basis of the summary statistics from the regression model, we can
conclude that the measures of centralization and sophistication do
not have an impact on our measure of cost effectiveness. For both of
these variables the <span class="math inline">\(t\)</span>-ratio is low, less than 1.0 in
absolute value. The effect of risk retention seems only somewhat
important. The coefficient has the appropriate sign although is only
1.35 standard errors below zero. This would not be considered
statistically significant at the 5% level, although it would be at
the 10% level (the <span class="math inline">\(p\)</span>-value is 9%). Perhaps most
perplexing is the coefficient associated with the CAP variable. We
theorized that this coefficient would be negative. However, in our
analysis of the data, the coefficient turns out to be positive and
is 1.43 standard errors above zero. This not only leads us to
disaffirm our theory, but also to search for new ideas that are in
accord with the information learned from the data. Schmit and Roth
suggest reasons that may help us interpret the results of our
hypothesis testing procedures. For example, they suggest that
managers in the sample may not have the most sophisticated tools
available to them when managing risks, resulting in an insignificant
coefficient associated with SOPH. They also discussed alternative
suggestions, as well as interpretations for the other results of the
tests of hypotheses.</p>
<p>How robust is this model? Section <a href="interpreting-regression-results.html#Sec62">6.2</a> emphasized some of the dangers of working with an inadequate model. Some readers may feel
uncomfortable with the model selected above because two out of the
six variables have <span class="math inline">\(t\)</span>-ratios less than 1 in absolute value
and four out of six have <span class="math inline">\(t\)</span>-ratios less than 1.5 in absolute
value. Perhaps even more important, histograms of the standardized
residuals and leverages, in Figure <a href="interpreting-regression-results.html#fig:Fig65">6.5</a>, show
several observations to be outliers and high leverage points. To
illustrate, the largest residual turns out to be <span class="math inline">\(e_{15}=83.73\)</span>. The
error sum of squares is <span class="math inline">\(Error~SS\)</span> = <span class="math inline">\((n-(k+1))s^{2}\)</span> =
<span class="math inline">\((73-7)(14.56)^{2}=13,987\)</span>. Thus, the
15th observation represents 50.1% of the error sum of squares <span class="math inline">\((=83.73^{2}/13,987)\)</span>, suggesting that this 1 observation out of 73
has a dominant impact on the model fit. Further, plots of
standardized residuals versus fitted values, not presented here,
displayed evidence of heteroscedastic residuals. Based on these
observations, it seems reasonable to assess the robustness of the
model.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig65"></span>
<img src="RegressionMarkdown_files/figure-html/Fig65-1.png" alt="Histograms of standardized residuals and leverages from a preliminary regression model fit." width="60%" />
<p class="caption">
Figure 6.5: <strong>Histograms of standardized residuals and leverages from a preliminary regression model fit.</strong>
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Tab62.Hide" href="javascript:togglecode('toggleCode.Tab62.Hide','displayCode.Tab62.Hide');"><i><strong>R Code to Produce Table 6.2 and Figure 6.5</strong></i></a>
</h5>
<div id="toggleCode.Tab62.Hide" style="display: none">
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="interpreting-regression-results.html#cb50-1" tabindex="-1"></a><span class="do">## Risk Manager&#39;s Analysis</span></span>
<span id="cb50-2"><a href="interpreting-regression-results.html#cb50-2" tabindex="-1"></a><span class="co"># Table 6.2</span></span>
<span id="cb50-3"><a href="interpreting-regression-results.html#cb50-3" tabindex="-1"></a>survey <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;CSVData/RiskSurvey.csv&quot;</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb50-4"><a href="interpreting-regression-results.html#cb50-4" tabindex="-1"></a>varSurvey <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;FIRMCOST&quot;</span>,<span class="st">&quot;ASSUME&quot;</span>,<span class="st">&quot;CAP&quot;</span>,<span class="st">&quot;SIZELOG&quot;</span>,<span class="st">&quot;INDCOST&quot;</span>,<span class="st">&quot;CENTRAL&quot;</span>,<span class="st">&quot;SOPH&quot;</span>)</span>
<span id="cb50-5"><a href="interpreting-regression-results.html#cb50-5" tabindex="-1"></a>survey1 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(survey[varSurvey])</span>
<span id="cb50-6"><a href="interpreting-regression-results.html#cb50-6" tabindex="-1"></a></span>
<span id="cb50-7"><a href="interpreting-regression-results.html#cb50-7" tabindex="-1"></a><span class="co">#  FIRST REGRESSION</span></span>
<span id="cb50-8"><a href="interpreting-regression-results.html#cb50-8" tabindex="-1"></a>lmsurvey1<span class="ot">&lt;-</span><span class="fu">lm</span>(FIRMCOST<span class="sc">~</span>ASSUME<span class="sc">+</span>CAP<span class="sc">+</span>SIZELOG<span class="sc">+</span>INDCOST<span class="sc">+</span>CENTRAL<span class="sc">+</span>SOPH,<span class="at">data=</span>survey1)</span>
<span id="cb50-9"><a href="interpreting-regression-results.html#cb50-9" tabindex="-1"></a>sum1 <span class="ot">&lt;-</span> <span class="fu">summary</span>(lmsurvey1)</span>
<span id="cb50-10"><a href="interpreting-regression-results.html#cb50-10" tabindex="-1"></a>tableout <span class="ot">&lt;-</span> sum1<span class="sc">$</span>coefficients[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]</span>
<span id="cb50-11"><a href="interpreting-regression-results.html#cb50-11" tabindex="-1"></a><span class="fu">colnames</span>(tableout) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Coefficient&quot;</span>, <span class="st">&quot;Standard Error&quot;</span>, <span class="st">&quot;$t$-Statistic&quot;</span>)</span>
<span id="cb50-12"><a href="interpreting-regression-results.html#cb50-12" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>tableout , </span>
<span id="cb50-13"><a href="interpreting-regression-results.html#cb50-13" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Regression Results from a Preliminary Model Fit&#39;</span>, </span>
<span id="cb50-14"><a href="interpreting-regression-results.html#cb50-14" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>,  <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,<span class="at">Digits =</span> <span class="dv">3</span>,</span>
<span id="cb50-15"><a href="interpreting-regression-results.html#cb50-15" tabindex="-1"></a>         <span class="at">ColWidth =</span> <span class="st">&quot;3cm&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="interpreting-regression-results.html#cb51-1" tabindex="-1"></a>ri1 <span class="ot">&lt;-</span> <span class="fu">rstandard</span>(lmsurvey1)</span>
<span id="cb51-2"><a href="interpreting-regression-results.html#cb51-2" tabindex="-1"></a>hii1 <span class="ot">&lt;-</span> <span class="fu">hatvalues</span>(lmsurvey1)</span>
<span id="cb51-3"><a href="interpreting-regression-results.html#cb51-3" tabindex="-1"></a></span>
<span id="cb51-4"><a href="interpreting-regression-results.html#cb51-4" tabindex="-1"></a><span class="co">#  FIGURE  6.5</span></span>
<span id="cb51-5"><a href="interpreting-regression-results.html#cb51-5" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>),<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">4.1</span>,<span class="fl">4.5</span>,.<span class="dv">2</span>,.<span class="dv">2</span>))</span>
<span id="cb51-6"><a href="interpreting-regression-results.html#cb51-6" tabindex="-1"></a><span class="fu">hist</span>(ri1, <span class="at">nclass=</span><span class="dv">16</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;Standardized Residuals&quot;</span>,<span class="at">las=</span><span class="dv">1</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb51-7"><a href="interpreting-regression-results.html#cb51-7" tabindex="-1"></a><span class="fu">hist</span>(hii1, <span class="at">nclass=</span><span class="dv">16</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;Leverages&quot;</span>,<span class="at">las=</span><span class="dv">1</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span></code></pre></div>
</div>
</div>
<div id="back-to-the-basics" class="section level4 unnumbered hasAnchor">
<h4>Back to the Basics<a href="interpreting-regression-results.html#back-to-the-basics" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To get a better understanding of the data, we begin by examining the
basic summary statistics in Table <a href="interpreting-regression-results.html#tab:Tab63">6.3</a> and
corresponding histograms in Figure <a href="interpreting-regression-results.html#fig:Fig66">6.6</a>. From
Table <a href="interpreting-regression-results.html#tab:Tab63">6.3</a>, the largest value of FIRMCOST is 97.55,
which is more than five standard deviations above the mean
<span class="math inline">\([10.97+5(16.16)=91.77]\)</span>. An examination of the data shows that this
point is observation 15, the same observation that was an outlier
in the preliminary regression fit. However, the histogram of
FIRMCOST in Figure <a href="interpreting-regression-results.html#fig:Fig66">6.6</a> reveals that this is not
the only unusual point. Two other observations have unusually large
values of FIRMCOST, resulting in a distribution that is skewed to
the right. The histogram, in Figure <a href="interpreting-regression-results.html#fig:Fig66">6.6</a>, of the
ASSUME variable shows that this distribution is also skewed to the
right, possibly due solely to two large observations. From the basic
summary statistics in Table <a href="interpreting-regression-results.html#tab:Tab63">6.3</a>, we see that the
largest value of ASSUME is more than seven standard deviations above
the mean. This observation may well turn out to be influential in
subsequent regression model fitting. The scatterplot of FIRMCOST
versus ASSUME in Figure <a href="interpreting-regression-results.html#fig:Fig66">6.6</a> tells us that the
observation with the largest value of FIRMCOST is not the same as
the observation with the largest value of ASSUME.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab63">Table 6.3: </span><strong>Summary Statistics of <span class="math inline">\(n=73\)</span> Risk Management Surveys</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Median
</th>
<th style="text-align:right;">
Standard Deviation
</th>
<th style="text-align:right;">
Minimum
</th>
<th style="text-align:right;">
Maximum
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
FIRMCOST
</td>
<td style="text-align:right;width: 1.6cm; ">
10.973
</td>
<td style="text-align:right;width: 1.6cm; ">
6.08
</td>
<td style="text-align:right;width: 1.6cm; ">
16.159
</td>
<td style="text-align:right;width: 1.6cm; ">
0.20
</td>
<td style="text-align:right;width: 1.6cm; ">
97.55
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
ASSUME
</td>
<td style="text-align:right;width: 1.6cm; ">
2.574
</td>
<td style="text-align:right;width: 1.6cm; ">
0.51
</td>
<td style="text-align:right;width: 1.6cm; ">
8.445
</td>
<td style="text-align:right;width: 1.6cm; ">
0.00
</td>
<td style="text-align:right;width: 1.6cm; ">
61.82
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
CAP
</td>
<td style="text-align:right;width: 1.6cm; ">
0.342
</td>
<td style="text-align:right;width: 1.6cm; ">
0.00
</td>
<td style="text-align:right;width: 1.6cm; ">
0.478
</td>
<td style="text-align:right;width: 1.6cm; ">
0.00
</td>
<td style="text-align:right;width: 1.6cm; ">
1.00
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SIZELOG
</td>
<td style="text-align:right;width: 1.6cm; ">
8.332
</td>
<td style="text-align:right;width: 1.6cm; ">
8.27
</td>
<td style="text-align:right;width: 1.6cm; ">
0.963
</td>
<td style="text-align:right;width: 1.6cm; ">
5.27
</td>
<td style="text-align:right;width: 1.6cm; ">
10.60
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
INDCOST
</td>
<td style="text-align:right;width: 1.6cm; ">
0.418
</td>
<td style="text-align:right;width: 1.6cm; ">
0.34
</td>
<td style="text-align:right;width: 1.6cm; ">
0.216
</td>
<td style="text-align:right;width: 1.6cm; ">
0.09
</td>
<td style="text-align:right;width: 1.6cm; ">
1.22
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
CENTRAL
</td>
<td style="text-align:right;width: 1.6cm; ">
2.247
</td>
<td style="text-align:right;width: 1.6cm; ">
2.00
</td>
<td style="text-align:right;width: 1.6cm; ">
1.256
</td>
<td style="text-align:right;width: 1.6cm; ">
1.00
</td>
<td style="text-align:right;width: 1.6cm; ">
5.00
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SOPH
</td>
<td style="text-align:right;width: 1.6cm; ">
21.192
</td>
<td style="text-align:right;width: 1.6cm; ">
23.00
</td>
<td style="text-align:right;width: 1.6cm; ">
5.304
</td>
<td style="text-align:right;width: 1.6cm; ">
5.00
</td>
<td style="text-align:right;width: 1.6cm; ">
31.00
</td>
</tr>
</tbody>
</table>
<p><em>Source</em>: Schmit and Roth, (1990)</p>
<p>From the histograms of SIZELOG, INDCOST, CENTRAL, and SOPH, we see
that these distributions are not heavily skewed. Taking logarithms
of the size of total company assets has served to make the
distribution more symmetric than in the original units. From the
histogram and summary statistics, we see that CENTRAL is a discrete
variables, taking on values one through five. The other discrete
variable is CAP, a binary variable taking values only zero and one.
The histogram and scatter plot corresponding to CAP is not presented
here. It is more informative to provide a <em>table of means</em> of
each variable by levels of CAP, as in Table <a href="interpreting-regression-results.html#tab:Tab64">6.4</a>. From
this table, we see that 25 of the 73 companies surveyed own captive
insurers. Further, on one
hand, the average FIRMCOST for those companies with captive insurers <span class="math inline">\((\)</span>CAP <span class="math inline">\(=1)\)</span> is larger than those without <span class="math inline">\((\)</span>CAP <span class="math inline">\(=0)\)</span>. On the other hand,
when moving to the logarithmic scale, the opposite is true; that is,
average COSTLOG for those companies with captive insurers <span class="math inline">\((\)</span>CAP <span class="math inline">\(=1)\)</span> is larger than those without <span class="math inline">\((\)</span>CAP <span class="math inline">\(=0)\)</span>.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab64">Table 6.4: </span><strong>Table of Means by Level of CAP</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
<span class="math inline">\(n\)</span>
</th>
<th style="text-align:right;">
FIRMCOST
</th>
<th style="text-align:right;">
ASSUME
</th>
<th style="text-align:right;">
SIZELOG
</th>
<th style="text-align:right;">
INDCOST
</th>
<th style="text-align:right;">
CENTRAL
</th>
<th style="text-align:right;">
SOPH
</th>
<th style="text-align:right;">
COSTLOG
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
CAP = 0
</td>
<td style="text-align:right;width: 1.5cm; ">
48
</td>
<td style="text-align:right;width: 1.5cm; ">
9.954
</td>
<td style="text-align:right;width: 1.5cm; ">
1.175
</td>
<td style="text-align:right;width: 1.5cm; ">
8.196
</td>
<td style="text-align:right;width: 1.5cm; ">
0.399
</td>
<td style="text-align:right;width: 1.5cm; ">
2.250
</td>
<td style="text-align:right;width: 1.5cm; ">
21.521
</td>
<td style="text-align:right;">
1.820
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
CAP = 1
</td>
<td style="text-align:right;width: 1.5cm; ">
25
</td>
<td style="text-align:right;width: 1.5cm; ">
12.931
</td>
<td style="text-align:right;width: 1.5cm; ">
5.258
</td>
<td style="text-align:right;width: 1.5cm; ">
8.592
</td>
<td style="text-align:right;width: 1.5cm; ">
0.455
</td>
<td style="text-align:right;width: 1.5cm; ">
2.240
</td>
<td style="text-align:right;width: 1.5cm; ">
20.560
</td>
<td style="text-align:right;">
1.595
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
TOTAL
</td>
<td style="text-align:right;width: 1.5cm; ">
73
</td>
<td style="text-align:right;width: 1.5cm; ">
10.973
</td>
<td style="text-align:right;width: 1.5cm; ">
2.574
</td>
<td style="text-align:right;width: 1.5cm; ">
8.332
</td>
<td style="text-align:right;width: 1.5cm; ">
0.418
</td>
<td style="text-align:right;width: 1.5cm; ">
2.247
</td>
<td style="text-align:right;width: 1.5cm; ">
21.192
</td>
<td style="text-align:right;">
1.743
</td>
</tr>
</tbody>
</table>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig66"></span>
<img src="RegressionMarkdown_files/figure-html/Fig66-1.png" alt="Histograms and scatter plots of FIRMCOST and several explanatory variables. The distributions of FIRMCOST and ASSUME are heavily skewed to the right. There is a negative relationship between FIRMCOST and SIZELOG, although nonlinear." width="80%" />
<p class="caption">
Figure 6.6: <strong>Histograms and scatter plots of FIRMCOST and several explanatory variables.</strong> The distributions of FIRMCOST and ASSUME are heavily skewed to the right. There is a negative relationship between FIRMCOST and SIZELOG, although nonlinear.
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Tab63.Hide" href="javascript:togglecode('toggleCode.Tab63.Hide','displayCode.Tab63.Hide');"><i><strong>R Code to Produce Tables 6.3 and 6.4 and Figure 6.6</strong></i></a>
</h5>
<div id="toggleCode.Tab63.Hide" style="display: none">
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="interpreting-regression-results.html#cb52-1" tabindex="-1"></a><span class="co">#  TABLE 6.3 SUMMARY STATISTICS</span></span>
<span id="cb52-2"><a href="interpreting-regression-results.html#cb52-2" tabindex="-1"></a>BookSummStats <span class="ot">&lt;-</span> <span class="cf">function</span>(Xymat){</span>
<span id="cb52-3"><a href="interpreting-regression-results.html#cb52-3" tabindex="-1"></a>meanSummary <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, mean,  <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb52-4"><a href="interpreting-regression-results.html#cb52-4" tabindex="-1"></a>sdSummary   <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, sd,    <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb52-5"><a href="interpreting-regression-results.html#cb52-5" tabindex="-1"></a>minSummary  <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, min,   <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb52-6"><a href="interpreting-regression-results.html#cb52-6" tabindex="-1"></a>maxSummary  <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, max,   <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb52-7"><a href="interpreting-regression-results.html#cb52-7" tabindex="-1"></a>medSummary  <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, median,<span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb52-8"><a href="interpreting-regression-results.html#cb52-8" tabindex="-1"></a>tableMat  <span class="ot">&lt;-</span> <span class="fu">cbind</span>(meanSummary, medSummary, sdSummary, minSummary, maxSummary)</span>
<span id="cb52-9"><a href="interpreting-regression-results.html#cb52-9" tabindex="-1"></a><span class="fu">return</span>(tableMat)</span>
<span id="cb52-10"><a href="interpreting-regression-results.html#cb52-10" tabindex="-1"></a>}</span>
<span id="cb52-11"><a href="interpreting-regression-results.html#cb52-11" tabindex="-1"></a>tableMat  <span class="ot">&lt;-</span> <span class="fu">BookSummStats</span>(survey1)</span>
<span id="cb52-12"><a href="interpreting-regression-results.html#cb52-12" tabindex="-1"></a><span class="fu">colnames</span>(tableMat)  <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Mean&quot;</span> , <span class="st">&quot;Median&quot;</span> , <span class="st">&quot;Standard Deviation&quot;</span> , </span>
<span id="cb52-13"><a href="interpreting-regression-results.html#cb52-13" tabindex="-1"></a>                         <span class="st">&quot;Minimum&quot;</span> , <span class="st">&quot;Maximum&quot;</span>)</span>
<span id="cb52-14"><a href="interpreting-regression-results.html#cb52-14" tabindex="-1"></a><span class="fu">rownames</span>(tableMat)  <span class="ot">&lt;-</span> varSurvey</span>
<span id="cb52-15"><a href="interpreting-regression-results.html#cb52-15" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>tableMat, </span>
<span id="cb52-16"><a href="interpreting-regression-results.html#cb52-16" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Summary Statistics of $n=73$ Risk Management Surveys&#39;</span>, </span>
<span id="cb52-17"><a href="interpreting-regression-results.html#cb52-17" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>, <span class="at">Digits=</span><span class="dv">3</span>, <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,</span>
<span id="cb52-18"><a href="interpreting-regression-results.html#cb52-18" tabindex="-1"></a>         <span class="at">ColWidth =</span> ColWidth5)</span></code></pre></div>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="interpreting-regression-results.html#cb53-1" tabindex="-1"></a><span class="co"># Table 6.4</span></span>
<span id="cb53-2"><a href="interpreting-regression-results.html#cb53-2" tabindex="-1"></a>varSum <span class="ot">&lt;-</span><span class="cf">function</span>(var){</span>
<span id="cb53-3"><a href="interpreting-regression-results.html#cb53-3" tabindex="-1"></a>   rows12 <span class="ot">&lt;-</span> Hmisc<span class="sc">::</span><span class="fu">summarize</span>(var, survey1<span class="sc">$</span>CAP, mean )[[<span class="dv">2</span>]]</span>
<span id="cb53-4"><a href="interpreting-regression-results.html#cb53-4" tabindex="-1"></a>   row3 <span class="ot">&lt;-</span> <span class="fu">mean</span>(var)  </span>
<span id="cb53-5"><a href="interpreting-regression-results.html#cb53-5" tabindex="-1"></a>   <span class="fu">return</span>( <span class="fu">c</span>(rows12, row3) )</span>
<span id="cb53-6"><a href="interpreting-regression-results.html#cb53-6" tabindex="-1"></a>   }</span>
<span id="cb53-7"><a href="interpreting-regression-results.html#cb53-7" tabindex="-1"></a>tableout1 <span class="ot">&lt;-</span> <span class="fu">cbind</span>(</span>
<span id="cb53-8"><a href="interpreting-regression-results.html#cb53-8" tabindex="-1"></a>  <span class="fu">varSum</span>(survey[,<span class="dv">1</span>]),  <span class="fu">varSum</span>(survey[,<span class="dv">2</span>]),</span>
<span id="cb53-9"><a href="interpreting-regression-results.html#cb53-9" tabindex="-1"></a>  <span class="fu">varSum</span>(survey[,<span class="dv">4</span>]),  <span class="fu">varSum</span>(survey[,<span class="dv">5</span>]),</span>
<span id="cb53-10"><a href="interpreting-regression-results.html#cb53-10" tabindex="-1"></a>  <span class="fu">varSum</span>(survey[,<span class="dv">6</span>]),  <span class="fu">varSum</span>(survey[,<span class="dv">7</span>]),</span>
<span id="cb53-11"><a href="interpreting-regression-results.html#cb53-11" tabindex="-1"></a>  <span class="fu">varSum</span>(<span class="fu">log</span>(survey[,<span class="dv">1</span>])) )</span>
<span id="cb53-12"><a href="interpreting-regression-results.html#cb53-12" tabindex="-1"></a>num.cap <span class="ot">&lt;-</span> <span class="fu">c</span>(Hmisc<span class="sc">::</span><span class="fu">summarize</span>(survey[,<span class="dv">1</span>], survey1<span class="sc">$</span>CAP, length )[[<span class="dv">2</span>]] ,</span>
<span id="cb53-13"><a href="interpreting-regression-results.html#cb53-13" tabindex="-1"></a>             <span class="fu">length</span>(survey[,<span class="dv">1</span>])  )</span>
<span id="cb53-14"><a href="interpreting-regression-results.html#cb53-14" tabindex="-1"></a>tableout <span class="ot">&lt;-</span> <span class="fu">cbind</span>(num.cap, tableout1)   </span>
<span id="cb53-15"><a href="interpreting-regression-results.html#cb53-15" tabindex="-1"></a><span class="fu">colnames</span>(tableout) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;$n$&quot;</span>,</span>
<span id="cb53-16"><a href="interpreting-regression-results.html#cb53-16" tabindex="-1"></a>  <span class="st">&quot;FIRMCOST&quot;</span>,<span class="st">&quot;ASSUME&quot;</span>,<span class="st">&quot;SIZELOG&quot;</span>,<span class="st">&quot;INDCOST&quot;</span>,<span class="st">&quot;CENTRAL&quot;</span>,<span class="st">&quot;SOPH&quot;</span>,<span class="st">&quot;COSTLOG&quot;</span>)</span>
<span id="cb53-17"><a href="interpreting-regression-results.html#cb53-17" tabindex="-1"></a><span class="fu">rownames</span>(tableout) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;CAP = 0&quot;</span>, <span class="st">&quot;CAP = 1&quot;</span>, <span class="st">&quot;TOTAL&quot;</span>)</span>
<span id="cb53-18"><a href="interpreting-regression-results.html#cb53-18" tabindex="-1"></a></span>
<span id="cb53-19"><a href="interpreting-regression-results.html#cb53-19" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>tableout , </span>
<span id="cb53-20"><a href="interpreting-regression-results.html#cb53-20" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Table of Means by Level of CAP&#39;</span>, </span>
<span id="cb53-21"><a href="interpreting-regression-results.html#cb53-21" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>,  <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>, <span class="at">Digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="interpreting-regression-results.html#cb54-1" tabindex="-1"></a><span class="co">#  FIGURE  6.6</span></span>
<span id="cb54-2"><a href="interpreting-regression-results.html#cb54-2" tabindex="-1"></a><span class="fu">layout</span>(<span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">11</span>,<span class="dv">12</span>),<span class="at">byrow=</span><span class="cn">TRUE</span>,<span class="at">ncol=</span><span class="dv">6</span>))</span>
<span id="cb54-3"><a href="interpreting-regression-results.html#cb54-3" tabindex="-1"></a><span class="fu">par</span>(<span class="st">&quot;oma&quot;</span><span class="ot">=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>),<span class="st">&quot;mai&quot;</span><span class="ot">=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>,<span class="dv">0</span>))</span>
<span id="cb54-4"><a href="interpreting-regression-results.html#cb54-4" tabindex="-1"></a></span>
<span id="cb54-5"><a href="interpreting-regression-results.html#cb54-5" tabindex="-1"></a><span class="fu">plot.new</span>()</span>
<span id="cb54-6"><a href="interpreting-regression-results.html#cb54-6" tabindex="-1"></a><span class="fu">hist</span>(survey1<span class="sc">$</span>ASSUME,<span class="at">breaks=</span><span class="dv">18</span>,<span class="at">main=</span><span class="st">&quot;ASSUME&quot;</span>,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb54-7"><a href="interpreting-regression-results.html#cb54-7" tabindex="-1"></a><span class="fu">hist</span>(survey1<span class="sc">$</span>SIZELOG,<span class="at">breaks=</span><span class="dv">18</span>,<span class="at">main=</span><span class="st">&quot;SIZELOG&quot;</span>,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb54-8"><a href="interpreting-regression-results.html#cb54-8" tabindex="-1"></a><span class="fu">hist</span>(survey1<span class="sc">$</span>INDCOST,<span class="at">breaks=</span><span class="dv">18</span>,<span class="at">main=</span><span class="st">&quot;INDCOST&quot;</span>,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb54-9"><a href="interpreting-regression-results.html#cb54-9" tabindex="-1"></a><span class="fu">hist</span>(survey1<span class="sc">$</span>CENTRAL,<span class="at">breaks=</span><span class="dv">18</span>,<span class="at">main=</span><span class="st">&quot;CENTRAL&quot;</span>,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb54-10"><a href="interpreting-regression-results.html#cb54-10" tabindex="-1"></a><span class="fu">hist</span>(survey1<span class="sc">$</span>SOPH,<span class="at">breaks=</span><span class="dv">18</span>,<span class="at">main=</span><span class="st">&quot;SOPH&quot;</span>,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb54-11"><a href="interpreting-regression-results.html#cb54-11" tabindex="-1"></a><span class="fu">hist</span>(survey1<span class="sc">$</span>FIRMCOST,<span class="at">breaks=</span><span class="dv">18</span>,<span class="at">main=</span><span class="st">&quot;FIRMCOST&quot;</span>,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb54-12"><a href="interpreting-regression-results.html#cb54-12" tabindex="-1"></a></span>
<span id="cb54-13"><a href="interpreting-regression-results.html#cb54-13" tabindex="-1"></a><span class="fu">plot</span>(survey1<span class="sc">$</span>ASSUME,survey1<span class="sc">$</span>FIRMCOST,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb54-14"><a href="interpreting-regression-results.html#cb54-14" tabindex="-1"></a><span class="fu">plot</span>(survey1<span class="sc">$</span>SIZELOG,survey1<span class="sc">$</span>FIRMCOST,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb54-15"><a href="interpreting-regression-results.html#cb54-15" tabindex="-1"></a><span class="fu">plot</span>(survey1<span class="sc">$</span>INDCOST,survey1<span class="sc">$</span>FIRMCOST,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb54-16"><a href="interpreting-regression-results.html#cb54-16" tabindex="-1"></a><span class="fu">plot</span>(survey1<span class="sc">$</span>CENTRAL,survey1<span class="sc">$</span>FIRMCOST,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb54-17"><a href="interpreting-regression-results.html#cb54-17" tabindex="-1"></a><span class="fu">plot</span>(survey1<span class="sc">$</span>SOPH,survey1<span class="sc">$</span>FIRMCOST,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span></code></pre></div>
</div>
<p>When examining relationships between pairs of variables, in Figure
<a href="interpreting-regression-results.html#fig:Fig66">6.6</a> we see some of the relationships that were
evident from preliminary regression fit. There is an inverse
relationship between FIRMCOST and SIZELOG, and the scatterplot plot
suggests this relationship may be nonlinear. There is also a mild
positive relationship between FIRMCOST and INDCOST and no apparent
relationships between FIRMCOST and any of the other explanatory
variables. These observations are reinforced by the table of
correlations given in Table <a href="interpreting-regression-results.html#tab:Tab64">6.4</a>. Note that the
table masks a feature that is evident in the scatter plots, the
effect of the unusually large observations.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab65">Table 6.5: </span><strong>Correlation Matrix</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
COSTLOG
</th>
<th style="text-align:right;">
FIRMCOST
</th>
<th style="text-align:right;">
ASSUME
</th>
<th style="text-align:right;">
CAP
</th>
<th style="text-align:right;">
SIZELOG
</th>
<th style="text-align:right;">
INDCOST
</th>
<th style="text-align:right;">
CENTRAL
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
FIRMCOST
</td>
<td style="text-align:right;width: 1.6cm; ">
0.713
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
ASSUME
</td>
<td style="text-align:right;width: 1.6cm; ">
0.165
</td>
<td style="text-align:right;width: 1.6cm; ">
0.039
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
CAP
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.088
</td>
<td style="text-align:right;width: 1.6cm; ">
0.088
</td>
<td style="text-align:right;width: 1.6cm; ">
0.231
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SIZELOG
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.637
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.366
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.209
</td>
<td style="text-align:right;">
0.196
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
INDCOST
</td>
<td style="text-align:right;width: 1.6cm; ">
0.395
</td>
<td style="text-align:right;width: 1.6cm; ">
0.326
</td>
<td style="text-align:right;width: 1.6cm; ">
0.249
</td>
<td style="text-align:right;">
0.122
</td>
<td style="text-align:right;">
-0.102
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
CENTRAL
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.054
</td>
<td style="text-align:right;width: 1.6cm; ">
0.014
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.068
</td>
<td style="text-align:right;">
-0.004
</td>
<td style="text-align:right;">
-0.08
</td>
<td style="text-align:right;">
-0.085
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SOPH
</td>
<td style="text-align:right;width: 1.6cm; ">
0.144
</td>
<td style="text-align:right;width: 1.6cm; ">
0.048
</td>
<td style="text-align:right;width: 1.6cm; ">
0.062
</td>
<td style="text-align:right;">
-0.087
</td>
<td style="text-align:right;">
-0.209
</td>
<td style="text-align:right;">
0.093
</td>
<td style="text-align:right;">
0.283
</td>
</tr>
</tbody>
</table>
<p>Because of the skewness of the distribution and the effect of the
unusually large observations, a transformation of the response
variable might lead to fruitful results. Figure <a href="interpreting-regression-results.html#fig:Fig67">6.7</a>
is the histogram of COSTLOG, defined to be the logarithm of
FIRMCOST. The distribution is much less skewed than the distribution
of FIRMCOST. The variable COSTLOG was also included in the
correlation matrix in Table <a href="interpreting-regression-results.html#tab:Tab64">6.4</a>. From this table,
the relationship between SIZELOG appears to be stronger with COSTLOG
than with FIRMCOST. Figure <a href="interpreting-regression-results.html#fig:Fig68">6.8</a> shows several
scatter plots illustrating the relationship between COSTLOG and the
explanatory variables. The relationship between COSTLOG and SIZELOG
appears to be linear. It is easier to interpret these scatter plots
than those in Figure <a href="interpreting-regression-results.html#fig:Fig66">6.6</a> due to the absence of
the large unusual values of the dependent variable.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig67"></span>
<img src="RegressionMarkdown_files/figure-html/Fig67-1.png" alt="Histogram of COSTLOG (the natural logarithm of FIRMCOST). The distribution of COSTLOG is less skewed than that of FIRMCOST." width="60%" />
<p class="caption">
Figure 6.7: <strong>Histogram of COSTLOG (the natural logarithm of FIRMCOST).</strong> The distribution of COSTLOG is less skewed than that of FIRMCOST.
</p>
</div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig68"></span>
<img src="RegressionMarkdown_files/figure-html/Fig68-1.png" alt="Scatter plots of COSTLOG versus several explanatory variables. There is a negative relationship between COSTLOG and SIZELOG and a mild positive relationship between COSTLOG and INDCOST." width="60%" />
<p class="caption">
Figure 6.8: <strong>Scatter plots of COSTLOG versus several explanatory variables.</strong> There is a negative relationship between COSTLOG and SIZELOG and a mild positive relationship between COSTLOG and INDCOST.
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Tab65.Hide" href="javascript:togglecode('toggleCode.Tab65.Hide','displayCode.Tab65.Hide');"><i><strong>R Code to Produce Table 6.5 and Figures 6.7 and 6.8</strong></i></a>
</h5>
<div id="toggleCode.Tab65.Hide" style="display: none">
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="interpreting-regression-results.html#cb55-1" tabindex="-1"></a><span class="co"># Table 6.5</span></span>
<span id="cb55-2"><a href="interpreting-regression-results.html#cb55-2" tabindex="-1"></a>survey2 <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">log</span>(survey1[,<span class="dv">1</span>]), survey1)</span>
<span id="cb55-3"><a href="interpreting-regression-results.html#cb55-3" tabindex="-1"></a><span class="fu">names</span>(survey2) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;COSTLOG&quot;</span>, varSurvey)</span>
<span id="cb55-4"><a href="interpreting-regression-results.html#cb55-4" tabindex="-1"></a>tableCor <span class="ot">&lt;-</span> <span class="fu">cor</span>(survey2)</span>
<span id="cb55-5"><a href="interpreting-regression-results.html#cb55-5" tabindex="-1"></a>tableCor <span class="ot">&lt;-</span> <span class="fu">round</span>(tableCor, <span class="at">digits =</span> <span class="dv">3</span>)</span>
<span id="cb55-6"><a href="interpreting-regression-results.html#cb55-6" tabindex="-1"></a>tableCor[<span class="fu">upper.tri</span>(tableCor, <span class="at">diag =</span> <span class="cn">TRUE</span>)] <span class="ot">&lt;-</span> <span class="st">&quot;&quot;</span></span>
<span id="cb55-7"><a href="interpreting-regression-results.html#cb55-7" tabindex="-1"></a>tablePrint <span class="ot">&lt;-</span> tableCor[<span class="sc">-</span><span class="dv">1</span>,]</span>
<span id="cb55-8"><a href="interpreting-regression-results.html#cb55-8" tabindex="-1"></a>tablePrint <span class="ot">&lt;-</span> tablePrint[,<span class="sc">-</span><span class="dv">8</span>]</span>
<span id="cb55-9"><a href="interpreting-regression-results.html#cb55-9" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>tablePrint, </span>
<span id="cb55-10"><a href="interpreting-regression-results.html#cb55-10" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Correlation Matrix&#39;</span>, </span>
<span id="cb55-11"><a href="interpreting-regression-results.html#cb55-11" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>, <span class="at">Digits=</span><span class="dv">3</span>, <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb55-12"><a href="interpreting-regression-results.html#cb55-12" tabindex="-1"></a>         <span class="at">ColWidth =</span> ColWidth5)</span></code></pre></div>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="interpreting-regression-results.html#cb56-1" tabindex="-1"></a><span class="co">#  FIGURE  6.7</span></span>
<span id="cb56-2"><a href="interpreting-regression-results.html#cb56-2" tabindex="-1"></a>COSTLOG <span class="ot">&lt;-</span> <span class="fu">log</span>(survey1<span class="sc">$</span>FIRMCOST)</span>
<span id="cb56-3"><a href="interpreting-regression-results.html#cb56-3" tabindex="-1"></a></span>
<span id="cb56-4"><a href="interpreting-regression-results.html#cb56-4" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="fl">4.4</span>,.<span class="dv">2</span>,.<span class="dv">2</span>))</span>
<span id="cb56-5"><a href="interpreting-regression-results.html#cb56-5" tabindex="-1"></a><span class="fu">hist</span>(COSTLOG, <span class="at">nclass=</span><span class="dv">16</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;COSTLOG&quot;</span>,<span class="at">las=</span><span class="dv">1</span>,<span class="at">cex.lab=</span><span class="dv">2</span>)</span></code></pre></div>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="interpreting-regression-results.html#cb57-1" tabindex="-1"></a><span class="co">#  FIGURE  6.8</span></span>
<span id="cb57-2"><a href="interpreting-regression-results.html#cb57-2" tabindex="-1"></a><span class="fu">layout</span>(<span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>),<span class="at">byrow=</span><span class="cn">TRUE</span>,<span class="at">ncol=</span><span class="dv">5</span>))</span>
<span id="cb57-3"><a href="interpreting-regression-results.html#cb57-3" tabindex="-1"></a><span class="fu">par</span>(<span class="st">&quot;oma&quot;</span><span class="ot">=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">3</span>),<span class="st">&quot;mai&quot;</span><span class="ot">=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.2</span>,<span class="dv">0</span>))</span>
<span id="cb57-4"><a href="interpreting-regression-results.html#cb57-4" tabindex="-1"></a><span class="fu">plot</span>(survey1<span class="sc">$</span>ASSUME,COSTLOG,<span class="at">main=</span><span class="st">&quot;ASSUME&quot;</span>,  <span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>,<span class="at">las=</span><span class="dv">1</span>)</span>
<span id="cb57-5"><a href="interpreting-regression-results.html#cb57-5" tabindex="-1"></a><span class="fu">plot</span>(survey1<span class="sc">$</span>SIZELOG,COSTLOG,<span class="at">main=</span><span class="st">&quot;SIZELOG&quot;</span>,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb57-6"><a href="interpreting-regression-results.html#cb57-6" tabindex="-1"></a><span class="fu">plot</span>(survey1<span class="sc">$</span>INDCOST,COSTLOG,<span class="at">main=</span><span class="st">&quot;INDCOST&quot;</span>,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb57-7"><a href="interpreting-regression-results.html#cb57-7" tabindex="-1"></a><span class="fu">plot</span>(survey1<span class="sc">$</span>CENTRAL,COSTLOG,<span class="at">main=</span><span class="st">&quot;CENTRAL&quot;</span>,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb57-8"><a href="interpreting-regression-results.html#cb57-8" tabindex="-1"></a><span class="fu">plot</span>(survey1<span class="sc">$</span>SOPH,COSTLOG,   <span class="at">main=</span><span class="st">&quot;SOPH&quot;</span>,   <span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span></code></pre></div>
</div>
</div>
<div id="some-new-models" class="section level4 unnumbered hasAnchor">
<h4>Some New Models<a href="interpreting-regression-results.html#some-new-models" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Now, we explore the use of COSTLOG as the dependent variable. This
line of thought is based on the work in the previous subsection and
the plots of residuals from the preliminary regression fit. As a
first step, we fit a model with all explanatory variables. Thus,
this model is the same as the preliminary regression fit except
using COSTLOG in lieu of FIRMCOST as the dependent variable. This
model serves as a useful benchmark for our subsequent work. Table
<a href="interpreting-regression-results.html#tab:Tab66">6.6</a> summarizes the fit.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab66">Table 6.6: </span><strong>Regression Results: COSTLOG as a Dependent Variable</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Coefficient
</th>
<th style="text-align:right;">
Standard Error
</th>
<th style="text-align:right;">
<span class="math inline">\(t\)</span>-Statistic
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
(Intercept)
</td>
<td style="text-align:right;width: 3cm; ">
7.643
</td>
<td style="text-align:right;width: 3cm; ">
1.155
</td>
<td style="text-align:right;width: 3cm; ">
6.617
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
ASSUME
</td>
<td style="text-align:right;width: 3cm; ">
-0.008
</td>
<td style="text-align:right;width: 3cm; ">
0.013
</td>
<td style="text-align:right;width: 3cm; ">
-0.609
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
CAP
</td>
<td style="text-align:right;width: 3cm; ">
0.015
</td>
<td style="text-align:right;width: 3cm; ">
0.233
</td>
<td style="text-align:right;width: 3cm; ">
0.064
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SIZELOG
</td>
<td style="text-align:right;width: 3cm; ">
-0.787
</td>
<td style="text-align:right;width: 3cm; ">
0.116
</td>
<td style="text-align:right;width: 3cm; ">
-6.752
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
INDCOST
</td>
<td style="text-align:right;width: 3cm; ">
1.905
</td>
<td style="text-align:right;width: 3cm; ">
0.503
</td>
<td style="text-align:right;width: 3cm; ">
3.787
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
CENTRAL
</td>
<td style="text-align:right;width: 3cm; ">
-0.080
</td>
<td style="text-align:right;width: 3cm; ">
0.087
</td>
<td style="text-align:right;width: 3cm; ">
-0.916
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SOPH
</td>
<td style="text-align:right;width: 3cm; ">
0.002
</td>
<td style="text-align:right;width: 3cm; ">
0.021
</td>
<td style="text-align:right;width: 3cm; ">
0.116
</td>
</tr>
</tbody>
</table>
<p>Here, <span class="math inline">\(R_{a}^{2}=48\%\)</span>, <span class="math inline">\(F\)</span>-ratio <span class="math inline">\(=12.1\)</span> and
<span class="math inline">\(s=0.882\)</span>. Figure <a href="interpreting-regression-results.html#fig:Fig69">6.9</a> shows that the
distribution of standardized residuals is less skewed than the
corresponding in Figure <a href="interpreting-regression-results.html#fig:Fig65">6.5</a>. The distribution of
leverages shows that there are still highly influential
observations. (As a matter of fact, the distribution of leverages
appear to be the same as in Figure <a href="interpreting-regression-results.html#fig:Fig65">6.5</a>. Why?)
Four of the six variables have <span class="math inline">\(t\)</span>-ratios less than one in
absolute value, suggesting that we continue our search for a better
model.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig69"></span>
<img src="RegressionMarkdown_files/figure-html/Fig69-1.png" alt="Histograms of standardized residuals and leverages using COSTLOG as the dependent variable." width="60%" />
<p class="caption">
Figure 6.9: <strong>Histograms of standardized residuals and leverages using COSTLOG as the dependent variable.</strong>
</p>
</div>
<p>To continue the search, a stepwise regression was run (although the
output is not reproduced here). The output from this search
technique, as well as the fitted regression model above, suggests
using the variables SIZELOG and INDCOST to explain the dependent
variable COSTLOG.</p>
<p>We can run regression using SIZELOG and INDCOST as explanatory
variables. From Figure <a href="interpreting-regression-results.html#fig:Fig610">6.10</a>, we see that the size
and shape of the distribution of standardized residuals are similar
to that in Figure <a href="interpreting-regression-results.html#fig:Fig69">6.9</a>. The leverages are much
smaller, reflecting the elimination of several explanatory variables
from the model. Remember that the average leverage is <span class="math inline">\(\bar{h} =(k+1)/n=3/73\approx 0.04\)</span>. Thus, we still have three points that exceed
three times the average and thus are considered high leverage points.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig610"></span>
<img src="RegressionMarkdown_files/figure-html/Fig610-1.png" alt="Histograms of standardized residuals and leverages using SIZELOG and INDCOST as explanatory variables." width="60%" />
<p class="caption">
Figure 6.10: <strong>Histograms of standardized residuals and leverages using SIZELOG and INDCOST as explanatory variables.</strong>
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Tab66.Hide" href="javascript:togglecode('toggleCode.Tab66.Hide','displayCode.Tab66.Hide');"><i><strong>R Code to Produce Table 6.6 and Figures 6.9 and 6.10</strong></i></a>
</h5>
<div id="toggleCode.Tab66.Hide" style="display: none">
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="interpreting-regression-results.html#cb58-1" tabindex="-1"></a><span class="co">#  SECOND REGRESSION</span></span>
<span id="cb58-2"><a href="interpreting-regression-results.html#cb58-2" tabindex="-1"></a><span class="co">#  Table 6.6</span></span>
<span id="cb58-3"><a href="interpreting-regression-results.html#cb58-3" tabindex="-1"></a>lmsurvey2<span class="ot">&lt;-</span><span class="fu">lm</span>(COSTLOG<span class="sc">~</span>ASSUME<span class="sc">+</span>CAP<span class="sc">+</span>SIZELOG<span class="sc">+</span>INDCOST<span class="sc">+</span>CENTRAL<span class="sc">+</span>SOPH,<span class="at">data=</span>survey2)</span>
<span id="cb58-4"><a href="interpreting-regression-results.html#cb58-4" tabindex="-1"></a>sum2 <span class="ot">&lt;-</span> <span class="fu">summary</span>(lmsurvey2)</span>
<span id="cb58-5"><a href="interpreting-regression-results.html#cb58-5" tabindex="-1"></a>tableout <span class="ot">&lt;-</span> sum2<span class="sc">$</span>coefficients[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]</span>
<span id="cb58-6"><a href="interpreting-regression-results.html#cb58-6" tabindex="-1"></a><span class="fu">colnames</span>(tableout) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Coefficient&quot;</span>, <span class="st">&quot;Standard Error&quot;</span>, <span class="st">&quot;$t$-Statistic&quot;</span>)</span>
<span id="cb58-7"><a href="interpreting-regression-results.html#cb58-7" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>tableout , </span>
<span id="cb58-8"><a href="interpreting-regression-results.html#cb58-8" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Regression Results: COSTLOG as a Dependent Variable&#39;</span>, </span>
<span id="cb58-9"><a href="interpreting-regression-results.html#cb58-9" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>,  <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,<span class="at">Digits =</span> <span class="dv">3</span>,</span>
<span id="cb58-10"><a href="interpreting-regression-results.html#cb58-10" tabindex="-1"></a>         <span class="at">ColWidth =</span> <span class="st">&quot;3cm&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="interpreting-regression-results.html#cb59-1" tabindex="-1"></a>ri2 <span class="ot">&lt;-</span> <span class="fu">rstandard</span>(lmsurvey2)</span>
<span id="cb59-2"><a href="interpreting-regression-results.html#cb59-2" tabindex="-1"></a>hii2 <span class="ot">&lt;-</span> <span class="fu">hatvalues</span>(lmsurvey2)</span>
<span id="cb59-3"><a href="interpreting-regression-results.html#cb59-3" tabindex="-1"></a><span class="co">#  FIGURE  6.9</span></span>
<span id="cb59-4"><a href="interpreting-regression-results.html#cb59-4" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>),<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">4.1</span>,<span class="fl">4.5</span>,.<span class="dv">2</span>,.<span class="dv">2</span>))</span>
<span id="cb59-5"><a href="interpreting-regression-results.html#cb59-5" tabindex="-1"></a><span class="fu">hist</span>(ri2, <span class="at">nclass=</span><span class="dv">16</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;Standardized Residuals&quot;</span>,<span class="at">las=</span><span class="dv">1</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb59-6"><a href="interpreting-regression-results.html#cb59-6" tabindex="-1"></a><span class="fu">hist</span>(hii2, <span class="at">nclass=</span><span class="dv">16</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;Leverages&quot;</span>,<span class="at">las=</span><span class="dv">1</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span></code></pre></div>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="interpreting-regression-results.html#cb60-1" tabindex="-1"></a>lmsurvey3<span class="ot">&lt;-</span><span class="fu">lm</span>(COSTLOG<span class="sc">~</span>SIZELOG<span class="sc">+</span>INDCOST,<span class="at">data=</span>survey2)</span>
<span id="cb60-2"><a href="interpreting-regression-results.html#cb60-2" tabindex="-1"></a>ri3 <span class="ot">&lt;-</span> <span class="fu">rstandard</span>(lmsurvey3)</span>
<span id="cb60-3"><a href="interpreting-regression-results.html#cb60-3" tabindex="-1"></a>hii3 <span class="ot">&lt;-</span> <span class="fu">hatvalues</span>(lmsurvey3)</span>
<span id="cb60-4"><a href="interpreting-regression-results.html#cb60-4" tabindex="-1"></a></span>
<span id="cb60-5"><a href="interpreting-regression-results.html#cb60-5" tabindex="-1"></a><span class="co">#  FIGURE  6.10</span></span>
<span id="cb60-6"><a href="interpreting-regression-results.html#cb60-6" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>),<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">4.1</span>,<span class="fl">4.5</span>,.<span class="dv">2</span>,.<span class="dv">2</span>))</span>
<span id="cb60-7"><a href="interpreting-regression-results.html#cb60-7" tabindex="-1"></a><span class="fu">hist</span>(ri3, <span class="at">nclass=</span><span class="dv">16</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;Standardized Residuals&quot;</span>,<span class="at">las=</span><span class="dv">1</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb60-8"><a href="interpreting-regression-results.html#cb60-8" tabindex="-1"></a><span class="fu">hist</span>(hii3, <span class="at">nclass=</span><span class="dv">16</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;Leverages&quot;</span>,<span class="at">las=</span><span class="dv">1</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span></code></pre></div>
</div>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab67">Table 6.7: </span><strong>Regression Results with a Quadratric term in INDCOST</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Coefficient
</th>
<th style="text-align:right;">
Standard Error
</th>
<th style="text-align:right;">
<span class="math inline">\(t\)</span>-Statistic
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
(Intercept)
</td>
<td style="text-align:right;width: 3cm; ">
6.353
</td>
<td style="text-align:right;width: 3cm; ">
0.953
</td>
<td style="text-align:right;width: 3cm; ">
6.666
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SIZELOG
</td>
<td style="text-align:right;width: 3cm; ">
-0.773
</td>
<td style="text-align:right;width: 3cm; ">
0.101
</td>
<td style="text-align:right;width: 3cm; ">
-7.626
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
INDCOST
</td>
<td style="text-align:right;width: 3cm; ">
6.264
</td>
<td style="text-align:right;width: 3cm; ">
1.610
</td>
<td style="text-align:right;width: 3cm; ">
3.889
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
INDCOSTSQ
</td>
<td style="text-align:right;width: 3cm; ">
-3.585
</td>
<td style="text-align:right;width: 3cm; ">
1.265
</td>
<td style="text-align:right;width: 3cm; ">
-2.833
</td>
</tr>
</tbody>
</table>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig611"></span>
<img src="RegressionMarkdown_files/figure-html/Fig611-1.png" alt="Scatter plot of residuals versus INDCOST. The smooth fitted curve (using lowess) suggests a quadratic term in INDCOST." width="60%" />
<p class="caption">
Figure 6.11: <strong>Scatter plot of residuals versus INDCOST.</strong> The smooth fitted curve (using lowess) suggests a quadratic term in INDCOST.
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Tab67.Hide" href="javascript:togglecode('toggleCode.Tab67.Hide','displayCode.Tab67.Hide');"><i><strong>R Code to Produce Table 6.7 and Figure 6.11</strong></i></a>
</h5>
<div id="toggleCode.Tab67.Hide" style="display: none">
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="interpreting-regression-results.html#cb61-1" tabindex="-1"></a><span class="co">#  THIRD REGRESSION</span></span>
<span id="cb61-2"><a href="interpreting-regression-results.html#cb61-2" tabindex="-1"></a><span class="co"># Table 6.7</span></span>
<span id="cb61-3"><a href="interpreting-regression-results.html#cb61-3" tabindex="-1"></a>survey2<span class="sc">$</span>INDCOSTSQ <span class="ot">&lt;-</span> survey2<span class="sc">$</span>INDCOST <span class="sc">*</span> survey2<span class="sc">$</span>INDCOST</span>
<span id="cb61-4"><a href="interpreting-regression-results.html#cb61-4" tabindex="-1"></a>lmsurvey4<span class="ot">&lt;-</span><span class="fu">lm</span>(COSTLOG<span class="sc">~</span> SIZELOG<span class="sc">+</span>INDCOST<span class="sc">+</span>INDCOSTSQ,<span class="at">data=</span>survey2)</span>
<span id="cb61-5"><a href="interpreting-regression-results.html#cb61-5" tabindex="-1"></a>sum4 <span class="ot">&lt;-</span> <span class="fu">summary</span>(lmsurvey4)</span>
<span id="cb61-6"><a href="interpreting-regression-results.html#cb61-6" tabindex="-1"></a>tableout <span class="ot">&lt;-</span> sum4<span class="sc">$</span>coefficients[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]</span>
<span id="cb61-7"><a href="interpreting-regression-results.html#cb61-7" tabindex="-1"></a><span class="fu">colnames</span>(tableout) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Coefficient&quot;</span>, <span class="st">&quot;Standard Error&quot;</span>, <span class="st">&quot;$t$-Statistic&quot;</span>)</span>
<span id="cb61-8"><a href="interpreting-regression-results.html#cb61-8" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>tableout , </span>
<span id="cb61-9"><a href="interpreting-regression-results.html#cb61-9" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Regression Results with a Quadratric term in INDCOST&#39;</span>, </span>
<span id="cb61-10"><a href="interpreting-regression-results.html#cb61-10" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>,  <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,<span class="at">Digits =</span> <span class="dv">3</span>,</span>
<span id="cb61-11"><a href="interpreting-regression-results.html#cb61-11" tabindex="-1"></a>         <span class="at">ColWidth =</span> <span class="st">&quot;3cm&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="interpreting-regression-results.html#cb62-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">4.1</span>,<span class="fl">2.2</span>,<span class="fl">1.7</span>,.<span class="dv">2</span>),<span class="at">cex=</span><span class="fl">1.2</span>)</span>
<span id="cb62-2"><a href="interpreting-regression-results.html#cb62-2" tabindex="-1"></a><span class="fu">plot</span>(survey2<span class="sc">$</span>INDCOST,lmsurvey4<span class="sc">$</span>residuals,<span class="at">xlab=</span><span class="st">&quot;INDCOST&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>,<span class="at">las=</span><span class="dv">1</span>)</span>
<span id="cb62-3"><a href="interpreting-regression-results.html#cb62-3" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;RESIDUAL&quot;</span>,<span class="at">side=</span><span class="dv">2</span>,<span class="at">las=</span><span class="dv">1</span>,<span class="at">at=</span><span class="fl">3.3</span>,<span class="at">cex=</span><span class="fl">1.2</span>,<span class="at">adj=</span>.<span class="dv">4</span>)</span>
<span id="cb62-4"><a href="interpreting-regression-results.html#cb62-4" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">lowess</span>(survey1<span class="sc">$</span>INDCOST,lmsurvey3<span class="sc">$</span>residuals, <span class="at">f=</span>.<span class="dv">8</span>))</span></code></pre></div>
</div>
<p>Plots of residuals versus the explanatory variables reveal some mild
patterns. The scatter plot of residuals versus INDCOST, in Figure
<a href="interpreting-regression-results.html#fig:Fig611">6.11</a>, displays a mild quadratic trend in INDCOST. To
see if this trend was important, the variable INDCOST was squared
and used as an explanatory variable in a regression model. The
results of this fit are in Table <a href="interpreting-regression-results.html#tab:Tab66">6.6</a>.</p>
<p>From the <span class="math inline">\(t\)</span>-ratio associated with (INDCOST)<span class="math inline">\(^{2}\)</span>, we see
that the variable seems to be important. The sign is reasonable,
indicating that the rate of increase of COSTLOG decreases as INDCOST
increases. That is, the expected change in COSTLOG per unit change
of INDCOST is positive and decreases as INDCOST increases.</p>
<p>Further diagnostic checks of the model revealed no additional patterns.
Thus, from the data available, we can not affirm any of the four hypotheses
that were introduced in the Introduction subsection. This is not to say that
these variables are not important. We are simply stating that the natural
variability of the data was large enough to obscure any relationships that
might exist. We have established, however, the importance of the size of the
firm and the firm’s industry risk.</p>
<p>Figure <a href="interpreting-regression-results.html#fig:Fig612">6.12</a> graphically summarizes the estimated
relationships among these variables. In particular, in lower right
hand panel, we see that for most of the firms in the sample,
FIRMCOST was relatively stable. However, for small firms, as
measured by SIZELOG, the industry risk, as measured by INDCOST, was
particularly important. For small firms, we see that the fitted
FIRMCOST increases as the variable INDCOST increases, with the rate
of increase leveling off. Although the model theoretically predicts
FIRMCOST to decrease with a large INDCOST <span class="math inline">\((&gt;1.2)\)</span>, no small firms
were actually in this area of the data region.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig612"></span>
<img src="RegressionMarkdown_files/figure-html/Fig612-1.png" alt="Graph of four fitted models versus INDCOST and SIZELOG." width="100%" />
<p class="caption">
Figure 6.12: <strong>Graph of four fitted models versus INDCOST and SIZELOG.</strong>
</p>
</div>
</div>
</div>
<div id="Sec66" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Further Reading and References<a href="interpreting-regression-results.html#Sec66" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This chapter concludes our Part I, an introduction to linear
regression. To learn more about linear regression, in Section 1.7 we
gave references to alternative statistics books that introduce the
topic. You may also be interested in a more technical presentation,
such as the classic work by Seber (1977) or a more recent work by
Abraham and Ledolter (2006). For other approaches, texts such as
Wooldridge (2009) provide an econometrics perspective where the
emphasis is on introducing regression in the context of economic
theory. Alternatively, books such as Agresti and Finlay (2008) give
an introduction from a broader social science perspective.</p>
<p>There are many explanations of regression for readers with different
perspectives and levels of quantitative training; this provides
further evidence that this is an important topic for actuaries and
other financial risk managers to understand. The other way of
getting further insights into linear regression is to see it applied
in a time series context in Part II of this book or in extensions to
nonlinear modeling in Part III.</p>
<p>See Bollen (1989) for a classic introduction to structural equations
modeling.</p>
<p><strong>Chapter References</strong></p>
<ul>
<li>Abraham, Bova and Johannes Ledolter (2006). <em>Introduction to Regression Modeling.</em> Thomson Higher Education, Belmont, CA.</li>
<li>Agresti, Alan and Barbara Finlay (2008). <em>Statistical Methods for the Social Sciences, Fourth Edition</em>. Prentice Hall, Upper Saddle, NJ.</li>
<li>Bollen, Kenneth A. (1989). <em>Structural Equations with Latent Variables</em>. New York: Wiley.</li>
<li>Box, George E. P. (1979). Robustness in the strategy of scientific model building. In R. Launer and G. Wilderson (editors), <em>Robustness in Statistics</em>, pages 201-236, Academic Press, New York.</li>
<li>Faraway, Julian J. (2005). <em>Linear Models with R</em>. Chapman &amp; Hall/CRC, Boca Raton, Florida.</li>
<li>Fienberg, S. E (1985). Insurance availability in Chicago. Chapter in <em>Data: A Collection of Problems from Many Fields for the Student and Research Worker</em>. Editors D.F. Andrews and A. M. Herzberg, Springer-Verlag, New York.</li>
<li>Goldberger, Arthur S. (1972). Structural equation methods in the social sciences. <em>Econometrica</em> 40, 979-1001.</li>
<li>Harrington, Scott E. and Greg Niehaus (1998). Race, redlining and automobile insurance prices. <em>Journal of Business</em> 71(3), 439-469.</li>
<li>Heckman, J. J. (1976). The common structure of statistical models of truncation, sample selection and limited dependent variables, and a simple estimator for such models. <em>Ann. Econ. Soc. Meas</em>. 5, 475-492.</li>
<li>Little, R. J. (1995). Modelling the drop-out mechanism in repeated-measures studies. <em>Journal of the American Statistical Association</em> 90, 1112-1121.</li>
<li>Little, R. J. and Rubin, D. B. (1987). <em>Statistical Analysis with Missing Data.</em> John Wiley, New York.</li>
<li>Roberts, Harry V. (1990). Business and economic statistics (with discussion). <em>Statistical Science</em> 4, 372-402.</li>
<li>Rubin, D. R. (1976). Inference and missing data. <em>Biometrika</em> 63, 581-592.</li>
<li>Schmit, Joan T. and K. Roth (1990). Cost effectiveness of risk management practices. <em>The Journal of Risk and Insurance</em> 57, No. 3, pages 455-470.</li>
<li>Seber, G. A. F. (1977). <em>Linear Regression Analysis.</em> John Wiley &amp; Sons, New York.</li>
<li>Wachter, K. W. and J. Trusell (1982). Estimating historical heights. <em>Journal of the American Statistical Association</em> 77, 279-301.</li>
<li>Wooldridge, Jeffrey (2009). <em>Introductory Econometrics: A Modern Approach, Fourth Edition.</em> South-Western Publishing, Mason, Ohio.</li>
</ul>
</div>
<div id="Sec67" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Exercises<a href="interpreting-regression-results.html#Sec67" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>6.1 <strong>Insurance Redlining.</strong> Do insurance companies use race as a determining factor when making
insurance available? Fienberg (1985) gathered data from a report
issued by the U.S. Commission on Civil Rights about the number of
homeowners and residential fire insurance policies issued in Chicago
over the months of December 1977 through February 1978. Policies
issued were categorized as either part of the standard voluntary
market or the substandard, involuntary market. The involuntary
market consists of “fair access to insurance requirements” (FAIR)
plans; these are state insurance programs sometimes subsidized by
private companies. These plans provide insurance to people who would
otherwise be denied insurance on their property due to high-risk
problems. The main purpose is to understand the relationship between
insurance activity and the variable “race,” the percentage
minority. Data are available for <span class="math inline">\(n=47\)</span> zip codes in the Chicago
area. These data have also been analyzed by Faraway (2005).</p>
<p>To help control for the size of the expected loss, Fienberg also
gathered theft and fire data from Chicago’s police and fire
departments. Another variable that gives some information about loss
size is the age of the house. The median income, from the Census
Bureau, gives indirect information on the size of the expected loss
as well as whether the applicant can afford insurance. Table
<a href="interpreting-regression-results.html#tab:Tab68">6.8</a> provides more details on these variables.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab68">Table 6.8: </span><strong>Insurance Availability in Chicago</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:left;">
Description
</th>
<th style="text-align:center;">
Mean
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
row.names
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
Zip (postal) code
</td>
<td style="text-align:center;width: 2cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
race
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
Racial composition in percent minority
</td>
<td style="text-align:center;width: 2cm; ">
35
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
fire
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
Fires per 1,000 housing units
</td>
<td style="text-align:center;width: 2cm; ">
12.3
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
theft
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
Thefts per 1,000 population
</td>
<td style="text-align:center;width: 2cm; ">
32.4
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
age
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
Percent of housing units built in or before 1939
</td>
<td style="text-align:center;width: 2cm; ">
60.3
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
volact
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
New homeowner policies plus renewals, minus cancelations and non-renewals per 100 housing units
</td>
<td style="text-align:center;width: 2cm; ">
6.53
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
involact
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
New FAIR plan policies and renewals per 100 housing units
</td>
<td style="text-align:center;width: 2cm; ">
0.615
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
income
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
Median family income
</td>
<td style="text-align:center;width: 2cm; ">
10696
</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li>Produce summary statistics of all variables, noting patterns of
skewness for each variable.</li>
<li>Create a scatter plot matrix of volact, involact and race.
Comment on the three pairwise relationships. Are the patterns
consistent with a hypothesis of racial discrimination?</li>
<li>To understand relationships among the variables, produce a table
of correlations.</li>
<li>Fit a linear model using volact as the dependent variable and
race, fire theft, age and income as explanatory variables. <br>
d(i). Comment on the sign and statistical significance of the
coefficient associated with race. <br>
d(ii). Two zip codes turn out to have high leverage. Repeat your
analysis after deleting these two observations. Has the significance
of the race variable changed? What about the other explanatory
variables? <br></li>
<li>Repeat the analysis in part (d) using involact as the dependent
variable.</li>
<li>Define proportion to be involact/(volact+involact). Repeat the
analysis in part (d) using proportion as the dependent variable.</li>
<li>The same two zip codes have high leverage in parts (d), (e) and
(f). Why is this so?</li>
<li>This analysis is done at the zip code level, not the individual
level. As emphasized by Harrington and Niehaus (1998), this
introduces substantial potential omitted variable bias. What
variables have been omitted from the analysis that you think might
affect homeowners insurance availability and race?</li>
<li>Fienberg notes that proximity of one zip code to another may
affect the dependence of observations. Describe how you might
incorporate spatial relations into a regression analysis.</li>
</ol>
<p>6.2 <strong>Gender Equity in Faculty Pay.</strong> The University of Wisconsin at Madison completed a study entitled “Gender Equity Study of Faculty Pay,” dated June 5, 1992. The main
purpose of the study was to determine whether women are treated
unfairly in salary determinations at a major research university in
the U.S. To this end, the committee that issued the report studied
1990 salaries of 1,898 faculty members in the university. It is
well-known that men are paid more than women. In fact, the mean 1990
salary for the 1,528 male faculty members is 54,478, which is 28%
higher than the mean 1990 salary for female faculty members, which
is 43,315. However, it is argued that male faculty members are in
general more senior (average years of experience is 18.8 years) than
female faculty members (average years of experience is 11.9 years),
and thus deserved higher pay. When comparing salaries of full
professors (thus controlling for years of experience), male faculty
members earned about 13% more than their female counterparts. Even
so, it is generally agreed that fields in demand must offer higher
salaries in order to maintain a world-class faculty. For example,
salaries in engineering are higher than salaries in humanities
simply because faculty in engineering have many more employment
opportunities outside of academia than faculty in humanities. Thus,
when considering salaries, one must also control for department.</p>
<p>To control for these variables, a faculty study reports a regression
analysis using the logarithm of salary as the dependent variable.
The explanatory variables included information on race, gender, rank
(either assistant professor/instructor, associate professor or full
professor), several measures of years of experience, 98 different
categories of departments and a measure of salary differential by
department. There were 109 explanatory variables in all (including
97 departmental binary variables), of which 12 were non-departmental
variables. Table <a href="interpreting-regression-results.html#tab:Tab69">6.9</a> reports variable definitions,
parameter estimates and <span class="math inline">\(t\)</span>-ratios for the 12 non-departmental
variables. The ANOVA Table <a href="interpreting-regression-results.html#tab:Tab610">6.10</a> summarizes the
regression fit.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab69">Table 6.9: </span><strong>Non-Departmental Variables and Parameter Estimates</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Explanatory Variable
</th>
<th style="text-align:left;">
Variable Description
</th>
<th style="text-align:center;">
Parameter Estimate
</th>
<th style="text-align:right;">
<span class="math inline">\(t\)</span>-Ratio
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
INTERCEPT
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
</td>
<td style="text-align:center;width: 2cm; ">
10.746
</td>
<td style="text-align:right;">
261.1
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
GENDER
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
= 1 if male, 0 otherwise
</td>
<td style="text-align:center;width: 2cm; ">
0.016
</td>
<td style="text-align:right;">
1.86
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
RACE
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
= 1 if white, 0 otherwise
</td>
<td style="text-align:center;width: 2cm; ">
-0.029
</td>
<td style="text-align:right;">
-2.44
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
FULL
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
= 1 if a full professor, 0 otherwise
</td>
<td style="text-align:center;width: 2cm; ">
0.186
</td>
<td style="text-align:right;">
16.42
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
ASSISTANT
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
= 1 if an assistant professor, 0 otherwise
</td>
<td style="text-align:center;width: 2cm; ">
-0.205
</td>
<td style="text-align:right;">
-15.93
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
ANYDOC
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
= 1 if has a terminal degree such as a Ph.D. 
</td>
<td style="text-align:center;width: 2cm; ">
0.022
</td>
<td style="text-align:right;">
1.11
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
COHORT1
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
= 1 if hired before 1969, 0 otherwise
</td>
<td style="text-align:center;width: 2cm; ">
-0.102
</td>
<td style="text-align:right;">
-4.84
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
COHORT2
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
= 1 if hired 1969-1985, 0 otherwise
</td>
<td style="text-align:center;width: 2cm; ">
-0.046
</td>
<td style="text-align:right;">
-3.48
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
FULLYEARS
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
Number of years as a full professor at UW
</td>
<td style="text-align:center;width: 2cm; ">
0.012
</td>
<td style="text-align:right;">
12.84
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
ASSOCYEARS
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
Number of years as an associate professor at UW
</td>
<td style="text-align:center;width: 2cm; ">
-0.012
</td>
<td style="text-align:right;">
-8.65
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
ASSISYEARS
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
Number of years as an assistant professor or an instructor at UW
</td>
<td style="text-align:center;width: 2cm; ">
0.002
</td>
<td style="text-align:right;">
0.91
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
DIFYRS
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
Number of years since receiving a terminal degree before arriving to UW
</td>
<td style="text-align:center;width: 2cm; ">
0.004
</td>
<td style="text-align:right;">
4.46
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
MRKTRATIO
</td>
<td style="text-align:left;width: 2cm; width: 9cm; border-right:1px solid;">
Natural logarithm of a ‘market ratio’ defined as the ratio of the average salary at peer institutions for a given discipline and rank
</td>
<td style="text-align:center;width: 2cm; ">
0.665
</td>
<td style="text-align:right;">
7.64
</td>
</tr>
</tbody>
</table>
<p><em>Source</em>: “Gender Equity Study of Faculty Pay,” June 5, 1992, The University of Wisconsin at Madison.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:Tab610">Table 6.10: </span><strong>Faculty Pay ANOVA Table</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Source
</th>
<th style="text-align:right;">
Sum of Squares
</th>
<th style="text-align:right;">
<span class="math inline">\(df\)</span>
</th>
<th style="text-align:right;">
Mean Square
</th>
<th style="text-align:right;">
<span class="math inline">\(F\)</span>-Ratio
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
Regression
</td>
<td style="text-align:right;width: 3cm; ">
114.048
</td>
<td style="text-align:right;width: 3cm; ">
109
</td>
<td style="text-align:right;width: 3cm; ">
1.0463
</td>
<td style="text-align:right;width: 3cm; ">
62.943
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
Error
</td>
<td style="text-align:right;width: 3cm; ">
29.739
</td>
<td style="text-align:right;width: 3cm; ">
1789
</td>
<td style="text-align:right;width: 3cm; ">
0.0166
</td>
<td style="text-align:right;width: 3cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;border-right:1px solid;">
Total
</td>
<td style="text-align:right;width: 3cm; ">
143.788
</td>
<td style="text-align:right;width: 3cm; ">
1898
</td>
<td style="text-align:right;width: 3cm; ">
</td>
<td style="text-align:right;width: 3cm; ">
</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li>Suppose that a female faculty member in the chemistry department
feels that her salary is below what it should be. Briefly describe
how this study can be used as a basis for performance evaluation.</li>
<li>Based on this study, do you think that salaries of women are
significantly lower than men? <br>
b(i). Cite statistical arguments supporting the fact that men are
not paid significantly more than women. <br>
b(ii). Cite statistical arguments supporting the fact that men are
paid significantly more than women. <br>
b(iii). Suppose that you decide that women are paid less than men.
Based on this study, how much would you raise female faculty members
salaries to be on par with their male counterparts?</li>
</ol>
</div>
<div id="Sec68" class="section level2 hasAnchor" number="6.8">
<h2><span class="header-section-number">6.8</span> Technical Supplements for Chapter 6<a href="interpreting-regression-results.html#Sec68" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="Sec681" class="section level3 hasAnchor" number="6.8.1">
<h3><span class="header-section-number">6.8.1</span> Effects of Model Misspecification<a href="interpreting-regression-results.html#Sec681" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Notation.</strong> Partition the matrix of explanatory variables <span class="math inline">\(\mathbf{X}\)</span> into two submatrices, each having <span class="math inline">\(n\)</span> rows, so that <span class="math inline">\(\mathbf{X}=(\mathbf{X}_{1} : \mathbf{X}_{2})\)</span>. For convenience, assume that <span class="math inline">\(\mathbf{X}_{1}\)</span> is an <span class="math inline">\(n \times p\)</span> matrix. Similarly, partition the vector of parameters <span class="math inline">\(\boldsymbol \beta =\left( \boldsymbol \beta_{1}^{\prime }, \boldsymbol \beta_{2}^{\prime }\right) ^{\prime }\)</span> such that <span class="math inline">\(\mathbf{X \boldsymbol \beta }=\mathbf{X}_{1} \boldsymbol \beta_{1}+ \mathbf{X}_{2} \boldsymbol \beta_{2}\)</span>. We compare the full, or “long,” model</p>
<p><span class="math display">\[
\mathbf{y}=\mathbf{X \boldsymbol \beta }+\boldsymbol \varepsilon = \mathbf{X}_{1} \boldsymbol \beta_{1}+\mathbf{X}_{2} \boldsymbol \beta_{2}+\boldsymbol \varepsilon
\]</span></p>
<p>to the reduced, or “short,” model</p>
<p><span class="math display">\[
\mathbf{y}=\mathbf{X}_{1} \boldsymbol \beta_{1}+\boldsymbol \varepsilon.
\]</span></p>
<p>This simply generalizes the set-up earlier to allow for omitting several variables.</p>
<p><strong>Effect of Underfitting.</strong> Suppose that the true representation is the long model but we mistakenly run the short model. Our parameter estimates when running the short model are given by <span class="math inline">\(\mathbf{b}_{1}=\mathbf{(X}_{1}^{\prime }\mathbf{X}_{1}\mathbf{)}^{-1}\mathbf{X}_{1}^{\prime }\mathbf{y}\)</span>. These estimates are biased because</p>
<p><span class="math display">\[
\begin{array}{ll}
\text{Bias} &amp;= \text{E }\mathbf{b}_{1}-\boldsymbol \beta_{1}
= \text{E}\mathbf{(X}_{1}^{\prime}\mathbf{X}_{1}\mathbf{)}^{-1}\mathbf{X}_{1}^{\prime }\mathbf{y} -\boldsymbol \beta_{1} \\
&amp;= \mathbf{(X}_{1}^{\prime }\mathbf{X}_{1}\mathbf{)}^{-1}\mathbf{X}_{1}^{\prime }\text{E }\mathbf{y}-\boldsymbol \beta_{1} \\
&amp;= \mathbf{(X}_{1}^{\prime }\mathbf{X}_{1}\mathbf{)}^{-1}\mathbf{X}_{1}^{\prime }\left( \mathbf{X}_{1}\boldsymbol \beta_{1}+\mathbf{X}_{2}\boldsymbol \beta_{2}\right) - \boldsymbol \beta_{1} \\
&amp;= \mathbf{(X}_{1}^{\prime}\mathbf{X}_{1}\mathbf{)}^{-1}\mathbf{X}_{1}^{\prime }\mathbf{X}_{2}\boldsymbol \beta_{2}
= \mathbf{A \boldsymbol \beta }_{2}.
\end{array}
\]</span>
Here, <span class="math inline">\(\mathbf{A}=\mathbf{(X}_{1}^{\prime}\mathbf{X}_{1}\mathbf{)}^{-1}\mathbf{X}_{1}^{\prime }\mathbf{X}_{2}\)</span> is called the <em>alias</em>, or bias, matrix. When running the short model, the estimated variance is <span class="math inline">\(s_{1}^{2}=(\mathbf{y}^{\prime }\mathbf{y}-\mathbf{b}_{1}^{\prime }\mathbf{X}_{1}^{\prime }\mathbf{y})/(n-p)\)</span>. It can be shown that</p>
<p><span class="math display" id="eq:eq63">\[\begin{equation}
\text{E }s_{1}^{2}=\sigma ^{2}+(n-p)^{-1}\boldsymbol \beta_{2}^{\prime }\left( \mathbf{X}_{2}^{\prime }\mathbf{X}_{2}-\mathbf{X}_{2}^{\prime }\mathbf{X}_{1}\mathbf{(X}_{1}^{\prime }\mathbf{X}_{1}\mathbf{)}^{-1}\mathbf{X}_{1}^{\prime }\mathbf{X}_{2}\right) \boldsymbol \beta_{2}.
\tag{6.3}
\end{equation}\]</span></p>
<p>Thus, <span class="math inline">\(s_{1}^{2}\)</span> is an “overbiased” estimate of <span class="math inline">\(\sigma ^{2}\)</span>.</p>
<p>Let <span class="math inline">\(\mathbf{x}_{1i}^{\prime }\)</span> and <span class="math inline">\(\mathbf{x}_{2i}^{\prime }\)</span> be the <span class="math inline">\(i\)</span>th rows of <span class="math inline">\(\mathbf{X}_{1}\)</span> and <span class="math inline">\(\mathbf{X}_{2}\)</span>, respectively. Using the fitted short model, the <span class="math inline">\(i\)</span>th fitted value is <span class="math inline">\(\hat{y}_{1i}=\mathbf{x}_{1i}^{\prime }\mathbf{b}_{1}\)</span>. The true <span class="math inline">\(i\)</span>th expected response is E <span class="math inline">\(\hat{y}_{1i}=\mathbf{x}_{1i}^{\prime } \boldsymbol \beta_{1} + \mathbf{x}_{2i}^{\prime } \boldsymbol \beta_{2}\)</span>. Thus, the bias of the <span class="math inline">\(i\)</span>th fitted value is</p>
<p><span class="math display">\[
\begin{array}{ll}
\text{Bias}(\hat{y}_{1i}) &amp;= \text{E }\hat{y}_{1i}-\text{E }y_{i}=\mathbf{x}_{1i}^{\prime }\text{E }\mathbf{b}_{1}-\left( \mathbf{x}_{1i}^{\prime }\boldsymbol \beta_{1}+\mathbf{x}_{2i}^{\prime } \boldsymbol \beta_{2}\right) \\
&amp; =\mathbf{x}_{1i}^{\prime }(\boldsymbol \beta_{1}+\mathbf{A \boldsymbol \beta }_{2})-\left( \mathbf{x}_{1i}^{\prime }\boldsymbol \beta_{1}+\mathbf{x}_{2i}^{\prime }\boldsymbol \beta_{2}\right) =(\mathbf{x}_{1i}^{\prime }\mathbf{A}-\mathbf{x}_{2i}^{\prime })\boldsymbol \beta_{2}.
\end{array}
\]</span></p>
<p>Using this and equation <a href="interpreting-regression-results.html#eq:eq63">(6.3)</a>, straightforward algebra shows that</p>
<p><span class="math display" id="eq:eq64">\[\begin{equation}
\text{E }s_{1}^{2}=\sigma ^{2}+(n-p)^{-1}\sum_{i=1}^{n}(\text{Bias}(\hat{y}_{1i}))^{2}.
\tag{6.4}
\end{equation}\]</span></p>
<p><strong>Effect of Overfitting.</strong> Now suppose that the true representation is the short model but we mistakenly use the large model. With the alias matrix <span class="math inline">\(\mathbf{A}=\mathbf{(X}_{1}^{\prime }\mathbf{X}_{1}\mathbf{)}^{-1}\mathbf{X}_{1}^{\prime }\mathbf{X}_{2}\)</span>, we can <em>reparameterize</em> the long model</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{y} &amp; =\mathbf{X}_{1}\boldsymbol \beta_{1}+\mathbf{X}_{2}\boldsymbol \beta_{2} + \boldsymbol \varepsilon = \mathbf{X}_{1}\left( \boldsymbol \beta_{1}+\mathbf{A \boldsymbol \beta }_{2}\right) + \mathbf{E}_{1}\boldsymbol \beta_{2} + \boldsymbol \varepsilon \\
&amp;= \mathbf{X}_{1}\boldsymbol \alpha_{1}+\mathbf{E}_{1}\boldsymbol \beta_{2} + \boldsymbol \varepsilon
\end{array}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{E}_{1}=\mathbf{X}_{2}-\mathbf{X}_{1}\mathbf{A}\)</span> and <span class="math inline">\(\boldsymbol \alpha_{1}=\boldsymbol \beta_{1}+\mathbf{A \boldsymbol \beta}_{2}\)</span>. The advantage of this new parameterization is that <span class="math inline">\(\mathbf{X}_{1}\)</span> is orthogonal to <span class="math inline">\(\mathbf{E}_{1}\)</span> because <span class="math inline">\(\mathbf{X}_{1}^{\prime }\mathbf{E}_{1}=\mathbf{X}_{1}^{\prime }(\mathbf{X}_{2}-\mathbf{X}_{1}\mathbf{A})=\mathbf{0}\)</span>. With <span class="math inline">\(\mathbf{X}^{\ast }=(\mathbf{X}_{1}: \mathbf{E}_{1})\)</span> and <span class="math inline">\(\boldsymbol \alpha =(\boldsymbol \alpha_{1}^{\prime }\boldsymbol \beta_{1}^{\prime })^{\prime }\)</span>, the vector of least squares estimates is</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{a} &amp;=
\begin{bmatrix}
\mathbf{a}_{1} \\
\mathbf{b}_{1}
\end{bmatrix} = \left( \mathbf{X}^{\ast \prime }\mathbf{X}^{\ast }\right) ^{-1}\mathbf{X}^{\ast \prime }\mathbf{y} \\
&amp; = \begin{bmatrix}
\mathbf{(X}_{1}^{\prime }\mathbf{X}_{1}\mathbf{)}^{-1} &amp; 0 \\
0 &amp; \mathbf{(E}_{1}^{\prime }\mathbf{E}_{1}\mathbf{)}^{-1}
\end{bmatrix}
\begin{bmatrix}
\mathbf{X}_{1}^{\prime }\mathbf{y} \\
\mathbf{E}_{1}^{\prime }\mathbf{y}
\end{bmatrix} \\
&amp; = \begin{bmatrix}
\mathbf{(X}_{1}^{\prime }\mathbf{X}_{1}\mathbf{)}^{-1}\mathbf{X}_{1}^{\prime }\mathbf{y} \\
\mathbf{(E}_{1}^{\prime }\mathbf{E}_{1}\mathbf{)}^{-1}\mathbf{E}_{1}^{\prime }\mathbf{y}
\end{bmatrix}.
\end{array}
\]</span></p>
<p>From the true (short) model, <span class="math inline">\(\mathrm{E}~\mathbf{y}=\mathbf{X}_{1}\boldsymbol \beta_{1}\)</span>, we have that</p>
<p><span class="math display">\[
\mathrm{E}~\mathbf{b}_{2}
=(\mathbf{E}_{1}^{\prime }\mathbf{E}_{1})^{-1}\mathbf{E}_{1}^{\prime }\mathrm{E}(\mathbf{y})
=(\mathbf{E}_{1}^{\prime }\mathbf{E}_{1})^{-1}\mathbf{E}_{1}^{\prime }\mathrm{E}~ (\mathbf{X}_{1}\mathbf{\beta}_{1})
=\mathbf{0},
\]</span></p>
<p>because <span class="math inline">\(\mathbf{X}_{1}^{\prime }\mathbf{E}_{1}=\mathbf{0}\)</span>. The least squares estimate of <span class="math inline">\(\boldsymbol \beta_{1}\)</span> is <span class="math inline">\(\mathbf{b}_{1}=\mathbf{a}_{1}-\mathbf{Ab}_{2}\)</span>. Because</p>
<p><span class="math display">\[
\mathrm{E}~\mathbf{a}_{1}=\mathbf{(X}_{1}^{\prime }\mathbf{X}_{1}\mathbf{)}^{-1}\mathbf{X}_{1}^{\prime }\mathrm{E}~\mathbf{y}=\boldsymbol \beta_{1}
\]</span>
under the short model, we have E <span class="math inline">\(\mathbf{b}_{1}=\)</span> E <span class="math inline">\(\mathbf{a}_{1}-\mathbf{A}\)</span>E <span class="math inline">\(\mathbf{b}_{2}=\boldsymbol \beta_{1}-\mathbf{0}=\boldsymbol \beta_{1}\)</span>. Thus, even though we mistakenly run the long model, <span class="math inline">\(\mathbf{b}_{1}\)</span> is still an unbiased estimator of <span class="math inline">\(\boldsymbol \beta_{1}\)</span> and <span class="math inline">\(\mathbf{b}_{2}\)</span> is an unbiased estimator of <span class="math inline">\(\mathbf{0}\)</span>. Thus, there is no bias in the <span class="math inline">\(i\)</span>th fitted value because</p>
<p><span class="math display">\[
\mathrm{E}~\hat{y}_{i}= \mathrm{E}~(\mathbf{x}_{1i}^{\prime }\mathbf{b}_{1}+\mathbf{x}_{2i}^{\prime }\mathbf{b}_{2})=\mathbf{x}_{1i}^{\prime }\boldsymbol \beta_{1}= \mathrm{E}~y_{i} .
\]</span></p>
<p><strong><span class="math inline">\(C_{p}\)</span> Statistic.</strong> Suppose initially that the true representation is the long model but we mistakenly use the short model. The <span class="math inline">\(i\)</span>th fitted value is <span class="math inline">\(\hat{y}_{1i}=\mathbf{x}_{1i}^{\prime }\mathbf{b}_{1}\)</span> that has mean square error
<span class="math display">\[
\text{MSE }\hat{y}_{1i} = \text{E}(\hat{y}_{1i} - \text{E }\hat{y}_{1i})^{2} = \text{Var }\hat{y}_{1i} + \left( \text{Bias }\hat{y}_{1i} \right)^{2}.
\]</span>
For the first part, we have that</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathrm{Var}~\hat{y}_{1i}&amp;
=\mathrm{Var}\left( \mathbf{x}_{1i}^{\prime }\mathbf{b}_{1}\right)
= \text{Var} \left( \mathbf{x}_{1i}^{\prime } \mathbf{(X}_{1}^{\prime }\mathbf{X}_{1}\mathbf{)}^{-1}\mathbf{X}_{1}^{\prime }\mathbf{y}\right) \\
&amp; = \sigma^{2} \mathbf{x}_{1i} \mathbf{(X}_{1}^{\prime }\mathbf{X}_{1}\mathbf{)}^{-1} \mathbf{x}_{1i}^{\prime }.
\end{array}
\]</span>
We can think of <span class="math inline">\(\mathbf{x}_{1i} \mathbf{(X}_{1}^{\prime }\mathbf{X}_{1}\mathbf{)}^{-1} \mathbf{x}_{1i}^{\prime }\)</span> as the <span class="math inline">\(i\)</span>th leverage, as in equation (5.3). Thus, <span class="math inline">\(\sum_{i=1}^{n} \mathbf{x}_{1i} \mathbf{(X}_{1}^{\prime }\mathbf{X}_{1}\mathbf{)}^{-1} \mathbf{x}_{1i}^{\prime } = p\)</span>, the number of columns of <span class="math inline">\(\mathbf{X}_{1}\)</span>. With this, we can define the <em>standardized total error</em></p>
<p><span class="math display">\[
\begin{array}{ll}
\frac{\sum_{i=1}^{n} \text{MSE }\hat{y}_{1i}}{\sigma^{2}}
&amp; = \frac{\sum_{i=1}^{n} \left( \text{Var }\hat{y}_{1i} + \left( \text{Bias }\hat{y}_{1i} \right)^{2} \right)}{\sigma^{2}} \\
&amp; = \frac{\sigma^{2} \sum_{i=1}^{n} \left( \mathbf{x}_{1i} \mathbf{(X}_{1}^{\prime }\mathbf{X}_{1}\mathbf{)}^{-1} \mathbf{x}_{1i}^{\prime } + \left( \text{Bias }\hat{y}_{1i} \right)^{2} \right)}{\sigma^{2}} \\
&amp;= p + \sigma^{-2} \sum_{i=1}^{n} \left( \text{Bias }\hat{y}_{1i} \right)^{2}.
\end{array}
\]</span></p>
<p>Now, if <span class="math inline">\(\sigma^{2}\)</span> is known, from equation <a href="interpreting-regression-results.html#eq:eq64">(6.4)</a>, an unbiased estimate of the standardized total error is <span class="math inline">\(p + \frac{(n-p)(s_{1}^{2} - \sigma^{2})}{\sigma^{2}}.\)</span> Because <span class="math inline">\(\sigma^{2}\)</span> is unknown, it must be estimated. If we are not sure whether the long or short model is the appropriate representation, a conservative choice is to use <span class="math inline">\(s^{2}\)</span> from the long, or full, model. Even if the short model is the true model, <span class="math inline">\(s^{2}\)</span> from the long model is still an unbiased estimate of <span class="math inline">\(\sigma^{2}\)</span>. Thus, we define</p>
<p><span class="math display">\[
C_{p} = p + \frac{(n-p)(s_{1}^{2} - s^{2})}{s^{2}}.
\]</span></p>
<p>If the short model is correct, then <span class="math inline">\(\mathrm{E}~s_{1}^{2}=\)</span> <span class="math inline">\(\mathrm{E}~s^{2}=\sigma^{2}\)</span> and <span class="math inline">\(\mathrm{E}~C_{p} \approx p\)</span>. If the long model is true, then <span class="math inline">\(\mathrm{E}~s_{1}^{2} &gt; \sigma^{2}\)</span> and <span class="math inline">\(\mathrm{E}~C_{p} &gt; p\)</span>.</p>

<!-- # Chap 1 -->
<!-- # Chap 2 -->
<!-- # Chap 3 -->
<!-- # Chap 4 -->
<!-- # Chap 5 -->
<!-- # Chap 6 -->
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="C5VarSelect.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="C7Trends.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
