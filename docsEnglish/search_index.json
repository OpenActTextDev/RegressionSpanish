[["preface.html", "Regression Modeling with Actuarial and Financial Applications Preface Dedication Forward", " Regression Modeling with Actuarial and Financial Applications Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University Preface Dedication There is an old saying, attributed to Sir Issac Newton and that can be found on web at Google Scholar, If I have seen far, it is by standing on the shoulders of giants. I dedicate this book to the memory of two giants who helped me, and everyone who knew them, see farther and live better lives: James C. Hickman and Joseph P. Sullivan. Forward Actuaries and other financial analysts quantify situations using data – we are ‘numbers’ people. Many of our approaches and models are stylized, based on years of experience and investigations performed by legions of analysts. However, the financial and risk management world evolves rapidly. Many analysts are confronted with new situations in which tried-and-true methods simply do not work. This is where a toolkit like regression analysis comes in. Regression is the study of relationships among variables. It is a generic statistics discipline that is not restricted to the financial world – it has applications in fields of social, biological and physical sciences. You can use regression techniques to investigate large and complex data sets. To familiarize you with regression, this book explores many examples and data sets based on actuarial and financial applications. This is not to say that you will not encounter applications outside of the financial world (for example, an actuary may need to understand the latest scientific evidence on genetic testing for underwriting purposes). However, as you become acquainted with this toolkit, you will see how regression can be applied in many (and sometimes new) situations. Who Is This Book For? This book is written for financial analysts who face uncertain events and wish to quantify these events using empirical information. No industry knowledge is assumed although readers will find the reading much easier if they have an interest in the applications discussed here! This book is designed for students who are just being introduced to the field as well as industry analysts who would like to brush up on old techniques and (for the later chapters) get an introduction to new developments. To read this book, I assume knowledge comparable to a one semester introduction to probability and statistics - Appendix A1 provides a brief review to brush up if you are rusty. Actuarial students in North America will have a one-year introduction to probability and statistics - this type of introduction will help readers grasp concepts more quickly than a one semester background. Finally, readers will find matrix, or linear, algebra helpful, although not a prerequisite for reading this text. Different readers are interested in understanding statistics at different levels. This book is written to accommodate the ‘armchair reader,’ that is, one who passively reads and does not get involved by attempting the exercises in the text. Consider an analogy to football, or any other game. Just like the armchair quarterback of football, there is a great deal that you can learn about the game just by watching. However, if you want to sharpen your skills, you have to go out and play the game. If you do the exercises or reproduce the statistical analyses in the text, you will become a better player. Still, this text is written by interweaving examples with the basic principles. Thus, even the armchair reader can obtain a solid understanding of regression techniques through this text. What Is This Book About? The Table of Contents provides an overview of the topics covered, organized into four parts. The first part introduces linear regression. This is the core material of the book, with refreshers on mathematical statistics, distributions and matrix algebra woven in as needed. The second part is devoted to topics in times series. Why integrate time series topics into a regression book? The reasons are simple, yet compelling; most accounting, financial and economic data become available over time. Although cross-sectional inferences are useful, business decisions need to be made in real time with currently available data. Chapters 7-10 introduce time series techniques that can be readily accomplished using regression tools (and there are many). Nonlinear regression is the subject of the third part. Many modern day ‘predictive modeling’ tools are based on nonlinear regression - these are the workhorses of statistical shops in the financial and risk management industry. The fourth part concerns ‘actuarial applications,’ topics that I have found relevant in my research and consulting work in financial risk management. The first four chapters of this part consists of variations of regression models that are particularly useful in risk management. The last two chapters focus on communications, specifically, report writing and designing graphs. Communicating information is an important aspect of every technical discipline and statistics is certainly no exception. How Does This Book Deliver Its Message? Chapter Development. Each chapter has several examples interwoven with theory. In chapters where a model is introduced, I begin with an example and discuss the data analysis without regard to the theory. This analysis is presented at an intuitive level, without reference to a specific model. This is straightforward, because it amounts to little more than curve fitting. The theme is to have students summarize data sensibly without having the notion of a model obscure good data analysis. Then, an introduction to the theory is provided in the context of the introductory example. One or more additional examples follow that reinforce the theory already introduced and provide a context for explaining additional theory. In Chapters 5 and 6, that do not introduce models but rather techniques for analysis, I begin with an introduction of the technique. This introduction is then followed by an example that reinforces the explanation. In this way, the data analysis can be easily omitted without loss of continuity, if time is a concern. Real Data. Many of the exercises ask the reader to work with real data. The need for working with real data is well documented; for example, see Hogg (1972) or Singer and Willett (1990). Some criteria of Singer and Willett for judging a good data set include: (1) authenticity, (2) availability of background information, (3) interest and relevance to substantive learning, and (4) availability of elements with which readers can identify. Of course, there are some important disadvantages to working with real data. Data sets can quickly become outdated. Further, the ideal data set to illustrate a specific statistical issue is difficult to find. This is because with real data, almost by definition, several issues occur simultaneously. This makes it difficult to isolate a specific aspect. I particularly enjoy working with large data sets. The larger the data set, the greater is the need for statistics to summarize the information content. Statistical Software and Data. My goal in writing this text is to reach a broad group of students and industry analysts. Thus, to avoid excluding large segments, I chose not to integrate any specific statistical software package into the text. Nonetheless, because of the applications orientation, it is critical that the methodology presented be easily accomplished using readily available packages. For the course taught at the University of Wisconsin, I use the statistical packages SAS and R. On the book web site, http://research.bus.wisc.edu/RegActuaries users will find scripts written in SAS and R for the analysis presented in the text. The data are available in text format, allowing readers to employ any statistical packages that they wish. When you see a display such as this in the margin, you will also be able to find this data set () on the book web site. Technical Supplements. The technical supplements reinforce and extend the results in the main body of the text by giving a more formal, mathematical treatment of the material. This treatment is in fact a supplement because the applications and examples are described in the main body of the text. For readers with sufficient mathematical background, the supplements provide additional material that is useful in communicating to technical audiences. The technical supplements provide a deeper, and broader, coverage of applied regression analysis. I believe that analysts should have an idea of ‘what is going on under the hood,’ or ‘how the engine works.’ Most of these topics will be omitted from the first reading of the material. However, as you work with regression, you will be confronted with questions on ‘Why?’ and you will need to get into the details to see exactly how a certain technique works. Further, the technical supplements provide a menu of optional items that an instructor may wish to cover. Suggested Courses. There is a wide variety of topics that can go into a regression course. Here are some suggested courses. The course that I teach at the University of Wisconsin is the first on the list in the following table. \\[ {\\small \\begin{array}{ll} \\begin{array}{lll} \\hline \\textbf{Audience} &amp; \\textbf{Nature of Course}&amp; \\textbf{Suggested Chapters} \\\\ \\hline \\text{One-year background in} &amp; \\text{Survey of regression and} &amp; \\text{Chapters } 1-8 ,11-13, 20-21,\\\\ ~~~\\text{probability and statistics} &amp; ~~~\\text{time series models} &amp; ~~~\\text{main body of text only} \\\\ \\text{One-year background in} &amp; \\text{Regression and time} &amp; \\text{ Chapters } 1-8, 20-21, \\text{selected} \\\\ ~~~\\text{probability and statistics} &amp; ~~~\\text{series models} &amp; ~~~\\text{portions of technical supplements} \\\\ \\text{One-year background in} &amp; \\text{Regression modeling} &amp; \\text{Chapters } 1-6, 11-13, 20-21, \\text{selected} \\\\ ~~~\\text{probability and statistics} &amp; &amp; ~~~ \\text{portions of technical supplements} \\\\ \\text{Background in statistics}&amp; \\text{Actuarial regression}&amp; \\text{ Chapters } 10-21, \\text{selected} \\\\ ~~~\\text{and linear regression}&amp; ~~~\\text{models} &amp; ~~~ \\text{portions of technical supplements} \\\\ \\hline \\end{array} \\end{array} } \\] In addition to these suggested courses, this book is designed for supplemental reading for a time series course as well as a reference book for industry analysts. My hope is that college students who use the beginning parts of the book in their university course will find the later chapters helpful in their industry positions. In this way I hope to promote life-long learning! Acknowledgements It is appropriate to begin the acknowledgement section by thanking the students in the actuarial program here at the University of Wisconsin; students are important partners in the knowledge creation and dissemination business at universities. Through their questions and feedback, I have learned a tremendous amount over the years. I have also benefited from assistance from those who have helped me pull together all the pieces for this book, specifically, Missy Pinney, Peng Shi, Yunjie (Winnie) Sun and Ziyan Xie. I have enjoyed working with several former students and colleagues on regression problems over the recent years, including Katrien Antonio, Jie Gao, Paul Johnson, Margie Rosenberg, Jiafeng Sun, Emil Valdez and Ping Wang. Their contributions are reflected indirectly throughout the text. Because of my long association with the University of Wisconsin-Madison, I am reluctant to go back further in time and provide a longer list for fear of missing important individuals. I have also been fortunate to have a more recent association with the Insurance Services Office (ISO). Colleagues at ISO have provided me with important insights into applications. Through this text that features applications of regression into actuarial and financial industry problems, I hope to encourage the fostering of additional partnerships between academia and industry. I am pleased to acknowledge detailed reviews that I have received from colleagues Tim Welnetz and Margie Rosenberg. I also wish to thank Bob Miller for permission to include our joint work on designing effective graphs in Chapter 21. Bob has taught me a lot about regression over the years. Moreover, I am happy to acknowledge financial support through the Assurant Health Professorship in Actuarial Science at the University of Wisconsin-Madison. Saving the most important for last, I thank my family for their support. Ten thousand thanks to my mother Mary, brothers Randy, Guy and Joe, my wife Deirdre and our sons Nathan and Adam. "],["regression-and-the-normal-distribution.html", "Chapter 1 Regression and the Normal Distribution 1.1 What is Regression Analysis? 1.2 Fitting Data to a Normal Distribution 1.3 Power Transforms 1.4 Sampling and the Role of Normality 1.5 Regression and Sampling Designs 1.6 Actuarial Applications of Regression 1.7 Further Reading and References 1.8 Exercises 1.9 Technical Supplement - Central Limit Theorem", " Chapter 1 Regression and the Normal Distribution Chapter Preview. Regression analysis is a statistical method that is widely used in many fields of study, with actuarial science being no exception. This chapter provides an introduction to the role of the normal distribution in regression, the use of logarithmic transformations in specifying regression relationships and the sampling basis that is critical for inferring regression results to broad populations of interest. 1.1 What is Regression Analysis? Statistics is about data. As a discipline, it is about the collection, summarization and analysis of data to make statements about the real world. When analysts collect data, they are really collecting information that is quantified, that is, transformed to a numerical scale. There are easy, well-understood rules for reducing the data, using either numerical or graphical summary measures. These summary measures can then be linked to a theoretical representation, or model, of the data. With a model that is calibrated by data, statements about the world can be made. Statistical methods have had a major impact on several fields of study. In the area of data collection, the careful design of sample surveys is crucial to market research groups and to the auditing procedures of accounting firms. Experimental design is a second subdiscipline devoted to data collection. The focus of experimental design is on constructing methods of data collection that will extract information in the most efficient way possible. This is especially important in fields such as agriculture and engineering where each observation is expensive, possibly costing millions of dollars. Other applied statistical methods focus on managing and predicting data. Process control deals with monitoring a process over time and deciding when intervention is most fruitful. Process control helps manage the quality of goods produced by manufacturers. Forecasting is about extrapolating a process into the future, whether it be sales of a product or movements of an interest rate. Regression analysis is a statistical method used to analyze data. As we will see, the distinguishing feature of this method is the ability to make statements about variables after having controlled for values of known explanatory variables. Important as other methods are, it is regression analysis that has been most influential. To illustrate, an index of business journals, ABI/INFORM, lists over twenty-four thousand articles using regression techniques over the thirty-year period 1978-2007. And these are only the applications that were considered innovative enough to be published in scholarly reviews! Regression analysis of data is so pervasive in modern business that it is easy to overlook the fact that the methodology is barely over 120 years old. Scholars attribute the birth of regression to the 1885 presidential address of Sir Francis Galton to the anthropological section of the British Association of the Advancement of Sciences. In that address, described in Stigler (1986), Galton provided a description of regression and linked it to normal curve theory. His discovery arose from his studies of properties of natural selection and inheritance. To illustrate a data set that can be analyzed using regression methods, Table 1.1 displays some data included in Galton’s 1885 paper. This table displays the heights of 928 adult children, classified by an index of their parents’ height. Here, all female heights were multiplied by 1.08, and the index was created by taking the average of the father’s height and rescaled mother’s height. Galton was aware that the parents’ and the adult child’s height could each be adequately approximated by a normal curve. In developing regression analysis, he provided a single model for the joint distribution of heights. Table 1.1: Galtons 1885 Regression Data &lt;64.0 64.5 65.5 66.5 67.5 68.5 69.5 70.5 71.5 72.5 &gt;73.0 Total &gt;73.7 0 0 0 0 0 0 5 3 2 4 0 14 73.2 0 0 0 0 0 3 4 3 2 2 3 17 72.2 0 0 1 0 4 4 11 4 9 7 1 41 71.2 0 0 2 0 11 18 20 7 4 2 0 64 70.2 0 0 5 4 19 21 25 14 10 1 0 99 69.2 1 2 7 13 38 48 33 18 5 2 0 167 68.2 1 0 7 14 28 34 20 12 3 1 0 120 67.2 2 5 11 17 38 31 27 3 4 0 0 138 66.2 2 5 11 17 36 25 17 1 3 0 0 117 65.2 1 1 7 2 15 16 4 1 1 0 0 48 64.2 4 4 5 5 14 11 16 0 0 0 0 59 63.2 2 4 9 3 5 7 1 1 0 0 0 32 62.2 0 1 0 3 3 0 0 0 0 0 0 7 &lt;61.2 1 1 1 0 0 1 0 1 0 0 0 5 Total 14 23 66 78 211 219 183 68 43 19 4 928 Source: Stigler (1986) Table 1.1 shows that much of the information concerning the height of an adult child can be attributed to, or ‘explained,’ in terms of the parents’ height. Thus, we use the term explanatory variable for measurements that provide information about a variable of interest. Regression analysis is a method to quantify the relationship between a variable of interest and explanatory variables. The methodology used to study the data in Table 1.1 can also be used to study actuarial and other risk management problems, the thesis of this book. 1.2 Fitting Data to a Normal Distribution Historically, the normal distribution had a pivotal role in the development of regression analysis. It continues to play an important role, although we will be interested in extending regression ideas to highly ‘non-normal’ data. Formally, the normal curve is defined by the function \\[\\begin{equation} \\mathrm{f}(y)=\\frac{1}{\\sigma \\sqrt{2\\pi }}\\exp \\left( -\\frac{1}{2\\sigma ^{2}% }\\left( y-\\mu \\right) ^{2}\\right) . \\tag{1.1} \\end{equation}\\] This curve is a probability density function with the whole real line as its domain. From equation (1.1), we see that the curve is symmetric about \\(\\mu\\) (the mean and median). The degree of peakedness is controlled by the parameter \\(\\sigma ^{2}\\). These two parameters, \\(\\mu\\) and \\(\\sigma ^{2}\\), are known as the location and scale parameters, respectively. Appendix A3.1 provides additional details about this curve, including a graph and tables of its cumulative distribution that we will use throughout the text. The normal curve is also depicted in Figure 1.1, a display of a now out-of-date German currency note, the ten Deutsche Mark. This note contains the image of German Carl Gauss, an eminent mathematician whose name is often linked with the normal curve (it is sometimes referred to as the Gaussian curve). Gauss developed the normal curve in connection with the theory of least squares for fitting curves to data in 1809, about the same time as related work by the French scientist Pierre LaPlace. According to Stigler (1986), there was quite a bit of acrimony between these two scientists about the priority of discovery! The normal curve was first used as an approximation to histograms of data around 1835 by Adolph Quetelet, a Belgian mathematician and social scientist. Like many good things, the normal curve had been around for some time, since about 1720 when Abraham de Moivre derived it for his work on modeling games of chance. The normal curve is popular because it is easy to use and has proved to be successful in many applications. Figure 1.1: Ten Deutsche Mark. German currency featuring scientist Gauss and the normal curve. Example: Massachusetts Bodily Injury Claims. For our first look at fitting the normal curve to a set of data, we consider data from Rempala and Derrig (2005). They considered claims arising from automobile bodily injury insurance coverages. These are amounts incurred for outpatient medical treatments that arise from automobile accidents, typically sprains, broken collarbones and the like. The data consists of a sample of 272 claims from Massachusetts that were closed in 2001 (by ‘closed,’ we mean that the claim is settled and no additional liabilities can arise from the same accident). Rempala and Derrig were interested in developing procedures for handling mixtures of ‘typical’ claims and others from providers who reported claims fraudulently. For this sample, we consider only those typical claims, ignoring the potentially fraudulent ones. Table 1.2 provides several statistics that summarize different aspects of the distribution. Claim amounts are in units of logarithms of thousands of dollars. The average logarithmic claim is 0.481, corresponding to $1,617.77 (=1000 \\(\\exp(0.481)\\)). The smallest and largest claims are -3.101 (45 dollars) and 3.912 (50,000 dollars), respectively. Table 1.2: Summary Statistics of Massachusetts Automobile Bodily Injury Claims Number Mean Median Standard Deviation Minimum Maximum 25th Percentile 75th Percentile Claims 272 0.481 0.793 1.101 -3.101 3.912 -0.114 1.168 For completeness, here are a few definitions. The sample is the set of data available for analysis, denoted by \\(y_1,\\ldots,y_n\\). Here, \\(n\\) is the number of observations, \\(y_1\\) represents the first observation, \\(y_2\\) the second, and so on up to \\(y_n\\) for the \\(nth\\) observation. Here are a few important summary statistics. Basic Summary Statistics The mean is the average of observations, that is, the sum of the observations divided by the number of units. Using algebraic notation, the mean is \\[ \\overline{y}=\\frac{1}{n}\\left( y_1 + \\cdots + y_n \\right) = \\frac{1}{n} \\sum_{i=1}^{n} y_i. \\] The median is the middle observation when the observations are ordered by size. That is, it is the observation at which 50% are below it (and 50% are above it). The standard deviation is a measure of the spread, or scale, of the distribution. It is computed as \\[ s_y = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^{2}} . \\] A percentile is a number at which a specified fraction of the observations is below it, when the observations are ordered by size. For example, the 25th percentile is that number so that 25% of observations are below it. To help visualize the distribution, Figure 1.2 displays a histogram of the data. Here, the height of each rectangle shows the relative frequency of observations that fall within the range given by its base. The histogram provides a quick visual impression of the distribution; it shows that the range of the data is approximately (-4,4), the central tendency is slightly greater than zero and that the distribution is roughly symmetric. Normal Curve Approximation. Figure 1.2 also shows a normal curve superimposed, using \\(\\overline{y}\\) for \\(\\mu\\) and \\(s_y^{2}\\) for \\(\\sigma ^{2}\\). With the normal curve, only two quantities (\\(\\mu\\) and \\(\\sigma ^{2}\\)) are required to summarize the entire distribution. For example, Table 1.2 shows that 1.168 is the 75th percentile, which is approximately the 204th (\\(=0.75\\times 272\\)) largest observation from the entire sample. From the equation (1.1) normal distribution, we have that \\(z=(y-\\mu )/\\sigma\\) is a standard normal, of which 0.675 is the 75th percentile. Thus, \\(\\overline{y}+0.675s_y=\\) \\(0.481+0.675\\times 1.101=1.224\\) is the 75th percentile using the normal curve approximation. Figure 1.2: Bodily Injury Relative Frequency with Normal Curve Superimposed. R Code to Produce Figure 1.2 injury &lt;- read.csv(&quot;CSVData/MassBodilyInjury.csv&quot;, header=TRUE) injury2&lt;-subset(injury, providerA != 0 ) LOGCLAIMS&lt;-log(injury2$claims) # FIGURE 1.2 x &lt;- seq(-4, 4, 0.01) y &lt;- dnorm(x, mean=mean(LOGCLAIMS), sd=sqrt(var(LOGCLAIMS))) hist(LOGCLAIMS, freq=FALSE, main=&quot;&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Density&quot;, side=2, at=.35,las=1, adj=.7,cex=1.4) lines(x,y) Box Plot. A quick visual inspection of a variable’s distribution can reveal some surprising features that are hidden by statistics, numerical summary measures. The box plot, also known as a ‘box and whiskers’ plot, is one such graphical device. Figure 1.3 illustrates a box plot for the bodily injury claims. Here, the box captures the middle 50% of the data, with the three horizontal lines corresponding to the 75th, 50th and 25th percentiles, reading from top to bottom. The horizontal lines above and below the box are the ‘whiskers.’ The upper whisker is 1.5 times the interquartile range (the difference between the 75th and 25th percentiles) above the 75th percentile. Similarly, the lower whisker is 1.5 times the interquartile range below the 25th percentile. Individual observations outside the whiskers are denoted by small circular plotting symbols, and are referred to as ‘outliers.’ Figure 1.3: Box plot of bodily injury claims. R Code to Produce Figure 1.3 boxplot(LOGCLAIMS, boxwex=.7, las=1) text(1, .57, &quot;median&quot;, cex=1.2) text(1.36, -0.2, &quot;25th percentile&quot;, cex=1.2) text(1.36, 1.1, &quot;75th percentile&quot;, cex=1.2) arrows(1.05, -2, 1.05, -3.3, code=3, angle=20, length=0.1) #arrows(1.05, -2, 1.05, -3.3, col=&quot;blue&quot;, code=3, angle=20, length=0.1) text(1.15, -2.5, &quot;outliers&quot;, cex=1.2) text(1.13, 3.9, &quot;outlier&quot;, cex=1.2) Graphs are powerful tools; they allow analysts to readily visualize nonlinear relationships that are hard to comprehend when expressed verbally or by mathematical formula. However, by their very flexibility, graphs can also readily deceive the analyst. Chapter 21 will underscore this point. For example, Figure 1.4 is a re-drawing of Figure 1.2; the difference is that Figure 1.4 uses more, and finer, rectangles. This finer analysis reveals the asymmetric nature of the sample distribution that was not evident in Figure 1.2. Figure 1.4: Redrawing of Figure 1.2 with an increased number of rectangles. R Code to Produce Figure 1.4 hist(LOGCLAIMS, freq=FALSE, nclass=32, main=&quot;&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Density&quot;, side=2, at=.75,las=1, adj=.7,cex=1.1) lines(x,y) Quantile-Quantile Plots. Increasing the number of rectangles can unmask features that were not previously apparent; however, there are in general fewer observations per rectangle meaning that the uncertainty of the relative frequency estimate increases. This represents a trade-off. Instead of forcing the analyst to make an arbitrary decision about the number of rectangles, an alternative is to use a graphical device for comparing a distribution to another known as a quantile-quantile, or qq, plot. Figure 1.5 illustrates a \\(qq\\) plot for the bodily injury data using the normal curve as a reference distribution. For each point, the vertical axis gives the quantile using the sample distribution. The horizontal axis gives the corresponding quantity using the normal curve. For example, earlier we considered the 75th percentile point. This point appears as (1.168, 0.675) on the graph. To interpret a \\(qq\\) plot, if the quantile points lie along the superimposed line, then the sample and the normal reference distribution have the same shape. (This line is defined by connecting the 75th and 25th percentiles.) In Figure 1.5, the small sample percentiles are consistently smaller than the corresponding values from the standard normal, indicating that the distribution is skewed to the left. The difference in values at the ends of the distribution are due to the outliers noted earlier that could also be interpreted as the sample distribution having larger tails than the normal reference distribution. Figure 1.5: A \\(qq\\) plot of Bodily Injury Claims, using a normal reference distribution. R Code to Produce Figure 1.5 qqnorm(LOGCLAIMS, main=&quot;&quot;, las=1, ylab=&quot;&quot;) mtext(&quot;Sample Quantiles&quot;, side=2, at=4.5, las=1,cex=1.1,adj=.4) qqline(LOGCLAIMS) 1.3 Power Transforms In the Section 1.2 example, we considered claims without justifying the use of the logarithmic scaling. When analyzing variables such as assets of firms, wages of individuals and housing prices of households in business and economic applications, it is common to consider logarithmic instead of the original units. A log transform retains the original ordering (for example, large wages remain large on the log wage scale) but serves to ‘pull in’ extreme values of the distribution. To illustrate, Figure 1.6 shows the bodily injury claims distribution in (thousands of) dollars. In order to graph the data meaningfully, the largest observation ($50,000) was removed prior to making this plot. Even with this observation removed, Figure 1.6 shows that the distribution is heavily lop-sided to the right, with several large values of claims appearing. Distributions that are lopsided in one direction or the other are known as skewed. Figure 1.6 is an example of a distribution skewed to the right, or positively skewed. Here, the tail of the distribution on the right is longer and there is a greater concentration of mass to the left. In contrast, a left-skewed, or negatively skewed distribution, has a longer tail on the left and a greater concentration of mass to the right. Many insurance claims distributions are right-skewed (see the text by Klugman, Panjer and Willmot, 2008, for extensive discussions). As we saw in Figures 1.4 and 1.5, a logarithmic transformation yields a distribution that is only mildly skewed to the left. Figure 1.6: Distribution of Bodily Injury Claims. Observations are in (thousands of) dollars with the largest observation omitted.. R Code to Produce Figure 1.6 injury3 = subset(injury, claims &lt; 25 ) CLAIMS25 &lt;- injury3$claims par(mar=c(4.2,4,1.2,.2),cex=1.1) hist(CLAIMS25, freq=FALSE, main=&quot;&quot;, las=1, ylab=&quot;&quot;, xlab=&quot;CLAIMS&quot;) mtext(&quot;Density&quot;, side=2, at=.28, las=1,cex=1.1) Logarithmic transformations are used extensively in applied statistics work. One advantage is that they serve to symmetrize distributions that are skewed. More generally, we consider power transforms, also known as the Box-Cox family of transforms. Within this family of transforms, in lieu of using the response \\(y\\), we use a transformed, or rescaled version, \\(y^{\\lambda}\\). Here, the power \\(\\lambda\\) (lambda, a Greek ‘el’) is a number that may be user specified. Typical values of \\(\\lambda\\) that are used in practice are \\(\\lambda\\)=1, 1/2, 0 or -1. When we use \\(\\lambda =0\\), we mean \\(\\ln (y)\\), that is, the natural logarithmic transform. More formally, the Box-Cox family can be expressed as \\[ y^{(\\lambda )}=\\left\\{ \\begin{array}{ll} \\frac{y^{\\lambda }-1}{\\lambda } &amp; \\lambda \\neq 0 \\\\ \\ln (y) &amp; \\lambda =0 \\end{array} \\right. . \\] As we will see, because regression estimates are not affected by location and scale shifts, in practice we do not need to subtract one nor divide by \\(\\lambda\\) when rescaling the response. The advantage of the above expression is that, if we let \\(\\lambda\\) approach 0, then \\(y^{(\\lambda )}\\) approaches \\(\\ln (y)\\), from some straightforward calculus arguments. To illustrate the usefulness of transformations, we simulated 500 observations from a chi-square distribution with two degrees of freedom. Appendix A3.2 introduces this distribution (that we will encounter again later in studying the behavior of test statistics). The upper left panel of Figure 1.7 shows the original distribution is heavily skewed to the right. The other panels in Figure 1.7 show the data rescaled using the square root, logarithmic and negative reciprocal transformations. The logarithmic transformation, in the lower left panel, provides the best approximation to symmetry for this example. The negative reciprocal transformation is based on \\(\\lambda =-1\\), and then multiplying the rescaled observations by minus one, so that large observations remain large. Figure 1.7: 500 simulated observations from a chi-square distribution. The upper left panel is based on the original distribution. The upper right corresponds to the square root transform, the lower left to the log transform and the lower right to the negative reciprocal transform. R Code to Produce Figure 1.7 set.seed(1237) X1 &lt;- 10000*rchisq(500*1, df=2) X2 &lt;- sqrt(X1) X3 &lt;- log(X1) X4 &lt;- -1/X1 par(mfrow=c(2, 2), cex=.75, mar=c(3,5,1.5,0)) hist(X1, freq=FALSE, nclass=16, main=&quot;&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, las=1, yaxt=&quot;n&quot;,xlim=c(0,200000),ylim=c(0,.00005)) axis(2, at=seq(0,.00005,.00001),las=1, cex=.3, labels=c(&quot;0&quot;, &quot;0.00001&quot;, &quot;0.00002&quot;,&quot;0.00003&quot;, &quot;0.00004&quot;, &quot;0.00005&quot;)) mtext(&quot;Density&quot;, side=2, at=.000055, las=1, cex=.75) mtext(&quot;y&quot;, side=1, cex=.75, line=2) par(mar=c(3,4,1.5,0.2)) hist(X2, freq=FALSE, nclass=16, main=&quot;&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, las=1,xlim=c(0,400), ylim=c(0,.008)) mtext(&quot;Density&quot;, side=2, at=.0088, las=1, cex=.75) mtext(&quot;Square root of y&quot;, side=1, cex=.75, line=2) par(mar=c(3.2,5,1,0)) hist(X3, freq=FALSE, nclass=16, main=&quot;&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, las=1, ylim=c(0,.4)) mtext(&quot;Density&quot;, side=2, at=.44, las=1, cex=.75) mtext(&quot;Logarithmic y&quot;, side=1, cex=.75, line=2) par(mar=c(3.2,4,1,0.2)) hist(X4, freq=FALSE, nclass=16, main=&quot;&quot;,xlab=&quot;&quot;, ylab=&quot;&quot;, las=1, ylim=c(0,100)) mtext(&quot;Density&quot;, side=2, at=110, las=1, cex=.75) mtext(&quot;Negative reciprocal of y&quot;, side=1, cex=.75, line=2) 1.4 Sampling and the Role of Normality A statistic is a summary measure of data, such as a mean, median or percentile. Collections of statistics are very useful for analysts, decision-makers and everyday consumers for understanding massive amounts of data that represent complex situations. To this point, our focus has been on introducing sensible techniques to summarize variables; techniques that will be used repeatedly thoughout this text. However, the true usefulness of the discipline of statistics is its ability to say something about the unknown, not merely to summarize information already available. To this end, we need to make some fairly formal assumptions about the manner in which the data are observed. As a science, a strong feature of statistics (as a discipline) is the ability to critique these assumptions and offer improved alternatives in specific situations. It is customary to assume that the data are drawn from a larger population that we are interested in describing. The process of drawing the data is known as the sampling, or data generating, process. We denote this sample as \\(\\{y_1,\\ldots,y_n\\}\\). So that we may critique, and modify, these sampling assumptions, we list them below in detail: \\[ \\begin{array}{l} \\hline \\textbf{Basic Sampling Assumptions} \\\\ \\hline 1. ~\\mathrm{E~}y_i=\\mu \\\\ 2. ~\\mathrm{Var~}y_i=\\sigma ^{2} \\\\ 3. ~\\{y_i\\} \\text{ are independent} \\\\ 4. ~\\{y_i\\} \\text{ are normally distributed}. \\\\ \\hline \\end{array} \\] In this basic set-up, \\(\\mu\\) and \\(\\sigma ^{2}\\) serve as parameters that describe the location and scale of the parent population. The goal is to infer something sensible about them based on statistics such as \\(\\overline{y}\\) and \\(s_y^{2}\\). For the third assumption, we assume independence among the draws. In a sampling scheme, this may be guaranteed by taking a simple random sample from a population. The fourth assumption is not required for many statistical inference procedures because central limit theorems provide approximate normality for many statistics of interest. However, a formal justification of some statistics, such as \\(t\\)-statistics, requires this additional assumption. Section 1.8 provides an explicit statement of one version of the central limit theorem, giving conditions under which \\(\\overline{y}\\) is approximately normally distributed. This section also discusses a related result, known as an Edgeworth approximation, that shows that the quality of the normal approximation is better for symmetric parent populations when compared to skewed distributions. How does this discussion apply to the study of regression analysis? After all, so far we have focused only on the simple arithmetic average, \\(\\overline{y}\\). In subsequent chapters, we will emphasize that linear regression is the study of weighted averages; specifically, many regression coefficients can be expressed as weighted averages with appropriately chosen weights. Central limit and Edgeworth approximation theorems are available for weighted averages - these results will ensure approximate normality of regression coefficients. To use normal curve approximations in a regression context, we will often transform variables to achieve approximate symmetry. 1.5 Regression and Sampling Designs Approximating normality will be an important issue in practical applications of linear regression. Parts I and II of this book focus on linear regression, where we will learn basic regression concepts and sampling design. Part III will focus on nonlinear regression, involving binary, count and fat-tailed responses, where the normal is not the most helpful reference distribution. Ideas concerning basic concepts and design will also be used in the nonlinear setting. In regression analysis, we focus on one measurement of interest and call this the dependent variable. Other measurements are used as explanatory variables. A goal is to compare differences in the dependent variable in terms of differences in the explanatory variables. As noted in Section 1.1, regression is used extensively in many scientific fields. Table 1.3 lists alternative terms that you may encounter as you read regression applications. Table 1.3. Terminology for Regression Variables \\[ {\\small \\begin{array}{ll}\\hline\\hline y-\\text{Variable} &amp; x-\\text{Variable} \\\\\\hline \\text{Outcome of interest} &amp; \\text{Explanatory variable} \\\\ \\text{Dependent variable} &amp; \\text{Independent variable} \\\\ \\text{Endogenous variable} &amp; \\text{Exogenous variable} \\\\ \\text{Response} &amp; \\text{Treatment} \\\\ \\text{Regressand} &amp; \\text{Regressor} \\\\ \\text{Left-hand side variable} &amp; \\text{Right-hand side variable} \\\\ \\text{Explained variable } &amp; \\text{Predictor variable} \\\\ \\text{Output} &amp; \\text{Input} \\\\ \\hline \\end{array} } \\] In the latter part of the nineteenth century and early part of the twentieth century, statistics was beginning to make an important impact on the development of experimental science. Experimental sciences often use designed studies, where the data are under the control of an analyst. Designed studies are performed in laboratory settings, where there are tight physical restrictions on every variable that a researcher thinks may be important. Designed studies also occur in larger field experiments, where the mechanisms for control are different than in laboratory settings. Agriculture and medicine use designed studies. Data from a designed study are said to be experimental data. To illustrate, a classic example is to consider the yield of a crop such as corn, where each of several parcels of land (the observations) are assigned various levels of fertilizer. The goal is to ascertain the effect of fertilizer (the explanatory variable) on the corn yield (the response variable). Although researchers attempt to make parcels of land as much alike as possible, differences inevitably arise. Agricultural researchers use randomization techniques to assign different levels of fertilizer to each parcel of land. In this way, analysts can explain the variation in corn yields in terms of the variation of fertilizer levels. Through the use of randomization techniques, researchers using designed studies can infer that the treatment has a causal effect on the response. Chapter 6 discusses causality further. Example: Rand Health Insurance Experiment. How are medical care expenditures related to the demand for insurance? Many studies have established a positive relation between the amount spent on medical care and the demand for health insurance. Those in poor health anticipate using more medical services than similarly positioned people in good or fair health and will seek higher levels of health insurance to compensate for these anticipated expenditures. They obtain this additional health insurance by (i) selecting a more generous health insurance plan from an employer, (ii) choosing an employer with a more generous health insurance plan or (iii) paying more for individual health insurance. Thus, it is difficult to disentangle the cause and effect relationship of medical care expenditures and the availability of health insurance. A study reported by Manning et al. (1987) sought to answer this question using a carefully designed experiment. In this study, enrolled households from six cities, between November 1974 and February 1977, were randomly assigned to one of 14 different insurance plans. These plans varied by the cost-sharing elements, the co-insurance rate (the percentage paid on out-of-pocket expenditures which varied by 0, 25, 50 and 95%) as well as the deductible (5, 10 or 15 percent of family income, up to a maximum of $1,000). Thus, there was a random assignment to levels of the treatment, the amount of health insurance. The study found that more favorable plans resulted in greater total expenditures, even after controlling for participants’ health status. For actuarial science and other social sciences, designed studies are the exception rather than the rule. For example, if we want to study the effects of smoking on mortality, it is highly unlikely that we could get study participants to agree to be randomly assigned to smoker/nonsmoker groups for several years just so that we could observe their mortality patterns! As with the Section 1.1 Galton study, social science researchers generally work with observational data. Observational data are not under control of the analyst. With observational data, we can not infer causal relationships but we can readily introduce measures of association. To illustrate, in the Galton data, it is apparent that ‘tall’ parents are likely to have ‘tall’ children and conversely ‘short’ parents are likely to have ‘short’ children. Chapter 2 will introduce a correlation and other measures of association. However, we can not infer causality from the data. For example, there may be another variable, such as family diet, that is related to both variables. Good diet in the family could be associated with tall heights of parents and adult children, whereas poor diet stifles growth. If this were the case, we would call family diet a confounding variable. In designed experiments such as the Rand Health Insurance Experiment, we can control for the effects of variables such as health status through random assignment methods. In observational studies, we use statistical control, rather than experimental control. To illustrate, for the Galton data, we might split our observations into two groups, one for ‘good family diet’ and one for ‘poor family diet,’ and examine the relationship between parents’ and children’s height for each subgroup. This is the essence of the regression method, to compare a \\(y\\) and an \\(x\\), ‘controlling for’ the effects of other explanatory variables. Of course, to use statistical control and regression methods, one must record family diet, and any other measures of height that may confound the effects of parents’ height on the height of their adult child. The difficulty in designing studies is trying to imagine all of the variables that could possibly affect a response variable, an impossible task in most social science problems of interest. To give some guidance on when ‘enough is enough,’ Chapter 6 will discuss measures of an explanatory variable’s importance and its impact on model selection. 1.6 Actuarial Applications of Regression This book introduces a statistical method, regression analysis. The introduction is organized around the traditional triad of statistical inference: hypothesis testing, estimation and prediction. Further, this book shows how this methodology can be used in applications that are likely to be of interest to actuaries and to other risk analysts. As such, it is helpful to begin with the three traditional areas of actuarial applications: pricing, reserving and solvency testing. Pricing and adverse selection. Regression analysis can be used to determine insurance prices for many lines of business. For example, in private passenger automobile insurance, expected claims vary by the insured’s gender, age, location (city versus rural), vehicle purpose (work or pleasure) and a host of other explanatory variables. Regression can be used to identify the variables that are important determinants of expected claims. In competitive markets, insurance companies do not use the same price for all insureds. If they did, ‘good risks,’ those with lower than average expected claims, would overpay and leave the company. In contrast, ‘bad risks,’ those with higher than average expected claims, would remain with the company. If the company continued this flat rate pricing policy, premiums would rise (to compensate for claims by the increasing share of bad risks) and market share would dwindle as the company loses good risks. This problem is known as ‘adverse selection.’ Using an appropriate set of explanatory variables, classification systems can be developed so that each insured pays their fair share. Reserving and solvency testing. Both reserving and solvency testing are concerned with predicting whether liabilities associated with a group of policies will exceed the capital devoted to meeting obligations arising from the policies. Reserving involves determining the appropriate amount of capital to meet these obligations. Solvency testing is about assessing the adequacy of capital to fund the obligations for a block of business. In some practice areas, regression can be used to forecast future obligations to help determine reserves (see, for example, Chapter 19). Regression can also be used to compare characteristics of healthy and financially distressed firms for solvency testing (see, for example, Chapter 14). Other risk management applications. Regression analysis is a quantitative tool that can be applied in a broad variety of business problems, not just the traditional areas of pricing, reserving and solvency testing. By becoming familiar with regression analysis, actuaries will have another quantitative skill that can be brought to bear on general problems involving the financial security of people, companies and governmental organizations. To help you develop insights, this book provides many examples of potential ‘non-actuarial’ applications through featured vignettes labeled as ‘examples’ and illustrative data sets. To help understand potential regression applications, start by reviewing the several data sets featured in the Chapter 1 Exercises. Even if you do not complete the exercises to strengthen your data summary skills (that require the use of a computer), a review of the problem descriptions will help you become more familiar with types of applications in which an actuary might use regression techniques. 1.7 Further Reading and References This book introduces regression and time series tools that are most relevant to actuaries and other financial risk analysts. Fortunately, there are other sources that provide excellent introductions to these statistical topics (although not from a risk management viewpoint). Particularly for analysts that intend to specialize in statistics, it is helpful to get another perspective. For regression, I recommend Weisburg (2005) and Faraway (2005). For time series, Diebold (2004) is a good source. Moreover, Klugman, Panjer and Willmot (2008) provides a good introduction to actuarial applications of statistics; this book is intended to complement the Klugman et al. book by focusing on regression and time series methods. Chapter References Beard, Robert E., Teivo Pentik\"{a}inen and Erkki Pesonen (1984). Risk Theory: The Stochastic Basis of Insurance (Third Edition). Chapman &amp; Hall, New York. Diebold, Francis. X. (2004). Elements of Forecasting, Third Edition. Thomson, South-Western, Mason, Ohio. Faraway, Julian J. (2005). Linear Models in R. Chapman &amp; Hall/CRC, New York. Hogg, Robert V. (1972). On statistical education. The American Statistician 26, 8-11. Klugman, Stuart A, Harry H. Panjer and Gordon E. Willmot (2008). Loss Models: From Data to Decisions. John Wiley &amp; Sons, Hoboken, New Jersey. Manning, Willard G., Joseph P. Newhouse, Naihua Duan, Emmett B. Keeler, Arleen Leibowitz and M. Susan Marquis (1987). Health insurance and the demand for medical care: Evidence from a randomized experiment. American Economic Review 77, No. 3, 251-277. Rempala, Grzegorz A. and Richard A. Derrig (2005). Modeling hidden exposures in claim severity via the EM algorithm. North American Actuarial Journal 9, No. 2, 108-128. Singer, Judith D. and Willett, J. B. (1990). Improving the teaching of applied statistics: Putting the data back into data analysis. The American Statistician 44, 223-230. Stigler, Steven M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. The Belknap Press of Harvard University Press, Cambridge, MA. Weisberg, Sanford (2005). Applied Linear Regression, Third Edition. John Wiley &amp; Sons, New York. 1.8 Exercises 1.1 MEPS health expenditures. This exercise considers data from the Medical Expenditure Panel Survey (MEPS), conducted by the U.S. Agency of Health Research and Quality. MEPS is a probability survey that provides nationally representative estimates of health care use, expenditures, sources of payment, and insurance coverage for the U.S. civilian population. This survey collects detailed information on individuals of each medical care episode by type of services including physician office visits, hospital emergency room visits, hospital outpatient visits, hospital inpatient stays, all other medical provider visits, and use of prescribed medicines. This detailed information allows one to develop models of health care utilization to predict future expenditures. You can learn more about MEPS at http://www.meps.ahrq.gov/mepsweb/. We consider MEPS data from the panels 7 and 8 of 2003 that consists of 18,735 individuals between ages 18 and 65. From this sample, we took a random sample of 2,000 individuals that appear in the file ‘HealthExpend’. From this sample, there are 157 individuals that had positive inpatient expenditures. There are also 1,352 that had positive outpatient expenditures. We will analyze these two samples separately. Our dependent variables consist of amounts of expenditures for inpatient (EXPENDIP) and outpatient (EXPENDOP) visits. For MEPS, outpatient events include hospital outpatient department visits, office-based provider visits and emergency room visits excluding dental services. (Dental services, compared to other types of health care services, are more predictable and occur in a more regular basis.) Hospital stays with the same date of admission and discharge, known as ‘zero-night stays,’ were included in outpatient counts and expenditures. (Payments associated with emergency room visits that immediately preceded an inpatient stay were included in the inpatient expenditures. Prescribed medicines that can be linked to hospital admissions were included in inpatient expenditures, not in outpatient utilization.) Part 1: Use only the 157 individuals that had positive inpatient expenditures and do the following analysis. Compute descriptive statistics for inpatient (EXPENDIP) expenditures. a(i). What is the typical (mean and median) expenditure? a(ii). How does the standard deviation compare to the mean? Do the data appear to be skewed? Compute a box plot, histogram and a (normal) \\(qq\\) plot for EXPENDIP. Comment on the shape of the distribution. Transformations. c(i). Take a square root transform of inpatient expenditures. Summarize the resulting distribution using a histogram and a \\(qq\\) plot. Does it appear to be approximately normally distributed? c(ii). Take a (natural) logarithmic transformation of inpatient expenditures. Summarize the resulting distribution using a histogram and a \\(qq\\) plot. Does it appear to be approximately normally distributed? Part 2: Use only the 1,352 individuals that had positive outpatient expenditures. Repeat part (a) and compute histograms for expenditures and logarithmic expenditures. Comment on the approximate normality for each histogram. 1.2 Nursing Home Utilization. This exercise considers nursing home data provided by the Wisconsin Department of Health and Family Services (DHFS). The State of Wisconsin Medicaid program funds nursing home care for individuals qualifying on the basis of need and financial status. As part of the conditions for participation, Medicaid-certified nursing homes must file an annual cost report to DHFS, summarizing the volume and cost of care provided to all of its residents, Medicaid-funded and otherwise. These cost reports are audited by DHFS staff and form the basis for facility-specific Medicaid daily payment rates for subsequent periods. The data are publicly available; see [http://dhs.wisconsin.gov] for more information. The DHFS is interested in predictive techniques that provide reliable utilization forecasts to update their Medicaid funding rate schedule of nursing facilities. In this assignment, we consider the data in the file ‘WiscNursingHome’ in cost report years 2000 and 2001. There are 362 facilities in 2000 and 355 facilities in 2001. Typically, utilization of nursing home care is measured in patient days (‘patient days’ is the number of days each patient was in the facility, summed over all patients). For this exercise, we define the outcome variable to be total patient years (TPY), the number of total patient days in the cost reporting period divided by number of facility operating days in the cost reporting period (see Rosenberg et al., 2007, Appendix 1, for further discussion of this choice). The number of beds (NUMBED) and square footage (SQRFOOT) of the nursing home both measure the size of the facility. Not surprisingly, these variables will be important predictors of TPY. Part 1: Use cost report year 2000 data, and do the following analysis. Compute descriptive statistics for TPY, NUMBED, and SQRFOOT. Summarize the distribution of TPY using a histogram and a \\(qq\\) plot. Does it appear to be approximately normally distributed? Transformations. Take a (natural) logarithmic transformation of TPY (LOGTPY). Summarize the resulting distribution using a histogram and a \\(qq\\) plot. Does it appear to be approximately normally distributed? Part 2: Use cost report year 2001 data and repeat parts (a) and (c). 1.3 Automobile Insurance Claims. As an actuarial analyst, you are working with a large insurance company to help them understand their claims distribution for their private passenger automobile policies. You have available claims data for a recent year, consisting of: STATE CODE: codes 01 through 17 used, with each code randomly assigned to an actual individual state CLASS: rating class of operator, based on age, gender, marital status and use of vehicle GENDER: operator gender AGE: operator age PAID: amount paid to settle and close a claim. You are focusing on older drivers, 50 and higher, for which there are \\(n = 6,773\\) claims available. Examine the histogram of the amount PAID and comment on the symmetry. Create a new variable, the (natural) logarithmic claims paid, LNPAID. Create a histogram and a \\(qq\\) plot of LNPAID. Comment on the symmetry of this variable. 1.4 Hospital Costs. Suppose that you are an employee benefits actuary working with a medium size company in Wisconsin. This company is considering offering, for the first time in their industry, hospital insurance coverage to dependent children of their employees. You have access to company records and so have available the number, age and gender of the dependent children but have no other information about hospital costs from the company. In particular, no firm in this industry has offered this coverage and so you have little historical industry experience upon which you can forecast expected claims. You gather data from the Nationwide Inpatient Sample of the Healthcare Cost and Utilization Project (NIS-HCUP), a nationwide survey of hospital costs conducted by the US Agency for Healthcare Research and Quality (AHRQ). You restrict consideration to Wisconsin hospitals and analyze a random sample of \\(n=500\\) claims from 2003 data. Although the data comes from hospital records, it is organized by individual discharge and so you have information about the age and gender of the patient discharged. Specifically, you consider patients aged 0-17 years. In a separate project, you will consider the frequency of hospitalization. For this project, the goal is to model the severity of hospital charges, by age and gender. Examine the distribution of the dependent variable, TOTCHG. Do this by making a histogram and then a \\(qq\\) plot, comparing the empirical to a normal distribution. Take a natural log transformation and call the new variable LNTOTCHG. Examine the distribution of this transformed variable. To visualize the logarithmic relationship, plot LNTOTCHG versus TOTCHG. 1.5 Automobile injury insurance claims. We consider automobile injury claims data using data from the Insurance Research Council (IRC), a division of the American Institute for Chartered Property Casualty Underwriters and the Insurance Institute of America. The data, collected in 2002, contains information on demographic information about the claimant, attorney involvement and the economic loss (LOSS, in thousands), among other variables. We consider here a sample of \\(n=1,340\\) losses from a single state. The full 2002 study contains over 70,000 closed claims based on data from thirty-two insurers. The IRC conducted similar studies in 1977, 1987, 1992 and 1997. Compute descriptive statistics for the total economic loss (LOSS). What is the typical loss? Compute a histogram and (normal) \\(qq\\) plot for LOSS. Comment on the shape of the distribution. Partition the data set into two subsamples, one corresponding to those claims that involved an ATTORNEY (=1) and the other where an ATTORNEY was not involved (=2). c(i). For each subsample, compute the typical loss. Does there appear to be a difference in the typical losses by attorney involvement? c(ii) To compare the distributions, compute a boxplot by level of attorney involvement. c(iii). For each subsample, compute a histogram and \\(qq\\) plot. Compare the two distributions to one another. 1.6 Insurance Company Expenses. Like other businesses, insurance companies seek to minimize expenses associated with doing business in order to enhance profitability. To study expenses, this exercise examines a random sample of 500 insurance companies from the National Association of Insurance Commissioners (NAIC) database of over 3,000 companies. The NAIC maintains one of the world’s largest insurance regulatory databases; we consider here data that are based on 2005 annual reports for all the property and casualty insurance companies in the United States. The annual reports are financial statements that use statutory accounting principles. Specifically, our dependent variable is EXPENSES, the non-claim expenses for a company. Although not needed for this exercise, non-claim expenses are based on three components: unallocated loss adjustment, underwriting and investment expenses. The unallocated loss adjustment expense is the expense not directly attributable to a claim but is indirectly associated with settling claims; it includes items such as the salaries of claims adjusters, legal fees, court costs, expert witnesses and investigation costs. Underwriting expenses consists of policy acquisition costs, such as commissions, as well as the portion of administrative, general and other expenses attributable to underwriting operations. Investment expense are those expenses related to investment activities of the insurer. Examine the distribution of the dependent variable, EXPENSES. Do this by making a histogram and then a \\(qq\\) plot, comparing the empirical to a normal distribution. Take a natural log transformation and examine the distribution of this transformed variable. Has the transformation helped to symmetrize the distribution? 1.7 National Life Expectancies. Who is doing health care right? Health care decisions are made at the individual, corporate and government levels. Virtually every person, corporation and government have their own perspective on health care; these different perspectives result in a wide variety of systems for managing health care. Comparing different health care systems help us learn about approaches other than our own, which in turn help us make better decisions in designing improved systems. Here, we consider health care systems from \\(n=185\\) countries throughout the world. As a measure of the quality of care, we use LIFEEXP, the life expectancy at birth. This dependent variable, with several explanatory variables, are listed in Table 1.4. From this table, you will note that although there are 185 countries considered in this study, not all countries provided information for each variable. Data not available are noted under the column Num Miss. The data are from the United Nations (UN) Human Development Report. Examine the distribution of the dependent variable, LIFEEXP. Do this by making a histogram and then a \\(qq\\) plot, comparing the empirical to a normal distribution. Take a natural log transformation and examine the distribution of this transformed variable. Has the transformation helped to symmetrize the distribution? Table 1.4. Life Expectancy, Economic and Demographic Characteristics of 185 Countries \\[ \\small{ \\begin{array}{ll|crrrrr} \\hline &amp; &amp; Num &amp; &amp; &amp; Std &amp; Mini- &amp; Maxi- \\\\ Variable &amp; Description &amp; Miss &amp; Mean &amp; Median &amp; Dev &amp; mum &amp; mum \\\\\\hline BIRTH &amp; \\text{ Births attended by skilled} &amp; 7 &amp; 78.25 &amp; 92.00 &amp; 26.42 &amp; 6.00 &amp; 100.00 \\\\ ~~ATTEND&amp; ~~ \\text{ health personnel} (\\%)\\\\ FEMALE &amp; \\text{ Legislators, senior officials} &amp; 87 &amp; 29.07 &amp; 30.00 &amp; 11.71 &amp; 2.00 &amp; 58.00 \\\\ ~~BOSS&amp; ~~ \\text{ and managers, % female }\\\\ FERTILITY &amp; \\text{ Total fertility rate,}&amp; 4 &amp; 3.19 &amp; 2.70 &amp; 1.71 &amp; 0.90 &amp; 7.50 \\\\ &amp; ~~ \\text{ births per woman }&amp; \\\\ GDP &amp; \\text{ Gross domestic product,} &amp; 7 &amp; 247.55 &amp; 14.20 &amp; 1,055.69 &amp; 0.10 &amp; 12,416.50 \\\\ &amp; ~~\\text{ in billions of USD} \\\\ HEALTH&amp; \\text{ 2004 Health expenditure} &amp; 5 &amp; 718.01 &amp; 297.50 &amp; 1,037.01 &amp; 15.00 &amp; 6,096.00 \\\\ ~~ EXPEND &amp; ~~ \\text{ per capita, PPP in USD} \\\\ ILLITERATE &amp; \\text{ Adult illiteracy rate,} &amp; 14 &amp; 17.69 &amp; 10.10 &amp; 19.86 &amp; 0.20 &amp; 76.40 \\\\ &amp; ~~ \\% \\text{ aged 15 and older} &amp; \\\\ PHYSICIAN &amp; \\text{ Physicians,}&amp; 3 &amp; 146.08 &amp; 107.50 &amp; 138.55 &amp; 2.00 &amp; 591.00 \\\\ &amp; ~~ \\text{ per 100,000 people} \\\\ POP &amp; \\text{ 2005 population,} &amp; 1 &amp; 35.36 &amp; 7.80 &amp; 131.70 &amp; 0.10 &amp; 1,313.00 \\\\ &amp; ~~\\text{ in millions }\\\\ PRIVATE &amp; \\text{ 2004 Private expenditure} &amp; 1 &amp; 2.52 &amp; 2.40 &amp; 1.33 &amp; 0.30 &amp; 8.50 \\\\ ~~HEALTH&amp; ~~\\text{on health, % of GDP} \\\\ PUBLIC &amp; \\text{ Public expenditure} &amp; 28 &amp; 4.69 &amp; 4.60 &amp; 2.05 &amp; 0.60 &amp; 13.40 \\\\ ~~EDUCATION&amp; ~~ \\text{ on education, % of GDP} \\\\ RESEAR &amp; \\text{ Researchers in R &amp; D,} &amp; 95 &amp; 2,034.66 &amp; 848.00 &amp; 4,942.93 &amp; 15.00 &amp; 45,454.00 \\\\ ~~CHERS&amp;~~ \\text{ per million people} &amp; \\\\ SMOKING &amp; \\text{ Prevalence of smoking,} &amp; 88 &amp; 35.09 &amp; 32.00 &amp; 14.40 &amp; 6.00 &amp; 68.00 \\\\ &amp; ~~\\text{ (male) % of adults } \\\\ \\hline LIFEEXP &amp; \\text{ Life expectancy at birth,}&amp; &amp; 67.05 &amp; 71.00 &amp; 11.08 &amp; 40.50 &amp; 82.30 \\\\ &amp; ~~ \\text{ in years } \\\\ \\hline \\end{array} } \\] Source: United Nations Human Development Report, available at http://hdr.undp.org/en/ . 1.9 Technical Supplement - Central Limit Theorem Central limit theorems form the basis for much of the statistical inference used in regression analysis. Thus, it is helpful to provide an explicit statement of one version of the central limit theorem. Central Limit Theorem. Suppose that \\(y_1,\\ldots,y_n\\) are independently distributed with mean \\(\\mu\\), finite variance \\(\\sigma ^{2}\\) and \\(\\mathrm{E}|y|^{3}\\) is finite. Then, \\[ \\lim_{n\\rightarrow \\infty }\\Pr \\left( \\frac{\\sqrt{n}}{\\sigma }(\\overline{y} -\\mu )\\leq x\\right) =\\Phi \\left( x\\right) , \\] for each \\(x,\\) where \\(\\Phi \\left( .\\right)\\) is the standard normal distribution function. Under the assumptions of this theorem, the re-scaled distribution of \\(\\overline{y}\\) approaches a standard normal as the sample size, \\(n\\), increases. We interpret this as meaning that, for ‘large’ sample sizes, the distribution of \\(\\overline{y}\\) may be approximated by a normal distribution. Empirical investigations have shown that sample sizes of \\(n=25\\) through 50 provide adequate approximations for most purposes. When does the central limit theorem not work well? Some insights are provided by another result from mathematical statistics. Edgeworth Approximation. Suppose that \\(y_1,\\ldots, y_n\\) are identically and independently distributed with mean \\(\\mu\\), finite variance \\(\\sigma ^{2}\\) and \\(\\mathrm{E}|y|^{3}\\) is finite. Then,% \\[ \\Pr \\left( \\frac{\\sqrt{n}}{\\sigma }(\\overline{y}-\\mu )\\leq x\\right) =\\Phi \\left( x\\right) +\\frac{1}{6}\\frac{1}{\\sqrt{2\\pi }}e^{-x^{2}/2}\\frac{\\mathrm{E }(y-\\mu )^{3}}{\\sigma ^{3}\\sqrt{n}}+\\frac{h_n}{\\sqrt{n}}, \\] for each \\(x,\\) where \\(h_n\\rightarrow 0\\) as \\(n\\rightarrow \\infty .\\) This result suggests that the distribution of \\(\\bar{y}\\) becomes closer to a normal distribution as the skewness, \\(\\mathrm{E}(\\overline{y} -\\mu )^{3}\\), becomes closer to zero. This is important in insurance applications because many distributions tend to be skewed. Historically, analysts used the second term on the right-hand side of the result to provide a ‘correction’ for the normal curve approximation. See, for example, Beard, Pentik\"{a}inen and Pesonen (1984) for further discussion of Edgeworth approximations in actuarial science. An alternative (used in this book) that we saw in Section 1.3 is to transform the data, thus achieving approximate symmetry. As suggested by the Edgeworth approximation theorem, if our parent population is close to symmetric, then the distribution of \\(\\overline{y}\\) will be approximately normal. "],["C2BasicLR.html", "Chapter 2 Basic Linear Regression 2.1 Correlations and Least Squares 2.2 Basic Linear Regression Model 2.3 Is the Model Useful? Some Basic Summary Measures 2.4 Properties of Regression Coefficient Estimators 2.5 Statistical Inference 2.6 Building a Better Model: Residual Analysis 2.7 Application: Capital Asset Pricing Model 2.8 Illustrative Regression Computer Output 2.9 Further Reading and References 2.10 Exercises 2.11 Technical Supplement - Elements of Matrix Algebra", " Chapter 2 Basic Linear Regression Chapter Preview. This chapter considers regression in the case of only one explanatory variable. Despite this seeming simplicity, most of the deep ideas of regression can be developed in this framework. By limiting ourselves to the one variable case, we are able to express many calculations using simple algebra. This will allow us to develop our intuition about regression techniques by reinforcing it with simple demonstrations. Further, we can illustrate the relationships between two variables graphically because we are working in only two dimensions. Graphical tools prove to be important for developing a link between the data and a model. 2.1 Correlations and Least Squares Regression is about relationships. Specifically, we will study how two variables, an \\(x\\) and a \\(y\\), are related. We want to be able to answer questions such as, if we change the level of \\(x\\), what will happen to the level of \\(y\\)? If we compare two “subjects” that appear similar except for the \\(x\\) measurement, how will their \\(y\\) measurements differ? Understanding relationships among variables is critical for quantitative management, particularly in actuarial science where uncertainty is so prevalent. It is helpful to work with a specific example to become familiar with key concepts. Analysis of lottery sales has not been part of traditional actuarial practice but it is a growth area in which actuaries could contribute. Example: Wisconsin Lottery Sales. State of Wisconsin lottery administrators are interested in assessing factors that affect lottery sales. Sales consists of online lottery tickets that are sold by selected retail establishments in Wisconsin. These tickets are generally priced at $1.00, so the number of tickets sold equals the lottery revenue. We analyze average lottery sales (SALES) over a forty-week period, April, 1998 through January, 1999, from fifty randomly selected areas identified by postal (ZIP) code within the state of Wisconsin. Although many economic and demographic variables might influence sales, our first analysis focuses on population (POP) as a key determinant. Chapter 3 will show how to consider additional explanatory variables. Intuitively, it seems clear that geographic areas with more people will have higher sales. So, other things being equal, a larger \\(x=POP\\) means a larger \\(y=SALES.\\) However, the lottery is an important source of revenue for the state and we want to be as precise as possible. A little additional notation will be useful subsequently. In this sample, there are fifty geographic areas and we use subscripts to identify each area. For example, \\(y_1\\) = 1,285.4 represents sales for the first area in the sample that has population \\(x_1\\) = 435. Call the ordered pair (\\(x_1\\), \\(y_1\\)) = (435, 1285.4) the first observation. Extending this notation, the entire sample containing fifty observations may be represented by (\\(x_1\\), \\(y_1\\)), …, (\\(x_{50}\\), \\(y_{50}\\)). The ellipses ( … ) mean that the pattern is continued until the final object is encountered. We will often speak of a generic member of the sample, referring to (\\(x_i\\), \\(y_i\\)) as the \\(i\\)th observation. Data sets can get complicated, so it will help if you begin by working with each variable separately. The two panels in Figure 2.1 show histograms that give a quick visual impression of the distribution of each variable in isolation of the other. Table 2.1 provides corresponding numerical summaries. To illustrate, for the population variable (POP), we see that the area with the smallest number contained 280 people whereas the largest contained 39,098. The average, over 50 ZIP codes, was 9,311.04. For our second variable, sales were as low as 189 and as high as 33,181. Figure 2.1: Histograms of Population and Sales. Each distribution is skewed to the right, indicating that there are many small areas compared to a few areas with larger sales and populations. Table 2.1: Summary Statistics of Each Variable Mean Median Standard Deviation Minimum Maximum POP 9,311 4,406 11,098 280 39,098 SALES 6,495 2,426 8,103 189 33,181 Source: Frees and Miller (2003) R Code to Produce Figure 2.1 and Table 2.1 Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURE 2.1 par(mfrow=c(1, 2), cex=1.3, mar=c(4.1,3.1,1.2,1)) hist(Lot$POP, main=&quot;&quot;, ylab=&quot;&quot;, las=1, xlab = &quot;POP&quot;) mtext(&quot;Frequency&quot;, side=2, at=30, las=1, cex=1.3, adj=.6) hist(Lot$SALES, main=&quot;&quot;, ylab=&quot;&quot;, las=1, xlab = &quot;SALES&quot;) mtext(&quot;Frequency&quot;, side=2, at=34, las=1, cex=1.3, adj=.6) # TABLE 2.1 SUMMARY STATISTICS BookSummStats &lt;- function(Xymat){ meanSummary &lt;- sapply(Xymat, mean, na.rm=TRUE) sdSummary &lt;- sapply(Xymat, sd, na.rm=TRUE) minSummary &lt;- sapply(Xymat, min, na.rm=TRUE) maxSummary &lt;- sapply(Xymat, max, na.rm=TRUE) medSummary &lt;- sapply(Xymat, median,na.rm=TRUE) tableMat &lt;- cbind(meanSummary, medSummary, sdSummary, minSummary, maxSummary) return(tableMat) } Xymat &lt;- data.frame(cbind(Lot$POP,Lot$SALES)) tableMat &lt;- BookSummStats(Xymat) colnames(tableMat) &lt;- c(&quot;Mean&quot; , &quot;Median&quot; , &quot;Standard Deviation&quot; , &quot;Minimum&quot; , &quot;Maximum&quot;) rownames(tableMat) &lt;- c(&quot;POP&quot;, &quot;SALES&quot;) tableMat1 &lt;- format(round(tableMat, digits=0), big.mark = &#39;,&#39;) TableGen1(TableData=tableMat1, TextTitle=&#39;Summary Statistics of Each Variable&#39;, Align=&#39;r&#39;, Digits=0, ColumnSpec=1:5, ColWidth = ColWidth5) %&gt;% footnote(general = &quot;Frees and Miller (2003)&quot;, general_title = &quot;Source:&quot;, footnote_as_chunk = TRUE) As Table 2.1 shows, the basic summary statistics give useful ideas of the structure of key features of the data. After we understand the information in each variable in isolation of the other, we can begin exploring the relationship between the two variables. Scatter Plot and Correlation Coefficients - Basic Summary Tools The basic graphical tool used to investigate the relationship between the two variables is a scatter plot such as in Figure 2.2. Although we may lose the exact values of the observations when graphing data, we gain a visual impression of the relationship between population and sales. From Figure 2.2 we see that areas with larger populations tend to purchase more lottery tickets. How strong is this relationship? Can knowledge of the area’s population help us anticipate the revenue from lottery sales? We explore these two questions below. Figure 2.2: A scatter plot of the lottery data. Each of the 50 plotting symbols corresponds to a zip code in the study. This figure suggests that postal areas with larger populations have larger lottery revenues. R Code to Produce Figure 2.2 #Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURE 2.2, WITH CORRELATIONS par(mar=c(4.1,3.8,2,1),cex=1.1) plot(Lot$POP, Lot$SALES, ylab=&quot;&quot;, las=1, xlab = &quot;POP&quot;) mtext(&quot;SALES&quot;,side=2, at=36000, las=1, cex=1.1) One way to summarize the strength of the relationship between two variables is through a correlation statistic. Definition. The ordinary, or Pearson, correlation coefficient is defined as \\[\\begin{equation*} r=\\frac{1}{(n-1)s_xs_y}\\sum_{i=1}^{n}\\left( x_{i}-\\overline{x}\\right) \\left( y_{i}-\\overline{y}\\right) . \\end{equation*}\\] Here, we use the sample standard deviation \\(s_y = \\sqrt{(n-1)^{-1} \\sum_{i=1}^{n}\\left( y_i - \\overline{y}\\right)^{2}}\\) defined in Section 1.2, with similar notation for \\(s_x\\). Although there are other correlation statistics, the correlation coefficient devised by Pearson (1895) has several desirable properties. One important property is that, for any data set, \\(r\\) is bounded by -1 and 1, that is, \\(-1\\leq r\\leq 1\\). (Exercise 2A.9 provides steps for you to check this property.) If \\(r\\) is greater than zero, the variables are said to be (positively) correlated. If \\(r\\) is less than zero, the variables are said to be negatively correlated. The larger the coefficient is in absolute value, the stronger is the relationship. In fact, if \\(r=1\\), then the variables are perfectly correlated. In this case, all of the data lie on a straight line that goes through the lower left and upper right-hand quadrants. If \\(r=-1\\), then all of the data lie on a line that goes through the upper left and lower right-hand quadrants. The coefficient \\(r\\) is a measure of a linear relationship between two variables. The correlation coefficient is said to be location and scale invariant. Thus, each variable’s center of location does not matter in the calculation of \\(r\\). For example, if we add $100 to the sales of each zip code, each \\(y_i\\) will increase by 100. However, \\(\\overline{y}\\), the average purchase price will also increase by 100 so that the deviation \\(y_i - \\overline{y}\\) remains unchanged, or invariant. Further, the scale of each variable does not matter in the calculation of \\(r\\). For example, suppose we divide each population by 1000 so that \\(x_i\\) now represents population in thousands. Thus, \\(\\overline{x}\\) is also divided by 1000 and you should check that \\(s_x\\) is also divided by 1000. Thus, the standardized version of \\(x_i\\), \\(\\left( x_i-\\overline{x}\\right) /s_x\\), remains unchanged, or invariant. Many statistical packages compute a standardized version of a variable by subtracting the average and dividing by the standard deviation. Now, let’s use \\(y_{i,std}=\\left( y_i- \\overline{y}\\right) /s_y\\) and \\(x_{i,std}=\\left( x_i-\\overline{x} \\right) /s_x\\) to be the standardized versions of \\(y_i\\) and \\(x_i\\), respectively. With this notation, we can express the correlation coefficient as \\(r=(n-1)^{-1}\\sum_{i=1}^{n}x_{i,std}\\times y_{i,std}.\\) The correlation coefficient is said to be a dimensionless measure. This is because we have taken away dollars, and all other units of measures, by considering the standardized variables \\(x_{i,std}\\) and \\(y_{i,std}\\). Because the correlation coefficient does not depend on units of measure, it is a statistic that can readily be compared across different data sets. In the world of business, the term “correlation” is often used as synonymous with the term “relationship.” For the purposes of this text, we use the term correlation when referring only to linear relationships. The classic nonlinear relationship is \\(y=x^{2}\\), a quadratic relationship. Consider this relationship and the fictitious data set for \\(x\\), \\(\\{-2,1,0,1,2\\}\\). Now, as an exercise (2.2), produce a rough graph of the data set: \\[ \\begin{array}{l|rrrrr} \\hline i &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\ \\hline x_i &amp; -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 \\\\ y_i &amp; 4 &amp; 1 &amp; 0 &amp; 1 &amp; 4 \\\\ \\hline \\end{array} \\] The correlation coefficient for this data set turns out to be \\(r=0\\) (check this). Thus, despite the fact that there is a perfect relationship between \\(x\\) and \\(y\\) (\\(=x^{2}\\)), there is a zero correlation. Recall that location and scale changes are not relevant in correlation discussions, so we could easily change the values of \\(x\\) and \\(y\\) to be more representative of a business data set. How strong is the relationship between \\(y\\) and \\(x\\) for the lottery data? Graphically, the response is a scatter plot, as in Figure 2.2. Numerically, the main response is the correlation coefficient which turns out to be \\(r\\) = 0.886 for this data set. We interpret this statistic by saying that SALES and POP are (positively) correlated. The strength of the relationship is strong because \\(r\\) = 0.886 is close to one. In summary, we may describe this relationship by saying that there is a strong correlation between SALES and POP. Method of Least Squares Now we begin to explore the question, “Can knowledge of population help us understand sales?” To respond to this question, we identify sales as the response, or dependent, variable. The population variable, which is used to help understand sales, is called the explanatory, or independent, variable. Suppose that we have available the sample data of fifty sales \\(\\{y_1, \\ldots, y_{50} \\}\\) and your job is to predict the sales of a randomly selected ZIP code. Without knowledge of the population variable, a sensible predictor is simply \\(\\overline{y}=6,495\\), the average of the available sample. Naturally, you anticipate that areas with larger populations will have larger sales. That is, if you also have knowledge of population, then can this estimate be improved? If so, then by how much? To answer these questions, the first step assumes an approximate linear relationship between \\(x\\) and \\(y\\). To fit a line to our data set, we use the method of least squares. We need a general technique so that, if different analysts agree on the data and agree on the fitting technique, then they will agree on the line. If different analysts fit a data set using eyeball approximations, in general they will arrive at different lines, even using the same data set. The method begins with the line \\(y=b_0^{\\ast}+b_1^{\\ast}x\\), where the intercept and slope, \\(b_0^{\\ast}\\) and \\(b_1^{\\ast}\\), are merely generic values. For the \\(i\\)th observation, $y_i-( b_0{}+b_1{}x_i) $ represents the deviation of the observed value \\(y_i\\) from the line at \\(x_i\\). The quantity \\[\\begin{equation*} SS(b_0^{\\ast},b_1^{\\ast})=\\sum_{i=1}^{n}\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) ^{2} \\end{equation*}\\] represents the sum of squared deviations for this candidate line. The least squares method consists of determining the values of \\(b_0^{\\ast}\\) and \\(b_1^{\\ast}\\) that minimize \\(SS(b_0^{\\ast},b_1^{\\ast})\\). This is an easy problem that can be solved by calculus, as follows. Taking partial derivatives with respect to each argument yields \\[\\begin{equation*} \\frac{\\partial }{\\partial b_0^{\\ast}}SS(b_0^{\\ast},b_1^{\\ast})=\\sum_{i=1}^{n}(-2)\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) \\end{equation*}\\] and \\[\\begin{equation*} \\frac{\\partial }{\\partial b_1^{\\ast}}SS(b_0^{\\ast},b_1^{\\ast})=\\sum_{i=1}^{n}(-2x_i)\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) . \\end{equation*}\\] The reader is invited to take second partial derivatives to ensure that we are minimizing, not maximizing, this function. Setting these quantities equal to zero  and canceling constant terms yields \\[\\begin{equation*} \\sum_{i=1}^{n}\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) =0 \\end{equation*}\\] and \\[\\begin{equation*} \\sum_{i=1}^{n}x_i\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_i\\right) \\right) =0, \\end{equation*}\\] which are known as the normal equations. Solving these equations yields the values of \\(b_0^{\\ast}\\) and \\(b_1^{\\ast}\\) that minimize the sum of squares, as follows. Definition. The least squares intercept and slope estimates are \\[\\begin{equation*} b_1=r\\frac{s_y}{s_x}~~~~~\\mathrm{and}~~~~~b_0=\\overline{y}-b_1 \\overline{x}. \\end{equation*}\\] The line that they determine, \\(\\widehat{y}=b_0+b_1x\\), is called the fitted regression line. We have dropped the asterisk, or star, notation because \\(b_0\\) and \\(b_1\\) are no longer “candidate” values. Does this procedure yield a sensible line for our Wisconsin lottery sales? Earlier, we computed \\(r=0.886\\). From this and the basic summary statistics in Table 2.1, we have \\(b_1 = 0.886 \\left( 8,103\\right) /11,098=0.647\\) and \\(b_0 = 6,495-(0.647)9,311 = 469.7.\\) This yields the fitted regression line \\[\\begin{equation*} \\widehat{y} = 469.7 + (0.647)x. \\end{equation*}\\] The carat, or “hat,” on top of the \\(y\\) reminds us that this \\(\\widehat{y}\\), or \\(\\widehat{SALES}\\), is a fitted value. One application of the regression line is to estimate sales for a specific population say, \\(x=10,000\\). The estimate is the height of the regression line, which is \\(469.7 + (0.647)(10,000) = 6,939.7\\). Example: Summarizing Simulations. Regression analysis is a tool for summarizing complex data. In practical work, actuaries often simulate complicated financial scenarios; it is often overlooked that regression can be used to summarize relationships of interest. To illustrate, Manistre and Hancock (2005) simulated many realizations of a 10-year European put option and demonstrated the relationship between two actuarial risk measures, the value-at-risk (VaR) and the conditional tail expectation (CTE). For one example, these authors examined lognormally distributed stock returns with an initial stock price of $100, so that in 10 years the price of the stock would be distributed as \\[\\begin{equation*} S(Z)=100 \\exp \\left( (.08) 10 + .15 \\sqrt{10} Z \\right), \\end{equation*}\\] based on an annual mean return of 8%, standard deviation 15% and the outcome from a standard normal random variable \\(Z\\). The put option pays the difference between the strike price, that will be taken to be 110 for this example, and \\(S(Z)\\). The present value of this option is \\[\\begin{equation*} C(Z)= \\mathrm{e}^{-0.06(10)} \\mathrm{max} \\left(0, 110-S(Z) \\right), \\end{equation*}\\] based on a 6% discount rate. To estimate the VaR and CTE, for each \\(i\\), 1000 i.i.d. standard normal random variables were simulated and used to calculate 1000 present values, \\(C_{i1}, \\ldots, C_{i,1000}.\\) The 95th percentile of these present values is the estimate of the value at risk, denoted as \\(VaR_i.\\) The average of the highest 50 (\\(= (1-.05) \\times 1000\\)) of the present values is the estimate of the conditional tail expectation, denoted as \\(CTE_i\\). Manistre and Hancock (2005) performed this calculation \\(i=1, \\ldots, 1000\\) times; the result is presented in Figure 2.3. The scatterplot shows a strong but not perfect relationship between the \\(VaR\\) and the \\(CTE\\), the correlation coefficient turns out to be \\(r=0.782\\). Figure 2.3: Plot of Conditional Tail Expectation (CTE) versus Value at Risk (VaR). Based on \\(n=1,000\\) simulations from a 10-year European put bond. Source: Manistre and Hancock (2005). R Code to Produce Figure 2.3 # FIGURE 2.3 #simulation S&lt;-vector(mode=&quot;numeric&quot;,length=1000) C&lt;-vector(mode=&quot;numeric&quot;,length=1000) Var&lt;-vector(mode=&quot;numeric&quot;,length=1000) CTE&lt;-vector(mode=&quot;numeric&quot;,length=1000) for (i in 1:1000){ for (j in 1:1000){ S[j]&lt;-100*exp(.08*10+.15*(10^.5)*rnorm(1)) C[j]&lt;-exp(-.06*10)*max(0,110-S[j]) } C&lt;-sort(C) Var[i]&lt;-C[950] CTE[i]&lt;-mean(C[950:1000]) } model&lt;-lm(CTE~Var) b0&lt;-round(model$coef[1],digits=3) b1&lt;-round(model$coef[2],digits=3) R2&lt;-round(summary(model)$r.squared,digits=4) plot(Var, CTE, xlab=expression(paste(&quot;VaR Estimates&quot;)) , ylab=expression(paste(&quot;CTE Estimates&quot;)), xlim=c(0,12),ylim=c(8,20),xaxs=&quot;i&quot;,yaxs=&quot;i&quot;,pch=20,cex=0.4) lines(Var,model$fitted,lwd=.5) abline(h=c(10,12,14,16,18,20),col=&quot;grey&quot;) 2.2 Basic Linear Regression Model The scatter plot, correlation coefficient and the fitted regression line are useful devices for summarizing the relationship between two variables for a specific data set. To infer general relationships, we need models to represent outcomes of broad populations. This chapter focuses on a “basic linear regression” model. The “linear regression” part comes from the fact that we fit a line to the data. The “basic” part is because we use only one explanatory variable, \\(x\\). This model is also known as a “simple” linear regression. This text avoids this language because it gives the false impression that regression ideas and interpretations with one explanatory variable are always straightforward. We now introduce two sets of assumptions of the basic model, the “observables” and the “error” representations. They are equivalent but each will help us as we later extend regression models beyond the basics. \\[ {\\small \\begin{array}{l} \\hline \\hline &amp;\\textbf{Basic Linear Regression Model} \\\\ &amp;\\textbf{Observables Representation Sampling Assumptions} \\\\ \\hline \\text{F1}. &amp; \\mathrm{E}~y_i=\\beta_0 + \\beta_1 x_i . \\\\ \\text{F2}. &amp; \\{x_1,\\ldots ,x_n\\} \\text{ are non-stochastic variables}. \\\\ \\text{F3}. &amp; \\mathrm{Var}~y_i=\\sigma ^{2}. \\\\ \\text{F4}. &amp; \\{ y_i\\} \\text{ are independent random variables}. \\\\ \\hline\\ \\end{array} } \\] The “observables representation” focuses on variables that we can see (or observe), \\((x_i,y_i)\\). Inference about the distribution of \\(y\\) is conditional on the observed explanatory variables, so that we may treat \\(\\{x_1,\\ldots ,x_n\\}\\) as non-stochastic variables (assumption F2). When considering types of sampling mechanisms for \\((x_i,y_i)\\), it is convenient to think of a stratified random sampling scheme, where values of \\(\\{x_1,\\ldots ,x_n\\}\\) are treated as the strata, or group. Under stratified sampling, for each unique value of \\(x_i\\), we draw a random sample from a population. To illustrate, suppose you are drawing from a database of firms to understand stock return performance (\\(y\\)) and wish to stratify based on the size of the firm. If the amount of assets is a continuous variable, then we can imagine drawing a sample of size 1 for each firm. In this way, we hypothesize a distribution of stock returns conditional on firm asset size. Digression: You will often see reports that summarize results for the “top 50 managers” or the “best 100 universities,” measured by some outcome variable. In regression applications, make sure that you do not select observations based on a dependent variable, such as the highest stock return, because this is stratifying based on the \\(y\\), not the \\(x\\). Chapter 6 will discuss sampling procedures in greater detail. Stratified sampling also provides motivation for assumption F4, the independence among responses. One can motivate assumption F1 by thinking of \\((x_i,y_i)\\) as a draw from a population, where the mean of the conditional distribution of \\(y_i\\) given {\\(x_i\\)} is linear in the explanatory variable. Assumption F3 is known as homoscedasticity that we will discuss extensively in Section 5.7. See Goldberger (1991) for additional background on this representation. A fifth assumption that is often implicitly used is: This assumption is not required for many statistical inference procedures because central limit theorems provide approximate normality for many statistics of interest. However, formal justification for some, such as \\(t\\)-statistics, do require this additional assumption. In contrast to the observables representation, an alternative set of assumptions focuses on the deviations, or “errors,” in the regression, defined as \\(\\varepsilon_i=y_i-\\left( \\beta_0 + \\beta_1 x_i \\right)\\). \\[ {\\small \\begin{array}{l} \\hline \\hline &amp;\\textbf{Basic Linear Regression Model} \\\\ &amp;\\textbf{Error Representation Sampling Assumptions} \\\\ \\hline \\text{E1}. &amp; y_i=\\beta_0 + \\beta_1 x_i + \\varepsilon_i . \\\\ \\text{E2}. &amp; \\{x_1,\\ldots ,x_n\\} \\text{ are non-stochastic variables}. \\\\ \\text{E3}. &amp; \\mathrm{E}~\\varepsilon _i=0 \\text{ and } \\mathrm{Var}~\\varepsilon _i=\\sigma ^{2}. \\\\ \\text{E4}. &amp; \\{ \\varepsilon_i\\} \\text{ are independent random variables}. \\\\ \\hline\\ \\end{array} } \\] The “error representation” is based on the Gaussian theory of errors (see Stigler, 1986, for a historical background). Assumption E1 assumes that \\(y\\) is in part due to a linear function of the observed explanatory variable, \\(x\\). Other unobserved variables that influence the measurement of \\(y\\) are interpreted to be included in the “error” term \\(\\varepsilon _i\\), which is also known as the “disturbance” term. The independence of errors, E4, can be motivated by assuming that {\\(\\varepsilon _i\\)} are realized through a simple random sample from an unknown population of errors. Assumptions E1-E4 are equivalent to F1-F4. The error representation provides a useful springboard for motivating goodness of fit measures (Section 2.3). However, a drawback of the error representation is that it draws the attention from the observable quantities \\((x_i,y_i)\\) to an unobservable quantity, {\\(\\varepsilon _i\\)}. To illustrate, the sampling basis, viewing {\\(\\varepsilon _i\\)} as a simple random sample, is not directly verifiable because one cannot directly observe the sample {$ _i$}. Moreover, the assumption of additive errors in E1 will be troublesome when we consider nonlinear regression models. Figure 2.4 illustrates some of the assumptions of the basic linear regression model. The data (\\(x_1,y_1\\)), (\\(x_2,y_2\\)) and (\\(x_3,y_3\\)) are observed and are represented by the circular opaque plotting symbols. According to the model, these observations should be close to the regression line \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x\\). Each deviation from the line is random. We will often assume that the distribution of deviations may be represented by a normal curve, as in Figure 2.4. Figure 2.4: The distribution of the response varies by the level of the explanatory variable. The basic linear regression model assumptions describe the underlying population. Table 2.2 highlights the idea that characteristics of this population can be summarized by the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma ^{2}\\). In Section 2.1, we summarized data from a sample, introducing the statistics \\(b_0\\) and \\(b_1\\). Section 2.3 will introduce \\(s^{2}\\), the statistic corresponding to the parameter \\(\\sigma ^{2}\\). Table 2.2. Summary Measures of the Population and Sample \\[ {\\small \\begin{array}{llccc}\\hline\\hline &amp; \\text{Summary} \\\\ \\text{Data} &amp; \\text{Measures} &amp; \\text{Intercept} &amp; \\text{Slope} &amp; \\text{Variance} \\\\\\hline \\text{Population} &amp; \\text{Parameters} &amp; \\beta_0 &amp; \\beta_1 &amp; \\sigma^2 \\\\ \\text{sample} &amp; \\text{Statistics} &amp; b_0 &amp; b_1 &amp; s^2 \\\\ \\hline \\end{array} } \\] 2.3 Is the Model Useful? Some Basic Summary Measures Although statistics is the science of summarizing data, it is also the art of arguing with data. This section develops some of the basic tools used to justify the basic linear regression model. A scatter plot may provide strong visual evidence that \\(x\\) influences \\(y\\); developing numerical evidence will enable us to quantify the strength of the relationship. Further, numerical evidence will be useful when we consider other data sets where the graphical evidence is not compelling. 2.3.1 Partitioning the Variability The squared deviations, \\(\\left( y_i-\\overline{y}\\right) ^2\\), provide a basis for measuring the spread of the data. If we wish to estimate the \\(i\\)th dependent variable without knowledge of \\(x\\), then \\(\\overline{y}\\) is an appropriate estimate and \\(y_i- \\overline{y}\\) represents the deviation of the estimate. We use \\(Total~SS=\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^2\\), the total sum of squares, to represent the variation in all of the responses. Suppose now that we also have knowledge of \\(x\\), an explanatory variable. Using the fitted regression line, for each observation we can compute the corresponding fitted value, \\(\\widehat{y}_i = b_0 + b_1x_i\\). The fitted value is our estimate with knowledge of the explanatory variable. As before, the difference between the response and the fitted value, \\(y_i- \\widehat{y}_i\\), represents the deviation of this estimate. We now have two “estimates” of \\(y_i\\), these are \\(\\widehat{y}_i\\) and \\(\\overline{y}\\). Presumably, if the regression line is useful, then $ _i$ is a more accurate measure than \\(\\overline{y}\\). To judge this usefulness, we algebraically decompose the total deviation as: \\[\\begin{equation} \\begin{array}{ccccc} \\underbrace{y_i-\\overline{y}} &amp; = &amp; \\underbrace{y_i-\\widehat{y}_i} &amp; + &amp; \\underbrace{\\widehat{y}_i-\\overline{y}} \\\\ \\text{total} &amp; = &amp; \\text{unexplained} &amp; + &amp; \\text{explained} \\\\ \\text{deviation} &amp; &amp; \\text{deviation} &amp; &amp; \\text{deviation} \\\\ \\end{array} \\tag{2.1} \\end{equation}\\] Interpret this equation as “the deviation without knowledge of \\(x\\) equals the deviation with knowledge of \\(x\\) plus the deviation explained by \\(x\\).” Figure 2.5 is a geometric display of this decomposition. In the figure, an observation above the line was chosen, yielding a positive deviation from the fitted regression line, to make the graph easier to read. A good exercise is to draw a rough sketch corresponding to Figure 2.5 with an observation below the fitted regression line. Figure 2.5: Geometric display of the deviation decomposition. Now, from the algebraic decomposition in equation (2.1), square each side of the equation and sum over all observations. After a little algebraic manipulation, this yields \\[\\begin{equation} \\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^2=\\sum_{i=1}^{n}\\left( y_i-\\widehat{y}_i\\right) ^2+\\sum_{i=1}^{n}\\left( \\widehat{y}_i- \\overline{y}\\right) ^2. \\tag{2.2} \\end{equation}\\] We rewrite this as \\(Total~SS=Error~SS+Regression~SS\\) where \\(SS\\) stands for sum of squares. We interpret: \\(Total~SS\\) as the total variation without knowledge of \\(x\\), \\(Error~SS\\) as the total variation remaining after the introduction of \\(x\\), and \\(Regression~SS\\) as the difference between the \\(Total~SS\\) and \\(Error~SS\\) , or the total variation “explained” through knowledge of \\(x\\). When squaring the right-hand side of equation (2.1), we have the cross-product term \\(2\\left( y_i-\\widehat{y}_i\\right) \\left( \\widehat{y}_i-\\overline{y}\\right)\\). With the “algebraic manipulation,” one can check that the sum of the cross-products over all observations is zero. This result is not true for all fitted lines but is a special property of the least squares fitted line. In many instances, the variability decomposition is reported through only a single statistic. Definition. The coefficient of determination is denoted by the symbol \\(R^2\\), called “\\(R\\)-square, and defined as \\[\\begin{equation*} R^2=\\frac{Regression~SS}{Total~SS}. \\end{equation*}\\] We interpret \\(R^2\\) to be the proportion of variability explained by the regression line. In one extreme case where the regression line fits the data perfectly, we have \\(Error~SS=0\\) and \\(R^2=1\\). In the other extreme case where the regression line provides no information about the response, we have \\(Regression~SS=0\\) and \\(R^2=0.\\) The coefficient of determination is constrained by the inequalities \\(0 \\leq R^2 \\leq 1\\) with larger values implying a better fit. 2.3.2 The Size of a Typical Deviation: s In the basic linear regression model, the deviation of the response from the regression line, $y_i-( _0+_1x_i) $, is not an observable quantity because the parameters \\(\\beta_0\\) and \\(\\beta_1\\)  are not observed. However, by using estimators \\(b_0\\) and \\(b_1\\), we can approximate this deviation using \\[\\begin{equation*} e_i=y_i-\\widehat{y}_i=y_i-\\left( b_0+b_1x_i\\right) , \\end{equation*}\\] known as the residual. Residuals will be critical to developing strategies for improving model specification in Section 2.6. We now show how to use the residuals to estimate \\(\\sigma ^2\\). From a first course in statistics, we know that if one could observe the deviations \\(\\varepsilon _i\\), then a desirable estimate of \\(\\sigma ^2\\) would be \\((n-1)^{-1}\\sum_{i=1}^{n}\\left( \\varepsilon _i-\\overline{\\varepsilon }\\right) ^2\\). Because {\\(\\varepsilon _i\\)} are not observed, we use the following. Definition. An estimator of \\(\\sigma ^2\\), the mean square error (MSE), is defined as \\[\\begin{equation} s^2=\\frac{1}{n-2}\\sum_{i=1}^{n}e_i{}^2. \\tag{2.3} \\end{equation}\\] The positive square root, \\(s=\\sqrt{s^2},\\) is called the residual standard deviation. Comparing the definitions of \\(s^2\\) and \\((n-1)^{-1}\\sum_{i=1}^{n}\\left( \\varepsilon _i-\\overline{\\varepsilon }\\right) ^2\\), you will see two important differences. First, in defining \\(s^2\\) we have not subtracted the average residual from each residual before squaring. This is because the average residual is zero, a special property of least squares estimation (see Exercise 2.14). This result can be shown using algebra and is guaranteed for all data sets. Second, in defining \\(s^2\\) we have divided by \\(n-2\\) instead of \\(n-1\\). Intuitively, dividing by either \\(n\\) or \\(n-1\\) tends to underestimate \\(\\sigma ^2\\). The reason is that, when fitting lines to data, we need at least two observations to determine a line. For example, we must have at least three observations for there to be any variability about a line. How much “freedom” is there for variability about a line? We will say that the error degrees of freedom is the number of observations available, \\(n\\), minus the number of observations needed to determine a line, 2 (with symbols, \\(df=n-2\\)). However, as we saw in the least squares estimation subsection, we do not need to identify two actual observations to determine a line. The idea is that if an analyst knows the line and \\(n-2\\) observations, then the remaining two observations can be determined, without variability. When dividing by \\(n-2\\), it can be shown that \\(s^2\\) is an unbiased estimator of \\(\\sigma ^2\\). We can also express \\(s^2\\) in terms of the sum of squares quantities. That is, \\[\\begin{equation*} s^2=\\frac{1}{n-2}\\sum_{i=1}^{n}\\left( y_i-\\widehat{y}_i\\right) ^2= \\frac{Error~SS}{n-2}=MSE. \\end{equation*}\\] This leads us to the analysis of variance, or ANOVA, table: \\[ \\begin{array}{llcl} \\hline \\hline \\text{ANOVA Table} \\\\ \\hline \\text{Source} &amp; \\text{Sum of Squares} &amp; df &amp; \\text{Mean Square} \\\\ \\hline \\text{Regression} &amp; Regression~SS &amp; 1 &amp; Regression~MS \\\\ \\text{Error} &amp; Error~SS &amp; n-2 &amp; MSE \\\\ \\text{Total} &amp; Total~SS &amp; n-1 &amp; \\\\ \\hline \\hline \\end{array} \\] The ANOVA table is merely a bookkeeping device used to keep track of the sources of variability; it routinely appears in statistical software packages as part of the regression output. The mean square column figures are defined to be the sums of square (\\(SS\\)) figures divided by their respective degrees of freedom (\\(df\\)). In particular, the mean square for errors (\\(MSE\\)) equals $ s^2$ and the regression sum of squares equals the regression mean square. This latter property is specific to the regression with one variable case; it is not true where we consider more than one explanatory variable. The error degrees of freedom in the ANOVA table is \\(n-2\\). The total degrees of freedom is \\(n-1\\), reflecting the fact that the total sum of squares is centered about the mean (at least two observations are required for positive variability). The single degree of freedom associated with the regression portion means that the slope, plus one observation, is enough information to determine the line. This is because it takes two observations to determine a line and at least three observations for there to be any variability about the line. The analysis of variance table for the lottery data is: Sum of Squares \\(df\\) Mean Square Regression 2,527,165,015 1 2,527,165,015 Error 690,116,755 48 14,377,432 Total 3,217,281,770 49 R Code to Produce Lottery ANOVA Table #Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) model.basiclinearreg&lt;-lm(Lot$SALES ~ Lot$POP) #summary(model.basiclinearreg) ANOVA &lt;- anova(model.basiclinearreg) row1 &lt;- c(ANOVA$`Sum Sq`[1], ANOVA$`Df`[1], ANOVA$`Mean Sq`[1]) row2 &lt;- c(ANOVA$`Sum Sq`[2], ANOVA$`Df`[2], ANOVA$`Mean Sq`[2]) row3 &lt;- c(ANOVA$`Sum Sq`[1]+ANOVA$`Sum Sq`[2], ANOVA$`Df`[1] +ANOVA$`Df`[2], NaN) ANOVATable &lt;- rbind(row1, row2, row3) ANOVATable1 &lt;- format(round(ANOVATable, digits = 0), big.mark = &#39;,&#39;) ANOVATable1[3,3] &lt;- &quot;&quot; rownames(ANOVATable1) &lt;- c(&quot;Regression&quot;, &quot;Error&quot;, &quot;Total&quot;) colnames(ANOVATable1) &lt;- c(&quot;Sum of Squares&quot;, &quot;$df$&quot;, &quot;Mean Square&quot;) kable(ANOVATable1, align = &#39;r&#39;) %&gt;% kable_styling(position = &quot;center&quot;, full_width = FALSE) %&gt;% kableExtra::kable_classic(font = 12, html_font = &quot;Cambria&quot;) From this table, you can check that \\(R^2=78.5\\%\\) and \\(s=3,792.\\) 2.4 Properties of Regression Coefficient Estimators The least squares estimates can be expressed as weighted sum of the responses. To see this, define the weights \\[\\begin{equation*} w_i=\\frac{x_i-\\overline{x}}{s_x^2(n-1)}. \\end{equation*}\\] Because the sum of \\(x\\)-deviations (\\(x_i-\\overline{x}\\)) is zero, we see that \\(\\sum_{i=1}^{n}w_i=0\\). Thus, we can express the slope estimate \\[\\begin{equation} b_1=r\\frac{s_y}{s_x}=\\frac{1}{(n-1)s_x^2}\\sum_{i=1}^{n}\\left( x_i-\\overline{x}\\right) \\left( y_i-\\overline{y}\\right) =\\sum_{i=1}^{n}w_i\\left( y_i-\\overline{y}\\right) =\\sum_{i=1}^{n}w_iy_i. \\tag{2.4} \\end{equation}\\] The exercises ask the reader to verify that \\(b_0\\) can also be expressed as a weighted sum of responses, so our discussion pertains to both regression coefficients. Because regression coefficients are weighted sums of responses, they can be affected dramatically by unusual observations (see Section 2.6). Because \\(b_1\\) is a weighted sum, it is straightforward to derive the expectation and variance of this statistic. By the linearity of expectations and Assumption F1, we have \\[\\begin{equation*} \\mathrm{E}~b_1=\\sum_{i=1}^{n}w_i~\\mathrm{E}~y_i=\\beta_0\\sum_{i=1}^{n}w_i+\\beta_1\\sum_{i=1}^{n}w_ix_i=\\beta_1. \\end{equation*}\\] That is, \\(b_1\\) is an unbiased estimator of \\(\\beta_1\\). Here, the sum $ _{i=1}^{n}w_ix_i$ \\(=\\) \\(\\left[ s_x^2(n-1)\\right] ^{-1}\\sum_{i=1}^{n}\\left( x_i-\\overline{x}\\right) x_i\\) \\(=\\left[s_x^2(n-1)\\right] ^{-1}\\sum_{i=1}^{n}\\left( x_i-\\overline{x}\\right) ^2=1.\\) From the definition of the weights, some easy algebra also shows that \\(\\sum_{i=1}^{n}w_i^2=1/\\left( s_x^2(n-1)\\right)\\). Further, the independence of the responses implies that the variance of the sum is the sum of the variances, and thus we have \\[\\begin{equation*} \\mathrm{Var}~b_1 =\\sum_{i=1}^{n}w_i^2\\mathrm{Var}~y_i=\\frac{\\sigma^2}{s_x^2(n-1)}. \\end{equation*}\\] Replacing \\(\\sigma ^2\\) by its estimator \\(s^2\\) and taking square roots leads to the following. Definition. The standard error of \\(b_1\\), the estimated standard deviation of \\(b_1\\), is defined as \\[\\begin{equation} se(b_1)=\\frac{s}{s_x\\sqrt{n-1}}. \\tag{2.5} \\end{equation}\\] This is our measure of the reliability, or precision, of the slope estimator. Using equation (2.5), we see that \\(se(b_1)\\) is determined by three quantities, \\(n\\), \\(s\\) and \\(s_x\\), as follows: If we have more observations so that \\(n\\) becomes larger, then $ se(b_1)$ becomes smaller, other things equal. If the observations have a greater tendency to lie closer to the line so that \\(s\\) becomes smaller, then \\(se(b_1)\\) becomes smaller, other things equal. If values of the explanatory variable become more spread out so that $ s_x$ increases, then \\(se(b_1)\\) becomes smaller, other things equal. Smaller values of \\(se(b_1)\\) offer a better opportunity to detect relations between \\(y\\) and \\(x\\). Figure 2.6 illustrates these relationships. Here, the scatter plot in the middle has the smallest value of \\(se(b_1)\\). Compared with the middle plot, the left-hand plot has a larger value of \\(s\\) and thus \\(se(b_1)\\). Compared with the right-hand plot, the middle plot has a larger \\(s_x\\), and thus smaller value of \\(se(b_1)\\). Figure 2.6: These three scatter plots exhibit the same linear relationship between \\(y\\) and \\(x\\). The plot on the left exhibits greater variability about the line than the plot in the middle. The plot on the right exhibits a smaller standard deviation in \\(x\\) than the plot in the middle. R Code to Produce Figure 2.6 # FIGURE 2.6 HERE par(mfrow=c(1, 3),mar=c(3.8,2.8,1,1), cex=1.3) x &lt;- c(1,2,2.3,2.5,1.5,1.7,2.6,2.8,.9,.88,.8,1.2,1.3,1.45,1.8,2.2,2.1) y &lt;- c(.5,2.2,2.6,2.5,.8,1.5,2.3,2.4,.75,.7,1.3,1.5,1.7,2.3,2.3,2.7,1.25) plot(x,y, xlim=c(0.5,3), ylim=c(0,3.5), bty=&quot;l&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab=&quot;&quot;) mtext(&quot;y&quot;, side=2, at=3.5, line=2, las=1, cex=1.3) mtext(&quot;x&quot;, side=1, line=2, cex=1.3) a &lt;- seq(.75,2.75, by = .001) b = a lines(a,b) x &lt;- c(1,2,2.3,2.5,1.5,1.7,2.6,2.8,.9,.88,.8,1.2,1.3,1.45,1.8,2.2,2.45) y &lt;- c(1,2,2.3,2.5,1.2,1.6,2.4,2.6,1.1,1.11,1.2,1.3,1.45,1.6,1.95,2.3,2.7) plot(x,y, xlim=c(0.5,3), ylim=c(0,3.5), bty=&quot;l&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab=&quot;&quot;) mtext(&quot;y&quot;, side=2, at=3.5, line=2, las=1, cex=1.3) mtext(&quot;x&quot;, side=1, line=2, cex=1.3) a &lt;- seq(.75,2.75, by = .001) b = a lines(a,b) x &lt;- c(1,2,2.3,2.5,1.5,1.7,2.6,2.8,2.6,1.5,2,1.2,1.3,1.45,1.8,2.2,2.45) y &lt;- c(1,2,2.3,2.5,1.2,1.6,2.4,2.6,2,2,2.4,1.3,1.55,1.6,1.95,1.6,2.7) plot(x,y, xlim=c(-1.5,5), ylim=c(-2,5.5), bty=&quot;l&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, xlab=&quot;&quot;) mtext(&quot;y&quot;, side=2, at=5.1, line=2, las=1, cex=1.3) mtext(&quot;x&quot;, side=1, line=2, cex=1.3) a &lt;- seq(-.5,4.5, by = .001) b = a lines(a,b) Equation (2.4) also implies that the regression coefficient $b_1 $ is normally distributed. That is, recall from mathematical statistics that linear combinations of normal random variables are also normal. Thus, if Assumption F5 holds, then \\(b_1\\) is normally distributed. Moreover, several versions of central limit theorems exists for weighted sums (see, for example, Serfling, 1980). Thus, as discussed in Section 1.4, if the responses \\(y_i\\) are even approximately normally distributed, then it will be reasonable to use a normal approximation for the sampling distribution of \\(b_1\\). Using \\(se(b_1)\\) as the estimated standard deviation of \\(b_1\\), for large values of \\(n\\) we have that \\(\\left( b_1-\\beta_1\\right) /se(b_1)\\) has an approximate standard normal distribution. Although we will not prove it here, under Assumption F5 \\(\\left( b_1-\\beta_1\\right) /se(b_1)\\) follows a $t $-distribution with degrees of freedom \\(df=n-2\\). 2.5 Statistical Inference Having fit a model with a data set, we can make a number of important statements. Generally, it is useful to think about these statements in three categories: (i) tests of hypothesized ideas, (ii) estimates of model parameters and (ii) predictions of new outcomes. 2.5.1 Is the Explanatory Variable Important?: The t-Test We respond to the question of whether the explanatory variable is important by investigating whether or not \\(\\beta_1=0\\). The logic is that if \\(\\beta_1=0\\), then the basic linear regression model no longer includes an explanatory variable \\(x\\). Thus, we translate our question of the importance of the explanatory variable into a narrower question that can be answered using the hypothesis testing framework. This narrower question is, is $ H_0:_1=0$ valid? We respond to this question by looking at the test statistic: \\[\\begin{equation*} t-\\mathrm{ratio}=\\frac{\\mathrm{estimator-hypothesized~value~of~parameter}} {\\mathrm{standard~error~of~the~estimator}}. \\end{equation*}\\] For the case of \\(H_0:\\beta_1=0\\) , we examine \\(t\\)-ratio \\(t(b_1)=b_1/se(b_1)\\) because the hypothesized value of \\(\\beta_1\\) is 0. This is the appropriate standardization because, under the null hypothesis and the model assumptions described in Section 2.4, the sampling distribution of \\(t(b_1)\\) can be shown to be the \\(t\\)-distribution with $ df=n-2$ degrees of freedom. Thus, to test the null hypothesis \\(H_0\\) against the alternative \\(H_{a}:\\beta_1\\neq 0\\), we reject \\(H_0\\) if favor of \\(H_{a}\\) if \\(|t(b_1)|\\) exceeds a \\(t\\)-value. Here, this \\(t\\)-value is a percentile from the \\(t\\)-distribution using \\(df=n-2\\) degrees of freedom.  We denote the significance level as $$  and this \\(t\\)-value as \\(t_{n-2,1-\\alpha /2}\\). Example: Lottery Sales - Continued. For the lottery sales example, the residual standard deviation is \\(s=3,792\\). From Table 2.1, we have \\(s_x = 11,098\\). Thus, the standard error of the slope is \\(se(b_1) = 3792/(11098\\sqrt{50-1})=0.0488\\). From Section 2.1, the slope estimate is \\(b_1=0.647\\). Thus, the \\(t\\)-statistic is \\(t(b_1) = 0.647/0.0488 = 13.4\\). We interpret this by saying that the slope is 13.4 standard errors above zero. For the significance level, we use the customary value of \\(\\alpha\\) = 5%. The 97.5th percentile from a \\(t\\)-distribution with \\(df=50-2=48\\) degrees of freedom is \\(t_{48,0.975}=2.011\\). Because \\(|13.4|&gt;2.011\\), we reject the null hypothesis that the slope \\(\\beta_1 = 0\\) in favor of the alternative that \\(\\beta_1 \\neq 0\\). Making decisions by comparing a \\(t\\)-ratio to a \\(t\\)-value is called a \\(t\\)-test. Testing \\(H_0:\\beta_1=0\\) versus \\(H_{a}:\\beta_1\\neq 0\\) is just one of many hypothesis tests that can be performed, although it is the most common. Table 2.3 outlines alternative decision-making procedures. These procedures are for testing \\(H_0:\\beta_1 = d\\) where \\(d\\) is a user-prescribed value that may be equal to zero or any other known value. For example, in our Section 2.7 example, we will use \\(d=1\\) to test financial theories about the stock market. Table 2.3 Decision-Making Procedures for Testing \\(H_0:\\beta_1 = d\\) \\[ \\begin{array}{c|c} \\hline \\text{Alternative Hypothesis} (H_{a}) &amp; \\text{Procedure: Reject } H_0 \\text{ in favor of } H_{a} \\text{ if} \\\\ \\hline \\beta_1&gt;d &amp; t-\\mathrm{ratio}&gt;t_{n-2,1-\\alpha }. \\\\ \\beta_1&lt;d &amp; t-\\mathrm{ratio}&lt;-t_{n-2,1-\\alpha }. \\\\ \\beta_1\\neq d &amp; |t-\\mathrm{ratio}\\mathit{|}&gt;t_{n-2,1-\\alpha /2}. \\\\ \\end{array} \\\\ {\\small \\begin{array}{l} \\hline \\text{Notes: The significance level is } \\alpha . \\text{Here, }t_{n-2,1-\\alpha} \\text{ the } (1-\\alpha )th \\text{ percentile}\\\\ ~~\\text{from the } t-\\text{distribution using } df=n-2 \\text{ degrees of freedom}.\\\\ ~~\\text{The test statistic is }t-\\mathrm{ratio} = (b_1 -d)/se(b_1) . \\\\ \\hline \\end{array} } \\] Alternatively, one can construct probability (\\(p\\)-) values and compare these to given significant levels. The \\(p\\)-value is a useful summary statistic for the data analyst to report since it allows the report reader to understand the strength of the deviation from the null hypothesis. Table 2.4 summarizes the procedure for calculating \\(p\\)-values. Table 2.4 Probability Values for Testing \\(H_0:\\beta_1 = d\\) \\[ \\begin{array}{c|ccc} \\hline \\text{Alternative} &amp; &amp; &amp; \\\\ \\text{Hypothesis} (H_a) &amp; \\beta_1&gt;d &amp; \\beta_1&lt;d &amp; \\beta_1\\neq d \\\\ \\hline p-value &amp; \\Pr(t_{n-2}&gt;t-\\mathrm{ratio}) &amp; \\Pr(t_{n-2}&lt;t-\\mathrm{ratio}) &amp; \\Pr (|t_{n-2}|&gt;|t-\\mathrm{ratio}\\mathit{|}) \\\\\\hline \\end{array} \\\\ {\\small \\begin{array}{l} \\hline \\text{Notes: Here, }t_{n-2} \\text{ is a } t-\\text{distributed random variable with } df=n-2 \\text{ degrees of freedom.}\\\\ ~~\\text{The test statistic is }t-\\mathrm{ratio} = (b_1 -d)/se(b_1) . \\\\ \\hline \\end{array} } \\] Another interesting way of addressing the question of the importance of an explanatory variable is through the correlation coefficient. Remember that the correlation coefficient is a measure of linear relationship between \\(x\\) and \\(y\\). Let’s denote this statistic by \\(r(y,x)\\). This quantity is unaffected by scale changes in either variable. For example, if we multiply the \\(x\\) variable by the number \\(b_1\\), then the correlation coefficient remains unchanged. Further, correlations are unchanged by additive shifts. Thus, if we add a number, say \\(b_0\\), to each \\(x\\) variable, then the correlation coefficient remains unchanged. Using a scale change and an additive shift on the \\(x\\) variable can be used to produce the fitted value \\(\\widehat{y}=b_0+b_1x\\). Thus, using notation, we have \\(|r(y,x)|=r(y,\\widehat{y}).\\) We may thus interpret the correlation between the responses and the explanatory variable to be equal to the correlation between the responses and the fitted values. This leads then to the following interesting algebraic fact, \\(R^2=r^2.\\) That is, the coefficient of determination equals the correlation coefficient squared. This is much easier to interpret if one thinks of \\(r\\) as the correlation between observed and fitted values. See Exercise 2.13 for steps useful in confirming this result. 2.5.2 Confidence Intervals Investigators often cite the formal hypothesis testing mechanism to respond to the question “Does the explanatory variable have a real influence on the response?” A natural follow-up question is “To what extent does \\(x\\) affect \\(y\\)?” To a certain degree, one could respond using the size of the \\(t\\)-ratio or the \\(p\\)-value. However, in many instances a confidence interval for the slope is more useful. To introduce confidence intervals for the slope, recall that \\(b_1\\) is our point estimator of the true, unknown slope \\(\\beta_1\\). Section 2.4 argued that this estimator has standard error \\(se(b_1)\\) and that \\(\\left( b_1-\\beta_1\\right) /se(b_1)\\) follows a \\(t\\)-distribution with \\(n-2\\) degrees of freedom. Probability statements can be inverted to yield confidence intervals. Using this logic, we have the following confidence interval for the slope \\(\\beta_1\\). Definition. A \\(100(1-\\alpha)\\)% confidence interval for the slope \\(\\beta_1\\) is \\[\\begin{equation} b_1\\pm t_{n-2,1-\\alpha /2} ~se(b_1). \\tag{2.6} \\end{equation}\\] As with hypothesis testing, \\(t_{n-2,1-\\alpha /2}\\) is the (1-$ $/2)th percentile from the \\(t\\)-distribution with \\(df=n-2\\) degrees of freedom. Because of the two-sided nature of confidence intervals, the percentile is 1 - (1 - confidence level) / 2. In this text, for notational simplicity we generally use a 95% confidence interval, so the percentile is 1-(1-0.95)/2 = 0.975. The confidence interval provides a range of reliability that measures the usefulness of the estimate. In Section 2.1, we established that the least squares slope estimate for the lottery sales example is \\(b_1=0.647\\). The interpretation is that if a zip code’s population differs by 1,000, then we expect mean lottery sales to differ by $647. How reliable is this estimate? It turns out that \\(se(b_1)=0.0488\\) and thus an approximate 95% confidence interval for the slope is \\[\\begin{equation*} 0.647\\pm (2.011)(.0488), \\end{equation*}\\] or (0.549, 0.745). Similarly, if population differs by 1,000, a 95% confidence interval for the expected change in sales is (549, 745). Here, we use the \\(t\\)-value \\(t_{48,0.975}=2.011\\) because there are 48 (= $ n$-2) degrees of freedom and, for a 95% confidence interval, we need the 97.5th percentile. 2.5.3 Prediction Intervals In Section 2.1, we showed how to use least squares estimators to predict the lottery sales for a zip code, outside of our sample, having a population of 10,000. Because prediction is such an important task for actuaries, we formalize the procedure so that it can be used on a regular basis. To predict an additional observation, we assume that the level of explanatory variable is known and is denoted by \\(x_{\\ast}\\). For example, in our previous lottery sales example we used \\(x_{\\ast} = 10,000\\). We also assume that the additional observation follows the same linear regression model as the observations in the sample. Using our least square estimators, our point prediction is \\(\\widehat{y}_{\\ast} = b_0 + b_1 x_{\\ast}\\), the height of the fitted regression line at \\(x_{\\ast}\\) We may decompose the prediction error into two parts: It can be shown that the standard error of the prediction is \\[\\begin{equation*} se(pred) = s \\sqrt{1+\\frac{1}{n}+\\frac{\\left( x_{\\ast}-\\overline{x}\\right) ^2}{(n-1)s_x^2}}. \\end{equation*}\\] As with \\(se(b_1)\\), the terms \\(n^{-1}\\) and $( x_{}- ) ^2/$ become close to zero as the sample size \\(n\\) becomes large. Thus, for large \\(n\\), we have that \\(se(pred)\\approx s\\), reflecting that the error in estimating the regression line at a point becomes negligible and deviation of the additional response from its mean becomes the entire source of uncertainty. Definition. A \\(100(1-\\alpha)\\)% prediction interval at \\(x_{\\ast}\\) is \\[\\begin{equation} \\widehat{y}_{\\ast} \\pm t_{n-2,1-\\alpha /2} ~se(pred) \\tag{2.7} \\end{equation}\\] where the \\(t\\)-value \\(t_{n-2,1-\\alpha /2}\\) is the same as used for hypothesis testing and the confidence interval. For example, the point prediction at \\(x_{\\ast} = 10,000\\) is \\(\\widehat{y}_{\\ast}\\)= 469.7 + 0.647 (10000) = 6,939.7. The standard error of this prediction is \\[\\begin{equation*} se(pred) = 3,792 \\sqrt{1+\\frac{1}{50} + \\frac{\\left( 10,000-9,311\\right)^2}{(50-1)(11,098)^2}} = 3,829.6. \\end{equation*}\\] With a \\(t\\)-value equal to 2.011, this yields an approximate 95% prediction interval \\[\\begin{equation*} 6,939.7 \\pm (2.011)(3,829.6) = 6,939.7 \\pm 7,701.3 = (-761.6, ~14,641.0). \\end{equation*}\\] We interpret these results by first pointing out that our best estimate of lottery sales for a zip code with a population of 10,000 is 6,939.70. Our 95% prediction interval represents a range of reliability for this prediction. If we could see many zip codes, each with a population of 10,000, on average we expect about 19 out of 20, or 95%, would have lottery sales between 0 and 14,641. It is customary to truncate the lower bound of the prediction interval to zero if negative values of the response are deemed to be inappropriate. R Code to Produce Section 2.5 Analyses # SECTION 2.1 OUTPUT model.basiclinearreg&lt;-lm(SALES ~ POP, data = Lot) summary(model.basiclinearreg) # SECTION 2.5.2 CONFIDENCE INTERVALS confint(model.basiclinearreg) confint(model.basiclinearreg, level=.90) # SECTION 2.5.3 PREDICTION INTERVALS newdata &lt;- data.frame(POP &lt;- 10000) predict(model.basiclinearreg, newdata, interval=&quot;prediction&quot;) predict(model.basiclinearreg, newdata, interval=&quot;prediction&quot;, level=.90) # PROVIDES THE 97.5TH PERCENTILE OF A T-DISTRIBUTION, JUST FOR CHECKING qt(.975, 48) 2.6 Building a Better Model: Residual Analysis Quantitative disciplines calibrate models with data. Statistics takes this one step further, using discrepancies between the assumptions and the data to improve model specification. We will examine the Section 2.2 modeling assumptions in light of the data and use any mismatch to specify a better model; this process is known as diagnostic checking (like when you go to a doctor and he or she performs diagnostic routines to check your health). We will begin with the Section 2.2 error representation. Under this set of assumptions, the deviations {\\(\\varepsilon _i\\)} are identically and independently distributed (i.i.d), and under assumption F5, normally distributed. To assess the validity of these assumptions, one uses (observed) residuals {\\(e_i\\)} as approximations for the (unobserved) deviations {\\(\\varepsilon _i\\)}. The basic theme is that if the residuals are related to a variable or display any other recognizable pattern, then we should be able to take advantage of this information and improve our model specification. The residuals should contain little or no information and represent only natural variation from the sampling that cannot be attributed to any specific source. Residual analysis is the exercise of checking the residuals for patterns. There are five types of model discrepancies that analysts commonly look for. If detected, the discrepancies can be corrected with the appropriate adjustments in the model specification. Model Misspecification Issues Lack of Independence. There may exist relationships among the deviations {\\(\\varepsilon _i\\)} so that they are not independent. Heteroscedasticity. Assumption E3 that indicates that all observations have a common (although unknown) variability, known as homoscedasticity. Heteroscedascity is the term used when the variability varies by observation. Relationships between Model Deviations and Explanatory Variables. If an explanatory variable has the ability to help explain the deviation $$, then one should be able to use this information to better predict \\(y\\). Nonnormal Distributions. If the distribution of the deviation represents a serious departure from normality, then the usual inference procedures are no longer valid. Unusual Points. Individual observations may have a large effect on the regression model fit, meaning that the results may be sensitive to the impact of a single observation. \\end{enumerate} This list will serve the reader throughout your study of regression analysis. Of course, with only an introduction to basic models we have not yet seen alternative models that might be used when we encounter these model discrepancies. In this book’s Part II on time series models, we will study lack of independence among data ordered over time. Chapter 5 will consider heteroscedasticity in further detail. The introduction to multiple linear regression in Chapter 3 will be our first look at handling relationships between {\\(\\varepsilon _i\\)} and additional explanatory variables. We have, however, already had an introduction to the effect of normal distributions, seeing that \\(qq\\) plots can detect non-normality and that transformations can help induce approximate normality. In this section, we discuss the effects of unusual points. Much of residual analysis is done by examining a standardized residual, a residual divided by its standard error. An approximate standard error of the residual is \\(s\\); in Chapter 3 we will give a precise mathematical definition. There are two reasons why we often examine standardized residuals in lieu of basic residuals. First, if responses are normally distributed, then standardized residuals are approximately realizations from a standard normal distribution. This provides a reference distribution to compare values of standardized residuals. For example, if a standardized residual exceeds two in absolute value, this is considered unusually large and the observation is called an outlier. Second, because standardized residuals are dimensionless, we get carryover of experience from one data set to another. This is true regardless of whether or not the normal reference distribution is applicable. Outliers and High Leverage Points Another important part of residual analysis is the identification of unusual observations in a data set. Because regression estimates are weighted averages with weights that vary by observation, some observations are more important than others. This weighting is more important than many users of regression analysis realize. In fact, the example below demonstrates that a single observation can have a dramatic effect in a large data set. There are two directions in which a data point can be unusual, the horizontal and vertical directions. By “unusual,” we mean that an observation under consideration seems to be far from the majority of the data set. An observation that is unusual in the vertical direction is called an outlier. An observation that is unusual in the horizontal directional is called a high leverage point. An observation may be both an outlier and a high leverage point. Example: Outliers and High Leverage Points. Consider the fictitious data set of 19 points plus three points, labeled A, B, and C, given in Figure 2.7 and Table 2.5. Think of the first 19 points as “good” observations that represent some type of phenomena. We want to investigate the effect of adding a single aberrant point. Table 2.5. 19 Base Points Plus Three Types of Unusual Observations \\[ \\small{ \\begin{array}{c|cccccccccc|ccc} \\hline Variables &amp; &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp; &amp; A &amp; B &amp; C \\\\ \\hline x &amp; 1.5 &amp; 1.7 &amp; 2.0 &amp; 2.2 &amp; 2.5 &amp; 2.5 &amp; 2.7 &amp; 2.9 &amp; 3.0 &amp; 3.5 &amp; 3.4 &amp; 9.5 &amp; 9.5 \\\\ y &amp; 3.0 &amp; 2.5 &amp; 3.5 &amp; 3.0 &amp; 3.1 &amp; 3.6 &amp; 3.2 &amp; 3.9 &amp; 4.0 &amp; 4.0 &amp; 8.0 &amp; 8.0 &amp; 2.5 \\\\ \\hline x &amp; 3.8 &amp; 4.2 &amp; 4.3 &amp; 4.6 &amp; 4.0 &amp; 5.1 &amp; 5.1 &amp; 5.2 &amp; 5.5 &amp; &amp; &amp; &amp; \\\\ y &amp; 4.2 &amp; 4.1 &amp; 4.8 &amp; 4.2 &amp; 5.1 &amp; 5.1 &amp; 5.1 &amp; 4.8 &amp; 5.3 &amp; &amp; &amp; &amp; \\\\ \\hline \\end{array} } \\] Figure 2.7: Scatterplot of 19 base plus three unusual points, labeled A, B and C. R Code to Produce Figure 2.7 # SECTION 2.6 OUTLIER EXAMPLE OUTLR &lt;- read.csv(&quot;CSVData/OutlierExample.csv&quot;, header=TRUE) # FIGURE 2.7 par(mar=c(4.1,3.1,1.1,.1), cex=1.3) plot(OUTLR$X, OUTLR$Y, xlab=&quot;x&quot;, ylab=&quot;&quot;, xlim=c(0, 10), ylim=c(2, 9), las=1) mtext(&quot;y&quot;, at=5.5,side=2,las=1,cex=1.3, line=2.3) points(4.3, 8.0) text(4.7, 8.0, &quot;A&quot;, cex=1.3) points(9.5, 8.0) text(9.9, 8.0, &quot;B&quot;, cex=1.3) points(9.5, 2.5) text(9.9, 2.5, &quot;C&quot;, cex=1.3) To investigate the effect of each type of aberrant point, Table 2.6 summarizes the results of four separate regressions. The first regression is for the nineteen base points. The other three regressions use the nineteen base points plus each type of unusual observation. Table 2.6. Results from Four Regressions \\[ \\begin{array}{l|rrrrr} \\hline Data &amp; b_0 &amp; b_1 &amp; s &amp; R^2(\\%) &amp; t(b_1) \\\\ \\hline 19 \\text{ Base Points} &amp; 1.869 &amp; 0.611 &amp; 0.288 &amp; 89.0 &amp; 11.71 \\\\ 19 \\text{ Base Points} ~+~ A &amp; 1.750 &amp; 0.693 &amp; 0.846 &amp; 53.7 &amp; 4.57 \\\\ 19 \\text{ Base Points} ~+~ B &amp; 1.775 &amp; 0.640 &amp; 0.285 &amp; 94.7 &amp; 18.01 \\\\ 19 \\text{ Base Points} ~+~ C &amp; 3.356 &amp; 0.155 &amp; 0.865 &amp; 10.3 &amp; 1.44 \\\\ \\hline \\end{array} \\] Table 2.6 shows that a regression line provides a good fit for the nineteen base points. The coefficient of determination, \\(R^2\\), indicates about 89% of the variability has been explained by the line. The size of the typical error, \\(s\\), is about 0.29, small compared to the scatter in the \\(y\\)-values. Further, the \\(t\\)-ratio for the slope coefficient is large. When the outlier point A is added to the nineteen base points, the situation deteriorates dramatically. The \\(R^2\\) drops from 89% to 53.7% and \\(s\\) increases from about 0.29 to about 0.85. The fitted regression line itself does not change that much even though our confidence in the estimates has decreased. An outlier is unusual in the \\(y\\)-value, but “unusual in the \\(y\\)-value” depends on the \\(x\\)-value. To see this, keep the \\(y\\)-value of Point A the same, but increase the \\(x\\)-value and call the point B. When the point B is added to the nineteen base points, the regression line provides a better fit. Point B is close to being on the line of the regression fit generated by the nineteen base points. Thus, the fitted regression line and the size of the typical error, \\(s\\), do not change much. However, \\(R^2\\) increases from 89% to nearly 95 percent. If we think of $ R^2$ as \\(1-(Error~SS)/(Total~SS)\\), by adding point B we have increased $ Total~SS$, the total squared deviations in the \\(y\\)’s, even though leaving $ Error~SS$ relatively unchanged. Point B is not an outlier, but it is a high leverage point. To show how influential this point is, drop the \\(y\\)-value considerably and call this the new point C. When this point is added to the nineteen base points, the situation deteriorates dramatically. The \\(R^2\\) coefficient drops from 89% to 10%, and the \\(s\\) more than triples, from 0.29 to 0.87. Further, the regression line coefficients change dramatically. Most users of regression at first do not believe that one point in twenty can have such a dramatic effect on the regression fit. The fit of a regression line can always be improved by removing an outlier. If the point is a high leverage point and not an outlier, it is not clear whether the fit will be improved when the point is removed. Simply because you can dramatically improve a regression fit by omitting an observation does not mean you should always do so! The goal of data analysis is to understand the information in the data. Throughout the text, we will encounter many data sets where the unusual points provide some of the most interesting information about the data. The goal of this subsection is to recognize the effects of unusual points; Chapter 5 will provide options for handling unusual points in your analysis. All quantitative disciplines, such as accounting, economics, linear programming, and so on, practice the art of sensitivity analysis. Sensitivity analysis is a description of the global changes in a system due to a small local change in an element of the system. Examining the effects of individual observations on the regression fit is a type of sensitivity analysis. Example: Lottery Sales – Continued. Figure 2.8 exhibits an outlier; the point in the upper left-hand side of the plot represents a zip code that includes Kenosha, Wisconsin. Sales for this zip code are unusually high given its population. Kenosha is close to the Illinois border; residents from Illinois probably participate in the Wisconsin lottery thus effectively increasing the potential pool of sales in Kenosha. Table 2.7 summarizes the regression fit both with and without this zip code. Table 2.7. Regression Results with and without Kenosha \\[ \\small{ \\begin{array}{l|rrrrr} \\hline \\text{Data} &amp; b_0 &amp; b_1 &amp; s &amp; R^2(\\%) &amp; t(b_1) \\\\ \\hline \\text{With Kenosha} &amp; 469.7 &amp; 0.647 &amp; 3,792 &amp; 78.5 &amp; 13.26 \\\\ \\text{Without Kenosha} &amp; -43.5 &amp; 0.662 &amp; 2,728 &amp; 88.3 &amp; 18.82 \\\\ \\hline \\end{array} } \\] Figure 2.8: Scatter plot of SALES versus POP, with the outlier corresponding to Kenosha marked. R Code to Produce Figure 2.8 and Table 2.7 Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURE 2.8 par(mar=c(4.1,3.9,2,1),cex=1.1) plot(Lot$POP, Lot$SALES, ylab=&quot;&quot;, las=1, xlab = &quot;POP&quot;) mtext(&quot;SALES&quot;,side=2, at=36000, las=1, cex=1.1) text(5000, 24000, &quot;Kenosha&quot;) # TABLE 2.7 model.basiclinearreg&lt;-lm(SALES ~ POP, Lot) summary(model.basiclinearreg) model.Kenosha&lt;-lm(SALES ~ POP, Lot, subset=-c(9)) summary(model.Kenosha) For the purposes of inference about the slope, the presence of Kenosha does not alter the results dramatically. Both slope estimates are qualitatively similar and the corresponding \\(t\\)-statistics are very high, well above cut-offs for statistical significance. However, there are dramatic differences when assessing the quality of the fit. The coefficient of determination, \\(R^2\\), increased from 78.5% to 88.3% when deleting Kenosha. Moreover, our “typical deviation” \\(s\\) dropped by over $1,000. This is particularly important if we wish to tighten our prediction intervals. To check the accuracy of our assumptions, it is also customary to check the normality assumption. One way of doing this is the \\(qq\\) plot, introduced in Section 1.2. The two panels in Figures 2.9 are \\(qq\\) plots with and without the Kenosha zip code. Recall that points “close” to linear indicate approximate normality. In the right-hand panel of Figure 2.9, the sequence does appear to be linear so that residuals are approximately normally distributed. This is not the case in the left-hand panel, where the sequence of points appears to climb dramatically for large quantiles. The interesting thing is that the non-normality of the distribution is due to a single outlier, not a pattern of skewness that is common to all the observations. Figure 2.9: \\(qq\\) Plots of Wisconsin Lottery Residuals. The left-hand panel is based on all 50 points. The right-hand panel is based on 49 points, residuals from a regression after removing Kenosha. R Code to Produce Figure 2.9 #Lot &lt;- read.csv(&quot;CSVData/WiscLottery.csv&quot;, header=TRUE) # FIGURE 2.9 # TABLE 2.7 model.basiclinearreg&lt;-lm(SALES ~ POP, Lot) #summary(model.basiclinearreg) model.Kenosha&lt;-lm(SALES ~ POP, Lot, subset=-c(9)) #summary(model.Kenosha) par(mfrow=c(1, 2), mar=c(4.1,3.9,1.7,1),cex=1.1) qqnorm(residuals(model.basiclinearreg), main=&quot;&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Sample Quantiles&quot;, side=2,at=20500,las=1,cex=1.1, adj=.5) qqnorm(residuals(model.Kenosha), main=&quot;&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;Sample Quantiles&quot;, side=2,at=9050,las=1,cex=1.1, adj=.5) 2.7 Application: Capital Asset Pricing Model In this section, we study a financial application, the Capital Asset Pricing Model, often referred to by the acronym CAPM. The name is something of a misnomer in that the model is really about returns based on capital assets, not the prices themselves. The types of assets that we examine are equity securities that are traded on an active market, such as the New York Stock Exchange (NYSE). For a stock on the exchange, we can relate returns to prices through the following expression: \\[ \\small{ \\mathrm{return=}\\frac{\\mathrm{ price~at~the~end~of~a~period+dividends-price~at~the~beginning~of~a~period}}{ \\mathrm{price~at~the~beginning~of~a~period}}. } \\] If we can estimate the returns that a stock generates, then knowledge of the price at the beginning of a generic financial period allows us to estimate the value at the end of the period (ending price plus dividends). Thus, we follow standard practice and model returns of a security. An intuitively appealing idea, and one of the basic characteristics of the CAPM, is that there should be a relationship between the performance of a security and the market. One rationale is simply that if economic forces are such that the market improves, then those same forces should act upon an individual stock, suggesting that it also improve. As noted above, we measure performance of a security through the return. To measure performance of the market, several market indices exist that summarize the performance of each exchange. We will use the “equally-weighted” index of the Standard &amp; Poor’s 500. The Standard &amp; Poor’s 500 is the collection of the 500 largest companies traded on the NYSE, where “large” is identified by Standard &amp; Poor’s, a financial services rating organization. The equally-weighted index is defined by assuming a portfolio is created by investing one dollar in each of the 500 companies. Another rationale for a relationship between security and market returns comes from financial economics theory. This is the CAPM theory, attributed to Sharpe (1964) and Lintner (1965) and based on the portfolio diversification ideas of Harry Markowitz (1959). Other things equal, investors would like to select a return with a high expected value and low standard deviation, the latter being a measure of risk. One of the desirable properties about using standard deviations as a measure of riskiness is that it is straight-forward to calculate the standard deviation of a portfolio. One only needs to know the standard deviation of each security and the correlations among securities. A notable security is a risk-free one, that is, a security that theoretically has a zero standard deviation. Investors often use a 30-day U.S. Treasury bill as an approximation of a risk-free security, arguing that the probability of default of the U.S. government within 30 days is negligible. Positing the existence of a risk-free asset and some other mild conditions, under the CAPM theory there exists an efficient frontier called the securities market line. This frontier specifies the minimum expected return that investors should demand for a specified level of risk. To estimate this line, we can use the equation \\[\\begin{equation*} \\mathrm{E}~r = \\beta_0 + \\beta_1 r_m \\end{equation*}\\] where \\(r\\) is the security return and \\(r_m\\) is the market return. We interpret \\(\\beta_1 r_m\\) as a measure of the amount of security return that is attributed to the behavior of the market. Testing economic theory, or models arising from any discipline, involves collecting data. The CAPM theory is about ex-ante (before the fact) returns even though we can only test with ex-post (after the fact) returns. Before the fact, the returns are unknown and there is an entire distribution of returns. After the fact, there is only a single realization of the security and market return. Because at least two observations are required to determine a line, CAPM models are estimated using security and market data gathered over time. In this way, several observations can be made. For the purposes of our discussions, we follow standard practice in the securities industry and examine monthly prices. Data To illustrate, consider monthly returns over the five year period from January, 1986 to December, 1990, inclusive. Specifically, we use the security returns from the Lincoln National Insurance Corporation as the dependent variable (\\(y\\)) and the market returns from the index of the Standard &amp; Poor’s 500 Index as the explanatory variable (\\(x\\)). At the time, the Lincoln was a large, multi-line, insurance company, headquartered in the midwest of the U.S., specifically in Fort Wayne, Indiana. Because it was well known for its’ prudent management and stability, it is a good company to begin our analysis of the relationship between the market and an individual stock. We begin by interpreting some basic summary statistics, in Table 2.8, in terms of financial theory. First, an investor in the Lincoln will be concerned that the five year average return, \\(\\overline{y}=0.00510\\), is below the return of the market, \\(\\overline{x}=0.00741\\). Students of interest theory recognize that monthly returns can be converted to an annual basis using geometric compounding. For example, the annual return of the Lincoln is \\((1.0051)^{12}-1=0.062946\\), or roughly 6.29 percent. This is compared to an annual return of 9.26% (= (1\\(00((1.00741)^{12}-1\\))) for the market. A measure of risk, or volatility, that is used in finance is the standard deviation. Thus, interpret \\(s_y\\) = 0.0859 \\(&gt;\\) 0.05254 = \\(s_x\\) to mean that an investment in the Lincoln is riskier than that of the market. Another interesting aspect of Table 2.8 is that the smallest market return, -0.22052, is 4.338 standard deviations below its average ((-0.22052-0.00741)/0.05254 = -4.338). This is highly unusual with respect to a normal distribution. knitr::kable(2, caption = &quot;Silly. Create a table just to update the counter...&quot;) Table 2.2: Silly. Create a table just to update the counter… x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Table 2.3: Silly. x 2 knitr::kable(2, caption = &quot;Silly. &quot;) Table 2.4: Silly. x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Table 2.5: Silly. x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Table 2.6: Silly. x 2 Table 2.7: Silly. x 2 Table 2.8: Summary Statistics of 60 Monthly Observations Mean Median Standard Deviation Minimum Maximum LINCOLN 0.0051 0.0075 0.0859 -0.2803 0.3147 MARKET 0.0074 0.0142 0.0525 -0.2205 0.1275 Source: Center for Research on Security Prices, University of Chicago We next examine the data over time, as is given graphically in Figure 2.10. These are scatter plots of the returns versus time, called time series plots. In Figure 2.10, one can clearly see the smallest market return and a quick glance at the horizontal axis reveals that this unusual point is in October, 1987, the time of the well-known market crash. Figure 2.10: Time series plot of returns from the Lincoln National Corporation and the market. There are 60 monthly returns over the period January, 1986 through December, 1990. The scatter plot in Figure 2.11 graphically summarizes the relationship between Lincoln’s return and the return of the market. The market crash is clearly evident in Figure 2.11 and represents a high leverage point. With the regression line (described below) superimposed, the two outlying points that can be seen in Figure 2.10 are also evident. Despite these anomalies, the plot in Figure 2.11 does suggest that there is a linear relationship between Lincoln and market returns. Figure 2.11: Scatterplot of Lincoln’s return versus the S&amp;P 500 Index return. The regression line is superimposed, enabling us to identify the market crash and two outliers. R Code to Produce Table 2.8 and Figures 2.10 and 2.11 CAPM &lt;- read.csv(&quot;CSVData/CAPM.csv&quot;, header=TRUE) # TABLE 2.8 SUMMARY STATISTICS Xymat &lt;- data.frame(cbind(CAPM$LINCOLN,CAPM$MARKET)) tableMat &lt;- BookSummStats(Xymat) colnames(tableMat) &lt;- c(&quot;Mean&quot; , &quot;Median&quot; , &quot;Standard Deviation&quot; , &quot;Minimum&quot; , &quot;Maximum&quot;) rownames(tableMat) &lt;- c(&quot;LINCOLN&quot;, &quot;MARKET&quot;) #tableMat1 &lt;- format(round(tableMat, digits=0), big.mark = &#39;,&#39;) TableGen1(TableData=tableMat, TextTitle=&#39;Summary Statistics of 60 Monthly Observations&#39;, Align=&#39;r&#39;, Digits=4, ColumnSpec=1:5, ColWidth = ColWidth5) %&gt;% footnote(general = &quot;Center for Research on Security Prices, University of Chicago&quot;, general_title = &quot;Source:&quot;, footnote_as_chunk = TRUE) # FIGURE 2.10 par(mar=c(4.1,3.1,2,1),cex=1.1, las=1) foo &lt;- ts(CAPM, freq = 12, start = c(1986, 1)) ts.plot(foo[,2], foo[,3], xlab=&quot;Year&quot;, ylab=&quot;&quot;, type=&quot;o&quot;, lty=c(1, 2)) mtext(&quot;Monthly Return&quot;, side=2, at=.38,las=1,cex=1.1, adj=.5) legend(1986, 0.3, c(&quot;LINCOLN&quot;, &quot;MARKET&quot;), lty=1:2, cex=0.5) # FIGURE 2.11 par(mar=c(4.1,3.1,1.4,0.2),cex=1.1, las=1) plot(CAPM$MARKET, CAPM$LINCOLN, xlab=&quot;MARKET&quot;, ylab=&quot;&quot;, xlim=c(-0.3, 0.2), ylim=c(-0.3, 0.4),las=1) mtext(&quot;LINCOLN&quot;, side=2,at=0.46,las=1, cex=1.1, adj=.5) reg &lt;- lm(LINCOLN ~ MARKET, data = CAPM) abline(reg) arrows(-0.22, -0.1, -0.22, -0.22,length=0.1, angle = 10) text(-0.22, -0.08, &quot;OCTOBER, 1987 CRASH&quot;, cex=0.8) arrows(0.1, 0.02, 0, -0.27,length=0.1, angle = 10) arrows(0.1, 0.02, 0.06, 0.3,length=0.1, angle = 10) text(0.16, 0.02, &quot;1990 OUTLIERS&quot;, cex=0.8) Unusual Points To summarize the relationship between the market and Lincoln’s return, a regression model was fit. The fitted regression is \\[\\begin{equation*} \\widehat{LINCOLN}=-0.00214+0.973 MARKET. \\end{equation*}\\] The resulting estimated standard error, \\(s\\) = 0.0696 is lower than the standard deviation of Lincoln’s returns, \\(s_y=0.0859\\). Thus, the regression model explains some of the variability of Lincoln’s returns. Further, the \\(t\\)-statistic associated with the slope \\(b_1\\) turns out to be \\(t(b_1)=5.64\\), which is significantly large. One disappointing aspect is that the statistic \\(R^2=35.4\\%\\) can be interpreted as saying that the market explains only a little over a third of the variability. Thus, even though the market is clearly an important determinant, as evidenced by the high \\(t\\)-statistic, it provides only a partial explanation of the performance of the Lincoln’s returns. In the context of the market model, we may interpret the standard deviation of the market, \\(s_x\\), as non-diversifiable risk. Thus, the risk of a security can be decomposed into two components, the diversifiable component and the market component, which is non-diversifiable. The idea here is that by combining several securities we can create a portfolio of securities that, in most instances, will reduce the riskiness of our holdings when compared with a single security. Again, the rationale for holding a security is that we are compensated through higher expected returns by holding a security with higher riskiness. To quantify the relative riskiness, it is not hard to show that \\[\\begin{equation} s_y^2 = b_1^2 s_x^2 + s^2 \\frac{n-2}{n-1}. \\tag{2.8} \\end{equation}\\] The riskiness of a security is due to the riskiness due to the market plus the riskiness due to a diversifiable component. Note that the riskiness due to the market component, \\(s_x^2\\), is larger for securities with larger slopes. For this reason, investors think of securities with slopes \\(b_1\\) greater than one as “aggressive” and slopes less than one as “defensive.” Sensitivity Analysis The above summary immediately raises two additional issues. First, what is the effect of the October, 1987 crash on the fitted regression equation? We know that unusual observations, such as the crash, may potentially influence the fit a great deal. To this end, the regression was re-run without the observation corresponding to the crash. The motivation for this is that the October 1987 crash represents a combination of highly unusual events (the interaction of several automated trading programs operated by the large stock brokerage houses) that we do not wish to represent using the same model as our other observations. Deleting this observation, the fitted regression is \\[\\begin{equation*} \\widehat{LINCOLN} = -0.00181 + 0.956 MARKET, \\end{equation*}\\] with \\(R^2=26.4\\%\\), \\(t(b_1)=4.52\\), \\(s=0.0702\\) and \\(s_y=0.0811\\). We interpret these statistics in the same fashion as the fitted model including the October 1987 crash. It is interesting to note, however, that the proportion of variability explained has actually decreased when excluding the influential point. This serves to illustrate an important point. High leverage points are often looked upon with dread by data analysts because they are, by definition, unlike other observations in the data set and require special attention. However, when fitting relationships among variables, they also represent an opportunity because they allow the data analyst to observe the relationship between variables over broader ranges than otherwise possible. The downside is that these relationships may be nonlinear or follow an entirely different pattern when compared to the relationships observed in the main portion of the data. The second question raised by the regression analysis is what can be said about the unusual circumstances that gave rise to the unusual behavior of Lincoln’s returns in October and November of 1990. A useful feature of regression analysis is to identify and raise the question; it does not resolve it. Because the analysis clearly pinpoints two highly unusual points, it suggests to the data analyst to go back and ask some specific questions about the sources of the data. In this case, the answer is straightforward. In October of 1990, the Travelers’ Insurance Company, a competitor, announced that it would take a large write-off in their real estate portfolio. due to an unprecedented number of mortgage defaults. The market reacted quickly to this news, and investors assumed that other large stock life insurers would also soon announce large write-offs. Anticipating this news, investors tried to sell their portfolios of, for example, Lincoln’s stock, thus causing the price to plummet. However, it turned out that investors overreacted to this news and that Lincoln’s portfolio of real estate was indeed sound. Thus, prices quickly returned to their historical levels. 2.8 Illustrative Regression Computer Output Computers and statistical software packages that perform specialized calculations play a vital role in modern-day statistical analyses. Inexpensive computing capabilities have allowed data analysts to focus on relationships of interest. Specifying models that are attractive merely for their computational simplicity is much less important now compared to times before the widespread availability of inexpensive computing. An important theme of this text is to focus on relationships of interest and to rely on widely available statistical software to estimate the models that we specify. With any computer package, generally the most difficult parts of operating the package are the (i) input, (ii) using the commands and (iii) interpreting the output. You will find that most modern statistical software packages accept spreadsheet or text-based files, making input of data relatively easy. Personal computer statistical software packages have menu-driven command languages with easily accessible on-line help facilities. Once you decide what to do, finding the right commands is relatively easy. This section provides guidance in interpreting the output of statistical packages. Most statistical packages generate similar output. Below, three examples of standard statistical software packages, EXCEL, SAS and R are given. The annotation symbol “[.]” marks a statistical quantity that is described in the legend. Thus, this section provides a link between the notation used in the text and output from some of the standard statistical software packages. EXCEL Output Regression Statistics Multiple R 0.886283[F] R Square 0.785497[k] Adjusted R Square 0.781028[l] Standard Error 3791.758[j] Observations 50[a] ANOVA df SS MS F Significance F Regression 1[m] 2527165015 [p] 2527165015 [s] 175.773[u] 1.15757E-17[v] Residual 48[n] 690116754.8[q] 14377432.39[t] Total 49[o] 3217281770 [r] Coefficients Standard Error t Stat P-value Intercept 469.7036[b] 702.9061896[d] 0.668230846[f] 0.507187[h] X Variable 1 0.647095[c] 0.048808085[e] 13.25794257[g] 1.16E-17[i] The SAS System The REG Procedure Dependent Variable: SALES Analysis of Variance Sum of Mean Source DF Squares Square F Value Pr &gt; F Model 1[m] 2527165015[p] 2527165015[s] 175.77[u] &lt;.0001[v] Error 48[n] 690116755[q] 14377432[t] Corrected Total 49[o] 3217281770[r] Root MSE 3791.75848[j] R-Square 0.7855[k] Dependent Mean 6494.82900[H] Adj R-Sq 0.7810[l] Coeff Var 58.38119[I] Parameter Estimates Parameter Standard Variable Label DF Estimate Error t Value Pr &gt; |t| Intercept Intercept 1 469.70360[b] 702.90619[d] 0.67[f] 0.5072[h] POP POP 1 0.64709[c] 0.04881[e] 13.26[g] &lt;.0001[i] R Output Analysis of Variance Table Response: SALES Df Sum Sq Mean Sq F value Pr(&gt;F) POP 1[m] 2527165015[p] 2527165015[s] 175.77304[u] &lt;2.22e-16[v]*** Residuals 48[n] 690116755[q] 14377432[t] --- Call: lm(formula = SALES ~ POP) Residuals: Min 1Q Median 3Q Max -6047 -1461 -670 486 18229 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 469.7036[b] 702.9062[d] 0.67[f] 0.51 [h] POP 0.6471[c] 0.0488[e] 13.26[g] &lt;2e-16 ***[i] --- Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 Residual standard error: 3792[j] on 48[n] degrees of freedom Multiple R-Squared: 0.785[k], Adjusted R-squared: 0.781[l] F-statistic: 176[u] on 1[m] and 48[n] DF, p-value: &lt;2e-16[v] Legend Annotation Definition, Symbol [a] Number of observations \\(n\\). [b] The estimated intercept \\(b_0\\). [c] The estimated slope \\(b_1\\). [d] The standard error of the intercept, \\(se(b_0)\\). [e] The standard error of the slope, \\(se(b_1)\\). [f] The \\(t\\)-ratio associated with the intercept, \\(t(b_0) = b_0/se(b_0)\\). [g] The \\(t\\)-ratio associated with the slope, \\(t(b_1) = b_1/se(b_1)\\). [h] The \\(p\\)-value associated with the intercept; here, \\(p-value=Pr(|t_{n-2}|&gt;|t(b_0)|)\\), where \\(t(b_0)\\) is the realized value (0.67 here) and \\(t_{n-2}\\) has a \\(t\\)-distribution with \\(df=n-2\\). [i] The \\(p\\)-value associated with the slope; here, \\(p-value=Pr(|t_{n-2}|&gt;|t(b_1)|)\\), where \\(t(b_1)\\) is the realized value (13.26 here) and \\(t_{n-2}\\) has a \\(t\\)-distribution with $df=n-2 $. [j] The residual standard deviation, \\(s\\). [k] The coefficient of determination, \\(R^2\\). [l] The coefficient of determination adjusted for degrees of freedom, \\(R_{a}^2\\). (This term will be defined in Chapter 3.) [m] Degree of freedom for the regression component. This is 1 for one explanatory variable. [n] Degree of freedom for the error component, \\(n-2\\), for regression with one explanatory variable. [o] Total degrees of freedoms, \\(n-1\\). [p] The regression sum of squares, \\(Regression~SS\\). [q] The error sum of squares, \\(Error~SS\\). [r] The total sum of squares, \\(Total~SS\\). [s] The regression mean square, \\(Regression~MS = Regression~SS/1\\), for one explanatory variable. [t] The error mean square, \\(s^2=Error~MS = Error~SS/(n-2)\\), for one explanatory variable. [u] The \\(F-ratio=(Regression~MS)/(Error~MS)\\). (This term will be defined in Chapter 3.) [v] The \\(p\\)-value associated with the \\(F-ratio\\). (This term will be defined in Chapter 3.) [w] The observation number, \\(i\\). [x] The value of the explanatory variable for the \\(i\\)th observation, \\(x_i\\). [y] The response for the \\(i\\)th observation, \\(y_i\\). [z] The fitted value for the \\(i\\)th observation, \\(\\widehat{y}_i\\). [A] The standard error of the fit, \\(se(\\widehat{y}_i)\\). [B] The residual for the \\(i\\)th observation, \\(e_i\\). [C] The standardized residual for the \\(i\\)th observation, \\(e_i/se(e_i)\\). The standard error \\(se(e_i)\\) will be defined in Section 5.3.1. [F] The multiple correlation coefficient is the square root of the coefficient of determination, \\(R=\\sqrt{R^2}\\). This will be defined in Chapter 3. [G] The standardized coefficient is \\(b_1s_x/s_y\\) For regression with one explanatory variable, this is equivalent to \\(r\\), the correlation coefficient. [H] The average response, \\(\\overline{y}\\). [I] The coefficient of variation of the response is \\(s_y/\\overline{y}\\). SAS prints out \\(100s_y/\\overline{y}\\). 2.9 Further Reading and References Relatively few applications of regression are basic in the sense that they use only one explanatory variable; the purpose of regression analysis is to reduce complex relationships among many variables. Section 2.7 described an important exception to this general rule, the CAPM finance model; see Panjer et al. (1998) for additional actuarial descriptions of this model. Campbell et al. (1997) gives a financial econometrics perspective. Chapter References Anscombe, Frank (1973). Graphs in statistical analysis. The American Statistician 27, 17-21. Campbell, John Y., Andrew W. Lo and A. Craig MacKinlay (1997). The Econometrics of Financial Markets. Princeton University Press, Princeton, New Jersey. Frees, Edward W. and Tom W. Miller (2003). Sales forecasting using longitudinal data models. International Journal of Forecasting 20, 97-111. Goldberger, Arthur (1991). A Course in Econometrics. Harvard University Press, Cambridge. Koch, Gary J. (1985). A basic demonstration of the [-1, 1] range for the correlation coefficient. American Statistician 39, 201-202. Linter, J. (1965). The valuation of risky assets and the selection of risky investments in stock portfolios and capital budgets. Review of Economics and Statistics, 13-37. Manistre, B. John and Geoffrey H. Hancock (2005). Variance of the CTE estimator. North American Actuarial Journal 9(2), 129-156. Markowitz, Harry (1959). Portfolio Selection: Efficient Diversification of Investments. John Wiley, New York. Panjer, Harry H., Phelim P. Boyle, Samuel H. Cox, Daniel Dufresne, Hans U. Gerber, Heinz H. Mueller, Hal W. Pedersen, Stanley R. Pliska, Michael Sherris, Elias S. Shiu and Ken S. Tan (1998). Financial Economics: With Applications to Investment, Insurance and Pensions. Society of Actuaries, Schaumburg, Illinois. Pearson, Karl (1895). Royal Society Proceedings 58, 241. Serfling, Robert J. (1980). Approximation Theorems of Mathematical Statistics. John Wiley and Sons, New York. Sharpe, William F. (1964). Capital asset prices: A theory of market equilibrium under risk. Journal of Finance, 425-442. Stigler, Steven M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. Harvard University Press, Cambridge, MA. 2.10 Exercises Sections 2.1-2.2 2.1 Consider the following data set \\[ \\begin{array}{l|ccc} \\hline i &amp; 1 &amp; 2 &amp; 3 \\\\ \\hline x_i &amp; 2 &amp; -6 &amp; 7 \\\\ y_i &amp; 3 &amp; 4 &amp; 6\\\\ \\hline \\end{array} \\] Fit a regression line using the method of least squares. Determine \\(r\\), \\(b_1\\) and \\(b_0\\). 2.2 A perfect relationship, yet zero correlation. Consider the quadratic relationship \\(y=x^2\\), with data \\[ \\begin{array}{l|ccccc} \\hline i &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5\\\\ \\hline x_i &amp; -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 \\\\ y_i &amp; 4 &amp; 1 &amp; 0 &amp; 1 &amp; 4\\\\ \\hline \\end{array} \\] Produce a rough graph for this data set. Check that the correlation coefficient is \\(r=0\\). 2.3 Boundedness of the correlation coefficient. Use the following steps to show that \\(r\\) is bounded by -1 and 1 (These steps are due to Koch, 1990). Let \\(a\\) and \\(c\\) be generic constants. Verify \\[\\begin{eqnarray*} 0 &amp; \\leq &amp; \\frac{1}{n-1}\\sum_{i=1}^{n}\\left( a\\frac{x_i-\\overline{x}}{s_x}-c \\frac{y_i-\\overline{y}}{s_y}\\right) ^2 \\\\ &amp;=&amp; a^2+c^2-2acr. \\end{eqnarray*}\\] Use the results in part (a) to show \\(2ac(r-1)\\leq (a-c)^2.\\) By taking \\(a=c\\), use the result in part (b) to show \\(r\\leq 1\\). By taking \\(a=-c\\), use the results in part (b) to show \\(r\\geq -1\\). Under what conditions is \\(r=-1\\)? Under what conditions is \\(r=1\\)? 2.4 Regression coefficients are weighted sums. Show that the intercept term, \\(b_0\\), can be expressed as a weighted sum of the dependent variables. That is, show that \\(b_0=\\sum_{i=1}^{n}w_{i,0}y_i.\\) Further, express the weights in terms of the slope weights, \\(w_i\\). 2.5 Another expression for the slope as a weighted sum Using algebra, establish an alternative expression \\[\\begin{equation*} b_1=\\frac{\\sum_{i=1}^{n}weight_i~slope_i}{ \\sum_{i=1}^{n}weight_i}. \\end{equation*}\\] Here, \\(slope_i\\) is the slope between \\((x_i,y_i)\\) and \\((\\bar{x},\\bar{y})\\). Give a precise form for the weight \\(weight_i\\) as a function of the explanatory variable \\(x\\). Suppose that \\(\\bar{x} = 4,\\bar{y} = 3, x_1 = 2 \\mathrm{~and~} y_1= 6\\). Determine the slope and weight for the first observation, that is, \\(slope_1\\) and \\(weight_1\\). 2.6 Consider two variables, \\(y\\) and \\(x\\). Do a regression of \\(y\\) on \\(x\\) to get a slope coefficient which we call \\(b_{1,x,y}\\). Do another regression of \\(x\\) on \\(y\\) to get a slope coefficient which we call \\(b_{1,y,x}\\). Show that the correlation coefficient between \\(x\\) and \\(y\\) is the geometric mean of the two slope coefficients up to sign, that is, show that \\(|r|=\\sqrt{ b_{1,x,y}b_{1,y,x}}.\\) 2.7 Regression through the origin. Consider the model \\(y_i=\\beta_1 x_i + \\varepsilon _i\\), that is, regression with one explanatory variable without the intercept term. This model is called regression through the origin because the true regression line \\(\\mathrm{E}y = \\beta_1 x\\) passes through the origin (the point (0, 0)). For this model, the least squares estimate of \\(\\beta_1\\) is that number \\(b_1\\) that minimizes the sum of squares \\(\\mathrm{SS}(b_1^{\\ast} )=\\sum_{i=1}^{n}\\left( y_i - b_1^{\\ast}x_i\\right) ^2.\\) Verify that \\[\\begin{equation*} b_1 = \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sum_{i=1}^{n}x_i^2}. \\end{equation*}\\] Consider the model \\(y_i=\\beta_1 z_i^2 + \\varepsilon _i\\), a quadratic model passing through the origin. Use the result of part to determine the least squares estimate of \\(\\beta_1\\). 2.8 a. Show that \\[\\begin{equation*} s_y^2=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) ^2= \\frac{1}{n-1}\\left( \\sum_{i=1}^{n}y_i^2-n\\overline{y}^2\\right) . \\end{equation*}\\] Follow the same steps to show \\(\\sum_{i=1}^{n}\\left( y_i - \\overline{y} \\right) \\left( x_i-\\overline{x}\\right) =\\sum_{i=1}^{n} x_i y_i - n \\overline{x}~\\overline{y}.\\) Show that \\[ b_{1}=\\frac{\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) \\left( x_i- \\overline{x}\\right) }{\\sum_{i=1}^{n}\\left( x_i - \\overline{x} \\right) ^2} \\] Establish the commonly used formula $$ b_{1}=\\frac{{i=1}^{n}x_iy_i-n~}{{i=1}{n}x_i2 n^2}. $$ 2.9 Interpretation of coefficients associated with a binary explanatory variable. Suppose that \\(x_i\\) only takes on the values 0 and 1. Out of the \\(n\\) observations, \\(n_1\\) take on the value \\(x=0\\). These $n_1 $ observations have an average \\(y\\) value of \\(\\overline{y}_1\\). the remaining \\(n-n_1\\) observations have value \\(x=1\\) and an average \\(y\\) value of \\(\\overline{y}_2\\). Use Exercise 2.8 to show that \\(b_1 = \\overline{y}_2 - \\overline{y}_1.\\) 2.10 Nursing Home Utilization. This exercise considers nursing home data provided by the Wisconsin Department of Health and Family Services (DHFS) and described in Exercise 1.2. Part 1: Use cost report year 2000 data, and do the following analysis. Correlations a(i). Calculate the correlation between TPY and LOGTPY. Comment on your result. a(ii). Calculate the correlation among TPY, NUMBED and SQRFOOT. Do these variables appear highly correlated? a(iii). Calculate the correlation between TPY and NUMBED/10. Comment on your result. Scatter plots. Plot TPY versus NUMBED and TPY versus SQRFOOT. Comment on the plots. Basic linear regression. c(i). Fit a basic linear regression model using TPY as the outcome variable and NUMBED as the explanatory variable. Summarize the fit by quoting the coefficient of determination, \\(R^2\\), and the \\(t\\)-statistic for NUMBED. c(ii). Repeat c(i), using SQRFOOT instead of NUMBED. In terms of \\(R^2\\), which model fits better? c(iii). Repeat c(i), using LOGTPY for the outcome variable and LOG(NUMBED) as the explanatory variable. c(iv). Repeat c(iii), using LOGTPY for the outcome variable and LOG(SQRFOOT) as the explanatory variable. Part 2: Fit the model in Part 1.c(i) using 2001 data. Are the patterns stable over time? Sections 2.3-2.4 2.11 Suppose that, for a sample size of \\(n\\) = 3, you have \\(e_2\\) = 24 and \\(e_{3}\\) = -1. Determine \\(e_{1}\\). 2.12 Suppose that \\(r=0\\), \\(n=15\\) and \\(s_y = 10\\). Determine \\(s\\). 2.13 The correlation coefficient and the coefficient of determination. Use the following steps to establish a relationship between the coefficient of determination and the correlation coefficient. Show that \\(\\widehat{y}_i-\\overline{y}=b_1(x_i-\\overline{x}).\\) Use part (a) to show that \\(Regress~SS=\\) \\(\\sum_{i=1}^{n}\\left(\\widehat{y}_i - \\overline{y} \\right)^2\\) \\(= b_1^2s_x^2(n-1).\\) Use part (b) to establish \\(R^2=r^2.\\) 2.14 Show that the average residual is zero, that is, show that \\(n^{-1}\\sum_{i=1}^{n} e_i=0.\\) 2.15 Correlation between residuals and explanatory variables. Consider a generic sequence of pairs of numbers (\\(x_1,y_1\\)), …, (\\(x_n,y_n\\)) with the correlation coefficient computed as \\(r(y,x)=\\left[ (n-1)s_ys_x\\right] ^{-1}\\sum_{i=1}^{n}\\left( y_i-\\overline{y}\\right) \\left( x_i-\\overline{x}\\right) .\\) Suppose that either \\(\\overline{y}=0,\\overline{x}=0\\) or both \\(\\overline{x}\\)  and \\(\\overline{y}=0.\\) Then, check that \\(r(y,x)=0\\) implies \\(\\sum_{i=1}^{n}y_i x_i=0\\) and vice-versa. Show that the correlation between the residuals and the explanatory variables is zero. Do this by using part (a) of Exercise 2.13 to show that \\(\\sum_{i=1}^{n} x_i e_i = 0\\) and then apply part (a). Show that the correlation between the residuals and fitted values is zero. Do this by showing that \\(\\sum_{i=1}^n \\widehat{y}_i e_i = 0\\)  and then apply part (a). 2.16 Correlation and \\(t\\)-statistics. Use the following steps to establish a relationship between the correlation coefficient and the \\(t\\)-statistic for the slope. a Use algebra to check that \\[\\begin{equation*} R^2=1-\\frac{n-2}{n-1}\\frac{s^2}{s_y^2}. \\end{equation*}\\] Use part (a) to establish the following quick computational formula for \\(s,\\) \\[\\begin{equation*}s = s_y \\sqrt{(1-r^2)\\frac{n-1}{n-2}}.\\end{equation*}\\] Use part (b) to show that \\[\\begin{equation*} t(b_1) = \\sqrt{n-2}\\frac{r}{\\sqrt{1-r^2}}. \\end{equation*}\\] Sections 2.6-2.7 2.17 Effects of an unusual point. You are analyzing a data set of size \\(n=100\\). You have just performed a regression analysis using one predictor variable and notice that the residual for the 10th observation is unusually large. Suppose that, in fact, it turns out that \\(e_{10}=8s\\). What percentage of the error sum of squares, \\(Error~SS\\), is due to the 10th observation? Suppose that \\(e_{10}=4s\\). What percentage of the error sum of squares, \\(Error~SS\\), is due to the 10th observation? Suppose that you reduce the data set to size \\(n=20\\). After running the regression, it turns out that we still have \\(e_{10}=4s\\). What percentage of the error sum of squares, \\(Error~SS\\), is due to the 10th observation? 2.18 Consider a data set consisting of 20 observations with the following summary statistics: \\(\\overline{x}=0\\), \\(\\overline{y}=9\\), \\(s_x=1\\) and \\(s_y=10\\). You run a regression using using one variable and determine that \\(s=7\\). Determine the standard error of a prediction at \\(x_{\\ast}=1.\\) 2.19 Summary statistics can hide important relationships. The data in Table 2.9 is due to Anscombe (1973). The purpose of this exercise is to demonstrate how plotting data can reveal important information that is not evident in numerical summary statistics. Table 2.9. Anscombe’s (1973) Data \\[ \\begin{array}{c|rrrrrr} \\hline obs &amp; &amp; &amp; &amp; &amp; &amp; \\\\ num &amp; x_1 &amp; y_1 &amp; y_2 &amp; y_3 &amp; x_2 &amp; y_4 \\\\ \\hline 1 &amp; 10 &amp; 8.04 &amp; 9.14 &amp; 7.46 &amp; 8 &amp; 6.58 \\\\ 2 &amp; 8 &amp; 6.95 &amp; 8.14 &amp; 6.77 &amp; 8 &amp; 5.76 \\\\ 3 &amp; 13 &amp; 7.58 &amp; 8.74 &amp; 12.74 &amp; 8 &amp; 7.71 \\\\ 4 &amp; 9 &amp; 8.81 &amp; 8.77 &amp; 7.11 &amp; 8 &amp; 8.84 \\\\ 5 &amp; 11 &amp; 8.33 &amp; 9.26 &amp; 7.81 &amp; 8 &amp; 8.47 \\\\ 6 &amp; 14 &amp; 9.96 &amp; 8.10 &amp; 8.84 &amp; 8 &amp; 7.04 \\\\ 7 &amp; 6 &amp; 7.24 &amp; 6.13 &amp; 6.08 &amp; 8 &amp; 5.25 \\\\ 8 &amp; 4 &amp; 4.26 &amp; 3.10 &amp; 5.39 &amp; 8 &amp; 5.56 \\\\ 9 &amp; 12 &amp; 10.84 &amp; 9.13 &amp; 8.15 &amp; 8 &amp; 7.91 \\\\ 10 &amp; 7 &amp; 4.82 &amp; 7.26 &amp; 6.42 &amp; 8 &amp; 6.89 \\\\ 11 &amp; 5 &amp; 5.68 &amp; 4.74 &amp; 5.73 &amp; 19 &amp; 12.50 \\\\ \\hline \\end{array} \\] Compute the averages and standard deviations of each column of data. Check that the averages and standard deviations of each of the \\(x\\) columns are the same, within two decimal places, and similarly for each of the \\(y\\) columns. Run four regressions, (1) \\(y_{1}\\) on \\(x_{1}\\), (2) \\(y_2\\) on \\(x_{1}\\), (3) \\(y_{3}\\) on \\(x_{1}\\) and (4) \\(y_{4}\\) on \\(x_2\\). Verify, for each of the four regressions fits, that \\(b_0\\approx 3.0\\), \\(b_{1}\\approx 0.5\\), \\(s\\approx 1.237\\) and \\(R^2\\approx 0.677\\), within two decimal places. Produce scatter plots for each of the four regression models that you fit in part (b). Discuss the fact that the fitted regression models produced in part (b) imply that the four data sets are similar although the four scatter plots produced in part (c) yield a dramatically different story. 2.20 Nursing Home Utilization. This exercise considers nursing home data provided by the Wisconsin Department of Health and Family Services (DHFS) and described in Exercise 1.2 and 2.10. You decide to examine the relationship between total patient years (LOGTPY) and the number of beds (LOGNUMBED), both in logarithmic units, using cost report year 2001 data. Summary statistics. Create basic summary statistics for each variable. Summarize the relationship through a correlation statistic and a scatter plot. Fit the basic linear model. Cite the basic summary statistics, include the coefficient of determination, the regression coefficient for LOGNUMBED and the corresponding \\(t\\)-statistic. Hypothesis testing. Test the following hypotheses at the 5% level of significance using a \\(t\\)-statistic. Also compute the corresponding \\(p\\)-value. c(i). Test \\(H_0: \\beta_1 = 0\\) versus \\(H_a: \\beta_1 \\neq 0\\). c(ii). Test \\(H_0: \\beta_1 = 1\\) versus \\(H_a: \\beta_1 \\neq 1\\). c(iii). Test \\(H_0: \\beta_1 = 1\\) versus \\(H_a: \\beta_1 &gt; 1\\). c(iv). Test \\(H_0: \\beta_1 = 1\\) versus \\(H_a: \\beta_1 &lt; 1\\). You are interested in the effect that a marginal change in LOGNUMBED has on the expected value of LOGTPY. d(i). Suppose that there is a marginal change in LOGNUMBED of 2. Provide a point estimate of the expected change in LOGTPY. d(ii). Provide a 95% confidence interval corresponding to the point estimate in part d(i). d(iii). Provide a 99% confidence interval corresponding to the point estimate in part d(i). At a specified number of beds estimate \\(x_{*} = 100\\), do these things: e(i). Find the predicted value of LOGTPY. e(ii). Obtain the standard error of the prediction. e(iii). Obtain a 95% prediction interval for your prediction. e(iv). Convert the point prediction in part e(i) and the prediction interval obtained in part e(iii) into total person years (through exponentiation). e(v). Obtain a prediction interval as in part e(iv), corresponding to a 90% level (in lieu of 95%). 2.21 Initial Public Offerings. As a financial analyst, you wish to convince a client of the merits of investing in firms that have just entered a stock exchange, as an IPO (initial public offering). Thus, you gather data on 116 firms that priced during the six-month time frame of January 1, 1998 through June 1, 1998. By looking at this recent historical data, you are able to compute RETURN, the firm’s one-year return (in percent). You are also interested in looking at financial characteristics of the firm that may help you understand (and predict) the return. You initially examine REVENUE, the firm’s 1997 revenues in millions of dollars. Unfortunately, this variable was not available for six firms. Thus, the statistics below are for the 110 firms that have both REVENUES and RETURNS. In addition, Table 2.9 provides information on the (natural) logarithmic revenues, denoted as LnREV, and the initial price of the stock, denoted as PRICEIPO. Table 2.9: Summary Statistics of Each Variable Mean Median Standard Deviation Minimum Maximum RETURN 0.106 -0.130 0.824 -0.938 4.333 REV 134.487 39.971 261.881 0.099 1455.761 LnREV 3.686 3.688 1.698 -2.316 7.283 PRICEIPO 13.195 13.000 4.694 4.000 29.000 You hypothesize that larger firms, as measured by revenues, are more stable and thus should enjoy greater returns. You have determined that the correlation between RETURN and REVENUE is -0.0175. a(i). Calculate the least squares fit using REVENUE to predict RETURN. Determine \\(b_0\\) and \\(b_1\\). a(ii). For Hyperion Telecommunications, REVENUEs are 95.55 (millions of dollars). Calculate the fitted RETURN using the regression fit in part a(i). Table 2.11. Regression Results from a Model Fit with Logarithmic Revenues \\[ \\begin{array}{l|rrr} \\hline &amp; &amp; \\text{Standard} &amp; \\\\ \\text{Variable} &amp; \\text{Coefficient} &amp; \\text{Error} &amp; t-\\text{statistic} \\\\ \\hline \\text{INTERCEPT} &amp; 0.438 &amp; 0.186 &amp; 2.35\\\\ \\text{LnREV} &amp; -0.090 &amp; 0.046 &amp; -1.97 \\\\ \\hline s = 0.8136, &amp; R^2 = 0.03452 \\\\ \\hline \\end{array} \\] b. Logarithmic revenues and returns. b(i). Suppose instead that you use LnREVs to predict RETURN. Calculate the fitted RETURN under this regression model. Is this equal your answer in part a(ii)? b(ii) Do logarithmic revenues significantly affect returns? To this end, provide a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion and your decision-making rule. Use a 10% level of significance. b(iii). You conjecture that, other things equal, that firms with larger revenues will be more stable and thus enjoy a larger initial return. Thus, you wish to consider the null hypothesis of no relation between LnREV and RETURN versus the alternative hypothesis that there is a positive relation between LnREV and RETURN. To this end, provide a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion and your decision-making rule. Use a 10% level of significance. Determine the correlation between LnREV and RETURN. Be sure to state whether this correlation is positive, negative or zero. You are considering investing in a firm that has LnREV = 2 (so revenues are \\(e^2\\) = 7.389 millions of dollars). d(i). Using the fitted regression model, determine the least squares point prediction. d(ii). Determine the 95% prediction interval corresponding to your prediction in part d(i). The \\(R^2\\) from the fitted regression model is a disappointing 3.5%. Part of the difficulty is due to observation number 59, the Inktomi Corporation. Inktomi sales are 12th smallest of the data set, with LnREV = 1.76 (so revenues are \\(e^{1.76} = 5.79\\) millions of dollars), yet it has the highest first year return, with RETURN = 433.33. e(i). Calculate the residual for this observation. e(ii). What proportion of the unexplained variability (error sum of squares) does this observation account for? e(iii). Define the idea of a high leverage observation. e(iv). Would this observation be considered a high leverage observation? Justify your answer. 2.22 National Life Expectancies. We continue the analysis begun in Exercise 1.7 by examining the relation between \\(y= LIFEEXP\\) and \\(x=FERTILITY\\), shown in Figure 2.12. Fit a linear regression model of \\(LIFEEXP\\) using the explanatory variable \\(x=FERTILITY\\). Figure 2.12: Plot of FERTILITY versus LIFEEXP. The US has a FERTILITY rate of 2.0. Determine the fitted life expectancy. The island nation Dominica did not report a FERTILITY rate and thus was not included in the regression. Suppose that its FERTILITY rate is 2.0. Provide a 95% prediction interval for the life expectancy in Dominica. China has a FERTILITY rate of 1.7 and a life expectancy of 72.5. Determine the residual under the model. How many multiples of \\(s\\) is this residual from zero? Suppose that your prior hypothesis is that the FERTILITY slope is -6.0 and you wish to test the null hypothesis that the slope has increased (that is, the slope is greater than -6.0). Test this hypothesis at the 5% level of significance. Also compute an approximate \\(p\\)-value. 2.11 Technical Supplement - Elements of Matrix Algebra Examples are an excellent tool for introducing technical topics such as regression. However, this chapter has also used algebra as well as basic probability and statistics to give you further insights into regression analysis. Going forward, we will be studying multivariate relationships. With many things happening concurrently in several dimensions, algebra is no longer useful for providing insights. Instead, we will need matrix algebra. This supplement provides a brief introduction to matrix algebra to allow you to study the linear regression chapters of this text. It re-introduces basic linear regression to give you a feel for things that will be coming up in subsequent chapters when we extend basic linear regression to the multivariate case. Appendix A3 defines additional matrix concepts. 2.11.1 Basic Definitions A matrix is a rectangular array of numbers arranged in rows and columns (the plural of matrix is matrices). For example, consider the income and age of 3 people. \\[\\begin{equation*} \\mathbf{A}= \\begin{array}{c} Row~1 \\\\ Row~2 \\\\ Row~3 \\end{array} \\overset{ \\begin{array}{cc} ~~~Col~1~ &amp; Col~2 \\end{array} }{\\left( \\begin{array}{cc} 6,000 &amp; 23 \\\\ 13,000 &amp; 47 \\\\ 11,000 &amp; 35 \\end{array} \\right) } \\end{equation*}\\] Here, column 1 represents income and column 2 represents age. Each row corresponds to an individual. For example, the first individual is 23 years old with an income of $6,000. The number of rows and columns is called the dimension of the matrix. For example, the dimension of the matrix \\(\\mathbf{A}\\) above is \\(3\\times 2\\) (read 3 “by” 2). This stands for 3 rows and 2 columns. If we were to represent the income and age of 100 people, then the dimension of the matrix would be \\(100\\times 2\\). It is convenient to represent a matrix using the notation \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\\\ a_{31} &amp; a_{32} \\end{array} \\right) . \\end{equation*}\\] Here, \\(a_{ij}\\) is the symbol for the number in the \\(i\\)th row and \\(j\\)th column of \\(\\mathbf{A}\\). In general, we work with matrices of the form \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nk} \\end{array} \\right) . \\end{equation*}\\] In this case, the matrix \\(\\mathbf{A}\\) has dimension \\(n\\times k\\). A vector is a special matrix. A row vector is a matrix containing only 1 row (\\(k=1\\)). A column vector is a matrix containing only 1 column (\\(n=1\\)). For example, \\[\\begin{equation*} \\text{column vector}\\rightarrow \\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\end{array} \\right) ~~~~\\text{row vector}\\rightarrow \\left( \\begin{array}{ccccc} 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\end{array} \\right) . \\end{equation*}\\] Notice above that the row vector takes much less room on a printed page than the corresponding column vector. A basic operation that relates these two quantities is the transpose. The transpose of a matrix \\(\\mathbf{A}\\) is defined by interchanging the rows and columns and is denoted by \\(\\mathbf{A }^{\\prime }\\) (or \\(\\mathbf{A}^{T}\\)). For example, \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} 6,000 &amp; 23 \\\\ 13,000 &amp; 47 \\\\ 11,000 &amp; 35 \\end{array} \\right) ~~~\\mathbf{A}^{\\prime }=\\left( \\begin{array}{ccc} 6,000 &amp; 13,000 &amp; 11,000 \\\\ 23 &amp; 47 &amp; 35 \\end{array} \\right) . \\end{equation*}\\] Thus, if \\(\\mathbf{A}\\) has dimension \\(n\\times k\\), then \\(\\mathbf{A}^{\\prime }\\) has dimensions \\(k\\times n\\). 2.11.2 Some Special Matrices 2.11.3 Basic Operations Scalar Multiplication Let \\(\\mathbf{A}\\) by a \\(n\\times k\\) matrix and let \\(c\\) be a real number. That is, a real number is a \\(1\\times 1\\) matrix and is also called a scalar. Multiplying a scalar \\(c\\) by a matrix \\(\\mathbf{A}\\) is denoted by \\(c\\mathbf{A}\\) and defined by \\[\\begin{equation*} c\\mathbf{A}=\\left( \\begin{array}{cccc} ca_{11} &amp; ca_{12} &amp; \\cdots &amp; ca_{1k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ ca_{n1} &amp; ca_{n2} &amp; \\cdots &amp; ca_{nk} \\end{array} \\right) . \\end{equation*}\\] For example, suppose that \\(c=10\\) and \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} 1 &amp; 2 \\\\ 6 &amp; 8 \\end{array} \\right) \\text{~~~~ then ~~~~ }\\mathbf{B}=c\\mathbf{A}=\\left( \\begin{array}{cc} 10 &amp; 20 \\\\ 60 &amp; 80 \\end{array} \\right) . \\end{equation*}\\] Note that \\(c\\mathbf{A}=\\mathbf{A}c\\). Addition and Subtraction of Matrices Let \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be matrices with dimensions \\(n\\times k\\). Use \\(a_{ij}\\) and \\(b_{ij}\\) to denote the numbers in the \\(i\\)th row and \\(j\\)th column of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), respectively. Then, the matrix $ =+$ is defined to be the matrix with $ (a_{ij}+b_{ij})$ in the \\(i\\)th row and \\(j\\)th column. Similarly, the matrix \\(\\mathbf{C}=\\mathbf{A}-\\mathbf{B}\\) is defined to be the matrix with \\((a_{ij}-b_{ij})\\) in the \\(i\\)th row and \\(j\\)th column. Symbolically, we write this as the following. \\[\\begin{equation*} \\text{If }\\mathbf{A=}\\left( a_{ij}\\right) _{ij}\\text{ and } \\mathbf{B=}\\left( b_{ij}\\right) _{ij}\\text{, then} \\end{equation*}\\] \\[\\begin{equation*} \\mathbf{C}=\\mathbf{A}+\\mathbf{B=}\\left( a_{ij}+b_{ij}\\right) _{ij}\\text{ and }\\mathbf{C}=\\mathbf{A}-\\mathbf{B=}\\left( a_{ij}-b_{ij}\\right) _{ij}. \\end{equation*}\\] For example, consider \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} 2 &amp; 5 \\\\ 4 &amp; 1 \\end{array} \\right) ~~~\\mathbf{B}=\\left( \\begin{array}{cc} 4 &amp; 6 \\\\ 8 &amp; 1 \\end{array} \\right). \\end{equation*}\\] Then \\[\\begin{equation*} \\mathbf{A}+\\mathbf{B}=\\left( \\begin{array}{cc} 6 &amp; 11 \\\\ 12 &amp; 2 \\end{array} \\right) ~~~\\mathbf{A}-\\mathbf{B}=\\left( \\begin{array}{cc} -2 &amp; -1 \\\\ -4 &amp; 0 \\end{array} \\right) . \\end{equation*}\\] Basic Linear Regression Example of Addition and Subtraction. Now, recall that the basic linear regression model can be written as \\(n\\) equations: \\[\\begin{equation*} \\begin{array}{c} y_1=\\beta_0+\\beta_1x_1+\\varepsilon _1 \\\\ \\vdots \\\\ y_n=\\beta_0+\\beta_1x_n+\\varepsilon _n. \\end{array} \\end{equation*}\\] We can define \\[\\begin{equation*} \\mathbf{y}=\\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) ~~~\\boldsymbol \\varepsilon = \\left( \\begin{array}{c} \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_n \\end{array} \\right) ~~~\\text{and}~~~ \\mathrm{E~}\\mathbf{y} =\\left( \\begin{array}{c} \\beta_0+\\beta_1 x_1 \\\\ \\vdots \\\\ \\beta_0 + \\beta_1 x_n \\end{array} \\right) . \\end{equation*}\\] With this notation, we can express the \\(n\\) equations more compactly as \\(\\mathbf{y} = \\mathrm{E~}\\mathbf{y}+\\boldsymbol \\varepsilon\\). Matrix Multiplication In general, if \\(\\mathbf{A}\\) is a matrix of dimension \\(n\\times c\\) and $ $ is a matrix of dimension \\(c\\times k\\), then \\(\\mathbf{C}=\\mathbf{AB }\\) is a matrix of dimension \\(n\\times k\\) and is defined by \\[\\begin{equation*} \\mathbf{C}=\\mathbf{AB=}\\left( \\sum_{s=1}^{c}a_{is}b_{sj}\\right) _{ij}. \\end{equation*}\\] For example consider the \\(2\\times 2\\) matrices \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{cc} 2 &amp; 5 \\\\ 4 &amp; 1 \\end{array} \\right) ~~~\\mathbf{B}=\\left( \\begin{array}{cc} 4 &amp; 6 \\\\ 8 &amp; 1 \\end{array} \\right) . \\end{equation*}\\] The matrix \\(\\mathbf{AB}\\) has dimension \\(2\\times 2\\). To illustrate the calculation, consider the number in the first row and second column of $ $. By the rule presented above, with \\(i=1\\) and \\(j=2\\), the corresponding element of \\(\\mathbf{AB}\\) is \\(\\sum_{s=1}^2a_{1s}b_{s2}=\\) \\(a_{11}b_{12}+a_{12}b_{22}=2(6)+5(1)=17\\). The other calculations are summarized as \\[\\begin{equation*} \\mathbf{AB}=\\left( \\begin{array}{cc} 2(4)+5(8) &amp; 2(6)+5(1) \\\\ 4(4)+1(8) &amp; 4(6)+1(1) \\end{array} \\right) =\\left( \\begin{array}{cc} 48 &amp; 17 \\\\ 24 &amp; 25 \\end{array} \\right) . \\end{equation*}\\] As another example, suppose \\[\\begin{equation*} \\mathbf{A}=\\left( \\begin{array}{ccc} 1 &amp; 2 &amp; 4 \\\\ 0 &amp; 5 &amp; 8 \\end{array} \\right) ~~~\\mathbf{B}=\\left( \\begin{array}{c} 3 \\\\ 5 \\\\ 2 \\end{array} \\right) . \\end{equation*}\\] Because \\(\\mathbf{A}\\) has dimension \\(2\\times 3\\) and \\(\\mathbf{B}\\) has dimension \\(3\\times 1\\), this means that the product \\(\\mathbf{AB}\\) has dimension \\(2\\times 1\\).. The calculations are summarized as \\[\\begin{equation*} \\mathbf{AB}=\\left( \\begin{array}{c} 1(3)+2(5)+4(2) \\\\ 0(3)+5(5)+8(2) \\end{array} \\right) =\\left( \\begin{array}{c} 21 \\\\ 41 \\end{array} \\right) . \\end{equation*}\\] For some additional examples, we have \\[\\begin{equation*} \\left( \\begin{array}{cc} 4 &amp; 2 \\\\ 5 &amp; 8 \\end{array} \\right) \\left( \\begin{array}{c} a_1 \\\\ a_2 \\end{array} \\right) =\\left( \\begin{array}{c} 4a_1+2a_2 \\\\ 5a_1+8a_2 \\end{array} \\right) . \\end{equation*}\\] \\[\\begin{equation*} \\left( \\begin{array}{ccc} 2 &amp; 3 &amp; 5 \\end{array} \\right) \\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 5 \\end{array} \\right) =2^2+3^2+5^2=38~~~\\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 5 \\end{array} \\right) \\left( \\begin{array}{ccc} 2 &amp; 3 &amp; 5 \\end{array} \\right) =\\left( \\begin{array}{ccc} 4 &amp; 6 &amp; 10 \\\\ 6 &amp; 9 &amp; 15 \\\\ 10 &amp; 15 &amp; 25 \\end{array} \\right) . \\end{equation*}\\] In general, you see that \\(\\mathbf{AB\\neq BA}\\) in matrix multiplication, unlike multiplication of scalars (real numbers). Further, we remark that the identity matrix serves the role of “one” in matrix multiplication, in that \\(\\mathbf{AI=A}\\) and \\(\\mathbf{IA=A}\\) for any matrix \\(\\mathbf{A}\\), providing that the dimensions are compatible to allow matrix multiplication. Basic Linear Regression Example of Matrix Multiplication. Define \\[\\begin{equation*} \\mathbf{X}=\\left( \\begin{array}{cc} 1 &amp; x_1 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{array} \\right) \\text{ and }\\boldsymbol \\beta =\\left( \\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array} \\right) \\text{, to get } \\mathbf X \\boldsymbol \\beta =\\left( \\text{\\ } \\begin{array}{c} \\beta_0+\\beta_1x_1 \\\\ \\vdots \\\\ \\beta_0+\\beta_1x_n \\end{array} \\right) =\\mathbf{\\mathrm{E~}\\mathbf{y.}} \\end{equation*}\\] Thus, this yields the familiar matrix expression of the regression model, $ + .$ Other useful quantities include \\[\\begin{equation*} \\mathbf{y}^{\\prime }\\mathbf{y=}\\left( \\begin{array}{ccc} y_1 &amp; \\cdots &amp; y_n \\end{array} \\right) \\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) =y_1^2+\\cdots +y_n^2=\\sum_{i=1}^{n}y_i^2, \\end{equation*}\\] \\[\\begin{equation*} \\mathbf{X}^{\\prime }\\mathbf{y=}\\left( \\begin{array}{ccc} 1 &amp; \\cdots &amp; 1 \\\\ x_1 &amp; \\cdots &amp; x_n \\end{array} \\right) \\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) =\\left( \\begin{array}{c} \\sum_{i=1}^{n}y_i \\\\ \\sum_{i=1}^{n}x_iy_i \\end{array} \\right) \\end{equation*}\\] and \\[\\begin{equation*} \\mathbf{X}^{\\prime }\\mathbf{X=}\\left( \\begin{array}{ccc} 1 &amp; \\cdots &amp; 1 \\\\ x_1 &amp; \\cdots &amp; x_n \\end{array} \\right) \\left( \\begin{array}{cc} 1 &amp; x_1 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{array} \\right) =\\left( \\begin{array}{cc} n &amp; \\sum_{i=1}^{n}x_i \\\\ \\sum_{i=1}^{n}x_i &amp; \\sum_{i=1}^{n} x_i^2 \\end{array} \\right) . \\end{equation*}\\] Note that \\(\\mathbf{X}^{\\prime }\\mathbf{X}\\) is a symmetric matrix. Matrix Inverses In matrix algebra, there is no concept of “division.” Instead, we extend the concept of “reciprocals” of real numbers. To begin, suppose that \\(\\mathbf{A}\\) is a square matrix of dimension \\(k \\times k\\) and let \\(\\mathbf{I}\\) be the \\(k\\times k\\) identity matrix. If there exists a \\(k \\times k\\) matrix \\(\\mathbf{B}\\) such that \\(\\mathbf{AB}=\\mathbf{I=BA}\\), then \\(\\mathbf{B}\\) is called the inverseof \\(\\mathbf{A}\\) and is written \\[\\begin{equation*} \\mathbf{B}=\\mathbf{A}^{-1}. \\end{equation*}\\] Now, not all square matrices have inverses. Further, even when an inverse exists, it may not be easy to compute by hand. One exception to this rule are diagonal matrices. Suppose that \\(\\mathbf{A}\\) is diagonal matrix of the form \\[\\begin{equation*} \\mathbf{A=}\\left( \\begin{array}{ccc} a_{11} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; a_{kk} \\end{array} \\right). \\text{ Then }\\mathbf{A}^{-1}\\mathbf{=}\\left( \\begin{array}{ccc} \\frac{1}{a_{11}} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; \\frac{1}{a_{kk}} \\end{array} \\right). \\end{equation*}\\] For example, \\[\\begin{equation*} \\begin{array}{cccc} \\left( \\begin{array}{cc} 2 &amp; 0 \\\\ 0 &amp; -19 \\end{array} \\right) &amp; \\left( \\begin{array}{cc} \\frac{1}{2} &amp; 0 \\\\ 0 &amp; -\\frac{1}{19} \\end{array} \\right) &amp; = &amp; \\left( \\begin{array}{cc} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right) \\\\ \\mathbf{A} &amp; \\mathbf{A}^{-1} &amp; = &amp; \\mathbf{I} \\end{array} . \\end{equation*}\\] In the case of a matrix of dimension \\(2\\times 2\\), the inversion procedure can be accomplished by hand easily even when the matrix is not diagonal. In the \\(2\\times 2\\) case, we suppose that if \\[\\begin{equation*} \\mathbf{A=}\\left( \\begin{array}{cc} a &amp; b \\\\ c &amp; d \\end{array} \\right), \\text{ then }\\mathbf{A}^{-1}\\mathbf{=}\\frac{1}{ad-bc}\\left( \\begin{array}{cc} d &amp; -b \\\\ -c &amp; a \\end{array} \\right) \\text{.} \\end{equation*}\\] Thus, for example, if \\[\\begin{equation*} \\mathbf{A=}\\left( \\begin{array}{cc} 2 &amp; 2 \\\\ 3 &amp; 4 \\end{array} \\right) \\text{ then }\\mathbf{A}^{-1}\\mathbf{=}\\frac{1}{2(4)-2(3)} \\left( \\begin{array}{cc} 4 &amp; -2 \\\\ -3 &amp; 2 \\end{array} \\right) =\\left( \\begin{array}{cc} 2 &amp; -1 \\\\ -3/2 &amp; 1 \\end{array} \\right) \\text{.} \\end{equation*}\\] As a check, we have \\[\\begin{equation*} \\mathbf{A\\mathbf{A}^{-1}=}\\left( \\begin{array}{cc} 2 &amp; 2 \\\\ 3 &amp; 4 \\end{array} \\right) \\left( \\begin{array}{cc} 2 &amp; -1 \\\\ -3/2 &amp; 1 \\end{array} \\right) =\\left( \\begin{array}{cc} 2(2)-2(3/2) &amp; 2(-1)+2(1) \\\\ 3(2)-4(3/2) &amp; 3(-1)+4(1) \\end{array} \\right) =\\left( \\begin{array}{cc} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right) =\\mathbf{I}\\text{.} \\end{equation*}\\] Basic Linear Regression Example of Matrix Inverses. With \\[\\begin{equation*} \\mathbf{X}^{\\prime }\\mathbf{X=}\\left( \\begin{array}{cc} n &amp; \\sum\\limits_{i=1}^{n}x_i \\\\ \\sum\\limits_{i=1}^{n}x_i &amp; \\sum\\limits_{i=1}^{n}x_i^2 \\end{array} \\right), \\end{equation*}\\] we have \\[\\begin{equation*} \\left( \\mathbf{X}^{\\prime }\\mathbf{X}\\right) ^{-1}\\mathbf{=}\\frac{1}{n\\sum_{i=1}^{n}x_i^2-\\left( \\sum_{i=1}^{n}x_i\\right) ^2}\\left( \\begin{array}{cc} \\sum\\limits_{i=1}^{n}x_i^2 &amp; -\\sum\\limits_{i=1}^{n}x_i \\\\ -\\sum\\limits_{i=1}^{n}x_i &amp; n \\end{array} \\right). \\end{equation*}\\] To simplify this expression, recall that \\(\\overline{x}=n^{-1} \\sum_{i=1}^{n}x_i\\). Thus, \\[\\begin{equation} \\left( \\mathbf{X}^{\\prime }\\mathbf{X}\\right) ^{-1}\\mathbf{=}\\frac{1}{ \\sum_{i=1}^{n}x_i^2-n\\overline{x}^2}\\left( \\begin{array}{cc} n^{-1}\\sum\\limits_{i=1}^{n}x_i^2 &amp; -\\overline{x} \\\\ -\\overline{x} &amp; 1 \\end{array} \\right) . \\tag{2.9} \\end{equation}\\] Section 3.1 will discuss the relation \\(\\mathbf{b}=\\left( \\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime}\\mathbf{y}\\). To illustrate the calculation, we have \\[\\begin{eqnarray*} \\mathbf{b} &amp;=&amp;\\left( \\mathbf{X}^{\\prime }\\mathbf{X}\\right) ^{-1}\\mathbf{X} ^{\\prime }\\mathbf{y=}\\frac{1}{\\sum_{i=1}^{n}x_i^2-n\\overline{x}^2} \\left( \\begin{array}{cc} n^{-1}\\sum\\limits_{i=1}^{n}x_i^2 &amp; -\\overline{x} \\\\ -\\overline{x} &amp; 1 \\end{array} \\right) \\left( \\begin{array}{c} \\sum\\limits_{i=1}^{n}y_i \\\\ \\sum\\limits_{i=1}^{n}x_iy_i \\end{array} \\right) \\\\ &amp;=&amp;\\frac{1}{\\sum_{i=1}^{n}x_i^2-n\\overline{x}^2}\\left( \\begin{array}{c} \\sum\\limits_{i=1}^{n}\\left( \\overline{y}x_i^2-\\overline{x} x_iy_i\\right) \\\\ \\sum\\limits_{i=1}^{n}x_iy_i-n\\overline{x}\\overline{y} \\end{array} \\right) =\\left( \\begin{array}{c} b_0 \\\\ b_1 \\end{array} \\right) . \\end{eqnarray*}\\] From this expression, we may see \\[\\begin{equation*} b_1=\\frac{\\sum\\limits_{i=1}^{n}x_iy_i-n\\overline{x}\\overline{y}}{ \\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x}^2} \\end{equation*}\\] and \\[\\begin{equation*} b_0=\\frac{\\overline{y}\\sum\\limits_{i=1}^{n}x_i^2-\\overline{x} \\sum\\limits_{i=1}^{n}x_iy_i}{\\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x} ^2}=\\frac{\\overline{y}\\left( \\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x} ^2\\right) -\\overline{x}\\left( \\sum\\limits_{i=1}^{n}x_i y_i - n\\overline{x} \\overline{y}\\right) }{\\sum\\limits_{i=1}^{n}x_i^2-n\\overline{x}^2}= \\overline{y}-b_1\\overline{x}. \\end{equation*}\\] These are the usual expressions for the slope \\(b_1\\) (Exercise 2A.8) and intercept \\(b_0\\). 2.11.4 Random Matrices Expectations. Consider a matrix of random variables \\[\\begin{equation*} \\mathbf{U=}\\left( \\begin{array}{cccc} u_{11} &amp; u_{12} &amp; \\cdots &amp; u_{1c} \\\\ u_{21} &amp; u_{22} &amp; \\cdots &amp; u_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ u_{n1} &amp; u_{n2} &amp; \\cdots &amp; u_{nc} \\end{array} \\right). \\end{equation*}\\] When we write the expectation of a matrix, this is short-hand for the matrix of expectations. Specifically, suppose that the joint probability function of \\({u_{11}, u_{12}, ..., u_{1c}, ..., u_{n1}, ..., u_{nc}}\\) is available to define the expectation operator. Then we define \\[\\begin{equation*} \\mathrm{E} ~ \\mathbf{U} = \\left( \\begin{array}{cccc} \\mathrm{E }u_{11} &amp; \\mathrm{E }u_{12} &amp; \\cdots &amp; \\mathrm{E }u_{1c} \\\\ \\mathrm{E }u_{21} &amp; \\mathrm{E }u_{22} &amp; \\cdots &amp; \\mathrm{E }u_{2c} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathrm{E }u_{n1} &amp; \\mathrm{E }u_{n2} &amp; \\cdots &amp; \\mathrm{E }u_{nc} \\end{array} \\right). \\end{equation*}\\] As an important special case, consider the joint probability function for the random variables \\(y_1, \\ldots, y_n\\) and the corresponding expectations operator. Then \\[\\begin{equation*} \\mathrm{E}~ \\mathbf{y=} \\mathrm{E } \\left( \\begin{array}{cccc} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) = \\left( \\begin{array}{cccc} \\mathrm{E }y_1 \\\\ \\vdots \\\\ \\mathrm{E }y_n \\end{array} \\right). \\end{equation*}\\] By the linearity of expectations, for a non-random matrix A and vector , we have E ( + ) = E . Variances. We can also work with second moments of random vectors. The variance of a vector of random variables is called the variance-covariance matrix. It is defined by \\[\\begin{equation} \\mathrm{Var} ~ \\mathbf{y} = \\mathrm{E} ( (\\mathbf{y} - \\mathrm{E} \\mathbf{y})(\\mathbf{y} - \\mathrm{E} \\mathbf{y})^{\\prime} ). \\tag{2.10} \\end{equation}\\] That is, we can express \\[\\begin{equation*} \\mathrm{Var}~\\mathbf{y=} \\mathrm{E } \\left( \\left( \\begin{array}{c} y_1 -\\mathrm{E } y_1 \\\\ \\vdots \\\\ y_n -\\mathrm{E } y_n \\end{array}\\right) \\left(\\begin{array}{ccc} y_1 - \\mathrm{E } y_1 &amp; \\cdots &amp; y_n - \\mathrm{E } y_n \\end{array}\\right) \\right) \\end{equation*}\\] \\[\\begin{equation*} = \\left( \\begin{array}{cccc} \\mathrm{Var}~y_1 &amp; \\mathrm{Cov}(y_1, y_2) &amp; \\cdots &amp;\\mathrm{Cov}(y_1, y_n) \\\\ \\mathrm{Cov}(y_2, y_1) &amp; \\mathrm{Var}~y_2 &amp; \\cdots &amp; \\mathrm{Cov}(y_2, y_n) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\mathrm{Cov}(y_n, y_1) &amp; \\mathrm{Cov}(y_n, y_2) &amp; \\cdots &amp; \\mathrm{Var}~y_n \\\\ \\end{array}\\right), \\end{equation*}\\] because \\(\\mathrm{E} ( (y_i - \\mathrm{E} y_i)(y_j - \\mathrm{E} y_j) ) = \\mathrm{Cov}(y_i, y_j)\\) for \\(i \\neq j\\) and \\(\\mathrm{Cov}(y_i, y_i) = \\mathrm{Var}~y_i\\). In the case that \\(y_1, \\ldots, y_n\\) are mutually uncorrelated, we have that \\(\\mathrm{Cov}(y_i, y_j)=0\\) for \\(i \\neq j\\) and thus \\[\\begin{equation*} \\mathrm{Var}~\\mathbf{y=} \\left( \\begin{array}{cccc} \\mathrm{Var}~y_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\mathrm{Var}~y_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0 &amp; \\cdots &amp; \\mathrm{Var}~y_n \\\\ \\end{array}\\right). \\end{equation*}\\] Further, if the variances are identical so that \\(\\mathrm{Var}~y_i=\\sigma ^2\\), then we can write \\(\\mathrm{Var} ~\\mathbf{y} = \\sigma ^2 \\mathbf{I}\\), where I is the \\(n \\times n\\) identity matrix. For example, if \\(y_1, \\ldots, y_n\\) are i.i.d., then \\(\\mathrm{Var} ~\\mathbf{y} = \\sigma ^2 \\mathbf{I}\\). From equation (2.10), it can be shown that \\[\\begin{equation} \\mathrm{Var}\\left( \\mathbf{Ay +B} \\right) = \\mathrm{Var}\\left( \\mathbf{Ay} \\right) = \\mathbf{A} \\left( \\mathrm{Var}~\\mathbf{y} \\right) \\mathbf{A}^{\\prime}. \\tag{2.11} \\end{equation}\\] For example, if \\(\\mathbf{A} = (a_1, a_2, \\ldots,a_n)= \\mathbf{a}^{\\prime}\\) and B = 0, then equation (2.11) reduces to \\[\\begin{equation*} \\mathrm{Var}\\left( \\sum_{i=1}^n a_i y_i \\right) = \\mathrm{Var} \\left( \\mathbf{a^{\\prime} y} \\right) = \\mathbf{a^{\\prime}} \\left( \\mathrm{Var} ~\\mathbf{y} \\right) \\mathbf{a} = (a_1, a_2, \\ldots,a_n) \\left( \\mathrm{Var} ~\\mathbf{y} \\right) \\left(\\begin{array}{c} a_1 \\\\ \\vdots \\\\ a_n \\end{array}\\right) \\end{equation*}\\] \\[\\begin{equation*} = \\sum_{i=1}^n a_i^2 \\mathrm{Var} ~y_i ~+~2 \\sum_{i=2}^n \\sum_{j=1}^{i-1} a_i a_j \\mathrm{Cov}(y_i, y_j). \\end{equation*}\\] Definition - Multivariate Normal Distribution. A vector of random variables \\(\\mathbf{y} = \\left(y_1, \\ldots, y_n \\right)^{\\prime}\\) is said to be multivariate normal if all linear combinations of the form \\(\\sum_{i=1}^n a_i y_i\\) are normally distributed. In this case, we write \\(\\mathbf{y} \\sim N (\\mathbf{\\boldsymbol \\mu}, \\mathbf{\\Sigma} )\\), where \\(\\mathbf{\\boldsymbol \\mu} = \\mathrm{E}~ \\mathbf{y}\\) is the expected value of y and \\(\\mathbf{\\Sigma}= \\mathrm{Var}~\\mathbf{y}\\) is the variance-covariance matrix of y. From the definition, we have that \\(\\mathbf{y}\\sim N (\\mathbf{\\boldsymbol \\mu}, \\mathbf{\\Sigma} )\\) implies that \\(\\mathbf{a^{\\prime}y}\\sim N (\\mathbf{a^{\\prime} \\boldsymbol \\mu}, \\mathbf{a^{\\prime}\\Sigma a})\\). Thus, if \\(y_i\\) are i.i.d., then \\(\\sum_{i=1}^n a_i y_i\\) is distributed normally with mean \\(\\mu \\sum_{i=1}^n a_i\\) and variance \\(\\sigma ^2 \\sum_{i=1}^n a_i ^2\\). "],["C3BasicMLR.html", "Chapter 3 Multiple Linear Regression - I 3.1 Method of Least Squares 3.2 Linear Regression Model and Properties of Estimators 3.3 Estimation and Goodness of Fit 3.4 Statistical Inference for a Single Coefficient 3.5 Some Special Explanatory Variables 3.6 Further Reading and References 3.7 Exercises", " Chapter 3 Multiple Linear Regression - I Chapter Preview. This chapter introduces linear regression in the case of several explanatory variables, known as multiple linear regression. Many basic linear regression concepts extend directly, including goodness of fit measures such as \\(R^2\\) and inference using \\(t\\)-statistics. Multiple linear regression models provide a framework for summarizing highly complex, multivariate data. Because this framework requires only linearity in the parameters, we are able to fit models that are nonlinear functions of the explanatory variables, thus providing a wide scope of potential applications. 3.1 Method of Least Squares 3.1.1 Least Squares Method Chapter 2 dealt with the problem of a response depending on a single explanatory variable. We now extend the focus of that chapter and study how a response may depend on several explanatory variables. Example: Term Life Insurance. Like all firms, life insurance companies continually seek new ways to deliver products to the market. Those involved in product development wish to know “who buys insurance and how much do they buy?” In economics, this is known as the demand side of a market for products. Analysts can readily get information on characteristics of current customers through company databases. Potential customers, those that do not have insurance with the company, are often the main focus for expanding market share. In this example, we examine the Survey of Consumer Finances (SCF), a nationally representative sample that contains extensive information on assets, liabilities, income, and demographic characteristics of those sampled (potential U.S. customers). We study a random sample of 500 households with positive incomes that were interviewed in the 2004 survey. We initially consider the subset of \\(n=275\\) families that purchased term life insurance. We wish to address the second portion of the demand question and determine family characteristics that influence the amount of insurance purchased. Chapter 11 will consider the first portion, whether or not a household purchases insurance, through models where the response is a binary random variable. For term life insurance, the quantity of insurance is measured by the policy FACE, the amount that the company will pay in the event of the death of the named insured. Characteristics that will turn out to be important include annual INCOME, the number of years of EDUCATION of the survey respondent, and the number of household members, NUMHH. 3.1.2 General Case with k Explanatory Variables In general, we will consider data sets where there are \\(k\\) explanatory variables and one response variable in a sample of size \\(n\\). That is, the data consist of: \\[ \\left\\{ \\begin{aligned} x_{11},x_{12},\\ldots,x_{1k},y_1 \\\\ x_{21},x_{22},\\ldots,x_{2k},y_2 \\\\ \\vdots \\\\ x_{n1},x_{n2},\\ldots,x_{nk},y_n \\end{aligned} \\right\\}. \\] The \\(i\\)th observation corresponds to the \\(i\\)th row, consisting of \\((x_{i1},x_{i2},\\ldots,x_{ik},y_i)\\). For this general case, we take \\(k+1\\) measurements on each entity. For the insurance demand example, \\(k=3\\) and the data consists of \\((x_{11},x_{12},x_{13}, y_1), \\ldots , (x_{275,1},x_{275,2},x_{275,3},y_{275})\\). That is, we use four measurements from each of the \\(n=275\\) households. Summarizing the Data We begin the data analysis by examining each variable in isolation from the others. Table 3.1 provides basic summary statistics of the four variables. For FACE and INCOME, we see that the mean is much greater than the median, suggesting that the distribution is skewed to the right. Histograms (not reported here) show that this is the case. It will turn out to be useful to also consider their logarithmic transforms, LNFACE and LNINCOME, respectively, which are also reported in Table 3.1. Table 3.1: Term Life Summary Statistics Mean Median Standard Deviation Minimum Maximum FACE 747,581 150,000 1,674,362 800 14,000,000 INCOME 208,975 65,000 824,010 260 10,000,000 EDUCATION 14.524 16 2.549 2 17 NUMHH 2.96 3 1.493 1 9 LNFACE 11.99 11.918 1.871 6.685 16.455 LNINCOME 11.149 11.082 1.295 5.561 16.118 The next step is to measure the relationship between each \\(x\\) and \\(y\\), beginning with the scatter plots in Figure 3.1. The left-hand panel is a plot of FACE versus INCOME; in this panel, we see a large clustering in the lower left-hand corner corresponding to households that have both small incomes and face amounts of insurance. Both variables have skewed distributions and their joint effect is highly nonlinear. The right-hand panel presents the same variables but using logarithmic transforms. Here, we see a relationship that can be more readily approximated with a line. Figure 3.1: Income versus Face Amount of Term Life Insurance. The left-panel is a plot of face versus income, showing a highly nonlinear pattern. In the right-hand panel, face versus income is in natural logarithmic units, suggesting a linear (although variable) pattern. R Code to Produce Table 3.1 and Figure 3.1 Term &lt;- read.csv(&quot;CSVData/TermLife.csv&quot;, header=TRUE) # PICK THE SUBSET OF THE DATA CORRESPONDING TO TERM PURCHASE Term2 &lt;-subset(Term, FACE &gt; 0) # TABLE 3.1 SUMMARY STATISTICS BookSummStats &lt;- function(Xymat){ meanSummary &lt;- sapply(Xymat, mean, na.rm=TRUE) sdSummary &lt;- sapply(Xymat, sd, na.rm=TRUE) minSummary &lt;- sapply(Xymat, min, na.rm=TRUE) maxSummary &lt;- sapply(Xymat, max, na.rm=TRUE) medSummary &lt;- sapply(Xymat, median,na.rm=TRUE) tableMat &lt;- cbind(meanSummary, medSummary, sdSummary, minSummary, maxSummary) return(tableMat) } LNFACE &lt;- log(Term2$FACE) LNINCOME &lt;- log(Term2$INCOME) Xymat &lt;- data.frame(cbind(Term2$FACE, Term2$INCOME,Term2$EDUCATION, Term2$NUMHH,LNFACE, LNINCOME) ) tableMat &lt;- BookSummStats(Xymat) colnames(tableMat) &lt;- c(&quot;Mean&quot; , &quot;Median&quot; , &quot;Standard Deviation&quot; , &quot;Minimum&quot; , &quot;Maximum&quot;) rownames(tableMat) &lt;- c(&quot;FACE&quot;, &quot;INCOME&quot;, &quot;EDUCATION&quot;, &quot;NUMHH&quot;, &quot;LNFACE&quot;, &quot;LNINCOME&quot;) tableMat1 &lt;- tableMat tableMat1[3:6,] &lt;- round(tableMat1[3:6,], digits = 3) tableMat1[1:2,] &lt;- format(round(tableMat[1:2,], digits=0), big.mark = &#39;,&#39;) TableGen1(TableData=tableMat1, TextTitle=&#39;Term Life Summary Statistics&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:5, ColWidth = ColWidth5) Term &lt;- read.csv(&quot;CSVData/TermLife.csv&quot;, header=TRUE) # PICK THE SUBSET OF THE DATA CORRESPONDING TO TERM PURCHASE Term2 &lt;-subset(Term, FACE &gt; 0) Term2$LNFACE &lt;- log(Term2$FACE) Term2$LNINCOME &lt;- log(Term2$INCOME) # FIGURE 3.1 par(mfrow=c(1, 2), cex=1.1, mar=c(4.1,4,1.5,1)) plot(Term2$INCOME, Term2$FACE, ylab=&quot;&quot;, las=1, yaxt=&quot;n&quot;, xaxt=&quot;n&quot;, xlab=&quot;INCOME (in Millions)&quot;) mtext(&quot;FACE (in Millions)&quot;, side=2, at=15200000, las=1, cex=1.1, adj=.4) axis(2,at=seq(0,14000000,2000000), labels=c(&quot;0&quot;, &quot;2&quot;, &quot;4&quot;, &quot;6&quot;, &quot;8&quot;,&quot;10&quot;,&quot;12&quot;,&quot;14&quot;), las=1) axis(1,at=seq(0,11000000,1000000), labels=c(&quot;0&quot;,&quot;1&quot;, &quot;2&quot;,&quot;3&quot;, &quot;4&quot;,&quot;5&quot;, &quot;6&quot;,&quot;7&quot;,&quot;8&quot;,&quot;9&quot;,&quot;10&quot;,&quot;11&quot;)) plot(Term2$LNINCOME, Term2$LNFACE, ylab=&quot;&quot;, xlab = &quot;LNINCOME&quot;, las=1) mtext(&quot;LNFACE&quot;, side=2, at=16.8, las=1, cex=1.1, adj=1.1) The Term Life data are multivariate in the sense that several measurements are taken on each household. It is difficult to produce a graph of observations in three or more dimensions on a two-dimensional platform, such as a piece of paper, that is not confusing, misleading, or both. To summarize graphically multivariate data in regression applications, consider using a scatterplot matrix such as in Figure 3.2. Each square of this figure represents a simple plot of one variable versus another. For each square, the row variable gives the units of the vertical axis and the column variable gives the units of the horizontal axis. The matrix is sometimes called a half scatterplot matrix because only the lower left-hand elements are presented. Figure 3.2: Scatterplot matrix of four variables. Each square is a scatter plot. The scatterplot matrix can be numerically summarized using a correlation matrix. Each correlation in Table 3.2 corresponds to a square of the scatterplot matrix in Figure 3.2. Analysts often present tables of correlations because they are easy to interpret. However, remember that a correlation coefficient merely measures the extent of linear relationships. Thus, a table of correlations provides a sense of linear relationships but may miss a nonlinear relationship that can be revealed in a scatterplot matrix. Table 3.2: Term Life Correlations NUMHH EDUCATION LNINCOME EDUCATION -0.064 LNINCOME 0.179 0.343 LNFACE 0.288 0.383 0.482 The scatterplot matrix and corresponding correlation matrix are useful devices for summarizing multivariate data. They are easy to produce and to interpret. Still, each device captures only relationships between pairs of variables and cannot quantify relationships among several variables. R Code to Produce Table 3.2 and Figure 3.2 tableCor &lt;- cor(Term1) tableCor &lt;- round(tableCor, digits = 3) tableCor[upper.tri(tableCor, diag = TRUE)] &lt;- &quot;&quot; tablePrint &lt;- tableCor[-1,] tablePrint &lt;- tablePrint[,-4] TableGen1(TableData=tablePrint, TextTitle=&#39;Term Life Correlations&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:3, ColWidth = ColWidth5) # FIGURE 3.2 par(mar=c(4.1,2.1,2.1,2.1), cex=1.1) varTerm &lt;- c(&quot;NUMHH&quot;,&quot;EDUCATION&quot;, &quot;LNINCOME&quot;, &quot;LNFACE&quot;) Term1 &lt;- Term2[varTerm] pairs(Term1,upper.panel=NULL, gap=0,cex.labels=1.25, las=1) Method of Least Squares Consider the question: “Can knowledge of education, household size, and income help us understand the demand for insurance?” The correlations in Table 3.2 and the graphs in Figures 3.1 and 3.2 suggest that each variable, EDUCATION, NUMHH, and LNINCOME, may be a useful explanatory variable of LNFACE when taken individually. It seems reasonable to investigate the joint effect of these variables on a response. The geometric concept of a plane is used to explore the linear relationship between a response and several explanatory variables. Recall that a plane extends the concept of a line to more than two dimensions. A plane may be defined through an algebraic equation such as \\[ y = b_0 + b_1 x_1 + \\ldots + b_k x_k. \\] This equation defines a plane in \\(k+1\\) dimensions. Figure 3.3 shows a plane in three dimensions. For this figure, there is one response variable, LNFACE, and two explanatory variables, EDUCATION and LNINCOME (NUMHH is held fixed). It is difficult to graph more than three dimensions in a meaningful way. Figure 3.3: An example of a three-dimensional plane We need a way to determine a plane based on the data. The difficulty is that in most regression analysis applications, the number of observations, \\(n\\), far exceeds the number of observations required to fit a plane, \\(k+1\\). Thus, it is generally not possible to find a single plane that passes through all \\(n\\) observations. As in Chapter 2, we use the method of least squares to determine a plane from the data. The method of least squares is based on determining the values of \\(b_0^{\\ast},b_1^{\\ast},\\ldots,b_k^{\\ast}\\) that minimize the quantity \\[\\begin{equation} SS(b_0^{\\ast},b_1^{\\ast},\\ldots,b_k^{\\ast})=\\sum_{i=1}^{n}\\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_{i1}+\\ldots+b_k^{\\ast}x_{ik}\\right) \\right) ^2. \\tag{3.1} \\end{equation}\\] We drop the asterisk, or star, notation and use \\(b_0, b_1, \\ldots, b_k\\) to denote the best values, known as the least squares estimates. With the least squares estimates, define the least squares, or fitted, regression plane as \\[ \\widehat{y} = b_0 + b_1 x_1 + \\ldots + b_k x_k. \\] The least squares estimates are determined by minimizing \\(SS(b_0^{\\ast},b_1^{\\ast},\\ldots,b_k^{\\ast})\\). It is difficult to write down the resulting least squares estimators using a simple formula unless one resorts to matrix notation. Because of their importance in applied statistical models, an explicit formula for the estimators is provided below. However, these formulas have been programmed into a wide variety of statistical and spreadsheet software packages. The availability of these packages allows data analysts to concentrate on the ideas of the estimation procedure instead of focusing on the details of the calculation procedures. As an example, a regression plane was fit to the Term Life data where three explanatory variables, \\(x_1\\) for EDUCATION, \\(x_2\\) for NUMHH, and \\(x_3\\) for LNINCOME, were used. The resulting fitted regression plane is \\[\\begin{equation} \\widehat{y} = 2.584 + 0.206 x_1 + 0.306 x_2 + 0.494 x_3. \\tag{3.2} \\end{equation}\\] Matrix Notation Assume that the data are of the form \\((x_{i0}, x_{i1}, \\ldots, x_{ik}, y_i)\\), where \\(i = 1, \\ldots, n\\). Here, the variable \\(x_{i0}\\) is associated with the “intercept” term. In most applications, we assume that \\(x_{i0}\\) is identically equal to 1 and thus need not be explicitly represented. However, there are important applications where this is not the case, and thus, to express the model in general notation, it is included here. The data are represented in matrix notation using: \\[ \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} ~~\\text{and}~~ \\mathbf{X} = \\begin{pmatrix} x_{10} &amp; x_{11} &amp; \\cdots &amp; x_{1k} \\\\ x_{20} &amp; x_{21} &amp; \\cdots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n0} &amp; x_{n1} &amp; \\cdots &amp; x_{nk} \\end{pmatrix}. \\] Here, \\(\\mathbf{y}\\) is the \\(n \\times 1\\) vector of responses and \\(\\mathbf{X}\\) is the \\(n \\times (k+1)\\) matrix of explanatory variables. We use the matrix algebra convention that lower and upper case bold letters represent vectors and matrices, respectively. (If you need to brush up on matrices, review Section 2.11.) Example: Term Life Insurance - Continued. Recall that \\(y\\) represents the logarithmic face, \\(x_1\\) for years of education, \\(x_2\\) for the number of household members, and \\(x_3\\) for logarithmic income. Thus, there are \\(k = 3\\) explanatory variables and \\(n = 275\\) households. The vector of responses and the matrix of explanatory variables are: \\[ \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{275} \\end{pmatrix} = \\begin{pmatrix} 9.904 \\\\ 11.775 \\\\ \\vdots \\\\ 9.210 \\end{pmatrix} ~~\\text{and}~~ \\\\ \\mathbf{X} = \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{275,1} &amp; x_{275,2} &amp; x_{275,3} \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 16 &amp; 3 &amp; 10.669 \\\\ 1 &amp; 9 &amp; 3 &amp; 9.393 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; 12 &amp; 1 &amp; 10.545 \\end{pmatrix}. \\] For example, for the first observation in the data set, the dependent variable is \\(y_1 = 9.904\\) (corresponding to \\(\\exp(9.904) = \\$20,000\\)), for a survey respondent with 16 years of education living in a household with 3 people and a logarithmic income of 10.669 (\\(\\exp(10.669) = \\$43,000\\)). Under the least squares estimation principle, our goal is to choose the coefficients \\(b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast}\\) to minimize the sum of squares function \\(SS(b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast})\\). Using calculus, we return to the equation for the sum of squares, take partial derivatives with respect to each coefficient, and set these quantities equal to zero: \\[ \\begin{array}{ll} \\frac{\\partial }{\\partial b_j^{\\ast}}SS(b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast}) &amp;= \\sum_{i=1}^{n}\\left( -2x_{ij}\\right) \\left( y_i-\\left( b_0^{\\ast}+b_1^{\\ast}x_{i1}+\\ldots+b_k^{\\ast}x_{ik}\\right) \\right) \\\\ &amp;= 0, ~~~ \\text{for} ~ j=0,1,\\ldots,k. \\end{array} \\] This is a system of \\(k+1\\) equations and \\(k+1\\) unknowns that can be readily solved using matrix notation, as follows. We may express the vector of parameters to be minimized as \\(\\mathbf{b}^{\\ast}=(b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast})^{\\prime}\\). Using this, the sum of squares can be written as \\(SS(\\mathbf{b}^{\\ast}) = (\\mathbf{y-Xb}^{\\ast})^{\\prime}(\\mathbf{y-Xb}^{\\ast})\\). Thus, in matrix form, the solution to the minimization problem can be expressed as \\(\\frac{\\partial}{\\partial \\mathbf{b}^{\\ast}} SS(\\mathbf{b}^{\\ast}) = \\mathbf{0}\\). This solution satisfies the normal equations: \\[\\begin{equation} \\mathbf{X^{\\prime}Xb} = \\mathbf{X}^{\\prime}\\mathbf{y}. \\tag{3.3} \\end{equation}\\] Here, the asterisk notation (*) has been dropped to denote the fact that \\(\\mathbf{b} = (b_0, b_1, \\ldots, b_k)^{\\prime}\\) represents the best vector of values in the sense of minimizing \\(SS(\\mathbf{b}^{\\ast})\\) over all choices of \\(\\mathbf{b}^{\\ast}\\). The least squares estimator \\(\\mathbf{b}\\) need not be unique. However, assuming that the explanatory variables are not linear combinations of one another, we have that \\(\\mathbf{X^{\\prime}X}\\) is invertible. In this case, we can write the unique solution as: \\[\\begin{equation} \\mathbf{b} = \\left( \\mathbf{X^{\\prime}X} \\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}. \\tag{3.4} \\end{equation}\\] To illustrate, for the Term Life example, equation (3.4) yields: \\[ \\mathbf{b} = \\begin{pmatrix} b_0 \\\\ b_1 \\\\ b_2 \\\\ b_3 \\\\ \\end{pmatrix} = \\begin{pmatrix} 2.584 \\\\ 0.206 \\\\ 0.306 \\\\ 0.494 \\\\ \\end{pmatrix}. \\] 3.2 Linear Regression Model and Properties of Estimators In the previous section, we learned how to use the method of least squares to fit a regression plane with a data set. This section describes the assumptions underpinning the regression model and some of the resulting properties of the regression coefficient estimators. With the model and the fitted data, we will be able to draw inferences about the sample data set to a larger population. Moreover, we will later use these regression model assumptions to help us improve the model specification in Chapter 5. 3.2.1 Regression Function Most of the assumptions of the multiple linear regression model will carry over directly from the basic linear regression model assumptions introduced in Section 2.2. The primary difference is that we now summarize the relationship between the response and the explanatory variables through the regression function: \\[\\begin{equation} \\mathrm{E~}y = \\beta_0 x_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k, \\tag{3.5} \\end{equation}\\] which is linear in the parameters \\(\\beta_0,\\ldots,\\beta_k\\). Henceforth, we will use \\(x_0 = 1\\) for the variable associated with the parameter \\(\\beta_0\\); this is the default in most statistical packages, and most applications of regression include the intercept term \\(\\beta_0\\). The intercept is the expected value of \\(y\\) when all of the explanatory variables are equal to zero. Although rarely of interest, the term \\(\\beta_0\\) serves to set the height of the fitted regression plane. In contrast, the other betas are typically important parameters from a regression study. To help interpret them, we initially assume that \\(x_j\\) varies continuously and is not related to the other explanatory variables. Then, we can interpret \\(\\beta_j\\) as the expected change in \\(y\\) per unit change in \\(x_j\\) assuming all other explanatory variables are held fixed. That is, from calculus, you will recognize that \\(\\beta_j\\) can be interpreted as a partial derivative. Specifically, using the equation above, we have that \\[ \\beta_j = \\frac{\\partial }{\\partial x_j}\\mathrm{E}~y. \\] 3.2.2 Regression Coefficient Interpretation Let us examine the regression coefficient estimates from the Term Life Insurance example and focus initially on the sign of the coefficients. For example, from equation (3.4), the coefficient associated with NUMHH is \\(b_2 = 0.306 &gt; 0\\). If we consider two households that have the same income and the same level of education, then the larger household (in terms of NUMHH) is expected to demand more term life insurance under the regression model. This is a sensible interpretation; larger households have more dependents for which term life insurance can provide needed financial assets in the event of the untimely death of a breadwinner. The positive coefficient associated with income (\\(b_3 = 0.494\\)) is also plausible; households with larger incomes have more disposable dollars to purchase insurance. The positive sign associated with EDUCATION (\\(b_1 = 0.206)\\) is also reasonable; more education suggests that respondents are more aware of their insurance needs, other things being equal. You will also need to interpret the amount of the regression coefficient. Consider first the EDUCATION coefficient. Using equation (3.4), fitted values of \\(\\widehat{\\mathrm{LNFACE}}\\) were calculated by allowing EDUCATION to vary and keeping NUMHH and LNINCOME fixed at the sample averages. The results are: Effects of Small Changes in Education EDUCATION 14 14.1 14.2 14.3 \\(\\widehat{\\mathit{LNFACE}}\\) 11.883 11.904 11.924 11.945 \\(\\widehat{\\mathit{FACE}}\\) 144,803 147,817 150,893 154,034 \\(\\widehat{\\mathit{FACE}}\\) % Change 2.081 2.081 2.081 As EDUCATION increases, \\(\\widehat{\\mathrm{LNFACE}}\\) increases. Further, the amount of \\(\\widehat{\\mathrm{LNFACE}}\\) increase is a constant 0.0206. This comes directly from equation (3.4); as EDUCATION increases by 0.1 years, we expect the demand for insurance to increase by 0.0206 logarithmic dollars, holding NUMHH and LNINCOME fixed. This interpretation is correct, but most product development directors are not overly fond of logarithmic dollars. To return to dollars, fitted face values can be calculated through exponentiation as \\(\\widehat{\\mathrm{FACE}} = \\exp(\\widehat{\\mathrm{LNFACE}})\\). Moreover, the percentage change can be computed; for example, \\(100 \\times (147,817/144,803 - 1) \\approx 2.08\\%\\). This provides another interpretation of the regression coefficient; as EDUCATION increases by 0.1 years, we expect the demand for insurance to increase by 2.08%. This is a simple consequence of calculus using \\(\\partial \\ln y / \\partial x = \\left(\\partial y / \\partial x \\right) / y\\); that is, a small change in the logarithmic value of \\(y\\) equals a small change in \\(y\\) as a proportion of \\(y\\). It is because of this calculus result that we use natural logs instead of common logs in regression analysis. Because this table uses a discrete change in EDUCATION, the 2.08% differs slightly from the continuous result \\(0.206 \\times (\\mathrm{change~in~EDUCATION}) = 2.06\\%\\). However, this proximity is usually regarded as suitable for interpretation purposes. Continuing this logic, consider small changes in logarithmic income. Effects of Small Changes in Logarithmic Income LNINCOME 11 11.1 11.2 11.3 INCOME 59,874 66,171 73,130 80,822 INCOME % Change 10.52 10.52 10.52 \\(\\widehat{\\mathit{LNFACE}}\\) 11.957 12.006 12.055 12.105 \\(\\widehat{\\mathit{FACE}}\\) 155,831 163,722 172,013 180,724 \\(\\widehat{\\mathit{FACE}}\\) % Change 5.06 5.06 5.06 \\(\\widehat{\\mathit{FACE}}\\) % Change / INCOME % Change 0.482 0.482 0.482 We can use the same logic to interpret the LNINCOME coefficient in equation (3.4). As logarithmic income increases by 0.1 units, we expect the demand for insurance to increase by 5.06%. This takes care of logarithmic units in the \\(y\\) but not the \\(x\\). We can use the same logic to say that as logarithmic income increases by 0.1 units, INCOME increases by 10.52%. Thus, a 10.52% change in INCOME corresponds to a 5.06% change in FACE. Summarizing, we say that, holding NUMHH and EDUCATION fixed, we expect that a 1% increase in INCOME is associated with a 0.482% increase in \\(\\widehat{\\mathrm{FACE}}\\) (as before, this is close to the parameter estimate \\(b_3 = 0.494\\)). The coefficient associated with income is known as an elasticity in economics. In economics, elasticity is the ratio of the percent change in one variable to the percent change in another variable. Mathematically, we summarize this as: \\[ \\frac{\\partial \\ln y}{\\partial \\ln x} = \\left(\\frac{\\partial y}{y}\\right)/\\left(\\frac{\\partial x}{x}\\right). \\] 3.2.3 Model Assumptions As in Section 2.2 for a single explanatory variable, there are two sets of assumptions that one can use for multiple linear regression. They are equivalent sets, each having comparative advantages as we proceed in our study of regression. The “observables” representation focuses on variables of interest \\((x_{i1}, \\ldots, x_{ik}, y_i)\\). The “error representation” provides a springboard for motivating our goodness of fit measures and study of residual analysis. However, the latter set of assumptions focuses on the additive errors case and obscures the sampling basis of the model. \\[ \\textbf{Multiple Linear Regression Model Sampling Assumptions} \\\\ \\small{ \\begin{array}{ll} \\text{Observables Representation} &amp; \\text{Error Representation} \\\\ \\hline F1.~ \\mathrm{E}~y_i=\\beta_0+\\beta_1 x_{i1}+\\ldots+\\beta_k x_{ik}. &amp; E1.~ y_i=\\beta_0+\\beta_1 x_{i1}+\\ldots+\\beta_k x_{ik}+\\varepsilon_i. \\\\ F2.~ \\{x_{i1},\\ldots ,x_{ik}\\} &amp; E2.~ \\{x_{i1},\\ldots ,x_{ik}\\} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{are non-stochastic variables.} &amp; \\ \\ \\ \\ \\ \\ \\ \\ \\text{are non-stochastic variables.} \\\\ F3.~ \\mathrm{Var}~y_i=\\sigma^2. &amp; E3.~ \\mathrm{E}~\\varepsilon_i=0 \\text{ and } \\mathrm{Var}~\\varepsilon_i=\\sigma^2. \\\\ F4.~ \\{y_i\\} \\text{ are independent random variables.} &amp; E4.~ \\{\\varepsilon_i\\} \\text{ are independent random variables.} \\\\ F5.~ \\{y_i\\} \\text{ are normally distributed.} &amp; E5.~ \\{\\varepsilon_i\\} \\text{ are normally distributed.} \\\\ \\hline \\end{array} } \\] To further motivate Assumptions F2 and F4, we will usually assume that our data have been realized as the result of a stratified sampling scheme, where each unique value of \\(\\{x_{i1}, \\ldots, x_{ik}\\}\\) is treated as a stratum. That is, for each value of \\(\\{x_{i1}, \\ldots, x_{ik}\\}\\), we draw a random sample of responses from a population. Thus, responses within each stratum are independent from one another, as are responses from different strata. Chapter 6 will discuss this sampling basis in further detail. 3.2.4 Properties of Regression Coefficient Estimators Section 3.1 described the least squares method for estimating regression coefficients. With the regression model assumptions, we can establish some basic properties of these estimators. To do this, from Section 2.11.4 we have that the expectation of a vector is the vector of expectations, so that \\[ \\mathrm{E}~\\mathbf{y} = \\left( \\begin{array}{l} \\mathrm{E}~y_1 \\\\ \\mathrm{E}~y_2 \\\\ \\vdots \\\\ \\mathrm{E}~y_n \\end{array} \\right) . \\] Further, basic matrix multiplication shows that \\[ \\small{ \\mathbf{X} \\boldsymbol \\beta = \\left( \\begin{array}{cccc} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; \\cdots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{nk} \\end{array} \\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) = \\left( \\begin{array}{c} \\beta_0 + \\beta_1 x_{11} + \\cdots + \\beta_k x_{1k} \\\\ \\beta_0 + \\beta_1 x_{21} + \\cdots + \\beta_k x_{2k} \\\\ \\vdots \\\\ \\beta_0 + \\beta_1 x_{n1} + \\cdots + \\beta_k x_{nk} \\end{array} \\right) . } \\] Because the \\(i\\)th row of assumption F1 is \\(\\mathrm{E}~y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik}\\), we may re-write this assumption in matrix formulation as \\(\\mathrm{E}~\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta\\). We are now in a position to state the first important property of least squares regression estimators. Property 1. Consider a regression model and let Assumptions F1-F4 hold. Then, the estimator \\(\\mathbf{b}\\) defined in equation (3.4) is an unbiased estimator of the parameter vector \\(\\boldsymbol \\beta\\). To establish Property 1, we have that \\[ \\mathrm{E}~\\mathbf{b} = \\mathrm{E}~\\left((\\mathbf{X^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathbf{y}\\right) = (\\mathbf{X^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathrm{E}~\\mathbf{y} = (\\mathbf{X^{\\prime}X)}^{-1} \\mathbf{X}^{\\prime} \\left( \\mathbf{X} \\boldsymbol \\beta \\right) = \\boldsymbol \\beta, \\] using matrix multiplication rules. This chapter assumes that \\(\\mathbf{X^{\\prime}X}\\) is invertible. One can also show that the least squares estimator need only be a solution of the normal equations for unbiasedness (not requiring that \\(\\mathbf{X^{\\prime}X}\\) be invertible, see Section 4.7.3). Thus, \\(\\mathbf{b}\\) is said to be an unbiased estimator of \\(\\boldsymbol \\beta\\). In particular, \\(\\mathrm{E}~b_j = \\beta_j\\) for \\(j = 0,1,\\ldots,k\\). Because independence implies zero covariance, from Assumption F4 we have that \\(\\mathrm{Cov}(y_i,y_j) = 0\\) for \\(i \\neq j\\). From this, Assumption F3 and the definition of the variance of a vector, we have that \\[ \\small{ \\mathrm{Var~}\\mathbf{y} = \\left( \\begin{array}{cccc} \\mathrm{Var~}y_1 &amp; \\mathrm{Cov}(y_1,y_2) &amp; \\cdots &amp; \\mathrm{Cov}(y_1,y_n) \\\\ \\mathrm{Cov}(y_2,y_1) &amp; \\mathrm{Var~}y_2 &amp; \\cdots &amp; \\mathrm{Cov}(y_2,y_n) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathrm{Cov}(y_n,y_1) &amp; \\mathrm{Cov}(y_n,y_2) &amp; \\cdots &amp; \\mathrm{Var~}y_n \\end{array} \\right) = \\left( \\begin{array}{cccc} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{array} \\right) = \\sigma^2 \\mathbf{I}, } \\] where \\(\\mathbf{I}\\) is an \\(n \\times n\\) identity matrix. We are now in a position to state the second important property of least squares regression estimators. Property 2. Consider a regression model and let Assumptions F1-F4 hold. Then, the estimator \\(\\mathbf{b}\\) defined in equation (3.4) has variance \\(\\mathrm{Var~}\\mathbf{b} = \\sigma^2(\\mathbf{X^{\\prime}X)}^{-1}\\). To establish Property 2, we have that \\[ \\mathrm{Var~}\\mathbf{b} = \\mathrm{Var}\\left((\\mathbf{X^{\\prime}X)}^{-1} \\mathbf{X}^{\\prime}\\mathbf{y}\\right) = \\left[ (\\mathbf{X^{\\prime}X)}^{-1} \\mathbf{X}^{\\prime}\\right] \\mathrm{Var}\\left( \\mathbf{y}\\right) \\left[\\mathbf{X}(\\mathbf{X^{\\prime}X)}^{-1}\\right] \\\\ = \\left[ (\\mathbf{X^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\right] \\sigma^2 \\mathbf{I}\\left[\\mathbf{X}(\\mathbf{X^{\\prime}X)}^{-1}\\right] = \\sigma^2(\\mathbf{X^{\\prime}X)}^{-1}, \\] as required. This important property will allow us to measure the precision of the estimator \\(\\mathbf{b}\\) when we discuss statistical inference. Specifically, by the definition of the variance of a vector (see Section 2.11.4), \\[\\begin{equation} \\mathrm{Var~}\\mathbf{b}= \\begin{pmatrix} \\mathrm{Var~}b_0 &amp; \\mathrm{Cov}(b_0,b_1) &amp; \\cdots &amp; \\mathrm{Cov}(b_0,b_k) \\\\ \\mathrm{Cov}(b_1,b_0) &amp; \\mathrm{Var~}b_1 &amp; \\cdots &amp; \\mathrm{Cov}(b_1,b_k) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathrm{Cov}(b_k,b_0) &amp; \\mathrm{Cov}(b_k,b_1) &amp; \\cdots &amp; \\mathrm{Var~}b_k \\end{pmatrix} = \\sigma^2 (\\mathbf{X^{\\prime}X)}^{-1}. \\tag{3.6} \\end{equation}\\] Thus, for example, \\(\\mathrm{Var~}b_j\\) is \\(\\sigma^2\\) times the \\((j+1)\\)st diagonal entry of \\((\\mathbf{X^{\\prime}X)}^{-1}\\). As another example, \\(\\mathrm{Cov}(b_0,b_j)\\) is \\(\\sigma^2\\) times the element in the first row and \\((j+1)\\)st column of \\((\\mathbf{X^{\\prime}X)}^{-1}\\). Although alternative methods are available that are preferable for specific applications, the least squares estimators have proven to be effective for many routine data analyses. One desirable characteristic of least squares regression estimators is summarized in the following well-known result. Gauss-Markov Theorem: Consider the regression model and let Assumptions F1-F4 hold. Then, within the class of estimators that are linear functions of the responses, the least squares estimator \\(\\mathbf{b}\\) defined in equation (3.4) is the minimum variance unbiased estimator of the parameter vector \\(\\boldsymbol{\\beta}\\). The Gauss-Markov theorem states that the least squares estimator is the most precise in the sense that it has the smallest variance. We have already seen in Property 1 that the least squares estimators are unbiased. The Gauss-Markov theorem states that the least squares estimator is the most precise in the sense that it has the smallest variance. (In a matrix context, “minimum variance” means that if \\(\\mathbf{b}^{\\ast}\\) is any other estimator, then the difference of the variance matrices, \\(\\mathrm{Var~} \\mathbf{b}^{\\ast} - \\mathrm{Var~}\\mathbf{b}\\), is nonnegative definite.) An additional important property concerns the distribution of the least squares regression estimators. Property 3: Consider a regression model and let Assumptions F1-F5 hold. Then, the least squares estimator \\(\\mathbf{b}\\) defined in equation (3.4) is normally distributed. To establish Property 3, we define the weight vectors, \\(\\mathbf{w}_i = (\\mathbf{X^{\\prime}X)}^{-1}(1, x_{i1}, \\ldots, x_{ik})^{\\prime}\\). With this notation, we note that \\[ \\mathbf{b} = (\\mathbf{X^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathbf{y} = \\sum_{i=1}^{n} \\mathbf{w}_i y_i, \\] so that \\(\\mathbf{b}\\) is a linear combination of responses. With Assumption F5, the responses are normally distributed. Because linear combinations of normally distributed random variables are normally distributed, we have the conclusion of Property 3. This result underpins much of the statistical inference that will be presented in Sections 3.4 and 4.2. 3.3 Estimation and Goodness of Fit Residual Standard Deviation Additional properties of the regression coefficient estimators will be discussed when we focus on statistical inference. We now continue our estimation discussion by providing an estimator of the other parameter in the linear regression model, \\(\\sigma^2\\). Our estimator for \\(\\sigma^2\\) can be developed using the principle of replacing theoretical expectations by sample averages. Examining \\(\\sigma^2 = \\mathrm{E}\\left( y-\\mathrm{E~}y\\right)^2\\), replacing the outer expectation by a sample average suggests using the estimator \\(n^{-1}\\sum_{i=1}^{n}(y_i-\\mathrm{E~}y_i)^2\\). Because we do not observe \\(\\mathrm{E}~y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik}\\), we use in its place the corresponding observed quantity \\(b_0 + b_1 x_{i1} + \\ldots + b_k x_{ik} = \\widehat{y}_i\\). This leads to the following. Definition. An estimator of \\(\\sigma^2\\), the mean square error (MSE), is defined as \\[\\begin{equation} s^2 = \\frac{1}{n-(k+1)}\\sum_{i=1}^{n}\\left( y_i - \\widehat{y}_i \\right)^2. \\tag{3.7} \\end{equation}\\] The positive square root, \\(s = \\sqrt{s^2}\\), is called the residual standard deviation. This expression generalizes the definition in equation (2.3), which is valid for \\(k=1\\). It turns out, by using \\(n-(k+1)\\) instead of \\(n\\) in the denominator of equation (3.7), that \\(s^2\\) is an unbiased estimator of \\(\\sigma^2\\). Essentially, by using \\(\\widehat{y}_i\\) instead of \\(\\mathrm{E~}y_i\\) in the definition, we have introduced some small dependencies among the deviations from the responses \\(y_i - \\widehat{y}_i\\), thus reducing the overall variability. To compensate for this lower variability, we also reduce the denominator in the definition of \\(s^2\\). To provide further intuition on the choice of \\(n-(k+1)\\) in the definition of \\(s^2\\), we introduced the concept of residuals in the context of multiple linear regression. From Assumption E1, recall that the random errors can be expressed as \\(\\varepsilon_i = y_i - (\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik})\\). Because the parameters \\(\\beta_0, \\ldots, \\beta_k\\) are not observed, the errors themselves are not observed. Instead, we examine the “estimated errors,” or residuals, defined by \\(e_i = y_i - \\widehat{y}_i\\). Unlike errors, there exist certain dependencies among the residuals. One dependency is due to the algebraic fact that the average residual is zero. Further, there must be at least \\(k+2\\) observations for there to be variation in the fit of the plane. If we have only \\(k+1\\) observations, we could fit a plane to the data perfectly, resulting in no variation in the fit. For example, if \\(k=1\\), because two observations determine a line, then at least three observations are required to observe any deviation from the line. Because of these dependencies, we have only \\(n-(k+1)\\) free, or unrestricted, residuals to estimate the variability about the regression plane. The positive square root of \\(s^2\\) is our estimator of \\(\\sigma\\). Using residuals, it can be expressed as \\[\\begin{equation} s = \\sqrt{\\frac{1}{n-(k+1)}\\sum_{i=1}^{n}e_i^2}. \\tag{3.8} \\end{equation}\\] Because it is based on residuals, we refer to \\(s\\) as the residual standard deviation. The quantity \\(s\\) is a measure of our “typical error.” For this reason, \\(s\\) is also called the standard error of the estimate. The Coefficient of Determination: \\(R^2\\) To summarize the goodness of fit of the model, as in Chapter 2 we partition the variability into pieces that are “explained” and “unexplained” by the regression fit. Algebraically, the calculations for regression using many variables are similar to the case of using only one variable. Unfortunately, when dealing with many variables, we do lose the easy graphical interpretation such as in Figure 2.4. Begin with the total sum of squared deviations, \\(Total~SS = \\sum_{i=1}^{n}\\left( y_i - \\overline{y} \\right)^2\\), as our measure of the total variation in the data set. As in equation (2.1), we may then interpret the equation \\[ \\small{ \\begin{array}{ccccc} \\underbrace{y_i - \\overline{y}} &amp; = &amp; \\underbrace{y_i - \\widehat{y}_i} &amp; + &amp; \\underbrace{\\widehat{y}_i - \\overline{y}} \\\\ \\text{total} &amp; = &amp; \\text{unexplained} &amp; + &amp; \\text{explained} \\\\ \\text{deviation} &amp; &amp; \\text{deviation} &amp; &amp; \\text{deviation} \\end{array} } \\] as the “deviation without knowledge of the explanatory variables equals the deviation not explained by the explanatory variables plus deviation explained by the explanatory variables.” Squaring each side and summing over all observations yields \\[ Total~SS = Error~SS + Regression~SS \\] where \\(Error~SS = \\sum_{i=1}^{n}\\left( y_i - \\widehat{y}_i \\right)^2\\) and \\(Regression~SS = \\sum_{i=1}^{n}\\left( \\widehat{y}_i - \\overline{y} \\right)^2\\). As in Section 2.3 for the one explanatory variable case, the sum of the cross-product terms turns out to be zero. A statistic that summarizes this relationship is the coefficient of determination, \\[ R^2 = \\frac{Regression~SS}{Total~SS}. \\] We interpret \\(R^2\\) to be the proportion of variability explained by the regression function. If the model is a desirable one for the data, one would expect a strong relationship between the observed responses and those “expected” under the model, the fitted values. An interesting algebraic fact is the following. If one squares the correlation coefficient between the responses and the fitted values, we get the coefficient of determination, that is, \\[ R^2 = \\left[ r \\left(y, \\widehat{y} \\right) \\right]^2. \\] As a result, \\(R\\), the positive square root of \\(R^2\\), is called the multiple correlation coefficient. It can be interpreted as the correlation between the response and the best linear combination of the explanatory variables, the fitted values. (This relationship is developed using matrix algebra in the technical supplement Section 5.10.1.) The variability decomposition is also summarized using the analysis of variance, or ANOVA, table, as follows. \\[ \\small{ \\begin{array}{l|lcl} \\hline \\text{Source} &amp; \\text{Sum of Squares} &amp; df &amp; \\text{Mean Square} \\\\ \\hline \\text{Regression} &amp; Regression~SS &amp; k &amp; Regression~MS \\\\ \\text{Error} &amp; Error~SS &amp; n - (k + 1) &amp; MSE \\\\ \\text{Total} &amp; Total~SS &amp; n - 1 &amp; \\\\ \\hline \\end{array} } \\] The mean square column figures are defined to be the sum of squares figures divided by their respective degrees of freedom. The error degrees of freedom denotes the number of unrestricted residuals. It is this number that we use in our definition of the “average,” or mean, square error. That is, we define \\[ MSE = Error~MS = \\frac{Error~SS}{n - (k + 1)} = s^2. \\] Similarly, the regression degrees of freedom is the number of explanatory variables. This yields \\[ Regression~MS = \\frac{Regression~SS}{k}. \\] When discussing the coefficient of determination, it can be established that whenever an explanatory variable is added to the model, \\(R^2\\) never decreases. This is true whether or not the additional variable is useful. We would like a measure of fit that decreases when useless variables are entered into the model as explanatory variables. To circumvent this anomaly, a widely used statistic is the coefficient of determination adjusted for degrees of freedom, defined by \\[\\begin{equation} R_{a}^2 = 1 - \\frac{(Error~SS) / [n - (k + 1)]}{(Total~SS) / (n - 1)} = 1 - \\frac{s^2}{s_{y}^2}. \\tag{3.9} \\end{equation}\\] To interpret this statistic, note that \\(s_y^2\\) does not depend on the model nor the model variables. Thus, \\(s^2\\) and \\(R_a^2\\) are equivalent measures of model fit. As the model fit improves, then \\(R_{a}^2\\) becomes larger and \\(s^2\\) becomes smaller, and vice versa. Put another way, choosing a model with the smallest \\(s^2\\) is equivalent to choosing a model with the largest \\(R_a^2\\). Example: Term Life Insurance - Continued. To illustrate, Table 3.3 displays the summary statistics for the regression of LNFACE on EDUCATION, NUMHH, and LNINCOME. From the degrees of freedom column, we remind ourselves that there are three explanatory variables and 275 observations. As measures of model fit, the coefficient of determination is \\(R^2 = 34.3\\%\\) (=\\(328.47 / 958.90\\)) and the residual standard deviation is \\(s = 1.525\\) (=\\(\\sqrt{2.326}\\)). If we were to attempt to estimate the logarithmic face amount without knowledge of the explanatory variables EDUCATION, NUMHH, and LNINCOME, then the size of the typical error would be \\(s_y = 1.871\\) (=\\(\\sqrt{958.90 / 274}\\)). Thus, by taking advantage of our knowledge of the explanatory variables, we have been able to reduce the size of the typical error. The measure of model fit that compares these two estimates of variability is the adjusted coefficient of determination, \\(R_a^2 = 1 - 2.326 / 1.871^2 = 33.6\\%\\). Table 3.3: Term Life ANOVA Table Sum of Squares \\(df\\) Mean Square Regression 328.47 3 109.49 Error 630.43 271 2.326 Total 958.9 274 Example: Why do Females Live Longer than Males? In an article with this title, Lemaire (2002) examined what he called the “female advantage,” the difference in life expectancy between females and males. Life expectancies are of interest because they are widely used measures of a nation’s health. Lemaire examined data from \\(n = 169\\) countries and found that the average female advantage was 4.51 years worldwide. He sought to explain this difference based on 45 behavioral measures, variables that capture a nation’s degree of economic modernization, social/cultural/religious mores, geographic position, and quality of health care available. After a detailed analysis, Lemaire reports coefficients from a regression model that appear in Table 3.4. This regression model explains \\(R^2 = 61\\%\\) of the variability. It is a parsimonious model consisting of only \\(k = 4\\) of the original 45 variables. Table 3.4. Regression Coefficients from a Model of Female Advantage \\[ \\small{ \\begin{array}{l|rr} \\hline \\text{Variable} &amp; \\text{Coefficient} &amp; t\\text{-statistic} \\\\ \\hline \\text{Intercept} &amp; 9.904 &amp; 12.928 \\\\ \\text{Logarithmic Number of Persons per Physician} &amp; -0.473 &amp; -3.212 \\\\ \\text{Fertility} &amp; -0.444 &amp; -3.477 \\\\ \\text{Percentage of Hindus and Buddhists} &amp; -0.018 &amp; -3.196 \\\\ \\text{Soviet Union Dummy} &amp; 4.922 &amp; 7.235 \\\\ \\hline \\end{array} } \\] Source: Lemaire (2002) All variables were strongly statistically significant. The number of persons per physician was also correlated with other variables that capture a country’s degree of economic modernization, such as urbanization, number of cars, and the percentage working in agriculture. Fertility, the number of births per woman, was highly correlated with education variables in the study, including female illiteracy and female school enrollment. The percentage of Hindus and Buddhists is a social/cultural/religious variable. The Soviet Union dummy is a geographic variable - it characterizes Eastern European countries that formerly belonged to the Soviet Union. Because of the high degree of collinearity among the 45 candidate variables, other analysts could easily pick an alternative set of variables. Nonetheless, Lemaire’s important point was that this simple model explains roughly 61% of the variability based on only behavioral variables, unrelated to biological sex differences. 3.4 Statistical Inference for a Single Coefficient 3.4.1 The t-Test In many applications, a single variable is of primary interest, and other variables are included in the regression to control for additional sources of variability. To illustrate, a sales agent might be interested in the effect that income has on the quantity of insurance demanded. In a regression analysis, one could also include other explanatory variables such as an individual’s gender, type of occupation, age, size of the household, education level, and so on. By including these additional explanatory variables, we hope to gain a better understanding of the relationship between income and insurance demand. To reach sensible conclusions, we will need some rules to decide whether a variable is important or not. We respond to the question “Is \\(x_j\\) important?” by investigating whether or not the corresponding slope parameter, \\(\\beta_j\\), equals zero. The question is whether \\(\\beta_j\\) is zero can be restated in the hypothesis testing framework as “Is \\(H_0:\\beta_j=0\\) valid?” We examine the proximity of \\(b_j\\) to zero in order to determine whether or not \\(\\beta_j\\) is zero. Because the units of \\(b_j\\) depend on the units of \\(y\\) and \\(x_j\\), we need to standardize this quantity. From Property 2 and equation (3.6), we saw that \\(\\mathrm{Var~}b_j\\) is \\(\\sigma^2\\) times the \\((j+1)^{st}\\) diagonal element of \\((\\mathbf{X^{\\prime}X})^{-1}\\). Replacing \\(\\sigma^2\\) by the estimator \\(s^2\\) and taking square roots, we have the following. Definition. The standard error of \\(b_j\\) can be expressed as \\[ se(b_j) = s \\sqrt{\\text{(j+1)st diagonal element of } (\\mathbf{X^{\\prime}X})^{-1}}. \\] Recall that a standard error is an estimated standard deviation. To test \\(H_0:\\beta_j=0\\), we examine the \\(t\\)-ratio, \\(t(b_j) = \\frac{b_j}{se(b_j)}\\). We interpret \\(t(b_j)\\) to be the number of standard errors that \\(b_j\\) is away from zero. This is the appropriate quantity because the sampling distribution of \\(t(b_j)\\) can be shown to be the \\(t\\)-distribution with \\(df=n-(k+1)\\) degrees of freedom, under the null hypothesis with the linear regression model assumptions F1-F5. This enables us to construct tests of the null hypothesis such as the following procedure: Procedure. The t-test for a Regression Coefficient (\\(\\beta\\)). The null hypothesis is \\(H_0:\\beta_j=0\\). The alternative hypothesis is \\(H_{a}:\\beta_j \\neq 0\\). Establish a significance level \\(\\alpha\\) (typically but not necessarily 5%). Construct the statistic, \\(t(b_j) = \\frac{b_j}{se(b_j)}\\). Procedure: Reject the null hypothesis in favor of the alternative if \\(|t(b_j)|\\) exceeds a \\(t\\)-value. Here, this \\(t\\)-value is the \\((1-\\alpha /2)^{th}\\) percentile from the \\(t\\)-distribution using \\(df=n-(k+1)\\) degrees of freedom, denoted as \\(t_{n-(k+1),1-\\alpha /2}\\). In many applications, the sample size will be large enough so that we may approximate the \\(t\\)-value by the corresponding percentile from the standard normal curve. At the 5% level of significance, this percentile is 1.96. Thus, as a rule of thumb, we can interpret a variable to be important if its \\(t\\)-ratio exceeds two in absolute value. Although it is the most common, testing \\(H_0:\\beta_j=0\\) versus \\(H_{a}:\\beta_j \\neq 0\\) is just one of many hypothesis tests that can be performed. Table 3.5 outlines alternative decision-making procedures. These procedures are for testing \\(H_0:\\beta_j = d\\). Here, \\(d\\) is a user-prescribed value that may be equal to zero or any other known value. Table 3.5. Decision-Making Procedures for Testing \\(H_0: \\beta_j = d\\) \\[ \\small{ \\begin{array}{cc} \\hline \\text{Alternative Hypothesis }(H_{a}) &amp; \\text{Procedure: Reject } H_0 \\text{ in favor of } H_a \\text{ if }\\\\ \\hline \\beta_j &gt; d &amp; t-\\mathrm{ratio}&gt;t_{n-(k+1),1-\\alpha } \\\\ \\beta_j &lt; d &amp; t-\\mathrm{ratio}&lt;-t_{n-(k+1),1-\\alpha } \\\\ \\beta_j\\neq d &amp; |t-\\mathrm{ratio}\\mathit{|}&gt;t_{n-(k+1),1-\\alpha/2} \\end{array} \\\\ \\begin{array}{ll}\\hline \\textit{Notes:} &amp;\\text{ The significance level is } \\alpha. \\text{ Here, } t_{n-(k+1),1-\\alpha}\\text{ is the }(1-\\alpha)^{th}\\text{ percentile} \\\\ &amp;~~\\text{from the }t-\\text{distribution using } df=n-(k+1)\\text{ degrees of freedom.} \\\\ &amp;~~\\text{The test statistic is }t-\\mathrm{ratio} = (b_j -d)/se(b_j) . \\\\ \\hline \\end{array} } \\] Alternatively, one can construct \\(p\\)-values and compare these to given significant levels. The \\(p\\)-value allows the report reader to understand the strength of the deviation from the null hypothesis. Table 3.6 summarizes the procedure for calculating \\(p\\)-values. Table 3.6. Probability Values for Testing \\(H_0:\\beta_j =d\\) \\[ \\small{ \\begin{array}{cccc} \\hline \\text{Alternative} &amp; &amp; &amp; \\\\ \\text{Hypothesis} (H_a ) &amp; \\beta_j &gt; d &amp; \\beta_j &lt; d &amp; \\beta_j \\neq d \\\\ \\hline p-value &amp; \\Pr(t_{n-(k+1)}&gt;t-ratio) &amp; \\Pr(t_{n-(k+1)}&lt;t-ratio) &amp; \\Pr(|t_{n-(k+1)}|&gt;|t-ratio|) \\\\ \\end{array} \\\\ \\begin{array}{ll}\\hline \\textit{Notes:} &amp; \\text{ Here, } t_{n-(k+1)} \\text{ is a }t\\text{-distributed random variable with }df=n-(k+1)\\text{ degrees of freedom.} \\\\ &amp;~~\\text{The test statistic is }t-\\mathrm{ratio} = (b_j -d)/se(b_j) . \\\\ \\hline \\end{array} } \\] Example: Term Life Insurance - Continued. A useful convention when reporting the results of a statistical analysis is to place the standard error of a statistic in parenthesis below that statistic. Thus, for example, in our regression of LNFACE on EDUCATION, NUMHH, and LNINCOME, the estimated regression equation is: \\[ \\begin{array}{lccccc} \\widehat{LNFACE} = &amp;2.584 ~ + &amp;0.206~ \\text{EDUCATION} + &amp;0.306 ~\\text{NUMHH} + &amp;0.494 ~\\text{LNINCOME}. \\\\ \\text{std error} &amp;(0.846) &amp;(0.039) &amp;(0.063) &amp;(0.078). \\end{array} \\] To illustrate the calculation of the standard errors, first note that from Table 3.3 we have that the residual standard deviation is \\(s=1.525\\). Using a statistical package, we have \\[ \\small{ (\\mathbf{X^{\\prime}X})^{-1} = \\begin{pmatrix} 0.307975 &amp; -0.004633 &amp; -0.002131 &amp; -0.020697 \\\\ -0.004633 &amp; 0.000648 &amp; 0.000143 &amp; -0.000467 \\\\ -0.002131 &amp; 0.000143 &amp; 0.001724 &amp; -0.000453 \\\\ -0.020697 &amp; -0.000467 &amp; -0.000453 &amp; 0.002585 \\end{pmatrix}. } \\] To illustrate, we can compute \\(se(b_3)=s \\times \\sqrt{0.002585} = 0.078\\), as above. Calculation of the standard errors, as well as the corresponding \\(t\\)-statistics, is part of the standard output from statistical software and need not be computed by users. Our purpose here is to illustrate the ideas underlying the routine calculations. With this information, we can immediately compute \\(t\\)-ratios to check whether a coefficient associated with an individual variable is significantly different from zero. For example, the \\(t\\)-ratio for the LNINCOME variable is \\(t(b_3) = \\frac{0.494}{0.078} = 6.3\\). The interpretation is that \\(b_3\\) is over four standard errors above zero and thus LNINCOME is an important variable in the model. More formally, we may be interested in testing the null hypothesis that \\(H_0:\\beta_3 = 0\\) versus \\(H_0:\\beta_3 \\neq 0\\). At a 5% level of significance, the \\(t\\)-value is 1.96, because \\(df=275-(1+3)=271\\). We thus reject the null in favor of the alternative hypothesis, that logarithmic income (LNINCOME) is important in determining the logarithmic face amount. 3.4.2 Confidence Intervals Confidence intervals for parameters represent another device for describing the strength of the contribution of the \\(j\\)th explanatory variable. The statistic \\(b_j\\) is called a point estimate of the parameter \\(\\beta_j\\). To provide a range of reliability, we use the confidence interval: \\[\\begin{equation} b_j \\pm t_{n-(k+1),1-\\alpha /2}~se(b_j). \\tag{3.10} \\end{equation}\\] Here, the \\(t\\)-value \\(t_{n-(k+1),1-\\alpha /2}\\) is a percentile from the \\(t\\)-distribution with \\(df=n-(k+1)\\) degrees of freedom. We use the same \\(t\\)-value as in the two-sided hypothesis test. Indeed, there is a duality between the confidence interval and the two-sided hypothesis test. For example, it is not hard to check that if a hypothesized value falls outside the confidence interval, then \\(H_0\\) will be rejected in favor of \\(H_{a}\\). Further, knowledge of the \\(p\\)-value, point estimate, and standard error can be used to determine a confidence interval. 3.4.3 Added Variable Plots To represent multivariate data graphically, we have seen that a scatterplot matrix is a useful device. However, the major shortcoming of the scatterplot matrix is that it only captures relationships between pairs of variables. When the data can be summarized using a regression model, a graphical device that does not have this shortcoming is an added variable plot. The added variable plot is also called a partial regression plot because, as we will see, it is constructed in terms of residuals from certain regression fits. We will also see that the added variable plot can be summarized in terms of a partial correlation coefficient, thus providing a link between correlation and regression. To introduce these ideas, we work in the context of the following example. Example: Refrigerator Prices. What characteristics of a refrigerator are important in determining its price (PRICE)? We consider here several characteristics of a refrigerator, including the size of the refrigerator in cubic feet (RSIZE), the size of the freezer compartment in cubic feet (FSIZE), the average amount of money spent per year to operate the refrigerator (ECOST, for “energy cost”), the number of shelves in the refrigerator and freezer doors (SHELVES), and the number of features (FEATURES). The features variable includes shelves for cans, see-through crispers, ice makers, egg racks, and so on. Both consumers and manufacturers are interested in models of refrigerator prices. Other things equal, consumers generally prefer larger refrigerators with lower energy costs that have more features. Due to forces of supply and demand, we would expect consumers to pay more for these refrigerators. A larger refrigerator with lower energy costs that has more features at a similar price is considered a bargain to the consumer. How much extra would the consumer be willing to pay for this additional space? A model of prices for refrigerators on the market provides some insight into this question. To this end, we analyze data from \\(n=37\\) refrigerators. Table 3.7 provides the basic summary statistics for the response variable PRICE and the five explanatory variables. From this table, we see that the average refrigerator price is \\(\\overline{y} = \\$626.40\\), with standard deviation \\(s_{y} = \\$139.80\\). Similarly, the average annual amount to operate a refrigerator, or average ECOST, is $70.51. knitr::kable(2, caption = &quot;Silly. Create a table just to update the counter...&quot;) Table 3.4: Silly. Create a table just to update the counter… x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Table 3.5: Silly. x 2 knitr::kable(2, caption = &quot;Silly.&quot;) Table 3.6: Silly. x 2 Table 3.7: Summary Statistics for each variable for 37 Refrigerators Mean Median Standard Deviation Minimum Maximum ECOST 70.514 68.0 9.140 60.0 94.0 RSIZE 13.400 13.2 0.600 12.6 14.7 FSIZE 5.184 5.1 0.938 4.1 7.4 SHELVES 2.514 2.0 1.121 1.0 5.0 FEATURES 3.459 3.0 2.512 1.0 12.0 PRICE 626.351 590.0 139.790 460.0 1200.0 To analyze relationships among pairs of variables, Table 3.8 provides a matrix of correlation coefficients. From the table, we see that there are strong linear relationships between PRICE and each of freezer space (FSIZE) and the number of FEATURES. Surprisingly, there is also a strong positive correlation between PRICE and ECOST. Recall that ECOST is the energy cost; one might expect that higher-priced refrigerators should enjoy lower energy costs. Table 3.8: Matrix of Correlation Coefficients ECOST RSIZE FSIZE SHELVES FEATURES RSIZE -0.033 FSIZE 0.855 -0.235 SHELVES 0.188 -0.363 0.251 FEATURES 0.334 -0.096 0.439 0.16 PRICE 0.522 -0.024 0.72 0.4 0.697 R Code to Produce Tables 3.7 and 3.8 Refrig &lt;- read.csv(&quot;CSVData/Refrigerator.csv&quot;, header=TRUE) # TABLE 3.7 SUMMARY STATISTICS varFrig &lt;- c(&quot;ECOST&quot;, &quot;RSIZE&quot;, &quot;FSIZE&quot;, &quot;SHELVES&quot;, &quot;FEATURES&quot;, &quot;PRICE&quot;) Refrig1 &lt;- Refrig[varFrig] tableMat &lt;- BookSummStats(Refrig1) colnames(tableMat) &lt;- c(&quot;Mean&quot; , &quot;Median&quot; , &quot;Standard Deviation&quot; , &quot;Minimum&quot; , &quot;Maximum&quot;) rownames(tableMat) &lt;- varFrig # tableMat1[1:2,] &lt;- format(round(tableMat[1:2,], digits=0), big.mark = &#39;,&#39;) TableGen1(TableData=tableMat, TextTitle=&#39;Summary Statistics for each variable for 37 Refrigerators&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:5, ColWidth = ColWidth5) tableCor &lt;- cor(Refrig1) tableCor &lt;- round(tableCor, digits = 3) tableCor[upper.tri(tableCor, diag = TRUE)] &lt;- &quot;&quot; tablePrint &lt;- tableCor[-1,] tablePrint &lt;- tablePrint[,-6] TableGen1(TableData=tablePrint, TextTitle=&#39;Matrix of Correlation Coefficients&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:5, ColWidth = ColWidth5) A regression model was fit to the data. The fitted regression equation appears in Table 3.9, with \\(s=60.65\\) and \\(R^2=83.8\\%\\). Table 3.9: Fitted Refrigerator Price Model Coefficient Standard Error \\(t\\)-Ratio Intercept 798.00 271.400 -2.9 ECOST -6.96 2.275 -3.1 RSIZE 76.50 19.440 3.9 FSIZE 137.00 23.760 5.8 SHELVES 37.90 9.886 3.8 FEATURES 23.80 4.512 5.3 From Table 3.9, the explanatory variables seem to be useful predictors of refrigerator prices. Together, these variables account for 83.8% of the variability. For understanding prices, the typical error has dropped from \\(s_{y} = \\$139.80\\) to \\(s = \\$60.65\\). The \\(t\\)-ratios for each of the explanatory variables exceed two in absolute value, indicating that each variable is important on an individual basis. What is surprising about the regression fit is the negative coefficient associated with energy cost. Remember, we can interpret \\(b_{ECOST} = -6.96\\) to mean that, for each dollar increase in ECOST, we expect the PRICE to decrease by $6.96. This negative relationship conforms to our economic intuition. However, it is surprising that the same data set has shown us that there is a positive relationship between PRICE and ECOST. This seeming anomaly is because correlation only measures relationships between pairs of variables although the regression fit can account for several variables simultaneously. To provide more insight into this seeming anomaly, we now introduce the added variable plot. Producing an Added Variable Plot The added variable plot provides additional links between the regression methodology and more fundamental tools such as scatter plots and correlations. We work in the context of the Refrigerator Price Example to demonstrate the construction of this plot. Procedure for producing an added variable plot. Run a regression of PRICE on RSIZE, FSIZE, SHELVES, and FEATURES, omitting ECOST. Compute the residuals from this regression, which we label \\(e_1\\). Run a regression of ECOST on RSIZE, FSIZE, SHELVES, and FEATURES. Compute the residuals from this regression, which we label \\(e_2\\). Plot \\(e_1\\) versus \\(e_2\\). This is the added variable plot of PRICE versus ECOST, controlling for the effects of the RSIZE, FSIZE, SHELVES, and FEATURES. This plot appears in Figure 3.4. Figure 3.4: An added variable plot. The residuals from the regression of PRICE on the explanatory variables, omitting ECOST, are on the horizontal axis. On the vertical axis are the residuals from the regression fit of ECOST on the other explanatory variables. The correlation coefficient is -0.48. R Code to Produce Figure 3.4 model.refrig1 &lt;- lm(PRICE ~ RSIZE + FSIZE + SHELVES + FEATURES, data = Refrig1) #summary(model.refrig1) model.refrig2 &lt;- lm(ECOST ~ RSIZE + FSIZE + SHELVES + FEATURES, data = Refrig1) #summary(model.refrig2) par(mar=c(4.1,4,.1,1), cex=1.1) plot(residuals(model.refrig2),residuals(model.refrig1), xlab=expression(e[1]),ylab=&quot;&quot;, las=1) mtext(expression(e[2]), side=2, at=0, line=3, las=1, cex=1.1) The error \\(\\varepsilon\\) can be interpreted as the natural variation in a sample. In many situations, this natural variation is small compared to the patterns evident in the nonrandom regression component. Thus, it is useful to think of the error, \\(\\varepsilon_i = y_i - \\left( \\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik} \\right)\\), as the response after controlling for the effects of the explanatory variables. In Section 3.3, we saw that a random error can be approximated by a residual, \\(e_i = y_i - \\left( b_0 + b_1 x_{i1} + \\cdots + b_k x_{ik} \\right)\\). Thus, in the same way, we may think of a residual as the response after “controlling for” the effects of the explanatory variables. With this in mind, we can interpret the vertical axis of Figure 3.4 as the refrigerator PRICE controlled for effects of RSIZE, FSIZE, SHELVES, and FEATURES. Similarly, we can interpret the horizontal axis as the ECOST controlled for effects of RSIZE, FSIZE, SHELVES, and FEATURES. The plot then provides a graphical representation of the relation between PRICE and ECOST, after controlling for the other explanatory variables. For comparison, a scatter plot of PRICE and ECOST (not shown here) does not control for other explanatory variables. Thus, it is possible that the positive relationship between PRICE and ECOST is not due to a causal relationship but rather one or more additional variables that cause both variables to be large. For example, from Table 3.8, we see that the freezer size (FSIZE) is positively correlated with both ECOST and PRICE. It certainly seems reasonable that increasing the size of a freezer would cause both the energy cost and the price to increase. Rather, the positive correlation may be due to the fact that large values of FSIZE mean large values of both ECOST and PRICE. Variables left out of a regression are called omitted variables. This omission could cause a serious problem in a regression model fit; regression coefficients could be not only strongly significant when they should not be, but they may also be of the incorrect sign. Selecting the proper set of variables to be included in the regression model is an important task; it is the subject of Chapters 5 and 6. 3.4.4 Partial Correlation Coefficients As we saw in Chapter 2, a correlation statistic is a useful quantity for summarizing plots. The correlation for the added variable plot is called a partial correlation coefficient. It is defined to be the correlation between the residuals \\(e_1\\) and \\(e_2\\) and is denoted by \\(r(y,x_j | x_1, \\ldots, x_{j-1}, x_{j+1}, \\ldots, x_k)\\). Because it summarizes an added variable plot, we may interpret \\(r(y,x_j | x_1, \\ldots, x_{j-1}, x_{j+1}, \\ldots, x_k)\\) to be the correlation between \\(y\\) and \\(x_j\\), in the presence of the other explanatory variables. To illustrate, the correlation between PRICE and ECOST in the presence of the other explanatory variables is -0.48. The partial correlation coefficient can also be calculated using \\[\\begin{equation} r(y,x_j | x_1, \\ldots, x_{j-1}, x_{j+1}, \\ldots, x_k) = \\frac{t(b_j)}{\\sqrt{t(b_j)^2 + n - (k + 1)}}. \\tag{3.11} \\end{equation}\\] Here, \\(t(b_j)\\) is the \\(t\\)-ratio for \\(b_j\\) from a regression of \\(y\\) on \\(x_1, \\ldots, x_k\\) (including the variable \\(x_j\\)). An important aspect of this equation is that it allows us to calculate partial correlation coefficients running only one regression. For example, from Table 3.9, the partial correlation between PRICE and ECOST in the presence of the other explanatory variables is \\(\\frac{-3.1}{\\sqrt{(-3.1)^2 + 37 - (5 + 1)}} \\approx -0.48\\). Calculation of partial correlation coefficients is quicker when using the relationship with the \\(t\\)-ratio, but may fail to detect nonlinear relationships. The information in Table 3.9 allows us to calculate all five partial correlation coefficients in the Refrigerator Price Example after running only one regression. The three-step procedure for producing added variable plots requires ten regressions, two for each of the five explanatory variables. Of course, by producing added variable plots, we can detect nonlinear relationships that are missed by correlation coefficients. Partial correlation coefficients provide another interpretation for \\(t\\)-ratios. The equation shows how to calculate a correlation statistic from a \\(t\\)-ratio, thus providing another link between correlation and regression analysis. Moreover, from the equation we see that the larger the \\(t\\)-ratio, the larger the partial correlation coefficient. That is, a large \\(t\\)-ratio means that there is a large correlation between the response and the explanatory variable, controlling for other explanatory variables. This provides a partial response to the question that is regularly asked by consumers of regression analyses, “Which variable is most important?” 3.5 Some Special Explanatory Variables The linear regression model is the basis of a rich family of models. This section provides several examples to illustrate the richness of this family. These examples demonstrate the use of (i) binary variables, (ii) transformation of explanatory variables, and (iii) interaction terms. This section also serves to underscore the meaning of the adjective linear in the phrase “linear regression”; the regression function is linear in the parameters but may be a highly nonlinear function of the explanatory variables. 3.5.1 Binary Variables Categorical variables provide a numerical label for measurements of observations that fall in distinct groups, or categories. Because of the grouping, categorical variables are discrete and generally take on a finite number of values. We begin our discussion with a categorical variable that can take on one of only two values, a binary variable. Further discussion of categorical variables is the topic of Chapter 4. Example: Term Life Insurance - Continued. We now consider the marital status of the survey respondent. In the Survey of Consumer Finances, respondents can select among several options describing their marital status including “married,” “living with a partner,” “divorced,” and so on. Marital status is not measured continuously but rather takes on values that fall into distinct groups. In this chapter, we group survey respondents according to whether or not they are single, defined to include those who are separated, divorced, widowed, never married, and are not married nor living with a partner. Chapter 4 will present a more complete analysis of marital status by including additional categories. The binary variable SINGLE is defined to be one if the survey respondent is single and 0 otherwise. The variable SINGLE is also known as an indicator variable because it indicates whether or not the respondent is single. Another name for this important type of variable is a dummy variable. We could use 0 and 100, or 20 and 36, or any other distinct values. However, 0 and 1 are convenient for the interpretation of the parameter values, discussed below. To streamline the discussion, we now present a model using only LNINCOME and SINGLE as explanatory variables. For our sample of \\(n = 275\\) households, 57 are single and the other 218 are not. To see the relationships among LNFACE, LNINCOME, and SINGLE, Figure 3.5 introduces a letter plot of LNFACE versus LNINCOME, with SINGLE as the code variable. We can see that Figure 3.5 is a scatter plot of LNFACE versus LNINCOME, using 50 randomly selected households from our sample of 275 (for clarity of the graph). However, instead of using the same plotting symbol for each observation, we have coded the symbols so that we can easily understand the behavior of a third variable, SINGLE. In other applications, you may elect to use other plotting symbols such as \\(\\clubsuit\\), \\(\\heartsuit\\), \\(\\spadesuit\\), and so on, or use different colors, to encode additional information. For this application, the letter codes “S” for single and “o” for other were selected because they remind the reader of the nature of the coding scheme. Regardless of the coding scheme, the important point is that a letter plot is a useful device for graphically portraying three or more variables in two dimensions. The main restriction is that the additional information must be categorized, such as with binary variables, to make the coding scheme work. Figure 3.5: Letter plot of LNFACE versus LNINCOME, with the letter code ‘S’ for single and ‘o’ for other. The fitted regression lines have been superimposed. The lower line is for single and the upper line is for other. Figure 3.5 suggests that LNFACE is lower for those single than others for a given level of income. Thus, we now consider a regression model, LNFACE = β_0 + β_1 LNINCOME + β_2 SINGLE + ϵ. The regression function can be written as: \\[ \\text{E } y = \\begin{cases} \\beta_0 + \\beta_1 \\text{LNINCOME} &amp; \\text{for other respondents} \\\\ \\beta_0 + \\beta_2 + \\beta_1 \\text{LNINCOME} &amp; \\text{for single respondents} \\end{cases} \\] The interpretation of the model coefficients differs from the continuous variable case. For continuous variables such as LNINCOME, we interpret \\(\\beta_1\\) as the expected change in \\(y\\) per unit change of logarithmic income, holding other variables fixed. For binary variables such as SINGLE, we interpret \\(\\beta_2\\) as the expected increase in \\(y\\) when going from the base level of SINGLE (=0) to the alternative level. Thus, although we have one model for both marital statuses, we can interpret the model using two regression equations, one for each type of marital status. By writing a separate equation for each marital status, we have been able to simplify a complicated multiple regression equation. Sometimes, you will find it easier to communicate a series of simple relationships compared to a single, complex relationship. Although the interpretation for binary explanatory variables differs from the continuous, the ordinary least squares estimation method remains valid. To illustrate, the fitted version of the above model is \\[ \\small{ \\begin{array}{cclll} \\widehat{LNFACE} &amp; = &amp; 5.09 &amp; + 0.634 \\text{LNINCOME} &amp; - 0.800 \\text{SINGLE} .\\\\ \\text{std error} &amp; &amp; (0.89) &amp; ~~(0.078) &amp; ~(0.248) \\\\ \\end{array} } \\] To interpret \\(b_2 = -0.800\\), we say that we expect the logarithmic face to be smaller by 0.80 for a survey respondent who is single compared to the other category. This assumes that other things, such as income, remain unchanged. For a graphical interpretation, the two fitted regression lines are superimposed in Figure 3.5. 3.5.2 Transforming Explanatory Variables Regression models have the ability to represent complex, nonlinear relationships between the expected response and the explanatory variables. For example, early regression texts, such as Plackett (1960, Chapter 6), devote an entire chapter of material to polynomial regression, \\[\\begin{equation} \\text{E } y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_p x^p. \\tag{3.12} \\end{equation}\\] Here, the idea is that a \\(p\\)th order polynomial in \\(x\\) can be used to approximate general, unknown nonlinear functions of \\(x\\). The modern-day treatment of polynomial regression does not require an entire chapter because the model in equation (3.12) can be expressed as a special case of the linear regression model. That is, with the regression function in equation (3.5), \\(\\text{E } y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k\\), we can choose \\(k = p\\) and \\(x_1 =x\\), \\(x_2 = x^2\\), \\(\\ldots\\), \\(x_p = x^p\\). Thus, with these choices of explanatory variables, we can model a highly nonlinear function of \\(x\\). We are not restricted to powers of \\(x\\) in our choice of transformations. For example, the model \\(\\text{E } y = \\beta_0 + \\beta_1 \\ln x\\), provides another way to represent a gently sloping curve in \\(x\\). This model can be written as a special case of the basic linear regression model using \\(x^{\\ast} = \\ln x\\) as the transformed version of \\(x\\). Transformations of explanatory variables need not be smooth functions. To illustrate, in some applications, it is useful to categorize a continuous explanatory variable. For example, suppose that \\(x\\) represents the number of years of education, ranging from 0 to 17. If we are relying on information self-reported by our sample of senior citizens, there may be a substantial amount of error in the measurement of \\(x\\). We could elect to use a less informative, but more reliable, transform of \\(x\\) such as \\(x^{\\ast}\\), a binary variable for finishing 13 years of school (finishing high school). Formally, we would code \\(x^{\\ast} = 1\\) if \\(x \\geq 13\\) and \\(x^{\\ast} = 0\\) if \\(x &lt; 13\\). Thus, there are several ways that nonlinear functions of the explanatory variables can be used in the regression model. An example of a nonlinear regression model is \\(y = \\beta_0 + \\exp (\\beta_1 x) + \\varepsilon.\\) These typically arise in science applications of regressions where there are fundamental scientific principles guiding the complex model development. 3.5.3 Interaction Terms We have so far discussed how explanatory variables, say \\(x_1\\) and \\(x_2\\), affect the mean response in an additive fashion, that is, \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). Here, we expect \\(y\\) to increase by \\(\\beta_1\\) per unit increase in \\(x_1\\), with \\(x_2\\) held fixed. What if the marginal rate of increase of \\(\\mathrm{E}~y\\) differs for high values of \\(x_2\\) when compared to low values of \\(x_2\\)? One way to represent this is to create an interaction variable \\(x_3 = x_1 \\times x_2\\) and consider the model \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\). With this model, the change in the expected \\(y\\) per unit change in \\(x_1\\) now depends on \\(x_2\\). Formally, we can assess small changes in the regression function as: \\[ \\frac{\\partial \\mathrm{E}~y}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} \\left(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\right) = \\beta_1 + \\beta_3 x_2 . \\] In this way, we may allow for more complicated functions of \\(x_1\\) and \\(x_2\\). Figure 3.6 illustrates this complex structure. From this figure and the above calculations, we see that the partial changes of \\(\\mathrm{E}~y\\) due to movement of \\(x_1\\) depend on the value of \\(x_2\\). In this way, we say that the partial changes due to each variable are not unrelated but rather “move together.” Figure 3.6: Plot of \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\) versus \\(x_1\\) and \\(x_2\\). More generally, an interaction term is a variable that is created as a nonlinear function of two or more explanatory variables. These special terms, even though permitting us to explore a rich family of nonlinear functions, can be cast as special cases of the linear regression model. To do this, we simply create the variable of interest and treat this new term as another explanatory variable. Of course, not every variable that we create will be useful. In some instances, the created variable will be so similar to variables already in our model that it will provide us with no new information. Fortunately, we can use \\(t\\)-tests to check whether the new variable is useful. Further, Chapter 4 will introduce a test to decide whether a group of variables is useful. The function that we use to create an interaction variable must be more than just a linear combination of other explanatory variables. For example, if we use \\(x_3 = x_1 + x_2\\), we will not be able to estimate all of the parameters. Chapter 5 will introduce some techniques to help avoid situations when one variable is a linear combination of the others. To give you some exposure to the wide variety of potential applications of special explanatory variables, we now present a series of short examples. Example: Term Life Insurance - Continued. How do we interpret the interaction of a binary variable with a continuous variable? To illustrate, consider a Term Life regression model, \\[ \\mathrm{LNFACE} = \\beta_0 + \\beta_1 \\mathrm{LNINCOME} + \\beta_2 \\mathrm{SINGLE} + \\beta_3 \\mathrm{LNINCOME*SINGLE} + \\varepsilon . \\] In this model, we have created a third explanatory variable through the interaction of LNINCOME and SINGLE. The regression function can be written as: \\[ \\mathrm{E}~y = \\begin{cases} \\beta_0 + \\beta_1 \\mathrm{LNINCOME}, &amp; \\text{for other respondents}, \\\\ \\beta_0 + \\beta_2 + (\\beta_1 + \\beta_3) \\mathrm{LNINCOME}, &amp; \\text{for single respondents}. \\end{cases} \\] Thus, through this single model with four parameters, we can create two separate regression lines, one for those single and one for others. Figure 3.7 shows the two fitted regression lines for our data. Figure 3.7: Letter plot of LNFACE versus LNINCOME, with the letter code S for single and o for other. The fitted regression lines have been superimposed. The lower line is for single and the upper line is for other. Example: Life Insurance Company Expenses. In a well-developed life insurance industry, minimizing expenses is critical for a company’s competitive position. Segal (2002) analyzed annual accounting data from over 100 firms for the period 1995-1998, inclusive, using a database from the National Association of Insurance Commissioners (NAIC) and other reported information. Segal modeled overall company expenses as a function of firm outputs and the price of inputs. The outputs consist of insurance production, measured by \\(x_1\\) through \\(x_5\\), described in Table 3.10. Segal also considered the square of each output, as well as an interaction term with a dummy/binary variable \\(D\\) that indicates whether or not the firm uses a branch company to distribute its products. (In a branch company, field managers are company employees, not independent agents.) Table 3.10.Twenty-Three Regression Coefficients from an Expense Cost Model \\[ \\small{ \\begin{array}{l|rrrr} \\hline &amp; \\text{Variable} &amp; &amp;\\text{Variable}&amp;\\text{ Squared} \\\\ &amp; \\text{Baseline} &amp; \\text{Interaction} &amp; \\text{Baseline} &amp; \\text{Interaction} \\\\ &amp; &amp; \\text{with}~~ &amp; &amp; \\text{with}~~ \\\\ \\text{Variable} &amp; (D=0) &amp; (D=1) &amp; (D=0) &amp; (D=1)\\\\ \\hline \\text{Number of Life Policies Issued } (x_1) &amp; -0.454 &amp; 0.152 &amp; 0.032 &amp; -0.007 \\\\ \\text{Amount of Term Life Insurance Sold } (x_2) &amp; 0.112 &amp; -0.206 &amp; 0.002 &amp; 0.005 \\\\ \\text{Amount of Whole Life Insurance Sold } (x_3) &amp; -0.184 &amp; 0.173 &amp; 0.008 &amp; -0.007 \\\\ \\text{Total Annuity Considerations } (x_4) &amp; 0.098 &amp; -0.169 &amp; -0.003 &amp; 0.009 \\\\ \\text{Total Accident and Health Premiums } (x_5) &amp;-0.171 &amp; 0.014 &amp; 0.010 &amp; 0.002 \\\\ \\text{Intercept } &amp; 7.726 &amp; &amp; &amp; \\\\ \\text{Price of Labor (PL)} &amp; 0.553 &amp; &amp; &amp; \\\\ \\text{Price of Capital (PC)} &amp; 0.102 &amp; &amp; &amp; \\\\ \\hline \\end{array} \\\\ \\begin{array}{l} \\textit{Note: } x_1 \\text{ through } x_5 \\text{ are in logarithmic units}.\\\\ \\textit{Source: Segal (2002)} \\end{array} } \\] For the price inputs, the price of labor (\\(PL\\)) is defined to be the total cost of employees and agents divided by their number, in logarithmic units. The price of capital (\\(PC\\)) is approximated by the ratio of capital expense to the number of employees and agents, also in logarithmic units. The price of materials consists of expenses other than labor and capital divided by the number of policies sold and terminated during the year. It does not appear directly as an explanatory variable. Rather, Segal took the dependent variable (\\(y\\)) to be total company expenses divided by the price of materials, again in logarithmic units. With these variable definitions, Segal estimated the following regression function: \\[ \\mathrm{E~}y=\\beta_0 + \\sum_{j=1}^5 \\left( \\beta_j x_j + \\beta_{j+5} D x_j + \\beta_{j+10} x_j^2 + \\beta_{j+15}D x_j^2 \\right) + \\beta_{21} PL + \\beta_{22} PC. \\] The parameter estimates appear in Table 3.10. For example, the marginal change in \\(\\mathrm{E}~y\\) per unit change in \\(x_1\\) is: \\[ \\frac{\\partial ~ \\mathrm{E}~y}{\\partial x_1}= \\beta_1 + \\beta_{6} D + 2 \\beta_{11} x_1 + 2 \\beta_{16}D x_1, \\] which is estimated as $ -0.454 + 0.152 D + (0.064 - 0.014 D) x_1$. For these data, the median number of policies issued was \\(x_1=15,944\\). At this value of \\(x_1\\), the estimated marginal change is $ -0.454 + 0.152 D + (0.064 - 0.014 D) (15944) = 0.165 + 0.017 D,$ or 0.165 for baseline \\((D=0)\\) and 0.182 for branch \\((D=1)\\) companies. These estimates are elasticities, as defined in Section 3.2.2. To interpret these coefficients further, let \\(COST\\) represent total general company expenses and \\(NUMPOL\\) represent the number of life policies issued. Then, for branch \\((D=1)\\) companies, we have: \\[ 0.182 \\approx \\frac{\\partial y }{\\partial x_1 } = \\frac{\\partial ~ \\mathrm{ln}~COST}{\\partial ~ \\mathrm{ln}~NUMPOL}= \\frac{ \\frac{\\partial ~ COST}{\\partial ~NUMPOL}} {\\frac{COST}{NUMPOL}}, \\] or \\(\\frac{\\partial ~ COST}{\\partial ~NUMPOL} \\approx 0.182 \\frac{COST}{NUMPOL}\\). The median cost is $15,992,000, so the marginal cost per policy at these median values is \\(0.182 \\times (15992000/15944) = \\$182.55\\). Special Case: Curvilinear Response Functions We can expand the polynomial functions of an explanatory variable to include several explanatory variables. For example, the expected response, or response function, for a second-order model with two explanatory variables is: \\[ \\mathrm{E} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2. \\] Figure 3.8 illustrates this response function. Similarly, the response function for a second-order model with three explanatory variables is: \\[ \\mathrm{E} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{33} x_3^2 + \\beta_{12} x_1 x_2 + \\beta_{13} x_1 x_3 + \\beta_{23} x_2 x_3. \\] When there is more than one explanatory variable, third and higher-order models are rarely used in applications. Figure 3.8: Plot of \\(\\mathrm{E}~y = \\beta_0 + \\beta_1~x_1 + \\beta_2~x_2 + \\beta_{11}~x_1^2 + \\beta_{22}~x_2^2 + \\beta_{12}~x_1~x_2\\) versus \\(x_1\\) and \\(x_2\\). R Code to Produce Figure 3.8 # FIGURE 3.8 X1 &lt;- seq(3, 16, length=15) X2 &lt;- seq(5, 15, length=15) f &lt;- function(X1,X2) {y &lt;- 50 + 2*X1 + 3*X2 + 3*(X1-10)*(X2-10)- .4*(X1-10)^2+.4*(X2-10)^2} y &lt;- outer(X1, X2, f) par(mar=c(0,2.1,0,.1)) persp(X1, X2, y, theta = 30, phi = 30, expand = 0.5, ticktype=&quot;detailed&quot;) Special Case: Nonlinear Functions of a Continuous Variable In some applications, we expect the response to have some abrupt changes in behavior at certain values of an explanatory variable, even if the variable is continuous. For example, suppose that we are trying to model an individual’s charitable contributions (\\(y\\)) in terms of their wages (\\(x\\)). For 2007 data, a simple model we might entertain is given in Figure 3.9. Figure 3.9: The marginal change in \\(\\mathrm{E}~y\\) is lower below $97,500. The parameter \\(\\beta_2\\) represents the difference in the slopes. R Code to Produce Figure 3.9 # FIGURE 3.9 x &lt;- seq(90000,105000,20) Ey &lt;- 20 + 1*x + 2*(x-97500)*(x&gt;97500) par(mar=c(4.1,3.1,.1,1), cex=1.3) plot(x,Ey, type=&quot;l&quot;,yaxt=&quot;n&quot;, ylab=&quot;&quot;, las=1) mtext(&quot;E y&quot;, side=2,las=1, line=1.5,cex=1.1) text(102000,102000,expression(beta[1]+beta[2]),cex=1.1) text(94000,92000,expression(beta[1]),cex=1.1) A rationale for this model is that, in 2007, individuals paid 7.65% of their income for Social Security taxes up to 97,500. No Social Security taxes are excised on wages in excess of 97,500. Thus, one theory is that, for wages in excess of $97,500, individuals have more disposable income per dollar and thus should be more willing to make charitable contributions. To model this relationship, define the binary variable \\(z\\) to be zero if \\(x &lt; 97,500\\) and to be one if \\(x \\ge 97,500\\). Define the regression function to be \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x + \\beta_2 z (x - 97,500)\\). This can be written as: \\[ \\mathrm{E}~y = \\begin{cases} \\beta_0 + \\beta_1 x &amp; x &lt; 97,500 \\\\ \\beta_0 - \\beta_2(97,500) + (\\beta_1+\\beta_2) x &amp; x \\geq 97,500 \\end{cases} \\] To estimate this model, we would run a regression of \\(y\\) on two explanatory variables, \\(x_1 = x\\) and \\(x_2 = z \\times (x - 97,500)\\). If \\(\\beta_2 &gt; 0\\), then the marginal rate of charitable contributions is higher for incomes exceeding $97,500. Figure 3.9 illustrates this relationship, known as piecewise linear regression or sometimes a “broken stick” model. The sharp break in Figure 3.9 at \\(x = 97,500\\) is called a “kink.” We have linear relationships above and below the kinks and have used a binary variable to put the two pieces together. We are not restricted to one kink. For example, suppose that we wish to do a historical study of Federal taxable income for 1992 single filers. Then, there were three tax brackets: the marginal tax rate below 21,450 was 15%, above 51,900 was 31%, and in between was 28%. For this example, we would use two kinks, at 21,450 and 51,900. Further, piecewise linear regression is not restricted to continuous response functions. For example, suppose that we are studying the commissions paid to stockbrokers (\\(y\\)) in terms of the number of shares purchased by a client (\\(x\\)). We might expect to see the relationship illustrated in Figure 3.10. Here, the discontinuity at \\(x = 100\\) reflects the administrative expenses of trading in odd lots, as trades of less than 100 shares are called. The lower marginal cost for trades in excess of 100 shares simply reflects the economies of scale for doing business in larger volumes. A regression model of this is \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 z x\\) where \\(z = 0\\) if \\(x &lt; 100\\) and \\(z = 1\\) if \\(x \\geq 100\\). The regression function depicted in Figure 3.10 is: \\[ \\mathrm{E}~y = \\begin{cases} \\beta_0 + \\beta_1 x_1 &amp; x &lt; 100 \\\\ \\beta_0 + \\beta_2 + (\\beta_1+\\beta_3) x_1 &amp; x \\geq 100 \\end{cases} \\] Figure 3.10: Plot of expected commissions (\\(\\mathrm{E}~y\\)) versus number of shares traded (\\(x\\)). The break at \\(x=100\\) reflects savings in administrative expenses. The lower slope for \\(x \\ge 100\\) reflects economies of scales in expenses. R Code to Produce Figure 3.10 # FIGURE 3.10 par(mar=c(4.1,4.5,.1,.1)) x &lt;- seq(50,150,.1) Ey &lt;- 20 + 2*x - 1*(x-100)*(x&gt;100) - 20*(x&gt;100) plot(x,Ey, type=&quot;p&quot;,ylab=&quot;&quot;, font.lab=1, cex.lab=1.1, cex=.25, las=1) mtext(&quot;E y&quot;, side=2,las=1, line=2.8,cex=1.1) 3.6 Further Reading and References For proofs of the Chapter 3 results, we refer the reader to Goldberger (1991). Nonlinear regression models are discussed in, for example, Bates and Watts (1988). Chapter 3 has introduced the fundamentals of multiple linear regression. Chapter 4 will widen the scope by introducing categorical variables and statistical inference methods for handling several coefficients simultaneously. Chapter 5 will introduce techniques to help you pick appropriate variables in a multiple linear regression model. Chapter 6 is a synthesis chapter, discussing model interpretation, variable selection and data collection. Chapter References Bates, Douglas M. and Watts, D. G. (1988). Nonlinear Regression Analysis and its Applications. John Wiley &amp; Sons, New York. Lemaire, Jean (2002). Why do females live longer than males? North American Actuarial Journal, 6(4), 21-37. Goldberger, Arthur (1991). A Course in Econometrics. Harvard University Press, Cambridge. Plackett, R.L. (1960). Regression Analysis. Clarendon Press, Oxford, England. Segal, Dan (2002). An economic analysis of life insurance company expenses. North American Actuarial Journal, 6(4), 81-94. 3.7 Exercises 3.1. Consider a fictitious data set of \\(n = 100\\) observations with \\(s_y = 100\\). We run a regression with three explanatory variables to get \\(s = 50\\). Calculate the adjusted coefficient of determination, \\(R_a^2\\). Complete the ANOVA table. \\[ \\small{ \\begin{array}{|l|l|c|l|} \\hline \\text{ANOVA Table} \\\\ \\hline \\text{Source} &amp; \\text{Sum of Squares} &amp; \\text{df} &amp; \\text{Mean Square} \\\\ \\hline \\text{Regression} &amp; &amp; &amp; \\\\ \\text{Error} &amp; &amp; &amp; \\\\ \\text{Total} &amp; &amp; &amp; \\\\ \\hline \\end{array} } \\] c. Calculate the (unadjusted) coefficient of determination, \\(R^2\\). 3.2. Consider a fictitious data set of \\(n = 100\\) observations with \\(s_y = 80\\). We run a regression with three explanatory variables to get \\(s = 50\\). We also get \\[ \\small{ \\left(\\mathbf{X^{\\prime} X} \\right)^{-1} = \\begin{pmatrix} 100 &amp; 20 &amp; 20 &amp; 20 \\\\ 20 &amp; 90 &amp; 30 &amp; 40 \\\\ 20 &amp; 30 &amp; 80 &amp; 50 \\\\ 20 &amp; 40 &amp; 50 &amp; 70 \\\\ \\end{pmatrix}. } \\] Determine the standard error of \\(b_3\\), \\(se(b_3)\\). Determine the estimated covariance between \\(b_2\\) and \\(b_3\\). Determine the estimated correlation between \\(b_2\\) and \\(b_3\\). Determine the estimated variance of \\(4b_2 + 3b_3\\). 3.3. Consider the following small fictitious data set. You will be fitting a regression model to \\(y\\) using two explanatory variables, \\(x_1\\) and \\(x_2\\). \\[ \\small{ \\begin{array}{c|cccc} \\hline i &amp; 1 &amp; 2 &amp; 3 &amp; 4 \\\\ \\hline x_{i,1} &amp; -1 &amp; 2 &amp; 4 &amp; 6 \\\\ x_{i,2} &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ y_i &amp; 0 &amp; 1 &amp; 5 &amp; 8 \\\\ \\hline \\end{array} } \\] From the fitted regression model, we have \\(s = 1.373\\) and \\[ \\small{ \\mathbf{b} = \\begin{pmatrix} 0.1538 \\\\ 0.6923 \\\\ 2.8846 \\end{pmatrix} ~~~\\text{and}~~~ \\left(\\mathbf{X^{\\prime} X} \\right)^{-1} = \\begin{pmatrix} 0.53846 &amp; -0.07692 &amp; -0.15385 \\\\ -0.07692 &amp; 0.15385 &amp; -0.69231 \\\\ -0.15385 &amp; -0.69231 &amp; 4.11538 \\\\ \\end{pmatrix}. } \\] Write down the vector of dependent variables, \\(\\mathbf{y}\\), and the matrix of explanatory variables, \\(\\mathbf{X}\\). Determine the numerical value for \\(\\widehat{y}_3\\), the fitted value for the third observation. Determine the numerical value for \\(se(b_2)\\). Determine the numerical value for \\(t(b_1)\\). 3.4. Wisconsin Lottery. Section 2.1 described a sample of \\(n=50\\) geographic areas (zip codes) containing sales data on the Wisconsin state lottery (\\(y = \\text{SALES}\\)). In that section, sales were analyzed using a basic linear regression model with \\(x = \\text{POP}\\), the area population, as the explanatory variable. This exercise extends that analysis by introducing additional explanatory variables given in Table 3.11. \\[ \\text{Table 3.11: Lottery, economic and demographic characteristics of fifty Wisconsin ZIP codes} \\] \\[ \\small{ \\begin{array}{ll} \\hline \\textbf{Lottery characteristics} \\\\ \\hline \\text{SALES} &amp; \\text{Online lottery sales to individual consumers} \\\\ \\hline \\textbf{Economic and } \\\\ ~~~~~\\textbf{demographic characteristics} \\\\\\hline \\text{PERPERHH} &amp; \\text{Persons per household} \\\\ \\text{MEDSCHYR} &amp; \\text{Median years of schooling} \\\\ \\text{MEDHVL} &amp; \\text{Median home value in \\$1000s for owner-occupied homes} \\\\ \\text{PRCRENT} &amp; \\text{Percent of housing that is renter occupied} \\\\ \\text{PRC55P} &amp; \\text{Percent of population that is 55 or older} \\\\ \\text{HHMEDAGE} &amp; \\text{Household median age} \\\\ \\text{MEDINC} &amp; \\text{Estimated median household income, in \\$1000s} \\\\ \\text{POP} &amp; \\text{Population} \\\\ \\hline \\end{array} } \\] Produce a table of summary statistics for all variables. One zip code (observation 11, zip = 53211, Shorewood Wisconsin, a suburb of Milwaukee) appears to have unusually large values of MEDSCHYR and MEDHVL. For this observation, how many standard deviations is the value of MEDSCHYR above the mean? For this observation, how many standard deviations is the value of MEDHVL above the mean? Produce a table of correlations. What three variables are most highly correlated with SALES? Produce a scatter plot matrix of all explanatory variables and SALES. In the plot of MEDSCHYR versus SALES, describe the position of observation 11. Fit a linear model of SALES on all eight explanatory variables. Summarize the fit of this model by citing the residual standard deviation, \\(s\\), the coefficient of determination, \\(R^2\\) and its adjusted version, \\(R_a^2\\). Based on your part (d) model fit, is MEDSCHYR a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion and your decision-making rule. Now fit a more parsimonious model, using SALES as the dependent variable and MEDSCHYR, MEDHVL and POP as explanatory variables. Summarize the fit of this model by citing the residual standard deviation, \\(s\\), the coefficient of determination, \\(R^2\\) and its adjusted version, \\(R_a^2\\). How do these values compare to the model fit in part (d)? Note that the sign of the regression coefficient associated with MEDSCHYR is negative. To help interpret this coefficient, compute the corresponding partial correlation coefficient. What is the interpretation of this coefficient? To get further insights into the relation between MEDSCHYR and SALES, produce an added variable plot controlling for the effects of MEDHVL and POP. Check that the correlation associated with this plot agrees with your answer in part (g). Re-run the regression in part (f), after removing observation 11. Cite the basic summary statistics from this regression. For this model fit, is MEDSCHYR a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion and your decision-making rule. Re-run the regression in part (f), after removing observation 9. Cite the basic summary statistics from this regression. 3.5. Insurance Company Expenses. This exercise considers insurance company data from the NAIC and described in Exercise 1. Table 3.10 describes several variables that can be used to explain expenses. As with Segal’s (2002) study of life insurers, firm “outputs” consist of premiums written (for property and casualty, these are subdivided into personal and commercial lines) as well as losses (subdivided into short and long tail lines). ASSETS and CASH are commonly used measures of the size of a company. GROUP, STOCK, and MUTUAL describe the organizational structure. Firm “inputs” were gathered from the Bureau of Labor Statistics (BLS, from the Occupational Employee Statistics program). STAFFWAGE is calculated as the average wage in the state where the insurance company is headquartered. AGENTWAGE is calculated as the weighted average of annual wages of the brokerage industry, weighted by the percentage of gross premium written in each state. Table 3.10: Insurer Expense Variables Variable Description EXPENSES Total expenses incurred, in millions of dollars LONGLOSS Losses incurred for long tail lines, in millions of dollars SHORTLOSS Losses incurred for short tail lines, in millions of dollars GPWPERSONAL Gross premium written for personal lines, in millions of dollars GPWCOMM Gross premium written for commercial lines, in millions of dollars ASSETS Net admitted assets, in millions of dollars CASH Cash and invested assets, in millions of dollars GROUP Indicates if the company is affiliated STOCK Indicates if the company is a stock company MUTUAL Indicates if the company is a mutual company STAFFWAGE Annual average wage of the insurer’s administrative staff, in thousands of dollars AGENTWAGE Annual average wage of the insurance agent, in thousands of dollars A preliminary inspection of the data showed that many firms did not report any insurance losses incurred in 2005. For this exercise, we consider the 384 companies with some losses in the file NAICExpense.csv. Summary Statistics Produce summary statistics of the response variable and the (non-binary) explanatory variables. Note the pattern of skewness for each variable. Note that many variables have negative values. Transform Variables Transform each non-binary variable through the modified logarithm transform, \\(\\ln(1+x)\\). Produce summary statistics of these modified non-binary explanatory variables. Let LNEXPENSES (\\(= \\ln(1+\\text{EXPENSES})\\)) denote the modified expense variable. For subsequent analysis, use only the modified variables described in part (b). Correlation Table Produce a table of correlations for the non-binary variables. What three variables are most highly correlated with LNEXPENSES? Boxplot of LNEXPENSES by GROUP Provide a boxplot of LNEXPENSES by level of GROUP. Which level of group has higher expenses? Linear Model on All Variables Fit a linear model of LNEXPENSES on all eleven explanatory variables. Summarize the fit of this model by citing the residual standard deviation, \\(s\\), the coefficient of determination, \\(R^2\\), and its adjusted version, \\(R^2_a\\). Reduced Model Fit a linear model of LNEXPENSES on a reduced model using eight explanatory variables, dropping LNCASH, STOCK, and MUTUAL. For the explanatory variables, include ASSETS, GROUP, both versions of losses, and gross premiums, as well as the two BLS variables. f(i). Summarize the fit of this model by citing \\(s\\), \\(R^2\\), and \\(R^2_a\\). f(ii). Interpret the coefficient associated with commercial lines gross premiums on the logarithmic scale. f(iii). Suppose that GPWCOMM increases by $1, how much do we expect EXPENSES to increase? Use your answer in part f(ii) and median values of GPWCOMM and EXPENSES for this question. Quadratic Terms Square each of the two loss and the two gross premium variables. Fit a linear model of LNEXPENSES on a reduced model using twelve explanatory variables, the eight variables in part (f) and the four additional squared terms just created. g(i). Summarize the fit of this model by citing \\(s\\), \\(R^2\\), and \\(R^2_a\\). g(ii). Do the quadratic variables appear to be useful explanatory variables? Excluding BLS Variables Now omit the two BLS variables, so you are fitting a model of LNEXPENSES on ASSETS, GROUP, both versions of losses, and gross premiums, as well as quadratic terms. Summarize the fit of this model by citing \\(s\\), \\(R^2\\), and \\(R^2_a\\). Comment on the number of observations used to fit this model compared to part (f). Interaction Terms Drop the quadratic terms in part (g) and add interaction terms with the dummy variable GROUP. Thus, there are now eleven variables: ASSETS, GROUP, both versions of losses and gross premiums, as well as interactions of GROUP with ASSETS and both versions of losses and gross premiums. i(i). Summarize the fit of this model by citing \\(s\\), \\(R^2\\), and \\(R^2_a\\). i(ii). Suppose that GPWCOMM increases by $1, how much do we expect EXPENSES to increase for GROUP=0 companies? Use the median values of GPWCOMM and EXPENSES of GROUP=0 companies for this question. i(iii). Suppose that GPWCOMM increases by $1, how much do we expect EXPENSES to increase for GROUP=1 companies? Use the median values of GPWCOMM and EXPENSES of GROUP=1 companies for this question. 3.6 National Life Expectancies. We continue the analysis begun in Exercises 1 and 2. Now fit a regression model on LIFEEXP using three explanatory variables: FERTILITY, PUBLICEDUCATION, and lnHEALTH (the natural logarithmic transform of PRIVATEHEALTH). Interpretation of Public Education Coefficient Interpret the regression coefficient associated with PUBLICEDUCATION. Interpretation of Health Expenditures Coefficient Interpret the regression coefficient associated with HEALTH expenditures without using the logarithmic scale for expenditures. Statistical Significance of PUBLICEDUCATION Based on the model fit, is PUBLICEDUCATION a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. Added Variable Plot The negative sign of the PUBLICEDUCATION coefficient is surprising, given that the sign of the correlation between PUBLICEDUCATION and LIFEEXP is positive and intuition suggests a positive relation. To check this result, an added variable plot appears in Figure 3.11. d(i). For an added variable plot, describe its purpose and a method for producing it. d(ii). Calculate the correlation corresponding to the added variable plot that appears in Figure 3.11. Figure 3.11: Added variable plot of PUBLICEDUCATION versus LIFEEXP, controlling for FERTILITY and lnHEALTH R Code to Produce Figure 3.11 # FIGURE 3.11 LifeExp &lt;- read.csv(&quot;CSVData/UNLifeExpectancy.csv&quot;, header=TRUE) # REMOVE MISSING LIFEEXPs LifeExp3 &lt;- subset(LifeExp, !is.na(LIFEEXP) ) varLife &lt;- c(&quot;FERTILITY&quot;,&quot;PUBLICEDUCATION&quot;, &quot;REGION&quot;,&quot;COUNTRY&quot;,&quot;LIFEEXP&quot;, &quot;HEALTHEXPEND&quot;) LifeExp4 &lt;- LifeExp3[varLife] LifeExp4$lnHEALTH &lt;- log(LifeExp4$HEALTHEXPEND) LifeExp4.good &lt;- na.omit(LifeExp4) # ADDED VARIABLE PLOT model4a &lt;- lm(LIFEEXP ~ FERTILITY+lnHEALTH, data=LifeExp4.good) model4b &lt;- lm(PUBLICEDUCATION ~ FERTILITY+lnHEALTH, data=LifeExp4.good) plot(residuals(model4b),residuals(model4a), xlab=&quot;residuals(PUBLICEDUCATION)&quot;,ylab=&quot;residuals(LIFEEXP)&quot;) "],["C4MLRANOVA.html", "Chapter 4 Multiple Linear Regression - II 4.1 The Role of Binary Variables 4.2 Statistical Inference for Several Coefficients 4.3 One Factor ANOVA Model 4.4 Combining Categorical and Continuous Explanatory Variables 4.5 Further Reading and References 4.6 Exercises 4.7 Technical Supplement - Matrix Expressions", " Chapter 4 Multiple Linear Regression - II Chapter Preview. This chapter extends the discussion of multiple linear regression by introducing statistical inference for handling several coefficients simultaneously. To motivate this extension, this chapter considers coefficients associated with categorical variables. These variables allow us to group observations into distinct categories. This chapter shows how to incorporate categorical variables into regression functions using binary variables, thus considerably widening the scope of potential applications for regression analysis. Statistical inference for several coefficients allows analysts to make decisions about categorical variables, as well as other important applications. Categorical explanatory variables also provide the basis for an ANOVA model, a special type of regression model that permits easier analysis and interpretation. 4.1 The Role of Binary Variables Categorical variables provide labels for observations to denote membership in distinct groups, or categories. A binary variable is a special case of a categorical variable. To illustrate, a binary variable may tell us whether or not someone has health insurance. A categorical variable could tell us whether someone has: private group insurance (offered by employers and associations), private individual health insurance (through insurance companies), public insurance (such as Medicare or Medicaid), no health insurance. For categorical variables, there may or may not be an ordering of the groups. In health insurance, it is difficult to order these four categories and say which is “larger,” private group, private individual, public, or no health insurance. In contrast, for education, we might group individuals into “low,” “intermediate,” and “high” years of education. In this case, there is an ordering among groups based on the level of educational achievement. As we will see, this ordering may or may not provide information about the dependent variable. Factor is another term used for an unordered categorical explanatory variable. For ordered categorical variables, analysts typically assign a numerical score to each outcome and treat the variable as if it were continuous. For example, if we had three levels of education, we might employ ranks and use \\[ \\small{ \\text{EDUCATION} = \\begin{cases} 1 &amp; \\text{for low education} \\\\ 2 &amp; \\text{for intermediate education} \\\\ 3 &amp; \\text{for high education.} \\end{cases} } \\] An alternative would be to use a numerical score that approximates an underlying value of the category. For example, we might use \\[ \\small{ \\text{EDUCATION} = \\begin{cases} 6 &amp; \\text{for low education} \\\\ 10 &amp; \\text{for intermediate education} \\\\ 14 &amp; \\text{for high education.} \\end{cases} } \\] This gives the approximate number of years of schooling that individuals in each category completed. The assignment of numerical scores and treating the variable as continuous has important implications for the regression modeling interpretation. Recall that the regression coefficient is the marginal change in the expected response; in this case, the \\(\\beta\\) for education assesses the increase in \\(\\mathrm{E }~y\\) per unit change in EDUCATION. If we record EDUCATION as a rank in a regression model, then the \\(\\beta\\) for education corresponds to the increase in \\(\\mathrm{E }~y\\) moving from EDUCATION=1 to EDUCATION=2 (from low to intermediate); this increase is the same as moving from EDUCATION=2 to EDUCATION=3 (from intermediate to high). Do we want to model this increase as the same? This is an assumption that the analyst makes with this coding of EDUCATION; it may or may not be valid but certainly needs to be recognized. Because of this interpretation of coefficients, analysts rarely use ranks or other numerical scores to summarize unordered categorical variables. The most direct way of handling factors in regression is through the use of binary variables. A categorical variable with \\(c\\) levels can be represented using \\(c\\) binary variables, one for each category. For example, suppose that we were uncertain about the direction of the education effect and so decide to treat it as a factor. Then, we could code \\(c=3\\) binary variables: (1) a variable to indicate low education, (2) one to indicate intermediate education, and (3) one to indicate high education. These binary variables are often known as dummy variables. In regression analysis with an intercept term, we use only \\(c-1\\) of these binary variables; the remaining variable enters implicitly through the intercept term. By identifying a variable as a factor, most statistical software packages will automatically create binary variables for you. Through the use of binary variables, we do not make use of the ordering of categories within a factor. Because no assumption is made regarding the ordering of the categories, for the model fit it does not matter which variable is dropped with regard to the fit of the model. However, it does matter for the interpretation of the regression coefficients. Consider the following example. Example: Term Life Insurance - Continued. We now return to the marital status of respondents from the Survey of Consumer Finances (SCF). Recall that marital status is not measured continuously but rather takes on values that fall into distinct groups that we treat as unordered. In Chapter 3, we grouped survey respondents according to whether or not they are “single,” where being single includes never married, separated, divorced, widowed, and not married but living with a partner. We now supplement this by considering the categorical variable, MARSTAT, which represents the marital status of the survey respondent. This may be: 1, for married 2, for living with a partner 0, for other (SCF further breaks down this category into separated, divorced, widowed, never married, inapplicable, persons age 17 or less, and no further persons). As before, the dependent variable is \\(y =\\) LNFACE, the amount that the company will pay in the event of the death of the named insured (in logarithmic dollars). Table 4.1 summarizes the dependent variable by the level of the categorical variable. This table shows that the marital status “married” is the most prevalent in the sample and that those married choose to have the most life insurance coverage. Figure 4.1 gives a more complete picture of the distribution of LNFACE for each of the three types of marital status. The table and figure also suggest that those living together have less life insurance coverage than the other two categories. Table 4.1: Summary Statistics of Logarithmic Face By Marital Status MARSTAT Number Mean Standard Deviation Other 0 57 10.958 1.566 Married 1 208 12.329 1.822 Living together 2 10 10.825 2.001 Total 275 11.990 1.871 Figure 4.1: Box Plots of Logarithmic Face, by Level of Marital Status R Code to Produce Table 4.1 and Figure 4.1 tableout &lt;- data.frame( MARSTAT &lt;- c(0,1,2,&quot;&quot;), Number &lt;- c(57, 208, 10, 275), Mean &lt;- c(10.958, 12.329, 10.825, 11.990), Standard_Deviation &lt;- c(1.566, 1.822, 2.001, 1.871) ) colnames(tableout) &lt;- c(&quot;MARSTAT&quot;, &quot;Number&quot;, &quot;Mean&quot;, &quot;Standard Deviation&quot;) rownames(tableout) &lt;- c(&quot;Other&quot;, &quot;Married&quot;, &quot;Living together&quot;, &quot;Total&quot;) TableGen1(TableData=tableout , TextTitle=&#39;Summary Statistics of Logarithmic Face By Marital Status&#39;, Align=&#39;crrr&#39;, Digits=3, ColumnSpec=1:3, ColWidth = ColWidth4) library(HH) Term &lt;- read.csv(&quot;CSVData/TermLife.csv&quot;, header=TRUE) Term2 &lt;- subset(Term, FACE &gt; 0) LNFACE &lt;- log(Term2$FACE) LNINCOME &lt;- log(Term2$INCOME) MAR0 &lt;- 1*(Term2$MARSTAT == 0) # FIGURE 4.1 par(mar=c(4.1,4,1,1), cex=1.1) boxplot(LNFACE ~ MARSTAT, data=Term2, ylab=&quot;&quot;, xlab=&quot;Marital Status&quot;) mtext(&quot;LNFACE&quot;, side=2, at=17.2, las=1, cex=1.1, adj=.4) Are the continuous and categorical variables jointly important determinants of response? To answer this, a regression was run using LNFACE as the response and five explanatory variables: three continuous and two binary (for marital status). Recall that our three continuous explanatory variables are: LNINCOME (logarithmic annual income), the number of years of EDUCATION of the survey respondent, and the number of household members, NUMHH. For the binary variables, first define MAR0 to be the binary variable that is one if MARSTAT=0 and zero otherwise. Similarly, define MAR1 and MAR2 to be binary variables that indicate MARSTAT=1 and MARSTAT=2, respectively. There is a perfect linear dependency among these three binary variables in that MAR0 + MAR1 + MAR2 = 1 for any survey respondent. Thus, we need only two of the three. However, there is not a perfect dependency among any two of the three. It turns out that cor(MAR0, MAR1) = -0.90, cor(MAR0, MAR2) = -0.10, and cor(MAR1, MAR2) = -0.34. A regression model was run using LNINCOME, EDUCATION, NUMHH, MAR0, and MAR2 as explanatory variables. The fitted regression equation turns out to be: \\[ \\small{ \\widehat{y} = 3.395 + 0.452 \\text{LNINCOME} + 0.205 \\text{EDUCATION} + 0.248 \\text{NUMHH} - 0.557 \\text{MAR0} - 0.789 \\text{MAR2}. } \\] To interpret the regression coefficients associated with marital status, consider a respondent who is married. In this case, MAR0=0, MAR1=1, and MAR2=0, so that: \\[ \\small{ \\widehat{y}_m = 3.395 + 0.452 \\text{LNINCOME} + 0.205 \\text{EDUCATION} + 0.248 \\text{NUMHH}. } \\] Similarly, if the respondent is coded as living together, then MAR0=0, MAR1=0, and MAR2=1, and: \\[ \\small{ \\widehat{y}_{lt} = 3.395 + 0.452 \\text{LNINCOME} + 0.205 \\text{EDUCATION} + 0.248 \\text{NUMHH} - 0.789. } \\] The difference between \\(\\widehat{y}_m\\) and \\(\\widehat{y}_{lt}\\) is \\(0.789.\\) Thus, we may interpret the regression coefficient associated with MAR2, \\(-0.789\\), to be the difference in fitted values for someone living together compared to a similar person who is married (the omitted category). Similarly, we can interpret \\(-0.557\\) to be the difference between the “other” category and the married category, holding other explanatory variables fixed. For the difference in fitted values between the “other” and the “living together” categories, we may use \\(-0.557 - (-0.789) = 0.232.\\) Although the regression was run using MAR0 and MAR2, any two out of the three would produce the same ANOVA Table (Table 4.2). However, the choice of binary variables does impact the regression coefficients. Table 4.3 shows three models, omitting MAR1, MAR2, and MAR0, respectively. For each fit, the coefficients associated with the continuous variables remain the same. As we have seen, the binary variable interpretations are with respect to the omitted category, known as the reference level. Although they change from model to model, their overall interpretation remains the same. That is, if we would like to estimate the difference in coverage between the “other” and the “living together” category, the estimate would be \\(0.232\\), regardless of the model. Table 4.2: Term Life with Marital Status ANOVA Table Source Sum of Squares \\(df\\) Mean Square Regression 343.28 5 68.66 Error 615.62 269 2.29 Total 948.90 274 Although the three models in Table 4.3 are the same except for different choices of parameters, they do appear different. In particular, the \\(t\\)-ratios differ and give different appearances of statistical significance. For example, both of the \\(t\\)-ratios associated with marital status in Model 2 are less than 2 in absolute value, suggesting that marital status is unimportant. In contrast, both Models 1 and 3 have at least one marital status binary that exceeds 2 in absolute value, suggesting statistical significance. Thus, you can influence the appearance of statistical significance by altering the choice of the reference level. To assess the overall importance of marital status (not just each binary variable), Section 4.2 will introduce tests of sets of regression coefficients. Table 4.3: Term Life Regression Coefficients with Marital Status Model 1 Coefficient Model 1 \\(t\\)-Ratio Model 2 Coefficient Model 2 \\(t\\)-Ratio Model 3 Coefficient Model 3 \\(t\\)-Ratio LNINCOME 0.452 5.74 0.452 5.74 0.452 5.74 EDUCATION 0.205 5.30 0.205 5.30 0.205 5.30 NUMHH 0.248 3.57 0.248 3.57 0.248 3.57 Intercept 3.395 3.77 3.395 2.74 2.838 3.34 MAR0 -0.557 -2.15 0.232 0.44 NA NA MAR1 NA NA 0.789 1.59 0.557 2.15 MAR2 -0.789 -1.59 NA NA -0.232 -0.44 Example: How does Cost-Sharing in Insurance Plans affect Expenditures in Healthcare? In one of many studies that resulted from the Rand Health Insurance Experiment (HIE) introduced in Section 1.5, Keeler and Rolph (1988) investigated the effects of cost-sharing in insurance plans. For this study, 14 health insurance plans were grouped by the co-insurance rate (the percentage paid as out-of-pocket expenditures that varied by 0, 25, 50 and 95%). One of the 95% plans limited annual out-of-pocket outpatient expenditures to $150 per person ($450 per family), providing in effect an individual outpatient deductible. This plan was analyzed as a separate group so that there were \\(c=5\\) categories of insurance plans. In most insurance studies, individuals choose insurance plans making it difficult to assess cost-sharing effects because of adverse selection. Adverse selection can arise because individuals in poor chronic health are more likely to choose plans with less cost sharing, thus giving the appearance that less coverage leads to greater expenditures. In the Rand HIE, individuals were randomly assigned to plans, thus removing this potential source of bias. Keeler and Rolph (1988) organized an individual’s expenditures into episodes of treatment; each episode contains spending associated with a given bout of illness, chronic condition or procedure. Episodes were classified as hospital, dental or outpatient; this classification was based primarily on diagnoses, not by location of services. Thus, for example, outpatient services preceding or following a hospitalization, as well as related drugs and tests, were included as part of a hospital episode. For simplicity, here we report only results for hospital episodes. Although families were randomly assigned to plans, Keeler and Rolph (1988) used regression methods to control for participant attributes and isolate the effects of plan cost-sharing. Table 4.4 summarizes the regression coefficients, based on a sample of \\(n=1,967\\) episode expenditures. In this regression, logarithmic expenditure was the dependent variable. The cost-sharing categorical variable was decomposed into five binary variables so that no functional form was imposed on the response to insurance. These variables are “Co-ins25,” “Co-ins50,” and “Co-ins95,” for coinsurance rates 25, 50 and 95%, respectively, and “Indiv Deductible” for the plan with individual deductibles. The omitted variable is the free insurance plan with 0% coinsurance. The HIE was conducted in six cities; a categorical variable to control for the location was represented with five binary variables, Dayton, Fitchburg, Franklin, Charleston and Georgetown, with Seattle being the omitted variable. A categorical factor with \\(c=6\\) levels was used for age and sex; binary variables in the model consisted of “Age 0-2,” “Age 3-5,” “Age 6-17,” “Woman age 18-65,” and “Man age 46-65,” the omitted category was “Man age 18-45.” Other control variables included a health status scale, socioeconomic status, number of medical visits in the year prior to the experiment on a logarithmic scale and race. Table 4.4 summarizes the effects of the variables. As noted by Keeler and Rolph, there were large differences by site and age although the regression only served to summarize \\(R^2=11\\%\\) of the variability. For the cost-sharing variables, only “Co-ins95” was statistically significant, and this only at the 5% level, not the 1% level. The paper of Keeler and Rolph (1988) examines other types of episode expenditures, as well as the frequency of expenditures. They concluded that cost-sharing of health insurance plans has little effect on the amount of expenditures per episode although there are important differences in the frequency of episodes. This is because an episode of treatment is composed of two decisions. The amount of treatment is made jointly between the patient and the physician and is largely unaffected by the type of health insurance plan. The decision to seek health care treatment is made by the patient; this decision-making process is more susceptible to economic incentives in cost-sharing aspects of health insurance plans. Table 4.4: Coefficients of Episode Expenditures from the Rand HIE Variable Regression Coefficient Variable Regression Coefficient Intercept 7.95 Dayton 0.13* Co-ins25 0.07 Fitchburg 0.12 Co-ins50 0.02 Franklin -0.01 Co-ins95 -0.13* Charleston 0.20* Indiv Deductible -0.03 Georgetown -0.18* Health scale -0.02* Age 0-2 -0.63** Socioeconomic status 0.03 Age 3-5 -0.64** Medical visits -0.03 Age 6-17 -0.30** Examination -0.10* Woman age 18-65 0.11 Black 0.14* Man age 46-65 0.26 Note: * significant at 5%, ** significant at 1% Source: Keeler and Rolph (1988) 4.2 Statistical Inference for Several Coefficients It can be useful to examine several regression coefficients at the same time. For example, when assessing the effect of a categorical variable with \\(c\\) levels, we need to say something jointly about the \\(c-1\\) binary variables that enter the regression equation. To do this, Section 4.2.1 introduces a method for handling linear combinations of regression coefficients. Section 4.2.2 shows how to test several linear combinations, and Section 4.2.3 presents other inference applications. 4.2.1 Sets of Regression Coefficients Recall that our regression coefficients are specified by \\(\\boldsymbol{\\beta} = \\left( \\beta_0, \\beta_1, \\ldots, \\beta_k \\right)^{\\prime},\\) a \\((k+1) \\times 1\\) vector. It will be convenient to express linear combinations of the regression coefficients using the notation \\(\\mathbf{C} \\boldsymbol{\\beta},\\) where \\(\\mathbf{C}\\) is a \\(p \\times (k+1)\\) matrix that is user-specified and depends on the application. Some applications involve estimating \\(\\mathbf{C} \\boldsymbol{\\beta}\\). Others involve testing whether \\(\\mathbf{C} \\boldsymbol{\\beta}\\) equals a specific known value (denoted as \\(\\mathbf{d}\\)). We call \\(H_0:\\mathbf{C \\boldsymbol{\\beta} = d}\\) the general linear hypothesis. To demonstrate the broad variety of applications in which sets of regression coefficients can be used, we now present a series of special cases. Special Case 1: One Regression Coefficient. In Section 3.4, we investigated the importance of a single coefficient, say \\(\\beta_j.\\) We may express this coefficient as \\(\\mathbf{C} \\boldsymbol{\\beta}\\) by choosing \\(p=1\\) and \\(\\mathbf{C}\\) to be a \\(1 \\times (k+1)\\) vector with a one in the \\((j+1)\\)st column and zeros otherwise. These choices result in \\[ \\mathbf{C \\boldsymbol{\\beta} =} \\left( 0~\\ldots~0~1~0~\\ldots~0\\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) = \\beta_j. \\] Special Case 2: Regression Function. Here, we choose \\(p=1\\) and \\(\\mathbf{C}\\) to be a \\(1 \\times (k+1)\\) vector representing the transpose of a set of explanatory variables. These choices result in \\[ \\mathbf{C \\boldsymbol{\\beta} =} \\left( x_0, x_1, \\ldots, x_k \\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) = \\beta_0 x_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k = \\mathrm{E}~y, \\] the regression function. Special Case 3: Linear Combination of Regression Coefficients. When \\(p=1\\), we use the convention that lower-case bold letters are vectors and let \\(\\mathbf{C = c^{\\prime}} = \\left( c_0, \\ldots, c_k \\right)^{\\prime}\\). In this case, \\(\\mathbf{C} \\boldsymbol{\\beta}\\) is a generic linear combination of regression coefficients \\[ \\mathbf{C \\boldsymbol{\\beta} = \\mathbf{c}^{\\prime} \\boldsymbol{\\beta} = c_0 \\beta_0 + \\ldots + c_k \\beta_k}. \\] Special Case 4: Testing Equality of Regression Coefficients. Suppose that the interest is in testing \\(H_0: \\beta_1 = \\beta_2.\\) For this purpose, let \\(p=1\\), \\(\\mathbf{c}^{\\prime} = \\left( 0, 1, -1, 0, \\ldots, 0\\right),\\) and \\(\\mathbf{d} = 0\\). With these choices, we have \\[ \\mathbf{C \\boldsymbol{\\beta} = c^{\\prime} \\boldsymbol{\\beta} =} \\left( 0, 1, -1, 0, \\ldots, 0\\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) = \\beta_1 - \\beta_2 = 0, \\] so that the general linear hypothesis reduces to \\(H_0: \\beta_1 = \\beta_2.\\) Special Case 5: Adequacy of the Model. It is customary in regression analysis to present a test of whether or not any of the explanatory variables are useful for explaining the response. Formally, this is a test of the null hypothesis \\(H_0:\\beta_1=\\beta_2=\\ldots=\\beta_k=0\\). Note that, as a convention, one does not test whether or not the intercept is zero. To test this using the general linear hypothesis, we choose \\(p=k\\), \\(\\mathbf{d}=\\left( 0~\\ldots~0\\right)^{\\prime}\\) to be a \\(k \\times 1\\) vector of zeros and \\(\\mathbf{C}\\) to be a \\(k \\times (k+1)\\) matrix such that \\[ \\small{ \\mathbf{C \\boldsymbol{\\beta} =}\\left( \\begin{array}{ccccc} 0 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{array} \\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) =\\left( \\begin{array}{c} \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right) =\\left( \\begin{array}{c} 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right) =\\mathbf{d}. } \\] Special Case 6: Testing Portions of the Model. Suppose that we are interested in comparing a full regression function \\[ \\mathrm{E~}y = \\beta_0 + \\beta_1 x_1 +\\ldots + \\beta_k x_k + \\beta_{k+1} x_{k+1} + \\ldots + \\beta_{k+p} x_{k+p} \\] to a reduced regression function, \\[ \\mathrm{E~}y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k. \\] Beginning with the full regression, we see that if the null hypothesis \\(H_0:\\beta_{k+1} = \\ldots = \\beta_{k+p} = 0\\) holds, then we arrive at the reduced regression. To illustrate, the variables \\(x_{k+1}, \\ldots, x_{k+p}\\) may refer to several binary variables representing a categorical variable and our interest is in whether the categorical variable is important. To test the importance of the categorical variable, we want to see whether the binary variables \\(x_{k+1}, \\ldots, x_{k+p}\\) jointly affect the dependent variables. To test this using the general linear hypothesis, we choose \\(\\mathbf{d}\\) and \\(\\mathbf{C}\\) such that \\[ \\small{ \\mathbf{C \\boldsymbol{\\beta} =}\\left( \\begin{array}{ccccccc} 0 &amp; \\cdots &amp; 0 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{array} \\right) \\left( \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\\\ \\beta_{k+1} \\\\ \\vdots \\\\ \\beta_{k+p} \\end{array} \\right) =\\left( \\begin{array}{c} \\beta_{k+1} \\\\ \\vdots \\\\ \\beta_{k+p} \\end{array} \\right) =\\left( \\begin{array}{c} 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right) =\\mathbf{d}. } \\] From a list of \\(k+p\\) variables \\(x_1, \\ldots, x_{k+p}\\), you may drop any \\(p\\) that you deem appropriate. The additional variables do not need to be the last \\(p\\) in the regression specification. Dropping \\(x_{k+1}, \\ldots, x_{k+p}\\) is for notational convenience only. 4.2.2 The General Linear Hypothesis To recap, the general linear hypothesis can be stated as \\(H_0:\\mathbf{C \\boldsymbol{\\beta} =d}\\). Here, \\(\\mathbf{C}\\) is a \\(p \\times (k+1)\\) matrix, \\(\\mathbf{d}\\) is a \\(p \\times 1\\) vector, and both \\(\\mathbf{C}\\) and \\(\\mathbf{d}\\) are user-specified and depend on the application at hand. Although \\(k+1\\) is the number of regression coefficients, \\(p\\) is the number of restrictions under \\(H_0\\) on these coefficients. (For those readers with knowledge of advanced matrix algebra, \\(p\\) is the rank of \\(\\mathbf{C}\\).) This null hypothesis is tested against the alternative \\(H_a:\\mathbf{C \\boldsymbol{\\beta} \\neq d}\\). This may be obvious, but we do require \\(p \\leq k+1\\) because we cannot test more constraints than free parameters. To understand the basis for the testing procedure, we first recall some of the basic properties of the regression coefficient estimators described in Section 3.3. Now, however, our goal is to understand properties of the linear combinations of regression coefficients specified by \\(\\mathbf{C \\boldsymbol{\\beta}}\\). A natural estimator of this quantity is \\(\\mathbf{Cb}\\). It is easy to see that \\(\\mathbf{Cb}\\) is an unbiased estimator of \\(\\mathbf{C \\boldsymbol{\\beta}}\\), because \\(\\mathrm{E~}\\mathbf{Cb=C}\\mathrm{E~}\\mathbf{b=C \\boldsymbol{\\beta}}\\). Moreover, the variance is \\(\\mathrm{Var}\\left( \\mathbf{Cb}\\right) \\mathbf{=C}\\mathrm{Var}\\left( \\mathbf{b}\\right) \\mathbf{C}^{\\prime}=\\sigma^2 \\mathbf{C}\\left( \\mathbf{X^{\\prime}X}\\right)^{-1} \\mathbf{C}^{\\prime}\\). To assess the difference between \\(\\mathbf{d}\\), the hypothesized value of \\(\\mathbf{C \\boldsymbol{\\beta}}\\), and its estimated value, \\(\\mathbf{Cb}\\), we use the following statistic: \\[ F-\\text{ratio}=\\frac{(\\mathbf{Cb-d)}^{\\prime}\\left( \\mathbf{C}\\left( \\mathbf{X^{\\prime}X} \\right)^{-1} \\mathbf{C}^{\\prime}\\right)^{-1}(\\mathbf{Cb-d)}}{ps_{full}^2}. \\tag{4.1} \\] Here, \\(s_{full}^2\\) is the mean square error from the full regression model. Using the theory of linear models, it can be checked that the statistic \\(F\\)-ratio has an \\(F\\)-distribution with numerator degrees of freedom \\(df_1=p\\) and denominator degrees of freedom \\(df_2=n-(k+1)\\). Both the statistic and the theoretical distribution are named for R. A. Fisher, a renowned scientist and statistician who did much to advance statistics as a science in the early half of the twentieth century. Like the normal and the \\(t\\)-distribution, the \\(F\\)-distribution is a continuous distribution. The \\(F\\)-distribution is the sampling distribution for the \\(F\\)-ratio and is proportional to the ratio of two sums of squares, each of which is positive or zero. Thus, unlike the normal distribution and the \\(t\\)-distribution, the \\(F\\)-distribution takes on only nonnegative values. Recall that the \\(t\\)-distribution is indexed by a single degree of freedom parameter. The \\(F\\)-distribution is indexed by two degrees of freedom parameters: one for the numerator, \\(df_1\\), and one for the denominator, \\(df_2\\). Appendix A3.4 provides additional details. The test statistic in equation (4.1) is complex in form. Fortunately, there is an alternative that is simpler to implement and to interpret; this alternative is based on the extra sum of squares principle. Procedure for Testing the General Linear Hypothesis Run the full regression and get the error sum of squares and mean square error, which we label as \\((Error~SS)_{full}\\) and \\(s_{full}^2\\), respectively. Consider the model assuming the null hypothesis is true. Run a regression with this model and get the error sum of squares, which we label \\((Error~SS)_{reduced}\\). Calculate \\[ F-\\text{ratio}=\\frac{(Error~SS)_{reduced}-(Error~SS)_{full}}{ps_{full}^2}. \\tag{4.2} \\] Reject the null hypothesis in favor of the alternative if the \\(F\\)-ratio exceeds an \\(F\\)-value. The \\(F\\)-value is a percentile from the \\(F\\)-distribution with \\(df_1=p\\) and \\(df_2=n-(k+1)\\) degrees of freedom. The percentile is one minus the significance level of the test. Following our notation with the \\(t\\)-distribution, we denote this percentile as \\(F_{p,n-(k+1),1-\\alpha}\\), where \\(\\alpha\\) is the significance level. This procedure is commonly known as an \\(F\\)-test. Section 4.7.2 provides the mathematical underpinnings. To understand the extra-sum-of-squares principle, recall that the error sum of squares for the full model is determined to be the minimum value of \\[ SS(b_0^{\\ast}, \\ldots, b_k^{\\ast}) = \\sum_{i=1}^{n} \\left( y_i - \\left( b_0^{\\ast} + \\ldots + b_k^{\\ast} x_{i,k} \\right) \\right)^2. \\] Here, \\(SS(b_0^{\\ast}, \\ldots, b_k^{\\ast})\\) is a function of \\(b_0^{\\ast}, \\ldots, b_k^{\\ast}\\) and \\((Error~SS)_{full}\\) is the minimum over all possible values of \\(b_0^{\\ast}, \\ldots, b_k^{\\ast}\\). Similarly, \\((Error~SS)_{reduced}\\) is the minimum error sum of squares under the constraints in the null hypothesis. Because there are fewer possibilities under the null hypothesis, we have that \\[ (Error~SS)_{full} \\leq (Error~SS)_{reduced}. \\tag{4.3} \\] To illustrate, consider our first special case where \\(H_0 : \\beta_j = 0\\). In this case, the difference between the full and reduced models amounts to dropping a variable. A consequence of equation (4.3) is that, when adding variables to a regression model, the error sum of squares never goes up (and, in fact, usually goes down). Thus, adding variables to a regression model increases \\(R^2\\), the coefficient of determination. How large a decrease in the error sum of squares is statistically significant? Intuitively, one can view the \\(F\\)-ratio as the difference in the error sum of squares divided by the number of constraints, \\(\\frac{(Error~SS)_{reduced}-(Error~SS)_{full}}{p}\\), and then rescaled by the best estimate of the variance term, the \\(s^2\\), from the full model. Under the null hypothesis, this statistic follows an \\(F\\)-distribution and we may compare the test statistic to this distribution to see if it is unusually large. Using the relationship \\(Regression~SS = Total~SS - Error~SS\\), we can re-express the difference in the error sum of squares as \\[ (Error~SS)_{reduced} - (Error~SS)_{full} = (Regression~SS)_{full} - (Regression~SS)_{reduced}. \\] This difference is known as a Type III Sum of Squares. When testing the importance of a set of explanatory variables, \\(x_{k+1}, \\ldots, x_{k+p}\\), in the presence of \\(x_1, \\ldots, x_k\\), you will find that many statistical software packages compute this quantity directly in a single regression run. The advantage of this is it allows the analyst to perform an \\(F\\)-test using a single regression run, instead of two regression runs as in our four-step procedure described above. Example: Term Life Insurance - Continued. Before discussing the logic and the implications of the \\(F\\)-test, let us illustrate its use. In the Term Life Insurance example, suppose that we wish to understand the impact of marital status. Table 4.3 presented a mixed message in terms of \\(t\\)-ratios; sometimes they were statistically significant and sometimes not. It would be helpful to have a formal test to give a definitive answer, at least in terms of statistical significance. Specifically, we consider a regression model using LNINCOME, EDUCATION, NUMHH, MAR0, and MAR2 as explanatory variables. The model equation is \\[ \\small{ \\begin{array}{ll} y &amp;= \\beta_0 + \\beta_1 \\text{LNINCOME} + \\beta_2 \\text{EDUCATION} + \\beta_3 \\text{NUMHH} \\\\ &amp; \\ \\ \\ \\ + \\beta_4 \\text{MAR0} + \\beta_5 \\text{MAR2}. \\end{array} } \\] Our goal is to test \\(H_0: \\beta_4 = \\beta_5 = 0\\). We begin by running a regression model with all \\(k+p=5\\) variables. The results were reported in Table 4.2, where we saw that \\((Error~SS)_{full} = 615.62\\) and \\(s_{full}^2 = (1.513)^2 = 2.289\\). The next step is to run the reduced model without MAR0 and MAR2. This was done in Table 3.3 of Chapter 3, where we saw that \\((Error~SS)_{reduced} = 630.43\\). We then calculate the test statistic \\[ F-\\text{ratio} = \\frac{(Error~SS)_{reduced} - (Error~SS)_{full}}{ps_{full}^2} = \\frac{630.43 - 615.62}{2 \\times 2.289} = 3.235. \\] The fourth step compares the test statistic to an \\(F\\)-distribution with \\(df_1=p=2\\) and \\(df_2 = n-(k+p+1) = 269\\) degrees of freedom. Using a 5% level of significance, it turns out that the 95th percentile is \\(F-\\text{value} \\approx 3.029\\). The corresponding \\(p\\)-value is \\(\\Pr(F &gt; 3.235) = 0.0409\\). At the 5% significance level, we reject the null hypothesis \\(H_0: \\beta_4 = \\beta_5 = 0\\). This suggests that it is important to use marital status to understand term life insurance coverage, even in the presence of income, education, and number of household members. Some Special Cases The general linear hypothesis test is available when you can express one model as a subset of another. For this reason, it is useful to think of it as a device for comparing “smaller” to “larger” models. However, the smaller model must be a subset of the larger model. For example, the general linear hypothesis test cannot be used to compare the regression functions \\(\\mathrm{E~}y = \\beta_0 + \\beta_7 x_7\\) versus \\(\\mathrm{E~}y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4\\). This is because the former, smaller function is not a subset of the latter, larger function. The general linear hypothesis can be used in many instances, although its use is not always necessary. For example, suppose that we wish to test \\(H_0:\\beta_k=0\\). We have already seen that this null hypothesis can be examined using the \\(t\\)-ratio test. In this special case, it turns out that \\((t-\\textrm{ratio})^2=F-\\textrm{ratio}\\). Thus, these tests are equivalent for testing \\(H_0:\\beta_k=0\\) versus \\(H_a:\\beta_k \\neq 0\\). The \\(F\\)-test has the advantage that it works for more than one predictor whereas the \\(t\\)-test has the advantage that one can consider one-sided alternatives. Thus, both tests are considered useful. Dividing the numerator and denominator of equation (4.4) by \\(Total~SS\\), the test statistic can also be written as: \\[ F-\\textrm{ratio}=\\frac{\\left( R_{full}^2-R_{reduced}^2\\right) /p}{\\left( 1-R_{full}^2\\right) / (n-(k+1))}. \\tag{4.4} \\] The interpretation of this expression is that the \\(F\\)-ratio measures the drop in the coefficient of determination, \\(R^2\\). The expression in equation (4.4) is particularly useful for testing the adequacy of the model, our Special Case 5. In this case, \\(p=k\\), and the regression sum of squares under the reduced model is zero. Thus, we have \\[ \\small{ F-\\textrm{ratio}=\\frac{\\left( (Regression~SS)_{full}\\right) /k}{s_{full}^2} =\\frac{(Regression~MS)_{full}}{(Error~SS)_{full}}. } \\] This test statistic is a regular feature of the ANOVA table for many statistical packages. For example, in our Term Life Insurance example, testing the adequacy of the model means evaluating \\(H_0: \\beta_1 = \\beta_2 =\\beta_3 =\\beta_4 =\\beta_5 = 0\\). From Table 4.2, the \\(F\\)-ratio is 68.66 / 2.29 = 29.98. With \\(df_1=5\\) and \\(df_2 = 269\\), we have that the \\(F\\)-value is approximately 2.248 and the corresponding \\(p\\)-value is \\(\\Pr(F &gt; 29.98) \\approx 0\\). This leads us to reject strongly the notion that the explanatory variables are not useful in understanding term life insurance coverage, reaffirming what we learned in the graphical and correlation analysis. Any other result would be surprising. For another expression, dividing by \\(Total~SS\\), we may write \\[ F-\\textrm{ratio}=\\frac{R^2}{1-R^2}\\frac{n-(k+1)}{k}. \\] Because both \\(F\\)-ratio and \\(R^2\\) are measures of model fit, it seems intuitively plausible that they be related in some fashion. A consequence of this relationship is the fact that as \\(R^2\\) increases, so does the \\(F\\)-ratio and vice versa. The \\(F\\)-ratio is used because its sampling distribution is known under a null hypothesis so we can make statements about statistical significance. The \\(R^2\\) measure is used because of the easy interpretations associated with it. 4.2.3 Estimating and Predicting Several Coefficients Estimating Linear Combinations of Regression Coefficients In some applications, the main interest is to estimate a linear combination of regression coefficients. To illustrate, recall in Section 3.5 that we developed a regression function for an individual’s charitable contributions (\\(y\\)) in terms of their wages (\\(x\\)). In this function, there was an abrupt change in the function at \\(x=97,500\\). To model this, we defined the binary variable \\(z\\) to be zero if \\(x&lt;97,500\\) and to be one if \\(x \\geq 97,500\\), and the regression function \\(\\mathrm{E~}y = \\beta_0 + \\beta_1 x + \\beta_2 z(x - 97,500)\\). Thus, the marginal expected change in contributions per dollar wage change for wages in excess of \\(97,500\\) is \\(\\frac{\\partial \\left( \\mathrm{E~}y\\right)}{\\partial x} = \\beta_1 + \\beta_2\\). To estimate \\(\\beta_1 + \\beta_2\\), a reasonable estimator is \\(b_1 + b_2\\) which is readily available from standard regression software. In addition, we would also like to compute standard errors for \\(b_1 + b_2\\) to be used, for example, in determining a confidence interval for \\(\\beta_1 + \\beta_2\\). However, \\(b_1\\) and \\(b_2\\) are typically correlated so that the calculation of the standard error of \\(b_1 + b_2\\) requires estimation of the covariance between \\(b_1\\) and \\(b_2\\). Estimating \\(\\beta_1 + \\beta_2\\) is an example of our Special Case 3 that considers linear combinations of regression coefficients of the form \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta = c_0 \\beta_0 + c_1 \\beta_1 + \\ldots + c_k \\beta_k\\). For our charitable contribution’s example, we would choose \\(c_1 = c_2 = 1\\) and other \\(c\\)’s equal to zero. To estimate \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta\\), we replace the vector of parameters by the vector of estimators and use \\(\\mathbf{c}^{\\prime} \\mathbf{b}\\). To assess the reliability of this estimator, as in Section 4.2.2, we have that \\(\\mathrm{Var}\\left( \\mathbf{c}^{\\prime} \\mathbf{b}\\right) = \\sigma^2 \\mathbf{c}^{\\prime}(\\mathbf{X^{\\prime} X})^{-1} \\mathbf{c}\\). Thus, we may define the estimated standard deviation, or standard error, of \\(\\mathbf{c}^{\\prime} \\mathbf{b}\\) to be \\[ se\\left( \\mathbf{c}^{\\prime} \\mathbf{b} \\right) = s \\sqrt{\\mathbf{c}^{\\prime} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{c}}. \\] With this quantity, a \\(100(1 - \\alpha) \\%\\) confidence interval for \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta\\) is \\[ \\mathbf{c}^{\\prime} \\mathbf{b} \\pm t_{n - (k + 1), 1 - \\alpha / 2} ~ se(\\mathbf{c}^{\\prime} \\mathbf{b}). \\tag{4.5} \\] The confidence interval in equation (4.5) is valid under Assumptions F1-F5. If we choose \\(\\mathbf{c}\\) to have a “1” in the \\((j + 1)^{\\text{st}}\\) row and zeros otherwise, then \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta = \\beta_j\\), \\(\\mathbf{c}^{\\prime} \\mathbf{b} = b_j\\), and \\[ se(b_j) = s \\sqrt{(j + 1)^{\\text{st}}~ \\textit{diagonal element of } (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1}}. \\] Thus, equation (4.5) provides a theoretical basis for the individual regression coefficient confidence intervals introduced in Section 3.4’s equation (3.10) and generalizes it to arbitrary linear combinations of regression coefficients. Another important application of equation (4.5) is the choice of \\(\\mathbf{c}\\) corresponding to a set of explanatory variables of interest, say, \\(\\mathbf{x}_{\\ast} = \\left( 1, x_{\\ast 1}, x_{\\ast 2}, \\ldots, x_{\\ast k} \\right)^{\\prime}\\). These may correspond to an observation within the data set or to a point outside the available data. The parameter of interest, \\(\\mathbf{c}^{\\prime} \\boldsymbol \\beta = \\mathbf{x}_{\\ast}^{\\prime} \\boldsymbol \\beta\\), is the expected response or the regression function at that point. Then, \\(\\mathbf{x}_{\\ast}^{\\prime} \\mathbf{b}\\) provides a point estimator and equation (4.5) provides the corresponding confidence interval. Prediction Intervals Prediction is an inferential goal that is closely related to estimating the regression function at a point. Suppose that, when considering charitable contributions, we know an individual’s wages (and thus whether wages are in excess of \\(97,500\\)) and wish to predict the amount of charitable contributions. In general, we assume that the set of explanatory variables \\(\\mathbf{x}_{\\ast}\\) is known and wish to predict the corresponding response \\(y_{\\ast}\\). This new response follows the assumptions as described in Section 3.2. Specifically, the expected response is \\(\\mathrm{E~}y_{\\ast} = \\mathbf{x}_{\\ast}^{\\prime} \\boldsymbol \\beta\\), \\(\\mathbf{x}_{\\ast}\\) is nonstochastic, \\(\\mathrm{Var~}y_{\\ast} = \\sigma^2\\), \\(y_{\\ast}\\) is independent of \\(\\{y_1, \\ldots, y_{n}\\}\\) and is normally distributed. Under these assumptions, a \\(100(1 - \\alpha)\\%\\) prediction interval for \\(y_{\\ast}\\) is \\[ \\mathbf{x}_{\\ast}^{\\prime} \\mathbf{b} \\pm t_{n - (k + 1), 1 - \\alpha / 2} ~ s \\sqrt{1 + \\mathbf{x}_{\\ast}^{\\prime} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{x}_{\\ast}}. \\tag{4.6} \\] Equation (4.6) generalizes the prediction interval introduced in Section 2.4. 4.3 One Factor ANOVA Model Section 4.1 showed how to incorporate unordered categorical variables, or factors, into a linear regression model through the use of binary variables. Factors are important in social science research; they can be used to classify people by gender, ethnicity, marital status and so on, or classify firms by geographic region, organizational structure and so forth. Within studies of insurance, factors are used by insurers to categorize policyholders according to a “risk classification system.” Here, the idea is to create groups of policyholders with similar risk characteristics that will have similar claims experience. These groups form the basis of insurance pricing, so that each policyholder is charged an amount that is appropriate to their risk category. This process is sometimes known as “segmentation.” Although factors may be represented as binary variables in a linear regression model, we study one factor models as a separate unit because: The method of least squares is much simpler, obviating the need to take inverses of high-dimensional matrices. The resulting interpretations of coefficients are more straightforward. The one factor model is still a special case of the linear regression model. Hence, no additional statistical theory is needed to establish its statistical inference capabilities. To establish notation for the one factor ANOVA model, we now consider the following example. Example: Automobile Insurance Claims. We examine claims experience from a large midwestern (US) property and casualty insurer for private passenger automobile insurance. The dependent variable is the amount paid on a closed claim, in (US) dollars (claims that were not closed by year end are handled separately). Insurers categorize policyholders according to a risk classification system. This insurer’s risk classification system is based on: Automobile operator characteristics (age, gender, marital status and whether the primary or occasional driver of a car). Vehicle characteristics (city versus farm usage, if the vehicle is used to commute to school or work, used for business or pleasure, and if commuting, the approximate distance of the commute). These factors are summarized by the risk class categorical variable CLASS. Table 4.5 shows 18 risk classes - further classification information is not given here to protect proprietary interests of the insurer. Table 4.5 summarizes the results from \\(n=6,773\\) claims for drivers aged 50 and above. We can see that the median claim varies from a low of $707.40 (CLASS F7) to a high of 1,231.25 (CLASS C72). The distribution of claims turns out to be skewed, so we consider \\(y\\) = logarithmic claims. The table presents means, medians and standard deviations. Because the distribution of logarithmic claims is less skewed, means are close to medians. Figure 4.2 shows the distribution of logarithmic claims by risk class. Table 4.5: Automobile Claims Summary Statistics by Risk Class 1 2 3 4 5 6 Class C1 C11 C1A C1B C1C C2 Number 726 1151 77 424 38 61 Median (dollars) 948.86 1,013.81 925.48 1,026.73 1,001.73 851.20 Median (in log dollars) 6.855 6.921 6.830 6.934 6.909 6.747 Mean (in log dollars) 6.941 6.952 6.866 6.998 6.786 6.801 Std. dev. (in log dollars) 1.064 1.074 1.072 1.068 1.110 0.948 Class C6 C7 C71 C72 C7A C7B Number 911 913 1129 85 113 686 Median (dollars) 1,011.24 957.68 960.40 1,231.25 1,139.93 1,113.13 Median (in log dollars) 6.919 6.865 6.867 7.116 7.039 7.015 Mean (in log dollars) 6.926 6.901 6.954 7.183 7.064 7.072 Std. dev. (in log dollars) 1.115 1.058 1.038 0.988 1.021 1.103 Class C7C F1 F11 F6 F7 F71 Number 81 29 40 157 59 93 Median (dollars) 1,200.00 1,078.04 774.79 1,105.04 707.40 1,118.73 Median (in log dollars) 7.090 6.983 6.652 7.008 6.562 7.020 Mean (in log dollars) 7.244 7.004 6.804 6.910 6.577 6.935 Std. dev. (in log dollars) 0.944 0.996 1.212 1.193 0.897 0.983 Figure 4.2: Box Plots of Logarithmic Claims by Risk Class R Code to Produce Table 4.5 and Figure 4.2 AutoC &lt;- read.csv(&quot;CSVData/AutoClaims.csv&quot;, header=TRUE) # CREATE A TABLE OF MEANS AND STANDARD DEVIATIONS library(Hmisc) t1 &lt;- summarize(log(AutoC$PAID), AutoC$CLASS, length ) t2 &lt;- summarize(AutoC$PAID, AutoC$CLASS, median) t3 &lt;- summarize(log(AutoC$PAID), AutoC$CLASS, median) t4 &lt;- summarize(log(AutoC$PAID), AutoC$CLASS, mean) t5 &lt;- summarize(log(AutoC$PAID), AutoC$CLASS, sd) tablemat &lt;- cbind(t1, format(round(t2[2], digits = 2), big.mark = &#39;,&#39;), round(t3[2], digits = 3), round(t4[2], digits = 3), round(t5[2], digits = 3)) block1 &lt;- t(tablemat[1:6,]) block2 &lt;- t(tablemat[7:12,]) block3 &lt;- t(tablemat[13:18,]) #tableout[3,] &lt;- format(round(tableout[3,], digits=0), big.mark = &#39;,&#39;) bigblock &lt;- rbind(block1, block2, block3) temprow &lt;- c(&quot;Class&quot;, &quot;Number&quot;, &quot;Median (dollars)&quot;, &quot;Median (in log dollars)&quot; ,&quot;Mean (in log dollars)&quot;, &quot;Std. dev. (in log dollars)&quot;) bigblock1 &lt;- cbind (c(temprow,temprow, temprow), bigblock) row.names(bigblock1) &lt;- NULL TableGen1(TableData=bigblock1 , TextTitle=&#39;Automobile Claims Summary Statistics by Risk Class&#39;, Align=&#39;lrrrrrr&#39;, Digits=3, ColumnSpec=1:6, ColWidth = ColWidth4) %&gt;% kableExtra::column_spec(1, width = &quot;4cm&quot;) library(HH) AutoC &lt;- read.csv(&quot;CSVData/AutoClaims.csv&quot;, header=TRUE) # AUTO CLAIMS # FIGURE 4.2 par(cex=0.6) boxplot(log(PAID) ~ CLASS,cex=.6, cex.labels=2, data = AutoC, xlab = &quot;&quot;, ylab = &quot;&quot;) This section focuses on the risk class (CLASS) as the explanatory variable. We use the notation \\(y_{ij}\\) to mean the \\(i\\)th observation of the \\(j\\)th risk class. For the \\(j\\)th risk class, we assume there are \\(n_j\\) observations. There are \\(n=n_1+n_2+\\ldots +n_c\\) observations. The data are: \\[ \\begin{array}{cccccc} \\text{Data for risk class }1 &amp; \\ \\ \\ \\ &amp; y_{11} &amp; y_{21} &amp; \\ldots &amp; y_{n_1,1} \\\\ \\text{Data for risk class }2 &amp; &amp; y_{12} &amp; y_{22} &amp; \\ldots &amp; y_{n_2,1} \\\\ . &amp; &amp; . &amp; . &amp; \\ldots &amp; . \\\\ \\text{Data for risk class } c &amp; &amp; y_{1c} &amp; y_{2c} &amp; \\ldots &amp; y_{n_c,c} \\end{array} \\] where \\(c=18\\) is the number of levels of the CLASS factor. Because each level of a factor can be arranged in a single row (or column), another term for this type of data is a “one way classification.” Thus, a one way model is another term for a one factor model. An important summary measure of each level of the factor is the sample average. Let \\[ \\overline{y}_j=\\frac{1}{n_j}\\sum_{i=1}^{n_j}y_{ij} \\] denote the average from the \\(j\\)th CLASS. Model Assumptions and Analysis The one factor ANOVA model equation is \\[ y_{ij}=\\mu_j+ \\varepsilon_{ij}\\ \\ \\ \\ \\ \\ i=1,\\ldots ,n_j,\\ \\ \\ \\ \\ j=1,\\ldots ,c. \\tag{4.7} \\] As with regression models, the random deviations \\(\\{\\varepsilon_{ij} \\}\\) are assumed to be zero mean with constant variance (Assumption E3) and independent of one another (Assumption E4). Because we assume the expected value of each deviation is zero, we have \\(\\text{E}~y_{ij}=\\mu_j\\). Thus, we interpret \\(\\mu_j\\) to be the expected value of the response \\(y_{ij}\\), that is, the mean \\(\\mu\\) varies by the \\(j\\)th factor level. To estimate the parameters \\(\\{\\mu_j\\}\\), as with regression we use the method of least squares, introduced in Section 2.1. That is, let \\(\\mu^{\\ast}_j\\) be a “candidate” estimate of \\(\\mu_j\\). The quantity \\[ SS(\\mu^{\\ast}_1, \\ldots , \\mu^{\\ast}_{c}) = \\sum_{j=1}^{c} \\sum_{i=1}^{n_j} (y_{ij}-\\mu^{\\ast}_j)^2 \\] represents the sum of squared deviations of the responses from these candidate estimates. From straightforward algebra, the value of \\(\\mu^{\\ast}_j\\) that minimizes this sum of squares is \\(\\bar{y}_j\\). Thus, \\(\\bar{y}_j\\) is the least squares estimate of \\(\\mu_j\\). To understand the reliability of the estimates, we can partition the variability as in the regression case, presented in Sections 2.3.1 and 3.3. The minimum sum of squared deviations is called the error sum of squares and is defined to be \\[ Error ~SS = SS(\\bar{y}_1, \\ldots, \\bar{y}_{c}) = \\sum_{j=1}^{c} \\sum_{i=1}^{n_j} \\left(y_{ij}-\\bar{y}_j \\right)^2. \\] The total variation in the data set is summarized by the total sum of squares, \\[ Total ~SS=\\sum_{j=1}^{c}\\sum_{i=1}^{n_j}(y_{ij}-\\bar{y})^2. \\] The difference, called the factor sum of squares, can be expressed as: \\[ \\begin{array}{ll} Factor~ SS &amp; = Total ~SS - Error ~SS \\\\ &amp; = \\sum_{j=1}^{c}\\sum_{i=1}^{n_j}(y_{ij}-\\bar{y})^2-\\sum_{j=1}^{c}\\sum_{i=1}^{n_j}(y_{ij}-\\bar{y}_j)^2 = \\sum_{j=1}^{c}\\sum_{i=1}^{n_j}(\\bar{y}_j-\\bar{y})^2 \\\\ &amp; = \\sum_{j=1}^{c}n_j(\\bar{y}_j-\\bar{y})^2. \\end{array} \\] The last two equalities follow from algebra manipulation. The \\(Factor ~SS\\) plays the same role as the \\(Regression ~SS\\) in Chapters 2 and 3. The variability decomposition is summarized in Table 4.6. Table 4.6: ANOVA Table for One Factor Model Source Sum of Squares \\(df\\) Mean Square Factor \\(Factor ~SS\\) \\(c-1\\) \\(Factor ~MS\\) Error \\(Error ~SS\\) \\(n-c\\) \\(Error ~MS\\) Total \\(Total ~SS\\) \\(n-1\\) The conventions for this table are the same as in the regression case. That is, the mean square (MS) column is defined by the sum of squares (SS) column divided by the degrees of freedom (df) column. Thus, \\(Factor~MS \\equiv (Factor~SS)/(c-1)\\) and \\(Error~MS \\equiv (Error~SS)/(n-c)\\). We use \\[ s^2 = \\text{Error MS} = \\frac{1}{n-c} \\sum_{j=1}^{c}\\sum_{i=1}^{n_j} e_{ij}^2 \\] to be our estimate of \\(\\sigma^2\\), where \\(e_{ij} = y_{ij} - \\bar{y}_j\\) is the residual. With this value for \\(s\\), it can be shown that the interval estimate for \\(\\mu_j\\) is \\[ \\bar{y}_j \\pm t_{n-c,1-\\alpha /2}\\frac{s}{\\sqrt{n_j}}. \\tag{4.8} \\] Here, the t-value \\(t_{n-c,1-\\alpha /2}\\) is a percentile from the t-distribution with \\(df=n-c\\) degrees of freedom. Example: Automobile Claims - Continued. To illustrate, the ANOVA table summarizing the fit for the automobile claims data appears in Table 4.7. Here, we see that the mean square error is \\(s^2 = 1.14.\\) Table 4.7: ANOVA Table for Logarithmic Automobile Claims Source Sum of Squares \\(df\\) Mean Square CLASS 39.2 17 2.31 Error 7729.0 6755 1.14 Total 7768.2 6772 In automobile ratemaking, one uses the average claims to help set prices for insurance coverages. As an example, for CLASS C72 the average logarithmic claim is 7.183. From equation (4.8), a 95% confidence interval is \\[ 7.183 \\pm (1.96) \\frac{\\sqrt{1.14}}{\\sqrt{85}} = 7.183 \\pm 0.227 = (6.956 ,7.410). \\] Note that these estimates are in natural logarithmic units. In dollars, our point estimate is \\(e^{7.183} = 1,316.85\\) and our 95% confidence interval is \\((e^{6.956} , e^{7.410}) \\text{ or } (\\$1,049.43, \\$1,652.43)\\). An important feature of the one factor ANOVA decomposition and estimation is the ease of computation. Although the sum of squares appear complex, it is important to note that no matrix calculations are required. Rather, all of the calculations can be done through averages and sums of squares. This has been an important consideration historically, before the age of readily available desktop computing. Moreover, insurers may segment their portfolios into hundreds or even thousands of risk classes instead of the 18 used in our Automobile Claims data. Thus, even today it can be helpful to identify a categorical variable as a factor and let your statistical software use ANOVA estimation techniques. Further, ANOVA estimation also provides for direct interpretation of the results. Link with Regression This subsection shows how a one factor ANOVA model can be rewritten as a regression model. To this end, we have seen that both the regression model and one factor ANOVA model use a linear error structure with Assumptions E3 and E4 for identically and independently distributed errors. Similarly, both use the normality assumption E5 for selected inference results (such as confidence intervals). Both employ non-stochastic explanatory variables as in Assumption E2. Both have an additive (mean zero) error term, so the main apparent difference is in the expected response, \\(\\mathrm{E }~y\\). For the linear regression model, \\(\\mathrm{E }~y\\) is a linear combination of explanatory variables (Assumption F1). For the one factor ANOVA model, \\(E[y_j] = \\mu_j\\) is a mean that depends on the level of the factor. To equate these two approaches, for the ANOVA factor with \\(c\\) levels, we define \\(c\\) binary variables, \\(x_1, x_2, \\ldots, x_c\\). Here, \\(x_j\\) indicates whether or not an observation falls in the \\(j\\)th level. With these variables, we can rewrite our one factor ANOVA model as \\[ y = \\mu_1 x_1 + \\mu_2 x_2 + \\ldots + \\mu_c x_c + \\varepsilon. \\tag{4.9} \\] Thus, we have re-written the one factor ANOVA expected response as a regression function, although using a no-intercept form (as in equation (3.5)). The one factor ANOVA is a special case of our usual regression model, using binary variables from the factor as explanatory variables in the regression function. As we have seen, no matrix calculations are needed for least squares estimation. However, one can always use the matrix procedures developed in Chapter 3. Section 4.7.1 shows how our usual matrix expression for regression coefficients (\\(\\mathbf{b} = \\left(\\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{\\prime}\\mathbf{y}\\)) reduces to the simple estimates \\(\\bar{y}_j\\) when using only one categorical variable. Reparameterization To include an intercept term, define \\(\\tau_j = \\mu_j - \\mu\\), where \\(\\mu\\) is an, as yet, unspecified parameter. Because each observation must fall into one of the \\(c\\) categories, we have \\(x_1 + x_2 + \\ldots + x_{c} = 1\\) for each observation. Thus, using \\(\\mu_j = \\tau_j + \\mu\\) in equation (4.9), we have \\[ y = \\mu + \\tau_1 x_1 + \\tau_2 x_2 + \\ldots + \\tau_{c} x_{c} + \\varepsilon, \\tag{4.10} \\] we have re-written the model into what appears to be our usual regression format. We use the \\(\\tau\\) in lieu of \\(\\beta\\) for historical reasons. ANOVA models were invented by R.A. Fisher in connection with agricultural experiments. Here, the typical set-up is to apply several treatments to plots of land in order to quantify crop yield responses. Thus, the Greek “t”, \\(\\tau\\), suggests the word treatment, another term used to describe levels of the factor of interest. A simpler version of equation (4.10) \\[ y_{ij} = \\mu + \\tau_j + \\varepsilon_{ij}. \\] can be given when we identify the factor level. That is, if we know an observation falls in the \\(j\\)th level, then only \\(x_j\\) is one and the other \\(x\\)’s are 0. Thus, a simpler expression for equation (4.10) \\[ y_{ij} = \\mu + \\tau_j + \\varepsilon_{ij}. \\] Comparing equations (4.9) and (4.10), we see that the number of parameters has increased by one. That is, in equation (4.9) there are \\(c\\) parameters, \\(\\mu_1, \\ldots, \\mu_c\\), even though in equation (4.10) there are \\(c + 1\\) parameters, \\(\\mu\\) and \\(\\tau_1, \\ldots, \\tau_c\\). The model in equation (4.10) is said to be overparameterized. It is possible to estimate this model directly, using the general theory of linear models, summarized in Section 4.7.3. In this theory, regression coefficients need not be identifiable. Alternatively, one can make these two expressions equivalent by restricting the movement of the parameters in equation (4.10). We now present two ways of imposing restrictions. The first type of restriction, usually done in the regression context, is to require one of the \\(\\tau\\)’s to be zero. This amounts to dropping one of the explanatory variables. For example, we might use \\[ y = \\mu + \\tau_1 x_1 + \\tau_2 x_2 + \\ldots + \\tau_{c-1} x_{c-1} + \\varepsilon, \\tag{4.11} \\] dropping \\(x_c\\). With this formulation, it is easy to fit the model in equation (4.11) using regression statistical software routines because one only needs to run the regression with \\(c-1\\) explanatory variables. However, one needs to be careful with the interpretation of parameters. To equate the models in equations (4.9) and (4.10), we need to define \\(\\mu \\equiv \\mu_c\\) and \\(\\tau_j = \\mu_j - \\mu_c\\) for \\(j=1,2,\\ldots,c-1\\). That is, the regression intercept term is the mean level of the category dropped, and each regression coefficient is the difference between a mean level and the mean level dropped. It is not necessary to drop the last level \\(c\\), and indeed, one could drop any level. However, the interpretation of the parameters does depend on the variable dropped. With this restriction, the fitted values are \\(\\hat{\\mu} = \\hat{\\mu}_c = \\bar{y}_c\\) and \\(\\hat{\\tau}_j = \\hat{\\mu}_j - \\hat{\\mu}_c = \\bar{y}_j - \\bar{y}_c.\\) Recall that the caret (\\(\\hat{\\cdot}\\)), or “hat,” stands for an estimated, or fitted, value. The second type of restriction is to interpret \\(\\mu\\) as a mean for the entire population. To this end, the usual requirement is \\(\\mu \\equiv \\frac{1}{n} \\sum_{j=1}^c n_j \\mu_j,\\) that is, \\(\\mu\\) is a weighted average of means. With this definition, we interpret \\(\\tau_j = \\mu_j - \\mu\\) as treatment differences between a mean level and the population mean. Another way of expressing this restriction is \\(\\sum_{j=1}^{c} n_j \\tau_j = 0,\\) that is, the (weighted) sum of treatment differences is zero. The disadvantage of this restriction is that it is not readily implementable with a regression routine and a special routine is needed. The advantage is that there is a symmetry in the definitions of the parameters. There is no need to worry about which variable is being dropped from the equation, an important consideration. With this restriction, the fitted values are \\[ \\hat{\\mu} = \\frac{1}{n} \\sum_{j=1}^{c} n_j \\hat{\\mu}_j = \\frac{1}{n} \\sum_{j=1}^{c} n_j \\bar{y}_j = \\bar{y} \\] and \\[ \\hat{\\tau}_j = \\hat{\\mu}_j - \\hat{\\mu} = \\bar{y}_j - \\bar{y}. \\] 4.4 Combining Categorical and Continuous Explanatory Variables There are several ways to combine categorical and continuous explanatory variables. We initially present the case of only one categorical and one continuous variable. We then briefly present the general case, called the general linear model. When combining categorical and continuous variable models, we use the terminology factor for the categorical variable and covariate for the continuous variable. Combining a Factor and Covariate Let us begin with the simplest models that use a factor and a covariate. In Section 4.3, we introduced the one factor model \\(y_{ij} = \\mu_j + \\varepsilon_{ij}\\). In Chapter 2, we introduced basic linear regression in terms of one continuous variable, or covariate, using \\(y_{ij} = \\beta_0 + \\beta_1 x_{ij} + \\varepsilon_{ij}\\). Table 4.8 summarizes different approaches that could be used to represent combinations of a factor and covariate. Table 4.8: Several Models that Represent Combinations of One Factor and One Covariate Model Description Notation One factor ANOVA (no covariate model) \\(y_{ij} = \\mu_j + \\varepsilon_{ij}\\) Regression with constant intercept and slope (no factor model) \\(y_{ij} = \\beta_0 + \\beta_1 x_{ij} + \\varepsilon_{ij}\\) Regression with variable intercept and constant slope (analysis of covariance model) \\(y_{ij} = \\beta_{0j} + \\beta_1 x_{ij} + \\varepsilon_{ij}\\) Regression with constant intercept and variable slope \\(y_{ij} = \\beta_0 + \\beta_{1j} x_{ij} + \\varepsilon_{ij}\\) Regression with variable intercept and slope \\(y_{ij} = \\beta_{0j} + \\beta_{1j} x_{ij} + \\varepsilon_{ij}\\) We can interpret the regression with variable intercept and constant slope to be an additive model, because we are adding the factor effect, \\(\\beta_{0j}\\), to the covariate effect, \\(\\beta_1 x_{ij}\\). Note that one could also use the notation, \\(\\mu_j\\), in lieu of \\(\\beta_{0j}\\) to suggest the presence of a factor effect. This is also known as an analysis of covariance (ANCOVA) model. The regression with variable intercept and slope can be thought of as an interaction model. Here, both the intercept, \\(\\beta_{0j}\\), and slope, \\(\\beta_{1j}\\), may vary by level of the factor. In this sense, we interpret the factor and covariate to be “interacting.” The model with constant intercept and variable slope is typically not used in practice; it is included here for completeness. With this model, the factor and covariate interact only through the variable slope. Figures 4.3, 4.4, and 4.5 illustrate the expected responses of these models. Figure 4.3: Plot of the expected response versus the covariate for the regression model with variable intercept and constant slope. Figure 4.4: Plot of the expected response versus the covariate for the regression model with constant intercept and variable slope. Figure 4.5: Plot of the expected response versus the covariate for the regression model with variable intercept and variable slope. For each model presented in Table 4.8, parameter estimates can be calculated using the method of least squares. As usual, this means writing the expected response, \\(E[y_{ij}]\\), as a function of known variables and unknown parameters. For the regression model with variable intercept and constant slope, the least squares estimates can be expressed compactly as: \\[ b_1 = \\frac{\\sum_{j=1}^{c}\\sum_{i=1}^{n_j} (x_{ij} - \\bar{x}_j) (y_{ij} - \\bar{y}_j)}{\\sum_{j=1}^{c}\\sum_{i=1}^{n_j} (x_{ij} - \\bar{x}_j)^2} \\] and \\(b_{0j} = \\bar{y}_j - b_1 \\bar{x}_j\\). Similarly, the least squares estimates for the regression model with variable intercept and slope can be expressed as: \\[ b_{1j} = \\frac{\\sum_{i=1}^{n_j} (x_{ij} - \\bar{x}_j) (y_{ij} - \\bar{y}_j)}{\\sum_{i=1}^{n_j} (x_{ij} - \\bar{x}_j)^2} \\] and \\(b_{0j} = \\bar{y}_j - b_{1j} \\bar{x}_j\\). With these parameter estimates, fitted values may be calculated. For each model, fitted values are defined to be the expected response with the unknown parameters replaced by their least squares estimates. For example, for the regression model with variable intercept and constant slope the fitted values are \\(\\hat{y}_{ij} = b_{0j} + b_1 x_{ij}.\\) Example: Wisconsin Hospital Costs. We now study the impact of various predictors on hospital charges in the state of Wisconsin. Identifying predictors of hospital charges can provide direction for hospitals, government, insurers, and consumers in controlling these variables, which in turn leads to better control of hospital costs. The data for the year 1989 were obtained from the Office of Health Care Information, Wisconsin’s Department of Health and Human Services. Cross-sectional data are used, which detail the 20 diagnosis-related group (DRG) discharge costs for hospitals in the state of Wisconsin, broken down into nine major health service areas and three types of payer (Fee for service, HMO, and other). Even though there are 540 potential DRG, area, and payer combinations (\\(20 \\times 9 \\times 3 = 540\\)), only 526 combinations were actually realized in the 1989 data set. Other predictor variables included the logarithm of the total number of discharges (NO DSCHG) and total number of hospital beds (NUM BEDS) for each combination. The response variable is the logarithm of total hospital charges per number of discharges (CHGNUM). To streamline the presentation, we now consider only costs associated with three diagnostic related groups (DRGs): DRG #209, DRG #391, and DRG #430. The covariate, \\(x\\), is the natural logarithm of the number of discharges. In ideal settings, hospitals with more patients enjoy lower costs due to economies of scale. In non-ideal settings, hospitals may not have excess capacity and thus, hospitals with more patients have higher costs. One purpose of this analysis is to investigate the relationship between hospital costs and hospital utilization. Recall that our measure of hospital charges is the logarithm of costs per discharge (\\(y\\)). The scatter plot in Figure 4.6 gives a preliminary idea of the relationship between \\(y\\) and \\(x\\). We note that there appears to be a negative relationship between \\(y\\) and \\(x\\). The negative relationship between \\(y\\) and \\(x\\) suggested by Figure 4.6 is misleading and is induced by an omitted variable, the category of the cost (DRG). To see the joint effect of the categorical variable DRG and the continuous variable \\(x\\), in Figure 4.7 is a plot of \\(y\\) versus \\(x\\) where the plotting symbols are codes for the level of the categorical variable. From this plot, we see that the level of cost varies by level of the factor DRG. Moreover, for each level of DRG, the slope between \\(y\\) and \\(x\\) is either zero or positive. The slopes are not negative, as suggested by Figure 4.6. Figure 4.6: Plot of natural logarithm of cost per discharge versus natural logarithm of the number of discharges. This plot suggests a misleading negative relationship. Figure 4.7: Letter plot of natural logarithm of cost per discharge versus natural logarithm of the number of discharges by DRG. Here, A is for DRG #209, B is for DRG #391, and C is for DRG #430. Table 4.9: Wisconsin Hospital Cost Models Goodness of Fit Model Description Model degrees of freedom Error degrees of freedom Error Sum of Squares R-squared (%) Mean Square One factor ANOVA 2 76 9.396 93.3 0.124 Regression with constant intercept and slope 1 77 115.059 18.2 1.222 Regression with variable intercept and constant slope 3 75 7.482 94.7 0.100 Regression with constant intercept and variable slope 3 75 14.048 90.0 0.187 Regression with variable intercept and slope 5 73 5.458 96.1 0.075 Each of the five models defined in Table 4.8 was fit to this subset of the Hospital case study. The summary statistics are in Table 4.9. For this data set, there are \\(n = 79\\) observations and \\(c = 3\\) levels of the DRG factor. For each model, the model degrees of freedom is the number of model parameters minus one. The error degrees of freedom is the number of observations minus the number of model parameters. Using binary variables, each of the models in Table 4.8 can be written in a regression format. As we have seen in Section 4.2, when a model can be written as a subset of another, larger model, we have formal testing procedures available to decide which model is more appropriate. To illustrate this testing procedure with our DRG example, from Table 4.9 and the associated plots, it seems clear that the DRG factor is important. Further, a \\(t\\)-test, not presented here, shows that the covariate \\(x\\) is important. Thus, let’s compare the full model \\(E[y_{ij}] = \\beta_{0,j} + \\beta_{1,j}x\\) to the reduced model \\(E[y_{ij}] = \\beta_{0,j} + \\beta_1x\\). In other words, is there a different slope for each DRG? Using the notation from Section 4.2, we call the variable intercept and slope the full model. Under the null hypothesis, \\(H_0: \\beta_{1,1} = \\beta_{1,2} = \\beta_{1,3}\\), we get the variable intercept, constant slope model. Thus, using the \\(F\\)-ratio in equation (4.2), we have \\[ F\\text{-ratio} = \\frac{(Error~SS)_{reduced} - (Error~SS)_{full}}{ps_{full}^2} = \\frac{7.482 - 5.458}{2 \\times 0.075} = 13.535. \\] The 95th percentile from the \\(F\\)-distribution with \\(df_1 = p = 2\\) and \\(df_2 = (df)_{full} = 73\\) is approximately 3.13. Thus, this test leads us to reject the null hypothesis and declare the alternative, the regression model with variable intercept and variable slope, to be valid. Combining Two Factors We have seen how to combine covariates as well as a covariate and factor, both additively and with interactions. In the same fashion, suppose that we have two factors, say sex (two levels, male/female) and age (three levels, young/middle/old). Let the corresponding binary variables be \\(x_1\\) to indicate whether the observation represents a female, \\(x_2\\) to indicate whether the observation represents a young person, and \\(x_3\\) to indicate whether the observation represents a middle-aged person. An additive model for these two factors may use the regression function \\[ \\mathrm{E }~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3. \\] As we have seen, this model is simple to interpret. For example, we can interpret \\(\\beta_1\\) to be the sex effect, holding age constant. We can also incorporate two interaction terms, \\(x_1 x_2\\) and \\(x_1 x_3\\). Using all five explanatory variables yields the regression function \\[ \\mathrm{E }~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3. \\tag{4.12} \\] Here, the variables \\(x_1\\), \\(x_2\\), and \\(x_3\\) are known as the main effects. Table 4.10 helps interpret this equation. Specifically, there are six types of people that we could encounter: males and females who are young, middle-aged, or old. We have six parameters in equation (4.12). Table 4.10 provides the link between the parameters and the types of people. By using the interaction terms, we do not impose any prior specifications on the additive effects of each factor. From Table 4.10, we see that the interpretation of the regression coefficients in equation (4.12) is not straightforward. However, using the additive model with interaction terms is equivalent to creating a new categorical variable with six levels, one for each type of person. If the interaction terms are critical in your study, you may wish to create a new factor that incorporates the interaction terms simply for ease of interpretation. Table 4.10: Regression Function for a Two Factor Model with Interactions Sex Age \\(x_1\\) \\(x_2\\) \\(x_3\\) \\(x_4\\) \\(x_5\\) Regression Function Male Young 0 1 0 0 0 \\(\\beta_0 + \\beta_2\\) Male Middle 0 0 1 0 0 \\(\\beta_0 + \\beta_3\\) Male Old 0 0 0 0 0 \\(\\beta_0\\) Female Young 1 1 0 1 0 \\(\\beta_0 + \\beta_1 + \\beta_2 + \\beta_4\\) Female Middle 1 0 1 0 1 \\(\\beta_0 + \\beta_1 + \\beta_3 + \\beta_5\\) Female Old 1 0 0 0 0 \\(\\beta_0 + \\beta_1\\) Extensions to more than two factors follow in a similar fashion. For example, suppose that you are examining the behavior of firms with headquarters in ten geographic regions, two organizational structures (profit versus non-profit) with four years of data. If you decide to treat each variable as a factor and want to model all interaction terms, then this is equivalent to a factor with \\(10 \\times 2 \\times 4 = 80\\) levels. Models with interaction terms can have a substantial number of parameters and the analyst must be prudent when specifying interactions to be considered. General Linear Model The general linear model extends the linear regression model in two ways. First, explanatory variables may be continuous, categorical, or a combination. The only restriction is that they enter linearly such that the resulting regression function \\[ \\mathrm{E}~y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k \\tag{4.13} \\] is a linear combination of coefficients. As we have seen, we can square continuous variables or take other nonlinear transforms (such as logarithms) as well as use binary variables to represent categorical variables, so this “restriction,” as the name suggests, allows for a broad class of general functions to represent data. The second extension is that the explanatory variables may be linear combinations of one another in the general linear model. Because of this, in the general linear model case, the parameter estimates need not be unique. However, an important feature of the general linear model is that the resulting fitted values turn out to be unique, using the method of least squares. For example, in Section 4.3 we saw that the one factor ANOVA model could be expressed as a regression model with \\(c\\) indicator variables. However, if we had attempted to estimate the model in equation (4.10), the method of least squares would not have arrived at a unique set of regression coefficient estimates. The reason is that, in equation (4.10), each explanatory variable can be expressed as a linear combination of the others. For example, observe that \\(x_c = 1 - (x_1 + x_2 + \\ldots + x_{c-1})\\). The fact that parameter estimates are not unique is a drawback, but not an overwhelming one. The assumption that the explanatory variables are not linear combinations of one another means that we can compute unique estimates of the regression coefficients using the method of least squares. In terms of matrices, because the explanatory variables are not linear combinations of one another, the matrix \\(\\mathbf{X}^{\\prime}\\mathbf{X}\\) is not invertible. Specifically, suppose that we are considering the regression function in equation (4.13) and, using the method of least squares, our regression coefficient estimates are \\(b_0^{o}, b_1^{o}, \\ldots, b_k^{o}\\). This set of regression coefficient estimates minimizes our error sum of squares, but there may be other sets of coefficients that also minimize the error sum of squares. The fitted values are computed as \\(\\hat{y}_i = b_0^{o} + b_1^{o} x_{i1} + \\ldots + b_k^{o} x_{ik}\\). It can be shown that the resulting fitted values are unique, in the sense that any set of coefficients that minimize the error sum of squares produce the same fitted values (see Section 4.7.3). Thus, for a set of data and a specified general linear model, fitted values are unique. Because residuals are computed as observed responses minus fitted values, we have that the residuals are unique. Because residuals are unique, we have the error sums of squares are unique. Thus, it seems reasonable, and is true, that we can use the general test of hypotheses described in Section 4.2 to decide whether collections of explanatory variables are important. To summarize, for general linear models, parameter estimates may not be unique and thus not meaningful. An important part of regression models is the interpretation of regression coefficients. This interpretation is not necessarily available in the general linear model context. However, for general linear models, we may still discuss the importance of an individual variable or collection of variables through partial F-tests. Further, fitted values, and the corresponding exercise of prediction, works in the general linear model context. The advantage of the general linear model context is that we need not worry about the type of restrictions to impose on the parameters. Although not the subject of this text, this advantage is particularly important in complicated experimental designs used in the life sciences. The reader will find that general linear model estimation routines are widely available in statistical software packages available on the market today. 4.5 Further Reading and References There are several good linear model books that focus on categorical variables and analysis of variable techniques. Hocking (2003) and Searle (1987) are good examples. Chapter References Hocking, Ronald R. (2003). Methods and Applications of Linear Models: Regression and the Analysis of Variance John Wiley and Sons, New York. Keeler, Emmett B., and John E. Rolph (1988). The demand for episodes of treatment in the Health Insurance Experiment. Journal of Health Economics 7: 337-367. Searle, Shayle R. (1987). Linear Models for Unbalanced Data. John Wiley &amp; Sons, New York. 4.6 Exercises 4.1. In this exercise, we consider relating two statistics that summarize how well a regression model fits, the \\(F\\)-ratio and \\(R^2\\), the coefficient of determination. (Here, the \\(F\\)-ratio is the statistic used to test model adequacy, not a partial \\(F\\) statistic.) Write down \\(R^2\\) in terms of \\(Error ~SS\\) and \\(Regression ~SS\\). Write down \\(F\\)-ratio in terms of \\(Error ~SS\\), \\(Regression ~SS\\), \\(k\\), and \\(n\\). Establish the algebraic relationship \\[ F\\text{-ratio} = \\frac{R^2}{1-R^2} \\frac{n-(k+1)}{k}. \\] Suppose that \\(n = 40\\), \\(k = 5\\), and \\(R^2 = 0.20\\). Calculate the \\(F\\)-ratio. Perform the usual test of model adequacy to determine whether or not the five explanatory variables jointly significantly affect the response variable. Suppose that \\(n = 400\\) (not 40), \\(k = 5\\), and \\(R^2 = 0.20\\). Calculate the \\(F\\)-ratio. Perform the usual test of model adequacy to determine whether or not the five explanatory variables jointly significantly affect the response variable. 4.2. Hospital Costs. This exercise considers hospital expenditures data provided by the US Agency for Healthcare Research and Quality (AHRQ) and described in Exercise 1.4. Produce a scatter plot, correlation, and a linear regression of \\(LNTOTCHG\\) on \\(AGE\\). Is \\(AGE\\) a significant predictor of \\(LNTOTCHG\\)? You are concerned that newborns follow a different pattern than other ages. Create a binary variable that indicates whether or not \\(AGE\\) equals zero. Run a regression using this binary variable and \\(AGE\\) as explanatory variables. Is the binary variable statistically significant? Now examine the gender effect, using the binary variable \\(FEMALE\\) that is one if the patient is female and zero otherwise. Run a regression using \\(AGE\\) and \\(FEMALE\\) as explanatory variables. Run a second regression including these two variables with an interaction term. Comment on whether the gender effect is important in either model. Now consider the type of admission, \\(APRDRG\\), an acronym for “all patient refined diagnostic related group.” This is a categorical explanatory variable that provides information on the type of hospital admission. There are several hundred levels of this category. For example, level 640 represents admission for a normal newborn, with neonatal weight greater than or equal to 2.5 kilograms. As another example, level 225 represents admission resulting in an appendectomy. d(i). Run a one-factor ANOVA model, using \\(APRDRG\\) to predict \\(LNTOTCHG\\). Examine the \\(R^2\\) from this model and compare it to the coefficient of determination of the linear regression model of \\(LNTOTCHG\\) on \\(AGE\\). Based on this comparison, which model do you think is preferred? d(ii). For the one-factor model in part d(i), provide a 95% confidence interval for the mean of \\(LNTOTCHG\\) for level 225 corresponding to an appendectomy. Convert your final answer from logarithmic dollars to dollars via exponentiation. d(iii). Run a regression model of \\(APRDRG\\), \\(FEMALE\\), and \\(AGE\\) on \\(LNTOTCHG\\). State whether \\(AGE\\) is a statistically significant predictor of \\(LNTOTCHG\\). State whether \\(FEMALE\\) is a statistically significant predictor of \\(LNTOTCHG\\). 4.3. Nursing Home Utilization. This exercise considers nursing home data provided by the Wisconsin Department of Health and Family Services (DHFS) and described in Exercises 1.2, 2.10, and 2.20. In addition to the size variables, we also have information on several binary variables. The variable \\(URBAN\\) is used to indicate the facility’s location. It is one if the facility is located in an urban environment and zero otherwise. The variable \\(MCERT\\) indicates whether the facility is Medicare-certified. Most, but not all, nursing homes are certified to provide Medicare-funded care. There are three organizational structures for nursing homes. They are government (state, counties, municipalities), for-profit businesses, and tax-exempt organizations. Periodically, facilities may change ownership and, less frequently, ownership type. We create two binary variables \\(PRO\\) and \\(TAXEXEMPT\\) to denote for-profit business and tax-exempt organizations, respectively. Some nursing homes opt not to purchase private insurance coverage for their employees. Instead, these facilities directly provide insurance and pension benefits to their employees; this is referred to as “self funding of insurance.” We use binary variable \\(SELFFUNDINS\\) to denote it. You decide to examine the relationship between \\(LOGTPY(y)\\) and the explanatory variables. Use cost report year 2001 data, and do the following analysis. There are three levels of organizational structures, but we only use two binary variables (\\(PRO\\) and \\(TAXEXEMPT\\)). Explain why. Run a one-way analysis of variance using \\(TAXEXEMPT\\) as the factor. Decide whether or not tax-exempt is an important factor in determining \\(LOGTPY\\). State your null hypothesis, alternative hypothesis, and all components of the decision-making rule. Use a 5% level of significance. Run a one-way analysis of variance using \\(MCERT\\) as the factor. Decide whether or not \\(MCERT\\) is an important factor in determining \\(LOGTPY\\). c(i). Provide a point estimate of \\(LOGTPY\\) for a nursing facility that is not Medicare-Certified. c(ii). Provide a 95% confidence interval for your point estimate in part (i). Run a regression model using the binary variables, \\(URBAN\\), \\(PRO\\), \\(TAXEXEMPT\\), \\(SELFFUNDINS\\), and \\(MCERT\\). Find \\(R^2\\). Which variables are statistically significant? Run a regression model using all explanatory variables, \\(LOGNUMBED\\), \\(LOGSQRFOOT\\), \\(URBAN\\), \\(PRO\\), \\(TAXEXEMPT\\), \\(SELFFUNDINS\\), and \\(MCERT\\). Find \\(R^2\\). Which variables are statistically significant? e(i). Calculate the partial correlation between \\(LOGTPY\\) and \\(LOGSQRFOOT\\). Compare this to the correlation between \\(LOGTPY\\) and \\(LOGSQRFOOT\\). Explain why the partial correlation is small. e(ii). Compare the low level of the \\(t\\)-ratios (for testing the importance of individual regression coefficients) and the high level of the \\(F\\)-ratio (for testing model adequacy). Describe the seeming inconsistency and provide an explanation for this inconsistency. 4.4. Automobile Insurance Claims. Refer to Exercise 1.3. Run a regression of \\(LNPAID\\) on \\(AGE\\). Is \\(AGE\\) a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. Also comment on the goodness of fit of this variable. Consider using class as a single explanatory variable. Use the one factor to estimate the model and respond to the following questions. b(i). What is the point estimate of claims in class C7, drivers 50-69, driving to work or school, less than 30 miles per week with annual mileage under 7500, in natural logarithmic units? b(ii). Determine the corresponding 95% confidence interval of expected claims, in natural logarithmic units. b(iii). Convert the 95% confidence interval of expected claims that you determined in part b(ii) to dollars. Run a regression of \\(LNPAID\\) on \\(AGE\\), \\(GENDER\\), and the categorical variables \\(STATE\\ CODE\\) and \\(CLASS\\). c(i). Is \\(GENDER\\) a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. c(ii). Is \\(CLASS\\) a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. c(iii). Use the model to provide a point estimate of claims in dollars (not log dollars) for a male age 60 in STATE 2 in CLASS C7. c(iv). Write down the coefficient associated with CLASS C7 and interpret this coefficient. 4.5. Wisconsin Lottery Sales. This exercise considers State of Wisconsin lottery sales data that were described in Section 2.1 and examined in Exercise 3.4. Part 1: You decide to examine the relationship between SALES (\\(y\\)) and all eight explanatory variables (\\(PERPERHH\\), \\(MEDSCHYR\\), \\(MEDHVL\\), \\(PRCRENT\\), \\(PRC55P\\), \\(HHMEDAGE\\), \\(MEDINC\\), and \\(POP\\)). Fit a regression model of SALES on all eight explanatory variables. Find \\(R^2\\). b(i). Use it to calculate the correlation coefficient between the observed and fitted values. b(ii). You want to use \\(R^2\\) to test the adequacy of the model in part (a). Use a formal test of hypothesis. State your null and alternative hypothesis, decision-making criterion, and your decision-making rules. Test whether \\(POP\\), \\(MEDSCHYR\\), and \\(MEDHVL\\) are jointly important explanatory variables for understanding SALES. Part 2: After the preliminary analysis in Part 1, you decide to examine the relationship between SALES (\\(y\\)) and \\(POP\\), \\(MEDSCHYR\\), and \\(MEDHVL\\). Fit a regression model of SALES on these three explanatory variables. Has the coefficient of determination decreased from the eight-variable regression model to the three-variable model? Does this mean that the model is not improved or does it provide little information? Explain your response. To state formally whether one should use the three or eight variable model, use a partial \\(F\\)-test. State your null and alternative hypotheses, decision-making criterion, and your decision-making rules. 4.6. Insurance Company Expenses. This exercise considers insurance company data from the NAIC and described in Exercises 1.6 and 3.5. Are the quadratic terms important? Consider a linear model of \\(LNEXPENSES\\) on twelve explanatory variables. For the explanatory variables, include \\(ASSETS\\), \\(GROUP\\), both versions of losses and gross premiums, as well as the two BLS variables. Also include the square of each of the two loss and the two gross premium variables. Test whether the four squared terms are jointly statistically significant, using a partial \\(F\\)-test. State your null and alternative hypotheses, decision-making criterion, and your decision-making rules. Are the interaction terms with \\(GROUP\\) important? Omit the two BLS variables, so that now there are eleven variables: \\(ASSETS\\), \\(GROUP\\), both versions of losses and gross premiums, as well as interactions of \\(GROUP\\) with \\(ASSETS\\) and both versions of losses and gross premiums. Test whether the five interaction terms are jointly statistically significant, using a partial \\(F\\)-test. State your null and alternative hypotheses, decision-making criterion, and your decision-making rules. You are examining a company that is not in the sample with values \\(LONGLOSS = 0.025\\), \\(SHORTLOSS = 0.040\\), \\(GPWPERSONAL = 0.050\\), \\(GPWCOMM = 0.120\\), \\(ASSETS = 0.400\\), \\(CASH = 0.350\\), and \\(GROUP = 1\\). Use the eleven variable interaction model in part (b) to produce a 95% prediction interval for this company. 4.7. National Life Expectancies. We continue the analysis begun in Exercises 1.7, 2.22, and 3.6. Consider the regression using three explanatory variables, \\(FERTILITY\\), \\(PUBLICEDUCATION\\), and \\(\\ln{HEALTH}\\) that you did in Exercise 3.6. Test whether \\(PUBLICEDUCATION\\) and \\(\\ln{HEALTH}\\) are jointly statistically significant, using a partial \\(F\\)-test. State your null and alternative hypotheses, decision-making criterion, and your decision-making rules. (Hint: Use the coefficient of determination form for calculating the test statistic.) Provide an approximate \\(p\\)-value for the test. We now introduce the \\(REGION\\) variable, summarized in Table 4.11. A boxplot of life expectancies versus \\(REGION\\) is given in Figure 4.8. Describe what we learn from the Table and boxplot about the effect of \\(REGION\\) on \\(LIFEEXP\\). Figure 4.8: Boxplots of LIFEEXP by REGION Table 4.11 summarizes average life expectancy by region. REGION Region Description Number Mean 1 Arab States 13 71.9 2 East Asia and the Pacific 17 69.1 3 Latin American and the Carribean 25 72.8 4 South Asia 7 65.1 5 Southern Europe 3 67.4 6 Sub-Saharan Africa 38 52.2 7 Central and Eastern Europe 24 71.6 8 High Income OECD 23 79.6 All 150 67.4 Fit a regression model using only the factor, \\(REGION\\). Is \\(REGION\\) a statistically significant determinant of \\(LIFEEXP\\)? State your null and alternative hypotheses, decision-making criterion, and your decision-making rules. Fit a regression model using three explanatory variables, \\(FERTILITY\\), \\(PUBLICEDUCATION\\), and \\(\\ln{HEALTH}\\), as well as the categorical variable \\(REGION\\). d(i). You are examining a country not in the sample with values \\(FERTILITY = 2.0\\), \\(PUBLICEDUCATION = 5.0\\), and \\(\\ln{HEALTH} = 1.0\\). Produce two predicted life expectancy values by assuming that the country is from (1) an Arab state and (2) Sub-Saharan Africa. d(ii). Provide a 95% confidence interval for the difference in life expectancies between an Arab state and a country from Sub-Saharan Africa. d(iii). Provide the (usual ordinary least squares) point estimate for the difference in life expectancies between a country from Sub-Saharan Africa and a high-income OECD (Organization for Economic Co-operation and Development) country. 4.7 Technical Supplement - Matrix Expressions 4.7.1 Expressing Models with Categorical Variables in Matrix Form Chapter 3 showed how to write the regression model equation in the form \\(\\mathbf{y = X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\) where \\(\\mathbf{X}\\) is a matrix of explanatory variables. This form permits straightforward calculation of regression coefficients, \\(\\mathbf{b} = \\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}\\). This section shows how the model and calculations reduce to simpler expressions when the explanatory variables are categorical. One Categorical Variable Model. Consider the model with one categorical variable introduced in Section 4.3 with \\(c\\) levels of the categorical variable. From equation (4.9), this model can be written as \\[ \\mathbf{y} = \\begin{bmatrix} y_{1,1} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ y_{n_1,1} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ y_{1,c} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ y_{n_{c},c} \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ 1 &amp; 0 &amp; \\cdots &amp; \\cdots \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} \\mu_1 \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\mu_c \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_{1,1} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\varepsilon_{n_1,1} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\varepsilon_{1,c} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\varepsilon_{n_{c},c} \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\] To make the notation more compact, we write \\(\\mathbf{0}\\) and \\(\\mathbf{1}\\) for a column of zeros and ones, respectively. With this convention, another way to express this as \\[ \\mathbf{y} = \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdots &amp; \\mathbf{0}_1 \\\\ \\mathbf{0}_2 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0}_c &amp; \\mathbf{0}_c &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix} \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_c \\end{bmatrix} + \\boldsymbol{\\varepsilon} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\tag{4.14} \\] Here, \\(\\mathbf{0}_1\\) and \\(\\mathbf{1}_1\\) stand for vector columns of length \\(n_1\\) of zeros and ones, respectively, and similarly for \\(\\mathbf{0}_2, \\mathbf{1}_2, \\ldots, \\mathbf{0}_c, \\mathbf{1}_c\\). Equation (4.14) allows us to apply the machinery developed for the regression model to the model with one categorical variable. As an intermediate calculation, we have \\[ \\begin{array}{ll} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} &amp;= \\left( \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_2 &amp; \\cdots &amp; \\mathbf{0}_c \\\\ \\mathbf{0}_1 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_c \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0}_1 &amp; \\mathbf{0}_2 &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix}^{\\prime} \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdots &amp; \\mathbf{0}_1 \\\\ \\mathbf{0}_2 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0}_c &amp; \\mathbf{0}_c &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix} \\right)^{-1}\\\\ &amp;= \\begin{bmatrix} n_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; n_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; n_c \\end{bmatrix}^{-1} = \\begin{bmatrix} \\frac{1}{n_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\frac{1}{n_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\frac{1}{n_c} \\end{bmatrix} . \\tag{4.15} \\end{array} \\] Thus, the parameter estimates are \\[ \\mathbf{b} = \\begin{bmatrix} \\hat{\\mu}_1 \\\\ \\vdots \\\\ \\hat{\\mu}_c \\end{bmatrix} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{X}^{\\prime} \\mathbf{y} = \\begin{bmatrix} \\frac{1}{n_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\frac{1}{n_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\frac{1}{n_c} \\end{bmatrix} \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_2 &amp; \\cdots &amp; \\mathbf{0}_c \\\\ \\mathbf{0}_1 &amp; \\mathbf{1}_2 &amp; \\cdots &amp; \\mathbf{0}_c \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0}_1 &amp; \\mathbf{0}_2 &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix}^{\\prime} \\begin{bmatrix} y_{1,1} \\\\ \\vdots \\\\ y_{n_1,1} \\\\ \\vdots \\\\ y_{1,c} \\\\ \\vdots \\\\ y_{n_{c},c} \\end{bmatrix} \\] \\[ = \\begin{bmatrix} \\frac{1}{n_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\frac{1}{n_2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\frac{1}{n_c} \\end{bmatrix} \\begin{bmatrix} \\sum_{i=1}^{n_1} y_{i1} \\\\ \\vdots \\\\ \\sum_{i=1}^{n_c} y_{ic} \\end{bmatrix} = \\begin{bmatrix} \\bar{y}_1 \\\\ \\vdots \\\\ \\bar{y}_c \\end{bmatrix} \\tag{4.16} \\] We have seen that the least squares estimate of \\(\\mu_j\\), \\(\\bar{y}_j\\), can been obtained directly from equation (4.9). By rewriting the model in matrix regression notation, we can appeal to linear regression model results and need not prove properties of models with categorical variables from first principles. That is, because this model is in regression format, we immediately have all the properties of the regression model. Telling your software package that a variable is categorical can mean more efficient calculations, as in the calculations of the least squares regression estimates in equation (4.16). Further, calculation of other quantities can also be done more directly. As another example, we have that the standard error of \\(\\hat{\\mu}_j\\) is \\[ se(\\hat{\\mu}_j) = s ~ \\sqrt{j \\text{th} \\textit{ diagonal element of } \\mathbf{(X}^{\\prime}\\mathbf{X)}^{-1}} = s/\\sqrt{n_j}. \\] One Categorical and One Continuous Variable Model As another illustration, we consider the variable intercept and constant slope model. This model is summarized as \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta + \\boldsymbol \\varepsilon\\) where \\[ \\mathbf{X} = \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdot \\cdot \\cdot &amp; \\mathbf{0}_1 &amp; \\mathbf{x}_1 \\\\ \\mathbf{0}_2 &amp; \\mathbf{1}_2 &amp; \\cdot \\cdot \\cdot &amp; \\mathbf{0}_2 &amp; \\mathbf{x}_2 \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\cdot \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\cdot \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\cdot \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\mathbf{0}_{c} &amp; \\mathbf{0}_{c} &amp; \\cdot \\cdot \\cdot &amp; \\mathbf{1}_c &amp; \\mathbf{x}_{c} \\end{bmatrix} \\text{ and } \\boldsymbol \\beta = \\begin{bmatrix} \\beta_{01} \\\\ \\beta_{02} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\beta_{0c} \\\\ \\beta_1 \\end{bmatrix}. \\tag{4.17} \\] Here, \\(\\mathbf{0}_j\\) and \\(\\mathbf{1}_j\\) stand for vector columns of length \\(n_j\\) of zeros and ones, respectively, and \\(\\mathbf{x}_j = (x_{1j}, x_{2j}, \\ldots, x_{n_j, j})^{\\prime}\\) is the column of the continuous variable at the \\(j\\)th level. Straightforward matrix algebra techniques provide the least squares estimates. 4.7.2 Calculating Least Squares Recursively When computing regression coefficients using least squares, \\(\\mathbf{b} = \\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}\\), for some applications, the dimension of \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\) can be large, causing computational difficulties. Fortunately, for some problems, the computations can be partitioned into smaller problems that can be solved recursively. Recursive Least Squares Calculation. Suppose that the regression function can be written as \\[ \\mathrm{E}~\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta = \\left( \\mathbf{X}_1 : \\mathbf{X}_2 \\right) \\left( \\begin{array}{c} \\boldsymbol \\beta_1 \\\\ \\boldsymbol \\beta_2 \\\\ \\end{array} \\right), \\tag{4.18} \\] where \\(\\mathbf{X}_1\\) has dimensions \\(n \\times k_1\\), \\(\\mathbf{X}_2\\) has dimensions \\(n \\times k_2\\), \\(k_1 + k_2 = k\\), \\(\\boldsymbol \\beta_1\\) has dimensions \\(k_1 \\times 1\\), and \\(\\boldsymbol \\beta_2\\) has dimensions \\(k_2 \\times 1\\). Define \\(\\mathbf{Q}_1 = \\mathbf{I} - \\mathbf{X}_1 \\left(\\mathbf{X}_1^{\\prime} \\mathbf{X}_1\\right)^{-1} \\mathbf{X}_1^{\\prime}\\). Then, the least squares estimator can be computed as: \\[ \\mathbf{b} = \\left( \\begin{array}{c} \\mathbf{b}_1 \\\\ \\mathbf{b}_2 \\\\ \\end{array} \\right) = \\left( \\begin{array}{c} (\\mathbf{X}_1^{\\prime} \\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime} \\left( \\mathbf{y} - \\mathbf{X}_2 \\mathbf{b}_2 \\right) \\\\ \\left(\\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2\\right)^{-1} \\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{y} \\\\ \\end{array} \\right). \\tag{4.19} \\] Equation (4.19) provides the first step in the recursion. It can easily be iterated to allow for a more detailed decomposition of \\(\\mathbf{X}\\). Special Case: One Categorical and One Continuous Variable Model. To illustrate the relevance of equation (4.19), consider the model summarized in equation (4.17). Here, the dimension of \\(\\mathbf{X}\\) is \\(n \\times (c+1)\\) and so the dimension of \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\) is \\((c+1) \\times (c+1)\\). Taking the inverse of this matrix could be difficult if \\(c\\) is large. To apply equation (4.19), we define \\[ \\mathbf{X}_1 = \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdot \\cdot \\cdot &amp; \\mathbf{0}_1 \\\\ \\mathbf{0}_2 &amp; \\mathbf{1}_2 &amp; \\cdot \\cdot \\cdot &amp; \\mathbf{0}_2 \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\cdot \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\cdot \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\cdot \\cdot &amp; \\cdot \\\\ \\mathbf{0}_c &amp; \\mathbf{0}_c &amp; \\cdot \\cdot \\cdot &amp; \\mathbf{1}_c \\end{bmatrix} \\text{ and } \\mathbf{X}_2 = \\begin{bmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\mathbf{x}_c \\end{bmatrix}. \\] In this case, we have seen in equation (4.15) how it is straightforward to compute \\(\\left(\\mathbf{X}_1^{\\prime} \\mathbf{X}_1\\right)^{-1}\\) without requiring matrix inversion. This makes calculating \\(\\mathbf{Q}_1\\) straightforward. With this, we can compute \\(\\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2\\) and, because it is a scalar, immediately get its inverse. This provides \\(\\mathbf{b}_2\\) which is then used to calculate \\(\\mathbf{b}_1\\). Although this procedure is not as direct as \\(\\mathbf{b} = \\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}\\), it can be computationally efficient. Partitioned Matrix Results To establish equation (4.19), we use results standard in matrix algebra regarding the inverses of partitioned matrices. Partitioned Matrix Results. Suppose that we can partition the \\((p+q) \\times (p+q)\\) matrix \\(\\mathbf{B}\\) as \\[ \\mathbf{B} = \\begin{bmatrix} \\mathbf{B}_{11} &amp; \\mathbf{B}_{12} \\\\ \\mathbf{B}_{12}^{\\prime} &amp; \\mathbf{B}_{22} \\end{bmatrix}, \\] where \\(\\mathbf{B}_{11}\\) is a \\(p \\times p\\) invertible matrix, \\(\\mathbf{B}_{22}\\) is a \\(q \\times q\\) invertible matrix, and \\(\\mathbf{B}_{12}\\) is a \\(p \\times q\\) matrix. Then \\[ \\mathbf{B}^{-1} = \\begin{bmatrix} \\mathbf{C}_{11}^{-1} &amp; - \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{C}_{22}^{-1} \\\\ - \\mathbf{C}_{22}^{-1} \\mathbf{B}_{12}^{\\prime} \\mathbf{B}_{11}^{-1} &amp; \\mathbf{C}_{22}^{-1} \\end{bmatrix}, \\tag{4.20} \\] where \\(\\mathbf{C}_{11} = \\mathbf{B}_{11} - \\mathbf{B}_{12} \\mathbf{B}_{22}^{-1} \\mathbf{B}_{12}^{\\prime}\\) and \\(\\mathbf{C}_{22} = \\mathbf{B}_{22} - \\mathbf{B}_{12}^{\\prime} \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12}.\\) To verify equation (4.20), multiply \\(\\mathbf{B}^{-1}\\) by \\(\\mathbf{B}\\) to obtain the identity matrix \\(\\mathbf{I}\\). Further, \\[ \\mathbf{C}_{11}^{-1} = \\mathbf{B}_{11}^{-1} + \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{C}_{22}^{-1} \\mathbf{B}_{12}^{\\prime} \\mathbf{B}_{11}^{-1}. \\tag{4.21} \\] Now, we first write the least squares estimator as \\[ \\begin{array}{ll} \\mathbf{b} &amp;= \\left( \\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y} = \\left( \\left( \\begin{array}{c} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{X}_2^{\\prime} \\\\ \\end{array} \\right) \\left( \\mathbf{X}_1 : \\mathbf{X}_2 \\right)\\right)^{-1} \\left( \\begin{array}{c} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{X}_2^{\\prime} \\\\ \\end{array} \\right) \\mathbf{y} \\\\ &amp; = \\left( \\begin{array}{cc} \\mathbf{X}_1^{\\prime} \\mathbf{X}_1 &amp; \\mathbf{X}_1^{\\prime} \\mathbf{X}_2 \\\\ \\mathbf{X}_2^{\\prime} \\mathbf{X}_1 &amp; \\mathbf{X}_2^{\\prime} \\mathbf{X}_2 \\\\ \\end{array} \\right)^{-1} \\left( \\begin{array}{c} \\mathbf{X}_1^{\\prime} \\mathbf{y} \\\\ \\mathbf{X}_2^{\\prime} \\mathbf{y} \\\\ \\end{array} \\right) = \\left( \\begin{array}{c} \\mathbf{b}_1 \\\\ \\mathbf{b}_2 \\\\ \\end{array} \\right). \\end{array} \\] To apply the partitioned matrix results, we define \\[ \\mathbf{Q}_j = \\mathbf{I} - \\mathbf{X}_j \\left(\\mathbf{X}_j^{\\prime}\\mathbf{X}_j \\right)^{-1} \\mathbf{X}_j^{\\prime}, \\] \\(j=1,2,\\) and \\(\\mathbf{B}_{j,k} = \\mathbf{X}_j^{\\prime}\\mathbf{X}_k\\) for \\(j,k=1,2.\\) This means that \\(\\mathbf{C}_{11}=\\mathbf{X}_1^{\\prime}\\mathbf{X}_1 - \\mathbf{X}_1^{\\prime}\\mathbf{X}_2 (\\mathbf{X}_2^{\\prime}\\mathbf{X}_2)^{-1} \\mathbf{X}_2^{\\prime}\\mathbf{X}_1^{\\prime }\\) \\(= \\mathbf{X}_1^{\\prime} \\mathbf{Q}_2 \\mathbf{X}_1\\) and similarly \\(\\mathbf{C}_{22} = \\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2\\). From the second row, we have \\[ \\begin{array}{ll} \\mathbf{b}_2 &amp;= \\mathbf{C}_{22}^{-1} \\left( -\\mathbf{B}_{12}^{\\prime} \\mathbf{B}_{11}^{-1}\\mathbf{X}_1^{\\prime} \\mathbf{y} + \\mathbf{X}_2^{\\prime} \\mathbf{y} \\right) \\\\ &amp;= \\left(\\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2 \\right)^{-1} \\left( - \\mathbf{X}_2^{\\prime} \\mathbf{X}_1 (\\mathbf{X}_1^{\\prime}\\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y} + \\mathbf{X}_2^{\\prime}\\mathbf{y} \\right) \\\\ &amp;= \\left(\\mathbf{X}_2^{\\prime} \\mathbf{Q}_1 \\mathbf{X}_2 \\right)^{-1} \\mathbf{X}_2^{\\prime} \\mathbf{Q}_1\\mathbf{y}. \\end{array} \\] From the first row, \\[ \\begin{array}{ll} \\mathbf{b}_1 &amp;= \\mathbf{C}_{11}^{-1} \\mathbf{X}_1^{\\prime}\\mathbf{y} - \\mathbf{B}_{11}^{-1}\\mathbf{B}_{12}\\mathbf{C}_{22}^{-1} \\mathbf{X}_2^{\\prime}\\mathbf{y} \\\\ &amp;= \\left( \\mathbf{B}_{11}^{-1} + \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{C}_{22}^{-1} \\mathbf{B}_{21} \\mathbf{B}_{11}^{-1} \\right) \\mathbf{X}_1^{\\prime}\\mathbf{y} - \\mathbf{B}_{11}^{-1}\\mathbf{B}_{12}\\mathbf{C}_{22}^{-1} \\mathbf{X}_2^{\\prime}\\mathbf{y} \\\\ &amp;= \\mathbf{B}_{11}^{-1}\\mathbf{X}_1^{\\prime}\\mathbf{y} - \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{C}_{22}^{-1} \\left( -\\mathbf{B}_{21} \\mathbf{B}_{11}^{-1} \\mathbf{X}_1^{\\prime}\\mathbf{y} + \\mathbf{X}_2^{\\prime}\\mathbf{y} \\right)\\\\ &amp;= \\mathbf{B}_{11}^{-1}\\mathbf{X}_1^{\\prime}\\mathbf{y} - \\mathbf{B}_{11}^{-1} \\mathbf{B}_{12} \\mathbf{b}_2 \\\\ &amp;= (\\mathbf{X}_1^{\\prime}\\mathbf{X}_1)^{-1}\\mathbf{X}_1^{\\prime}\\mathbf{y} - (\\mathbf{X}_1^{\\prime}\\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime}\\mathbf{X}_2 \\mathbf{b}_2 \\\\ &amp;= (\\mathbf{X}_1^{\\prime}\\mathbf{X}_1)^{-1}\\mathbf{X}_1^{\\prime} \\left( \\mathbf{y} - \\mathbf{X}_2 \\mathbf{b}_2 \\right). \\end{array} \\] This establishes equation (4.19). Reparameterized Model. For the partitioned regression function in equation (4.18), define \\(\\mathbf{A}= \\left( \\mathbf{X}_1^{\\prime} \\mathbf{X}_1 \\right)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{X}_2\\) and \\(\\mathbf{E}_2 = \\mathbf{X}_2 - \\mathbf{X}_1 \\mathbf{A}\\). If one were to run a “multivariate” regression using \\(\\mathbf{X}_2\\) as the response and \\(\\mathbf{X}_1\\) as explanatory variables, then the parameter estimates would be \\(\\mathbf{A}\\) and the residuals \\(\\mathbf{E}_2\\). With these definitions, use equation (4.18) to define the reparameterized regression model \\[ \\begin{array}{ll} \\mathbf{y} &amp; = \\mathbf{X}_1 \\boldsymbol \\beta _1 + \\mathbf{X}_2 \\boldsymbol \\beta _2 + \\boldsymbol \\varepsilon = \\mathbf{X}_1 \\boldsymbol \\beta _1 + (\\mathbf{E}_2 + \\mathbf{X}_1 \\mathbf{A})\\boldsymbol \\beta _2 + \\boldsymbol \\varepsilon \\\\ &amp; = \\mathbf{X}_1 \\boldsymbol \\alpha _1 + \\mathbf{E}_2 \\boldsymbol \\beta _2 + \\boldsymbol \\varepsilon, \\end{array} \\tag{4.22} \\] where $_1 = _1 + _2 $ is a new vector of parameters. The reason for introducing this new parameterization is that now the vector of explanatory variables is orthogonal to the other explanatory variables, that is, straightforward algebra shows that \\(\\mathbf{X}_1^{\\prime }\\mathbf{E}_2=\\mathbf{0}\\). By equation (4.19), the vector of least squares estimates is \\[ \\begin{array}{ll} \\mathbf{a} = \\begin{bmatrix} \\mathbf{a}_1 \\\\ \\mathbf{b}_2 \\end{bmatrix} = \\left( \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{E}_2^{\\prime} \\end{bmatrix} \\begin{bmatrix} \\mathbf{X}_1 &amp; \\mathbf{E}_2 \\end{bmatrix} \\right) ^{-1} \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{E}_2^{\\prime} \\end{bmatrix} \\mathbf{y} = \\begin{bmatrix} \\left( \\mathbf{X}_1^{\\prime} \\mathbf{X}_1 \\right)^{-1} \\mathbf{X}_1^{ \\prime} \\mathbf{y} \\\\ \\left( \\mathbf{E}_2^{\\prime}\\mathbf{E}_2\\right) ^{-1} \\mathbf{E}_2^{\\prime}\\mathbf{y} \\end{bmatrix}. \\end{array} \\tag{4.23} \\] Extra Sum of Squares. Suppose that we wish to consider the increase in the error sum of squares going from a reduced model \\[ \\mathbf{y} = \\mathbf{X}_1 \\boldsymbol \\beta_1 + \\boldsymbol \\varepsilon \\] to a full model \\[ \\mathbf{y} = \\mathbf{X}_1 \\boldsymbol \\beta_1 + \\mathbf{X}_2 \\boldsymbol \\beta_2 + \\boldsymbol \\varepsilon. \\] For the reduced model, the error sum of squares is \\[ (Error ~SS)_{reduced} = \\mathbf{y}^{\\prime} \\mathbf{y} - \\mathbf{y}^{\\prime} \\mathbf{X}_1 (\\mathbf{X}_1^{\\prime} \\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y}. \\tag{4.24} \\] Using the reparameterized version of the full model, the error sum of squares is \\[ \\begin{array}{ll} (Error ~SS)_{full} &amp;= \\mathbf{y}^{\\prime} \\mathbf{y} - \\mathbf{a}^{\\prime} \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{E}_2^{\\prime} \\end{bmatrix} \\mathbf{y} = \\mathbf{y}^{\\prime}\\mathbf{y} - \\begin{bmatrix} \\left( \\mathbf{X}_1^{\\prime} \\mathbf{X}_1 \\right)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y} \\\\ \\left( \\mathbf{E}_2^{\\prime}\\mathbf{E}_2\\right)^{-1} \\mathbf{E}_2^{\\prime} \\mathbf{y} \\end{bmatrix}^{\\prime} \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\mathbf{y} \\\\ \\mathbf{E}_2^{\\prime}\\mathbf{y} \\end{bmatrix} \\\\ &amp;= \\mathbf{y}^{\\prime} \\mathbf{y} - \\mathbf{y}^{\\prime} \\mathbf{X}_1 (\\mathbf{X}_1^{\\prime} \\mathbf{X}_1)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y} - \\mathbf{y}^{\\prime} \\mathbf{E}_2 (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1} \\mathbf{E}_2^{\\prime} \\mathbf{y}. \\tag{4.25} \\end{array} \\] Thus, the reduction in the error sum of squares by adding \\(\\mathbf{X}_2\\) to the model is \\[ (Error ~SS)_{reduced} - (Error ~SS)_{full} = \\mathbf{y}^{\\prime} \\mathbf{E}_2 (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1} \\mathbf{E}_2^{\\prime} \\mathbf{y}. \\tag{4.26} \\] As noted in Section 4.3, the quantity \\((Error ~SS)_{reduced} - (Error ~SS)_{full}\\) is called the extra sum of squares, or Type III sum of squares. It is produced automatically by some statistical software packages, thus obviating the need to run separate regressions. 4.7.3 General Linear Model Recall the general linear model from Section 4.4. That is, we use \\[ y_i = \\beta_0 x_{i0} + \\beta_1 x_{i1} + \\ldots + \\beta _k x_{ik} + \\varepsilon_i, \\] or, in matrix notation, \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta + \\boldsymbol \\varepsilon\\). As before, we use Assumptions F1-F4 (or E1-E4) so that the disturbance terms are i.i.d. with mean zero and common variance \\(\\sigma^2\\), and the explanatory variables \\(\\{x_{i0},x_{i1},x_{i2},\\ldots,x_{ik}\\}\\) are non-stochastic. In the general linear model, we do not require that \\(\\mathbf{X}^{\\prime}\\mathbf{X}\\) be invertible. As we have seen in Chapter 4, an important reason for this generalization relates to handling categorical variables. That is, in order to use categorical variables, they are generally re-coded using binary variables. For this re-coding, generally some type of restrictions need to be made on the set of parameters associated with the indicator variables. However, it is not always clear what type of restrictions are the most intuitive. By expressing the model without requiring that \\(\\mathbf{X}^{\\prime}\\mathbf{X}\\) be invertible, the restrictions can be imposed after the estimation is done, not before. Normal Equations. Even when \\(\\mathbf{X}^{\\prime}\\mathbf{X}\\) is not invertible, solutions to the normal equations still provide least squares estimates of \\(\\boldsymbol \\beta\\). That is, the sum of squares is \\[ SS(\\mathbf{b}^{\\ast}) = \\mathbf{(y - Xb}^{\\ast}\\mathbf{)}^{\\prime}\\mathbf{(y - Xb}^{\\ast}\\mathbf{)}, \\] where \\(\\mathbf{b}^{\\ast} = (b_0^{\\ast}, b_1^{\\ast}, \\ldots, b_k^{\\ast})^{\\prime}\\) is a vector of candidate estimates. Solutions of the normal equations are those vectors \\(\\mathbf{b}^{\\circ }\\) that satisfy the normal equations \\[ \\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\circ } = \\mathbf{X}^{\\prime}\\mathbf{y}. \\tag{4.27} \\] We use the notation \\(^{\\circ }\\) to remind ourselves that \\(\\mathbf{b}^{\\circ }\\) need not be unique. However, it is a minimizer of the sum of squares. To see this, consider another candidate vector \\(\\mathbf{b}^{\\ast}\\) and note that \\[ SS(\\mathbf{b}^{\\ast}) = \\mathbf{y}^{\\prime}\\mathbf{y} - 2\\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{y} + \\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\ast}. \\] Then, using equation (4.27), we have \\[ SS(\\mathbf{b}^{\\ast}) - SS(\\mathbf{b}^{\\circ }) = -2\\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{y} + \\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\ast} - (-2\\mathbf{b}^{\\circ \\prime }\\mathbf{Xy} + \\mathbf{b}^{\\circ \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\circ }) \\] \\[ = -2\\mathbf{b}^{\\ast \\prime }\\mathbf{Xb}^{\\circ } + \\mathbf{b}^{\\ast \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\ast} + \\mathbf{b}^{\\circ \\prime }\\mathbf{X}^{\\prime}\\mathbf{Xb}^{\\circ } \\] \\[ = \\mathbf{(b}^{\\ast} - \\mathbf{b}^{\\circ})^{\\prime} \\mathbf{X}^{\\prime} \\mathbf{X} (\\mathbf{b}^{\\ast} - \\mathbf{b}^{\\circ}) = \\mathbf{z}^{\\prime} \\mathbf{z} \\geq 0, \\] where \\(\\mathbf{z} = \\mathbf{X} (\\mathbf{b}^{\\ast} - \\mathbf{b}^{\\circ})\\). Thus, any other candidate \\(\\mathbf{b}^{\\ast}\\) yields a sum of squares at least as large as \\(SS(\\mathbf{b}^{\\circ})\\). Unique Fitted Values Despite the fact that there may be (infinitely) many solutions to the normal equations, the resulting fitted values, \\(\\mathbf{\\hat{y}} = \\mathbf{Xb}^{\\circ}\\), are unique. To see this, suppose that \\(\\mathbf{b}_1^{\\circ}\\) and \\(\\mathbf{b}_2^{\\circ}\\) are two different solutions of equation (4.27). Let \\(\\mathbf{\\hat{y}}_1 = \\mathbf{Xb}_1^{\\circ}\\) and \\(\\mathbf{\\hat{y}}_2 = \\mathbf{Xb}_2^{\\circ}\\) denote the vectors of fitted values generated by these estimates. Then, \\[ \\mathbf{(\\hat{y}}_1 - \\mathbf{\\hat{y}}_2)^{\\prime} (\\mathbf{\\hat{y}}_1 - \\mathbf{\\hat{y}}_2) = \\mathbf{(b}_1^{\\circ} - \\mathbf{b}_2^{\\circ})^{\\prime} \\mathbf{X}^{\\prime} \\mathbf{X} (\\mathbf{b}_1^{\\circ} - \\mathbf{b}_2^{\\circ}) = 0 \\] because \\(\\mathbf{X}^{\\prime} \\mathbf{X} (\\mathbf{b}_1^{\\circ} - \\mathbf{b}_2^{\\circ}) = \\mathbf{X}^{\\prime} \\mathbf{y} - \\mathbf{X}^{\\prime} \\mathbf{y} = \\mathbf{0}\\), from equation (4.27). Hence we have that \\(\\mathbf{\\hat{y}}_1 = \\mathbf{\\hat{y}}_2\\) for any choice of \\(\\mathbf{b}_1^{\\circ}\\) and \\(\\mathbf{b}_2^{\\circ}\\), thus establishing the uniqueness of the fitted values. Because the fitted values are unique, the residuals are also unique. Thus, the error sum of squares and estimates of variability (such as \\(s^2\\)) are also unique. Generalized Inverses A generalized inverse of a matrix \\(\\mathbf{A}\\) is a matrix \\(\\mathbf{B}\\) such that \\(\\mathbf{ABA = A}\\). We use the notation \\(\\mathbf{A}^{\\mathbf{-}}\\) to denote the generalized inverse of \\(\\mathbf{A}\\). In the case that \\(\\mathbf{A}\\) is invertible, then \\(\\mathbf{A}^{\\mathbf{-}}\\) is unique and equals \\(\\mathbf{A}^{\\mathbf{-1}}\\). Although there are several definitions of generalized inverses, the above definition suffices for our purposes. See Searle (1987) for further discussion of alternative definitions of generalized inverses. With this definition, it can be shown that a solution to the equation \\(\\mathbf{Ab = c}\\) can be expressed as \\(\\mathbf{b = A}^{-} \\mathbf{c}\\). Thus, we can express a least squares estimate of \\(\\boldsymbol \\beta\\) as \\(\\mathbf{b}^{\\circ} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-} \\mathbf{X}^{\\prime} \\mathbf{y}\\). Statistical software packages can calculate versions of \\((\\mathbf{X}^{\\prime} \\mathbf{X})^{-}\\) and thus generate \\(\\mathbf{b}^{\\circ}\\). Estimable Functions Above, we saw that each fitted value \\(\\hat{y}_i\\) is unique. Because fitted values are simply linear combinations of parameter estimates, it seems reasonable to ask what other linear combinations of parameter estimates are unique. To this end, we say that \\(\\mathbf{C \\boldsymbol \\beta}\\) is an estimable function of parameters if \\(\\mathbf{Cb}^{\\circ}\\) does not depend (is invariant) on the choice of \\(\\mathbf{b}^{\\circ}\\). Because fitted values are invariant to the choice of \\(\\mathbf{b}^{\\circ}\\), we have that \\(\\mathbf{C = X}\\) produces one type of estimable function. Interestingly, it turns out that all estimable functions are of the form \\(\\mathbf{LXb}^{\\circ}\\), that is, \\(\\mathbf{C} = \\mathbf{LX}\\). See Searle (1987, page 284) for a demonstration of this. Thus, all estimable functions are linear combinations of fitted values, that is, \\(\\mathbf{LXb}^{\\circ} = \\mathbf{L\\hat{y}}\\). Estimable functions are unbiased and have a variance that does not depend on the choice of the generalized inverse. That is, it can be shown that $ ^{} = $ and \\(\\text{Var } \\mathbf{Cb}^{\\circ} = \\sigma^2 \\mathbf{C(X}^{\\prime} \\mathbf{X})^{-} \\mathbf{C}^{\\prime}\\) does not depend on the choice of \\(\\mathbf{(X}^{\\prime} \\mathbf{X})^{-}\\). Testable Hypotheses As described in Section 4.2, it is often of interest to test \\(H_0\\): \\(\\mathbf{C \\boldsymbol \\beta} = \\mathbf{d}\\), where \\(\\mathbf{d}\\) is a specified vector. This hypothesis is said to be testable if \\(\\mathbf{C \\boldsymbol \\beta}\\) is an estimable function, \\(\\mathbf{C}\\) is of full row rank, and the rank of \\(\\mathbf{C}\\) is less than the rank of \\(\\mathbf{X}\\). For consistency with the notation of Section 4.2, let \\(p\\) be the rank of \\(\\mathbf{C}\\) and \\(k + 1\\) be the rank of \\(\\mathbf{X}\\). Recall that the rank of a matrix is the smaller of the number of linearly independent rows and linearly independent columns. When we say that \\(\\mathbf{C}\\) has full row rank, we mean that there are \\(p\\) rows in \\(\\mathbf{C}\\), so that the number of rows equals the rank. General Linear Hypothesis As in Section 4.2, the test statistic for examining \\(H_0\\): \\(\\mathbf{C \\boldsymbol \\beta} = \\mathbf{d}\\) is \\[ F\\text{-ratio} = \\frac{\\mathbf{(Cb}^{\\circ} - \\mathbf{d})^{\\prime} \\mathbf{(C(X}^{\\prime} \\mathbf{X})^{-} \\mathbf{C}^{\\prime})^{-1} \\mathbf{(Cb}^{\\circ} - \\mathbf{d})}{ps_{full}^2}. \\] Note that the statistic \\(F\\)-ratio does not depend on the choice of \\(\\mathbf{b}^{\\circ}\\) because \\(\\mathbf{C b}^{\\circ}\\) is invariant to \\(\\mathbf{b}^{\\circ}\\). If \\(H_0\\): \\(\\mathbf{C \\boldsymbol \\beta} = \\mathbf{d}\\) is a testable hypothesis and the errors \\(\\varepsilon_i\\) are i.i.d. \\(\\text{N}(0, \\sigma^2)\\), then the \\(F\\)-ratio has an \\(F\\)-distribution with \\(df_1 = p\\) and \\(df_2 = n - (k + 1)\\). One Categorical Variable Model We now illustrate the general linear model by considering an over-parameterized version of the one factor model that appears in equation (4.10) using \\[ y_{ij} = \\mu + \\tau_j + e_{ij} = \\mu + \\tau_1 x_{i1} + \\tau_2 x_{i2} + \\ldots + \\tau_c x_{ic} + \\varepsilon_{ij}. \\] At this point, we do not impose additional restrictions in the parameters. As with equation (4.13), this can be written in matrix form as \\[ \\mathbf{y} = \\begin{bmatrix} \\mathbf{1}_1 &amp; \\mathbf{1}_1 &amp; \\mathbf{0}_1 &amp; \\cdots &amp; \\mathbf{0}_1 \\\\ \\mathbf{1}_2 &amp; \\mathbf{0}_2 &amp; \\mathbf{1}_{\\mathbf{2}} &amp; \\cdots &amp; \\mathbf{0}_2 \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\mathbf{1}_c &amp; \\mathbf{0}_c &amp; \\mathbf{0}_c &amp; \\cdots &amp; \\mathbf{1}_c \\end{bmatrix} \\begin{bmatrix} \\mu \\\\ \\tau_1 \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\tau_c \\end{bmatrix} + \\boldsymbol \\varepsilon = \\mathbf{X \\boldsymbol \\beta + \\boldsymbol \\varepsilon}. \\] Thus, the \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\) matrix is \\[ \\mathbf{X}^{\\prime} \\mathbf{X} = \\begin{bmatrix} n &amp; n_1 &amp; n_2 &amp; \\cdots &amp; n_c \\\\ n_1 &amp; n_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ n_2 &amp; 0 &amp; n_2 &amp; \\cdots &amp; 0 \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdots &amp; \\cdot \\\\ n_c &amp; 0 &amp; 0 &amp; \\cdots &amp; n_c \\end{bmatrix}. \\] where \\(n = n_1 + n_2 + \\ldots + n_{c}\\). This matrix is not invertible. To see this, note that by adding the last \\(c\\) rows together yields the first row. Thus, the last \\(c\\) rows are an exact linear combination of the first row, meaning that the matrix is not full rank. The (non-unique) least squares estimates can be expressed as \\[ \\mathbf{b}^{\\circ} = \\begin{bmatrix} \\mu^{\\circ} \\\\ \\tau_1^{\\circ} \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\tau_c^{\\circ} \\end{bmatrix} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-} \\mathbf{X}^{\\prime} \\mathbf{y}. \\] Estimable functions are linear combinations of fitted values. Because fitted values are \\(\\hat{y}_{ij} = \\bar{y}_j\\), estimable functions can be expressed as $L = _{j=1}^{c} a_j {y}_j $ where \\(a_1, \\ldots, a_{c}\\) are constants. This linear combination of fitted values is an unbiased estimator of \\(\\text{E } L = \\sum_{i=1}^{c} a_i (\\mu + \\tau_i)\\). Thus, for example, by choosing \\(a_1 = 1\\) and the other \\(a_i = 0\\), we see that \\(\\mu + \\tau_1\\) is estimable. As another example, by choosing \\(a_1 = 1\\), \\(a_2 = -1\\), and the other \\(a_i = 0\\), we see that \\(\\tau_1 - \\tau_2\\) is estimable. It can be shown that \\(\\mu\\) is not an estimable parameter without further restrictions on \\(\\tau_1, \\ldots, \\tau_c\\). "],["C5VarSelect.html", "Chapter 5 Variable Selection 5.1 An Iterative Approach to Data Analysis and Modeling 5.2 Automatic Variable Selection Procedures 5.3 Residual Analysis 5.4 Influential Points 5.5 Collinearity 5.6 Selection Criteria 5.7 Heteroscedasticity 5.8 Further Reading and References 5.9 Exercises 5.10 Technical Supplements for Chapter 5", " Chapter 5 Variable Selection Chapter Preview. This chapter describes tools and techniques to help you select variables to enter into a linear regression model, beginning with an iterative model selection process. In applications with many potential explanatory variables, automatic variable selection procedures will help you quickly evaluate many models. Nonetheless, automatic procedures have serious limitations, including the inability to account properly for nonlinearities such as the impact of unusual points; this chapter expands upon the Chapter 2 discussion of unusual points. It also describes collinearity, a common feature of regression data where explanatory variables are linearly related to one another. Other topics that impact variable selection, including heteroscedasticity and out-of-sample validation, are also introduced. 5.1 An Iterative Approach to Data Analysis and Modeling In our introduction of basic linear regression in Chapter 2, we examined the data graphically, hypothesized a model structure, and compared the data to a candidate model to formulate an improved model. Box (1980) describes this as an iterative process, which is shown in Figure 5.1. Figure 5.1: The iterative model specification process This iterative process provides a useful recipe for structuring the task of specifying a model to represent a set of data. The first step, the model formulation stage, is accomplished by examining the data graphically and using prior knowledge of relationships, such as from economic theory or standard industry practice. The second step in the iteration is based on the assumptions of the specified model. These assumptions must be consistent with the data to make valid use of the model. The third step, diagnostic checking, is also known as data and model criticism; the data and model must be consistent with one another before additional inferences can be made. Diagnostic checking is an important part of the model formulation; it can reveal mistakes made in previous steps and provide ways to correct these mistakes. 5.2 Automatic Variable Selection Procedures Business and economics relationships are complicated; there are typically many variables that could serve as useful predictors of the dependent variable. In searching for a suitable relationship, there is a large number of potential models that are based on linear combinations of explanatory variables and an infinite number that can be formed from nonlinear combinations. To search among models based on linear combinations, several automatic procedures are available to select variables to be included in the model. These automatic procedures are easy to use and will suggest one or more models that you can explore in further detail. To illustrate how large is the potential number of linear models, suppose that there are only four variables, f\\(x_{1},\\) \\(x_2,\\) \\(x_3\\) and \\(x_4\\), under consideration for fitting a model to \\(y\\). Without any consideration of multiplication or other nonlinear combinations of explanatory variables, how many possible models are there? Table 5.1 shows that the answer is 16. Table 5.1: Sixteen Possible Models Expression Combinations Models E \\(y=\\beta_0\\) 1 model with no independent variables E \\(y=\\beta_0+\\beta_1x_i\\) \\(i\\) = 1,2,3,4 4 models with one independent variable E \\(y = \\beta_0 + \\beta_1 x_i + \\beta_2 x_j\\) (\\(i,j\\)) = (1,2),(1,3),(1,4),(2,3),(2,4),(3,4) 6 models with two independent variables E \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_j +\\beta_3x_{k}\\) (\\(i,j,k\\)) = (1,2,3),(1,2,4),(1,3,4),(2,3,4) 4 models with three independent variables E \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +\\beta_3 x_3 + \\beta_4 x_4\\) 1 model with all independent variables If there were only three explanatory variables, then you can use the same logic to verify that there are eight possible models. Extrapolating from these two examples, how many linear models will there be if there are ten explanatory variables? The answer is 1,024, which is quite a few. In general, the answer is \\(2^k\\), where \\(k\\) is the number of explanatory variables. For example, \\(2^3\\) is 8, \\(2^4\\) is 16, and so on. In any case, for a moderately large number of explanatory variables, there are many potential models that are based on linear combinations of explanatory variables. We would like a procedure to search quickly through these potential models to give us more time to think about other interesting aspects of model selection. Stepwise regression are procedures that employ \\(t\\)-tests to check the “significance” of explanatory variables entered into, or deleted from, the model. To begin, in the forward selection version of stepwise regression, variables are added one at a time. In the first stage, out of all the candidate variables the one that is most statistically significant is added to the model. At the next stage, with the first stage variable already included, the next most statistically significant variable is added. This procedure is repeated until all statistically significant variables have been added. Here, statistical significance is typically assessed using a variable’s \\(t\\)-ratio – the cut-off for statistical significance is typically a pre-determined \\(t\\)-value (such as two, corresponding to an approximate 95% significance level). The backwards selection version works in a similar manner, except that all variables are included in the initial stage and then are dropped one at a time (instead of added). More generally, an algorithm that adds and deletes variables at each stage is sometimes known as the stepwise regression algorithm. Stepwise Regression Algorithm. Suppose that the analyst has identified one variable as the response, \\(y\\), and \\(k\\) potential explanatory variables, \\(x_1, x_2, \\ldots, x_k\\). Consider all possible regressions using one explanatory variable. For each of the \\(k\\) regressions, compute \\(t(b_1)\\), the \\(t\\)-ratio for the slope. Choose that variable with the largest \\(t\\)-ratio. If the \\(t\\)-ratio does not exceed a pre-specified \\(t\\)-value (such as two), then do not choose any variables and halt the procedure. Add a variable to the model from the previous step. The variable to enter is the one that makes the largest significant contribution. To determine the size of contribution, use the absolute value of the variable’s \\(t\\)-ratio. To enter, the \\(t\\)-ratio must exceed a specified \\(t\\)-value in absolute value. Delete a variable from the model from the previous step. The variable to be removed is the one that makes the smallest contribution. To determine the size of contribution, use the absolute value of the variable’s \\(t\\)-ratio. To be removed, the \\(t\\)-ratio must be less than a specified \\(t\\)-value in absolute value. Repeat steps (ii) and (iii) until all possible additions and deletions are performed. When implementing this routine, some statistical software packages use an \\(F\\)-test in lieu of \\(t\\)-tests. Recall, when only one variable is being considered, that \\((t\\text{-ratio})^2 = F\\)-ratio and thus these procedures are equivalent. This algorithm is useful in that it quickly searches through a number of candidate models. However, there are several drawbacks: The procedure “snoops” through a large number of models and may fit the data “too well.” There is no guarantee that the selected model is the best. The algorithm does not consider models that are based on nonlinear combinations of explanatory variables. It also ignores the presence of outliers and high leverage points. In addition, the algorithm does not even search all \\(2^{k}\\) possible linear regressions. The algorithm uses one criterion, a \\(t\\)-ratio, and does not consider other criteria such as \\(s\\), \\(R^2\\), \\(R_a^2\\), and so on. There is a sequence of significance tests involved. Thus, the significance level that determines the \\(t\\)-value is not meaningful. By considering each variable separately, the algorithm does not take into account the joint effect of explanatory variables. Purely automatic procedures may not take into account an investigator’s special knowledge. Many of the criticisms of the basic stepwise regression algorithm can be addressed with modern computing software that is now widely available. We now consider each drawback, in reverse order. To respond to drawback number (7), many statistical software routines have options for forcing variables into a model equation. In this way, if other evidence indicates that one or more variables should be included in the model, then the investigator can force the inclusion of these variables. For drawback number (6), in Section 5.5.4 on suppressor variables, we will provide examples of variables that do not have important individual effects but are important when considered jointly. These combinations of variables may not be detected with the basic algorithm but will be detected with the backwards selection algorithm. Because the backwards procedure starts with all variables, it will detect, and retain, variables that are jointly important. Drawback number (5) is really a suggestion about the way to use stepwise regression. Bendel and Afifi (1977) suggested using a cut-off smaller than you ordinarily might. For example, in lieu of using \\(t\\)-value = 2 corresponding approximately to a 5% significance level, consider using \\(t\\)-value = 1.645 corresponding approximately to a 10% significance level. In this way, there is less chance of screening out variables that may be important. A lower bound, but still a good choice for exploratory work, is a cut-off as small as \\(t\\)-value = 1. This choice is motivated by an algebraic result: when a variable enters a model, \\(s\\) will decrease if the \\(t\\)-ratio exceeds one in absolute value. To address drawbacks number (3) and (4), we now introduce the best regressions routine. Best regressions is a useful algorithm that is now widely available in statistical software packages. The best regression algorithm searches over all possible combinations of explanatory variables, unlike stepwise regression, that adds and deletes one variable at a time. For example, suppose that there are four possible explanatory variables, \\(x_1\\), \\(x_2\\), \\(x_3\\), and \\(x_4\\), and the user would like to know what is the best two-variable model. The best regression algorithm searches over all six models of the form \\(\\mathrm{E}~y = \\beta_0 + \\beta_1 x_i + \\beta_2 x_j\\). Typically, a best regression routine recommends one or two models for each \\(p\\) coefficient model, where p is a number that is user-specified. Because it has specified the number of coefficients to enter the model, it does not matter which of the criteria we use: \\(R^2\\), \\(R_a^2\\), or \\(s\\). The best regression algorithm performs its search by a clever use of the algebraic fact that, when a variable is added to the model, the error sum of squares does not increase. Because of this fact, certain combinations of variables included in the model need not be computed. An important drawback of this algorithm is that it can take a considerable amount of time when the number of variables considered is large. Users of regression do not always appreciate the depth of drawback number (1), data-snooping. Data-snooping occurs when the analyst fits a great number of models to a data set. We will address the problem of data-snooping in Section 5.6.2 on model validation. Here, we illustrate the effect of data-snooping in stepwise regression. Example: Data-Snooping in Stepwise Regression. The idea of this illustration is due to Rencher and Pun (1980). Consider \\(n = 100\\) observations of \\(y\\) and fifty explanatory variables, \\(x_1, x_2, \\ldots, x_{50}\\). The data we consider here were simulated using independent standard normal random variates. Because the variables were simulated independently, we are working under the null hypothesis of no relation between the response and the explanatory variables, that is, \\(H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_{50} = 0\\). Indeed, when the model with all fifty explanatory variables was fit, it turns out that \\(s = 1.142\\), \\(R^2 = 46.2\\%\\), and \\(F\\)-ratio = \\(\\frac{Regression~MS}{Error~MS} = 0.84\\). Using an \\(F\\)-distribution with \\(df_1 = 50\\) and \\(df_2 = 49\\), the 95th percentile is 1.604. In fact, 0.84 is the 27th percentile of this distribution, indicating that the \\(p\\)-value is 0.73. Thus, as expected, the data are in congruence with \\(H_0\\). Next, a stepwise regression with \\(t\\)-value = 2 was performed. Two variables were retained by this procedure, yielding a model with \\(s = 1.05\\), \\(R^2 = 9.5\\%\\), and \\(F\\)-ratio = 5.09. For an \\(F\\)-distribution with \\(df_1 = 2\\) and \\(df_2 = 97\\), the 95th percentile is \\(F\\)-value = 3.09. This indicates that the two variables are statistically significant predictors of \\(y\\). At first glance, this result is surprising. The data were generated so that \\(y\\) is unrelated to the explanatory variables. However, because \\(F\\)-ratio \\(&gt;\\) \\(F\\)-value, the \\(F\\)-test indicates that two explanatory variables are significantly related to \\(y\\). The reason is that stepwise regression has performed many hypothesis tests on the data. For example, in Step 1, fifty tests were performed to find significant variables. Recall that a 5% level means that we expect to make roughly one mistake in 20. Thus, with fifty tests, we expect to find \\(50 \\times 0.05 = 2.5\\) “significant” variables, even under the null hypothesis of no relationship between \\(y\\) and the explanatory variables. To continue, a stepwise regression with \\(t\\)-value = 1.645 was performed. Six variables were retained by this procedure, yielding a model with \\(s = 0.99\\), \\(R^2 = 22.9\\%\\), and \\(F\\)-ratio = 4.61. As before, an \\(F\\)-test indicates a significant relationship between the response and these six explanatory variables. To summarize, using simulation we constructed a data set so that the explanatory variables have no relationship with the response. However, when using stepwise regression to examine the data, we “found” seemingly significant relationships between the response and certain subsets of the explanatory variables. This example illustrates a general caveat in model selection: when explanatory variables are selected using the data, \\(t\\)-ratios and \\(F\\)-ratios will be too large, thus overstating the importance of variables in the model. Stepwise regression and best regressions are examples of automatic variable selection procedures. In your modeling work, you will find these procedures to be useful because they can quickly search through several candidate models. However, these procedures do ignore nonlinear alternatives as well as the effect of outliers and high leverage points. The main point of the procedures is to mechanize certain routine tasks. This automatic selection approach can be extended and indeed, there are a number of so-called “expert systems” available in the market. For example, algorithms are available that “automatically” handle unusual points such as outliers and high leverage points. A model suggested by automatic variable selection procedures should be subject to the same careful diagnostic checking procedures as a model arrived at by any other means. 5.3 Residual Analysis Recall the role of a residual in the linear regression model introduced in Section 2.6. A residual is a response minus the corresponding fitted value under the model. Because the model summarizes the linear effect of several explanatory variables, we may think of a residual as a response controlled for values of the explanatory variables. If the model is an adequate representation of the data, then residuals should closely approximate random errors. Random errors are used to represent the natural variation in the model; they represent the result of an unpredictable mechanism. Thus, to the extent that residuals resemble random errors, there should be no discernible patterns in the residuals. Patterns in the residuals indicate the presence of additional information that we hope to incorporate into the model. A lack of patterns in the residuals indicates that the model seems to account for the primary relationships in the data. 5.3.1 Residuals There are at least four types of patterns that can be uncovered through the residual analysis. In this section, we discuss the first two: residuals that are unusual and those that are related to other explanatory variables. We then introduce the third type, residuals that display a heteroscedastic pattern, in Section 5.7. In our study of time series data that begins in Chapter 7, we will introduce the fourth type, residuals that display patterns through time. When examining residuals, it is usually easier to work with a standardized residual, a residual that has been rescaled to be dimensionless. We generally work with standardized residuals because we achieve some carry-over of experience from one data set to another and may thus focus on relationships of interest. By using standardized residuals, we can train ourselves to look at a variety of residual plots and immediately recognize an unusual point when working in standard units. There are a number of ways of defining a standardized residual. Using \\(e_i = y_i - \\hat{y}_i\\) as the \\(i\\)th residual, here are three commonly used definitions: \\[ \\text{(a) }\\frac{e_i}{s}, \\quad \\text{(b) }\\frac{e_i}{s\\sqrt{1 - h_{ii}}}, \\quad \\text{(c) }\\frac{e_i}{s_{(i)}\\sqrt{1 - h_{ii}}}. \\tag{5.1} \\] Here, \\(h_{ii}\\) is the \\(i\\)th leverage. It is calculated based on values of the explanatory variables and will be defined in Section 5.4.1. Recall that \\(s\\) is the residual standard deviation (defined in equation 3.8). Similarly, define \\(s_{(i)}\\) to be the residual standard deviation when running a regression after having deleted the \\(i\\)th observation. Now, the first definition in (a) is simple and easy to explain. An easy calculation shows that the sample standard deviation of the residuals is approximately \\(s\\) (one reason that \\(s\\) is often referred to as the residual standard deviation). Thus, it seems reasonable to standardize residuals by dividing by \\(s\\). The second choice presented in (b), although more complex, is more precise. The variance of the \\(i\\)th residual is \\[ \\text{Var}(e_i) = \\sigma^2(1 - h_{ii}). \\] This result will be established in equation (5.15) of Section 5.10. Note that this variance is smaller than the variance of the error term, Var\\((\\varepsilon_i) = \\sigma^2\\). Now, we can replace \\(\\sigma\\) by its estimate, \\(s\\). Then, this result leads to using the quantity \\(s\\sqrt{1 - h_{ii}}\\) as an estimated standard deviation, or standard error, for \\(e_i\\). Thus, we define the standard error of \\(e_i\\) to be \\[ \\text{se}(e_i) = s \\sqrt{1 - h_{ii}}. \\] Following the conventions introduced in Section 2.6, in this text we use \\(e_i / \\text{se}(e_i)\\) to be our standardized residual. The third choice presented in (c) is a modification of (b) and is known as a studentized residual. As emphasized in Section 5.3.2, one important use of residuals is to identify unusually large responses. Now, suppose that the \\(i\\)th response is unusually large and that this is measured through its residual. This unusually large residual will also cause the value of \\(s\\) to be large. Because the large effect appears in both the numerator and denominator, the standardized residual may not detect this unusual response. However, this large response will not inflate \\(s_{(i)}\\) because it is constructed after having deleted the \\(i\\)th observation. Thus, when using studentized residuals, we get a better measure of observations that have unusually large residuals. By omitting this observation from the estimate of \\(\\sigma\\), the size of the observation affects only the numerator \\(e_i\\) and not the denominator \\(s_{(i)}\\). As another advantage, studentized residuals follow a \\(t\\)-distribution with \\(n - (k + 1)\\) degrees of freedom, assuming the errors are normally distributed (assumption E5). This knowledge of the precise distribution helps us assess the degree of model fit and is particularly useful in small samples. It is this relationship with the “Student’s” \\(t\\)-distribution that suggests the name “studentized” residuals. 5.3.2 Using Residuals to Identify Outliers One important role of residual analysis is to identify outliers. An outlier is an observation that is not well fit by the model; these are observations where the residual is unusually large. A rule of thumb that is used by many statistical packages is that an observation is marked as an outlier if the standardized residual exceeds two in absolute value. To the extent that the distribution of standardized residuals mimics the standard normal curve, we expect about only one in 20 observations, or 95%, to exceed two in absolute value and very few observations to exceed three. Outliers provide a signal that an observation should be investigated to understand special causes associated with this point. An outlier is an observation that seems unusual with respect to the rest of the data set. It is often the case that the reason for this atypical behavior may be uncovered after additional investigation. Indeed, this may be the primary purpose of the regression analysis of a data set. Consider a simple example of so-called performance analysis. Suppose we have available a sample of \\(n\\) salespeople and are trying to understand each person’s second-year sales based on their first-year sales. To a certain extent, we expect that higher first-year sales are associated with higher second-year sales. High sales may be due to a salesperson’s natural ability, ambition, good territory, and so on. First-year sales may be thought of as a proxy variable that summarizes these factors. We expect variation in sales performance both cross-sectionally and across years. It is interesting when one salesperson performs unusually well (or poorly) in the second year compared to their first-year performance. Residuals provide a formal mechanism for evaluating second-year sales after controlling for the effects of first-year sales. There are a number of options available for handling outliers. Options for Handling Outliers Include the observation in the usual summary statistics but comment on its effects. An outlier may be large but not so large as to skew the results of the entire analysis. If no special causes for this unusual observation can be determined, then this observation may simply reflect the variability in the data. Delete the observation from the data set. The observation may be determined to be unrepresentative of the population from which the sample is drawn. If this is the case, then there may be little information contained in the observation that can be used to make general statements about the population. This option means that we would omit the observation from the regression summary statistics and discuss it in our report as a separate case. Create a binary variable to indicate the presence of an outlier. If one or several special causes have been identified to explain an outlier, then these causes could be introduced into the modeling procedure formally by introducing a variable to indicate the presence (or absence) of these causes. This approach is similar to point deletion but allows the outlier to be formally included in the model formulation so that, if additional observations arise that are affected by the same causes, then they can be handled on an automatic basis. 5.3.3 Using Residuals to Select Explanatory Variables Another important role of residual analysis is to help identify additional explanatory variables that may be used to improve the formulation of the model. If we have specified the model correctly, then residuals should resemble random errors and contain no discernible patterns. Thus, when comparing residuals to explanatory variables, we do not expect any relationships. If we do detect a relationship, then this suggests the need to control for this additional variable. This can be accomplished by introducing the additional variable into the regression model. Relationships between residuals and explanatory variables can be quickly established using correlation statistics. However, if an explanatory variable is already included in the regression model, then the correlation between the residuals and an explanatory variable will be zero (see Section 5.10.1 for the algebraic demonstration). It is a good idea to reinforce this correlation with a scatter plot. Not only will a plot of residuals versus explanatory variables reinforce graphically the correlation statistic, it will also serve to detect potential nonlinear relationships. For example, a quadratic relationship can be detected using a scatter plot, not a correlation statistic. If you detect a relationship between the residuals from a preliminary model fit and an additional explanatory variable, then introducing this additional variable will not always improve your model specification. The reason is that the additional variable may be linearly related to the variables that are already in the model. If you would like a guarantee that adding an additional variable will improve your model, then construct an added variable plot (see Section 3.4.3). To summarize, after a preliminary model fit, you should: Calculate summary statistics and display the distribution of (standardized) residuals to identify outliers. Calculate the correlation between the (standardized) residuals and additional explanatory variables to search for linear relationships. Create scatter plots between the (standardized) residuals and additional explanatory variables to search for nonlinear relationships. Example: Stock Market Liquidity. An investor’s decision to purchase a stock is generally made with a number of criteria in mind. First, investors usually look for a high expected return. A second criterion is the riskiness of a stock, which can be measured through the variability of the returns. Third, many investors are concerned with the length of time that they are committing their capital with the purchase of a security. Many income stocks, such as utilities, regularly return portions of capital investments in the form of dividends. Other stocks, particularly growth stocks, return nothing until the sale of the security. Thus, the average length of investment in a security is another criterion. Fourth, investors are concerned with the ability to sell the stock at any time convenient to the investor. We refer to this fourth criterion as the liquidity of the stock. The more liquid the stock, the easier it is to sell. To measure liquidity, in this study we use the number of shares traded on an exchange over a specified period of time (called the VOLUME). We are interested in studying the relationship between the volume and other financial characteristics of a stock. We begin this study with 126 companies whose options were traded on December 3, 1984. The stock data were obtained from Francis Emory Fitch, Inc. for the period from December 3, 1984 to February 28, 1985. For the trading activity variables, we examine: The three months total trading volume (VOLUME, in millions of shares), The three months total number of transactions (NTRAN), and The average time between transactions (AVGT, measured in minutes). For the firm size variables, we use: Opening stock price on January 2, 1985 (PRICE), The number of outstanding shares on December 31, 1984 (SHARE, in millions of shares), and The market equity value (VALUE, in billions of dollars) obtained by taking the product of PRICE and SHARE. Finally, for the financial leverage, we examine the debt-to-equity ratio (DEB_EQ) obtained from the Compustat Industrial Tape and the Moody’s manual. The data in SHARE are obtained from the Center for Research in Security Prices (CRSP) monthly tape. After examining some preliminary summary statistics of the data, three companies were deleted because they either had an unusually large volume or high price. They are Teledyne and Capital Cities Communication, whose prices were more than four times the average price of the remaining companies, and American Telephone and Telegraph, whose total volume was more than seven times the average total volume of the remaining companies. Based on additional investigation, the details of which are not presented here, these companies were deleted because they seemed to represent special circumstances that we would not wish to model. Table 5.2 summarizes the descriptive statistics based on the remaining \\(n = 123\\) companies. For example, from Table 5.2, we see that the average time between transactions is about five minutes and this time ranges from a minimum of less than 1 minute to a maximum of about 20 minutes. Table 5.2: Summary Statistics of the Stock Liquidity Variables Mean Median Standard Deviation Minimum Maximum VOLUME 13.423 11.556 10.632 0.658 64.572 AVGT 5.441 4.284 3.853 0.590 20.772 NTRAN 6436.000 5071.000 5310.000 999.000 36420.000 PRICE 38.800 34.380 21.370 9.120 122.380 SHARE 94.730 53.830 115.100 6.740 783.050 VALUE 4.116 2.065 8.157 0.115 75.437 DEBEQ 2.697 1.105 6.509 0.185 53.628 Source: Francis Emory Fitch, Inc., Standard &amp; Poor’s Compustat, and University of Chicago’s Center for Research on Security Prices. Table 5.3 reports the correlation coefficients and Figure 5.2 provides the corresponding scatterplot matrix. If you have a background in finance, you will find it interesting to note that the financial leverage, measured by DEB_EQ, does not seem to be related to the other variables. From the scatterplot and correlation matrix, we see a strong relationship between VOLUME and the size of the firm as measured by SHARE and VALUE. Further, the three trading activity variables, VOLUME, AVGT, and NTRAN, are all highly related to one another. Table 5.3: Correlation Matrix of the Stock Liquidity AVGT NTRAN PRICE SHARE VALUE DEB_EQ VOLUME AVGT 1.000 -0.668 -0.128 -0.429 -0.318 0.094 -0.674 NTRAN -0.668 1.000 0.190 0.817 0.760 -0.092 0.913 PRICE -0.128 0.190 1.000 0.177 0.457 -0.038 0.168 SHARE -0.429 0.817 0.177 1.000 0.829 -0.077 0.773 VALUE -0.318 0.760 0.457 0.829 1.000 -0.077 0.702 DEB_EQ 0.094 -0.092 -0.038 -0.077 -0.077 1.000 -0.052 VOLUME -0.674 0.913 0.168 0.773 0.702 -0.052 1.000 R Code to Produce Tables 5.2 and 5.3 liquidity &lt;- read.csv(&quot;CSVData/Liquidity.csv&quot;, header=TRUE) varLiquid &lt;- c(&quot;AVGT&quot;, &quot;NTRAN&quot;, &quot;PRICE&quot;, &quot;SHARE&quot;, &quot;VALUE&quot;, &quot;DEBEQ&quot;, &quot;VOLUME&quot;) liquidMat &lt;- data.frame(liquidity[varLiquid]) names(liquidMat)[names(liquidMat) == &quot;DEBEQ&quot;] &lt;- &quot;DEB_EQ&quot; # TABLE 5.2 SUMMARY STATISTICS BookSummStats &lt;- function(Xymat){ meanSummary &lt;- sapply(Xymat, mean, na.rm=TRUE) sdSummary &lt;- sapply(Xymat, sd, na.rm=TRUE) minSummary &lt;- sapply(Xymat, min, na.rm=TRUE) maxSummary &lt;- sapply(Xymat, max, na.rm=TRUE) medSummary &lt;- sapply(Xymat, median,na.rm=TRUE) tableMat &lt;- cbind(meanSummary, medSummary, sdSummary, minSummary, maxSummary) return(tableMat) } liquidMat1 &lt;- liquidMat[,c(7,1:6)] tableMat &lt;- BookSummStats(liquidMat1) colnames(tableMat) &lt;- c(&quot;Mean&quot; , &quot;Median&quot; , &quot;Standard Deviation&quot; , &quot;Minimum&quot; , &quot;Maximum&quot;) rownames(tableMat) &lt;- varLiquid[c(7,1:6)] tableMat &lt;- round(tableMat, digits = 3) tableMat[3,] &lt;- round(tableMat[3,], digits = 0) tableMat[4:5,] &lt;- round(tableMat[4:5,], digits = 2) TableGen1(TableData=tableMat, TextTitle=&#39;Summary Statistics of the Stock Liquidity Variables&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:5, ColWidth = ColWidth5) cor_matrix &lt;- cor(liquidMat) rownames(cor_matrix) &lt;- colnames(cor_matrix) &lt;- c(&quot;AVGT&quot;, &quot;NTRAN&quot;, &quot;PRICE&quot;, &quot;SHARE&quot;, &quot;VALUE&quot;, &quot;DEB_EQ&quot;, &quot;VOLUME&quot;) TableGen1(TableData=cor_matrix, TextTitle=&#39;Correlation Matrix of the Stock Liquidity&#39;, Align=&#39;r&#39;, Digits=3, ColumnSpec=1:6, ColWidth = ColWidth6) Figure 5.2 shows that the variable AVGT is inversely related to VOLUME and NTRAN is inversely related to AVGT. In fact, it turned out the correlation between the average time between transactions and the reciprocal of the number of transactions was \\(99.98\\%!\\) This is not so surprising when one thinks about how AVGT might be calculated. For example, on the New York Stock Exchange, the market is open from 10:00 A.M. to 4:00 P.M. For each stock on a particular day, the average time between transactions times the number of transactions is nearly equal to 360 minutes (= 6 hours). Thus, except for rounding errors because transactions are only recorded to the nearest minute, there is a perfect linear relationship between AVGT and the reciprocal of NTRAN. Figure 5.2: Scatterplot matrix for stock liquidity variables. The number of transactions variable (NTRAN) appears to be strongly related to the VOLUME of shares traded, and inversely related to AVGT. To begin to understand the liquidity measure VOLUME, we first fit a regression model using NTRAN as an explanatory variable. The fitted regression model is: \\[ \\begin{array}{lcc} \\text{VOLUME} &amp;= 1.65 &amp;+ 0.00183 \\text{ NTRAN} \\\\ \\text{standard errors} &amp; (0.0018) &amp; (0.000074) \\end{array} \\] with \\(R^2 = 83.4\\%\\) and \\(s = 4.35\\). Note that the \\(t\\)-ratio for the slope associated with NTRAN is \\[ t(b_1) = \\frac{b_1}{se(b_1)} = \\frac{0.00183}{0.000074} = 24.7 \\] indicating strong statistical significance. Residuals were computed using this estimated model. To see if the residuals are related to the other explanatory variables, Table 5.4 shows correlations. Table 5.4: First Table of Correlations Variable AVGT PRICE SHARE VALUE DEB_EQ RESID -0.155 -0.017 0.055 0.007 0.078 Note: The residuals were created from a regression of VOLUME on NTRAN. The correlation between the residual and AVGT and the scatter plot (not shown here) indicates that there may be some information in the variable AVGT in the residual. Thus, it seems sensible to use AVGT directly in the regression model. Remember that we are interpreting the residual as the value of VOLUME having controlled for the effect of NTRAN. We next fit a regression model using NTRAN and AVGT as explanatory variables. The fitted regression model is: \\[ \\begin{array}{lccc} \\text{VOLUME} &amp;= 4.41 &amp;- 0.322 \\text{ AVGT} &amp;+ 0.00167 \\text{ NTRAN} \\\\ \\text{standard errors} &amp; (1.30)&amp; (0.135)&amp; (0.000098) \\end{array} \\] with \\(R^2 = 84.2\\%\\) and \\(s = 4.26\\). Based on the \\(t\\)-ratio for AVGT, \\(t(b_{AVGT}) = \\frac{-0.322}{0.135} = -2.39\\), it seems as if AVGT is a useful explanatory variable in the model. Note also that \\(s\\) has decreased, indicating that \\(R_a^2\\) has increased. Table 5.5 provides correlations between the model residuals and other potential explanatory variables and indicates that there does not seem to be much additional information in the explanatory variables. This is reaffirmed by the corresponding table of scatter plots in Figure 5.3. The histograms in Figure 5.3 suggest that although the distribution of the residuals is fairly symmetric, the distribution of each explanatory variable is skewed. Because of this, transformations of the explanatory variables were explored. This line of thought provided no real improvements and thus the details are not provided here. Figure 5.3: Scatterplot matrix of the residuals from the regression of VOLUME on NTRAN and AVGT on the vertical axis and the remaining predictor variables on the horizontal axes. Table 5.5: Second Table of Correlations Variable PRICE SHARE VALUE DEB_EQ RESID -0.015 0.096 0.071 0.089 Note: The residuals were created from a regression of VOLUME on NTRAN and AVGT. 5.4 Influential Points Not all points are created equal – in this section we will see that specific observations can potentially have a disproportionate effect on the overall regression fit. We will call such points “influential.” This is not too surprising; we have already seen that regression coefficients estimates are weighted sums of responses (see Section 3.2.4). Some observations have heavier weights than others and thus have a greater influence on the regression coefficient estimates. Of course, simply because an observation is influential does not mean that it is incorrect or that its impact on the model is misleading. As analysts, we would simply like to know whether our fitted model is sensitive to mild changes such as the removal of a single point so that we feel comfortable generalizing our results from the sample to a larger population. To assess influence, we think of observations as being unusual responses, given a set of explanatory variables, or having an unusual set of explanatory variables. We have already seen in Section 5.3 how to assess unusual responses using residuals. This section focuses on unusual sets of explanatory variables. 5.4.1 Leverage We introduced this topic in Section 2.6 where we called an observation having an unusual explanatory variable a “high leverage point.” With more than one explanatory variable, determining whether an observation is a high leverage point is not as straightforward. For example, it is possible for an observation to be “not unusual” for any single variable and yet still be unusual in the space of explanatory variables. Consider the fictitious data set represented in Figure 5.4. Visually, it seems clear that the point marked in the upper right-hand corner is unusual. However, it is not unusual when examining the histogram of either \\(x_1\\) or \\(x_2\\). It is only unusual when the explanatory variables are considered jointly. Figure 5.4: The ellipsoid represents most of the data. The arrow marks an unusual point. For two explanatory variables, this is apparent when examining the data graphically. Because it is difficult to examine graphically data having more than two explanatory variables, we need a numerical procedure for assessing leverage. To define the concept of leverage in multiple linear regression, we use some concepts from matrix algebra. Specifically, in Section 3.1, we showed that the vector of least squares regression coefficients could be calculated using \\(\\mathbf{b} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}\\). Thus, we can express the vector of fitted values \\(\\hat{\\mathbf{y}} = (\\hat{y}_1, \\ldots, \\hat{y}_n)^{\\prime}\\) as \\[ \\mathbf{\\hat{y}} = \\mathbf{Xb} . \\tag{5.2} \\] Similarly, the vector of residuals is the vector of response minus the vector of fitted values, that is, \\(\\mathbf{e} = \\mathbf{y - \\hat{y}}\\). From the expression for the regression coefficients \\(\\mathbf{b}\\) in equation (3.4), we have \\[ \\mathbf{\\hat{y}} = \\mathbf{X} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{X}^{\\prime} \\mathbf{y} \\] This equation suggests defining \\[ \\mathbf{H} = \\mathbf{X} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{X}^{\\prime} \\] so that \\[ \\mathbf{\\hat{y}} = \\mathbf{Hy} \\] From this, the matrix \\(\\mathbf{H}\\) is said to project the vector of responses \\(\\mathbf{y}\\) onto the vector of fitted values \\(\\mathbf{\\hat{y}}\\). Alternatively, you may think of \\(\\mathbf{H}\\) as the matrix that puts the “hat,” or carat, on \\(\\mathbf{y}\\). From the \\(i\\)th row of the vector equation \\(\\mathbf{\\hat{y}} = \\mathbf{Hy}\\), we have \\[ \\hat{y}_i = h_{i1} y_1 + h_{i2} y_2 + \\cdots + h_{ii} y_i + \\cdots + h_{in} y_n \\] Here, \\(h_{ij}\\) is the number in the \\(i\\)th row and \\(j\\)th column of \\(\\mathbf{H}\\). From this expression, we see that the larger \\(h_{ii}\\) is, the larger the effect that the \\(i\\)th response \\((y_i)\\) has on the corresponding fitted value \\((\\hat{y}_i)\\). Thus, we call \\(h_{ii}\\) the leverage for the \\(i\\)th observation. Because \\(h_{ii}\\) is the \\(i\\)th diagonal element of \\(\\mathbf{H}\\), a direct expression for \\(h_{ii}\\) is \\[ h_{ii} = \\mathbf{x}_i^{\\prime} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{x}_i \\tag{5.3} \\] where \\(\\mathbf{x}_i = (x_{i0}, x_{i1}, \\ldots, x_{ik})^{\\prime}\\). Because the values \\(h_{ii}\\) are calculated based on the explanatory variables, the values of the response variable do not affect the calculation of leverages. Large leverage values indicate that an observation may exhibit a disproportionate effect on the fit, essentially because it is distant from the other observations (when looking at the space of explanatory variables). How large is large? Some guidelines are available from matrix algebra, where we have that \\[ \\frac{1}{n} \\leq h_{ii} \\leq 1 \\] and \\[ \\bar{h} = \\frac{1}{n} \\sum_{i=1}^{n} h_{ii} = \\frac{k+1}{n}. \\] Thus, each leverage is bounded by \\(n^{-1}\\) and \\(1\\), and the average leverage equals the number of regression coefficients divided by the number of observations. From these and related arguments, we use a widely adopted convention and declare an observation to be a high leverage point if the leverage exceeds three times the average, that is, if \\[ h_{ii} &gt; \\frac{3(k+1)}{n}. \\] Having identified high leverage points, as with outliers it is important for the analyst to search for special causes that may have produced these unusual points. To illustrate, in Section 2.7 we identified the 1987 market crash as the reason behind the high leverage point. Further, high leverage points are often due to clerical errors in coding the data, which may or may not be easy to rectify. In general, the options for dealing with high leverage points are similar to those available for dealing with outliers. Options for Handling High Leverage Points Include the observation in the summary statistics but comment on its effect. For example, an observation may barely exceed a cut-off and its effect may not be important in the overall analysis. Delete the observation from the data set. Again, the basic rationale for this action is that the observation is deemed not representative of some larger population. An intermediate course of action between (1) and (2) is to present the analysis both with and without the high leverage point. In this way, the impact of the point is fully demonstrated and the reader of your analysis may decide which option is more appropriate. Choose another variable to represent the information. In some instances, another explanatory variable will be available to serve as a replacement. For example, in an apartment rents example, we could use the number of bedrooms to replace a square footage variable as a measure of apartment size. Although an apartment’s square footage may be unusually large causing it to be a high leverage point, it may have one, two, or three bedrooms, depending on the sample examined. Use a nonlinear transformation of an explanatory variable. To illustrate, with our Stock Liquidity example in Section 5.5.3, we can transform the debt-to-equity DEB_EQ continuous variable into a variable that indicates the presence of “high” debt-to-equity. For example, we might code DE_IND = 1 if DEB_EQ &gt; 5 and DE_IND = 0 if DEB_EQ ≤ 5. With this recoding, we still retain information on the financial leverage of a company without allowing the large values of DEB_EQ to drive the regression fit. Some analysts use “robust” estimation methodologies as an alternative to least squares estimation. The basic idea of these techniques is to reduce the effect of any particular observation. These techniques are useful in reducing the effect of both outliers and high leverage points. This tactic may be viewed as intermediate between one extreme procedure, ignoring the effect of unusual points, and another extreme, giving unusual points full credibility by deleting them from the data set. The word robust is meant to suggest that these estimation methodologies are “healthy” even when attacked by an occasional bad observation (a germ). We have seen that this is not true for least squares estimation. 5.4.2 Cook’s Distance To quantify the influence of a point, a measure that considers both the response and explanatory variables is Cook’s Distance. This distance, \\(D_i\\), is defined as \\[ \\begin{array}{ll} D_i &amp;= \\frac{\\sum_{j=1}^{n} (\\hat{y}_j - \\hat{y}_{j(i)})^2}{(k+1) s^2} \\tag{5.4} \\\\ &amp;= \\left( \\frac{e_i}{se(e_i)} \\right)^2 \\frac{h_{ii}}{(k+1)(1 - h_{ii})}. \\end{array} \\] The first expression provides a definition. Here, \\(\\hat{y}_{j(i)}\\) is the prediction of the \\(j\\)th observation, computed leaving the \\(i\\)th observation out of the regression fit. To measure the impact of the \\(i\\)th observation, we compare the fitted values with and without the \\(i\\)th observation. Each difference is then squared and summed over all observations to summarize the impact. The second equation provides another interpretation of the distance \\(D_i\\). The first part, \\(\\left( \\frac{e_i}{se(e_i)} \\right)^2\\), is the square of the \\(i\\)th standardized residual. The second part, \\(\\frac{h_{ii}}{(k+1)(1 - h_{ii})}\\), is attributable solely to the leverage. Thus, the distance \\(D_i\\) is composed of a measure for outliers times a measure for leverage. In this way, Cook’s distance accounts for both the response and explanatory variables. Section 5.10.3 establishes the validity of equation (5.4). To get an idea of the expected size of \\(D_i\\) for a point that is not unusual, recall that we expect the standardized residuals to be about one and the leverage \\(h_{ii}\\) to be about \\(\\frac{k+1}{n}\\). Thus, we anticipate that \\(D_i\\) should be about \\(\\frac{1}{n}\\). Another rule of thumb is to compare \\(D_i\\) to an \\(F\\)-distribution with \\(df_1 = k+1\\) and \\(df_2 = n - (k+1)\\) degrees of freedom. Values of \\(D_i\\) that are large compared to this distribution merit attention. Example: Outliers and High Leverage Points - Continued. To illustrate, we return to our example in Section 2.6. In this example, we considered 19 “good,” or base, points plus each of the three types of unusual points, labeled A, B, and C. Table 5.6 summarizes the calculations. Table 5.6: Measures of Three Types of Unusual Points Observation Standardized Residual \\(e / se(e)\\) Leverage \\(h\\) Cook’s Distance \\(D\\) A 4.00 0.067 0.577 B 0.77 0.550 0.363 C -4.01 0.550 9.832 As noted in Section 2.6, from the standardized residual column we see that both points A and C are outliers. To judge the size of the leverages, because there are \\(n=20\\) points, the leverages are bounded by 0.05 and 1.00 with the average leverage being \\(\\bar{h} = \\frac{2}{20} = 0.10\\). Using 0.3 (\\(= 3 \\times \\bar{h}\\)) as a cut-off, both points B and C are high leverage points. Note that their values are the same. This is because, from Figure 2.7, the values of the explanatory variables are the same and only the response variable has been changed. The column for Cook’s distance captures both types of unusual behavior. Because the typical value of \\(D_i\\) is \\(\\frac{1}{n}\\) or 0.05, Cook’s distance provides one statistic to alert us to the fact that each point is unusual in one respect or another. In particular, point C has a very large \\(D_i\\), reflecting the fact that it is both an outlier and a high leverage point. The 95th percentile of an \\(F\\)-distribution with \\(df_1 = 2\\) and \\(df_2 = 18\\) is 3.555. The fact that point C has a value of \\(D_i\\) that well exceeds this cut-off indicates the substantial influence of this point. 5.5 Collinearity 5.5.1 What is Collinearity? Collinearity, or multicollinearity, occurs when one explanatory variable is, or nearly is, a linear combination of the other explanatory variables. Intuitively, with collinear data it is useful to think of explanatory variables as being highly correlated with one another. If an explanatory variable is collinear, then the question arises as to whether it is redundant, that is, whether the variable provides little additional information over and above the information in the other explanatory variables. The issues are: Is collinearity important? If so, how does it affect our model fit and how do we detect it? To address the first question, consider a somewhat pathological example. Example: Perfectly Correlated Explanatory Variables. Joe Finance was asked to fit the model \\(\\mathrm{E} ~y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) to a data set. His resulting fitted model was \\(\\hat{y} = -87 + x_1 + 18 x_2.\\) The data set under consideration is: \\[ \\begin{array}{l|cccc} \\hline i &amp; 1 &amp; 2 &amp; 3 &amp; 4 \\\\ \\hline y_i &amp; 23 &amp; 83 &amp; 63 &amp; 103 \\\\ x_{i1} &amp; 2 &amp; 8 &amp;6 &amp; 10 \\\\ x_{i2} &amp; 6 &amp; 9 &amp; 8 &amp; 10 \\\\ \\hline \\end{array} \\] Joe checked the fit for each observation. Joe was very happy because he fit the data perfectly! For example, for the third observation the fitted value is \\(\\hat{y}_3 = -87 + 6 + 18 \\times 8 = 63\\) which is equal to the third response, \\(y_3\\). Because the response equals the fitted value, the residual is zero. You may check that this is true of each observation and thus the \\(R^2\\) turned out to be \\(100\\%\\). However, Jane Actuary came along and fit the model \\(\\hat{y} = -7 + 9 x_1 + 2 x_2.\\) Jane performed the same careful checks that Joe did and also got a perfect fit (\\(R^2 = 1\\)). Who is right? The answer is both and neither one. There are, in fact, an infinite number of fits. This is due to the perfect relationship \\(x_2 = 5 + \\frac{x_1}{2}\\) between the two explanatory variables. This example illustrates some important facts about collinearity. Collinearity Facts Collinearity neither precludes us from getting good fits nor from making predictions of new observations. Note that in the above example we got perfect fits. Estimates of error variances and, therefore, tests of model adequacy, are still reliable. In cases of serious collinearity, standard errors of individual regression coefficients are larger than cases where, other things equal, serious collinearity does not exist. With large standard errors, individual regression coefficients may not be meaningful. Further, because a large standard error means that the corresponding \\(t\\)-ratio is small, it is difficult to detect the importance of a variable. To detect collinearity, begin with a matrix of correlation coefficients of the explanatory variables. This matrix is simple to create, easy to interpret, and quickly captures linear relationships between pairs of variables. A scatterplot matrix provides a visual reinforcement of the summary statistics in the correlation matrix. 5.5.2 Variance Inflation Factors Correlation and scatterplot matrices capture only relationships between pairs of variables. To capture more complex relationships among several variables, we introduce the variance inflation factor (VIF). To define a VIF, suppose that the set of explanatory variables is labeled \\(x_1, x_2, \\ldots, x_{k}\\). Now, run the regression using \\(x_j\\) as the “response” and the other \\(x\\)’s (\\(x_1, x_2, \\ldots, x_{j-1}, x_{j+1}, \\ldots, x_{k}\\)) as the explanatory variables. Denote the coefficient of determination from this regression by \\(R_j^2\\). We interpret \\(R_j = \\sqrt{R_j^2}\\) as the multiple correlation coefficient between \\(x_j\\) and linear combinations of the other \\(x\\)’s. From this coefficient of determination, we define the variance inflation factor \\[ VIF_j = \\frac{1}{1 - R_j^2}, \\text{ for } j = 1, 2, \\ldots, k. \\] A larger \\(R_j^2\\) results in a larger \\(VIF_j\\); this means greater collinearity between \\(x_j\\) and the other \\(x\\)’s. Now, \\(R_j^2\\) alone is enough to capture the linear relationship of interest. However, we use \\(VIF_j\\) in lieu of \\(R_j^2\\) as our measure for collinearity because of the algebraic relationship \\[ se(b_j) = s \\frac{\\sqrt{VIF_j}}{s_{x_j} \\sqrt{n - 1}}. \\tag{5.5} \\] Here, \\(se(b_j)\\) and \\(s\\) are standard errors and residual standard deviation from a full regression fit of \\(y\\) on \\(x_1, \\ldots, x_{k}\\). Further, \\(s_{x_j} = \\sqrt{(n - 1)^{-1} \\sum_{i=1}^{n} (x_{ij} - \\bar{x}_j)^2 }\\) is the sample standard deviation of the \\(j\\)th variable \\(x_j\\). Section 5.10.3 provides a verification of equation (5.5). Thus, a larger \\(VIF_j\\) results in a larger standard error associated with the \\(j\\)th slope, \\(b_j\\). Recall that \\(se(b_j)\\) is \\(s\\) times the square root of the \\((j+1)\\)st diagonal element of \\((\\mathbf{X}^{\\prime} \\mathbf{X})^{-1}\\). The idea is that when collinearity occurs, the matrix \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\) has properties similar to the number zero. When we attempt to calculate the inverse of \\(\\mathbf{X}^{\\prime} \\mathbf{X}\\), this is analogous to dividing by zero for scalar numbers. As a rule of thumb, when \\(VIF_j\\) exceeds 10 (which is equivalent to \\(R_j^2 &gt; 90\\%\\)), we say that severe collinearity exists. This may signal a need for action. Tolerance, defined as the reciprocal of the variance inflation factor, is another measure of collinearity used by some analysts. For example, with \\(k = 2\\) explanatory variables in the model, then \\(R_1^2\\) is the squared correlation between the two explanatory variables, say \\(r_{12}^2\\). Then, from the equation above, we have that \\[ se(b_j) = s \\left(s_{x_j} \\sqrt{n - 1} \\right)^{-1} \\left(1 - r_{12}^2 \\right)^{-1/2}, \\text{ for } j = 1, 2. \\] As the correlation approaches one in absolute value, \\(|r_{12}| \\rightarrow 1\\), then the standard error becomes large meaning that the corresponding \\(t\\)-statistic becomes small. In summary, a high \\(VIF\\) may mean small \\(t\\)-statistics even though variables are important. Further, one can check that the correlation between \\(b_1\\) and \\(b_2\\) is \\(-r_{12}\\), indicating that the coefficient estimates are highly correlated. Example: Stock Market Liquidity - Continued. As an example, consider a regression of VOLUME on PRICE, SHARE, and VALUE. Unlike the explanatory variables considered in Section 5.5.3, these three explanatory variables are not measures of trading activity. From a regression fit, we have \\(R^2 = 61\\%\\) and \\(s = 6.72\\). The statistics associated with the regression coefficients are in Table 5.7. Table 5.7: Statistics from a regression of VOLUME on PRICE, SHARE and VALUE \\(x_j\\) \\(s_{x_j}\\) \\(b_j\\) \\(se(b_j)\\) \\(t(b_j)\\) \\(VIF_j\\) PRICE 21.370 -0.022 0.035 -0.63 1.5 SHARE 115.100 0.054 0.010 5.19 3.8 VALUE 8.157 0.313 0.162 1.94 4.7 You may check that the relationship in equation (5.5) is valid for each of the explanatory variables in Table 5.7. Because each \\(VIF\\) statistic is less than ten, there is little reason to suspect severe collinearity. This is interesting because you may recall that there is a perfect relationship between PRICE, SHARE, and VALUE in that we defined the market value to be VALUE = PRICE \\(\\times\\) SHARE. However, the relationship is multiplicative, and hence is nonlinear. Because the variables are not linearly related, it is valid to enter all three into the regression model. From a financial perspective, the variable VALUE is important because it measures the worth of a firm. From a statistical perspective, the variable VALUE quantifies the interaction between PRICE and SHARE (interaction variables were introduced in Section 3.5.3). For collinearity, we are only interested in detecting linear trends, so nonlinear relationships between variables are not an issue here. For example, we have seen that it is sometimes useful to retain both an explanatory variable \\(x\\) and its square \\(x^2\\), despite the fact that there is a perfect (nonlinear) relationship between the two. Still, we must check that nonlinear relationships are not approximately linear over the sampling region. Even though the relationship is theoretically nonlinear, if it is close to linear for our available sample, then problems of collinearity might arise. Figure 5.5 illustrates this situation. Figure 5.5: The relationship between \\(x_1\\) and \\(x_2\\) is nonlinear. However, over the region sampled, the variables have close to a linear relationship. What can we do in the presence of collinearity? One option is to center each variable, by subtracting its average and dividing by its standard deviation. For example, create a new variable \\(x_{ij}^{\\ast} = (x_{ij} - \\bar{x}_j) / s_{x_j}\\). Occasionally, one variable appears as millions of units and another variable appears as fractions of units. Compared to the first mentioned variable, the second variable is close to a constant column of zeros (in that computers typically retain a finite number of digits). If this is true, then the second variable looks very much like a linear shift of the constant column of ones corresponding to the intercept. This is a problem because, with the least squares operations, we are implicitly squaring numbers that can make these columns appear even more similar. This problem is simply a computational one and is easy to rectify. Simply recode the variables so that the units are of similar order of magnitude. Some data analysts automatically center all variables to avoid these problems. This is a legitimate approach because regression techniques search for linear relationships; location and scale shifts do not affect linear relationships. Another option is to simply not explicitly account for collinearity in the analysis but to discuss some of its implications when interpreting the results of the regression analysis. This approach is probably the most commonly adopted one. It is a fact of life that, when dealing with business and economic data, collinearity does tend to exist among variables. Because the data tends to be observational in lieu of experimental in nature, there is little that the analyst can do to avoid this situation. In the best-case situation, an auxiliary variable that provides similar information and that eases the collinearity problem is available to replace a variable. Similar to our discussion of high leverage points, a transformed version of the explanatory variable may also be a useful substitute. In some situations, such an ideal replacement is not available and we are forced to remove one or more variables. Deciding which variables to remove is a difficult choice. When deciding among variables, often the choice will be dictated by the investigator’s judgement as to which is the most relevant set of variables. 5.5.3 Collinearity and Leverage Measures of collinearity and leverage share common characteristics and yet are designed to capture different aspects of a data set. Both are useful for data and model criticism; they are applied after a preliminary model fit with the objective of improving model specification. Further, both are calculated using only the explanatory variables; values of the responses do not enter into either calculation. Our measure of collinearity, the variance inflation factor, is designed to help with model criticism. It is a measure calculated for each explanatory variable, designed to explain the relationship with other explanatory variables. The leverage statistic is designed to help us with data criticism. It is a measure calculated for each observation to help us explain how unusual an observation is with respect to other observations. Collinearity may be masked, or induced, by high leverage points, as pointed out by Mason and Gunst (1985) and Hadi (1988). Figures 5.6 and 5.7 provide illustrations of each case. These simple examples underscore an important point: data criticism and model criticism are not separate exercises. Figure 5.6: With the exception of the marked point, \\(x_1\\) and \\(x_2\\) are highly linearly related. Figure 5.7: The highly linear relationship between \\(x_1\\) and \\(x_2\\) is primarily due to the marked point. The examples in Figures 5.6 and 5.7 also help us to see one way in which high leverage points may affect standard errors of regression coefficients. Recall, in Section 5.4.1, we saw that high leverage points may affect the model fitted values. In Figures 5.6 and 5.7, we see that high leverage points affect collinearity. Thus, from equation (5.5), we have that high leverage points can also affect our standard errors of regression coefficients. 5.5.4 Suppressor Variables As we have seen, severe collinearity can seriously inflate standard errors of regression coefficients. Because we rely on these standard errors for judging the usefulness of explanatory variables, our model selection procedures and inferences may be deficient in the presence of severe collinearity. Despite these drawbacks, mild collinearity in a data set should not be viewed as a deficiency of the data set; it is simply an attribute of the available explanatory variables. Even if one explanatory variable is nearly a linear combination of the others, that does not necessarily mean that the information that it provides is redundant. To illustrate, we now consider a suppressor variable, an explanatory variable that increases the importance of other explanatory variables when included in the model. Example: Suppressor Variable. Figure 5.8 shows a scatterplot matrix of a hypothetical data set of fifty observations. This data set contains a response and two explanatory variables. Table 5.8 provides the corresponding matrix of correlation coefficients. Here, we see that the two explanatory variables are highly correlated. Now recall, for regression with one explanatory variable, that the correlation coefficient squared is the coefficient of determination. Thus, using Table 5.8, for a regression of \\(y\\) on \\(x_1\\), the coefficient of determination is \\((0.188)^2 = 3.5\\%\\). Similarly, for a regression of \\(y\\) on \\(x_2\\), the coefficient of determination is \\((-0.022)^2 = 0.04\\%\\). However, for a regression of \\(y\\) on \\(x_1\\) and \\(x_2\\), the coefficient of determination turns out to be a surprisingly high \\(80.7\\%\\). The interpretation is that individually, both \\(x_1\\) and \\(x_2\\) have little impact on \\(y\\). However, when taken jointly, the two explanatory variables have a significant effect on \\(y\\). Although Table 5.8 shows that \\(x_1\\) and \\(x_2\\) are strongly linearly related, this relationship does not mean that \\(x_1\\) and \\(x_2\\) provide the same information. In fact, in this example the two variables complement one another. Figure 5.8: Scatterplot matrix of a response and two explanatory variables for the suppressor variable example Table 5.8: Correlation Matrix for the Suppressor Example \\(x_1\\) \\(x_2\\) \\(x_2\\) 0.972 \\(y\\) 0.188 -0.022 5.5.5 Orthogonal Variables Another way to understand the impact of collinearity is to study the case when there are no relationships among sets of explanatory variables. Mathematically, two matrices \\(\\mathbf{X}_1\\) and \\(\\mathbf{X}_2\\) are said to be orthogonal if \\(\\mathbf{X}_1^{\\prime} \\mathbf{X}_2 = \\mathbf{0}\\). Intuitively, because we generally work with centered variables (with zero averages), this means that each column of \\(\\mathbf{X}_1\\) is uncorrelated with each column of \\(\\mathbf{X}_2\\). Although unlikely to occur with observational data in the social sciences, when designing experimental treatments or constructing high degree polynomials, applications of orthogonal variables are regularly used (see for example, Hocking, 2003). For our purposes, we will work with orthogonal variables simply to understand the logical consequences of a total lack of collinearity. Suppose that \\(\\mathbf{x}_2\\) is an explanatory variable that is orthogonal to \\(\\mathbf{X}_1\\), where \\(\\mathbf{X}_1\\) is a matrix of explanatory variables that includes the intercept. Then, it is straightforward to check that the addition of \\(\\mathbf{x}_2\\) to the regression equation does not change the fit for coefficients corresponding to \\(\\mathbf{X}_1\\). That is, without \\(\\mathbf{x}_2\\), the coefficients corresponding to \\(\\mathbf{X}_1\\) would be calculated as \\(\\mathbf{b}_1 = \\left(\\mathbf{X}_1^{\\prime} \\mathbf{X}_1 \\right)^{-1} \\mathbf{X}_1^{\\prime} \\mathbf{y}\\). Using the orthogonal \\(\\mathbf{x}_2\\) as part of the least squares calculation would not change the result for \\(\\mathbf{b}_1\\) (see the recursive least squares calculation in Section 4.7.2). Further, the variance inflation factor for \\(\\mathbf{x}_2\\) is 1, indicating that the standard error is unaffected by the other explanatory variables. In the same vein, the reduction in the error sum of squares by adding the orthogonal variable \\(\\mathbf{x}_2\\) is due only to that variable, and not its interaction with other variables in \\(\\mathbf{X}_1\\). Orthogonal variables can be created for observational social science data (as well as other collinear data) using the method of principal components. With this method, one uses a linear transformation of the matrix of explanatory variables of the form, \\(\\mathbf{X}^{\\ast} = \\mathbf{X} \\mathbf{P}\\), so that the resulting matrix \\(\\mathbf{X}^{\\ast}\\) is composed of orthogonal columns. The transformed regression function is \\(\\mathrm{E~}\\mathbf{y} = \\mathbf{X} \\boldsymbol \\beta = \\mathbf{X} \\mathbf{P} \\mathbf{P}^{-1} \\boldsymbol \\beta = \\mathbf{X}^{\\ast} \\boldsymbol \\beta^{\\ast}\\), where \\(\\boldsymbol \\beta^{\\ast} = \\mathbf{P}^{-1} \\boldsymbol \\beta\\) is the set of new regression coefficients. Estimation proceeds as before, with the orthogonal set of explanatory variables. By choosing the matrix \\(\\mathbf{P}\\) appropriately, each column of \\(\\mathbf{X}^{\\ast}\\) has an identifiable contribution. Thus, we can readily use variable selection techniques to identify the “principal components” portions of \\(\\mathbf{X}^{\\ast}\\) to use in the regression equation. Principal components regression is a widely used method in some application areas, such as psychology. It can easily address highly collinear data in a disciplined manner. The main drawback of this technique is that the resulting parameter estimates are difficult to interpret. 5.6 Selection Criteria 5.6.1 Goodness of Fit How well does the model fit the data? Criteria that measure the proximity of the fitted model and realized data are known as goodness of fit statistics. Specifically, we interpret the fitted value \\(\\hat{y}_i\\) to be the best model approximation of the \\(i\\)th observation and compare it to the actual value \\(y_i\\). In linear regression, we examine the difference through the residual \\(e_i = y_i - \\hat{y}_i\\); small residuals imply a good model fit. We have quantified this through the size of the typical error \\((s)\\), including the coefficient of determination \\((R^2)\\) and an adjusted version \\((R_{a}^2)\\). For nonlinear models, we will need additional measures, and it is helpful to introduce these measures in this simpler linear case. One such measure is Akaike’s Information Criterion that will be defined in terms of likelihood fits in Section 11.9.4. For linear regression, it reduces to \\[ AIC = n \\ln (s^2) + n \\ln (2 \\pi) + n + 3 + k. \\tag{5.6} \\] For model comparison, the smaller the \\(AIC\\), the better is the fit. Comparing models with the same number of variables (\\(k\\)) means that selecting a model with small values of \\(AIC\\) leads to the same choice as selecting a model with small values of the residual standard deviation \\(s\\). Further, a small number of parameters means a small value of \\(AIC\\), other things being equal. The idea is that this measure balances the fit (\\(n \\ln (s^2)\\)) with a penalty for complexity (the number of parameters, \\(k+2\\)). Statistical packages often omit constants such as \\(n \\ln (2 \\pi)\\) and \\(n+3\\) when reporting \\(AIC\\) because they do not matter when comparing models. Section 11.9.4 will introduce another measure, the Bayes Information Criterion (\\(BIC\\)), that gives a smaller weight to the penalty for complexity. A third goodness of fit measure that is used in linear regression models is the \\(C_p\\) statistic. To define this statistic, assume that we have available \\(k\\) explanatory variables \\(x_1, ..., x_{k}\\) and run a regression to get \\(s_{full}^2\\) as the mean square error. Now, suppose that we are considering using only \\(p-1\\) explanatory variables so that there are \\(p\\) regression coefficients. With these \\(p-1\\) explanatory variables, we run a regression to get the error sum of squares \\((Error~SS)_p\\). Thus, we are in the position to define \\[ C_{p} = \\frac{(Error~SS)_p}{s_{full}^2} - n + 2p. \\] As a selection criterion, we choose the model with a “small” \\(C_{p}\\) coefficient, where small is taken to be relative to \\(p\\). In general, models with smaller values of \\(C_{p}\\) are more desirable. Like the \\(AIC\\) and \\(BIC\\) statistics, the \\(C_{p}\\) statistic strikes a balance between the model fit and complexity. That is, each statistic summarizes the trade-off between model fit and complexity, although with different weights. For most data sets, they recommend the same model and so an analyst can report any or all three statistics. However, for some applications, they lead to different recommended models. In this case, the analyst needs to rely more heavily on non-data driven criteria for model selection (which are always important in any regression application). 5.6.2 Model Validation Model validation is the process of confirming that our proposed model is appropriate, especially in light of the purposes of the investigation. Recall the iterative model formulation selection process described in Section 5.1. An important criticism of this iterative process is that it is guilty of data-snooping, that is, fitting a great number of models to a single set of data. As we saw in Section 5.2 on data-snooping in stepwise regression, by looking at a large number of models we may overfit the data and understate the natural variation in our representation. We can respond to this criticism by using a technique called out-of-sample validation. The ideal situation is to have available two sets of data, one for model development and one for model validation. We initially develop one, or several, models on a first data set. The models developed from the first set of data are called our candidate models. Then, the relative performance of the candidate models could be measured on a second set of data. In this way, the data used to validate the model is unaffected by the procedures used to formulate the model. Unfortunately, rarely will two sets of data be available to the investigator. However, we can implement the validation process by splitting the data set into two subsamples. We call these the model development and validation subsamples, respectively. They are also known as training and testing samples, respectively. To see how the process works in the linear regression context, consider the following procedure. Out-of-Sample Validation Procedure Begin with a sample size of \\(n\\) and divide it into two subsamples, called the model development and validation subsamples. Let \\(n_1\\) and \\(n_2\\) denote the size of each subsample. In cross-sectional regression, do this split using a random sampling mechanism. Use the notation \\(i=1,...,n_1\\) to represent observations from the model development subsample and \\(i=n_1+1,...,n_1+n_2=n\\) for the observations from the validation subsample. Figure 5.9 illustrates this procedure. Using the model development subsample, fit a candidate model to the data set \\(i=1,...,n_1\\). Using the model created in Step (ii) and the explanatory variables from the validation subsample, “predict” the dependent variables in the validation subsample, \\(\\hat{y}_i\\), where \\(i=n_1+1,...,n_1+n_2\\). (To get these predictions, you may need to transform the dependent variables back to the original scale.) Assess the proximity of the predictions to the held-out data. One measure is the sum of squared prediction errors \\[ SSPE = \\sum_{i=n_1+1}^{n_1+n_2} (y_i - \\hat{y}_i)^2 . \\tag{5.7} \\] Repeat Steps (ii) through (iv) for each candidate model. Choose the model with the smallest SSPE. Figure 5.9: For model validation, a data set of size \\(n\\) is randomly split into two subsamples There are a number of criticisms of the SSPE. First, it is clear that it takes a considerable amount of time and effort to calculate this statistic for each of several candidate models. However, as with many statistical techniques, this is merely a matter of having specialized statistical software available to perform the steps described above. Second, because the statistic itself is based on a random subset of the sample, its value will vary from analyst to analyst. This objection could be overcome by using the first \\(n_1\\) observations from the sample. In most applications, this is not done in case there is a lurking relationship in the order of the observations. Third, and perhaps most important, is the fact that the choice of the relative subset sizes, \\(n_1\\) and \\(n_2\\), is not clear. Various researchers recommend different proportions for the allocation. Snee (1977) suggests that data-splitting not be done unless the sample size is moderately large, specifically, \\(n \\geq 2(k+1) + 20\\). The guidelines of Picard and Berk (1990) show that the greater the number of parameters to be estimated, the greater the proportion of observations needed for the model development subsample. As a rule of thumb, for data sets with 100 or fewer observations, use about 25-35% of the sample for out-of-sample validation. For data sets with 500 or more observations, use 50% of the sample for out-of-sample validation. Hastie, Tibshirani, and Friedman (2001) remark that a typical split is 50% for development/training, 25% for validation, and the remaining 25% for a third stage for further validation that they call testing. Because of these criticisms, several variants of the basic out-of-sample validation process are used by analysts. Although there is no theoretically best procedure, it is widely agreed that model validation is an important part of confirming the usefulness of a model. 5.6.3 Cross-Validation Cross-validation is the technique of model validation that splits the data into two disjoint sets. Section 5.6.2 discussed out-of-sample validation where the data was split randomly into two subsets both containing a sizeable percentage of data. Another popular method is leave-one-out cross-validation, where the validation sample consists of a single observation and the development sample is based on the remainder of the data set. Especially for small sample sizes, an attractive leave-one-out cross-validation statistic is PRESS, the Predicted Residual Sum of Squares. To define the statistic, consider the following procedure where we suppose that a candidate model is available. PRESS Validation Procedure From the full sample, omit the \\(i\\)th point and use the remaining \\(n-1\\) observations to compute regression coefficients. Use the regression coefficients computed in step one and the explanatory variables for the \\(i\\)th observation to compute the predicted response, \\(\\hat{y}_{(i)}\\). This part of the procedure is similar to the calculation of the SSPE statistic with \\(n_1=n-1\\) and \\(n_2=1\\). Now, repeat (i) and (ii) for \\(i=1,...,n\\). Summarizing, define \\[ PRESS = \\sum_{i=1}^{n} (y_i - \\hat{y}_{(i)})^2 . \\tag{5.8} \\] As with SSPE, this statistic is calculated for each of several competing models. Under this criterion, we choose the model with the smallest PRESS. Based on this definition, the statistic seems very computationally intensive in that it requires \\(n\\) regression fits to evaluate it. To address this, interested readers will find that Section 5.10.2 establishes \\[ y_i - \\hat{y}_{(i)} = \\frac{e_i}{1 - h_{ii}} . \\tag{5.9} \\] Here, \\(e_i\\) and \\(h_{ii}\\) represent the \\(i\\)th residual and leverage from the regression fit using the complete data set. This yields \\[ PRESS = \\sum_{i=1}^{n} \\left( \\frac{e_i}{1 - h_{ii}} \\right)^2 , \\tag{5.10} \\] which is a much easier computational formula. Thus, the PRESS statistic is less computationally intensive than SSPE. Another important advantage of this statistic, when compared to SSPE, is that we do not need to make an arbitrary choice as to our relative subset sizes split. Indeed, because we are performing an “out-of-sample” validation for each observation, it can be argued that this procedure is more efficient, an especially important consideration when the sample size is small (say, less than 50 observations). A disadvantage is that because the model is re-fit for each point deleted, PRESS does not enjoy the appearance of independence between the estimation and prediction aspects, unlike SSPE. 5.7 Heteroscedasticity In most regression applications, the goal is to understand determinants of the regression function \\(\\mathrm{E~}y_i = \\mathbf{x}_i^{\\prime} \\boldsymbol \\beta = \\mu_i\\). Our ability to understand the mean is strongly influenced by the amount of spread from the mean that we quantify using the variance \\(\\mathrm{E}\\left(y_i - \\mu_i\\right)^2\\). In some applications, such as when I weigh myself on a scale, there is relatively little variability; repeated measurements yield almost the same result. In other applications, such as the time it takes me to fly to New York, repeated measurements yield substantial variability and are fraught with inherent uncertainty. The amount of uncertainty can also vary on a case-by-case basis. We denote the case of “varying variability” with the notation \\(\\sigma_i^2 = \\mathrm{E}\\left(y_i - \\mu_i\\right)^2\\). When the variability varies by observation, this is known as heteroscedasticity for “different scatter.” In contrast, the usual assumption of common variability (assumption E3/F3 in Section 3.2) is called homoscedasticity, meaning “same scatter.” Our estimation strategies depend on the extent of heteroscedasticity. For datasets with only a mild amount of heteroscedasticity, one can use least squares to estimate the regression coefficients, perhaps combined with an adjustment for the standard errors (described in Section 5.7.2). This is because least squares estimators are unbiased even in the presence of heteroscedasticity (see Property 1 in Section 3.2). However, with heteroscedastic dependent variables, the Gauss-Markov theorem no longer applies and so the least squares estimators are not guaranteed to be optimal. In cases of severe heteroscedasticity, alternative estimators are used, the most common being those based on transformations of the dependent variable, as will be described in Section 5.7.4. 5.7.1 Detecting Heteroscedasticity To decide a strategy for handling potential heteroscedasticity, we must first assess, or detect, its presence. To detect heteroscedasticity graphically, a good idea is to perform a preliminary regression fit of the data and plot the residuals versus the fitted values. To illustrate, Figure 5.10 is a plot of a fictitious data set with one explanatory variable where the scatter increases as the explanatory variable increases. A least squares regression was performed - residuals and fitted values were computed. Figure 5.11 is an example of a plot of residuals versus fitted values. The preliminary regression fit removes many of the major patterns in the data and leaves the eye free to concentrate on other patterns that may influence the fit. We plot residuals versus fitted values because the fitted values are an approximation of the expected value of the response and, in many situations, the variability grows with the expected response. Figure 5.10: The shaded area represents the data. The line is the true regression line. Figure 5.11: Residuals plotted versus the fitted values for the data in Figure 5.10. More formal tests of heteroscedasticity are also available in the regression literature. To illustrate, let us consider a test due to Breusch and Pagan (1980). Specifically, this test examines the alternative hypothesis \\(H_a\\): $ y_i = ^2 + _i^{} $, where \\(\\mathbf{z}_i\\) is a known vector of variables and \\(\\boldsymbol \\gamma\\) is a \\(p\\)-dimensional vector of parameters. Thus, the null hypothesis is \\(H_0:~ \\boldsymbol \\gamma = \\mathbf{0}\\) is equivalent to homoscedasticity, \\(\\mathrm{Var~} y_i = \\sigma^2.\\) Procedure to Test for Heteroscedasticity Fit a regression model and calculate the model residuals, \\(e_i\\). Calculate squared standardized residuals, \\(e_i^{\\ast 2} = e_i^2 / s^2\\). Fit a regression model of \\(e_i^{\\ast 2}\\) on \\(\\mathbf{z}_i\\). The test statistic is \\(LM = \\frac{\\text{Regress~SS}_z}{2}\\), where \\(Regress~SS_z\\) is the regression sum of squares from the model fit in step (iii). Reject the null hypothesis if \\(LM\\) exceeds a percentile from a chi-square distribution with \\(p\\) degrees of freedom. The percentile is one minus the significance level of the test. Here, we use \\(LM\\) to denote the test statistic because Breusch and Pagan derived it as a Lagrange multiplier statistic; see Breusch and Pagan (1980) for more details. 5.7.2 Heteroscedasticity-Consistent Standard Errors For data sets with only mild heteroscedasticity, a sensible strategy is to employ least squares estimators of the regression coefficients and to adjust the calculation of standard errors to account for the heteroscedasticity. From the Section 3.2 on properties, we saw that least squares regression coefficients could be written as \\(\\mathbf{b} = \\sum_{i=1}^n \\mathbf{w}_i y_i,\\) where \\(\\mathbf{w}_i = \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1} \\mathbf{x}_i\\). Thus, with \\(\\sigma_i^2 = \\mathrm{Var~} y_i\\), we have \\[ \\mathrm{Var~}\\mathbf{b} = \\sum_{i=1}^n \\mathbf{w}_i \\mathbf{w}_i^{\\prime} \\sigma_i^2 = \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1} \\left( \\sum_{i=1}^n \\sigma_i^2 \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\right) \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1}. \\tag{5.11} \\] This quantity is known except for \\(\\sigma_i^2\\). We can compute residuals using the least squares regression coefficients as \\(e_i = y_i - \\mathbf{x}_i^{\\prime} \\mathbf{b}\\). With these, we may define the empirical, or robust, estimate of the variance-covariance matrix as \\[ \\widehat{\\mathrm{Var~}\\mathbf{b}} = \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1} \\left( \\sum_{i=1}^n e_i^2 \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\right) \\left( \\mathbf{X}^{\\prime}\\mathbf{X} \\right)^{-1}. \\] The corresponding “heteroscedasticity-consistent” standard errors are \\[ se_r(b_j) = \\sqrt{(j+1)^{\\text{st}}~ \\text{diagonal element of }\\widehat{\\mathrm{Var~}\\mathbf{b}}}. \\tag{5.12} \\] The logic behind this estimator is that each squared residual, \\(e_i^2\\), may be a poor estimate of \\(\\sigma_i^2\\). However, our interest is estimating a (weighted) sum of variances in equation (5.11); estimating the sum is a much easier task than estimating any individual variance estimate. Robust, or heteroscedasticity-consistent, standard errors are widely available in statistical software packages. Here, you will also see alternative definitions of residuals employed, as in Section 5.3.1. If your statistical package offers options, the robust estimator using studentized residuals is generally preferred. 5.7.3 Weighted Least Squares The least squares estimators are less useful for datasets with severe heteroscedasticity. One strategy is to use a variation of least squares estimation by weighting observations. The idea is that, when minimizing the sum of squared errors using heteroscedastic data, the expected variability of some observations is smaller than others. Intuitively, it seems reasonable that the smaller the variability of the response, the more reliable that response and the greater weight that it should receive in the minimization procedure. Weighted least squares is a technique that accounts for this “varying variability.” Specifically, we use Section 3.2.3 assumptions E1, E2, and E4, with E3 replaced by E \\(\\varepsilon_i = 0\\) and \\(\\text{Var} \\varepsilon_i = \\sigma^2 / w_i\\), so that the variability is proportional to a known weight \\(w_i\\). For example, if unit of analysis \\(i\\) represents a geographical entity such as a state, you might use the number of people in the state as a weight. Or, if \\(i\\) represents a firm, you might use firm assets for the weighting variable. Larger values of \\(w_i\\) indicate a more precise response variable through the smaller variability. In actuarial applications, weights are used to account for an exposure such as the amount of insurance premium, number of employees, size of the payroll, number of insured vehicles, and so forth (further discussion is in Chapter 18). This model can be readily converted to the “ordinary” least squares problem by multiplying all regression variables by \\(\\sqrt{w_i}\\). That is, if we define \\(y_i^{\\ast} = y_i \\times \\sqrt{w_i}\\) and \\(x_{ij}^{\\ast} = x_{ij} \\times \\sqrt{w_i}\\), then from assumption E1 we have \\[ y_i^{\\ast} = y_i \\times \\sqrt{w_i} = \\left( \\beta_0 x_{i0} + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik} + \\varepsilon_i \\right) \\sqrt{w_i} = \\beta_0 x_{i0}^{\\ast} + \\beta_1 x_{i1}^{\\ast} + \\ldots + \\beta_k x_{ik}^{\\ast} + \\varepsilon_i^{\\ast} \\] where \\(\\varepsilon_i^{\\ast} = \\varepsilon_i \\times \\sqrt{w_i}\\) has homoscedastic variance \\(\\sigma^2\\). Thus, with the rescaled variables, all inference can proceed as before. This work has been automated in statistical packages where the user merely specifies the weights \\(w_i\\) and the package does the rest. In terms of matrix algebra, this procedure can be accomplished by defining an \\(n \\times n\\) weight matrix \\(\\mathbf{W} = \\text{diag}(w_i)\\) so that the \\(i\\)th diagonal element of \\(\\mathbf{W}\\) is \\(w_i\\). Extending equation (3.14) for example, the weighted least squares estimates can be expressed as \\[ \\mathbf{b}_{WLS} = \\left( \\mathbf{X}^{\\prime} \\mathbf{W} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{W} \\mathbf{y}. \\tag{5.13} \\] Additional discussions of weighted least squares estimation will be presented in Section 15.1.1. 5.7.4 Transformations Another approach that handles severe heteroscedasticity, introduced in Section 1.3, is to transform the dependent variable, typically with a logarithmic transformation of the form \\(y^{\\ast} = \\ln y\\). As we saw in Section 1.3, transformations can serve to “shrink” spread out data and symmetrize a distribution. Through a change of scale, a transformation also changes the variability, potentially altering a heteroscedastic dataset into a homoscedastic one. This is both a strength and limitation of the transformation approach - a transformation simultaneously affects both the distribution and the heteroscedasticity. Power transformations, such as the logarithmic transform, are most useful when the variability of the data grows with the mean. In this case, the transform will serve to “shrink” the data to a scale that appears to be homoscedastic. Conversely, because transformations are monotonic functions, they will not help with patterns of variability that are non-monotonic. Further, if your data is reasonably symmetric but heteroscedastic, a transformation will not be useful because any choice that mitigates the heteroscedasticity will skew the distribution. When data are non-positive, it is common to add a constant to each observation so that all observations are positive prior to transformation. For example, the transform \\(\\ln(1+y)\\) accommodates the presence of zeros. One can also multiply by a constant so that the approximate original units are retained. For example, the transform \\(100 \\ln(1 + y/100)\\) may be applied to percentage data where negative percentages sometimes appear. Our discussions of transformations have focused on transforming dependent variables. As noted in Section 3.5, transformations of explanatory variables are also possible. This is because the regression assumptions condition on explanatory variables (Section 3.2.3). Some analysts prefer to transform variables to approximate normality, thinking of multivariate normal distributions as a foundation for regression analysis. Others are reluctant to transform explanatory variables because of the difficulties in interpreting resulting models. The approach taken here is to use transforms that can be readily interpretable, such as those introduced in Section 3.5. Other transforms are certainly candidates to include in a selected model but they should provide substantial dividends in terms of fit or predictive power if they are difficult to communicate. 5.8 Further Reading and References Long and Ervin (2000) gather compelling evidence for the use of alternative heteroscedasticity-consistent estimators of standard errors that have better finite sample performance than the classic versions. The large sample properties of empirical estimators have been established by Eicker (1967), Huber (1967) and White (1980) in the linear regression case. For the linear regression case, MacKinnon and White (1985) suggest alternatives that provide superior small-sample properties. For small samples, the evidence is based on (1) the biasedness of the estimators, (2) their motivation as jackknife estimators and (3) their performance in simulation studies. Other measures of collinearity based on matrix algebra concepts involving eigenvalues, such as condition numbers and condition indices, are used by some analysts. See Belseley, Kuh and Welsch (1980) for a solid treatment of collinearity and regression diagnostics. Hocking (2003) provides additional background reading on collinearity and principal components. See Carroll and Ruppert (1988) for further discussions of transformations in regression. Hastie, Tibshirani and Friedman (2001) give an advanced discussion of model selection issues, focusing on predictive aspects of models in the language of machine learning. Chapter References Belseley, David A., Edwin Kuh and Roy E. Welsch (1980). Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. Wiley, New York. Bendel, R. B. and Afifi, A. A. (1977). Comparison of stopping rules in forward “stepwise” regression. Journal of the American Statistical Association 72, 46-53. Box, George E. P. (1980). Sampling and Bayes inference in scientific modeling and robustness (with discussion). Journal of the Royal Statistical Society, Series A, 143, 383-430. Breusch, T. S. and A. R. Pagan (1980). The Lagrange multiplier test and its applications to model specification in econometrics. Review of Economic Studies, 47, 239-53. Carroll, Raymond J. and David Ruppert (1988). Transformation and Weighting in Regression, Chapman-Hall. Eicker, F. (1967). Limit theorems for regressions with unequal and dependent errors. Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability 1, LeCam, L. M. and J. Neyman, editors, University of California Press, pp, 59-82. Hadi, A. S. (1988). Diagnosing collinearity-influential observations. Computational Statistics and Data Analysis 7, 143-159. Hastie, Trevor, Robert Tibshirani and Jerome Friedman (2001). The Elements of Statistical Learning: Data Mining, Inference and Prediction. Springer-Verlag, New York. Hocking, Ronald R. (2003). Methods and Applications of Linear Models: Regression and the Analysis of Variance. Wiley, New York. Huber, P. J. (1967). The behaviour of maximum likelihood estimators under non-standard conditions. Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability 1, LeCam, L. M. and Neyman, J. editors, University of California Press, pp, 221-33. Long, J.S. and L.H. Ervin (2000). Using heteroscedasticity consistent standard errors in the linear regression model. American Statistician 54, 217-224. MacKinnon, J.G. and H. White (1985). Some heteroskedasticity consistent covariance matrix estimators with improved finite sample properties. Journal of Econometrics 29, 53-57. Mason, R. L. and Gunst, R. F. (1985). Outlier-induced collinearities. Technometrics 27, 401-407. Picard, R. R. and Berk, K. N. (1990). Data splitting. The American Statistician 44, 140-147. Rencher, A. C. and Pun, F. C. (1980). Inflation of \\(R^2\\) in best subset regression. Technometrics 22, 49-53. Snee, R. D. (1977). Validation of regression models. Methods and examples. Technometrics 19, 415-428. 5.9 Exercises 5.1. You are doing regression with one explanatory variable and so consider the basic linear regression model \\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\). Show that the \\(i\\)th leverage can be simplified to \\[ h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\overline{x})^2}{(n-1) s_x^2}. \\] Show that \\(\\overline{h} = 2 / n\\). Suppose that \\(h_{ii} = 6/n\\). How many standard deviations is \\(x_i\\) away (either above or below) from the mean? 5.2. Consider the output of a regression using one explanatory variable on \\(n=3\\) observations. The residuals and leverages are: \\[ \\small{ \\begin{array}{l|ccc} \\hline i &amp; 1 &amp; 2 &amp; 3 \\\\ \\hline \\text{Residuals } e_i &amp; 3.181 &amp; -6.362 &amp; 3.181 \\\\ \\text{Leverages } h_{ii} &amp; 0.8333 &amp; 0.3333 &amp; 0.8333 \\\\ \\hline \\end{array} } \\] Compute the \\(PRESS\\) statistic. 5.3. National Life Expectancies. We continue the analysis begun in Exercises 1.7, 2.22, 3.6 and 4.7. The focus of this exercise is variable selection. Begin with the data from \\(n=185\\) countries throughout the world that have valid (non-missing) life expectancies. Plot the life expectancy versus the gross domestic product and private expenditures on health. From these plots, describe why it is desirable to use logarithmic transforms, lnGDP and lnHEALTH, respectively. Also plot life expectancy versus lnGDP and lnHEALTH to confirm your intuition. Use a stepwise regression algorithm to help you select a model. Do not consider the variables RESEARCHERS, SMOKING, and FEMALEBOSS as these have many missing values. For the remaining variables, use only the observations without any missing values. Do this twice, with and without the categorical variable EGION. Return to the full data set of \\(n=185\\) countries and run a regression model using FERTILITY, PUBLICEDUCATION, and $lnHEALTH as explanatory variables. c(i). Provide histograms of standardized residuals and leverages. c(ii). Identify the standardized residual and leverage associated with Lesotho, formerly Basutoland, a kingdom surrounded by South Africa. Is this observation an outlier, high leverage point, or both? c(iii). Re-run the regression without Lesotho. Cite any differences in the statistical coefficients between this model and the one in part c(i). 5.4. Term Life Insurance. We continue our study of Term Life Insurance Demand from Chapters 3 and 4. Specifically, we examine the 2004 Survey of Consumer Finances (SCF), a nationally representative sample that contains extensive information on assets, liabilities, income, and demographic characteristics of those sampled (potential U.S. customers). We study a random sample of 500 families with positive incomes. From the sample of 500, we initially consider a subsample of \\(n=275\\) families that purchased term life insurance. Consider a linear regression of LNINCOME, EDUCATION, NUMHH, MARSTAT, AGE, and GENDER on LNFACE}. Collinearity. Not all of the variables turned out to be statistically significant. To investigate one possible explanation, calculate variance inflation factors. a(i). Briefly explain the idea of collinearity and a variance inflation factor. a(ii). What constitutes a large variance inflation factor? a(iii). If a large variance inflation factor is detected, what possible courses of action do we have to address this aspect of the data? a(iv). Supplement the variance inflation factor statistics with a table of correlations of explanatory variables. Based on these statistics, is collinearity an issue with this fitted model? Why or why not? Unusual Points. Sometimes a poor model fit can be due to unusual points. b(i). Define the idea of leverage for an observation. b(ii). For this fitted model, give standard rules of thumb for identifying points with unusual leverage. Identify any unusual points from the attached summary statistics. b(iii). An analyst is concerned with leverage values for this fitted model and suggests using FACE as the dependent variable instead of LNFACE. Describe how leverage values would change using this alternative dependent variable. Residual Analysis. We can learn how to improve model fits from analyses of residuals. c(i). Provide a plot of residuals versus fitted values. What do we hope to learn from this type of plot? Does this plot display any model inadequacies? c(ii). Provide a \\(qq\\) plot of residuals. What do we hope to learn from this type of plot? Does this plot display any model inadequacies? c(iii). Provide a plot of residuals versus leverages. What do we hope to learn from this type of plot? Does this plot display any model inadequacies? Stepwise Regression. Run a stepwise regression algorithm. Suppose that this algorithm suggests a model using LNINCOME}, EDUCATION, NUMHH, and GENDER as explanatory variables to predict the dependent variable LNFACE. d(i). What is the purpose of stepwise regression? d(ii). Describe two important drawbacks of stepwise regression algorithms. 5.10 Technical Supplements for Chapter 5 5.10.1 Projection Matrix Hat Matrix. We define the hat matrix to be \\(\\mathbf{H} = \\mathbf{X(X}^{\\prime}\\mathbf{X)}^{-1} \\mathbf{X}^{\\prime}\\), so that \\(\\mathbf{\\hat{y}} = \\mathbf{X b} = \\mathbf{Hy}\\). From this, the matrix \\(\\mathbf{H}\\) is said to project the vector of responses \\(\\mathbf{y}\\) onto the vector of fitted values \\(\\mathbf{\\hat{y}}\\). Because \\(\\mathbf{H}^{\\prime} = \\mathbf{H}\\), the hat matrix is symmetric. Further, it is also an idempotent matrix due to the property that \\(\\mathbf{HH} = \\mathbf{H}\\). To see this, we have that \\[ \\begin{array}{ll} \\mathbf{HH} &amp;= \\mathbf{(X(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathbf{)(X(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime}\\mathbf{)} \\\\ &amp;= \\mathbf{X(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{(\\mathbf{X}^{\\prime}X)(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime} = \\mathbf{X(\\mathbf{X}^{\\prime}X)}^{-1}\\mathbf{X}^{\\prime} = \\mathbf{H}. \\end{array} \\] Similarly, it is easy to check that \\(\\mathbf{I-H}\\) is idempotent. Because \\(\\mathbf{H}\\) is idempotent, from some results in matrix algebra, it is straightforward to show that \\[ \\sum_{i=1}^{n} h_{ii} = k + 1. \\] As discussed in Section 5.4.1, we use our bounds and the average leverage, \\(\\bar{h} = (k + 1)/n\\), to help identify observations with unusually high leverage. Variance of Residuals. Using the model equation \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\), we can express the vector of residuals as \\[ \\mathbf{e} = \\mathbf{y} - \\mathbf{\\hat{y}} = \\mathbf{y - Hy} = \\mathbf{(I-H)(X \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon})} = \\mathbf{(I-H) \\boldsymbol{\\varepsilon}}. \\tag{5.14} \\] The last equality is due to the fact that \\(\\mathbf{(I-H)X} = \\mathbf{X - HX} = \\mathbf{X - X} = \\mathbf{0}\\). Using \\(\\text{Var~} \\boldsymbol{\\varepsilon} = \\sigma^2 \\mathbf{I}\\), we have \\[ \\begin{array}{ll} \\text{Var } \\mathbf{e} &amp;= \\text{Var }\\left[ \\mathbf{(I-H)\\boldsymbol{\\varepsilon}} \\right] = \\mathbf{(I-H)} \\text{Var } \\boldsymbol{\\varepsilon} \\mathbf{(I-H)} \\\\ &amp;= \\sigma^2 \\mathbf{(I-H)} \\mathbf{I} \\mathbf{(I-H)} = \\sigma^2 \\mathbf{(I-H)}. \\end{array} \\] The last equality comes from the fact that \\(\\mathbf{I-H}\\) is idempotent. Thus, we have that \\[ \\text{Var } e_i = \\sigma^2 (1 - h_{ii}) \\text{ and Cov } (e_i, e_j) = -\\sigma^2 h_{ij}. \\tag{5.15} \\] Thus, although the true errors \\(\\boldsymbol{\\varepsilon}\\) are uncorrelated, there is a small negative correlation among residuals \\(\\mathbf{e}\\). Dominance of the Error in the Residual. Examining the \\(i\\)th row of equation (5.14), we have that the \\(i\\)th residual \\[ e_i = \\varepsilon_i - \\sum_{j=1}^{n} h_{ij} \\varepsilon_j \\tag{5.16} \\] can be expressed as a linear combination of independent errors. The relation \\(\\mathbf{H} = \\mathbf{HH}\\) yields \\[ h_{ii} = \\sum_{j=1}^{n} h_{ij}^2. \\tag{5.17} \\] Because \\(h_{ii}\\) is, on average, \\((k + 1)/n\\), this indicates that each \\(h_{ij}\\) is small relative to 1. Thus, when interpreting equation (5.16), we say that most of the information in \\(e_i\\) is due to \\(\\varepsilon_i\\). Correlations with Residuals. First define \\(\\mathbf{x}^j = (x_{1j}, x_{2j}, \\dots, x_{nj})^{\\prime}\\) to be the column representing the \\(j\\)th variable. With this notation, we can partition the matrix of explanatory variables as \\(\\mathbf{X} = \\left( \\mathbf{x}^{0}, \\mathbf{x}^{1}, \\dots, \\mathbf{x}^{k} \\right)\\). Now, examining the \\(j\\)th column of the relation \\(\\mathbf{(I-H)X} = \\mathbf{0}\\), we have \\(\\mathbf{(I-H)x}^{j} = \\mathbf{0}\\). With \\(\\mathbf{e} = \\mathbf{(I-H) \\boldsymbol{\\varepsilon}}\\), this yields \\[ \\mathbf{e}^{\\prime} \\mathbf{x}^{j} = \\boldsymbol{\\varepsilon}^{\\prime} \\mathbf{(I-H)x}^{j} = 0, \\] for \\(j = 0, 1, \\ldots, k.\\) This result has several implications. If the intercept is in the model, then \\(\\mathbf{x}^{0} = (1, 1, \\ldots, 1)^{\\prime}\\) is a vector of ones. Here, \\(\\mathbf{e}^{\\prime} \\mathbf{x}^{0} = 0\\) means that \\(\\sum_{i=1}^{n} e_i = 0\\) or, the average residual is zero. Further, because \\(\\mathbf{e}^{\\prime} \\mathbf{x}^{j} = 0\\), it is easy to check that the sample correlation between \\(\\mathbf{e}\\) and \\(\\mathbf{x}^{j}\\) is zero. Along the same line, we also have that \\(\\mathbf{e}^{\\prime} \\mathbf{\\hat{y}} = \\mathbf{e}^{\\prime} \\mathbf{(I-H)Xb} = \\mathbf{0}\\). Thus, using the same argument as above, the sample correlation between \\(\\mathbf{e}\\) and \\(\\mathbf{\\hat{y}}\\) is zero. Multiple Correlation Coefficient. For an example of a non-zero correlation, consider \\(r(\\mathbf{y, \\hat{y}})\\), the sample correlation between \\(\\mathbf{y}\\) and \\(\\mathbf{\\hat{y}}\\). Because \\(\\mathbf{(I-H)x}^{0} = \\mathbf{0}\\), we have \\(\\mathbf{x}^{0} = \\mathbf{Hx}^{0}\\) and thus, \\(\\mathbf{\\hat{y}}^{\\prime} \\mathbf{x}^{0} = \\mathbf{y}^{\\prime} \\mathbf{Hx}^{0} = \\mathbf{y^{\\prime} x}^{0}\\). Assuming \\(\\mathbf{x}^{0} = (1, 1, \\ldots, 1)^{\\prime}\\), this means that \\(\\sum_{i=1}^{n} \\hat{y}_i = \\sum_{i=1}^{n} y_i\\), so that the average fitted value is \\(\\bar{y}\\). \\[ r(\\mathbf{y, \\hat{y}}) = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(\\hat{y}_i - \\bar{y})}{(n-1) s_y s_{\\hat{y}}}. \\] Recall that \\((n-1) s_y^2 = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = \\text{Total SS}\\) and \\((n-1) s_{\\hat{y}}^2 = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2 = \\text{Regress SS}\\). Further, with \\(\\mathbf{x}^0 = (1, 1, \\ldots, 1)^{\\prime}\\), \\[ \\sum_{i=1}^{n} (y_i - \\bar{y})(\\hat{y}_i - \\bar{y}) = (\\mathbf{y} - \\bar{y} \\mathbf{x}^0)^{\\prime} (\\mathbf{\\hat{y}} - \\bar{y} \\mathbf{x}^0) = \\mathbf{y}^{\\prime} \\mathbf{\\hat{y}} - \\bar{y}^2 \\mathbf{x}^{0 \\prime} \\mathbf{x}^0 \\] \\[ = \\mathbf{y}^{\\prime} \\mathbf{Xb} - n \\bar{y}^2 = \\text{Regress SS}. \\] This yields \\[ r(\\mathbf{y, \\hat{y}}) = \\frac{\\text{Regress SS}}{\\sqrt{\\left( \\text{Total SS} \\right) \\left( \\text{Regress SS} \\right)}} = \\sqrt{\\frac{\\text{Regress SS}}{\\text{Total SS}}} = \\sqrt{R^2}. \\tag{5.18} \\] That is, the coefficient of determination can be interpreted as the square root of the correlation between the observed and fitted responses. 5.10.2 Leave One Out Statistics Notation. To test the sensitivity of regression quantities, there are a number of statistics of interest that are based on the notion of “leaving out,” or omitting, an observation. To this end, the subscript notation \\((i)\\) means to leave out the \\(i\\)th observation. For example, omitting the row of explanatory variables \\(\\mathbf{x}_i^{\\prime} = (x_{i0}, x_{i1}, \\dots, x_{ik})\\) from \\(\\mathbf{X}\\) yields \\(\\mathbf{X}_{(i)}\\), a \\((n-1) \\times (k+1)\\) matrix of explanatory variables. Similarly, \\(\\mathbf{y}_{(i)}\\) is a \\((n-1) \\times 1\\) vector, based on removing the \\(i\\)th row from \\(\\mathbf{y}\\). Basic Matrix Result. Suppose that \\(\\mathbf{A}\\) is an invertible, \\(p \\times p\\) matrix and \\(\\mathbf{z}\\) is a \\(p \\times 1\\) vector. The following result from matrix algebra provides an important tool for understanding leave one out statistics in linear regression analysis. \\[ \\left( \\mathbf{A - zz}^{\\prime} \\right)^{-1} = \\mathbf{A}^{-1} + \\frac{\\mathbf{A}^{-1} \\mathbf{zz}^{\\prime} \\mathbf{A}^{-1}}{1 - \\mathbf{z}^{\\prime} \\mathbf{A}^{-1} \\mathbf{z}}. \\tag{5.19} \\] To check this result, simply multiply \\(\\mathbf{A - zz}^{\\prime}\\) by the right hand side of the equation to get \\(\\mathbf{I}\\), the identity matrix. Vector of Regression Coefficients. Omitting the \\(i\\)th observation, our new vector of regression coefficients is \\(\\mathbf{b}_{(i)} = \\left( \\mathbf{X}_{(i)}^{\\prime} \\mathbf{X}_{(i)} \\right)^{-1} \\mathbf{X}_{(i)}^{\\prime} \\mathbf{y}_{(i)}.\\) An alternative expression for \\(\\mathbf{b}_{(i)}\\) that is simpler to compute turns out to be \\[ \\mathbf{b}_{(i)} = \\mathbf{b} - \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}}.S \\tag{5.20} \\] To verify this, first use the matrix inversion result with \\(\\mathbf{A} = \\mathbf{X}^{\\prime} \\mathbf{X}\\) and \\(\\mathbf{z} = \\mathbf{x}_i\\) to get \\[ \\left( \\mathbf{X}_{(i)}^{\\prime} \\mathbf{X}_{(i)} \\right)^{-1} = (\\mathbf{X}^{\\prime} \\mathbf{X} - \\mathbf{x}_i \\mathbf{x}_i^{\\prime})^{-1} = (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} + \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1}}{1 - h_{ii}}, \\] where, from the leverage result, we have \\(h_{ii} = \\mathbf{x}_i^{\\prime} (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} \\mathbf{x}_i\\). Multiplying each side by \\[ \\mathbf{X}_{(i)}^{\\prime} \\mathbf{y}_{(i)} = \\mathbf{X}^{\\prime} \\mathbf{y} - \\mathbf{x}_i y_i \\] yields \\[ \\begin{array}{ll} \\mathbf{b}_{(i)} &amp;= \\left( \\mathbf{X}_{(i)}^{\\prime} \\mathbf{X}_{(i)} \\right)^{-1} \\mathbf{X}_{(i)}^{\\prime} \\mathbf{y}_{(i)} \\\\ &amp;= \\left( (\\mathbf{X}^{\\prime} \\mathbf{X})^{-1} + \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1}}{1 - h_{ii}} \\right) \\left( \\mathbf{X}^{\\prime} \\mathbf{y} - \\mathbf{x}_i y_i \\right) \\\\ &amp;= \\mathbf{b} - \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i y_i + \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\mathbf{b} - \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i y_i}{1 - h_{ii}} \\\\ &amp;= \\mathbf{b} - \\frac{\\left( 1 - h_{ii} \\right) \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i y_i - \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\mathbf{b} - \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i h_{ii} y_i}{1 - h_{ii}} \\\\ &amp;= \\mathbf{b} - \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i y_i - \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i \\mathbf{x}_i^{\\prime} \\mathbf{b}}{1 - h_{ii}} \\\\ &amp;= \\mathbf{b} - \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}}. \\end{array} \\] This establishes the result. Cook’s Distance. To measure the effect, or influence, of omitting the \\(i\\)th observation, Cook examined the difference between fitted values with and without the observation. We define Cook’s Distance to be \\[ D_i = \\frac{\\left( \\mathbf{\\hat{y} - \\hat{y}}_{(i)} \\right)^{\\prime} \\left( \\mathbf{\\hat{y} - \\hat{y}}_{(i)} \\right)}{(k+1) s^2} \\] where \\(\\mathbf{\\hat{y}}_{(i)} = \\mathbf{Xb}_{(i)}\\) is the vector of fitted values calculated omitting the \\(i\\)th point. Using equation (5.20) and \\(\\mathbf{\\hat{y}} = \\mathbf{Xb}\\), an alternative expression for Cook’s Distance is \\[ \\begin{array}{ll} D_i &amp;= \\frac{\\left( \\mathbf{b - b}_{(i)} \\right)^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right) \\left( \\mathbf{b - b}_{(i)} \\right)}{(k+1) s^2} \\\\ &amp;= \\frac{e_i^2}{(1 - h_{ii})^2} \\frac{\\mathbf{x}_i^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right) \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i}{(k+1) s^2} \\\\ &amp; = \\frac{e_i^2}{(1 - h_{ii})^2} \\frac{h_{ii}}{(k+1) s^2} \\\\ &amp;= \\left( \\frac{e_i}{s \\sqrt{1 - h_{ii}}} \\right)^2 \\frac{h_{ii}}{(k+1) (1 - h_{ii})}. \\end{array} \\] This result is not only useful computationally, it also serves to decompose the statistic into the part due to the standardized residual, \\((e_i/(s \\sqrt{1 - h_{ii}}))^2\\), and due to the leverage, \\(\\frac{h_{ii}}{(k+1) (1 - h_{ii})}\\). Leave One Out Residual. The leave one out residual is defined by \\(e_{(i)} = y_i - \\mathbf{x}_i^{\\prime} \\mathbf{b}_{(i)}\\). It is used in computing the PRESS statistic, described in Section 5.6.3. A simple computational expression is \\(e_{(i)} = \\frac{e_i}{1 - h_{ii}}\\). To verify this, use equation (5.20) to get \\[ e_{(i)} = y_i - \\mathbf{x}_i^{\\prime} \\mathbf{b}_{(i)} = y_i - \\mathbf{x}_i^{\\prime} \\left( \\mathbf{b} - \\frac{\\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}} \\right) \\] \\[ = e_i + \\frac{\\mathbf{x}_i \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}} = e_i + \\frac{h_{ii} e_i}{1 - h_{ii}} = \\frac{e_i}{1 - h_{ii}}. \\] Leave One Out Variance Estimate. The leave one out estimate of the variance is defined by \\[ s_{(i)}^2 = \\frac{((n - 1) - (k + 1))^{-1} \\sum_{j \\ne i} \\left( y_j - \\mathbf{x}_j^{\\prime} \\mathbf{b}_{(i)} \\right)^2}{(n - 1) - (k + 1)}. \\] It is used in the definition of the studentized residual, defined in Section 5.3.1. A simple computational expression is given by \\[ s_{(i)}^2 = \\frac{(n - (k + 1)) s^2 - \\frac{e_i^2}{1 - h_{ii}}}{(n - 1) - (k + 1)}. \\tag{5.21} \\] To see this, first note that from equation (5.14), we have \\(\\mathbf{He} = \\mathbf{H(I - H) \\boldsymbol{\\varepsilon}} = \\mathbf{0}\\), because \\(\\mathbf{H} = \\mathbf{HH}\\). In particular, from the \\(i\\)th row of \\(\\mathbf{He} = \\mathbf{0}\\), we have \\(\\sum_{j=1}^{n} h_{ij} e_j = 0\\). Now, using equations (5.17) and (5.20), we have \\[ \\begin{array}{ll} \\sum_{j \\ne i} \\left( y_j - \\mathbf{x}_j^{\\prime} \\mathbf{b}_{(i)} \\right)^2 &amp;= \\sum_{j=1}^{n} \\left( y_j - \\mathbf{x}_j^{\\prime} \\mathbf{b}_{(i)} \\right)^2 - \\left( y_i - \\mathbf{x}_i^{\\prime} \\mathbf{b}_{(i)} \\right)^2 \\\\ &amp;= \\sum_{j=1}^{n} \\left( y_j - \\mathbf{x}_j^{\\prime} \\mathbf{b} + \\frac{\\mathbf{x}_j^{\\prime} \\left( \\mathbf{X}^{\\prime} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i e_i}{1 - h_{ii}} \\right) - e_{(i)}^2 \\\\ &amp;= \\sum_{j=1}^{n} \\left( e_j + \\frac{h_{ij} e_i}{1 - h_{ii}} \\right)^2 - \\frac{e_i^2}{(1 - h_{ii})^2} \\\\ &amp;= \\sum_{j=1}^{n} e_j^2 + 0 + \\frac{e_i^2}{(1 - h_{ii})^2} h_{ii} - \\frac{e_i^2}{(1 - h_{ii})^2} \\\\ &amp;= \\sum_{j=1}^{n} e_j^2 - \\frac{e_i^2}{1 - h_{ii}} = (n - (k + 1)) s^2 - \\frac{e_i^2}{1 - h_{ii}}. \\end{array} \\] This establishes equation (5.21). 5.10.3 Omitting Variables Notation. To measure the effect on regression quantities, there are a number of statistics of interest that are based on the notion of omitting an explanatory variable. To this end, the superscript notation \\((j)\\) means to omit the \\(j\\)th variable, where \\(j=0,1,\\ldots,k\\). First, recall that \\(\\mathbf{x}^{j} = (x_{1j}, x_{2j}, \\ldots, x_{nj})^{\\prime}\\) is the column representing the \\(j\\)th variable. Further, define \\(\\mathbf{X}^{(j)}\\) to be the \\(n \\times k\\) matrix of explanatory variables defined by removing \\(\\mathbf{x}^{j}\\) from \\(\\mathbf{X}\\). For example, taking \\(j=k\\), we often partition \\(\\mathbf{X}\\) as \\(\\mathbf{X} = \\left( \\mathbf{X}^{(k)}: \\mathbf{x}^k \\right)\\). Employing the results of Section 4.7.2, we will use \\(\\mathbf{X}^{(k)} = \\mathbf{X}_1\\) and \\(\\mathbf{x}^k = \\mathbf{X}_2\\). Variance Inflation Factor. We first would like to establish the relationship between the definition of the standard error of \\(b_j\\) given by \\[ se(b_j) = s \\sqrt{(j+1)\\text{th diagonal element of }(\\mathbf{X}^{\\prime}\\mathbf{X})^{-1}} \\] and the relationship involving the variance inflation factor, \\[ se(b_j) = s \\frac{\\sqrt{VIF_j}}{s_{x_j}\\sqrt{n-1}}. \\] By symmetry of the independent variables, we only need to consider the case where \\(j=k\\). Thus, we would like to establish \\[ (k+1)\\text{th diagonal element of }(\\mathbf{X}^{\\prime}\\mathbf{X})^{-1} = \\frac{VIF_{k}}{(n-1) s_{x_{k}}^2}. \\tag{5.22} \\] First consider the reparameterized model in equation (4.22). From equation (4.23), we can express the regression coefficient estimate \\[ b_{k} = \\frac{\\mathbf{e}_1^{\\prime}\\mathbf{y}}{\\mathbf{e}_1^{\\prime}\\mathbf{e}_1}. \\] From equation (4.23), we have that \\(\\text{Var} \\, b_{k} = \\sigma^2 (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1}\\) and thus \\[ se(b_{k}) = s (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1/2}. \\tag{5.23} \\] Thus, \\((\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)^{-1}\\) is the \\((k+1)\\)th diagonal element of \\[ \\left( \\begin{bmatrix} \\mathbf{X}_1^{\\prime} \\\\ \\mathbf{E}_2^{\\prime} \\end{bmatrix} \\begin{bmatrix} \\mathbf{X}_1 &amp; \\mathbf{E}_2 \\end{bmatrix} \\right)^{-1} \\] and is also the \\((k+1)\\)th diagonal element of \\((\\mathbf{X}^{\\prime} \\mathbf{X})^{-1}\\). Alternatively, this can be verified directly using the partitioned matrix inverse in equation (4.19). Now, suppose that we run a regression using \\(\\mathbf{x}^{k} = \\mathbf{X}_2\\) as the response vector and \\(\\mathbf{X}^{(k)} = \\mathbf{X}_1\\) as the matrix of explanatory variables. As noted above equation (4.22), \\(\\mathbf{E}_2\\) represents the “residuals” from this regression and thus \\(\\mathbf{E}_2^{\\prime} \\mathbf{E}_2\\) represents the error sum of squares. For this regression, the total sum of squares is \\[ \\sum_{i=1}^{n} (x_{ik} - \\bar{x}_{k})^2 = (n-1) s_{x_{k}}^2 \\] and the coefficient of determination is \\(R_{k}^2\\). Thus, \\[ \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 = \\text{Error SS} = \\text{Total SS} (1 - R_{k}^2) = \\frac{(n-1) s_{x_{k}}^2}{VIF_{k}}. \\] This establishes the result. Establishing \\(t^2 = F\\). For testing the null hypothesis \\(H_0\\): \\(\\beta_{k} = 0\\), the material in Section 3.4.1 provides a description of a test based on the \\(t\\)-statistic, \\(t(b_{k}) = \\frac{b_{k}}{se(b_{k})}\\). An alternative test procedure, described in Sections 4.2.2, uses the test statistic \\[ F-\\text{ratio} = \\frac{(\\text{Error SS})_{\\text{reduced}} - (\\text{Error SS})_{\\text{full}}}{p \\times (Error~MS)_{\\text{full}}} = \\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right)^2}{s^2 \\mathbf{E}_2^{\\prime} \\mathbf{E}_2} \\] from equation (4.26). Alternatively, from equations (4.23) and (5.23), we have \\[ t(b_{k}) = \\frac{b_{k}}{se(b_{k})} = \\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right) / \\left( \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 \\right)}{s / \\sqrt{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2}} = \\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right)}{s \\sqrt{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2}}. \\tag{5.24} \\] Thus, \\(t(b_{k})^2 = F\\)-ratio. Partial Correlation Coefficients. From the full regression model \\[ \\mathbf{y} = \\mathbf{X}^{(k)} \\boldsymbol{\\beta}^{(k)} + \\mathbf{x}_{k} \\beta_{k} + \\boldsymbol{\\varepsilon}, \\] consider two separate regressions. A regression using \\(\\mathbf{x}^{k}\\) as the response vector and \\(\\mathbf{X}^{(k)}\\) as the matrix of explanatory variables yields the residuals \\(\\mathbf{E}_2\\). Similarly, a regression with \\(\\mathbf{y}\\) as the response vector and \\(\\mathbf{X}^{(k)}\\) as the matrix of explanatory variables yields the residuals \\[ \\mathbf{E}_1 = \\mathbf{y} - \\mathbf{X}^{(k)} \\left( \\mathbf{X}^{(k)\\prime} \\mathbf{X}^{(k)} \\right)^{-1} \\mathbf{X}^{(k)} \\mathbf{y}. \\] If \\(x^{0} = (1,1,\\ldots,1)^{\\prime}\\), then the average of \\(\\mathbf{E}_1\\) and \\(\\mathbf{E}_2\\) is zero. In this case, the sample correlation between \\(\\mathbf{E}_1\\) and \\(\\mathbf{E}_2\\) is \\[ r(\\mathbf{E}_1, \\mathbf{E}_2) = \\frac{\\sum_{i=1}^{n} E_{1i} E_{2i}}{\\sqrt{\\left( \\sum_{i=1}^{n} E_{1i}^2 \\right) \\left( \\sum_{i=1}^{n} E_{2i}^2 \\right)}} = \\frac{\\mathbf{E}_1^{\\prime} \\mathbf{E}_2}{\\sqrt{\\left( \\mathbf{E}_1^{\\prime} \\mathbf{E}_1 \\right) \\left( \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 \\right)}}. \\] Because \\(\\mathbf{E}_2\\) is a vector of residuals using \\(\\mathbf{X}^{(k)}\\) as the matrix of explanatory variables, we have that \\(\\mathbf{E}_2^{\\prime} \\mathbf{X}^{(k)} = 0\\). Thus, for the numerator, we have \\[ \\mathbf{E}_2^{\\prime} \\mathbf{E}_1 = \\mathbf{E}_2^{\\prime} \\left( \\mathbf{y} - \\mathbf{X}^{(k)} \\left( \\mathbf{X}^{(k)\\prime} \\mathbf{X}^{(k)} \\right)^{-1} \\mathbf{X}^{(k)} \\mathbf{y} \\right) = \\mathbf{E}_2^{\\prime} \\mathbf{y}. \\] From equations (4.24) and (4.25), we have that \\[ (n - (k+1)) s^2 = (\\text{Error SS})_{\\text{full}} = \\mathbf{E}_1^{\\prime} \\mathbf{E}_1 - \\frac{\\left( \\mathbf{E}_1^{\\prime} \\mathbf{y} \\right)^2}{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2} = \\mathbf{E}_1^{\\prime} \\mathbf{E}_1 - \\frac{\\left( \\mathbf{E}_1^{\\prime} \\mathbf{E}_2 \\right)^2}{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2}. \\] Thus, from equation (5.24) \\[ \\begin{array}{ll} \\frac{t(b_{k})}{\\sqrt{t(b_{k})^2 + n - (k+1)}} &amp;= \\frac{\\mathbf{E}_2^{\\prime} \\mathbf{y} / \\left(s \\sqrt{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2}\\right)}{\\sqrt{\\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right)^2}{s^2 \\mathbf{E}_2^{\\prime} \\mathbf{E}_2} + n - (k+1)}} \\\\ &amp; = \\frac{\\mathbf{E}_2^{\\prime} \\mathbf{y}}{\\sqrt{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{y} \\right)^2 + \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 s^2 \\left(n - (k+1) \\right)}} \\\\ &amp; = \\frac{\\mathbf{E}_2^{\\prime} \\mathbf{E}_1}{\\sqrt{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{E}_1 \\right)^2 + \\mathbf{E}_2^{\\prime} \\mathbf{E}_2 \\left( \\mathbf{E}_1^{\\prime} \\mathbf{E}_1 - \\frac{\\left( \\mathbf{E}_2^{\\prime} \\mathbf{E}_1 \\right)^2}{\\mathbf{E}_2^{\\prime} \\mathbf{E}_2} \\right)}} \\\\ &amp;= \\frac{\\mathbf{E}_1^{\\prime} \\mathbf{E}_2}{\\sqrt{(\\mathbf{E}_1^{\\prime} \\mathbf{E}_1) (\\mathbf{E}_2^{\\prime} \\mathbf{E}_2)}} = r(\\mathbf{E}_1, \\mathbf{E}_2). \\end{array} \\] This establishes the relationship between the partial correlation coefficient and the \\(t\\)-ratio statistic. "],["bibliography.html", "Bibliography", " Bibliography "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
