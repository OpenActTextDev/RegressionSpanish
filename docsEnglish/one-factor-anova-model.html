<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 One Factor ANOVA Model | Regression Modeling with Actuarial and Financial Applications</title>
  <meta name="description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 One Factor ANOVA Model | Regression Modeling with Actuarial and Financial Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 One Factor ANOVA Model | Regression Modeling with Actuarial and Financial Applications" />
  
  <meta name="twitter:description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  

<meta name="author" content="Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="estimating-and-predicting-several-coefficients.html"/>
<link rel="next" href="technical-supplement---matrix-expressions.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript"  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleSolution(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}      
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>



<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Regression Modeling With Actuarial and Financial Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#forward"><i class="fa fa-check"></i>Forward</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#who-is-this-book-for"><i class="fa fa-check"></i>Who Is This Book For?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#what-is-this-book-about"><i class="fa fa-check"></i>What Is This Book About?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#how-does-this-book-deliver-its-message"><i class="fa fa-check"></i>How Does This Book Deliver Its Message?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="chap-1.html"><a href="chap-1.html"><i class="fa fa-check"></i><b>1</b> Chap 1</a></li>
<li class="chapter" data-level="2" data-path="chap-2.html"><a href="chap-2.html"><i class="fa fa-check"></i><b>2</b> Chap 2</a></li>
<li class="chapter" data-level="3" data-path="chap-3.html"><a href="chap-3.html"><i class="fa fa-check"></i><b>3</b> Chap 3</a></li>
<li class="chapter" data-level="4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression - II</a>
<ul>
<li class="chapter" data-level="4.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#S4:BinaryVar"><i class="fa fa-check"></i><b>4.1</b> The Role of Binary Variables</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#example-how-does-cost-sharing-in-insurance-plans-affect-expenditures-in-healthcare"><i class="fa fa-check"></i><b>4.1.1</b> Example: How does Cost-Sharing in Insurance Plans affect Expenditures in Healthcare?</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#S4:SeveralCoeff"><i class="fa fa-check"></i><b>4.2</b> Statistical Inference for Several Coefficients</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#S4:SetsRegCoeff"><i class="fa fa-check"></i><b>4.2.1</b> Sets of Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#S4:GenLinHypo"><i class="fa fa-check"></i><b>4.3</b> The General Linear Hypothesis</a></li>
<li class="chapter" data-level="4.4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#some-special-cases"><i class="fa fa-check"></i><b>4.4</b> Some Special Cases</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="estimating-and-predicting-several-coefficients.html"><a href="estimating-and-predicting-several-coefficients.html"><i class="fa fa-check"></i><b>5</b> Estimating and Predicting Several Coefficients</a>
<ul>
<li class="chapter" data-level="5.1" data-path="estimating-and-predicting-several-coefficients.html"><a href="estimating-and-predicting-several-coefficients.html#estimating-linear-combinations-of-regression-coefficients"><i class="fa fa-check"></i><b>5.1</b> Estimating Linear Combinations of Regression Coefficients</a></li>
<li class="chapter" data-level="5.2" data-path="estimating-and-predicting-several-coefficients.html"><a href="estimating-and-predicting-several-coefficients.html#prediction-intervals"><i class="fa fa-check"></i><b>5.2</b> Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="one-factor-anova-model.html"><a href="one-factor-anova-model.html"><i class="fa fa-check"></i><b>6</b> One Factor ANOVA Model</a>
<ul>
<li class="chapter" data-level="6.0.1" data-path="one-factor-anova-model.html"><a href="one-factor-anova-model.html#model-assumptions-and-analysis"><i class="fa fa-check"></i><b>6.0.1</b> Model Assumptions and Analysis</a></li>
<li class="chapter" data-level="6.0.2" data-path="one-factor-anova-model.html"><a href="one-factor-anova-model.html#link-with-regression"><i class="fa fa-check"></i><b>6.0.2</b> Link with Regression</a></li>
<li class="chapter" data-level="6.0.3" data-path="one-factor-anova-model.html"><a href="one-factor-anova-model.html#reparameterization"><i class="fa fa-check"></i><b>6.0.3</b> Reparameterization</a></li>
<li class="chapter" data-level="6.1" data-path="one-factor-anova-model.html"><a href="one-factor-anova-model.html#combining-categorical-and-continuous-explanatory-variables"><i class="fa fa-check"></i><b>6.1</b> Combining Categorical and Continuous Explanatory Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="one-factor-anova-model.html"><a href="one-factor-anova-model.html#combining-a-factor-and-covariate"><i class="fa fa-check"></i><b>6.1.1</b> Combining a Factor and Covariate</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="one-factor-anova-model.html"><a href="one-factor-anova-model.html#example-wisconsin-hospital-costs"><i class="fa fa-check"></i><b>6.2</b> Example: Wisconsin Hospital Costs</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="one-factor-anova-model.html"><a href="one-factor-anova-model.html#combining-two-factors"><i class="fa fa-check"></i><b>6.2.1</b> Combining Two Factors</a></li>
<li class="chapter" data-level="6.2.2" data-path="one-factor-anova-model.html"><a href="one-factor-anova-model.html#general-linear-model"><i class="fa fa-check"></i><b>6.2.2</b> General Linear Model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="one-factor-anova-model.html"><a href="one-factor-anova-model.html#further-reading-and-references"><i class="fa fa-check"></i><b>6.3</b> Further Reading and References</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="one-factor-anova-model.html"><a href="one-factor-anova-model.html#chapter-references"><i class="fa fa-check"></i><b>6.3.1</b> Chapter References</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="one-factor-anova-model.html"><a href="one-factor-anova-model.html#exercises"><i class="fa fa-check"></i><b>6.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="technical-supplement---matrix-expressions.html"><a href="technical-supplement---matrix-expressions.html"><i class="fa fa-check"></i><b>7</b> Technical Supplement - Matrix Expressions</a>
<ul>
<li class="chapter" data-level="7.1" data-path="technical-supplement---matrix-expressions.html"><a href="technical-supplement---matrix-expressions.html#expressing-models-with-categorical-variables-in-matrix-form"><i class="fa fa-check"></i><b>7.1</b> Expressing Models with Categorical Variables in Matrix Form</a></li>
<li class="chapter" data-level="7.2" data-path="technical-supplement---matrix-expressions.html"><a href="technical-supplement---matrix-expressions.html#technical-supplement---matrix-expressions-1"><i class="fa fa-check"></i><b>7.2</b> Technical Supplement - Matrix Expressions</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="technical-supplement---matrix-expressions.html"><a href="technical-supplement---matrix-expressions.html#expressing-models-with-categorical-variables-in-matrix-form-1"><i class="fa fa-check"></i><b>7.2.1</b> Expressing Models with Categorical Variables in Matrix Form</a></li>
<li class="chapter" data-level="7.2.2" data-path="technical-supplement---matrix-expressions.html"><a href="technical-supplement---matrix-expressions.html#one-categorical-and-one-continuous-variable-model"><i class="fa fa-check"></i><b>7.2.2</b> One Categorical and One Continuous Variable Model</a></li>
<li class="chapter" data-level="7.2.3" data-path="technical-supplement---matrix-expressions.html"><a href="technical-supplement---matrix-expressions.html#calculating-least-squares-recursively"><i class="fa fa-check"></i><b>7.2.3</b> Calculating Least Squares Recursively</a></li>
<li class="chapter" data-level="7.2.4" data-path="technical-supplement---matrix-expressions.html"><a href="technical-supplement---matrix-expressions.html#partitioned-matrix-results"><i class="fa fa-check"></i><b>7.2.4</b> Partitioned Matrix Results</a></li>
<li class="chapter" data-level="7.2.5" data-path="technical-supplement---matrix-expressions.html"><a href="technical-supplement---matrix-expressions.html#general-linear-model-1"><i class="fa fa-check"></i><b>7.2.5</b> General Linear Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTextDev/RegressionSpanish/" target="blank">Spanish Regression on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Regression Modeling with Actuarial and Financial Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="one-factor-anova-model" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> One Factor ANOVA Model<a href="one-factor-anova-model.html#one-factor-anova-model" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Section <span class="math inline">\(\ref{S4:BinaryVar}\)</span> showed how to incorporate unordered
categorical variables, or factors, into a linear regression model
through the use of binary variables. Factors are important in social
science research; they can be used to classify people by gender,
ethnicity, marital status and so on, or classify firms by geographic
region, organizational structure and so forth. Within studies of
insurance, factors are used by insurers to categorize policyholders
according to a <code>risk classification system.'' Here, the idea is to create groups of policyholders with similar risk characteristics that will have similar claims experience. These groups form the basis of insurance pricing, so that each policyholder is charged an amount that is appropriate to their risk category. This process is sometimes known as</code>segmentation.’’</p>
<p>Although factors may be represented as binary variables in a linear
regression model, we study one factor models as a separate unit
because:
- The method of least squares is much simpler, obviating the need to take inverses of high-dimensional matrices.
- The resulting interpretations of coefficients are more straightforward.</p>
<p>The one factor model is still a special case of the linear
regression model. Hence, no additional statistical theory is needed
to establish its statistical inference capabilities.</p>
<p>To establish notation for the one factor ANOVA model, we now
consider the following example.</p>
<hr />
<p><strong>Example: Automobile Insurance Claims.</strong> We examine claims experience
from a large midwestern (US) property and casualty insurer for
private passenger automobile insurance. The dependent variable is
the amount paid on a closed claim, in (US) dollars (claims that were
not closed by year end are handled separately). Insurers categorize
policyholders according to a risk classification system. This
insurer’s risk classification system is based on:
- Automobile operator characteristics (age, gender, marital
status and whether the primary or occasional driver of a car).
- Vehicle characteristics (city versus farm usage, if the vehicle is used to commute to school or
work, used for business or pleasure, and if commuting, the
approximate distance of the commute).</p>
<p>These factors are summarized by the risk class categorical variable
CLASS. Table <span class="math inline">\(\ref{T4:AutoSumStats}\)</span> shows 18 risk classes - further
classification information is not given here to protect proprietary
interests of the insurer.</p>
<p>Table <span class="math inline">\(\ref{T4:AutoSumStats}\)</span> summarizes the results from <span class="math inline">\(n=6,773\)</span>
claims for drivers aged 50 and above. We can see that the median
claim varies from a low of $707.40 (CLASS F7) to a high of
$1,231.25 (CLASS C72). The distribution of claims turns out to be
skewed, so we consider <span class="math inline">\(y\)</span> = logarithmic claims. The table presents
means, medians and standard deviations. Because the distribution of
logarithmic claims is less skewed, means are close to medians.
Figure <span class="math inline">\(\ref{F4:BoxplotAuto}\)</span> shows the distribution of logarithmic
claims by risk class.</p>
<hr />
<p>This section focuses on the risk class (CLASS) as the explanatory variable. We use the notation <span class="math inline">\(y_{ij}\)</span> to mean the <span class="math inline">\(i\)</span>th observation of the <span class="math inline">\(j\)</span>th risk class. For the <span class="math inline">\(j\)</span>th risk class, we assume there are <span class="math inline">\(n_j\)</span> observations. There are <span class="math inline">\(n=n_1+n_2+\ldots +n_c\)</span> observations. The data are:</p>
<p>where <span class="math inline">\(c=18\)</span> is the number of levels of the CLASS factor. Because each level of a factor can be arranged in a single row (or column), another term for this type of data is a ``one way classification.’’ Thus, a <em>one way model</em> is another term for a one factor model.</p>
<p>An important summary measure of each level of the factor is the sample average. Let
<span class="math display">\[
\overline{y}_j=\frac{1}{n_j}\sum_{i=1}^{n_j}y_{ij}
\]</span>
denote the average from the <span class="math inline">\(j\)</span>th CLASS.</p>
<div id="model-assumptions-and-analysis" class="section level3 hasAnchor" number="6.0.1">
<h3><span class="header-section-number">6.0.1</span> Model Assumptions and Analysis<a href="one-factor-anova-model.html#model-assumptions-and-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The one factor ANOVA model equation is
<span class="math display">\[
y_{ij}=\mu_j+ \varepsilon_{ij}\ \ \ \ \ \ i=1,\ldots ,n_j,\ \ \ \ \
j=1,\ldots ,c.
\]</span>
As with regression models, the random deviations <span class="math inline">\(\{\varepsilon_{ij} \}\)</span> are assumed to be zero mean with constant variance (Assumption E3) and independent of one another (Assumption E4). Because we assume the expected value of each deviation is zero, we have <span class="math inline">\(\text{E}~y_{ij}=\mu_j\)</span>. Thus, we interpret <span class="math inline">\(\mu_j\)</span> to be the expected value of the response <span class="math inline">\(y_{ij}\)</span>, that is, the mean <span class="math inline">\(\mu\)</span> varies by the <span class="math inline">\(j\)</span>th factor level.</p>
<p>To estimate the parameters <span class="math inline">\(\{\mu_j\}\)</span>, as with regression we use the <em>method of least squares</em>, introduced in Section 2.1. That is, let <span class="math inline">\(\mu^{\ast}_j\)</span> be a ``candidate’’ estimate of <span class="math inline">\(\mu_j\)</span>. The quantity
<span class="math display">\[
\text{SS}(\mu^{\ast}_1, \ldots , \mu^{\ast}_{c}) = \sum_{j=1}^{c} \sum_{i=1}^{n_j} (y_{ij}-\mu^{\ast}_j)^2
\]</span>
represents the sum of squared deviations of the responses from these candidate estimates. From straightforward algebra, the value of <span class="math inline">\(\mu^{\ast}_j\)</span> that minimizes this sum of squares is <span class="math inline">\(\bar{y}_j\)</span>. Thus, <span class="math inline">\(\bar{y}_j\)</span> is the <em>least squares estimate</em> of <span class="math inline">\(\mu_j\)</span>.</p>
<p>To understand the reliability of the estimates, we can partition the variability as in the regression case, presented in Sections 2.3.1 and 3.3. The minimum sum of squared deviations is called the <em>error sum of squares</em> and is defined to be
<span class="math display">\[
\text{Error SS} = \text{SS}(\bar{y}_1, \ldots, \bar{y}_{c}) = \sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij}-\bar{y}_j \right)^2.
\]</span>
The total variation in the data set is summarized by the <em>total sum of squares</em>,
<span class="math display">\[
\text{Total SS}=\sum_{j=1}^{c}\sum_{i=1}^{n_j}(y_{ij}-\bar{y})^2.
\]</span>
The difference, called the <em>factor sum of squares</em>, can be expressed as:
<span class="math display">\[
\text{Factor SS } = \text{Total SS -- Error SS}
\]</span>
<span class="math display">\[
\text{Factor SS } = \sum_{j=1}^{c}\sum_{i=1}^{n_j}(y_{ij}-\bar{y})^2-\sum_{j=1}^{c}\sum_{i=1}^{n_j}(y_{ij}-\bar{y}_j)^2 = \sum_{j=1}^{c}\sum_{i=1}^{n_j}(\bar{y}_j-\bar{y})^2
\]</span>
<span class="math display">\[
\text{Factor SS } = \sum_{j=1}^{c}n_j(\bar{y}_j-\bar{y})^2.
\]</span>
The last two equalities follow from algebra manipulation. The Factor SS plays the same role as the Regression SS in Chapters 2 and 3. The variability decomposition is summarized in the following analysis of variance (ANOVA) table.</p>
<p>The conventions for this table are the same as in the regression case. That is, the mean square (MS) column is defined by the sum of squares (SS) column divided by the degrees of freedom (<em>df</em>) column. Thus, <span class="math inline">\(Factor~MS \equiv (Factor~SS)/(c-1)\)</span> and <span class="math inline">\(Error~MS \equiv (Error~SS)/(n-c)\)</span>. We use
<span class="math display">\[
s^2 = \text{Error MS} = \frac{1}{n-c} \sum_{j=1}^{c}\sum_{i=1}^{n_j} e_{ij}^2
\]</span>
to be our estimate of <span class="math inline">\(\sigma^2\)</span>, where <span class="math inline">\(e_{ij} = y_{ij} - \bar{y}_j\)</span> is the residual.</p>
<p>With this value for <span class="math inline">\(s\)</span>, it can be shown that the interval estimate
for <span class="math inline">\(\mu_j\)</span> is
<span class="math display">\[
\bar{y}_j \pm t_{n-c,1-\alpha /2}\frac{s}{\sqrt{n_j}}.
\]</span></p>
<p>Here, the <em>t</em>-value <span class="math inline">\(t_{n-c,1-\alpha /2}\)</span> is a percentile from the <em>t</em>-distribution with <span class="math inline">\(df=n-c\)</span> degrees of freedom.</p>
<hr />
<p><strong>Example: Automobile Claims - Continued.</strong> To illustrate, the
ANOVA table summarizing the fit for the automobile claims data
appears in Table <span class="math inline">\(\ref{T4:ANOVAAuto}\)</span>. Here, we see that the mean
square error is <span class="math inline">\(s^2 = 1.14.\)</span></p>
<p>In automobile ratemaking, one uses the average claims to help set
prices for insurance coverages. As an example, for CLASS C72 the
average logarithmic claim is 7.183. From equation
<span class="math display">\[
7.183 \pm (1.96) \frac{\sqrt{1.14}}{\sqrt{85}} = 7.183 \pm 0.227 = (6.952 ,7.410).
\]</span>
Note that these estimates are in natural logarithmic units. In dollars, our point estimate is
<span class="math display">\[
e^{7.183} = \$1,316.85
\]</span>
and our 95% confidence interval is
<span class="math display">\[
(e^{6.952} , e^{7.410}) \text{ or } (\$1,045.24, \$1,652.43).
\]</span></p>
<hr />
<p>An important feature of the one factor ANOVA decomposition and
estimation is the ease of computation. Although the sum of squares
appear complex, it is important to note that <em>no matrix
calculations are required</em>. Rather, all of the calculations can be
done through averages and sums of squares. This has been an important
consideration historically, before the age of readily available
desktop computing. Moreover, insurers may segment their portfolios
into hundreds or even thousands of risk classes instead of the 18
used in our Automobile Claims data. Thus, even today it can be
helpful to identify a categorical variable as a factor and let your
statistical software use ANOVA estimation techniques. Further, ANOVA
estimation also provides for direct interpretation of the results.</p>
</div>
<div id="link-with-regression" class="section level3 hasAnchor" number="6.0.2">
<h3><span class="header-section-number">6.0.2</span> Link with Regression<a href="one-factor-anova-model.html#link-with-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This subsection shows how a one factor ANOVA model can be rewritten
as a regression model. To this end, we have seen that both the
regression model and one factor ANOVA model use a linear error
structure with Assumptions E3 and E4 for identically and
independently distributed errors. Similarly, both use the normality
assumption E5 for selected inference results (such as confidence
intervals). Both employ non-stochastic explanatory variables as in
Assumption E2. Both have an additive (mean zero) error term, so the
main apparent difference is in the expected response, <span class="math inline">\(E[y]\)</span>.</p>
<p>For the linear regression model, <span class="math inline">\(E[y]\)</span> is a linear combination of
explanatory variables (Assumption F1). For the one factor ANOVA
model, <span class="math inline">\(E[y_j] = \mu_j\)</span> is a mean that depends on the level of the
factor. To equate these two approaches, for the ANOVA factor with
<span class="math inline">\(c\)</span> levels, we define <span class="math inline">\(c\)</span> binary variables, <span class="math inline">\(x_1, x_2, \ldots, x_c\)</span>. Here, <span class="math inline">\(x_j\)</span> indicates whether or not an observation falls in
the <span class="math inline">\(j\)</span>th level. With these variables, we can rewrite our one factor
ANOVA model as
<span class="math display">\[
y = \mu_1 x_1 + \mu_2 x_2 + \ldots + \mu_c x_c + \varepsilon.
\]</span>
Thus, we have re-written the one factor ANOVA expected response as a
regression function, although using a no-intercept form (as in
equation 3.5).</p>
<p>The one factor ANOVA is a special case of our usual regression
model, using binary variables from the factor as explanatory
variables in the regression function. As we have seen, no matrix
calculations are needed for least squares estimation. However, one
can always use the matrix procedures developed in Chapter 3. Section
<span class="math inline">\(\ref{S4:CatVarMatrix}\)</span> shows how our usual matrix expression for
regression coefficients (<span class="math inline">\(\mathbf{b} = \left(\mathbf{X}^{\prime}\mathbf{X}\right)^{-1}\mathbf{X}^{\prime}\mathbf{y}\)</span>) reduces to the simple
estimates <span class="math inline">\(\bar{y}_j\)</span> when using only one categorical variable.</p>
</div>
<div id="reparameterization" class="section level3 hasAnchor" number="6.0.3">
<h3><span class="header-section-number">6.0.3</span> Reparameterization<a href="one-factor-anova-model.html#reparameterization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To include an intercept term, define <span class="math inline">\(\tau_j = \mu_j - \mu\)</span>, where
<span class="math inline">\(\mu\)</span> is an, as yet, unspecified parameter. Because each observation
must fall into one of the <span class="math inline">\(c\)</span> categories, we have <span class="math inline">\(x_1 + x_2 + \ldots + x_{c} = 1\)</span> for each observation. Thus, using <span class="math inline">\(\mu_j = \tau_j + \mu\)</span>
in equation
<span class="math display">\[
y = \mu + \tau_1 x_1 + \tau_2 x_2 + \ldots + \tau_{c} x_{c} + \varepsilon,
\]</span>
we have re-written the model into what appears to be our usual
regression format.</p>
<p>We use the <span class="math inline">\(\tau\)</span> in lieu of <span class="math inline">\(\beta\)</span> for historical reasons. ANOVA
models were invented by R.A. Fisher in connection with agricultural
experiments. Here, the typical set-up is to apply several
<em>treatments</em> to plots of land in order to quantify crop yield
responses. Thus, the Greek “t”, <span class="math inline">\(\tau\)</span>, suggests the word
treatment, another term used to describe levels of the factor of
interest.</p>
<p>A simpler version of equation
<span class="math display">\[
y_{ij} = \mu + \tau_j + \varepsilon_{ij}.
\]</span>
can be given when we identify the factor level. That is, if we know an
observation falls in the <span class="math inline">\(j\)</span>th level, then only <span class="math inline">\(x_j\)</span> is one and the
other <span class="math inline">\(x\)</span>’s are 0. Thus, a simpler expression for equation
<span class="math display">\[
y_{ij} = \mu + \tau_j + \varepsilon_{ij}.
\]</span></p>
<p>Comparing equations
<span class="math display">\[
y = \mu_1 x_1 + \mu_2 x_2 + \ldots + \mu_c x_c + \varepsilon
\]</span>
and
<span class="math display">\[
y = \mu + \tau_1 x_1 + \tau_2 x_2 + \ldots + \tau_{c} x_{c} + \varepsilon,
\]</span>
we see that the number of parameters has increased by one. That is, in equation
<span class="math display">\[
y = \mu_1 x_1 + \mu_2 x_2 + \ldots + \mu_c x_c + \varepsilon,
\]</span>
there are <span class="math inline">\(c\)</span> parameters, <span class="math inline">\(\mu_1, \ldots, \mu_c\)</span>, even though in equation
<span class="math display">\[
y = \mu + \tau_1 x_1 + \tau_2 x_2 + \ldots + \tau_{c} x_{c} + \varepsilon,
\]</span>
there are <span class="math inline">\(c + 1\)</span> parameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau_1, \ldots, \tau_c\)</span>. The model in equation
<span class="math display">\[
y = \mu + \tau_1 x_1 + \tau_2 x_2 + \ldots + \tau_{c} x_{c} + \varepsilon
\]</span>
is said to be <em>overparameterized</em>. It is possible to estimate
this model directly, using the general theory of linear models,
summarized in Section <span class="math inline">\(\ref{S4:GeneralLinearModel}\)</span>. In this theory,
regression coefficients need not be identifiable. Alternatively, one
can make these two expressions equivalent by <em>restricting</em> the
movement of the parameters in
<span class="math display">\[
y = \mu + \tau_1 x_1 + \tau_2 x_2 + \ldots + \tau_{c} x_{c} + \varepsilon.
\]</span>
We now present two ways of imposing restrictions.</p>
<p>The first type of restriction, usually done in the regression context, is to
require one of the <span class="math inline">\(\tau\)</span>’s to be zero. This amounts to <em>dropping</em>
one of the explanatory variables. For example, we might use
<span class="math display">\[
y = \mu + \tau_1 x_1 + \tau_2 x_2 + \ldots + \tau_{c-1} x_{c-1} + \varepsilon,
\]</span>
dropping <span class="math inline">\(x_c\)</span>. With this formulation, it is easy to fit the model
in equation
<span class="math display">\[
y = \mu + \tau_1 x_1 + \tau_2 x_2 + \ldots + \tau_{c-1} x_{c-1} + \varepsilon,
\]</span>
using regression statistical software routines because one only needs to run the regression with <span class="math inline">\(c-1\)</span> explanatory variables. However, one needs to be careful with
the interpretation of parameters. To equate the models in
<span class="math display">\[
y = \mu_1 x_1 + \mu_2 x_2 + \ldots + \mu_c x_c + \varepsilon
\]</span>
and
<span class="math display">\[
y = \mu + \tau_1 x_1 + \tau_2 x_2 + \ldots + \tau_{c} x_{c} + \varepsilon,
\]</span>
we need to define <span class="math inline">\(\mu \equiv \mu_c\)</span> and <span class="math inline">\(\tau_j = \mu_j - \mu_c\)</span> for <span class="math inline">\(j=1,2,\ldots,c-1\)</span>. That is, the regression intercept term is the mean level of the
category dropped, and each regression coefficient is the difference
between a mean level and the mean level dropped. It is not necessary
to drop the last level <span class="math inline">\(c\)</span>, and indeed, one could drop any level.
However, the interpretation of the parameters does depend on the
variable dropped. With this restriction, the fitted values are
<span class="math display">\[
\hat{\mu} = \hat{\mu}_c = \bar{y}_c
\]</span>
and
<span class="math display">\[
\hat{\tau}_j = \hat{\mu}_j - \hat{\mu}_c = \bar{y}_j - \bar{y}_c.
\]</span>
Recall that the caret (<span class="math inline">\(\symbol{94}\)</span>), or “hat,” stands for an estimated, or fitted, value.</p>
<p>The second type of restriction is to
interpret <span class="math inline">\(\mu\)</span> as a mean for the entire population. To this end,
the usual requirement is
<span class="math display">\[
\mu \equiv \frac{1}{n} \sum_{j=1}^c n_j \mu_j,
\]</span>
that is, <span class="math inline">\(\mu\)</span> is a weighted average of means. With this
definition, we interpret <span class="math inline">\(\tau_j = \mu_j - \mu\)</span> as treatment
differences between a mean level and the population mean. Another
way of expressing this restriction is
<span class="math display">\[
\sum_{j=1}^{c} n_j \tau_j = 0,
\]</span>
that is, the (weighted) sum of treatment differences is zero. The
disadvantage of this restriction is that it is not readily
implementable with a regression routine and a special routine is
needed. The advantage is that there is a symmetry in the definitions
of the parameters. There is no need to worry about which variable is
being dropped from the equation, an important consideration. With
this restriction, the fitted values are
<span class="math display">\[
\hat{\mu} = \frac{1}{n} \sum_{j=1}^{c} n_j \hat{\mu}_j = \frac{1}{n} \sum_{j=1}^{c} n_j \bar{y}_j = \bar{y}
\]</span>
and
<span class="math display">\[
\hat{\tau}_j = \hat{\mu}_j - \hat{\mu} = \bar{y}_j - \bar{y}.
\]</span></p>
</div>
<div id="combining-categorical-and-continuous-explanatory-variables" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Combining Categorical and Continuous Explanatory Variables<a href="one-factor-anova-model.html#combining-categorical-and-continuous-explanatory-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are several ways to combine categorical and continuous
explanatory variables. We initially present the case of only one
categorical and one continuous variable. We then briefly present the
general case, called the <em>general linear model</em>. When
combining categorical and continuous variable models, we use the
terminology <em>factor</em> for the categorical variable and
<em>covariate</em> for the continuous variable.</p>
<div id="combining-a-factor-and-covariate" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Combining a Factor and Covariate<a href="one-factor-anova-model.html#combining-a-factor-and-covariate" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us begin with the simplest models that use a factor and a
covariate. In Section <span class="math inline">\(\ref{S4:OneFactor}\)</span>, we introduced the one
factor model <span class="math inline">\(y_{ij} = \mu_j + \varepsilon_{ij}\)</span>. In Chapter 2, we
introduced basic linear regression in terms of one continuous
variable, or covariate, using <span class="math inline">\(y_{ij} = \beta_0 + \beta_1 x_{ij} + \varepsilon_{ij}\)</span>. Table <a href="#T4:OneFactorCovariate"><strong>??</strong></a> summarizes
different approaches that could be used to represent combinations of
a factor and covariate.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="one-factor-anova-model.html#cb5-1" tabindex="-1"></a><span class="co"># Create a data frame for the one factor and one covariate models table</span></span>
<span id="cb5-2"><a href="one-factor-anova-model.html#cb5-2" tabindex="-1"></a>one_factor_covariate <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb5-3"><a href="one-factor-anova-model.html#cb5-3" tabindex="-1"></a>  <span class="at">Model_Description =</span> <span class="fu">c</span>(</span>
<span id="cb5-4"><a href="one-factor-anova-model.html#cb5-4" tabindex="-1"></a>    <span class="st">&quot;One factor ANOVA (no covariate model)&quot;</span>,</span>
<span id="cb5-5"><a href="one-factor-anova-model.html#cb5-5" tabindex="-1"></a>    <span class="st">&quot;Regression with constant intercept and slope (no factor model)&quot;</span>,</span>
<span id="cb5-6"><a href="one-factor-anova-model.html#cb5-6" tabindex="-1"></a>    <span class="st">&quot;Regression with variable intercept and constant slope&quot;</span>,</span>
<span id="cb5-7"><a href="one-factor-anova-model.html#cb5-7" tabindex="-1"></a>    <span class="st">&quot;~~~(analysis of covariance model)&quot;</span>,</span>
<span id="cb5-8"><a href="one-factor-anova-model.html#cb5-8" tabindex="-1"></a>    <span class="st">&quot;Regression with constant intercept and variable slope&quot;</span>,</span>
<span id="cb5-9"><a href="one-factor-anova-model.html#cb5-9" tabindex="-1"></a>    <span class="st">&quot;Regression with variable intercept and slope&quot;</span></span>
<span id="cb5-10"><a href="one-factor-anova-model.html#cb5-10" tabindex="-1"></a>  ),</span>
<span id="cb5-11"><a href="one-factor-anova-model.html#cb5-11" tabindex="-1"></a>  <span class="at">Notation =</span> <span class="fu">c</span>(</span>
<span id="cb5-12"><a href="one-factor-anova-model.html#cb5-12" tabindex="-1"></a>    <span class="st">&quot;$y_{ij} = </span><span class="sc">\\</span><span class="st">mu_j + </span><span class="sc">\\</span><span class="st">varepsilon_{ij}$&quot;</span>,</span>
<span id="cb5-13"><a href="one-factor-anova-model.html#cb5-13" tabindex="-1"></a>    <span class="st">&quot;$y_{ij} = </span><span class="sc">\\</span><span class="st">beta_0 + </span><span class="sc">\\</span><span class="st">beta_1 x_{ij} + </span><span class="sc">\\</span><span class="st">varepsilon_{ij}$&quot;</span>,</span>
<span id="cb5-14"><a href="one-factor-anova-model.html#cb5-14" tabindex="-1"></a>    <span class="st">&quot;$y_{ij} = </span><span class="sc">\\</span><span class="st">beta_{0j} + </span><span class="sc">\\</span><span class="st">beta_1 x_{ij} + </span><span class="sc">\\</span><span class="st">varepsilon_{ij}$&quot;</span>,</span>
<span id="cb5-15"><a href="one-factor-anova-model.html#cb5-15" tabindex="-1"></a>    <span class="st">&quot;&quot;</span>,</span>
<span id="cb5-16"><a href="one-factor-anova-model.html#cb5-16" tabindex="-1"></a>    <span class="st">&quot;$y_{ij} = </span><span class="sc">\\</span><span class="st">beta_0 + </span><span class="sc">\\</span><span class="st">beta_{1j} x_{ij} + </span><span class="sc">\\</span><span class="st">varepsilon_{ij}$&quot;</span>,</span>
<span id="cb5-17"><a href="one-factor-anova-model.html#cb5-17" tabindex="-1"></a>    <span class="st">&quot;$y_{ij} = </span><span class="sc">\\</span><span class="st">beta_{0j} + </span><span class="sc">\\</span><span class="st">beta_{1j} x_{ij} + </span><span class="sc">\\</span><span class="st">varepsilon_{ij}$&quot;</span></span>
<span id="cb5-18"><a href="one-factor-anova-model.html#cb5-18" tabindex="-1"></a>  )</span>
<span id="cb5-19"><a href="one-factor-anova-model.html#cb5-19" tabindex="-1"></a>)</span>
<span id="cb5-20"><a href="one-factor-anova-model.html#cb5-20" tabindex="-1"></a></span>
<span id="cb5-21"><a href="one-factor-anova-model.html#cb5-21" tabindex="-1"></a><span class="co"># Print the table using kable</span></span>
<span id="cb5-22"><a href="one-factor-anova-model.html#cb5-22" tabindex="-1"></a><span class="fu">kable</span>(one_factor_covariate, <span class="at">caption =</span> <span class="st">&quot;Several Models that Represent Combinations of One Factor and One Covariate&quot;</span>,</span>
<span id="cb5-23"><a href="one-factor-anova-model.html#cb5-23" tabindex="-1"></a>      <span class="at">format =</span> <span class="st">&quot;markdown&quot;</span>, <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">&quot;Model Description&quot;</span>, <span class="st">&quot;Notation&quot;</span>))</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-11">Table 6.1: </span>Several Models that Represent Combinations of One Factor and One Covariate</caption>
<colgroup>
<col width="50%" />
<col width="49%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Model Description</th>
<th align="left">Notation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">One factor ANOVA (no covariate model)</td>
<td align="left"><span class="math inline">\(y_{ij} = \mu_j + \varepsilon_{ij}\)</span></td>
</tr>
<tr class="even">
<td align="left">Regression with constant intercept and slope (no factor model)</td>
<td align="left"><span class="math inline">\(y_{ij} = \beta_0 + \beta_1 x_{ij} + \varepsilon_{ij}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Regression with variable intercept and constant slope</td>
<td align="left"><span class="math inline">\(y_{ij} = \beta_{0j} + \beta_1 x_{ij} + \varepsilon_{ij}\)</span></td>
</tr>
<tr class="even">
<td align="left">~~~(analysis of covariance model)</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Regression with constant intercept and variable slope</td>
<td align="left"><span class="math inline">\(y_{ij} = \beta_0 + \beta_{1j} x_{ij} + \varepsilon_{ij}\)</span></td>
</tr>
<tr class="even">
<td align="left">Regression with variable intercept and slope</td>
<td align="left"><span class="math inline">\(y_{ij} = \beta_{0j} + \beta_{1j} x_{ij} + \varepsilon_{ij}\)</span></td>
</tr>
</tbody>
</table>
<p>We can interpret the regression with variable intercept and constant
slope to be an additive model, because we are adding the factor
effect, <span class="math inline">\(\beta_{0j}\)</span>, to the covariate effect, <span class="math inline">\(\beta_1 x_{ij}\)</span>. Note
that one could also use the notation, <span class="math inline">\(\mu_j\)</span>, in lieu of <span class="math inline">\(\beta_{0j}\)</span> to suggest the presence of a factor effect. This is also
known as an <em>analysis of covariance (ANCOVA) model</em>. The
regression with variable intercept and slope can be thought of as an
<em>interaction model</em>. Here, both the intercept, <span class="math inline">\(\beta_{0j}\)</span>,
and slope, <span class="math inline">\(\beta_{1j}\)</span>, may vary by level of the factor. In this
sense, we interpret the factor and covariate to be “interacting.”
The model with constant intercept and variable slope is typically
not used in practice; it is included here for completeness. With
this model, the factor and covariate interact only through the
variable slope. Figures <span class="math inline">\(\ref{F4:TheoryVarIntConSlope}\)</span>,
<span class="math inline">\(\ref{F4:TheoryConIntVarSlope}\)</span>, and <span class="math inline">\(\ref{F4:TheoryVarIntVarSlope}\)</span>
illustrate the expected responses of these models.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="one-factor-anova-model.html#cb6-1" tabindex="-1"></a><span class="co"># Insert figure code here (if applicable) for rendering in R Markdown</span></span></code></pre></div>
<p>For each model presented in Table <a href="#T4:OneFactorCovariate"><strong>??</strong></a>,
parameter estimates can be calculated using the method of least
squares. As usual, this means writing the expected response, <span class="math inline">\(E[y_{ij}]\)</span>, as a function of known variables and unknown parameters.
For the regression model with variable intercept and constant slope,
the least squares estimates can be expressed compactly as:</p>
<p><span class="math display">\[
b_1 = \frac{\sum_{j=1}^{c}\sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j) (y_{ij} - \bar{y}_j)}{\sum_{j=1}^{c}\sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2}
\]</span></p>
<p>and <span class="math inline">\(b_{0j} = \bar{y}_j - b_1 \bar{x}_j\)</span>. Similarly, the least
squares estimates for the regression model with variable intercept
and slope can be expressed as:</p>
<p><span class="math display">\[
b_{1j} = \frac{\sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j) (y_{ij} - \bar{y}_j)}{\sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2}
\]</span></p>
<p>and <span class="math inline">\(b_{0j} = \bar{y}_j - b_{1j} \bar{x}_j\)</span>. With these parameter
estimates, fitted values may be calculated.</p>
<p>For each model, fitted values are defined to be the expected response with
the unknown parameters replaced by their least squares estimates.
For example, for the regression model with variable intercept and
constant slope the fitted values are
<span class="math display">\[
\hat{y}_{ij} = b_{0j} + b_1 x_{ij}.
\]</span></p>
</div>
</div>
<div id="example-wisconsin-hospital-costs" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Example: Wisconsin Hospital Costs<a href="one-factor-anova-model.html#example-wisconsin-hospital-costs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now study the impact of various predictors on hospital
charges in the state of Wisconsin. Identifying predictors of
hospital charges can provide direction for hospitals, government,
insurers, and consumers in controlling these variables, which in turn
leads to better control of hospital costs. The data for the year
1989 were obtained from the Office of Health Care Information,
Wisconsin’s Department of Health and Human Services. Cross-sectional
data are used, which detail the 20 diagnosis-related group (DRG)
discharge costs for hospitals in the state of Wisconsin, broken down
into nine major health service areas and three types of payer (Fee
for service, HMO, and other). Even though there are 540 potential
DRG, area, and payer combinations (<span class="math inline">\(20 \times 9 \times 3 = 540\)</span>), only
526 combinations were actually realized in the 1989 data set. Other
predictor variables included the logarithm of the total number of
discharges (<span class="math inline">\(\text{NO DSCHG}\)</span>) and total number of hospital beds (<span class="math inline">\(\text{NUM BEDS}\)</span>)
for each combination. The response variable is the logarithm of
total hospital charges per number of discharges (<span class="math inline">\(\text{CHGNUM}\)</span>). To
streamline the presentation, we now consider only costs associated
with three diagnostic related groups (DRGs): DRG #209, DRG #391,
and DRG #430.</p>
<p>The covariate, <span class="math inline">\(x\)</span>, is the natural logarithm of the number of
discharges. In ideal settings, hospitals with more patients enjoy
lower costs due to economies of scale. In non-ideal settings,
hospitals may not have excess capacity and thus, hospitals with more
patients have higher costs. One purpose of this analysis is to
investigate the relationship between hospital costs and hospital
utilization.</p>
<p>Recall that our measure of hospital charges is the logarithm of
costs per discharge (<span class="math inline">\(y\)</span>). The scatter plot in Figure
<span class="math inline">\(\ref{F4:CostperNumber}\)</span> gives a preliminary idea of the relationship
between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>. We note that there appears to be a negative
relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>.</p>
<p>The negative relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> suggested by Figure
<span class="math inline">\(\ref{F4:CostperNumber}\)</span> is misleading and is induced by an
<em>omitted variable</em>, the category of the cost (DRG). To see
the joint effect of the categorical variable DRG and the continuous
variable <span class="math inline">\(x\)</span>, in Figure <span class="math inline">\(\ref{F4:DRGbyNumber}\)</span> is a plot of <span class="math inline">\(y\)</span> versus
<span class="math inline">\(x\)</span> where the plotting symbols are codes for the level of the
categorical variable. From this plot, we see that the level of cost
varies by level of the factor DRG. Moreover, for each level of DRG,
the slope between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> is either zero or positive. The slopes
are not negative, as suggested by Figure <span class="math inline">\(\ref{F4:CostperNumber}\)</span>.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="one-factor-anova-model.html#cb7-1" tabindex="-1"></a><span class="co"># Insert figure code here (if applicable) for rendering in R Markdown</span></span></code></pre></div>
<table>
<caption>(#tab:T4:DRGModels)Wisconsin Hospital Cost Models Goodness of Fit.</caption>
<colgroup>
<col width="26%" />
<col width="18%" />
<col width="18%" />
<col width="15%" />
<col width="10%" />
<col width="9%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Model Description</th>
<th align="right">Model degrees of freedom</th>
<th align="right">Error degrees of freedom</th>
<th align="right">Error Sum of Squares</th>
<th align="right">R-squared (%)</th>
<th align="right">Mean Square</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">One factor ANOVA</td>
<td align="right">2</td>
<td align="right">76</td>
<td align="right">9.396</td>
<td align="right">93.3</td>
<td align="right">0.124</td>
</tr>
<tr class="even">
<td align="left">Regression with constant intercept</td>
<td align="right">1</td>
<td align="right">77</td>
<td align="right">115.059</td>
<td align="right">18.2</td>
<td align="right">1.222</td>
</tr>
<tr class="odd">
<td align="left">~~~and slope</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="left">Regression with variable intercept</td>
<td align="right">3</td>
<td align="right">75</td>
<td align="right">7.482</td>
<td align="right">94.7</td>
<td align="right">0.100</td>
</tr>
<tr class="odd">
<td align="left">~~~and constant slope</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="left">Regression with constant intercept</td>
<td align="right">3</td>
<td align="right">75</td>
<td align="right">14.048</td>
<td align="right">90.0</td>
<td align="right">0.187</td>
</tr>
<tr class="odd">
<td align="left">~~~and variable slope</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="left">Regression with variable intercept</td>
<td align="right">5</td>
<td align="right">73</td>
<td align="right">5.458</td>
<td align="right">96.1</td>
<td align="right">0.075</td>
</tr>
<tr class="odd">
<td align="left">~~~and slope</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
</tbody>
</table>
<p>Each of the five models defined in Table <a href="#T4:OneFactorCovariate"><strong>??</strong></a>
was fit to this subset of the Hospital case study. The summary
statistics are in Table <span class="math inline">\(\ref{T4:DRGModels}\)</span>. For this data set, there
are <span class="math inline">\(n = 79\)</span> observations and <span class="math inline">\(c = 3\)</span> levels of the DRG factor. For each
model, the model degrees of freedom is the number of model
parameters minus one. The error degrees of freedom is the number of
observations minus the number of model parameters.</p>
<p>Using binary variables, each of the models in Table <a href="#T4:OneFactorCovariate"><strong>??</strong></a> can be written in a regression format. As we have seen in Section <span class="math inline">\(\ref{S4:SeveralCoeff}\)</span>, when a model can be written as a subset of another, larger model, we have formal testing procedures available to decide which model is more appropriate. To illustrate this testing procedure with our DRG example, from Table <span class="math inline">\(\ref{T4:DRGModels}\)</span> and the associated plots, it seems clear that the DRG factor is important. Further, a <span class="math inline">\(t\)</span>-test, not presented here, shows that the covariate <span class="math inline">\(x\)</span> is important. Thus, let’s compare the full model <span class="math inline">\(E[y_{ij}] = \beta_{0,j} + \beta_{1,j}x\)</span> to the reduced model <span class="math inline">\(E[y_{ij}] = \beta_{0,j} + \beta_1x\)</span>. In other words, is there a different slope for each DRG?</p>
<p>Using the notation from Section <span class="math inline">\(\ref{S4:SeveralCoeff}\)</span>, we call the variable intercept and slope the full model. Under the null hypothesis, <span class="math inline">\(H_0: \beta_{1,1} = \beta_{1,2} = \beta_{1,3}\)</span>, we get the variable intercept, constant slope model. Thus, using the <span class="math inline">\(F\)</span>-ratio in equation (<span class="math inline">\(\ref{E4:FratioErrSumSquares}\)</span>), we have</p>
<p><span class="math display">\[
F\text{-ratio} = \frac{(Error~SS)_{reduced} - (Error~SS)_{full}}{ps_{full}^2} = \frac{7.482 - 5.458}{2 \times 0.075} = 13.535.
\]</span></p>
<p>The 95th percentile from the <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(df_1 = p = 2\)</span> and <span class="math inline">\(df_2 = (df)_{full} = 73\)</span> is approximately 3.13. Thus, this test leads us to reject the null hypothesis and declare the alternative, the regression model with variable intercept and variable slope, to be valid.</p>
<hr />
<div id="combining-two-factors" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Combining Two Factors<a href="one-factor-anova-model.html#combining-two-factors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have seen how to combine covariates as well as a covariate and factor, both additively and with interactions. In the same fashion, suppose that we have two factors, say gender (two levels, male/female) and age (three levels, young/middle/old). Let the corresponding binary variables be <span class="math inline">\(x_1\)</span> to indicate whether the observation represents a female, <span class="math inline">\(x_2\)</span> to indicate whether the observation represents a young person, and <span class="math inline">\(x_3\)</span> to indicate whether the observation represents a middle-aged person.</p>
<p>An <em>additive model</em> for these two factors may use the regression function</p>
<p><span class="math display">\[
E[y] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3.
\]</span></p>
<p>As we have seen, this model is simple to interpret. For example, we can interpret <span class="math inline">\(\beta_1\)</span> to be the gender effect, holding age constant.</p>
<p>We can also incorporate two interaction terms, <span class="math inline">\(x_1 x_2\)</span> and <span class="math inline">\(x_1 x_3\)</span>. Using all five explanatory variables yields the regression function</p>
<p><span class="math display">\[
E[y] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3.
\]</span></p>
<p>Here, the variables <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span> are known as the <em>main effects</em>. Table <span class="math inline">\(\ref{T4:TwoFactorModel}\)</span> helps interpret this equation. Specifically, there are six types of people that we could encounter: males and females who are young, middle-aged, or old. We have six parameters in the equation above. Table <span class="math inline">\(\ref{T4:TwoFactorModel}\)</span> provides the link between the parameters and the types of people. By using the interaction terms, we do not impose any prior specifications on the additive effects of each factor. From Table <span class="math inline">\(\ref{T4:TwoFactorModel}\)</span>, we see that the interpretation of the regression coefficients is not straightforward. However, using the additive model with interaction terms is equivalent to creating a new categorical variable with six levels, one for each type of person. If the interaction terms are critical in your study, you may wish to create a new factor that incorporates the interaction terms simply for ease of interpretation.</p>
<table style="width:100%;">
<caption>(#tab:T4:TwoFactorModel)Regression Function for a Two Factor Model with Interactions</caption>
<colgroup>
<col width="8%" />
<col width="8%" />
<col width="7%" />
<col width="7%" />
<col width="7%" />
<col width="7%" />
<col width="7%" />
<col width="47%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Gender</th>
<th align="left">Age</th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right"><span class="math inline">\(x_2\)</span></th>
<th align="right"><span class="math inline">\(x_3\)</span></th>
<th align="right"><span class="math inline">\(x_4\)</span></th>
<th align="right"><span class="math inline">\(x_5\)</span></th>
<th align="left">Regression Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Male</td>
<td align="left">Young</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left"><span class="math inline">\(\beta_0 + \beta_2\)</span></td>
</tr>
<tr class="even">
<td align="left">Male</td>
<td align="left">Middle</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left"><span class="math inline">\(\beta_0 + \beta_3\)</span></td>
</tr>
<tr class="odd">
<td align="left">Male</td>
<td align="left">Old</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left"><span class="math inline">\(\beta_0\)</span></td>
</tr>
<tr class="even">
<td align="left">Female</td>
<td align="left">Young</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left"><span class="math inline">\(\beta_0 + \beta_1 + \beta_2 + \beta_4\)</span></td>
</tr>
<tr class="odd">
<td align="left">Female</td>
<td align="left">Middle</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left"><span class="math inline">\(\beta_0 + \beta_1 + \beta_3 + \beta_5\)</span></td>
</tr>
<tr class="even">
<td align="left">Female</td>
<td align="left">Old</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left"><span class="math inline">\(\beta_0 + \beta_1\)</span></td>
</tr>
</tbody>
</table>
<p>Extensions to more than two factors follow in a similar fashion. For example, suppose that you are examining the behavior of firms with headquarters in ten geographic regions, two organizational structures (profit versus non-profit) with four years of data. If you decide to treat each variable as a factor and want to model all interaction terms, then this is equivalent to a factor with <span class="math inline">\(10 \times 2 \times 4 = 80\)</span> levels. Models with interaction terms can have a substantial number of parameters and the analyst must be prudent when specifying interactions to be considered.</p>
</div>
<div id="general-linear-model" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> General Linear Model<a href="one-factor-anova-model.html#general-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The general linear model extends the linear regression model in two ways. First, explanatory variables may be continuous, categorical, or a combination. The only restriction is that they enter linearly such that the resulting regression function</p>
<p><span class="math display">\[
\mathrm{E}~y = \beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k
\]</span></p>
<p>is a linear combination of coefficients. As we have seen, we can square continuous variables or take other nonlinear transforms (such as logarithms) as well as use binary variables to represent categorical variables, so this “restriction,” as the name suggests, allows for a broad class of general functions to represent data.</p>
<p>The second extension is that the explanatory variables may be linear combinations of one another in the general linear model. Because of this, in the general linear model case, the parameter estimates need not be unique. However, an important feature of the general linear model is that the resulting fitted values turn out to be unique, using the method of least squares.</p>
<p>For example, in Section 4.3 we saw that the one factor ANOVA model could be expressed as a regression model with <span class="math inline">\(c\)</span> indicator variables. However, if we had attempted to estimate the model in</p>
<p><span class="math display">\[
y_{ij} = \beta_{0j} + \beta_{1j} x_{ij} + \varepsilon_{ij}
\]</span></p>
<p>the method of least squares would not have arrived at a unique set of regression coefficient estimates. The reason is that, in this equation, each explanatory variable can be expressed as a linear combination of the others. For example, observe that <span class="math inline">\(x_c = 1 - (x_1 + x_2 + \ldots + x_{c-1})\)</span>.</p>
<p>The fact that parameter estimates are not unique is a drawback, but not an overwhelming one. The assumption that the explanatory variables are not linear combinations of one another means that we can compute unique estimates of the regression coefficients using the method of least squares. In terms of matrices, because the explanatory variables are not linear combinations of one another, the matrix <span class="math inline">\(\mathbf{X}^{\prime}\mathbf{X}\)</span> is not invertible.</p>
<p>Specifically, suppose that we are considering the regression function in</p>
<p><span class="math display">\[
\mathrm{E}~y = \beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k
\]</span></p>
<p>and, using the method of least squares, our regression coefficient estimates are <span class="math inline">\(b_0^{o}, b_1^{o}, \ldots, b_k^{o}\)</span>. This set of regression coefficient estimates minimizes our error sum of squares, but there may be other sets of coefficients that also minimize the error sum of squares. The fitted values are computed as <span class="math inline">\(\hat{y}_i = b_0^{o} + b_1^{o} x_{i1} + \ldots + b_k^{o} x_{ik}\)</span>. It can be shown that the resulting fitted values are unique, in the sense that any set of coefficients that minimize the error sum of squares produce the same fitted values (see Section <span class="math inline">\(\ref{S4:GeneralLinearModel}\)</span>).</p>
<p>Thus, for a set of data and a specified general linear model, fitted values are unique. Because residuals are computed as observed responses minus fitted values, we have that the residuals are unique. Because residuals are unique, we have the error sums of squares are unique. Thus, it seems reasonable, and is true, that we can use the general test of hypotheses described in Section <span class="math inline">\(\ref{S4:SeveralCoeff}\)</span> to decide whether collections of explanatory variables are important.</p>
<p>To summarize, for general linear models, parameter estimates may not be unique and thus not meaningful. An important part of regression models is the interpretation of regression coefficients. This interpretation is not necessarily available in the general linear model context. However, for general linear models, we may still discuss the importance of an individual variable or collection of variables through partial <em>F</em>-tests. Further, fitted values, and the corresponding exercise of prediction, works in the general linear model context. The advantage of the general linear model context is that we need not worry about the type of restrictions to impose on the parameters. Although not the subject of this text, this advantage is particularly important in complicated experimental designs used in the life sciences. The reader will find that general linear model estimation routines are widely available in statistical software packages available on the market today.</p>
</div>
</div>
<div id="further-reading-and-references" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Further Reading and References<a href="one-factor-anova-model.html#further-reading-and-references" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="chapter-references" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Chapter References<a href="one-factor-anova-model.html#chapter-references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Keeler, Emmett B., and John E. Rolph (1988). The demand for episodes of treatment in the Health Insurance Experiment. <em>Journal of Health Economics</em> 7: 337-367.</p></li>
<li><p>Searle, Shayle R. (1987). <em>Linear Models for Unbalanced Data</em>. John Wiley &amp; Sons, New York.</p></li>
</ul>
</div>
</div>
<div id="exercises" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Exercises<a href="one-factor-anova-model.html#exercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>In this exercise, we consider relating two statistics that summarize how well a regression model fits, the <span class="math inline">\(F\)</span>-ratio and <span class="math inline">\(R^2\)</span>, the coefficient of determination. (Here, the <span class="math inline">\(F\)</span>-ratio is the statistic used to test model adequacy, not a partial <span class="math inline">\(F\)</span> statistic.)</p>
<ol style="list-style-type: lower-alpha">
<li><p>Write down both <span class="math inline">\(R^2\)</span> in terms of Error SS and Regression SS.</p></li>
<li><p>Write down <span class="math inline">\(F\)</span>-ratio in terms of Error SS, Regression SS, <span class="math inline">\(k\)</span>, and <span class="math inline">\(n\)</span>.</p></li>
<li><p>Establish the algebraic relationship
<span class="math display">\[
F\text{-ratio} = \frac{R^2}{1-R^2} \frac{n-(k+1)}{k}.
\]</span></p></li>
<li><p>Suppose that <span class="math inline">\(n = 40\)</span>, <span class="math inline">\(k = 5\)</span>, and <span class="math inline">\(R^2 = 0.20\)</span>. Calculate the <span class="math inline">\(F\)</span>-ratio. Perform the usual test of model adequacy to determine whether or not the five explanatory variables jointly significantly affect the response variable.</p></li>
<li><p>Suppose that <span class="math inline">\(n = 400\)</span> (not 40), <span class="math inline">\(k = 5\)</span>, and <span class="math inline">\(R^2 = 0.20\)</span>. Calculate the <span class="math inline">\(F\)</span>-ratio. Perform the usual test of model adequacy to determine whether or not the five explanatory variables jointly significantly affect the response variable.</p></li>
</ol>
<p></p></li>
<li><p><strong>Hospital Costs</strong>. This exercise considers hospital expenditures data provided by the US Agency for Healthcare Research and Quality (AHRQ) and described in Exercise 1.<span class="math inline">\(\ref{Ex:HospExpend}\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Produce a scatter plot, correlation, and a linear regression of <span class="math inline">\(LNTOTCHG\)</span> on <span class="math inline">\(AGE\)</span>. Is <span class="math inline">\(AGE\)</span> a significant predictor of <span class="math inline">\(LNTOTCHG\)</span>?</p></li>
<li><p>You are concerned that newborns follow a different pattern than other ages. Create a binary variable that indicates whether or not <span class="math inline">\(AGE\)</span> equals zero. Run a regression using this binary variable and <span class="math inline">\(AGE\)</span> as explanatory variables. Is the binary variable statistically significant?</p></li>
<li><p>Now examine the gender effect, using the binary variable <span class="math inline">\(FEMALE\)</span> that is one if the patient is female and zero otherwise. Run a regression using <span class="math inline">\(AGE\)</span> and <span class="math inline">\(FEMALE\)</span> as explanatory variables. Run a second regression including these two variables with an interaction term. Comment on whether the gender effect is important in either model.</p></li>
<li><p>Now consider the type of admission, <span class="math inline">\(APRDRG\)</span>, an acronym for “all patient refined diagnostic related group.” This is a categorical explanatory variable that provides information on the type of hospital admission. There are several hundred levels of this category. For example, level 640 represents admission for a normal newborn, with neonatal weight greater than or equal to 2.5 kilograms. As another example, level 225 represents admission resulting in an appendectomy.</p></li>
</ol>
<p>d(i). Run a one-factor ANOVA model, using <span class="math inline">\(APRDRG\)</span> to predict <span class="math inline">\(LNTOTCHG\)</span>. Examine the <span class="math inline">\(R^2\)</span> from this model and compare it to the coefficient of determination of the linear regression model of <span class="math inline">\(LNTOTCHG\)</span> on <span class="math inline">\(AGE\)</span>. Based on this comparison, which model do you think is preferred?</p>
<p>d(ii). For the one-factor model in part d(i), provide a 95% confidence interval for <span class="math inline">\(LNTOTCHG\)</span> for level 225 corresponding to an appendectomy. Convert your final answer from logarithmic dollars to dollars via exponentiation.</p>
<p>d(iii). Run a regression model of <span class="math inline">\(APRDRG\)</span>, <span class="math inline">\(FEMALE\)</span>, and <span class="math inline">\(AGE\)</span> on <span class="math inline">\(LNTOTCHG\)</span>. State whether <span class="math inline">\(AGE\)</span> is a statistically significant predictor of <span class="math inline">\(LNTOTCHG\)</span>. State whether <span class="math inline">\(FEMALE\)</span> is a statistically significant predictor of <span class="math inline">\(LNTOTCHG\)</span>.</p>
<p></p></li>
<li><p><strong>Nursing Home Utilization</strong>. This exercise considers nursing home data provided by the Wisconsin Department of Health and Family Services (DHFS) and described in Exercises 1.<span class="math inline">\(\ref{Ex:NursHome}\)</span>, 2.<span class="math inline">\(\ref{Ex:NursHome2a}\)</span>, and 2.<span class="math inline">\(\ref{Ex:NursHome2b}\)</span>.</p>
<p>In addition to the size variables, we also have information on several binary variables. The variable <span class="math inline">\(URBAN\)</span> is used to indicate the facility’s location. It is one if the facility is located in an urban environment and zero otherwise. The variable <span class="math inline">\(MCERT\)</span> indicates whether the facility is Medicare-certified. Most, but not all, nursing homes are certified to provide Medicare-funded care. There are three organizational structures for nursing homes. They are government (state, counties, municipalities), for-profit businesses, and tax-exempt organizations. Periodically, facilities may change ownership and, less frequently, ownership type. We create two binary variables <span class="math inline">\(PRO\)</span> and <span class="math inline">\(TAXEXEMPT\)</span> to denote for-profit business and tax-exempt organizations, respectively. Some nursing homes opt not to purchase private insurance coverage for their employees. Instead, these facilities directly provide insurance and pension benefits to their employees; this is referred to as “self funding of insurance.” We use binary variable <span class="math inline">\(SELFFUNDINS\)</span> to denote it.</p>
<p>You decide to examine the relationship between <span class="math inline">\(LOGTPY(y)\)</span> and the explanatory variables. Use cost report year 2001 data, and do the following analysis.</p>
<ol style="list-style-type: lower-alpha">
<li><p>There are three levels of organizational structures, but we only use two binary variables (<span class="math inline">\(PRO\)</span> and <span class="math inline">\(TAXEXEMPT\)</span>). Explain why.</p></li>
<li><p>Run a one-way analysis of variance using <span class="math inline">\(TAXEXEMPT\)</span> as the factor. Decide whether or not tax-exempt is an important factor in determining <span class="math inline">\(LOGTPY\)</span>. State your null hypothesis, alternative hypothesis, and all components of the decision-making rule. Use a 5% level of significance.</p></li>
<li><p>Run a one-way analysis of variance using <span class="math inline">\(MCERT\)</span> as the factor. Decide whether or not location is an important factor in determining <span class="math inline">\(LOGTPY\)</span>.</p></li>
</ol>
<p>c(i). Provide a point estimate of <span class="math inline">\(LOGTPY\)</span> for a nursing facility that is not Medicare-Certified.</p>
<p>c(ii). Provide a 95% confidence interval for your point estimate in part (i).</p>
<ol start="4" style="list-style-type: lower-alpha">
<li><p>Run a regression model using the binary variables, <span class="math inline">\(URBAN\)</span>, <span class="math inline">\(PRO\)</span>, <span class="math inline">\(TAXEXEMPT\)</span>, <span class="math inline">\(SELFFUNDINS\)</span>, and <span class="math inline">\(MCERT\)</span>. Find <span class="math inline">\(R^2\)</span>. Which variables are statistically significant?</p></li>
<li><p>Run a regression model using all explanatory variables, <span class="math inline">\(LOGNUMBED\)</span>, <span class="math inline">\(LOGSQRFOOT\)</span>, <span class="math inline">\(URBAN\)</span>, <span class="math inline">\(PRO\)</span>, <span class="math inline">\(TAXEXEMPT\)</span>, <span class="math inline">\(SELFFUNDINS\)</span>, and <span class="math inline">\(MCERT\)</span>. Find <span class="math inline">\(R^2\)</span>. Which variables are statistically significant?</p></li>
</ol>
<p>e(i). Calculate the partial correlation between <span class="math inline">\(LOGTPY\)</span> and <span class="math inline">\(LOGSQRFOOT\)</span>. Compare this to the correlation between <span class="math inline">\(LOGTPY\)</span> and <span class="math inline">\(LOGSQRFOOT\)</span>. Explain why the partial correlation is small.</p>
<p>e(ii). Compare the low level of the <span class="math inline">\(t\)</span>-ratios (for testing the importance of individual regression coefficients) and the high level of the <span class="math inline">\(F\)</span>-ratio (for testing model adequacy). Describe the seeming inconsistency and provide an explanation for this inconsistency.</p></li>
<li><p><strong>Automobile Insurance Claims</strong>. Refer to Exercise 1.<span class="math inline">\(\ref{Ex:AutoClaims}\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Run a regression of <span class="math inline">\(LNPAID\)</span> on <span class="math inline">\(AGE\)</span>. Is <span class="math inline">\(AGE\)</span> a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. Also comment on the goodness of fit of this variable.</p></li>
<li><p>Consider using class as a single explanatory variable. Use the one factor to estimate the model and respond to the following questions.</p></li>
</ol>
<p>b(i). What is the point estimate of claims in class C7, drivers 50-69, driving to work or school, less than 30 miles per week with annual mileage under 7500, in natural logarithmic units?</p>
<p>b(ii). Determine the corresponding 95% confidence interval of expected claims, in natural logarithmic units.</p>
<p>b(iii). Convert the 95% confidence interval of expected claims that you determined in part b(ii) to dollars.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Run a regression of <span class="math inline">\(LNPAID\)</span> on <span class="math inline">\(AGE\)</span>, <span class="math inline">\(GENDER\)</span>, and the categorical variables <span class="math inline">\(STATE\ CODE\)</span> and <span class="math inline">\(CLASS\)</span>.</li>
</ol>
<p>c(i). Is <span class="math inline">\(GENDER\)</span> a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule.</p>
<p>c(ii). Is <span class="math inline">\(CLASS\)</span> a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule.</p>
<p>c(iii). Use the model to provide a point estimate of claims in dollars (not log dollars) for a male age 60 in STATE 2 in CLASS C7.</p>
<p>c(iv). Write down the coefficient associated with CLASS C7 and interpret this coefficient.</p>
<p></p></li>
<li><p><strong>Wisconsin Lottery Sales</strong>. This exercise considers State of Wisconsin lottery sales data that were described in Section 2.1 and examined in Exercise 3.<span class="math inline">\(\ref{Ex:Lottery3}\)</span>.</p>
<p><strong>Part 1:</strong> You decide to examine the relationship between SALES (<span class="math inline">\(y\)</span>) and all eight explanatory variables (<span class="math inline">\(PERPERHH\)</span>, <span class="math inline">\(MEDSCHYR\)</span>, <span class="math inline">\(MEDHVL\)</span>, <span class="math inline">\(PRCRENT\)</span>, <span class="math inline">\(PRC55P\)</span>, <span class="math inline">\(HHMEDAGE\)</span>, <span class="math inline">\(MEDINC\)</span>, and <span class="math inline">\(POP\)</span>).</p>
<ol style="list-style-type: lower-alpha">
<li><p>Fit a regression model of SALES on all eight explanatory variables.</p></li>
<li><p>Find <span class="math inline">\(R^2\)</span>.</p></li>
</ol>
<p>b(i). Use it to calculate the correlation coefficient between the observed and fitted values.</p>
<p>b(ii). You want to use <span class="math inline">\(R^2\)</span> to test the adequacy of the model in part (a). Use a formal test of hypothesis. State your null and alternative hypothesis, decision-making criterion, and your decision-making rules.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Test whether <span class="math inline">\(POP\)</span>, <span class="math inline">\(MEDSCHYR\)</span>, and <span class="math inline">\(MEDHVL\)</span> are jointly important explanatory variables for understanding SALES.</li>
</ol>
<p><strong>Part 2:</strong> After the preliminary analysis in Part 1, you decide to examine the relationship between SALES (<span class="math inline">\(y\)</span>) and <span class="math inline">\(POP\)</span>, <span class="math inline">\(MEDSCHYR\)</span>, and <span class="math inline">\(MEDHVL\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Fit a regression model of SALES on these three explanatory variables.</p></li>
<li><p>Has the coefficient of determination decreased from the eight-variable regression model to the three-variable model? Does this mean that the model is not improved or does it provide little information? Explain your response.</p></li>
<li><p>To state formally whether one should use the three or eight variable model, use a partial <span class="math inline">\(F\)</span>-test. State your null and alternative hypotheses, decision-making criterion, and your decision-making rules.</p></li>
</ol>
<p></p></li>
<li><p><strong>Insurance Company Expenses</strong>. This exercise considers insurance company data from the NAIC and described in Exercises 1.<span class="math inline">\(\ref{Ex:NAICExpense}\)</span> and 3.<span class="math inline">\(\ref{Ex:NAICExpense3}\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Are the quadratic terms important?</li>
</ol>
<p>Consider a linear model of <span class="math inline">\(LNEXPENSES\)</span> on twelve explanatory variables. For the explanatory variables, include <span class="math inline">\(ASSETS\)</span>, <span class="math inline">\(GROUP\)</span>, both versions of losses and gross premiums, as well as the two BLS variables. Also include the square of each of the two loss and the two gross premium variables.</p>
<p>Test whether the four squared terms are jointly statistically significant, using a partial <span class="math inline">\(F\)</span>-test. State your null and alternative hypotheses, decision-making criterion, and your decision-making rules.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Are the interaction terms with <span class="math inline">\(GROUP\)</span> important?</li>
</ol>
<p>Omit the two BLS variables, so that now there are eleven variables: <span class="math inline">\(ASSETS\)</span>, <span class="math inline">\(GROUP\)</span>, both versions of losses and gross premiums, as well as interactions of <span class="math inline">\(GROUP\)</span> with <span class="math inline">\(ASSETS\)</span> and both versions of losses and gross premiums.</p>
<p>Test whether the five interaction terms are jointly statistically significant, using a partial <span class="math inline">\(F\)</span>-test. State your null and alternative hypotheses, decision-making criterion, and your decision-making rules.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>You are examining a company that is not in the sample with values <span class="math inline">\(LOSSLONG = 0.025\)</span>, <span class="math inline">\(LOSSSHORT = 0.040\)</span>, <span class="math inline">\(GPWPERSONAL = 0.050\)</span>, <span class="math inline">\(GPWCOMM = 0.120\)</span>, <span class="math inline">\(ASSETS = 0.400\)</span>, <span class="math inline">\(CASH = 0.350\)</span>, and <span class="math inline">\(GROUP = 1\)</span>.</li>
</ol>
<p>Use the eleven variable interaction model in part (b) to produce a 95% prediction interval for this company.</p></li>
<li><p><strong>National Life Expectancies</strong>. We continue the analysis begun in Exercises 1.<span class="math inline">\(\ref{Ex:UNLIFE}\)</span>, 2.<span class="math inline">\(\ref{Ex:UNLIFE2}\)</span>, and 3.<span class="math inline">\(\ref{Ex:UNLIFE3}\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Consider the regression using three explanatory variables, <span class="math inline">\(FERTILITY\)</span>, <span class="math inline">\(PUBLICEDUCATION\)</span>, and <span class="math inline">\(\ln{HEALTH}\)</span> that you did in Exercise 3.<span class="math inline">\(\ref{Ex:UNLIFE3}\)</span>. Test whether <span class="math inline">\(PUBLICEDUCATION\)</span> and <span class="math inline">\(\ln{HEALTH}\)</span> are jointly statistically significant, using a partial <span class="math inline">\(F\)</span>-test. State your null and alternative hypotheses, decision-making criterion, and your decision-making rules. (Hint: Use the coefficient of determination form for calculating the test statistic.) Provide an approximate <span class="math inline">\(p\)</span>-value for the test.</p></li>
<li><p>We now introduce the <span class="math inline">\(REGION\)</span> variable, summarized in Table <span class="math inline">\(\ref{Ex:UNLIFERegionStats}\)</span>. A boxplot of life expectancies versus <span class="math inline">\(REGION\)</span> is given in Figure <span class="math inline">\(\ref{Ex:UNLIFEPlot3}\)</span>. Describe what we learn from the Table and boxplot about the effect of <span class="math inline">\(REGION\)</span> on <span class="math inline">\(LIFEEXP\)</span>.</p></li>
</ol>
<p><embed src="Chapter4/UNLife3.eps" />
<em>Figure <span class="math inline">\(\ref{Ex:UNLIFEPlot3}\)</span>: Boxplots of LIFEEXP by REGION.</em></p>
<p>Table <span class="math inline">\(\ref{Ex:UNLIFERegionStats}\)</span> summarizes average life expectancy by region.</p>
<table>
<thead>
<tr class="header">
<th>REGION</th>
<th>Region Description</th>
<th>Number</th>
<th>Mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Arab States</td>
<td>13</td>
<td>71.9</td>
</tr>
<tr class="even">
<td>2</td>
<td>East Asia and the Pacific</td>
<td>17</td>
<td>69.1</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Latin American and the Carribean</td>
<td>25</td>
<td>72.8</td>
</tr>
<tr class="even">
<td>4</td>
<td>South Asia</td>
<td>7</td>
<td>65.1</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Southern Europe</td>
<td>3</td>
<td>67.4</td>
</tr>
<tr class="even">
<td>6</td>
<td>Sub-Saharan Africa</td>
<td>38</td>
<td>52.2</td>
</tr>
<tr class="odd">
<td>7</td>
<td>Central and Eastern Europe</td>
<td>24</td>
<td>71.6</td>
</tr>
<tr class="even">
<td>8</td>
<td>High Income OECD</td>
<td>23</td>
<td>79.6</td>
</tr>
<tr class="odd">
<td></td>
<td>All</td>
<td>150</td>
<td>67.4</td>
</tr>
</tbody>
</table>
<ol start="3" style="list-style-type: lower-alpha">
<li><p>Fit a regression model using only the factor, <span class="math inline">\(REGION\)</span>. Is <span class="math inline">\(REGION\)</span> a statistically significant determinant of <span class="math inline">\(LIFEEXP\)</span>? State your null and alternative hypotheses, decision-making criterion, and your decision-making rules.</p></li>
<li><p>Fit a regression model using three explanatory variables, <span class="math inline">\(FERTILITY\)</span>, <span class="math inline">\(PUBLICEDUCATION\)</span>, and <span class="math inline">\(\ln{HEALTH}\)</span>, as well as the categorical variable <span class="math inline">\(REGION\)</span>.</p></li>
</ol>
<p>d(i). You are examining a country not in the sample with values <span class="math inline">\(FERTILITY = 2.0\)</span>, <span class="math inline">\(PUBLICEDUCATION = 5.0\)</span>, and <span class="math inline">\(\ln{HEALTH} = 1.0\)</span>. Produce two predicted life expectancy values by assuming that the country is from (1) an Arab state and (2) Sub-Saharan Africa.</p>
<p>d(ii). Provide a 95% confidence interval for the difference in life expectancies between an Arab state and a country from Sub-Saharan Africa.</p>
<p>d(iii). Provide the (usual ordinary least squares) point estimate for the difference in life expectancies between a country from Sub-Saharan Africa and a high-income OECD (Organization for Economic Co-operation and Development) country.</p></li>
</ol>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="estimating-and-predicting-several-coefficients.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="technical-supplement---matrix-expressions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
