<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Estimation and Goodness of Fit | Regression Modeling with Actuarial and Financial Applications</title>
  <meta name="description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Estimation and Goodness of Fit | Regression Modeling with Actuarial and Financial Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Estimation and Goodness of Fit | Regression Modeling with Actuarial and Financial Applications" />
  
  <meta name="twitter:description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  

<meta name="author" content="Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multiple-linear-regression---i.html"/>
<link rel="next" href="some-special-explanatory-variables.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript"  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleSolution(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}      
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>



<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Regression Modeling With Actuarial and Financial Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#forward"><i class="fa fa-check"></i>Forward</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#who-is-this-book-for"><i class="fa fa-check"></i>Who Is This Book For?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#what-is-this-book-about"><i class="fa fa-check"></i>What Is This Book About?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#how-does-this-book-deliver-its-message"><i class="fa fa-check"></i>How Does This Book Deliver Its Message?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="multiple-linear-regression---i.html"><a href="multiple-linear-regression---i.html"><i class="fa fa-check"></i><b>1</b> Multiple Linear Regression - I</a>
<ul>
<li class="chapter" data-level="1.1" data-path="multiple-linear-regression---i.html"><a href="multiple-linear-regression---i.html#S3:LSMethod"><i class="fa fa-check"></i><b>1.1</b> Method of Least Squares</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="multiple-linear-regression---i.html"><a href="multiple-linear-regression---i.html#least-squares-method"><i class="fa fa-check"></i><b>1.1.1</b> Least Squares Method</a></li>
<li class="chapter" data-level="1.1.2" data-path="multiple-linear-regression---i.html"><a href="multiple-linear-regression---i.html#example-term-life-insurance"><i class="fa fa-check"></i><b>1.1.2</b> Example: Term Life Insurance</a></li>
<li class="chapter" data-level="1.1.3" data-path="multiple-linear-regression---i.html"><a href="multiple-linear-regression---i.html#general-case-with-k-explanatory-variables"><i class="fa fa-check"></i><b>1.1.3</b> General Case with k Explanatory Variables</a></li>
<li class="chapter" data-level="1.1.4" data-path="multiple-linear-regression---i.html"><a href="multiple-linear-regression---i.html#summarizing-the-data"><i class="fa fa-check"></i><b>1.1.4</b> Summarizing the Data</a></li>
<li class="chapter" data-level="1.1.5" data-path="multiple-linear-regression---i.html"><a href="multiple-linear-regression---i.html#method-of-least-squares"><i class="fa fa-check"></i><b>1.1.5</b> Method of Least Squares</a></li>
<li class="chapter" data-level="1.1.6" data-path="multiple-linear-regression---i.html"><a href="multiple-linear-regression---i.html#matrix-notation"><i class="fa fa-check"></i><b>1.1.6</b> Matrix Notation</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="multiple-linear-regression---i.html"><a href="multiple-linear-regression---i.html#linear-regression-model-and-properties-of-estimators"><i class="fa fa-check"></i><b>1.2</b> Linear Regression Model and Properties of Estimators</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="multiple-linear-regression---i.html"><a href="multiple-linear-regression---i.html#regression-function"><i class="fa fa-check"></i><b>1.2.1</b> Regression Function</a></li>
<li class="chapter" data-level="1.2.2" data-path="multiple-linear-regression---i.html"><a href="multiple-linear-regression---i.html#regression-coefficient-interpretation"><i class="fa fa-check"></i><b>1.2.2</b> Regression Coefficient Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="multiple-linear-regression---i.html"><a href="multiple-linear-regression---i.html#model-assumptions"><i class="fa fa-check"></i><b>1.3</b> Model Assumptions</a></li>
<li class="chapter" data-level="1.4" data-path="multiple-linear-regression---i.html"><a href="multiple-linear-regression---i.html#properties-of-regression-coefficient-estimators"><i class="fa fa-check"></i><b>1.4</b> Properties of Regression Coefficient Estimators</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="estimation-and-goodness-of-fit.html"><a href="estimation-and-goodness-of-fit.html"><i class="fa fa-check"></i><b>2</b> Estimation and Goodness of Fit</a>
<ul>
<li class="chapter" data-level="2.1" data-path="estimation-and-goodness-of-fit.html"><a href="estimation-and-goodness-of-fit.html#residual-standard-deviation"><i class="fa fa-check"></i><b>2.1</b> Residual Standard Deviation</a></li>
<li class="chapter" data-level="2.2" data-path="estimation-and-goodness-of-fit.html"><a href="estimation-and-goodness-of-fit.html#the-coefficient-of-determination-r2"><i class="fa fa-check"></i><b>2.2</b> The Coefficient of Determination: <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="2.3" data-path="estimation-and-goodness-of-fit.html"><a href="estimation-and-goodness-of-fit.html#statistical-inference-for-a-single-coefficient"><i class="fa fa-check"></i><b>2.3</b> Statistical Inference for a Single Coefficient</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="estimation-and-goodness-of-fit.html"><a href="estimation-and-goodness-of-fit.html#the-t-test"><i class="fa fa-check"></i><b>2.3.1</b> The <em>t</em>-Test</a></li>
<li class="chapter" data-level="2.3.2" data-path="estimation-and-goodness-of-fit.html"><a href="estimation-and-goodness-of-fit.html#confidence-intervals"><i class="fa fa-check"></i><b>2.3.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="2.3.3" data-path="estimation-and-goodness-of-fit.html"><a href="estimation-and-goodness-of-fit.html#added-variable-plots"><i class="fa fa-check"></i><b>2.3.3</b> Added Variable Plots</a></li>
<li class="chapter" data-level="2.3.4" data-path="estimation-and-goodness-of-fit.html"><a href="estimation-and-goodness-of-fit.html#producing-an-added-variable-plot"><i class="fa fa-check"></i><b>2.3.4</b> Producing an Added Variable Plot</a></li>
<li class="chapter" data-level="2.3.5" data-path="estimation-and-goodness-of-fit.html"><a href="estimation-and-goodness-of-fit.html#partial-correlation-coefficients"><i class="fa fa-check"></i><b>2.3.5</b> Partial Correlation Coefficients</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="some-special-explanatory-variables.html"><a href="some-special-explanatory-variables.html"><i class="fa fa-check"></i><b>3</b> Some Special Explanatory Variables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="some-special-explanatory-variables.html"><a href="some-special-explanatory-variables.html#binary-variables"><i class="fa fa-check"></i><b>3.1</b> Binary Variables</a></li>
<li class="chapter" data-level="3.2" data-path="some-special-explanatory-variables.html"><a href="some-special-explanatory-variables.html#transforming-explanatory-variables"><i class="fa fa-check"></i><b>3.2</b> Transforming Explanatory Variables</a></li>
<li class="chapter" data-level="3.3" data-path="some-special-explanatory-variables.html"><a href="some-special-explanatory-variables.html#interaction-terms"><i class="fa fa-check"></i><b>3.3</b> Interaction Terms</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="some-special-explanatory-variables.html"><a href="some-special-explanatory-variables.html#example-term-life-insurance---continued"><i class="fa fa-check"></i><b>3.3.1</b> Example: Term Life Insurance - Continued</a></li>
<li class="chapter" data-level="3.3.2" data-path="some-special-explanatory-variables.html"><a href="some-special-explanatory-variables.html#example-life-insurance-company-expenses"><i class="fa fa-check"></i><b>3.3.2</b> Example: Life Insurance Company Expenses</a></li>
<li class="chapter" data-level="3.3.3" data-path="some-special-explanatory-variables.html"><a href="some-special-explanatory-variables.html#special-case-curvilinear-response-functions"><i class="fa fa-check"></i><b>3.3.3</b> Special Case: Curvilinear Response Functions</a></li>
<li class="chapter" data-level="3.3.4" data-path="some-special-explanatory-variables.html"><a href="some-special-explanatory-variables.html#special-case-nonlinear-functions-of-a-continuous-variable"><i class="fa fa-check"></i><b>3.3.4</b> Special Case: Nonlinear Functions of a Continuous Variable</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="some-special-explanatory-variables.html"><a href="some-special-explanatory-variables.html#further-reading-and-references"><i class="fa fa-check"></i><b>3.4</b> Further Reading and References</a></li>
<li class="chapter" data-level="3.5" data-path="some-special-explanatory-variables.html"><a href="some-special-explanatory-variables.html#exercises"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTextDev/RegressionSpanish/" target="blank">Spanish Regression on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Regression Modeling with Actuarial and Financial Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimation-and-goodness-of-fit" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Estimation and Goodness of Fit<a href="estimation-and-goodness-of-fit.html#estimation-and-goodness-of-fit" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="residual-standard-deviation" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Residual Standard Deviation<a href="estimation-and-goodness-of-fit.html#residual-standard-deviation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Additional properties of the regression coefficient estimators will be discussed when we focus on statistical inference. We now continue our estimation discussion by providing an estimator of the other parameter in the linear regression model, <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Our estimator for <span class="math inline">\(\sigma^2\)</span> can be developed using the principle of replacing theoretical expectations by sample averages. Examining <span class="math inline">\(\sigma^2 = \mathrm{E}\left( y-\mathrm{E~}y\right)^2\)</span>, replacing the outer expectation by a sample average suggests using the estimator <span class="math inline">\(n^{-1}\sum_{i=1}^{n}(y_i-\mathrm{E~}y_i)^2\)</span>. Because we do not observe <span class="math inline">\(\mathrm{E}~y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik}\)</span>, we use in its place the corresponding observed quantity <span class="math inline">\(b_0 + b_1 x_{i1} + \ldots + b_k x_{ik} = \widehat{y}_i\)</span>. This leads to the following.</p>
<hr />
<p><em>Definition.</em> An estimator of <span class="math inline">\(\sigma^2\)</span>, the <em>mean square error (MSE)</em>, is defined as</p>
<p><span class="math display">\[
s^2 = \frac{1}{n-(k+1)}\sum_{i=1}^{n}\left( y_i - \widehat{y}_i \right)^2.
\]</span></p>
<p>The positive square root, <span class="math inline">\(s = \sqrt{s^2}\)</span>, is called the <em>residual standard deviation</em>.</p>
<hr />
<p>This expression generalizes the definition in equation (2.3), which is valid for <span class="math inline">\(k=1\)</span>. It turns out, by using <span class="math inline">\(n-(k+1)\)</span> instead of <span class="math inline">\(n\)</span> in the denominator of the equation above, that <span class="math inline">\(s^2\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. Essentially, by using <span class="math inline">\(\widehat{y}_i\)</span> instead of <span class="math inline">\(\mathrm{E~}y_i\)</span> in the definition, we have introduced some small dependencies among the deviations from the responses <span class="math inline">\(y_i - \widehat{y}_i\)</span>, thus reducing the overall variability. To compensate for this lower variability, we also reduce the denominator in the definition of <span class="math inline">\(s^2\)</span>.</p>
<p>To provide further intuition on the choice of <span class="math inline">\(n-(k+1)\)</span> in the definition of <span class="math inline">\(s^2\)</span>, we introduced the concept of residuals in the context of multiple linear regression. From Assumption E1, recall that the random errors can be expressed as <span class="math inline">\(\varepsilon_i = y_i - (\beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik})\)</span>. Because the parameters <span class="math inline">\(\beta_0, \ldots, \beta_k\)</span> are not observed, the errors themselves are not observed. Instead, we examine the “estimated errors,” or <em>residuals</em>, defined by <span class="math inline">\(e_i = y_i - \widehat{y}_i\)</span>.</p>
<p>Unlike errors, there exist certain dependencies among the residuals. One dependency is due to the algebraic fact that the average residual is zero. Further, there must be at least <span class="math inline">\(k+2\)</span> observations for there to be variation in the fit of the plane. If we have only <span class="math inline">\(k+1\)</span> observations, we could fit a plane to the data perfectly, resulting in no variation in the fit. For example, if <span class="math inline">\(k=1\)</span>, because two observations determine a line, then at least three observations are required to observe any deviation from the line. Because of these dependencies, we have only <span class="math inline">\(n-(k+1)\)</span> free, or unrestricted, residuals to estimate the variability about the regression plane.</p>
<p>The positive square root of <span class="math inline">\(s^2\)</span> is our estimator of <span class="math inline">\(\sigma\)</span>. Using residuals, it can be expressed as</p>
<p><span class="math display">\[
s = \sqrt{\frac{1}{n-(k+1)}\sum_{i=1}^{n}e_i^2}.
\]</span></p>
<p>Because it is based on residuals, we refer to <span class="math inline">\(s\)</span> as the <em>residual standard deviation</em>. The quantity <span class="math inline">\(s\)</span> is a measure of our “typical error.” For this reason, <span class="math inline">\(s\)</span> is also called the <em>standard error of the estimate</em>.</p>
</div>
<div id="the-coefficient-of-determination-r2" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> The Coefficient of Determination: <span class="math inline">\(R^2\)</span><a href="estimation-and-goodness-of-fit.html#the-coefficient-of-determination-r2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To summarize the goodness of fit of the model, as in Chapter 2 we partition the variability into pieces that are “explained” and “unexplained” by the regression fit. Algebraically, the calculations for regression using many variables are similar to the case of using only one variable. Unfortunately, when dealing with many variables, we do lose the easy graphical interpretation such as in Figure 2.4.</p>
<p>Begin with the total sum of squared deviations, <span class="math inline">\(Total~SS = \sum_{i=1}^{n}\left( y_i - \overline{y} \right)^2\)</span>, as our measure of the total variation in the data set. As in equation (2.1), we may then interpret the equation</p>
<p><span class="math display">\[
\begin{array}{ccccc}
\underbrace{y_i - \overline{y}} &amp; = &amp;
\underbrace{y_i - \widehat{y}_i} &amp; + &amp; \underbrace{\widehat{y}_i - \overline{y}} \\
\text{total} &amp; = &amp; \text{unexplained} &amp; + &amp; \text{explained} \\
\text{deviation} &amp; &amp; \text{deviation} &amp; &amp; \text{deviation}
\end{array}
\]</span></p>
<p>as the “deviation without knowledge of the explanatory variables equals the deviation not explained by the explanatory variables plus deviation explained by the explanatory variables.” Squaring each side and summing over all observations yields</p>
<p><span class="math display">\[
Total~SS = Error~SS + Regression~SS
\]</span></p>
<p>where <span class="math inline">\(Error~SS = \sum_{i=1}^{n}\left( y_i - \widehat{y}_i \right)^2\)</span> and <span class="math inline">\(Regression~SS = \sum_{i=1}^{n}\left( \widehat{y}_i - \overline{y} \right)^2\)</span>. As in Section 2.3 for the one explanatory variable case, the sum of the cross-product terms turns out to be zero.</p>
<p>A statistic that summarizes this relationship is the <em>coefficient of determination</em>,</p>
<p><span class="math display">\[
R^2 = \frac{Regression~SS}{Total~SS}.
\]</span></p>
<p>We interpret <span class="math inline">\(R^2\)</span> to be the proportion of variability explained by the regression function.</p>
<p>If the model is a desirable one for the data, one would expect a strong relationship between the observed responses and those “expected” under the model, the fitted values. An interesting algebraic fact is the following. If one squares the correlation coefficient between the responses and the fitted values, we get the coefficient of determination, that is,</p>
<p><span class="math display">\[
R^2 = \left[ r \left(y, \widehat{y} \right) \right]^2.
\]</span></p>
<p>As a result, <span class="math inline">\(R\)</span>, the positive square root of <span class="math inline">\(R^2\)</span>, is called the <em>multiple correlation coefficient</em>. It can be interpreted as the correlation between the response and the best linear combination of the explanatory variables, the fitted values. (This relationship is developed using matrix algebra in the technical supplement Section 5.10.1.)</p>
<p>The variability decomposition is also summarized using the <em>analysis of variance</em>, or <em>ANOVA</em>, table, as follows.</p>
<p><span class="math display">\[
\begin{array}{l|lcl}
\hline
\text{Source} &amp; \text{Sum of Squares} &amp; df &amp; \text{Mean Square} \\
\hline
\text{Regression} &amp; Regression~SS &amp; k &amp; Regression~MS \\
\text{Error} &amp; Error~SS &amp; n - (k + 1) &amp; MSE \\
\text{Total} &amp; Total~SS &amp; n - 1 &amp; \\
\hline
\end{array}
\]</span></p>
<p>The mean square column figures are defined to be the sum of squares figures divided by their respective degrees of freedom. The error degrees of freedom denotes the number of unrestricted residuals. It is this number that we use in our definition of the “average,” or mean, square error. That is, we define</p>
<p><span class="math display">\[
MSE = Error~MS = \frac{Error~SS}{n - (k + 1)} = s^2.
\]</span></p>
<p>Similarly, the regression degrees of freedom is the number of explanatory variables. This yields</p>
<p><span class="math display">\[
Regression~MS = \frac{Regression~SS}{k}.
\]</span></p>
<p>When discussing the coefficient of determination, it can be established that whenever an explanatory variable is added to the model, <span class="math inline">\(R^2\)</span> never decreases. This is true whether or not the additional variable is useful. We would like a measure of fit that decreases when useless variables are entered into the model as explanatory variables. To circumvent this anomaly, a widely used statistic is the <em>coefficient of determination adjusted for degrees of freedom</em>, defined by</p>
<p><span class="math display">\[
R_{a}^2 = 1 - \frac{(Error~SS) / [n - (k + 1)]}{(Total~SS) / (n - 1)} = 1 - \frac{s^2}{s_{y}^2}.
\]</span></p>
<p>To interpret this statistic, note that <span class="math inline">\(s_y^2\)</span> does not depend on the model nor the model variables. Thus, <span class="math inline">\(s^2\)</span> and <span class="math inline">\(R_a^2\)</span> are equivalent measures of model fit. As the model fit improves, then <span class="math inline">\(R_{a}^2\)</span> becomes larger and <span class="math inline">\(s^2\)</span> becomes smaller, and vice versa. Put another way, choosing a model with the smallest <span class="math inline">\(s^2\)</span> is equivalent to choosing a model with the largest <span class="math inline">\(R_a^2\)</span>.</p>
<hr />
<p><strong>Example: Term Life Insurance - Continued.</strong> To illustrate, Table @ref{T3:ANOVATerm} displays the summary statistics for the regression of LNFACE on EDUCATION, NUMHH, and LNINCOME. From the degrees of freedom column, we remind ourselves that there are three explanatory variables and 275 observations. As measures of model fit, the coefficient of determination is <span class="math inline">\(R^2 = 34.3\%\)</span> (=<span class="math inline">\(328.47 / 958.90\)</span>) and the residual standard deviation is <span class="math inline">\(s = 1.525\)</span> (=<span class="math inline">\(\sqrt{2.326}\)</span>). If we were to attempt to estimate the logarithmic face amount without knowledge of the explanatory variables EDUCATION, NUMHH, and LNINCOME, then the size of the typical error would be <span class="math inline">\(s_y = 1.871\)</span> (=<span class="math inline">\(\sqrt{958.90 / 274}\)</span>). Thus, by taking advantage of our knowledge of the explanatory variables, we have been able to reduce the size of the typical error. The measure of model fit that compares these two estimates of variability is the adjusted coefficient of determination, <span class="math inline">\(R_a^2 = 1 - 2.326 / 1.871^2 = 33.6\%\)</span>.</p>
<p><strong>Example: Why do Females Live Longer than Males?</strong> In an article with this title, Lemaire (2002) examined what he called the “female advantage,” the difference in life expectancy between females and males. Life expectancies are of interest because they are widely used measures of a nation’s health. Lemaire examined data from <span class="math inline">\(n = 169\)</span> countries and found that the average female advantage was 4.51 years worldwide. He sought to explain this difference based on 45 behavioral measures, variables that capture a nation’s degree of economic modernization, social/cultural/religious mores, geographic position, and quality of health care available.</p>
<p>After a detailed analysis, Lemaire reports coefficients from a regression model that appear in Table @ref{T6:FemaleAdvantage}. This regression model explains <span class="math inline">\(R^2 = 61\%\)</span> of the variability. It is a parsimonious model consisting of only <span class="math inline">\(k = 4\)</span> of the original 45 variables.</p>
<p><span class="math display">\[
\begin{array}{l|rr}
\hline
\text{Variable} &amp; \text{Coefficient} &amp; t\text{-statistic} \\
\hline
\text{Intercept} &amp; 9.904 &amp; 12.928 \\
\text{Logarithmic Number of Persons per Physician} &amp; -0.473 &amp; -3.212 \\
\text{Fertility} &amp; -0.444 &amp; -3.477 \\
\text{Percentage of Hindus and Buddhists} &amp; -0.018 &amp; -3.196 \\
\text{Soviet Union Dummy} &amp; 4.922 &amp; 7.235 \\
\hline
\end{array}
\]</span></p>
<p><em>Source: Lemaire (2002)</em></p>
<p>All variables were strongly statistically significant. The number of persons per physician was also correlated with other variables that capture a country’s degree of economic modernization, such as urbanization, number of cars, and the percentage working in agriculture. Fertility, the number of births per woman, was highly correlated with education variables in the study, including female illiteracy and female school enrollment. The percentage of Hindus and Buddhists is a social/cultural/religious variable. The Soviet Union dummy is a geographic variable - it characterizes Eastern European countries that formerly belonged to the Soviet Union. Because of the high degree of collinearity among the 45 candidate variables, other analysts could easily pick an alternative set of variables. Nonetheless, Lemaire’s important point was that this simple model explains roughly 61% of the variability based on only behavioral variables, unrelated to biological sex differences.</p>
</div>
<div id="statistical-inference-for-a-single-coefficient" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Statistical Inference for a Single Coefficient<a href="estimation-and-goodness-of-fit.html#statistical-inference-for-a-single-coefficient" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-t-test" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> The <em>t</em>-Test<a href="estimation-and-goodness-of-fit.html#the-t-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In many applications, a single variable is of primary interest, and other variables are included in the regression to control for additional sources of variability. To illustrate, a sales agent might be interested in the effect that income has on the quantity of insurance demanded. In a regression analysis, one could also include other explanatory variables such as an individual’s gender, type of occupation, age, size of the household, education level, and so on. By including these additional explanatory variables, we hope to gain a better understanding of the relationship between income and insurance demand. To reach sensible conclusions, we will need some rules to decide whether a variable is important or not.</p>
<p>We respond to the question “Is <span class="math inline">\(x_j\)</span> important?” by investigating whether or not the corresponding slope parameter, <span class="math inline">\(\beta_j\)</span>, equals zero. The question is whether <span class="math inline">\(\beta_j\)</span> is zero can be restated in the hypothesis testing framework as “Is <span class="math inline">\(H_0:\beta_j=0\)</span> valid?”</p>
<p>We examine the proximity of <span class="math inline">\(b_j\)</span> to zero in order to determine whether or not <span class="math inline">\(\beta_j\)</span> is zero. Because the units of <span class="math inline">\(b_j\)</span> depend on the units of <span class="math inline">\(y\)</span> and <span class="math inline">\(x_j\)</span>, we need to standardize this quantity. From Property 2 and equation (@ref{E3:VarVec}), we saw that <span class="math inline">\(\mathrm{Var~}b_j\)</span> is <span class="math inline">\(\sigma^2\)</span> times the <span class="math inline">\((j+1)^{st}\)</span> diagonal element of <span class="math inline">\((\mathbf{X^{\prime}X})^{-1}\)</span>. Replacing <span class="math inline">\(\sigma^2\)</span> by the estimator <span class="math inline">\(s^2\)</span> and taking square roots, we have the following.</p>
<hr />
<p><strong>Definition</strong>. The standard error of <span class="math inline">\(b_j\)</span> can be expressed as</p>
<p><span class="math display">\[
se(b_j) = s \sqrt{\text{(j+1)st diagonal element of } (\mathbf{X^{\prime}X})^{-1}}.
\]</span></p>
<hr />
<p>Recall that a standard error is an estimated standard deviation. To test <span class="math inline">\(H_0:\beta_j=0\)</span>, we examine the <span class="math inline">\(t\)</span>-ratio,</p>
<p><span class="math display">\[
t(b_j) = \frac{b_j}{se(b_j)}.
\]</span></p>
<p>We interpret <span class="math inline">\(t(b_j)\)</span> to be the number of standard errors that <span class="math inline">\(b_j\)</span> is away from zero. This is the appropriate quantity because the sampling distribution of <span class="math inline">\(t(b_j)\)</span> can be shown to be the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df=n-(k+1)\)</span> degrees of freedom, under the null hypothesis with the linear regression model assumptions F1-F5. This enables us to construct tests of the null hypothesis such as the following procedure.</p>
<hr />
<p><strong>Procedure</strong>. The <em>t</em>-test for a Regression Coefficient (<span class="math inline">\(\beta\)</span>).
- The null hypothesis is <span class="math inline">\(H_0:\beta_j=0\)</span>.
- The alternative hypothesis is <span class="math inline">\(H_{a}:\beta_j \neq 0\)</span>.
- Establish a significance level <span class="math inline">\(\alpha\)</span> (typically but not necessarily 5%).
- Construct the statistic,</p>
<p><span class="math display">\[
t(b_j) = \frac{b_j}{se(b_j)}.
\]</span></p>
<ul>
<li>Procedure: Reject the null hypothesis in favor of the alternative if <span class="math inline">\(|t(b_j)|\)</span> exceeds a <span class="math inline">\(t\)</span>-value. Here, this <span class="math inline">\(t\)</span>-value is the <span class="math inline">\((1-\alpha /2)^{th}\)</span> percentile from the <span class="math inline">\(t\)</span>-distribution using <span class="math inline">\(df=n-(k+1)\)</span> degrees of freedom, denoted as <span class="math inline">\(t_{n-(k+1),1-\alpha /2}\)</span>.</li>
</ul>
<hr />
<p>In many applications, the sample size will be large enough so that we may approximate the <span class="math inline">\(t\)</span>-value by the corresponding percentile from the standard normal curve. At the 5% level of significance, this percentile is 1.96. Thus, as a rule of thumb, we can interpret a variable to be important if its <span class="math inline">\(t\)</span>-ratio exceeds two in absolute value.</p>
<p>Although it is the most common, testing <span class="math inline">\(H_0:\beta_j=0\)</span> versus <span class="math inline">\(H_{a}:\beta_j \neq 0\)</span> is just one of many hypothesis tests that can be performed. Table @ref{T3:Decisions} outlines alternative decision-making procedures. These procedures are for testing <span class="math inline">\(H_0:\beta_j = d\)</span>. Here, <span class="math inline">\(d\)</span> is a user-prescribed value that may be equal to zero or any other known value.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="estimation-and-goodness-of-fit.html#cb2-1" tabindex="-1"></a><span class="co"># Table for Decision-Making Procedures</span></span>
<span id="cb2-2"><a href="estimation-and-goodness-of-fit.html#cb2-2" tabindex="-1"></a><span class="fu">library</span>(knitr)</span>
<span id="cb2-3"><a href="estimation-and-goodness-of-fit.html#cb2-3" tabindex="-1"></a><span class="fu">kable</span>(</span>
<span id="cb2-4"><a href="estimation-and-goodness-of-fit.html#cb2-4" tabindex="-1"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb2-5"><a href="estimation-and-goodness-of-fit.html#cb2-5" tabindex="-1"></a>    <span class="st">`</span><span class="at">Alternative Hypothesis ($H_{a}$)</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">beta_j &gt; d$&quot;</span>, <span class="st">&quot;$</span><span class="sc">\\</span><span class="st">beta_j &lt; d$&quot;</span>, <span class="st">&quot;$</span><span class="sc">\\</span><span class="st">beta_j </span><span class="sc">\\</span><span class="st">neq d$&quot;</span>),</span>
<span id="cb2-6"><a href="estimation-and-goodness-of-fit.html#cb2-6" tabindex="-1"></a>    <span class="st">`</span><span class="at">Procedure: Reject $H_0$ in favor of $H_a$ if</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">c</span>(</span>
<span id="cb2-7"><a href="estimation-and-goodness-of-fit.html#cb2-7" tabindex="-1"></a>      <span class="st">&quot;$t$-ratio $&gt;$ $t_{n-(k+1),1-</span><span class="sc">\\</span><span class="st">alpha}$&quot;</span>,</span>
<span id="cb2-8"><a href="estimation-and-goodness-of-fit.html#cb2-8" tabindex="-1"></a>      <span class="st">&quot;$t$-ratio $&lt;$ $-t_{n-(k+1),1-</span><span class="sc">\\</span><span class="st">alpha}$&quot;</span>,</span>
<span id="cb2-9"><a href="estimation-and-goodness-of-fit.html#cb2-9" tabindex="-1"></a>      <span class="st">&quot;$|t$-ratio$| &gt; t_{n-(k+1),1-</span><span class="sc">\\</span><span class="st">alpha /2}$&quot;</span></span>
<span id="cb2-10"><a href="estimation-and-goodness-of-fit.html#cb2-10" tabindex="-1"></a>    )</span>
<span id="cb2-11"><a href="estimation-and-goodness-of-fit.html#cb2-11" tabindex="-1"></a>  ),</span>
<span id="cb2-12"><a href="estimation-and-goodness-of-fit.html#cb2-12" tabindex="-1"></a>  <span class="at">caption =</span> <span class="st">&quot;Decision-Making Procedures for Testing $H_0: </span><span class="sc">\\</span><span class="st">beta_j = d$&quot;</span>,</span>
<span id="cb2-13"><a href="estimation-and-goodness-of-fit.html#cb2-13" tabindex="-1"></a>  <span class="at">align =</span> <span class="st">&quot;cc&quot;</span></span>
<span id="cb2-14"><a href="estimation-and-goodness-of-fit.html#cb2-14" tabindex="-1"></a>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-7">Table 2.1: </span>Decision-Making Procedures for Testing <span class="math inline">\(H_0: \beta_j = d\)</span></caption>
<colgroup>
<col width="40%" />
<col width="60%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Alternative.Hypothesis…H_.a…</th>
<th align="center">Procedure..Reject..H_0..in.favor.of..H_a..if</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\beta_j &gt; d\)</span></td>
<td align="center"><span class="math inline">\(t\)</span>-ratio <span class="math inline">\(&gt;\)</span> <span class="math inline">\(t_{n-(k+1),1-\alpha}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\beta_j &lt; d\)</span></td>
<td align="center"><span class="math inline">\(t\)</span>-ratio <span class="math inline">\(&lt;\)</span> <span class="math inline">\(-t_{n-(k+1),1-\alpha}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\beta_j \neq d\)</span></td>
<td align="center"><span class="math inline">\(&amp;#124;t\)</span>-ratio<span class="math inline">\(&amp;#124; &gt; t_{n-(k+1),1-\alpha /2}\)</span></td>
</tr>
</tbody>
</table>
<p>Alternatively, one can construct <span class="math inline">\(p\)</span>-values and compare these to given significant levels. The <span class="math inline">\(p\)</span>-value allows the report reader to understand the strength of the deviation from the null hypothesis. Table @ref{T3:Pvalues} summarizes the procedure for calculating <span class="math inline">\(p\)</span>-values.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="estimation-and-goodness-of-fit.html#cb3-1" tabindex="-1"></a><span class="co"># Table for Probability Values</span></span>
<span id="cb3-2"><a href="estimation-and-goodness-of-fit.html#cb3-2" tabindex="-1"></a><span class="fu">kable</span>(</span>
<span id="cb3-3"><a href="estimation-and-goodness-of-fit.html#cb3-3" tabindex="-1"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb3-4"><a href="estimation-and-goodness-of-fit.html#cb3-4" tabindex="-1"></a>    <span class="st">`</span><span class="at">Alternative Hypothesis ($H_a$)</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">beta_j &gt; d$&quot;</span>, <span class="st">&quot;$</span><span class="sc">\\</span><span class="st">beta_j &lt; d$&quot;</span>, <span class="st">&quot;$</span><span class="sc">\\</span><span class="st">beta_j </span><span class="sc">\\</span><span class="st">neq d$&quot;</span>),</span>
<span id="cb3-5"><a href="estimation-and-goodness-of-fit.html#cb3-5" tabindex="-1"></a>    <span class="st">`</span><span class="at">$p$-value</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">c</span>(</span>
<span id="cb3-6"><a href="estimation-and-goodness-of-fit.html#cb3-6" tabindex="-1"></a>      <span class="st">&quot;Pr($t_{n-(k+1)} &gt; </span><span class="sc">\\</span><span class="st">text{t-ratio}$)&quot;</span>,</span>
<span id="cb3-7"><a href="estimation-and-goodness-of-fit.html#cb3-7" tabindex="-1"></a>      <span class="st">&quot;Pr($t_{n-(k+1)} &lt; </span><span class="sc">\\</span><span class="st">text{t-ratio}$)&quot;</span>,</span>
<span id="cb3-8"><a href="estimation-and-goodness-of-fit.html#cb3-8" tabindex="-1"></a>      <span class="st">&quot;Pr($|t_{n-(k+1)}| &gt; |</span><span class="sc">\\</span><span class="st">text{t-ratio}|$)&quot;</span></span>
<span id="cb3-9"><a href="estimation-and-goodness-of-fit.html#cb3-9" tabindex="-1"></a>    )</span>
<span id="cb3-10"><a href="estimation-and-goodness-of-fit.html#cb3-10" tabindex="-1"></a>  ),</span>
<span id="cb3-11"><a href="estimation-and-goodness-of-fit.html#cb3-11" tabindex="-1"></a>  <span class="at">caption =</span> <span class="st">&quot;Probability Values for Testing $H_0:</span><span class="sc">\\</span><span class="st">beta_j = d$&quot;</span>,</span>
<span id="cb3-12"><a href="estimation-and-goodness-of-fit.html#cb3-12" tabindex="-1"></a>  <span class="at">align =</span> <span class="st">&quot;cccc&quot;</span></span>
<span id="cb3-13"><a href="estimation-and-goodness-of-fit.html#cb3-13" tabindex="-1"></a>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-8">Table 2.2: </span>Probability Values for Testing <span class="math inline">\(H_0:\beta_j = d\)</span></caption>
<colgroup>
<col width="34%" />
<col width="65%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Alternative.Hypothesis…H_a..</th>
<th align="center">X.p..value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\beta_j &gt; d\)</span></td>
<td align="center">Pr(<span class="math inline">\(t_{n-(k+1)} &gt; \text{t-ratio}\)</span>)</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\beta_j &lt; d\)</span></td>
<td align="center">Pr(<span class="math inline">\(t_{n-(k+1)} &lt; \text{t-ratio}\)</span>)</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\beta_j \neq d\)</span></td>
<td align="center">Pr(<span class="math inline">\(&amp;#124;t_{n-(k+1)}&amp;#124; &gt; &amp;#124;\text{t-ratio}&amp;#124;\)</span>)</td>
</tr>
</tbody>
</table>
<hr />
<p><strong>Example: Term Life Insurance - Continued.</strong> A useful convention when reporting the results of a statistical analysis is to place the standard error of a statistic in parenthesis below that statistic. Thus, for example, in our regression of LNFACE on EDUCATION, NUMHH, and LNINCOME, the estimated regression equation is:</p>
<p><span class="math display">\[
\widehat{LNFACE} = 2.584 + 0.206 \text{EDUCATION} + 0.306 \text{NUMHH} + 0.494 \text{LNINCOME}.
\]</span></p>
<p>Standard errors are:</p>
<p><span class="math display">\[
\text{std error} = (0.846) \quad (0.039) \quad (0.063) \quad (0.078).
\]</span></p>
<p>To illustrate the calculation of the standard errors, first note that from Table @ref{T3:ANOVATerm} we have that the residual standard deviation is <span class="math inline">\(s=1.525\)</span>. Using a statistical package, we have</p>
<p><span class="math display">\[
(\mathbf{X^{\prime}X})^{-1} = \begin{pmatrix}
0.307975 &amp; -0.004633 &amp; -0.002131 &amp; -0.020697 \\
-0.004633 &amp; 0.000648 &amp; 0.000143 &amp; -0.000467 \\
-0.002131 &amp; 0.000143 &amp; 0.001724 &amp; -0.000453 \\
-0.020697 &amp; -0.000467 &amp; -0.000453 &amp; 0.002585
\end{pmatrix}.
\]</span></p>
<p>To illustrate, we can compute <span class="math inline">\(se(b_3)=s \times \sqrt{0.002585} = 0.078\)</span>, as above. Calculation of the standard errors, as well as the corresponding <span class="math inline">\(t\)</span>-statistics, is part of the standard output from statistical software and need not be computed by users. Our purpose here is to illustrate the ideas underlying the routine calculations.</p>
<p>With this information, we can immediately compute <span class="math inline">\(t\)</span>-ratios to check whether a coefficient associated with an individual variable is significantly different from zero. For example, the <span class="math inline">\(t\)</span>-ratio for the LNINCOME variable is</p>
<p><span class="math display">\[
t(b_3) = \frac{0.494}{0.078} = 6.3.
\]</span></p>
<p>The interpretation is that <span class="math inline">\(b_3\)</span> is over four standard errors above zero and thus LNINCOME is an important variable in the model. More formally, we may be interested in testing the null hypothesis that <span class="math inline">\(H_0:\beta_3 = 0\)</span> versus <span class="math inline">\(H_0:\beta_3 \neq 0\)</span>. At a 5% level of significance, the <span class="math inline">\(t\)</span>-value is 1.96, because <span class="math inline">\(df=275-(1+3)=271\)</span>. We thus reject the null in favor of the alternative hypothesis, that logarithmic income (LNINCOME) is important in determining the logarithmic face amount.</p>
</div>
<div id="confidence-intervals" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Confidence Intervals<a href="estimation-and-goodness-of-fit.html#confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Confidence intervals</em> for parameters represent another device for describing the strength of the contribution of the <span class="math inline">\(j\)</span>th explanatory variable. The statistic <span class="math inline">\(b_j\)</span> is called a <em>point estimate</em> of the parameter <span class="math inline">\(\beta_j\)</span>. To provide a range of reliability, we use the confidence interval:</p>
<p><span class="math display">\[
b_j \pm t_{n-(k+1),1-\alpha /2}se(b_j).
\]</span></p>
<p>Here, the <span class="math inline">\(t\)</span>-value <span class="math inline">\(t_{n-(k+1),1-\alpha /2}\)</span> is a percentile from the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df=n-(k+1)\)</span> degrees of freedom. We use the same <span class="math inline">\(t\)</span>-value as in the two-sided hypothesis test. Indeed, there is a duality between the confidence interval and the two-sided hypothesis test. For example, it is not hard to check that if a hypothesized value falls outside the confidence interval, then <span class="math inline">\(H_0\)</span> will be rejected in favor of <span class="math inline">\(H_{a}\)</span>. Further, knowledge of the <span class="math inline">\(p\)</span>-value, point estimate, and standard error can be used to determine a confidence interval.</p>
</div>
<div id="added-variable-plots" class="section level3 hasAnchor" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Added Variable Plots<a href="estimation-and-goodness-of-fit.html#added-variable-plots" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To represent multivariate data graphically, we have seen that a scatterplot matrix is a useful device. However, the major shortcoming of the scatterplot matrix is that it only captures relationships between pairs of variables. When the data can be summarized using a regression model, a graphical device that does not have this shortcoming is an <em>added variable plot</em>. The added variable plot is also called a <em>partial regression plot</em> because, as we will see, it is constructed in terms of residuals from certain regression fits. We will also see that the added variable plot can be summarized in terms of a partial correlation coefficient, thus providing a link between correlation and regression. To introduce these ideas, we work in the context of the following example.</p>
<hr />
<div id="example-refrigerator-prices" class="section level4 hasAnchor" number="2.3.3.1">
<h4><span class="header-section-number">2.3.3.1</span> Example: Refrigerator Prices<a href="estimation-and-goodness-of-fit.html#example-refrigerator-prices" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>What characteristics of a refrigerator are important in determining its price (PRICE)? We consider here several characteristics of a refrigerator, including the size of the refrigerator in cubic feet (RSIZE), the size of the freezer compartment in cubic feet (FSIZE), the average amount of money spent per year to operate the refrigerator (ECOST, for “energy cost”), the number of shelves in the refrigerator and freezer doors (SHELVES), and the number of features (FEATURES). The features variable includes shelves for cans, see-through crispers, ice makers, egg racks, and so on.</p>
<p>Both consumers and manufacturers are interested in models of refrigerator prices. Other things equal, consumers generally prefer larger refrigerators with lower energy costs that have more features. Due to forces of supply and demand, we would expect consumers to pay more for these refrigerators. A larger refrigerator with lower energy costs that has more features at a similar price is considered a bargain to the consumer. How much extra would the consumer be willing to pay for this additional space? A model of prices for refrigerators on the market provides some insight into this question.</p>
<p>To this end, we analyze data from <span class="math inline">\(n=37\)</span> refrigerators. Table <a href="estimation-and-goodness-of-fit.html#tab:T3-RefrigSumStats">2.3</a> provides the basic summary statistics for the response variable PRICE and the five explanatory variables. From this table, we see that the average refrigerator price is <span class="math inline">\(\overline{y} = \$626.40\)</span>, with standard deviation <span class="math inline">\(s_{y} = \$139.80\)</span>. Similarly, the average annual amount to operate a refrigerator, or average ECOST, is $70.51.</p>
<table>
<caption><span id="tab:T3-RefrigSumStats">Table 2.3: </span>Summary Statistics for each variable for 37 Refrigerators</caption>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="right">Mean</th>
<th align="right">Median</th>
<th align="right">Standard.Deviation</th>
<th align="right">Minimum</th>
<th align="right">Maximum</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ECOST</td>
<td align="right">70.510</td>
<td align="right">68.0</td>
<td align="right">9.140</td>
<td align="right">60.0</td>
<td align="right">94.0</td>
</tr>
<tr class="even">
<td align="left">RSIZE</td>
<td align="right">13.400</td>
<td align="right">13.2</td>
<td align="right">0.600</td>
<td align="right">12.6</td>
<td align="right">14.7</td>
</tr>
<tr class="odd">
<td align="left">FSIZE</td>
<td align="right">5.184</td>
<td align="right">5.1</td>
<td align="right">0.938</td>
<td align="right">4.1</td>
<td align="right">7.4</td>
</tr>
<tr class="even">
<td align="left">SHELVES</td>
<td align="right">2.514</td>
<td align="right">2.0</td>
<td align="right">1.121</td>
<td align="right">1.0</td>
<td align="right">5.0</td>
</tr>
<tr class="odd">
<td align="left">FEATURES</td>
<td align="right">3.459</td>
<td align="right">3.0</td>
<td align="right">2.512</td>
<td align="right">1.0</td>
<td align="right">12.0</td>
</tr>
<tr class="even">
<td align="left">PRICE</td>
<td align="right">626.400</td>
<td align="right">590.0</td>
<td align="right">139.800</td>
<td align="right">460.0</td>
<td align="right">1200.0</td>
</tr>
</tbody>
</table>
<p>To analyze relationships among pairs of variables, Table <a href="estimation-and-goodness-of-fit.html#tab:T3-RefrigCorr">2.4</a> provides a matrix of correlation coefficients. From the table, we see that there are strong linear relationships between PRICE and each of freezer space (FSIZE) and the number of FEATURES. Surprisingly, there is also a strong positive correlation between PRICE and ECOST. Recall that ECOST is the energy cost; one might expect that higher-priced refrigerators should enjoy lower energy costs.</p>
<table>
<caption><span id="tab:T3-RefrigCorr">Table 2.4: </span>Matrix of Correlation Coefficients</caption>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="right">ECOST</th>
<th align="right">RSIZE</th>
<th align="right">FSIZE</th>
<th align="right">SHELVES</th>
<th align="right">FEATURES</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ECOST</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="left">RSIZE</td>
<td align="right">0.333</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="odd">
<td align="left">FSIZE</td>
<td align="right">0.855</td>
<td align="right">0.235</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="left">SHELVES</td>
<td align="right">0.188</td>
<td align="right">0.363</td>
<td align="right">0.251</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="odd">
<td align="left">FEATURES</td>
<td align="right">0.334</td>
<td align="right">0.096</td>
<td align="right">0.439</td>
<td align="right">0.16</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="left">PRICE</td>
<td align="right">0.522</td>
<td align="right">0.024</td>
<td align="right">0.720</td>
<td align="right">0.40</td>
<td align="right">0.697</td>
</tr>
</tbody>
</table>
<p>A regression model was fit to the data. The fitted regression equation appears in Table <a href="estimation-and-goodness-of-fit.html#tab:T3-RefrigFittedModel">2.5</a>, with <span class="math inline">\(s=60.65\)</span> and <span class="math inline">\(R^2=83.8\%\)</span>.</p>
<table>
<caption><span id="tab:T3-RefrigFittedModel">Table 2.5: </span>Fitted Refrigerator Price Model</caption>
<thead>
<tr class="header">
<th align="left">Term</th>
<th align="right">Coefficient</th>
<th align="right">Standard.Error</th>
<th align="right">t.ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">798.00</td>
<td align="right">271.400</td>
<td align="right">-2.9</td>
</tr>
<tr class="even">
<td align="left">ECOST</td>
<td align="right">-6.96</td>
<td align="right">2.275</td>
<td align="right">-3.1</td>
</tr>
<tr class="odd">
<td align="left">RSIZE</td>
<td align="right">76.50</td>
<td align="right">19.440</td>
<td align="right">3.9</td>
</tr>
<tr class="even">
<td align="left">FSIZE</td>
<td align="right">137.00</td>
<td align="right">23.760</td>
<td align="right">5.8</td>
</tr>
<tr class="odd">
<td align="left">SHELVES</td>
<td align="right">37.90</td>
<td align="right">9.886</td>
<td align="right">3.8</td>
</tr>
<tr class="even">
<td align="left">FEATURES</td>
<td align="right">23.80</td>
<td align="right">4.512</td>
<td align="right">5.3</td>
</tr>
</tbody>
</table>
<p>From Table <a href="estimation-and-goodness-of-fit.html#tab:T3-RefrigFittedModel">2.5</a>, the explanatory variables seem to be useful predictors of refrigerator prices. Together, these variables account for 83.8% of the variability. For understanding prices, the typical error has dropped from <span class="math inline">\(s_{y} = \$139.80\)</span> to <span class="math inline">\(s = \$60.65\)</span>. The <span class="math inline">\(t\)</span>-ratios for each of the explanatory variables exceed two in absolute value, indicating that each variable is important on an individual basis.</p>
<p>What is surprising about the regression fit is the negative coefficient associated with energy cost. Remember, we can interpret <span class="math inline">\(b_{ECOST} = -6.96\)</span> to mean that, for each dollar increase in ECOST, we expect the PRICE to decrease by $6.96. This negative relationship conforms to our economic intuition. However, it is surprising that the same data set has shown us that there is a positive relationship between PRICE and ECOST. This seeming anomaly is because correlation only measures relationships between pairs of variables although the regression fit can account for several variables simultaneously. To provide more insight into this seeming anomaly, we now introduce the <em>added variable plot</em>.</p>
<hr />
</div>
</div>
<div id="producing-an-added-variable-plot" class="section level3 hasAnchor" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> Producing an Added Variable Plot<a href="estimation-and-goodness-of-fit.html#producing-an-added-variable-plot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The added variable plot provides additional links between the regression methodology and more fundamental tools such as scatter plots and correlations. We work in the context of the Refrigerator Price Example to demonstrate the construction of this plot.</p>
<hr />
<p><em>Procedure for producing an added variable plot.</em></p>
<ol style="list-style-type: decimal">
<li><p>Run a regression of PRICE on RSIZE, FSIZE, SHELVES, and FEATURES, omitting ECOST. Compute the residuals from this regression, which we label <span class="math inline">\(e_1\)</span>.</p></li>
<li><p>Run a regression of ECOST on RSIZE, FSIZE, SHELVES, and FEATURES. Compute the residuals from this regression, which we label <span class="math inline">\(e_2\)</span>.</p></li>
<li><p>Plot <span class="math inline">\(e_1\)</span> versus <span class="math inline">\(e_2\)</span>. This is the added variable plot of PRICE versus ECOST, controlling for the effects of the RSIZE, FSIZE, SHELVES, and FEATURES. This plot appears in Figure <a href="estimation-and-goodness-of-fit.html#fig:RefrigAddedVarPlot">2.1</a>.</p></li>
</ol>
<hr />

<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="estimation-and-goodness-of-fit.html#cb4-1" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">&quot;Chapters/Chapter3/F3RefrigAddedVarPlot.eps&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RefrigAddedVarPlot"></span>
<img src="Chapters/Chapter3/F3RefrigAddedVarPlot.eps" alt="An added variable plot. The residuals from the regression of PRICE on the explanatory variables, omitting ECOST, are on the vertical axis. On the horizontal axis are the residuals from the regression fit of ECOST on the other explanatory variables. The correlation coefficient is -0.48." width=".6\textwidth" />
<p class="caption">
Figure 2.1: <strong>An added variable plot.</strong> The residuals from the regression of PRICE on the explanatory variables, omitting ECOST, are on the vertical axis. On the horizontal axis are the residuals from the regression fit of ECOST on the other explanatory variables. The correlation coefficient is -0.48.
</p>
</div>
<p>The error <span class="math inline">\(\varepsilon\)</span> can be interpreted as the natural variation in a sample. In many situations, this natural variation is small compared to the patterns evident in the nonrandom regression component. Thus, it is useful to think of the error, <span class="math inline">\(\varepsilon_i = y_i - \left( \beta_0 + \beta_1 x_{i1} + \ldots + \beta_k x_{ik} \right)\)</span>, as the response after controlling for the effects of the explanatory variables. In Section 3.3, we saw that a random error can be approximated by a residual, <span class="math inline">\(e_i = y_i - \left( b_0 + b_1 x_{i1} + \cdots + b_k x_{ik} \right)\)</span>. Thus, in the same way, we may think of a residual as the response after “controlling for” the effects of the explanatory variables.</p>
<p>With this in mind, we can interpret the vertical axis of Figure <a href="estimation-and-goodness-of-fit.html#fig:RefrigAddedVarPlot">2.1</a> as the refrigerator PRICE controlled for effects of RSIZE, FSIZE, SHELVES, and FEATURES. Similarly, we can interpret the horizontal axis as the ECOST controlled for effects of RSIZE, FSIZE, SHELVES, and FEATURES. The plot then provides a graphical representation of the relation between PRICE and ECOST, after controlling for the other explanatory variables. For comparison, a scatter plot of PRICE and ECOST (not shown here) does not control for other explanatory variables. Thus, it is possible that the positive relationship between PRICE and ECOST is not due to a causal relationship but rather one or more additional variables that cause both variables to be large.</p>
<p>For example, from Table <a href="#tab:RefrigSumStats"><strong>??</strong></a>, we see that the freezer size (FSIZE) is positively correlated with both ECOST and PRICE. It certainly seems reasonable that increasing the size of a freezer would cause both the energy cost and the price to increase. Rather, the positive correlation may be due to the fact that large values of FSIZE mean large values of both ECOST and PRICE.</p>
<p>Variables left out of a regression are called <em>omitted variables</em>. This omission could cause a serious problem in a regression model fit; regression coefficients could be not only strongly significant when they should not be, but they may also be of the incorrect sign. Selecting the proper set of variables to be included in the regression model is an important task; it is the subject of Chapters 5 and 6.</p>
</div>
<div id="partial-correlation-coefficients" class="section level3 hasAnchor" number="2.3.5">
<h3><span class="header-section-number">2.3.5</span> Partial Correlation Coefficients<a href="estimation-and-goodness-of-fit.html#partial-correlation-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we saw in Chapter 2, a correlation statistic is a useful quantity for summarizing plots. The correlation for the added variable plot is called a <em>partial correlation coefficient</em>. It is defined to be the correlation between the residuals <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> and is denoted by <span class="math inline">\(r(y,x_j | x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_k)\)</span>. Because it summarizes an added variable plot, we may interpret <span class="math inline">\(r(y,x_j | x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_k)\)</span> to be the correlation between <span class="math inline">\(y\)</span> and <span class="math inline">\(x_j\)</span>, in the presence of the other explanatory variables. To illustrate, the correlation between PRICE and ECOST in the presence of the other explanatory variables is -0.48.</p>
<p>The partial correlation coefficient can also be calculated using</p>
<p><span class="math display">\[
r(y,x_j | x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_k) = \frac{t(b_j)}{\sqrt{t(b_j)^2 + n - (k + 1)}}.
\]</span></p>
<p>Here, <span class="math inline">\(t(b_j)\)</span> is the <span class="math inline">\(t\)</span>-ratio for <span class="math inline">\(b_j\)</span> from a regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x_1, \ldots, x_k\)</span> (including the variable <span class="math inline">\(x_j\)</span>). An important aspect of this equation is that it allows us to calculate partial correlation coefficients running only one regression. For example, from Table <a href="#tab:RefrigFittedModel"><strong>??</strong></a>, the partial correlation between PRICE and ECOST in the presence of the other explanatory variables is <span class="math inline">\(\frac{-3.1}{\sqrt{(-3.1)^2 + 37 - (5 + 1)}} \approx -0.48\)</span>.</p>
<p>Calculation of partial correlation coefficients is quicker when using the relationship with the <span class="math inline">\(t\)</span>-ratio, but may fail to detect nonlinear relationships. The information in Table <a href="#tab:RefrigFittedModel"><strong>??</strong></a> allows us to calculate all five partial correlation coefficients in the Refrigerator Price Example after running only one regression. The three-step procedure for producing added variable plots requires ten regressions, two for each of the five explanatory variables. Of course, by producing added variable plots, we can detect nonlinear relationships that are missed by correlation coefficients.</p>
<p>Partial correlation coefficients provide another interpretation for <span class="math inline">\(t\)</span>-ratios. The equation shows how to calculate a correlation statistic from a <span class="math inline">\(t\)</span>-ratio, thus providing another link between correlation and regression analysis. Moreover, from the equation we see that the larger the <span class="math inline">\(t\)</span>-ratio, the larger the partial correlation coefficient. That is, a large <span class="math inline">\(t\)</span>-ratio means that there is a large correlation between the response and the explanatory variable, controlling for other explanatory variables. This provides a partial response to the question that is regularly asked by consumers of regression analyses, “Which variable is most important?”</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multiple-linear-regression---i.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="some-special-explanatory-variables.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
