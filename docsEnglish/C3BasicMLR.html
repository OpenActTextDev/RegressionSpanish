<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Multiple Linear Regression - I | Regression Modeling with Actuarial and Financial Applications</title>
  <meta name="description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Multiple Linear Regression - I | Regression Modeling with Actuarial and Financial Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Multiple Linear Regression - I | Regression Modeling with Actuarial and Financial Applications" />
  
  <meta name="twitter:description" content="Development of a research monograph that provides quantitative tools to assess the relevance of dependence in insurance risk management." />
  

<meta name="author" content="Edward (Jed) Frees, University of Wisconsin - Madison, Australian National University" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="C2BasicLR.html"/>
<link rel="next" href="C4MLRANOVA.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript"  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleSolution(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}      
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>



<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Regression Modeling With Actuarial and Financial Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#forward"><i class="fa fa-check"></i>Forward</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#who-is-this-book-for"><i class="fa fa-check"></i>Who Is This Book For?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#what-is-this-book-about"><i class="fa fa-check"></i>What Is This Book About?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#how-does-this-book-deliver-its-message"><i class="fa fa-check"></i>How Does This Book Deliver Its Message?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html"><i class="fa fa-check"></i><b>1</b> Regression and the Normal Distribution</a>
<ul>
<li class="chapter" data-level="1.1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#S1:Intro"><i class="fa fa-check"></i><b>1.1</b> What is Regression Analysis?</a></li>
<li class="chapter" data-level="1.2" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#fitting-data-to-a-normal-distribution"><i class="fa fa-check"></i><b>1.2</b> Fitting Data to a Normal Distribution</a></li>
<li class="chapter" data-level="1.3" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#S1:PowerTransforms"><i class="fa fa-check"></i><b>1.3</b> Power Transforms</a></li>
<li class="chapter" data-level="1.4" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#sampling-and-the-role-of-normality"><i class="fa fa-check"></i><b>1.4</b> Sampling and the Role of Normality</a></li>
<li class="chapter" data-level="1.5" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#regression-and-sampling-designs"><i class="fa fa-check"></i><b>1.5</b> Regression and Sampling Designs</a></li>
<li class="chapter" data-level="1.6" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#actuarial-applications-of-regression"><i class="fa fa-check"></i><b>1.6</b> Actuarial Applications of Regression</a></li>
<li class="chapter" data-level="1.7" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#further-reading-and-references"><i class="fa fa-check"></i><b>1.7</b> Further Reading and References</a></li>
<li class="chapter" data-level="1.8" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li>
<li class="chapter" data-level="1.9" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#S1:TechSupp"><i class="fa fa-check"></i><b>1.9</b> Technical Supplement - Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="C2BasicLR.html"><a href="C2BasicLR.html"><i class="fa fa-check"></i><b>2</b> Basic Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#correlations-and-least-squares"><i class="fa fa-check"></i><b>2.1</b> Correlations and Least Squares</a>
<ul>
<li class="chapter" data-level="" data-path="C2BasicLR.html"><a href="C2BasicLR.html#scatter-plot-and-correlation-coefficients---basic-summary-tools"><i class="fa fa-check"></i>Scatter Plot and Correlation Coefficients - Basic Summary Tools</a></li>
<li class="chapter" data-level="" data-path="C2BasicLR.html"><a href="C2BasicLR.html#method-of-least-squares"><i class="fa fa-check"></i>Method of Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#basic-linear-regression-model"><i class="fa fa-check"></i><b>2.2</b> Basic Linear Regression Model</a></li>
<li class="chapter" data-level="2.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#S2:SummStats"><i class="fa fa-check"></i><b>2.3</b> Is the Model Useful? Some Basic Summary Measures</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#partitioning-the-variability"><i class="fa fa-check"></i><b>2.3.1</b> Partitioning the Variability</a></li>
<li class="chapter" data-level="2.3.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#the-size-of-a-typical-deviation-s"><i class="fa fa-check"></i><b>2.3.2</b> The Size of a Typical Deviation: <em>s</em></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="C2BasicLR.html"><a href="C2BasicLR.html#properties-of-regression-coefficient-estimators"><i class="fa fa-check"></i><b>2.4</b> Properties of Regression Coefficient Estimators</a></li>
<li class="chapter" data-level="2.5" data-path="C2BasicLR.html"><a href="C2BasicLR.html#statistical-inference"><i class="fa fa-check"></i><b>2.5</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#is-the-explanatory-variable-important-the-t-test"><i class="fa fa-check"></i><b>2.5.1</b> Is the Explanatory Variable Important?: The <em>t</em>-Test</a></li>
<li class="chapter" data-level="2.5.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#confidence-intervals"><i class="fa fa-check"></i><b>2.5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="2.5.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#prediction-intervals"><i class="fa fa-check"></i><b>2.5.3</b> Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="C2BasicLR.html"><a href="C2BasicLR.html#S2:ResidualAnalsis"><i class="fa fa-check"></i><b>2.6</b> Building a Better Model: Residual Analysis</a></li>
<li class="chapter" data-level="2.7" data-path="C2BasicLR.html"><a href="C2BasicLR.html#S2:CAPM"><i class="fa fa-check"></i><b>2.7</b> Application: Capital Asset Pricing Model</a></li>
<li class="chapter" data-level="2.8" data-path="C2BasicLR.html"><a href="C2BasicLR.html#illustrative-regression-computer-output"><i class="fa fa-check"></i><b>2.8</b> Illustrative Regression Computer Output</a></li>
<li class="chapter" data-level="2.9" data-path="C2BasicLR.html"><a href="C2BasicLR.html#further-reading-and-references-1"><i class="fa fa-check"></i><b>2.9</b> Further Reading and References</a></li>
<li class="chapter" data-level="2.10" data-path="C2BasicLR.html"><a href="C2BasicLR.html#exercises-1"><i class="fa fa-check"></i><b>2.10</b> Exercises</a></li>
<li class="chapter" data-level="2.11" data-path="C2BasicLR.html"><a href="C2BasicLR.html#technical-supplement---elements-of-matrix-algebra"><i class="fa fa-check"></i><b>2.11</b> Technical Supplement - Elements of Matrix Algebra</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="C2BasicLR.html"><a href="C2BasicLR.html#basic-definitions"><i class="fa fa-check"></i><b>2.11.1</b> Basic Definitions</a></li>
<li class="chapter" data-level="2.11.2" data-path="C2BasicLR.html"><a href="C2BasicLR.html#some-special-matrices"><i class="fa fa-check"></i><b>2.11.2</b> Some Special Matrices</a></li>
<li class="chapter" data-level="2.11.3" data-path="C2BasicLR.html"><a href="C2BasicLR.html#basic-operations"><i class="fa fa-check"></i><b>2.11.3</b> Basic Operations</a></li>
<li class="chapter" data-level="" data-path="C2BasicLR.html"><a href="C2BasicLR.html#scalar-multiplication"><i class="fa fa-check"></i>Scalar Multiplication</a></li>
<li class="chapter" data-level="" data-path="C2BasicLR.html"><a href="C2BasicLR.html#addition-and-subtraction-of-matrices"><i class="fa fa-check"></i>Addition and Subtraction of Matrices</a></li>
<li class="chapter" data-level="" data-path="C2BasicLR.html"><a href="C2BasicLR.html#matrix-multiplication"><i class="fa fa-check"></i>Matrix Multiplication</a></li>
<li class="chapter" data-level="" data-path="C2BasicLR.html"><a href="C2BasicLR.html#matrix-inverses"><i class="fa fa-check"></i>Matrix Inverses</a></li>
<li class="chapter" data-level="2.11.4" data-path="C2BasicLR.html"><a href="C2BasicLR.html#random-matrices"><i class="fa fa-check"></i><b>2.11.4</b> Random Matrices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html"><i class="fa fa-check"></i><b>3</b> Multiple Linear Regression - I</a>
<ul>
<li class="chapter" data-level="3.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#S3:LSMethod"><i class="fa fa-check"></i><b>3.1</b> Method of Least Squares</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#least-squares-method"><i class="fa fa-check"></i><b>3.1.1</b> Least Squares Method</a></li>
<li class="chapter" data-level="3.1.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#general-case-with-k-explanatory-variables"><i class="fa fa-check"></i><b>3.1.2</b> General Case with <span class="math inline">\(k\)</span> Explanatory Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#linear-regression-model-and-properties-of-estimators"><i class="fa fa-check"></i><b>3.2</b> Linear Regression Model and Properties of Estimators</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#regression-function"><i class="fa fa-check"></i><b>3.2.1</b> Regression Function</a></li>
<li class="chapter" data-level="3.2.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#regression-coefficient-interpretation"><i class="fa fa-check"></i><b>3.2.2</b> Regression Coefficient Interpretation</a></li>
<li class="chapter" data-level="3.2.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#model-assumptions"><i class="fa fa-check"></i><b>3.2.3</b> Model Assumptions</a></li>
<li class="chapter" data-level="3.2.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#properties-of-regression-coefficient-estimators-1"><i class="fa fa-check"></i><b>3.2.4</b> Properties of Regression Coefficient Estimators</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#estimation-and-goodness-of-fit"><i class="fa fa-check"></i><b>3.3</b> Estimation and Goodness of Fit</a></li>
<li class="chapter" data-level="3.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#statistical-inference-for-a-single-coefficient"><i class="fa fa-check"></i><b>3.4</b> Statistical Inference for a Single Coefficient</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#the-t-test"><i class="fa fa-check"></i><b>3.4.1</b> The <em>t</em>-Test</a></li>
<li class="chapter" data-level="3.4.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#confidence-intervals-1"><i class="fa fa-check"></i><b>3.4.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="3.4.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#added-variable-plots"><i class="fa fa-check"></i><b>3.4.3</b> Added Variable Plots</a></li>
<li class="chapter" data-level="3.4.4" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#partial-correlation-coefficients"><i class="fa fa-check"></i><b>3.4.4</b> Partial Correlation Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#some-special-explanatory-variables"><i class="fa fa-check"></i><b>3.5</b> Some Special Explanatory Variables</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#binary-variables"><i class="fa fa-check"></i><b>3.5.1</b> Binary Variables</a></li>
<li class="chapter" data-level="3.5.2" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#transforming-explanatory-variables"><i class="fa fa-check"></i><b>3.5.2</b> Transforming Explanatory Variables</a></li>
<li class="chapter" data-level="3.5.3" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#interaction-terms"><i class="fa fa-check"></i><b>3.5.3</b> Interaction Terms</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#further-reading-and-references-2"><i class="fa fa-check"></i><b>3.6</b> Further Reading and References</a></li>
<li class="chapter" data-level="3.7" data-path="C3BasicMLR.html"><a href="C3BasicMLR.html#exercises-2"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression - II</a>
<ul>
<li class="chapter" data-level="4.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#S4:BinaryVar"><i class="fa fa-check"></i><b>4.1</b> The Role of Binary Variables</a></li>
<li class="chapter" data-level="4.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#S4:SeveralCoeff"><i class="fa fa-check"></i><b>4.2</b> Statistical Inference for Several Coefficients</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#S4:SetsRegCoeff"><i class="fa fa-check"></i><b>4.2.1</b> Sets of Regression Coefficients</a></li>
<li class="chapter" data-level="4.2.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#S4:GenLinHypo"><i class="fa fa-check"></i><b>4.2.2</b> The General Linear Hypothesis</a></li>
<li class="chapter" data-level="4.2.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#S4:SetsInference"><i class="fa fa-check"></i><b>4.2.3</b> Estimating and Predicting Several Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#one-factor-anova-model"><i class="fa fa-check"></i><b>4.3</b> One Factor ANOVA Model</a></li>
<li class="chapter" data-level="4.4" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#combining-categorical-and-continuous-explanatory-variables"><i class="fa fa-check"></i><b>4.4</b> Combining Categorical and Continuous Explanatory Variables</a></li>
<li class="chapter" data-level="4.5" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#further-reading-and-references-3"><i class="fa fa-check"></i><b>4.5</b> Further Reading and References</a></li>
<li class="chapter" data-level="4.6" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#exercises-3"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#technical-supplement---matrix-expressions"><i class="fa fa-check"></i><b>4.7</b> Technical Supplement - Matrix Expressions</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#expressing-models-with-categorical-variables-in-matrix-form"><i class="fa fa-check"></i><b>4.7.1</b> Expressing Models with Categorical Variables in Matrix Form</a></li>
<li class="chapter" data-level="4.7.2" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#calculating-least-squares-recursively"><i class="fa fa-check"></i><b>4.7.2</b> Calculating Least Squares Recursively</a></li>
<li class="chapter" data-level="4.7.3" data-path="C4MLRANOVA.html"><a href="C4MLRANOVA.html#general-linear-model-1"><i class="fa fa-check"></i><b>4.7.3</b> General Linear Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTextDev/RegressionSpanish/" target="blank">Spanish Regression on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Regression Modeling with Actuarial and Financial Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="C3BasicMLR" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Multiple Linear Regression - I<a href="C3BasicMLR.html#C3BasicMLR" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Chapter Preview</em>. This chapter introduces linear regression in the case of several explanatory variables, known as <em>multiple linear regression</em>. Many basic linear regression concepts extend directly, including goodness of fit measures such as <span class="math inline">\(R^2\)</span> and inference using <span class="math inline">\(t\)</span>-statistics. Multiple linear regression models provide a framework for summarizing highly complex, multivariate data. Because this framework requires only linearity in the parameters, we are able to fit models that are nonlinear functions of the explanatory variables, thus providing a wide scope of potential applications.</p>
<div id="S3:LSMethod" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Method of Least Squares<a href="C3BasicMLR.html#S3:LSMethod" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="least-squares-method" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Least Squares Method<a href="C3BasicMLR.html#least-squares-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Chapter 2 dealt with the problem of a response depending on a single explanatory variable. We now extend the focus of that chapter and study how a response may depend on several explanatory variables.</p>
<hr />
<p><strong>Example: Term Life Insurance.</strong> Like all firms, life insurance companies continually seek new ways to deliver products to the market. Those involved in product development wish to know “who buys insurance and how much do they buy?” In economics, this is known as the <em>demand</em> side of a market for products. Analysts can readily get information on characteristics of current customers through company databases. Potential customers, those that do not have insurance with the company, are often the main focus for expanding market share.</p>
<p>In this example, we examine the Survey of Consumer Finances (SCF), a nationally representative sample that contains extensive information on assets, liabilities, income, and demographic characteristics of those sampled (potential U.S. customers). We study a random sample of 500 households with positive incomes that were interviewed in the 2004 survey. We initially consider the subset of <span class="math inline">\(n=275\)</span> families that purchased term life insurance. We wish to address the second portion of the demand question and determine family characteristics that influence the amount of insurance purchased. Chapter 11 will consider the first portion, whether or not a household purchases insurance, through models where the response is a binary random variable.</p>
<p>For term life insurance, the quantity of insurance is measured by the policy FACE, the amount that the company will pay in the event of the death of the named insured. Characteristics that will turn out to be important include annual INCOME, the number of years of EDUCATION of the survey respondent, and the number of household members, NUMHH.</p>
</div>
<div id="general-case-with-k-explanatory-variables" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> General Case with <span class="math inline">\(k\)</span> Explanatory Variables<a href="C3BasicMLR.html#general-case-with-k-explanatory-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In general, we will consider data sets where there are <span class="math inline">\(k\)</span> explanatory variables and one response variable in a sample of size <span class="math inline">\(n\)</span>. That is, the data consist of:</p>
<p><span class="math display">\[
\left\{
\begin{aligned}
x_{11},x_{12},\ldots,x_{1k},y_1 \\
x_{21},x_{22},\ldots,x_{2k},y_2 \\
\vdots \\
x_{n1},x_{n2},\ldots,x_{nk},y_n
\end{aligned}
\right\}.
\]</span></p>
<p>The <span class="math inline">\(i\)</span>th observation corresponds to the <span class="math inline">\(i\)</span>th row, consisting of <span class="math inline">\((x_{i1},x_{i2},\ldots,x_{ik},y_i)\)</span>. For this general case, we take <span class="math inline">\(k+1\)</span> measurements on each entity. For the insurance demand example, <span class="math inline">\(k=3\)</span> and the data consists of <span class="math inline">\((x_{11},x_{12},x_{13}, y_1), \ldots , (x_{275,1},x_{275,2},x_{275,3},y_{275})\)</span>. That is, we use four measurements from each of the <span class="math inline">\(n=275\)</span> households.</p>
<div id="summarizing-the-data" class="section level4 unnumbered hasAnchor">
<h4>Summarizing the Data<a href="C3BasicMLR.html#summarizing-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We begin the data analysis by examining each variable in isolation from the others. Table <a href="C3BasicMLR.html#tab:FaceSumStats">3.1</a> provides basic summary statistics of the four variables. For FACE and INCOME, we see that the mean is much greater than the median, suggesting that the distribution is skewed to the right. Histograms (not reported here) show that this is the case. It will turn out to be useful to also consider their logarithmic transforms, LNFACE and LNINCOME, respectively, which are also reported in Table <a href="C3BasicMLR.html#tab:FaceSumStats">3.1</a>.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:FaceSumStats">Table 3.1: </span><strong>Term Life Summary Statistics</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Median
</th>
<th style="text-align:right;">
Standard Deviation
</th>
<th style="text-align:right;">
Minimum
</th>
<th style="text-align:right;">
Maximum
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
FACE
</td>
<td style="text-align:right;width: 1.6cm; ">
747,581
</td>
<td style="text-align:right;width: 1.6cm; ">
150,000
</td>
<td style="text-align:right;width: 1.6cm; ">
1,674,362
</td>
<td style="text-align:right;width: 1.6cm; ">
800
</td>
<td style="text-align:right;width: 1.6cm; ">
14,000,000
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
INCOME
</td>
<td style="text-align:right;width: 1.6cm; ">
208,975
</td>
<td style="text-align:right;width: 1.6cm; ">
65,000
</td>
<td style="text-align:right;width: 1.6cm; ">
824,010
</td>
<td style="text-align:right;width: 1.6cm; ">
260
</td>
<td style="text-align:right;width: 1.6cm; ">
10,000,000
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
EDUCATION
</td>
<td style="text-align:right;width: 1.6cm; ">
14.524
</td>
<td style="text-align:right;width: 1.6cm; ">
16
</td>
<td style="text-align:right;width: 1.6cm; ">
2.549
</td>
<td style="text-align:right;width: 1.6cm; ">
2
</td>
<td style="text-align:right;width: 1.6cm; ">
17
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
NUMHH
</td>
<td style="text-align:right;width: 1.6cm; ">
2.96
</td>
<td style="text-align:right;width: 1.6cm; ">
3
</td>
<td style="text-align:right;width: 1.6cm; ">
1.493
</td>
<td style="text-align:right;width: 1.6cm; ">
1
</td>
<td style="text-align:right;width: 1.6cm; ">
9
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
LNFACE
</td>
<td style="text-align:right;width: 1.6cm; ">
11.99
</td>
<td style="text-align:right;width: 1.6cm; ">
11.918
</td>
<td style="text-align:right;width: 1.6cm; ">
1.871
</td>
<td style="text-align:right;width: 1.6cm; ">
6.685
</td>
<td style="text-align:right;width: 1.6cm; ">
16.455
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
LNINCOME
</td>
<td style="text-align:right;width: 1.6cm; ">
11.149
</td>
<td style="text-align:right;width: 1.6cm; ">
11.082
</td>
<td style="text-align:right;width: 1.6cm; ">
1.295
</td>
<td style="text-align:right;width: 1.6cm; ">
5.561
</td>
<td style="text-align:right;width: 1.6cm; ">
16.118
</td>
</tr>
</tbody>
</table>
<p>The next step is to measure the relationship between each <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, beginning with the scatter plots in Figure <a href="C3BasicMLR.html#fig:TermLifeTwoPlots">3.1</a>. The left-hand panel is a plot of FACE versus INCOME; in this panel, we see a large clustering in the lower left-hand corner corresponding to households that have both small incomes and face amounts of insurance. Both variables have skewed distributions and their joint effect is highly nonlinear. The right-hand panel presents the same variables but using logarithmic transforms. Here, we see a relationship that can be more readily approximated with a line.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:TermLifeTwoPlots"></span>
<img src="RegressionMarkdown_files/figure-html/TermLifeTwoPlots-1.png" alt="Income versus Face Amount of Term Life Insurance. The left-panel is a plot of face versus income, showing a highly nonlinear pattern. In the right-hand panel, face versus income is in natural logarithmic units, suggesting a linear (although variable) pattern." width="80%" />
<p class="caption">
Figure 3.1: <strong>Income versus Face Amount of Term Life Insurance.</strong> The left-panel is a plot of face versus income, showing a highly nonlinear pattern. In the right-hand panel, face versus income is in natural logarithmic units, suggesting a linear (although variable) pattern.
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Fig31.Hide" href="javascript:togglecode('toggleCode.Fig31.Hide','displayCode.Fig31.Hide');"><i><strong>R Code to Produce Table 3.1 and Figure 3.1</strong></i></a>
</h5>
<div id="toggleCode.Fig31.Hide" style="display: none">
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="C3BasicMLR.html#cb30-1" tabindex="-1"></a>Term <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;CSVData/TermLife.csv&quot;</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb30-2"><a href="C3BasicMLR.html#cb30-2" tabindex="-1"></a><span class="co">#  PICK THE SUBSET OF THE DATA CORRESPONDING TO TERM PURCHASE</span></span>
<span id="cb30-3"><a href="C3BasicMLR.html#cb30-3" tabindex="-1"></a>Term2  <span class="ot">&lt;-</span><span class="fu">subset</span>(Term, FACE <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb30-4"><a href="C3BasicMLR.html#cb30-4" tabindex="-1"></a><span class="co">#  TABLE 3.1 SUMMARY STATISTICS</span></span>
<span id="cb30-5"><a href="C3BasicMLR.html#cb30-5" tabindex="-1"></a>BookSummStats <span class="ot">&lt;-</span> <span class="cf">function</span>(Xymat){</span>
<span id="cb30-6"><a href="C3BasicMLR.html#cb30-6" tabindex="-1"></a>meanSummary <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, mean,  <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb30-7"><a href="C3BasicMLR.html#cb30-7" tabindex="-1"></a>sdSummary   <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, sd,    <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb30-8"><a href="C3BasicMLR.html#cb30-8" tabindex="-1"></a>minSummary  <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, min,   <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb30-9"><a href="C3BasicMLR.html#cb30-9" tabindex="-1"></a>maxSummary  <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, max,   <span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb30-10"><a href="C3BasicMLR.html#cb30-10" tabindex="-1"></a>medSummary  <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Xymat, median,<span class="at">na.rm=</span><span class="cn">TRUE</span>) </span>
<span id="cb30-11"><a href="C3BasicMLR.html#cb30-11" tabindex="-1"></a>tableMat  <span class="ot">&lt;-</span> <span class="fu">cbind</span>(meanSummary, medSummary, sdSummary, minSummary, maxSummary)</span>
<span id="cb30-12"><a href="C3BasicMLR.html#cb30-12" tabindex="-1"></a><span class="fu">return</span>(tableMat)</span>
<span id="cb30-13"><a href="C3BasicMLR.html#cb30-13" tabindex="-1"></a>}</span>
<span id="cb30-14"><a href="C3BasicMLR.html#cb30-14" tabindex="-1"></a></span>
<span id="cb30-15"><a href="C3BasicMLR.html#cb30-15" tabindex="-1"></a>LNFACE <span class="ot">&lt;-</span> <span class="fu">log</span>(Term2<span class="sc">$</span>FACE)</span>
<span id="cb30-16"><a href="C3BasicMLR.html#cb30-16" tabindex="-1"></a>LNINCOME <span class="ot">&lt;-</span> <span class="fu">log</span>(Term2<span class="sc">$</span>INCOME)</span>
<span id="cb30-17"><a href="C3BasicMLR.html#cb30-17" tabindex="-1"></a></span>
<span id="cb30-18"><a href="C3BasicMLR.html#cb30-18" tabindex="-1"></a>Xymat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">cbind</span>(Term2<span class="sc">$</span>FACE,</span>
<span id="cb30-19"><a href="C3BasicMLR.html#cb30-19" tabindex="-1"></a>Term2<span class="sc">$</span>INCOME,Term2<span class="sc">$</span>EDUCATION, Term2<span class="sc">$</span>NUMHH,LNFACE, LNINCOME) ) </span>
<span id="cb30-20"><a href="C3BasicMLR.html#cb30-20" tabindex="-1"></a>tableMat  <span class="ot">&lt;-</span> <span class="fu">BookSummStats</span>(Xymat)</span>
<span id="cb30-21"><a href="C3BasicMLR.html#cb30-21" tabindex="-1"></a></span>
<span id="cb30-22"><a href="C3BasicMLR.html#cb30-22" tabindex="-1"></a><span class="fu">colnames</span>(tableMat)  <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Mean&quot;</span> , <span class="st">&quot;Median&quot;</span> , <span class="st">&quot;Standard Deviation&quot;</span> , </span>
<span id="cb30-23"><a href="C3BasicMLR.html#cb30-23" tabindex="-1"></a>                         <span class="st">&quot;Minimum&quot;</span> , <span class="st">&quot;Maximum&quot;</span>)</span>
<span id="cb30-24"><a href="C3BasicMLR.html#cb30-24" tabindex="-1"></a><span class="fu">rownames</span>(tableMat)  <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;FACE&quot;</span>, <span class="st">&quot;INCOME&quot;</span>, <span class="st">&quot;EDUCATION&quot;</span>, <span class="st">&quot;NUMHH&quot;</span>,</span>
<span id="cb30-25"><a href="C3BasicMLR.html#cb30-25" tabindex="-1"></a>                         <span class="st">&quot;LNFACE&quot;</span>, <span class="st">&quot;LNINCOME&quot;</span>)</span>
<span id="cb30-26"><a href="C3BasicMLR.html#cb30-26" tabindex="-1"></a>tableMat1 <span class="ot">&lt;-</span> tableMat</span>
<span id="cb30-27"><a href="C3BasicMLR.html#cb30-27" tabindex="-1"></a>tableMat1[<span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>,]  <span class="ot">&lt;-</span> <span class="fu">round</span>(tableMat1[<span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>,], <span class="at">digits =</span> <span class="dv">3</span>)</span>
<span id="cb30-28"><a href="C3BasicMLR.html#cb30-28" tabindex="-1"></a>tableMat1[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,] <span class="ot">&lt;-</span> <span class="fu">format</span>(<span class="fu">round</span>(tableMat[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,], <span class="at">digits=</span><span class="dv">0</span>), <span class="at">big.mark =</span> <span class="st">&#39;,&#39;</span>)</span>
<span id="cb30-29"><a href="C3BasicMLR.html#cb30-29" tabindex="-1"></a></span>
<span id="cb30-30"><a href="C3BasicMLR.html#cb30-30" tabindex="-1"></a></span>
<span id="cb30-31"><a href="C3BasicMLR.html#cb30-31" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>tableMat1, </span>
<span id="cb30-32"><a href="C3BasicMLR.html#cb30-32" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Term Life Summary Statistics&#39;</span>, </span>
<span id="cb30-33"><a href="C3BasicMLR.html#cb30-33" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>, <span class="at">Digits=</span><span class="dv">3</span>, <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,</span>
<span id="cb30-34"><a href="C3BasicMLR.html#cb30-34" tabindex="-1"></a>         <span class="at">ColWidth =</span> ColWidth5) </span></code></pre></div>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="C3BasicMLR.html#cb31-1" tabindex="-1"></a>Term <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;CSVData/TermLife.csv&quot;</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb31-2"><a href="C3BasicMLR.html#cb31-2" tabindex="-1"></a><span class="co">#  PICK THE SUBSET OF THE DATA CORRESPONDING TO TERM PURCHASE</span></span>
<span id="cb31-3"><a href="C3BasicMLR.html#cb31-3" tabindex="-1"></a>Term2  <span class="ot">&lt;-</span><span class="fu">subset</span>(Term, FACE <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb31-4"><a href="C3BasicMLR.html#cb31-4" tabindex="-1"></a>Term2<span class="sc">$</span>LNFACE <span class="ot">&lt;-</span> <span class="fu">log</span>(Term2<span class="sc">$</span>FACE)</span>
<span id="cb31-5"><a href="C3BasicMLR.html#cb31-5" tabindex="-1"></a>Term2<span class="sc">$</span>LNINCOME <span class="ot">&lt;-</span> <span class="fu">log</span>(Term2<span class="sc">$</span>INCOME)</span>
<span id="cb31-6"><a href="C3BasicMLR.html#cb31-6" tabindex="-1"></a><span class="co">#  FIGURE 3.1</span></span>
<span id="cb31-7"><a href="C3BasicMLR.html#cb31-7" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">cex=</span><span class="fl">1.1</span>, <span class="at">mar=</span><span class="fu">c</span>(<span class="fl">4.1</span>,<span class="dv">4</span>,<span class="fl">1.5</span>,<span class="dv">1</span>))</span>
<span id="cb31-8"><a href="C3BasicMLR.html#cb31-8" tabindex="-1"></a><span class="fu">plot</span>(Term2<span class="sc">$</span>INCOME, Term2<span class="sc">$</span>FACE, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">las=</span><span class="dv">1</span>, <span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;INCOME (in Millions)&quot;</span>)</span>
<span id="cb31-9"><a href="C3BasicMLR.html#cb31-9" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;FACE (in Millions)&quot;</span>, <span class="at">side=</span><span class="dv">2</span>, <span class="at">at=</span><span class="dv">15200000</span>, <span class="at">las=</span><span class="dv">1</span>, <span class="at">cex=</span><span class="fl">1.1</span>, <span class="at">adj=</span>.<span class="dv">4</span>)</span>
<span id="cb31-10"><a href="C3BasicMLR.html#cb31-10" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">2</span>,<span class="at">at=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">14000000</span>,<span class="dv">2000000</span>), <span class="at">labels=</span><span class="fu">c</span>(<span class="st">&quot;0&quot;</span>, <span class="st">&quot;2&quot;</span>, </span>
<span id="cb31-11"><a href="C3BasicMLR.html#cb31-11" tabindex="-1"></a>    <span class="st">&quot;4&quot;</span>, <span class="st">&quot;6&quot;</span>, <span class="st">&quot;8&quot;</span>,<span class="st">&quot;10&quot;</span>,<span class="st">&quot;12&quot;</span>,<span class="st">&quot;14&quot;</span>), <span class="at">las=</span><span class="dv">1</span>)</span>
<span id="cb31-12"><a href="C3BasicMLR.html#cb31-12" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>,<span class="at">at=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">11000000</span>,<span class="dv">1000000</span>), <span class="at">labels=</span><span class="fu">c</span>(<span class="st">&quot;0&quot;</span>,<span class="st">&quot;1&quot;</span>, <span class="st">&quot;2&quot;</span>,<span class="st">&quot;3&quot;</span>, </span>
<span id="cb31-13"><a href="C3BasicMLR.html#cb31-13" tabindex="-1"></a>    <span class="st">&quot;4&quot;</span>,<span class="st">&quot;5&quot;</span>, <span class="st">&quot;6&quot;</span>,<span class="st">&quot;7&quot;</span>,<span class="st">&quot;8&quot;</span>,<span class="st">&quot;9&quot;</span>,<span class="st">&quot;10&quot;</span>,<span class="st">&quot;11&quot;</span>))</span>
<span id="cb31-14"><a href="C3BasicMLR.html#cb31-14" tabindex="-1"></a><span class="fu">plot</span>(Term2<span class="sc">$</span>LNINCOME, Term2<span class="sc">$</span>LNFACE, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;LNINCOME&quot;</span>, <span class="at">las=</span><span class="dv">1</span>)</span>
<span id="cb31-15"><a href="C3BasicMLR.html#cb31-15" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;LNFACE&quot;</span>, <span class="at">side=</span><span class="dv">2</span>, <span class="at">at=</span><span class="fl">16.8</span>, <span class="at">las=</span><span class="dv">1</span>, <span class="at">cex=</span><span class="fl">1.1</span>, <span class="at">adj=</span><span class="fl">1.1</span>)</span></code></pre></div>
</div>
<p>The Term Life data are <em>multivariate</em> in the sense that several measurements are taken on each household. It is difficult to produce a graph of observations in three or more dimensions on a two-dimensional platform, such as a piece of paper, that is not confusing, misleading, or both. To summarize graphically multivariate data in regression applications, consider using a <em>scatterplot matrix</em> such as in Figure <a href="C3BasicMLR.html#fig:TermLifeSMatrix">3.2</a>. Each square of this figure represents a simple plot of one variable versus another. For each square, the row variable gives the units of the vertical axis and the column variable gives the units of the horizontal axis. The matrix is sometimes called a <em>half scatterplot matrix</em> because only the lower left-hand elements are presented.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:TermLifeSMatrix"></span>
<img src="RegressionMarkdown_files/figure-html/TermLifeSMatrix-1.png" alt="Scatterplot matrix of four variables. Each square is a scatter plot." width="80%" />
<p class="caption">
Figure 3.2: <strong>Scatterplot matrix of four variables.</strong> Each square is a scatter plot.
</p>
</div>
<p>The scatterplot matrix can be numerically summarized using a correlation matrix. Each correlation in Table <a href="C3BasicMLR.html#tab:TermLifeCorr">3.2</a> corresponds to a square of the scatterplot matrix in Figure <a href="C3BasicMLR.html#fig:TermLifeSMatrix">3.2</a>. Analysts often present tables of correlations because they are easy to interpret. However, remember that a correlation coefficient merely measures the extent of linear relationships. Thus, a table of correlations provides a sense of linear relationships but may miss a nonlinear relationship that can be revealed in a scatterplot matrix.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:TermLifeCorr">Table 3.2: </span><strong>Term Life Correlations</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
NUMHH
</th>
<th style="text-align:right;">
EDUCATION
</th>
<th style="text-align:right;">
LNINCOME
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
EDUCATION
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.064
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
LNINCOME
</td>
<td style="text-align:right;width: 1.6cm; ">
0.179
</td>
<td style="text-align:right;width: 1.6cm; ">
0.343
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
LNFACE
</td>
<td style="text-align:right;width: 1.6cm; ">
0.288
</td>
<td style="text-align:right;width: 1.6cm; ">
0.383
</td>
<td style="text-align:right;width: 1.6cm; ">
0.482
</td>
</tr>
</tbody>
</table>
<p>The scatterplot matrix and corresponding correlation matrix are useful devices for summarizing multivariate data. They are easy to produce and to interpret. Still, each device captures only relationships between pairs of variables and cannot quantify relationships among several variables.</p>
<h5 style="text-align: center;">
<a id="displayCode.Fig32.Hide" href="javascript:togglecode('toggleCode.Fig32.Hide','displayCode.Fig32.Hide');"><i><strong>R Code to Produce Table 3.2 and Figure 3.2</strong></i></a>
</h5>
<div id="toggleCode.Fig32.Hide" style="display: none">
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="C3BasicMLR.html#cb32-1" tabindex="-1"></a>tableCor <span class="ot">&lt;-</span> <span class="fu">cor</span>(Term1)</span>
<span id="cb32-2"><a href="C3BasicMLR.html#cb32-2" tabindex="-1"></a>tableCor <span class="ot">&lt;-</span> <span class="fu">round</span>(tableCor, <span class="at">digits =</span> <span class="dv">3</span>)</span>
<span id="cb32-3"><a href="C3BasicMLR.html#cb32-3" tabindex="-1"></a>tableCor[<span class="fu">upper.tri</span>(tableCor, <span class="at">diag =</span> <span class="cn">TRUE</span>)] <span class="ot">&lt;-</span> <span class="st">&quot;&quot;</span></span>
<span id="cb32-4"><a href="C3BasicMLR.html#cb32-4" tabindex="-1"></a>tablePrint <span class="ot">&lt;-</span> tableCor[<span class="sc">-</span><span class="dv">1</span>,]</span>
<span id="cb32-5"><a href="C3BasicMLR.html#cb32-5" tabindex="-1"></a>tablePrint <span class="ot">&lt;-</span> tablePrint[,<span class="sc">-</span><span class="dv">4</span>]</span>
<span id="cb32-6"><a href="C3BasicMLR.html#cb32-6" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>tablePrint, </span>
<span id="cb32-7"><a href="C3BasicMLR.html#cb32-7" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Term Life Correlations&#39;</span>, </span>
<span id="cb32-8"><a href="C3BasicMLR.html#cb32-8" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>, <span class="at">Digits=</span><span class="dv">3</span>, <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb32-9"><a href="C3BasicMLR.html#cb32-9" tabindex="-1"></a>         <span class="at">ColWidth =</span> ColWidth5)</span></code></pre></div>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="C3BasicMLR.html#cb33-1" tabindex="-1"></a><span class="co">#  FIGURE 3.2</span></span>
<span id="cb33-2"><a href="C3BasicMLR.html#cb33-2" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">4.1</span>,<span class="fl">2.1</span>,<span class="fl">2.1</span>,<span class="fl">2.1</span>), <span class="at">cex=</span><span class="fl">1.1</span>)</span>
<span id="cb33-3"><a href="C3BasicMLR.html#cb33-3" tabindex="-1"></a>varTerm <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;NUMHH&quot;</span>,<span class="st">&quot;EDUCATION&quot;</span>, <span class="st">&quot;LNINCOME&quot;</span>, <span class="st">&quot;LNFACE&quot;</span>)</span>
<span id="cb33-4"><a href="C3BasicMLR.html#cb33-4" tabindex="-1"></a>Term1 <span class="ot">&lt;-</span> Term2[varTerm]</span>
<span id="cb33-5"><a href="C3BasicMLR.html#cb33-5" tabindex="-1"></a><span class="fu">pairs</span>(Term1,<span class="at">upper.panel=</span><span class="cn">NULL</span>, <span class="at">gap=</span><span class="dv">0</span>,<span class="at">cex.labels=</span><span class="fl">1.25</span>, <span class="at">las=</span><span class="dv">1</span>)</span></code></pre></div>
</div>
</div>
<div id="method-of-least-squares-1" class="section level4 unnumbered hasAnchor">
<h4>Method of Least Squares<a href="C3BasicMLR.html#method-of-least-squares-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Consider the question: “Can knowledge of education, household size, and income help us understand the demand for insurance?” The correlations in Table <a href="C3BasicMLR.html#tab:TermLifeCorr">3.2</a> and the graphs in Figures <a href="C3BasicMLR.html#fig:TermLifeTwoPlots">3.1</a> and <a href="C3BasicMLR.html#fig:TermLifeSMatrix">3.2</a> suggest that each variable, EDUCATION, NUMHH, and LNINCOME, may be a useful explanatory variable of LNFACE when taken individually. It seems reasonable to investigate the <em>joint</em> effect of these variables on a response.</p>
<p>The geometric concept of a <em>plane</em> is used to explore the linear relationship between a response and several explanatory variables. Recall that a plane extends the concept of a line to more than two dimensions. A plane may be defined through an algebraic equation such as</p>
<p><span class="math display">\[
y = b_0 + b_1 x_1 + \ldots + b_k x_k.
\]</span></p>
<p>This equation defines a plane in <span class="math inline">\(k+1\)</span> dimensions. Figure <a href="C3BasicMLR.html#fig:3DPlane">3.3</a> shows a plane in three dimensions. For this figure, there is one response variable, LNFACE, and two explanatory variables, EDUCATION and LNINCOME (NUMHH is held fixed). It is difficult to graph more than three dimensions in a meaningful way.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:3DPlane"></span>
<img src="RegressionMarkdown_files/figure-html/3DPlane-1.png" alt="An example of a three-dimensional plane" width="50%" />
<p class="caption">
Figure 3.3: <strong>An example of a three-dimensional plane</strong>
</p>
</div>
<p>We need a way to determine a plane based on the data. The difficulty is that in most regression analysis applications, the number of observations, <span class="math inline">\(n\)</span>, far exceeds the number of observations required to fit a plane, <span class="math inline">\(k+1\)</span>. Thus, it is generally not possible to find a single plane that passes through all <span class="math inline">\(n\)</span> observations. As in Chapter 2, we use the <em>method of least squares</em> to determine a plane from the data.</p>
<p>The method of least squares is based on determining the values of <span class="math inline">\(b_0^{\ast},b_1^{\ast},\ldots,b_k^{\ast}\)</span> that minimize the quantity</p>
<p><span class="math display" id="eq:eq31">\[
SS(b_0^{\ast},b_1^{\ast},\ldots,b_k^{\ast})=\sum_{i=1}^{n}\left( y_i-\left( b_0^{\ast}+b_1^{\ast}x_{i1}+\ldots+b_k^{\ast}x_{ik}\right) \right) ^2.
\tag{3.1}
\]</span></p>
<p>We drop the asterisk, or star, notation and use <span class="math inline">\(b_0, b_1, \ldots, b_k\)</span> to denote the best values, known as the <em>least squares estimates</em>. With the least squares estimates, define the <em>least squares, or fitted, regression plane</em> as</p>
<p><span class="math display">\[
\widehat{y} = b_0 + b_1 x_1 + \ldots + b_k x_k.
\]</span></p>
<p>The least squares estimates are determined by minimizing <span class="math inline">\(SS(b_0^{\ast},b_1^{\ast},\ldots,b_k^{\ast})\)</span>. It is difficult to write down the resulting least squares estimators using a simple formula unless one resorts to matrix notation. Because of their importance in applied statistical models, an explicit formula for the estimators is provided below. However, these formulas have been programmed into a wide variety of statistical and spreadsheet software packages. The availability of these packages allows data analysts to concentrate on the ideas of the estimation procedure instead of focusing on the details of the calculation procedures.</p>
<p>As an example, a regression plane was fit to the Term Life data where three explanatory variables, <span class="math inline">\(x_1\)</span> for EDUCATION, <span class="math inline">\(x_2\)</span> for NUMHH, and <span class="math inline">\(x_3\)</span> for LNINCOME, were used. The resulting fitted regression plane is</p>
<p><span class="math display" id="eq:eq32">\[
\widehat{y} = 2.584 + 0.206 x_1 + 0.306 x_2 + 0.494 x_3.
\tag{3.2}
\]</span></p>
</div>
<div id="matrix-notation" class="section level4 unnumbered hasAnchor">
<h4>Matrix Notation<a href="C3BasicMLR.html#matrix-notation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Assume that the data are of the form <span class="math inline">\((x_{i0}, x_{i1}, \ldots, x_{ik}, y_i)\)</span>, where <span class="math inline">\(i = 1, \ldots, n\)</span>. Here, the variable <span class="math inline">\(x_{i0}\)</span> is associated with the “intercept” term. In most applications, we assume that <span class="math inline">\(x_{i0}\)</span> is identically equal to 1 and thus need not be explicitly represented. However, there are important applications where this is not the case, and thus, to express the model in general notation, it is included here. The data are represented in matrix notation using:</p>
<p><span class="math display">\[
\mathbf{y} = \begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}
~~\text{and}~~
\mathbf{X} = \begin{pmatrix}
x_{10} &amp; x_{11} &amp; \cdots &amp; x_{1k} \\
x_{20} &amp; x_{21} &amp; \cdots &amp; x_{2k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n0} &amp; x_{n1} &amp; \cdots &amp; x_{nk}
\end{pmatrix}.
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{y}\)</span> is the <span class="math inline">\(n \times 1\)</span> vector of responses and <span class="math inline">\(\mathbf{X}\)</span> is the <span class="math inline">\(n \times (k+1)\)</span> matrix of explanatory variables. We use the matrix algebra convention that lower and upper case bold letters represent vectors and matrices, respectively. (If you need to brush up on matrices, review Section 2.11.)</p>
<hr />
<p><strong>Example: Term Life Insurance - Continued.</strong> Recall that <span class="math inline">\(y\)</span> represents the logarithmic face, <span class="math inline">\(x_1\)</span> for years of education, <span class="math inline">\(x_2\)</span> for the number of household members, and <span class="math inline">\(x_3\)</span> for logarithmic income. Thus, there are <span class="math inline">\(k = 3\)</span> explanatory variables and <span class="math inline">\(n = 275\)</span> households. The vector of responses and the matrix of explanatory variables are:</p>
<p><span class="math display">\[
\mathbf{y} = \begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_{275}
\end{pmatrix}
=
\begin{pmatrix}
9.904 \\
11.775 \\
\vdots \\
9.210
\end{pmatrix}
~~\text{and}~~ \\
\mathbf{X} = \begin{pmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; x_{13} \\
1 &amp; x_{21} &amp; x_{22} &amp; x_{23} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_{275,1} &amp; x_{275,2} &amp; x_{275,3}
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 16 &amp; 3 &amp; 10.669 \\
1 &amp; 9 &amp; 3 &amp; 9.393 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; 12 &amp; 1 &amp; 10.545
\end{pmatrix}.
\]</span></p>
<p>For example, for the first observation in the data set, the dependent variable is <span class="math inline">\(y_1 = 9.904\)</span> (corresponding to <span class="math inline">\(\exp(9.904) = \$20,000\)</span>), for a survey respondent with 16 years of education living in a household with 3 people and a logarithmic income of 10.669 (<span class="math inline">\(\exp(10.669) = \$43,000\)</span>).</p>
<hr />
<p>Under the least squares estimation principle, our goal is to choose the coefficients <span class="math inline">\(b_0^{\ast}, b_1^{\ast}, \ldots, b_k^{\ast}\)</span> to minimize the sum of squares function <span class="math inline">\(SS(b_0^{\ast}, b_1^{\ast}, \ldots, b_k^{\ast})\)</span>. Using calculus, we return to the equation for the sum of squares, take partial derivatives with respect to each coefficient, and set these quantities equal to zero:</p>
<p><span class="math display">\[
\begin{array}{ll}
\frac{\partial }{\partial b_j^{\ast}}SS(b_0^{\ast}, b_1^{\ast}, \ldots, b_k^{\ast}) &amp;= \sum_{i=1}^{n}\left( -2x_{ij}\right) \left( y_i-\left( b_0^{\ast}+b_1^{\ast}x_{i1}+\ldots+b_k^{\ast}x_{ik}\right) \right) \\
&amp;= 0, ~~~ \text{for} ~ j=0,1,\ldots,k.
\end{array}
\]</span></p>
<p>This is a system of <span class="math inline">\(k+1\)</span> equations and <span class="math inline">\(k+1\)</span> unknowns that can be readily solved using matrix notation, as follows.</p>
<p>We may express the vector of parameters to be minimized as <span class="math inline">\(\mathbf{b}^{\ast}=(b_0^{\ast}, b_1^{\ast}, \ldots, b_k^{\ast})^{\prime}\)</span>. Using this, the sum of squares can be written as <span class="math inline">\(SS(\mathbf{b}^{\ast}) = (\mathbf{y-Xb}^{\ast})^{\prime}(\mathbf{y-Xb}^{\ast})\)</span>. Thus, in matrix form, the solution to the minimization problem can be expressed as <span class="math inline">\(\frac{\partial}{\partial \mathbf{b}^{\ast}} SS(\mathbf{b}^{\ast}) = \mathbf{0}\)</span>. This solution satisfies the <em>normal equations</em>:</p>
<p><span class="math display" id="eq:eq33">\[
\mathbf{X^{\prime}Xb} = \mathbf{X}^{\prime}\mathbf{y}.
\tag{3.3}
\]</span></p>
<p>Here, the asterisk notation (*) has been dropped to denote the fact that <span class="math inline">\(\mathbf{b} = (b_0, b_1, \ldots, b_k)^{\prime}\)</span> represents the best vector of values in the sense of minimizing <span class="math inline">\(SS(\mathbf{b}^{\ast})\)</span> over all choices of <span class="math inline">\(\mathbf{b}^{\ast}\)</span>.</p>
<p>The least squares estimator <span class="math inline">\(\mathbf{b}\)</span> need not be unique. However, assuming that the explanatory variables are not linear combinations of one another, we have that <span class="math inline">\(\mathbf{X^{\prime}X}\)</span> is invertible. In this case, we can write the unique solution as:</p>
<p><span class="math display" id="eq:eq34">\[
\mathbf{b} = \left( \mathbf{X^{\prime}X} \right)^{-1} \mathbf{X}^{\prime} \mathbf{y}.
\tag{3.4}
\]</span></p>
<p>To illustrate, for the Term Life example, equation <a href="C3BasicMLR.html#eq:eq34">(3.4)</a> yields:</p>
<p><span class="math display">\[
\mathbf{b} = \begin{pmatrix}
b_0 \\
b_1 \\
b_2 \\
b_3 \\
\end{pmatrix}
=
\begin{pmatrix}
2.584 \\
0.206 \\
0.306 \\
0.494 \\
\end{pmatrix}.
\]</span></p>
</div>
</div>
</div>
<div id="linear-regression-model-and-properties-of-estimators" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Linear Regression Model and Properties of Estimators<a href="C3BasicMLR.html#linear-regression-model-and-properties-of-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous section, we learned how to use the method of least squares to fit a regression plane with a data set. This section describes the assumptions underpinning the regression model and some of the resulting properties of the regression coefficient estimators. With the model and the fitted data, we will be able to draw inferences about the sample data set to a larger population. Moreover, we will later use these regression model assumptions to help us improve the model specification in Chapter 5.</p>
<div id="regression-function" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Regression Function<a href="C3BasicMLR.html#regression-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Most of the assumptions of the multiple linear regression model will carry over directly from the basic linear regression model assumptions introduced in Section 2.2. The primary difference is that we now summarize the relationship between the response and the explanatory variables through the <em>regression function</em>:</p>
<p><span class="math display" id="eq:eq35">\[
\mathrm{E~}y = \beta_0 x_0 + \beta_1 x_1 + \ldots + \beta_k x_k,
\tag{3.5}
\]</span></p>
<p>which is linear in the parameters <span class="math inline">\(\beta_0,\ldots,\beta_k\)</span>. Henceforth, we will use <span class="math inline">\(x_0 = 1\)</span> for the variable associated with the parameter <span class="math inline">\(\beta_0\)</span>; this is the default in most statistical packages, and most applications of regression include the intercept term <span class="math inline">\(\beta_0\)</span>. The intercept is the expected value of <span class="math inline">\(y\)</span> when all of the explanatory variables are equal to zero. Although rarely of interest, the term <span class="math inline">\(\beta_0\)</span> serves to set the height of the fitted regression plane.</p>
<p>In contrast, the other betas are typically important parameters from a regression study. To help interpret them, we initially assume that <span class="math inline">\(x_j\)</span> varies continuously and is not related to the other explanatory variables. Then, we can interpret <span class="math inline">\(\beta_j\)</span> as the expected change in <span class="math inline">\(y\)</span> per unit change in <span class="math inline">\(x_j\)</span> <em>assuming all other explanatory variables are held fixed</em>. That is, from calculus, you will recognize that <span class="math inline">\(\beta_j\)</span> can be interpreted as a partial derivative. Specifically, using the equation above, we have that</p>
<p><span class="math display">\[
\beta_j = \frac{\partial }{\partial x_j}\mathrm{E}~y.
\]</span></p>
</div>
<div id="regression-coefficient-interpretation" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Regression Coefficient Interpretation<a href="C3BasicMLR.html#regression-coefficient-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us examine the regression coefficient estimates from the Term Life Insurance example and focus initially on the <em>sign</em> of the coefficients. For example, from equation <a href="C3BasicMLR.html#eq:eq34">(3.4)</a>, the coefficient associated with NUMHH is <span class="math inline">\(b_2 = 0.306 &gt; 0\)</span>. If we consider two households that have the same income and the same level of education, then the larger household (in terms of NUMHH) is expected to demand <em>more</em> term life insurance under the regression model. This is a sensible interpretation; larger households have more dependents for which term life insurance can provide needed financial assets in the event of the untimely death of a breadwinner. The positive coefficient associated with income (<span class="math inline">\(b_3 = 0.494\)</span>) is also plausible; households with larger incomes have more disposable dollars to purchase insurance. The positive sign associated with EDUCATION (<span class="math inline">\(b_1 = 0.206)\)</span> is also reasonable; more education suggests that respondents are more aware of their insurance needs, other things being equal.</p>
<p>You will also need to interpret the <em>amount</em> of the regression coefficient. Consider first the EDUCATION coefficient. Using equation <a href="C3BasicMLR.html#eq:eq34">(3.4)</a>, fitted values of <span class="math inline">\(\widehat{\mathrm{LNFACE}}\)</span> were calculated by allowing EDUCATION to vary and keeping NUMHH and LNINCOME fixed at the sample averages. The results are:</p>
<p><strong>Effects of Small Changes in Education</strong></p>
<table class=" lightable-classic" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td style="text-align:left;">
EDUCATION
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
14.1
</td>
<td style="text-align:right;">
14.2
</td>
<td style="text-align:right;">
14.3
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\widehat{\mathit{LNFACE}}\)</span>
</td>
<td style="text-align:right;">
11.883
</td>
<td style="text-align:right;">
11.904
</td>
<td style="text-align:right;">
11.924
</td>
<td style="text-align:right;">
11.945
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\widehat{\mathit{FACE}}\)</span>
</td>
<td style="text-align:right;">
144,803
</td>
<td style="text-align:right;">
147,817
</td>
<td style="text-align:right;">
150,893
</td>
<td style="text-align:right;">
154,034
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\widehat{\mathit{FACE}}\)</span> % Change
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
2.081
</td>
<td style="text-align:right;">
2.081
</td>
<td style="text-align:right;">
2.081
</td>
</tr>
</tbody>
</table>
<p>As EDUCATION increases, <span class="math inline">\(\widehat{\mathrm{LNFACE}}\)</span> increases. Further, the amount of <span class="math inline">\(\widehat{\mathrm{LNFACE}}\)</span> increase is a constant 0.0206. This comes directly from equation <a href="C3BasicMLR.html#eq:eq34">(3.4)</a>; as EDUCATION increases by 0.1 years, we expect the demand for insurance to increase by 0.0206 logarithmic dollars, holding NUMHH and LNINCOME fixed. This interpretation is correct, but most product development directors are not overly fond of logarithmic dollars. To return to dollars, fitted face values can be calculated through exponentiation as <span class="math inline">\(\widehat{\mathrm{FACE}} = \exp(\widehat{\mathrm{LNFACE}})\)</span>. Moreover, the percentage change can be computed; for example, <span class="math inline">\(100 \times (147,817/144,803 - 1) \approx 2.08\%\)</span>.</p>
<p>This provides another interpretation of the regression coefficient; as EDUCATION increases by 0.1 years, we expect the demand for insurance to increase by 2.08%. This is a simple consequence of calculus using <span class="math inline">\(\partial \ln y / \partial x = \left(\partial y / \partial x \right) / y\)</span>; that is, a small change in the logarithmic value of <span class="math inline">\(y\)</span> equals a small change in <span class="math inline">\(y\)</span> as a proportion of <span class="math inline">\(y\)</span>. It is because of this calculus result that we use natural logs instead of common logs in regression analysis. Because this table uses a discrete change in EDUCATION, the 2.08% differs slightly from the continuous result <span class="math inline">\(0.206 \times (\mathrm{change~in~EDUCATION}) = 2.06\%\)</span>. However, this proximity is usually regarded as suitable for interpretation purposes.</p>
<p>Continuing this logic, consider small changes in logarithmic income.</p>
<strong>Effects of Small Changes in Logarithmic Income</strong>
<table class=" lightable-classic" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td style="text-align:left;">
LNINCOME
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
11.1
</td>
<td style="text-align:right;">
11.2
</td>
<td style="text-align:right;">
11.3
</td>
</tr>
<tr>
<td style="text-align:left;">
INCOME
</td>
<td style="text-align:right;">
59,874
</td>
<td style="text-align:right;">
66,171
</td>
<td style="text-align:right;">
73,130
</td>
<td style="text-align:right;">
80,822
</td>
</tr>
<tr>
<td style="text-align:left;">
INCOME % Change
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
10.52
</td>
<td style="text-align:right;">
10.52
</td>
<td style="text-align:right;">
10.52
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\widehat{\mathit{LNFACE}}\)</span>
</td>
<td style="text-align:right;">
11.957
</td>
<td style="text-align:right;">
12.006
</td>
<td style="text-align:right;">
12.055
</td>
<td style="text-align:right;">
12.105
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\widehat{\mathit{FACE}}\)</span>
</td>
<td style="text-align:right;">
155,831
</td>
<td style="text-align:right;">
163,722
</td>
<td style="text-align:right;">
172,013
</td>
<td style="text-align:right;">
180,724
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\widehat{\mathit{FACE}}\)</span> % Change
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
5.06
</td>
<td style="text-align:right;">
5.06
</td>
<td style="text-align:right;">
5.06
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\widehat{\mathit{FACE}}\)</span> % Change / INCOME % Change
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
0.482
</td>
<td style="text-align:right;">
0.482
</td>
<td style="text-align:right;">
0.482
</td>
</tr>
</tbody>
</table>
<p>We can use the same logic to interpret the LNINCOME coefficient in equation <a href="C3BasicMLR.html#eq:eq34">(3.4)</a>. As logarithmic income increases by 0.1 units, we expect the demand for insurance to increase by 5.06%. This takes care of logarithmic units in the <span class="math inline">\(y\)</span> but not the <span class="math inline">\(x\)</span>. We can use the same logic to say that as logarithmic income increases by 0.1 units, INCOME increases by 10.52%. Thus, a 10.52% change in INCOME corresponds to a 5.06% change in FACE. Summarizing, we say that, holding NUMHH and EDUCATION fixed, we expect that a 1% increase in INCOME is associated with a 0.482% increase in <span class="math inline">\(\widehat{\mathrm{FACE}}\)</span> (as before, this is close to the parameter estimate <span class="math inline">\(b_3 = 0.494\)</span>). The coefficient associated with income is known as an <em>elasticity</em> in economics. In economics, elasticity is the ratio of the percent change in one variable to the percent change in another variable. Mathematically, we summarize this as:</p>
<p><span class="math display">\[
\frac{\partial \ln y}{\partial \ln x} = \left(\frac{\partial y}{y}\right)/\left(\frac{\partial x}{x}\right).
\]</span></p>
</div>
<div id="model-assumptions" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Model Assumptions<a href="C3BasicMLR.html#model-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As in Section 2.2 for a single explanatory variable, there are two sets of assumptions that one can use for multiple linear regression. They are equivalent sets, each having comparative advantages as we proceed in our study of regression. The “observables” representation focuses on variables of interest <span class="math inline">\((x_{i1}, \ldots, x_{ik}, y_i)\)</span>. The “error representation” provides a springboard for motivating our goodness of fit measures and study of residual analysis. However, the latter set of assumptions focuses on the additive errors case and obscures the sampling basis of the model.</p>
<p><span class="math display">\[
\textbf{Multiple Linear Regression Model Sampling Assumptions} \\
\small{
\begin{array}{ll}
\text{Observables Representation} &amp; \text{Error Representation} \\ \hline
F1.~ \mathrm{E}~y_i=\beta_0+\beta_1
x_{i1}+\ldots+\beta_k x_{ik}.
&amp; E1.~ y_i=\beta_0+\beta_1 x_{i1}+\ldots+\beta_k x_{ik}+\varepsilon_i. \\
F2.~ \{x_{i1},\ldots ,x_{ik}\} &amp;
E2.~
\{x_{i1},\ldots ,x_{ik}\} \\
\ \ \ \ \ \ \ \  \text{are non-stochastic variables.} &amp; \ \ \ \ \ \ \ \  \text{are non-stochastic variables.} \\
F3.~ \mathrm{Var}~y_i=\sigma^2. &amp;
E3.~ \mathrm{E}~\varepsilon_i=0 \text{ and } \mathrm{Var}~\varepsilon_i=\sigma^2. \\
F4.~ \{y_i\} \text{ are independent random variables.}
&amp; E4.~ \{\varepsilon_i\} \text{ are independent random variables.} \\
F5.~ \{y_i\} \text{ are normally distributed.} &amp;
E5.~ \{\varepsilon_i\} \text{ are normally distributed.} \\
\hline
\end{array}
}
\]</span></p>
<p>To further motivate Assumptions F2 and F4, we will usually assume that our data have been realized as the result of a stratified sampling scheme, where each unique value of <span class="math inline">\(\{x_{i1}, \ldots, x_{ik}\}\)</span> is treated as a stratum. That is, for each value of <span class="math inline">\(\{x_{i1}, \ldots, x_{ik}\}\)</span>, we draw a random sample of responses from a population. Thus, responses within each stratum are independent from one another, as are responses from different strata. Chapter 6 will discuss this sampling basis in further detail.</p>
</div>
<div id="properties-of-regression-coefficient-estimators-1" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Properties of Regression Coefficient Estimators<a href="C3BasicMLR.html#properties-of-regression-coefficient-estimators-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Section <a href="C3BasicMLR.html#S3:LSMethod">3.1</a> described the least squares method for estimating regression coefficients. With the regression model assumptions, we can establish some basic properties of these estimators. To do this, from Section 2.11.4 we have that the expectation of a vector is the vector of expectations, so that</p>
<p><span class="math display">\[
\mathrm{E}~\mathbf{y} = \left(
\begin{array}{l}
\mathrm{E}~y_1 \\
\mathrm{E}~y_2 \\
\vdots \\
\mathrm{E}~y_n
\end{array}
\right) .
\]</span></p>
<p>Further, basic matrix multiplication shows that</p>
<p><span class="math display">\[
\small{
\mathbf{X} \boldsymbol \beta = \left(
\begin{array}{cccc}
1 &amp; x_{11} &amp; \cdots &amp; x_{1k} \\
1 &amp; x_{21} &amp; \cdots &amp; x_{2k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n1} &amp; \cdots &amp; x_{nk}
\end{array}
\right) \left(
\begin{array}{c}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_k
\end{array}
\right) = \left(
\begin{array}{c}
\beta_0 + \beta_1 x_{11} + \cdots + \beta_k x_{1k} \\
\beta_0 + \beta_1 x_{21} + \cdots + \beta_k x_{2k} \\
\vdots \\
\beta_0 + \beta_1 x_{n1} + \cdots + \beta_k x_{nk}
\end{array}
\right) .
}
\]</span></p>
<p>Because the <span class="math inline">\(i\)</span>th row of assumption F1 is <span class="math inline">\(\mathrm{E}~y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik}\)</span>, we may re-write this assumption in matrix formulation as <span class="math inline">\(\mathrm{E}~\mathbf{y} = \mathbf{X} \boldsymbol \beta\)</span>. We are now in a position to state the first important property of least squares regression estimators.</p>
<div class="blackbox">
<p><strong>Property 1</strong>. Consider a regression model and let Assumptions F1-F4 hold. Then, the estimator <span class="math inline">\(\mathbf{b}\)</span> defined in equation <a href="C3BasicMLR.html#eq:eq34">(3.4)</a>
is an unbiased estimator of the parameter vector <span class="math inline">\(\boldsymbol \beta\)</span>.</p>
</div>
<p>To establish Property 1, we have that</p>
<p><span class="math display">\[
\mathrm{E}~\mathbf{b} = \mathrm{E}~\left((\mathbf{X^{\prime}X)}^{-1}\mathbf{X}^{\prime}\mathbf{y}\right)
= (\mathbf{X^{\prime}X)}^{-1}\mathbf{X}^{\prime}\mathrm{E}~\mathbf{y} = (\mathbf{X^{\prime}X)}^{-1} \mathbf{X}^{\prime} \left( \mathbf{X} \boldsymbol \beta \right) = \boldsymbol \beta,
\]</span></p>
<p>using matrix multiplication rules. This chapter assumes that <span class="math inline">\(\mathbf{X^{\prime}X}\)</span> is invertible. One can also show that the least squares estimator need only be a solution of the normal equations for unbiasedness (not requiring that <span class="math inline">\(\mathbf{X^{\prime}X}\)</span> be invertible, see Section 4.7.3). Thus, <span class="math inline">\(\mathbf{b}\)</span> is said to be an <em>unbiased estimator</em> of <span class="math inline">\(\boldsymbol \beta\)</span>. In particular, <span class="math inline">\(\mathrm{E}~b_j = \beta_j\)</span> for <span class="math inline">\(j = 0,1,\ldots,k\)</span>.</p>
<p>Because independence implies zero covariance, from Assumption F4 we have that <span class="math inline">\(\mathrm{Cov}(y_i,y_j) = 0\)</span> for <span class="math inline">\(i \neq j\)</span>. From this, Assumption F3 and the definition of the variance of a vector, we have that</p>
<p><span class="math display">\[
\small{
\mathrm{Var~}\mathbf{y} = \left(
\begin{array}{cccc}
\mathrm{Var~}y_1 &amp; \mathrm{Cov}(y_1,y_2) &amp; \cdots &amp; \mathrm{Cov}(y_1,y_n) \\
\mathrm{Cov}(y_2,y_1) &amp; \mathrm{Var~}y_2 &amp; \cdots &amp; \mathrm{Cov}(y_2,y_n) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathrm{Cov}(y_n,y_1) &amp; \mathrm{Cov}(y_n,y_2) &amp; \cdots &amp; \mathrm{Var~}y_n
\end{array}
\right) = \left(
\begin{array}{cccc}
\sigma^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma^2
\end{array}
\right) = \sigma^2 \mathbf{I},
}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{I}\)</span> is an <span class="math inline">\(n \times n\)</span> identity matrix. We are now in a position to state the second important property of least squares regression estimators.</p>
<div class="blackbox">
<p><strong>Property 2</strong>. Consider a regression model and let Assumptions F1-F4 hold. Then, the estimator <span class="math inline">\(\mathbf{b}\)</span> defined in equation <a href="C3BasicMLR.html#eq:eq34">(3.4)</a> has variance <span class="math inline">\(\mathrm{Var~}\mathbf{b} = \sigma^2(\mathbf{X^{\prime}X)}^{-1}\)</span>.</p>
</div>
<p>To establish Property 2, we have that</p>
<p><span class="math display">\[
\mathrm{Var~}\mathbf{b} = \mathrm{Var}\left((\mathbf{X^{\prime}X)}^{-1} \mathbf{X}^{\prime}\mathbf{y}\right) = \left[ (\mathbf{X^{\prime}X)}^{-1} \mathbf{X}^{\prime}\right] \mathrm{Var}\left( \mathbf{y}\right) \left[\mathbf{X}(\mathbf{X^{\prime}X)}^{-1}\right] \\
= \left[ (\mathbf{X^{\prime}X)}^{-1}\mathbf{X}^{\prime}\right] \sigma^2 \mathbf{I}\left[\mathbf{X}(\mathbf{X^{\prime}X)}^{-1}\right] = \sigma^2(\mathbf{X^{\prime}X)}^{-1},
\]</span></p>
<p>as required. This important property will allow us to measure the precision of the estimator <span class="math inline">\(\mathbf{b}\)</span> when we discuss statistical inference. Specifically, by the definition of the variance of a vector (see Section 2.11.4),</p>
<p><span class="math display" id="eq:eq36">\[
\mathrm{Var~}\mathbf{b}=
\begin{pmatrix}
\mathrm{Var~}b_0 &amp; \mathrm{Cov}(b_0,b_1) &amp; \cdots &amp; \mathrm{Cov}(b_0,b_k) \\
\mathrm{Cov}(b_1,b_0) &amp; \mathrm{Var~}b_1 &amp; \cdots &amp; \mathrm{Cov}(b_1,b_k) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathrm{Cov}(b_k,b_0) &amp; \mathrm{Cov}(b_k,b_1) &amp; \cdots &amp; \mathrm{Var~}b_k
\end{pmatrix}
= \sigma^2 (\mathbf{X^{\prime}X)}^{-1}.
\tag{3.6}
\]</span></p>
<p>Thus, for example, <span class="math inline">\(\mathrm{Var~}b_j\)</span> is <span class="math inline">\(\sigma^2\)</span> times the <span class="math inline">\((j+1)\)</span>st diagonal entry of <span class="math inline">\((\mathbf{X^{\prime}X)}^{-1}\)</span>. As another example, <span class="math inline">\(\mathrm{Cov}(b_0,b_j)\)</span> is <span class="math inline">\(\sigma^2\)</span> times the element in the first row and <span class="math inline">\((j+1)\)</span>st column of <span class="math inline">\((\mathbf{X^{\prime}X)}^{-1}\)</span>.</p>
<p>Although alternative methods are available that are preferable for specific applications, the least squares estimators have proven to be effective for many routine data analyses. One desirable characteristic of least squares regression estimators is summarized in the following well-known result.</p>
<div class="blackbox">
<p><strong>Gauss-Markov Theorem</strong>: Consider the regression model and let Assumptions F1-F4 hold. Then, within the class of estimators that are linear functions of the responses, the least squares estimator <span class="math inline">\(\mathbf{b}\)</span> defined in equation <a href="C3BasicMLR.html#eq:eq34">(3.4)</a> is the minimum variance unbiased estimator of the parameter vector <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
</div>
<p>The Gauss-Markov theorem states that the least squares estimator is the most precise in the sense that it has the smallest variance.</p>
<p>We have already seen in Property 1 that the least squares estimators are unbiased. The Gauss-Markov theorem states that the least squares estimator is the most precise in the sense that it has the smallest variance. (In a matrix context, “minimum variance” means that if <span class="math inline">\(\mathbf{b}^{\ast}\)</span> is any other estimator, then the difference of the variance matrices, <span class="math inline">\(\mathrm{Var~} \mathbf{b}^{\ast} - \mathrm{Var~}\mathbf{b}\)</span>, is nonnegative definite.)</p>
<p>An additional important property concerns the distribution of the least squares regression estimators.</p>
<div class="blackbox">
<p><strong>Property 3</strong>: Consider a regression model and let Assumptions F1-F5 hold. Then, the least squares estimator <span class="math inline">\(\mathbf{b}\)</span> defined in equation <a href="C3BasicMLR.html#eq:eq34">(3.4)</a> is normally distributed.</p>
</div>
<p>To establish Property 3, we define the weight vectors, <span class="math inline">\(\mathbf{w}_i = (\mathbf{X^{\prime}X)}^{-1}(1, x_{i1}, \ldots, x_{ik})^{\prime}\)</span>. With this notation, we note that</p>
<p><span class="math display">\[
\mathbf{b} = (\mathbf{X^{\prime}X)}^{-1}\mathbf{X}^{\prime}\mathbf{y} = \sum_{i=1}^{n} \mathbf{w}_i y_i,
\]</span></p>
<p>so that <span class="math inline">\(\mathbf{b}\)</span> is a linear combination of responses. With Assumption F5, the responses are normally distributed. Because linear combinations of normally distributed random variables are normally distributed, we have the conclusion of Property 3. This result underpins much of the statistical inference that will be presented in Sections 3.4 and 4.2.</p>
</div>
</div>
<div id="estimation-and-goodness-of-fit" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Estimation and Goodness of Fit<a href="C3BasicMLR.html#estimation-and-goodness-of-fit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="residual-standard-deviation" class="section level4 unnumbered hasAnchor">
<h4>Residual Standard Deviation<a href="C3BasicMLR.html#residual-standard-deviation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Additional properties of the regression coefficient estimators will be discussed when we focus on statistical inference. We now continue our estimation discussion by providing an estimator of the other parameter in the linear regression model, <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Our estimator for <span class="math inline">\(\sigma^2\)</span> can be developed using the principle of replacing theoretical expectations by sample averages. Examining <span class="math inline">\(\sigma^2 = \mathrm{E}\left( y-\mathrm{E~}y\right)^2\)</span>, replacing the outer expectation by a sample average suggests using the estimator <span class="math inline">\(n^{-1}\sum_{i=1}^{n}(y_i-\mathrm{E~}y_i)^2\)</span>. Because we do not observe <span class="math inline">\(\mathrm{E}~y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik}\)</span>, we use in its place the corresponding observed quantity <span class="math inline">\(b_0 + b_1 x_{i1} + \ldots + b_k x_{ik} = \widehat{y}_i\)</span>. This leads to the following.</p>
<div class="blackbox">
<p><em>Definition.</em> An estimator of <span class="math inline">\(\sigma^2\)</span>, the <em>mean square error (MSE)</em>, is defined as</p>
<p><span class="math display" id="eq:eq37">\[
s^2 = \frac{1}{n-(k+1)}\sum_{i=1}^{n}\left( y_i - \widehat{y}_i \right)^2.
\tag{3.7}
\]</span></p>
<p>The positive square root, <span class="math inline">\(s = \sqrt{s^2}\)</span>, is called the <em>residual standard deviation</em>.</p>
</div>
<p>This expression generalizes the definition in equation (2.3), which is valid for <span class="math inline">\(k=1\)</span>. It turns out, by using <span class="math inline">\(n-(k+1)\)</span> instead of <span class="math inline">\(n\)</span> in the denominator of equation <a href="C3BasicMLR.html#eq:eq37">(3.7)</a>, that <span class="math inline">\(s^2\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. Essentially, by using <span class="math inline">\(\widehat{y}_i\)</span> instead of <span class="math inline">\(\mathrm{E~}y_i\)</span> in the definition, we have introduced some small dependencies among the deviations from the responses <span class="math inline">\(y_i - \widehat{y}_i\)</span>, thus reducing the overall variability. To compensate for this lower variability, we also reduce the denominator in the definition of <span class="math inline">\(s^2\)</span>.</p>
<p>To provide further intuition on the choice of <span class="math inline">\(n-(k+1)\)</span> in the definition of <span class="math inline">\(s^2\)</span>, we introduced the concept of residuals in the context of multiple linear regression. From Assumption E1, recall that the random errors can be expressed as <span class="math inline">\(\varepsilon_i = y_i - (\beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik})\)</span>. Because the parameters <span class="math inline">\(\beta_0, \ldots, \beta_k\)</span> are not observed, the errors themselves are not observed. Instead, we examine the “estimated errors,” or <em>residuals</em>, defined by <span class="math inline">\(e_i = y_i - \widehat{y}_i\)</span>.</p>
<p>Unlike errors, there exist certain dependencies among the residuals. One dependency is due to the algebraic fact that the average residual is zero. Further, there must be at least <span class="math inline">\(k+2\)</span> observations for there to be variation in the fit of the plane. If we have only <span class="math inline">\(k+1\)</span> observations, we could fit a plane to the data perfectly, resulting in no variation in the fit. For example, if <span class="math inline">\(k=1\)</span>, because two observations determine a line, then at least three observations are required to observe any deviation from the line. Because of these dependencies, we have only <span class="math inline">\(n-(k+1)\)</span> free, or unrestricted, residuals to estimate the variability about the regression plane.</p>
<p>The positive square root of <span class="math inline">\(s^2\)</span> is our estimator of <span class="math inline">\(\sigma\)</span>. Using residuals, it can be expressed as</p>
<p><span class="math display" id="eq:eq38">\[
s = \sqrt{\frac{1}{n-(k+1)}\sum_{i=1}^{n}e_i^2}.
\tag{3.8}
\]</span></p>
<p>Because it is based on residuals, we refer to <span class="math inline">\(s\)</span> as the <em>residual standard deviation</em>. The quantity <span class="math inline">\(s\)</span> is a measure of our “typical error.” For this reason, <span class="math inline">\(s\)</span> is also called the <em>standard error of the estimate</em>.</p>
</div>
<div id="the-coefficient-of-determination-r2" class="section level4 unnumbered hasAnchor">
<h4>The Coefficient of Determination: <span class="math inline">\(R^2\)</span><a href="C3BasicMLR.html#the-coefficient-of-determination-r2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To summarize the goodness of fit of the model, as in Chapter 2 we partition the variability into pieces that are “explained” and “unexplained” by the regression fit. Algebraically, the calculations for regression using many variables are similar to the case of using only one variable. Unfortunately, when dealing with many variables, we do lose the easy graphical interpretation such as in Figure 2.4.</p>
<p>Begin with the total sum of squared deviations, <span class="math inline">\(Total~SS = \sum_{i=1}^{n}\left( y_i - \overline{y} \right)^2\)</span>, as our measure of the total variation in the data set. As in equation (2.1), we may then interpret the equation</p>
<p><span class="math display">\[
\small{
\begin{array}{ccccc}
\underbrace{y_i - \overline{y}} &amp; = &amp;
\underbrace{y_i - \widehat{y}_i} &amp; + &amp; \underbrace{\widehat{y}_i - \overline{y}} \\
\text{total} &amp; = &amp; \text{unexplained} &amp; + &amp; \text{explained} \\
\text{deviation} &amp; &amp; \text{deviation} &amp; &amp; \text{deviation}
\end{array}
}
\]</span></p>
<p>as the “deviation without knowledge of the explanatory variables equals the deviation not explained by the explanatory variables plus deviation explained by the explanatory variables.” Squaring each side and summing over all observations yields</p>
<p><span class="math display">\[
Total~SS = Error~SS + Regression~SS
\]</span></p>
<p>where <span class="math inline">\(Error~SS = \sum_{i=1}^{n}\left( y_i - \widehat{y}_i \right)^2\)</span> and <span class="math inline">\(Regression~SS = \sum_{i=1}^{n}\left( \widehat{y}_i - \overline{y} \right)^2\)</span>. As in Section 2.3 for the one explanatory variable case, the sum of the cross-product terms turns out to be zero.</p>
<p>A statistic that summarizes this relationship is the <em>coefficient of determination</em>,</p>
<p><span class="math display">\[
R^2 = \frac{Regression~SS}{Total~SS}.
\]</span></p>
<p>We interpret <span class="math inline">\(R^2\)</span> to be the proportion of variability explained by the regression function.</p>
<p>If the model is a desirable one for the data, one would expect a strong relationship between the observed responses and those “expected” under the model, the fitted values. An interesting algebraic fact is the following. If one squares the correlation coefficient between the responses and the fitted values, we get the coefficient of determination, that is,</p>
<p><span class="math display">\[
R^2 = \left[ r \left(y, \widehat{y} \right) \right]^2.
\]</span></p>
<p>As a result, <span class="math inline">\(R\)</span>, the positive square root of <span class="math inline">\(R^2\)</span>, is called the <em>multiple correlation coefficient</em>. It can be interpreted as the correlation between the response and the best linear combination of the explanatory variables, the fitted values. (This relationship is developed using matrix algebra in the technical supplement Section 5.10.1.)</p>
<p>The variability decomposition is also summarized using the <em>analysis of variance</em>, or <em>ANOVA</em>, table, as follows.</p>
<p><span class="math display">\[
\small{
\begin{array}{l|lcl}
\hline
\text{Source} &amp; \text{Sum of Squares} &amp; df &amp; \text{Mean Square} \\
\hline
\text{Regression} &amp; Regression~SS &amp; k &amp; Regression~MS \\
\text{Error} &amp; Error~SS &amp; n - (k + 1) &amp; MSE \\
\text{Total} &amp; Total~SS &amp; n - 1 &amp; \\
\hline
\end{array}
}
\]</span></p>
<p>The mean square column figures are defined to be the sum of squares figures divided by their respective degrees of freedom. The error degrees of freedom denotes the number of unrestricted residuals. It is this number that we use in our definition of the “average,” or mean, square error. That is, we define</p>
<p><span class="math display">\[
MSE = Error~MS = \frac{Error~SS}{n - (k + 1)} = s^2.
\]</span></p>
<p>Similarly, the regression degrees of freedom is the number of explanatory variables. This yields</p>
<p><span class="math display">\[
Regression~MS = \frac{Regression~SS}{k}.
\]</span></p>
<p>When discussing the coefficient of determination, it can be established that whenever an explanatory variable is added to the model, <span class="math inline">\(R^2\)</span> never decreases. This is true whether or not the additional variable is useful. We would like a measure of fit that decreases when useless variables are entered into the model as explanatory variables. To circumvent this anomaly, a widely used statistic is the <em>coefficient of determination adjusted for degrees of freedom</em>, defined by</p>
<p><span class="math display" id="eq:eq39">\[
R_{a}^2 = 1 - \frac{(Error~SS) / [n - (k + 1)]}{(Total~SS) / (n - 1)} = 1 - \frac{s^2}{s_{y}^2}.
\tag{3.9}
\]</span></p>
<p>To interpret this statistic, note that <span class="math inline">\(s_y^2\)</span> does not depend on the model nor the model variables. Thus, <span class="math inline">\(s^2\)</span> and <span class="math inline">\(R_a^2\)</span> are equivalent measures of model fit. As the model fit improves, then <span class="math inline">\(R_{a}^2\)</span> becomes larger and <span class="math inline">\(s^2\)</span> becomes smaller, and vice versa. Put another way, choosing a model with the smallest <span class="math inline">\(s^2\)</span> is equivalent to choosing a model with the largest <span class="math inline">\(R_a^2\)</span>.</p>
<hr />
<p><strong>Example: Term Life Insurance - Continued.</strong> To illustrate, Table <a href="C3BasicMLR.html#tab:ANOVATerm">3.3</a> displays the summary statistics for the regression of LNFACE on EDUCATION, NUMHH, and LNINCOME. From the degrees of freedom column, we remind ourselves that there are three explanatory variables and 275 observations. As measures of model fit, the coefficient of determination is <span class="math inline">\(R^2 = 34.3\%\)</span> (=<span class="math inline">\(328.47 / 958.90\)</span>) and the residual standard deviation is <span class="math inline">\(s = 1.525\)</span> (=<span class="math inline">\(\sqrt{2.326}\)</span>). If we were to attempt to estimate the logarithmic face amount without knowledge of the explanatory variables EDUCATION, NUMHH, and LNINCOME, then the size of the typical error would be <span class="math inline">\(s_y = 1.871\)</span> (=<span class="math inline">\(\sqrt{958.90 / 274}\)</span>). Thus, by taking advantage of our knowledge of the explanatory variables, we have been able to reduce the size of the typical error. The measure of model fit that compares these two estimates of variability is the adjusted coefficient of determination, <span class="math inline">\(R_a^2 = 1 - 2.326 / 1.871^2 = 33.6\%\)</span>.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:ANOVATerm">Table 3.3: </span><strong>Term Life ANOVA Table </strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Sum of Squares
</th>
<th style="text-align:right;">
<span class="math inline">\(df\)</span>
</th>
<th style="text-align:right;">
Mean Square
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Regression
</td>
<td style="text-align:right;width: 1.6cm; ">
328.47
</td>
<td style="text-align:right;width: 1.6cm; ">
3
</td>
<td style="text-align:right;width: 1.6cm; ">
109.49
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Error
</td>
<td style="text-align:right;width: 1.6cm; ">
630.43
</td>
<td style="text-align:right;width: 1.6cm; ">
271
</td>
<td style="text-align:right;width: 1.6cm; ">
2.326
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
Total
</td>
<td style="text-align:right;width: 1.6cm; ">
958.9
</td>
<td style="text-align:right;width: 1.6cm; ">
274
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
</tr>
</tbody>
</table>
<hr />
<p><strong>Example: Why do Females Live Longer than Males?</strong> In an article with this title, Lemaire (2002) examined what he called the “female advantage,” the difference in life expectancy between females and males. Life expectancies are of interest because they are widely used measures of a nation’s health. Lemaire examined data from <span class="math inline">\(n = 169\)</span> countries and found that the average female advantage was 4.51 years worldwide. He sought to explain this difference based on 45 behavioral measures, variables that capture a nation’s degree of economic modernization, social/cultural/religious mores, geographic position, and quality of health care available.</p>
<p>After a detailed analysis, Lemaire reports coefficients from a regression model that appear in <a href="C3BasicMLR.html#Table34">Table 3.4</a>. This regression model explains <span class="math inline">\(R^2 = 61\%\)</span> of the variability. It is a parsimonious model consisting of only <span class="math inline">\(k = 4\)</span> of the original 45 variables.</p>
<p><a id=Table34></a></p>
<p><span id="Table34">Table 3.4</span>. <strong>Regression Coefficients from a Model of Female Advantage</strong></p>
<p><span class="math display">\[
\small{
\begin{array}{l|rr}
\hline
\text{Variable} &amp; \text{Coefficient} &amp; t\text{-statistic} \\
\hline
\text{Intercept} &amp; 9.904 &amp; 12.928 \\
\text{Logarithmic Number of Persons per Physician} &amp; -0.473 &amp; -3.212 \\
\text{Fertility} &amp; -0.444 &amp; -3.477 \\
\text{Percentage of Hindus and Buddhists} &amp; -0.018 &amp; -3.196 \\
\text{Soviet Union Dummy} &amp; 4.922 &amp; 7.235 \\
\hline
\end{array}
}
\]</span></p>
<p><em>Source: Lemaire (2002)</em></p>
<p>All variables were strongly statistically significant. The number of persons per physician was also correlated with other variables that capture a country’s degree of economic modernization, such as urbanization, number of cars, and the percentage working in agriculture. Fertility, the number of births per woman, was highly correlated with education variables in the study, including female illiteracy and female school enrollment. The percentage of Hindus and Buddhists is a social/cultural/religious variable. The Soviet Union dummy is a geographic variable - it characterizes Eastern European countries that formerly belonged to the Soviet Union. Because of the high degree of collinearity among the 45 candidate variables, other analysts could easily pick an alternative set of variables. Nonetheless, Lemaire’s important point was that this simple model explains roughly 61% of the variability based on only behavioral variables, unrelated to biological sex differences.</p>
</div>
</div>
<div id="statistical-inference-for-a-single-coefficient" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Statistical Inference for a Single Coefficient<a href="C3BasicMLR.html#statistical-inference-for-a-single-coefficient" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-t-test" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> The <em>t</em>-Test<a href="C3BasicMLR.html#the-t-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In many applications, a single variable is of primary interest, and other variables are included in the regression to control for additional sources of variability. To illustrate, a sales agent might be interested in the effect that income has on the quantity of insurance demanded. In a regression analysis, one could also include other explanatory variables such as an individual’s gender, type of occupation, age, size of the household, education level, and so on. By including these additional explanatory variables, we hope to gain a better understanding of the relationship between income and insurance demand. To reach sensible conclusions, we will need some rules to decide whether a variable is important or not.</p>
<p>We respond to the question “Is <span class="math inline">\(x_j\)</span> important?” by investigating whether or not the corresponding slope parameter, <span class="math inline">\(\beta_j\)</span>, equals zero. The question is whether <span class="math inline">\(\beta_j\)</span> is zero can be restated in the hypothesis testing framework as “Is <span class="math inline">\(H_0:\beta_j=0\)</span> valid?”</p>
<p>We examine the proximity of <span class="math inline">\(b_j\)</span> to zero in order to determine whether or not <span class="math inline">\(\beta_j\)</span> is zero. Because the units of <span class="math inline">\(b_j\)</span> depend on the units of <span class="math inline">\(y\)</span> and <span class="math inline">\(x_j\)</span>, we need to standardize this quantity. From Property 2 and equation <a href="C3BasicMLR.html#eq:eq36">(3.6)</a>, we saw that <span class="math inline">\(\mathrm{Var~}b_j\)</span> is <span class="math inline">\(\sigma^2\)</span> times the <span class="math inline">\((j+1)^{st}\)</span> diagonal element of <span class="math inline">\((\mathbf{X^{\prime}X})^{-1}\)</span>. Replacing <span class="math inline">\(\sigma^2\)</span> by the estimator <span class="math inline">\(s^2\)</span> and taking square roots, we have the following.</p>
<div class="blackbox">
<p><strong>Definition</strong>. The standard error of <span class="math inline">\(b_j\)</span> can be expressed as</p>
<p><span class="math display">\[
se(b_j) = s \sqrt{\text{(j+1)st diagonal element of } (\mathbf{X^{\prime}X})^{-1}}.
\]</span></p>
</div>
<p>Recall that a standard error is an estimated standard deviation. To test <span class="math inline">\(H_0:\beta_j=0\)</span>, we examine the <span class="math inline">\(t\)</span>-ratio, <span class="math inline">\(t(b_j) = \frac{b_j}{se(b_j)}\)</span>. We interpret <span class="math inline">\(t(b_j)\)</span> to be the number of standard errors that <span class="math inline">\(b_j\)</span> is away from zero. This is the appropriate quantity because the sampling distribution of <span class="math inline">\(t(b_j)\)</span> can be shown to be the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df=n-(k+1)\)</span> degrees of freedom, under the null hypothesis with the linear regression model assumptions F1-F5. This enables us to construct tests of the null hypothesis such as the following procedure:</p>
<div class="blackbox">
<p><strong>Procedure</strong>. The <em>t</em>-test for a Regression Coefficient (<span class="math inline">\(\beta\)</span>).</p>
<ul>
<li>The null hypothesis is <span class="math inline">\(H_0:\beta_j=0\)</span>.</li>
<li>The alternative hypothesis is <span class="math inline">\(H_{a}:\beta_j \neq 0\)</span>.</li>
<li>Establish a significance level <span class="math inline">\(\alpha\)</span> (typically but not necessarily 5%).</li>
<li>Construct the statistic, <span class="math inline">\(t(b_j) = \frac{b_j}{se(b_j)}\)</span>.</li>
<li>Procedure: Reject the null hypothesis in favor of the alternative if <span class="math inline">\(|t(b_j)|\)</span> exceeds a <span class="math inline">\(t\)</span>-value. Here, this <span class="math inline">\(t\)</span>-value is the <span class="math inline">\((1-\alpha /2)^{th}\)</span> percentile from the <span class="math inline">\(t\)</span>-distribution using <span class="math inline">\(df=n-(k+1)\)</span> degrees of freedom, denoted as <span class="math inline">\(t_{n-(k+1),1-\alpha /2}\)</span>.</li>
</ul>
</div>
<p>In many applications, the sample size will be large enough so that we may approximate the <span class="math inline">\(t\)</span>-value by the corresponding percentile from the standard normal curve. At the 5% level of significance, this percentile is 1.96. Thus, as a rule of thumb, we can interpret a variable to be important if its <span class="math inline">\(t\)</span>-ratio exceeds two in absolute value.</p>
<p>Although it is the most common, testing <span class="math inline">\(H_0:\beta_j=0\)</span> versus <span class="math inline">\(H_{a}:\beta_j \neq 0\)</span> is just one of many hypothesis tests that can be performed. <a href="C3BasicMLR.html#Table35">Table 3.5</a> outlines alternative decision-making procedures. These procedures are for testing <span class="math inline">\(H_0:\beta_j = d\)</span>. Here, <span class="math inline">\(d\)</span> is a user-prescribed value that may be equal to zero or any other known value.</p>
<p><a id=Table35></a></p>
<p><span id="Table35">Table 3.5</span>. <strong>Decision-Making Procedures for Testing <span class="math inline">\(H_0: \beta_j = d\)</span></strong></p>
<p><span class="math display">\[
\small{
\begin{array}{cc}
\hline \text{Alternative Hypothesis }(H_{a}) &amp; \text{Procedure: Reject } H_0 \text{ in
favor of } H_a \text{ if }\\ \hline
\beta_j &gt; d &amp; t-\mathrm{ratio}&gt;t_{n-(k+1),1-\alpha } \\
\beta_j &lt; d &amp; t-\mathrm{ratio}&lt;-t_{n-(k+1),1-\alpha } \\
\beta_j\neq d  &amp; |t-\mathrm{ratio}\mathit{|}&gt;t_{n-(k+1),1-\alpha/2}
\end{array}
\\
\begin{array}{ll}\hline
\textit{Notes:} &amp;\text{ The significance level is } \alpha.
\text{ Here, } t_{n-(k+1),1-\alpha}\text{ is the }(1-\alpha)^{th}\text{ percentile} \\
&amp;~~\text{from the }t-\text{distribution using } df=n-(k+1)\text{ degrees of freedom.} \\
&amp;~~\text{The test statistic is }t-\mathrm{ratio} = (b_j -d)/se(b_j) . \\
\hline
\end{array}
}
\]</span></p>
<p>Alternatively, one can construct <span class="math inline">\(p\)</span>-values and compare these to given significant levels. The <span class="math inline">\(p\)</span>-value allows the report reader to understand the strength of the deviation from the null hypothesis. <a href="C3BasicMLR.html#Table36">Table 3.6</a> summarizes the procedure for calculating <span class="math inline">\(p\)</span>-values.</p>
<p><a id=Table36></a></p>
<p><span id="Table36">Table 3.6</span>. <strong>Probability Values for Testing <span class="math inline">\(H_0:\beta_j =d\)</span></strong></p>
<p><span class="math display">\[
\small{
\begin{array}{cccc}
\hline
\text{Alternative} &amp;  &amp;  &amp;  \\
\text{Hypothesis} (H_a ) &amp; \beta_j &gt; d &amp; \beta_j &lt; d &amp; \beta_j \neq d  \\
\hline p-value &amp; \Pr(t_{n-(k+1)}&gt;t-ratio) &amp;
\Pr(t_{n-(k+1)}&lt;t-ratio) &amp; \Pr(|t_{n-(k+1)}|&gt;|t-ratio|) \\
\end{array}
\\
\begin{array}{ll}\hline
\textit{Notes:} &amp;
\text{ Here, } t_{n-(k+1)}
\text{ is a }t\text{-distributed random variable with }df=n-(k+1)\text{ degrees of freedom.} \\
&amp;~~\text{The test statistic is }t-\mathrm{ratio} = (b_j -d)/se(b_j) . \\
\hline
\end{array}
}
\]</span></p>
<hr />
<p><strong>Example: Term Life Insurance - Continued.</strong> A useful convention when reporting the results of a statistical analysis is to place the standard error of a statistic in parenthesis below that statistic. Thus, for example, in our regression of LNFACE on EDUCATION, NUMHH, and LNINCOME, the estimated regression equation is:</p>
<p><span class="math display">\[
\begin{array}{lccccc}
\widehat{LNFACE} = &amp;2.584 ~ +  &amp;0.206~ \text{EDUCATION} +  &amp;0.306 ~\text{NUMHH} +  &amp;0.494 ~\text{LNINCOME}. \\
\text{std error}  &amp;(0.846)   &amp;(0.039)  &amp;(0.063)   &amp;(0.078).
\end{array}
\]</span></p>
<p>To illustrate the calculation of the standard errors, first note that from Table <a href="C3BasicMLR.html#tab:ANOVATerm">3.3</a> we have that the residual standard deviation is <span class="math inline">\(s=1.525\)</span>. Using a statistical package, we have</p>
<p><span class="math display">\[
\small{
(\mathbf{X^{\prime}X})^{-1} = \begin{pmatrix}
0.307975 &amp; -0.004633 &amp; -0.002131 &amp; -0.020697 \\
-0.004633 &amp; 0.000648 &amp; 0.000143 &amp; -0.000467 \\
-0.002131 &amp; 0.000143 &amp; 0.001724 &amp; -0.000453 \\
-0.020697 &amp; -0.000467 &amp; -0.000453 &amp; 0.002585
\end{pmatrix}.
}
\]</span></p>
<p>To illustrate, we can compute <span class="math inline">\(se(b_3)=s \times \sqrt{0.002585} = 0.078\)</span>, as above. Calculation of the standard errors, as well as the corresponding <span class="math inline">\(t\)</span>-statistics, is part of the standard output from statistical software and need not be computed by users. Our purpose here is to illustrate the ideas underlying the routine calculations.</p>
<p>With this information, we can immediately compute <span class="math inline">\(t\)</span>-ratios to check whether a coefficient associated with an individual variable is significantly different from zero. For example, the <span class="math inline">\(t\)</span>-ratio for the LNINCOME variable is <span class="math inline">\(t(b_3) = \frac{0.494}{0.078} = 6.3\)</span>. The interpretation is that <span class="math inline">\(b_3\)</span> is over four standard errors above zero and thus LNINCOME is an important variable in the model. More formally, we may be interested in testing the null hypothesis that <span class="math inline">\(H_0:\beta_3 = 0\)</span> versus <span class="math inline">\(H_0:\beta_3 \neq 0\)</span>. At a 5% level of significance, the <span class="math inline">\(t\)</span>-value is 1.96, because <span class="math inline">\(df=275-(1+3)=271\)</span>. We thus reject the null in favor of the alternative hypothesis, that logarithmic income (LNINCOME) is important in determining the logarithmic face amount.</p>
</div>
<div id="confidence-intervals-1" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Confidence Intervals<a href="C3BasicMLR.html#confidence-intervals-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Confidence intervals</em> for parameters represent another device for describing the strength of the contribution of the <span class="math inline">\(j\)</span>th explanatory variable. The statistic <span class="math inline">\(b_j\)</span> is called a <em>point estimate</em> of the parameter <span class="math inline">\(\beta_j\)</span>. To provide a range of reliability, we use the confidence interval:</p>
<p><span class="math display" id="eq:eq310">\[
b_j \pm t_{n-(k+1),1-\alpha /2}~se(b_j).
\tag{3.10}
\]</span></p>
<p>Here, the <span class="math inline">\(t\)</span>-value <span class="math inline">\(t_{n-(k+1),1-\alpha /2}\)</span> is a percentile from the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df=n-(k+1)\)</span> degrees of freedom. We use the same <span class="math inline">\(t\)</span>-value as in the two-sided hypothesis test. Indeed, there is a duality between the confidence interval and the two-sided hypothesis test. For example, it is not hard to check that if a hypothesized value falls outside the confidence interval, then <span class="math inline">\(H_0\)</span> will be rejected in favor of <span class="math inline">\(H_{a}\)</span>. Further, knowledge of the <span class="math inline">\(p\)</span>-value, point estimate, and standard error can be used to determine a confidence interval.</p>
</div>
<div id="added-variable-plots" class="section level3 hasAnchor" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Added Variable Plots<a href="C3BasicMLR.html#added-variable-plots" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To represent multivariate data graphically, we have seen that a scatterplot matrix is a useful device. However, the major shortcoming of the scatterplot matrix is that it only captures relationships between pairs of variables. When the data can be summarized using a regression model, a graphical device that does not have this shortcoming is an <em>added variable plot</em>. The added variable plot is also called a <em>partial regression plot</em> because, as we will see, it is constructed in terms of residuals from certain regression fits. We will also see that the added variable plot can be summarized in terms of a partial correlation coefficient, thus providing a link between correlation and regression. To introduce these ideas, we work in the context of the following example.</p>
<hr />
<p><strong>Example: Refrigerator Prices.</strong> What characteristics of a refrigerator are important in determining its price (PRICE)? We consider here several characteristics of a refrigerator, including the size of the refrigerator in cubic feet (RSIZE), the size of the freezer compartment in cubic feet (FSIZE), the average amount of money spent per year to operate the refrigerator (ECOST, for “energy cost”), the number of shelves in the refrigerator and freezer doors (SHELVES), and the number of features (FEATURES). The features variable includes shelves for cans, see-through crispers, ice makers, egg racks, and so on.</p>
<p>Both consumers and manufacturers are interested in models of refrigerator prices. Other things equal, consumers generally prefer larger refrigerators with lower energy costs that have more features. Due to forces of supply and demand, we would expect consumers to pay more for these refrigerators. A larger refrigerator with lower energy costs that has more features at a similar price is considered a bargain to the consumer. How much extra would the consumer be willing to pay for this additional space? A model of prices for refrigerators on the market provides some insight into this question.</p>
<p>To this end, we analyze data from <span class="math inline">\(n=37\)</span> refrigerators. Table <a href="C3BasicMLR.html#tab:RefrigSumStats">3.7</a> provides the basic summary statistics for the response variable PRICE and the five explanatory variables. From this table, we see that the average refrigerator price is <span class="math inline">\(\overline{y} = \$626.40\)</span>, with standard deviation <span class="math inline">\(s_{y} = \$139.80\)</span>. Similarly, the average annual amount to operate a refrigerator, or average ECOST, is $70.51.</p>
<h5 style="text-align: center;">
<a id="displayCode.Motivation.1Silly" href="javascript:togglecode('toggleCode.Motivation.1Silly','displayCode.Motivation.1Silly');"><i><strong></strong></i></a>
</h5>
<div id="toggleCode.Motivation.1Silly" style="display: none">
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="C3BasicMLR.html#cb34-1" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="dv">2</span>, <span class="at">caption =</span> <span class="st">&quot;Silly. Create a table just to update the counter...&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-45">Table 3.4: </span>Silly. Create a table just to update the counter…</caption>
<thead>
<tr class="header">
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="C3BasicMLR.html#cb35-1" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="dv">2</span>, <span class="at">caption =</span> <span class="st">&quot;Silly.&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-46">Table 3.5: </span>Silly.</caption>
<thead>
<tr class="header">
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="C3BasicMLR.html#cb36-1" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="dv">2</span>, <span class="at">caption =</span> <span class="st">&quot;Silly.&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-47">Table 3.6: </span>Silly.</caption>
<thead>
<tr class="header">
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
</tr>
</tbody>
</table>
</div>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:RefrigSumStats">Table 3.7: </span><strong>Summary Statistics for each variable for 37 Refrigerators</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Median
</th>
<th style="text-align:right;">
Standard Deviation
</th>
<th style="text-align:right;">
Minimum
</th>
<th style="text-align:right;">
Maximum
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
ECOST
</td>
<td style="text-align:right;width: 1.6cm; ">
70.514
</td>
<td style="text-align:right;width: 1.6cm; ">
68.0
</td>
<td style="text-align:right;width: 1.6cm; ">
9.140
</td>
<td style="text-align:right;width: 1.6cm; ">
60.0
</td>
<td style="text-align:right;width: 1.6cm; ">
94.0
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
RSIZE
</td>
<td style="text-align:right;width: 1.6cm; ">
13.400
</td>
<td style="text-align:right;width: 1.6cm; ">
13.2
</td>
<td style="text-align:right;width: 1.6cm; ">
0.600
</td>
<td style="text-align:right;width: 1.6cm; ">
12.6
</td>
<td style="text-align:right;width: 1.6cm; ">
14.7
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
FSIZE
</td>
<td style="text-align:right;width: 1.6cm; ">
5.184
</td>
<td style="text-align:right;width: 1.6cm; ">
5.1
</td>
<td style="text-align:right;width: 1.6cm; ">
0.938
</td>
<td style="text-align:right;width: 1.6cm; ">
4.1
</td>
<td style="text-align:right;width: 1.6cm; ">
7.4
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SHELVES
</td>
<td style="text-align:right;width: 1.6cm; ">
2.514
</td>
<td style="text-align:right;width: 1.6cm; ">
2.0
</td>
<td style="text-align:right;width: 1.6cm; ">
1.121
</td>
<td style="text-align:right;width: 1.6cm; ">
1.0
</td>
<td style="text-align:right;width: 1.6cm; ">
5.0
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
FEATURES
</td>
<td style="text-align:right;width: 1.6cm; ">
3.459
</td>
<td style="text-align:right;width: 1.6cm; ">
3.0
</td>
<td style="text-align:right;width: 1.6cm; ">
2.512
</td>
<td style="text-align:right;width: 1.6cm; ">
1.0
</td>
<td style="text-align:right;width: 1.6cm; ">
12.0
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
PRICE
</td>
<td style="text-align:right;width: 1.6cm; ">
626.351
</td>
<td style="text-align:right;width: 1.6cm; ">
590.0
</td>
<td style="text-align:right;width: 1.6cm; ">
139.790
</td>
<td style="text-align:right;width: 1.6cm; ">
460.0
</td>
<td style="text-align:right;width: 1.6cm; ">
1200.0
</td>
</tr>
</tbody>
</table>
<p>To analyze relationships among pairs of variables, Table <a href="C3BasicMLR.html#tab:RefrigCorr">3.8</a> provides a matrix of correlation coefficients. From the table, we see that there are strong linear relationships between PRICE and each of freezer space (FSIZE) and the number of FEATURES. Surprisingly, there is also a strong positive correlation between PRICE and ECOST. Recall that ECOST is the energy cost; one might expect that higher-priced refrigerators should enjoy lower energy costs.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:RefrigCorr">Table 3.8: </span><strong>Matrix of Correlation Coefficients</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
ECOST
</th>
<th style="text-align:right;">
RSIZE
</th>
<th style="text-align:right;">
FSIZE
</th>
<th style="text-align:right;">
SHELVES
</th>
<th style="text-align:right;">
FEATURES
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
RSIZE
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.033
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
FSIZE
</td>
<td style="text-align:right;width: 1.6cm; ">
0.855
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.235
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
SHELVES
</td>
<td style="text-align:right;width: 1.6cm; ">
0.188
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.363
</td>
<td style="text-align:right;width: 1.6cm; ">
0.251
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
FEATURES
</td>
<td style="text-align:right;width: 1.6cm; ">
0.334
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.096
</td>
<td style="text-align:right;width: 1.6cm; ">
0.439
</td>
<td style="text-align:right;width: 1.6cm; ">
0.16
</td>
<td style="text-align:right;width: 1.6cm; ">
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;">
PRICE
</td>
<td style="text-align:right;width: 1.6cm; ">
0.522
</td>
<td style="text-align:right;width: 1.6cm; ">
-0.024
</td>
<td style="text-align:right;width: 1.6cm; ">
0.72
</td>
<td style="text-align:right;width: 1.6cm; ">
0.4
</td>
<td style="text-align:right;width: 1.6cm; ">
0.697
</td>
</tr>
</tbody>
</table>
<h5 style="text-align: center;">
<a id="displayCode.Refrig.Hide" href="javascript:togglecode('toggleCode.Refrig.Hide','displayCode.Refrig.Hide');"><i><strong>R Code to Produce Tables 3.7 and 3.8</strong></i></a>
</h5>
<div id="toggleCode.Refrig.Hide" style="display: none">
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="C3BasicMLR.html#cb37-1" tabindex="-1"></a>Refrig <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;CSVData/Refrigerator.csv&quot;</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb37-2"><a href="C3BasicMLR.html#cb37-2" tabindex="-1"></a><span class="co">#  TABLE 3.7 SUMMARY STATISTICS</span></span>
<span id="cb37-3"><a href="C3BasicMLR.html#cb37-3" tabindex="-1"></a>varFrig <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;ECOST&quot;</span>, <span class="st">&quot;RSIZE&quot;</span>, <span class="st">&quot;FSIZE&quot;</span>, <span class="st">&quot;SHELVES&quot;</span>, <span class="st">&quot;FEATURES&quot;</span>, <span class="st">&quot;PRICE&quot;</span>)</span>
<span id="cb37-4"><a href="C3BasicMLR.html#cb37-4" tabindex="-1"></a>Refrig1 <span class="ot">&lt;-</span> Refrig[varFrig]</span>
<span id="cb37-5"><a href="C3BasicMLR.html#cb37-5" tabindex="-1"></a>tableMat  <span class="ot">&lt;-</span> <span class="fu">BookSummStats</span>(Refrig1)</span>
<span id="cb37-6"><a href="C3BasicMLR.html#cb37-6" tabindex="-1"></a></span>
<span id="cb37-7"><a href="C3BasicMLR.html#cb37-7" tabindex="-1"></a><span class="fu">colnames</span>(tableMat)  <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Mean&quot;</span> , <span class="st">&quot;Median&quot;</span> , <span class="st">&quot;Standard Deviation&quot;</span> , </span>
<span id="cb37-8"><a href="C3BasicMLR.html#cb37-8" tabindex="-1"></a>                         <span class="st">&quot;Minimum&quot;</span> , <span class="st">&quot;Maximum&quot;</span>)</span>
<span id="cb37-9"><a href="C3BasicMLR.html#cb37-9" tabindex="-1"></a><span class="fu">rownames</span>(tableMat)  <span class="ot">&lt;-</span> varFrig </span>
<span id="cb37-10"><a href="C3BasicMLR.html#cb37-10" tabindex="-1"></a></span>
<span id="cb37-11"><a href="C3BasicMLR.html#cb37-11" tabindex="-1"></a><span class="co"># tableMat1[1:2,] &lt;- format(round(tableMat[1:2,], digits=0), big.mark = &#39;,&#39;)</span></span>
<span id="cb37-12"><a href="C3BasicMLR.html#cb37-12" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>tableMat, </span>
<span id="cb37-13"><a href="C3BasicMLR.html#cb37-13" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Summary Statistics for each variable for 37 Refrigerators&#39;</span>, </span>
<span id="cb37-14"><a href="C3BasicMLR.html#cb37-14" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>, <span class="at">Digits=</span><span class="dv">3</span>, <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,</span>
<span id="cb37-15"><a href="C3BasicMLR.html#cb37-15" tabindex="-1"></a>         <span class="at">ColWidth =</span> ColWidth5) </span></code></pre></div>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="C3BasicMLR.html#cb38-1" tabindex="-1"></a>tableCor <span class="ot">&lt;-</span> <span class="fu">cor</span>(Refrig1)</span>
<span id="cb38-2"><a href="C3BasicMLR.html#cb38-2" tabindex="-1"></a>tableCor <span class="ot">&lt;-</span> <span class="fu">round</span>(tableCor, <span class="at">digits =</span> <span class="dv">3</span>)</span>
<span id="cb38-3"><a href="C3BasicMLR.html#cb38-3" tabindex="-1"></a>tableCor[<span class="fu">upper.tri</span>(tableCor, <span class="at">diag =</span> <span class="cn">TRUE</span>)] <span class="ot">&lt;-</span> <span class="st">&quot;&quot;</span></span>
<span id="cb38-4"><a href="C3BasicMLR.html#cb38-4" tabindex="-1"></a>tablePrint <span class="ot">&lt;-</span> tableCor[<span class="sc">-</span><span class="dv">1</span>,]</span>
<span id="cb38-5"><a href="C3BasicMLR.html#cb38-5" tabindex="-1"></a>tablePrint <span class="ot">&lt;-</span> tablePrint[,<span class="sc">-</span><span class="dv">6</span>]</span>
<span id="cb38-6"><a href="C3BasicMLR.html#cb38-6" tabindex="-1"></a><span class="fu">TableGen1</span>(<span class="at">TableData=</span>tablePrint, </span>
<span id="cb38-7"><a href="C3BasicMLR.html#cb38-7" tabindex="-1"></a>         <span class="at">TextTitle=</span><span class="st">&#39;Matrix of Correlation Coefficients&#39;</span>, </span>
<span id="cb38-8"><a href="C3BasicMLR.html#cb38-8" tabindex="-1"></a>         <span class="at">Align=</span><span class="st">&#39;r&#39;</span>, <span class="at">Digits=</span><span class="dv">3</span>, <span class="at">ColumnSpec=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,</span>
<span id="cb38-9"><a href="C3BasicMLR.html#cb38-9" tabindex="-1"></a>         <span class="at">ColWidth =</span> ColWidth5)</span></code></pre></div>
</div>
<p>A regression model was fit to the data. The fitted regression equation appears in Table <a href="C3BasicMLR.html#tab:RefrigFittedModel">3.9</a>, with <span class="math inline">\(s=60.65\)</span> and <span class="math inline">\(R^2=83.8\%\)</span>.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:RefrigFittedModel">Table 3.9: </span><strong>Fitted Refrigerator Price Model</strong>
</caption>
<thead>
<tr>
<th style="text-align:right;">
</th>
<th style="text-align:right;">
Coefficient
</th>
<th style="text-align:right;">
Standard Error
</th>
<th style="text-align:right;">
<span class="math inline">\(t\)</span>-Ratio
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;width: 2.5cm; border-right:1px solid;">
Intercept
</td>
<td style="text-align:right;width: 1.6cm; ">
798.00
</td>
<td style="text-align:right;width: 1.6cm; ">
271.400
</td>
<td style="text-align:right;width: 1.6cm; ">
-2.9
</td>
</tr>
<tr>
<td style="text-align:right;width: 2.5cm; border-right:1px solid;">
ECOST
</td>
<td style="text-align:right;width: 1.6cm; ">
-6.96
</td>
<td style="text-align:right;width: 1.6cm; ">
2.275
</td>
<td style="text-align:right;width: 1.6cm; ">
-3.1
</td>
</tr>
<tr>
<td style="text-align:right;width: 2.5cm; border-right:1px solid;">
RSIZE
</td>
<td style="text-align:right;width: 1.6cm; ">
76.50
</td>
<td style="text-align:right;width: 1.6cm; ">
19.440
</td>
<td style="text-align:right;width: 1.6cm; ">
3.9
</td>
</tr>
<tr>
<td style="text-align:right;width: 2.5cm; border-right:1px solid;">
FSIZE
</td>
<td style="text-align:right;width: 1.6cm; ">
137.00
</td>
<td style="text-align:right;width: 1.6cm; ">
23.760
</td>
<td style="text-align:right;width: 1.6cm; ">
5.8
</td>
</tr>
<tr>
<td style="text-align:right;width: 2.5cm; border-right:1px solid;">
SHELVES
</td>
<td style="text-align:right;width: 1.6cm; ">
37.90
</td>
<td style="text-align:right;width: 1.6cm; ">
9.886
</td>
<td style="text-align:right;width: 1.6cm; ">
3.8
</td>
</tr>
<tr>
<td style="text-align:right;width: 2.5cm; border-right:1px solid;">
FEATURES
</td>
<td style="text-align:right;width: 1.6cm; ">
23.80
</td>
<td style="text-align:right;width: 1.6cm; ">
4.512
</td>
<td style="text-align:right;width: 1.6cm; ">
5.3
</td>
</tr>
</tbody>
</table>
<p>From Table <a href="C3BasicMLR.html#tab:RefrigFittedModel">3.9</a>, the explanatory variables seem to be useful predictors of refrigerator prices. Together, these variables account for 83.8% of the variability. For understanding prices, the typical error has dropped from <span class="math inline">\(s_{y} = \$139.80\)</span> to <span class="math inline">\(s = \$60.65\)</span>. The <span class="math inline">\(t\)</span>-ratios for each of the explanatory variables exceed two in absolute value, indicating that each variable is important on an individual basis.</p>
<p>What is surprising about the regression fit is the negative coefficient associated with energy cost. Remember, we can interpret <span class="math inline">\(b_{ECOST} = -6.96\)</span> to mean that, for each dollar increase in ECOST, we expect the PRICE to decrease by $6.96. This negative relationship conforms to our economic intuition. However, it is surprising that the same data set has shown us that there is a positive relationship between PRICE and ECOST. This seeming anomaly is because correlation only measures relationships between pairs of variables although the regression fit can account for several variables simultaneously. To provide more insight into this seeming anomaly, we now introduce the <em>added variable plot</em>.</p>
<hr />
<div id="producing-an-added-variable-plot" class="section level4 unnumbered hasAnchor">
<h4>Producing an Added Variable Plot<a href="C3BasicMLR.html#producing-an-added-variable-plot" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The added variable plot provides additional links between the regression methodology and more fundamental tools such as scatter plots and correlations. We work in the context of the Refrigerator Price Example to demonstrate the construction of this plot.</p>
<div class="blackbox">
<p><em>Procedure for producing an added variable plot.</em></p>
<ol style="list-style-type: decimal">
<li><p>Run a regression of PRICE on RSIZE, FSIZE, SHELVES, and FEATURES, omitting ECOST. Compute the residuals from this regression, which we label <span class="math inline">\(e_1\)</span>.</p></li>
<li><p>Run a regression of ECOST on RSIZE, FSIZE, SHELVES, and FEATURES. Compute the residuals from this regression, which we label <span class="math inline">\(e_2\)</span>.</p></li>
<li><p>Plot <span class="math inline">\(e_1\)</span> versus <span class="math inline">\(e_2\)</span>. This is the added variable plot of PRICE versus ECOST, controlling for the effects of the RSIZE, FSIZE, SHELVES, and FEATURES. This plot appears in Figure <a href="C3BasicMLR.html#fig:RefrigAddedVarPlot">3.4</a>.</p></li>
</ol>
</div>
<p><br></p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RefrigAddedVarPlot"></span>
<img src="RegressionMarkdown_files/figure-html/RefrigAddedVarPlot-1.png" alt="An added variable plot. The residuals from the regression of PRICE on the explanatory variables, omitting ECOST, are on the horizontal axis. On the vertical axis are the residuals from the regression fit of ECOST on the other explanatory variables. The correlation coefficient is -0.48." width="60%" />
<p class="caption">
Figure 3.4: <strong>An added variable plot.</strong> The residuals from the regression of PRICE on the explanatory variables, omitting ECOST, are on the horizontal axis. On the vertical axis are the residuals from the regression fit of ECOST on the other explanatory variables. The correlation coefficient is -0.48.
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Refrig2.Hide" href="javascript:togglecode('toggleCode.Refrig2.Hide','displayCode.Refrig2.Hide');"><i><strong>R Code to Produce Figure 3.4</strong></i></a>
</h5>
<div id="toggleCode.Refrig2.Hide" style="display: none">
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="C3BasicMLR.html#cb39-1" tabindex="-1"></a>model.refrig1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(PRICE <span class="sc">~</span> RSIZE <span class="sc">+</span> FSIZE <span class="sc">+</span> SHELVES <span class="sc">+</span> FEATURES, <span class="at">data =</span> Refrig1)</span>
<span id="cb39-2"><a href="C3BasicMLR.html#cb39-2" tabindex="-1"></a><span class="co">#summary(model.refrig1)</span></span>
<span id="cb39-3"><a href="C3BasicMLR.html#cb39-3" tabindex="-1"></a>model.refrig2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(ECOST <span class="sc">~</span> RSIZE <span class="sc">+</span> FSIZE <span class="sc">+</span> SHELVES <span class="sc">+</span> FEATURES, <span class="at">data =</span> Refrig1)</span>
<span id="cb39-4"><a href="C3BasicMLR.html#cb39-4" tabindex="-1"></a><span class="co">#summary(model.refrig2)</span></span>
<span id="cb39-5"><a href="C3BasicMLR.html#cb39-5" tabindex="-1"></a></span>
<span id="cb39-6"><a href="C3BasicMLR.html#cb39-6" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">4.1</span>,<span class="dv">4</span>,.<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">cex=</span><span class="fl">1.1</span>)</span>
<span id="cb39-7"><a href="C3BasicMLR.html#cb39-7" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">residuals</span>(model.refrig2),<span class="fu">residuals</span>(model.refrig1),</span>
<span id="cb39-8"><a href="C3BasicMLR.html#cb39-8" tabindex="-1"></a>     <span class="at">xlab=</span><span class="fu">expression</span>(e[<span class="dv">1</span>]),<span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">las=</span><span class="dv">1</span>)</span>
<span id="cb39-9"><a href="C3BasicMLR.html#cb39-9" tabindex="-1"></a><span class="fu">mtext</span>(<span class="fu">expression</span>(e[<span class="dv">2</span>]), <span class="at">side=</span><span class="dv">2</span>, <span class="at">at=</span><span class="dv">0</span>, <span class="at">line=</span><span class="dv">3</span>, <span class="at">las=</span><span class="dv">1</span>, <span class="at">cex=</span><span class="fl">1.1</span>)</span></code></pre></div>
</div>
<p>The error <span class="math inline">\(\varepsilon\)</span> can be interpreted as the natural variation in a sample. In many situations, this natural variation is small compared to the patterns evident in the nonrandom regression component. Thus, it is useful to think of the error, <span class="math inline">\(\varepsilon_i = y_i - \left( \beta_0 + \beta_1 x_{i1} + \ldots + \beta_k x_{ik} \right)\)</span>, as the response after controlling for the effects of the explanatory variables. In Section 3.3, we saw that a random error can be approximated by a residual, <span class="math inline">\(e_i = y_i - \left( b_0 + b_1 x_{i1} + \cdots + b_k x_{ik} \right)\)</span>. Thus, in the same way, we may think of a residual as the response after “controlling for” the effects of the explanatory variables.</p>
<p>With this in mind, we can interpret the vertical axis of Figure <a href="C3BasicMLR.html#fig:RefrigAddedVarPlot">3.4</a> as the refrigerator PRICE controlled for effects of RSIZE, FSIZE, SHELVES, and FEATURES. Similarly, we can interpret the horizontal axis as the ECOST controlled for effects of RSIZE, FSIZE, SHELVES, and FEATURES. The plot then provides a graphical representation of the relation between PRICE and ECOST, after controlling for the other explanatory variables. For comparison, a scatter plot of PRICE and ECOST (not shown here) does not control for other explanatory variables. Thus, it is possible that the positive relationship between PRICE and ECOST is not due to a causal relationship but rather one or more additional variables that cause both variables to be large.</p>
<p>For example, from Table <a href="C3BasicMLR.html#tab:RefrigCorr">3.8</a>, we see that the freezer size (FSIZE) is positively correlated with both ECOST and PRICE. It certainly seems reasonable that increasing the size of a freezer would cause both the energy cost and the price to increase. Rather, the positive correlation may be due to the fact that large values of FSIZE mean large values of both ECOST and PRICE.</p>
<p>Variables left out of a regression are called <em>omitted variables</em>. This omission could cause a serious problem in a regression model fit; regression coefficients could be not only strongly significant when they should not be, but they may also be of the incorrect sign. Selecting the proper set of variables to be included in the regression model is an important task; it is the subject of Chapters 5 and 6.</p>
</div>
</div>
<div id="partial-correlation-coefficients" class="section level3 hasAnchor" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Partial Correlation Coefficients<a href="C3BasicMLR.html#partial-correlation-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we saw in Chapter 2, a correlation statistic is a useful quantity for summarizing plots. The correlation for the added variable plot is called a <em>partial correlation coefficient</em>. It is defined to be the correlation between the residuals <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> and is denoted by <span class="math inline">\(r(y,x_j | x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_k)\)</span>. Because it summarizes an added variable plot, we may interpret <span class="math inline">\(r(y,x_j | x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_k)\)</span> to be the correlation between <span class="math inline">\(y\)</span> and <span class="math inline">\(x_j\)</span>, in the presence of the other explanatory variables. To illustrate, the correlation between PRICE and ECOST in the presence of the other explanatory variables is -0.48.</p>
<p>The partial correlation coefficient can also be calculated using</p>
<p><span class="math display" id="eq:eq311">\[
r(y,x_j | x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_k) = \frac{t(b_j)}{\sqrt{t(b_j)^2 + n - (k + 1)}}.
\tag{3.11}
\]</span></p>
<p>Here, <span class="math inline">\(t(b_j)\)</span> is the <span class="math inline">\(t\)</span>-ratio for <span class="math inline">\(b_j\)</span> from a regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x_1, \ldots, x_k\)</span> (including the variable <span class="math inline">\(x_j\)</span>). An important aspect of this equation is that it allows us to calculate partial correlation coefficients running only one regression. For example, from Table <a href="C3BasicMLR.html#tab:RefrigFittedModel">3.9</a>, the partial correlation between PRICE and ECOST in the presence of the other explanatory variables is <span class="math inline">\(\frac{-3.1}{\sqrt{(-3.1)^2 + 37 - (5 + 1)}} \approx -0.48\)</span>.</p>
<p>Calculation of partial correlation coefficients is quicker when using the relationship with the <span class="math inline">\(t\)</span>-ratio, but may fail to detect nonlinear relationships. The information in Table <a href="C3BasicMLR.html#tab:RefrigFittedModel">3.9</a> allows us to calculate all five partial correlation coefficients in the Refrigerator Price Example after running only one regression. The three-step procedure for producing added variable plots requires ten regressions, two for each of the five explanatory variables. Of course, by producing added variable plots, we can detect nonlinear relationships that are missed by correlation coefficients.</p>
<p>Partial correlation coefficients provide another interpretation for <span class="math inline">\(t\)</span>-ratios. The equation shows how to calculate a correlation statistic from a <span class="math inline">\(t\)</span>-ratio, thus providing another link between correlation and regression analysis. Moreover, from the equation we see that the larger the <span class="math inline">\(t\)</span>-ratio, the larger the partial correlation coefficient. That is, a large <span class="math inline">\(t\)</span>-ratio means that there is a large correlation between the response and the explanatory variable, controlling for other explanatory variables. This provides a partial response to the question that is regularly asked by consumers of regression analyses, “Which variable is most important?”</p>
</div>
</div>
<div id="some-special-explanatory-variables" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Some Special Explanatory Variables<a href="C3BasicMLR.html#some-special-explanatory-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The linear regression model is the basis of a rich family of models. This section provides several examples to illustrate the richness of this family. These examples demonstrate the use of (i) binary variables, (ii) transformation of explanatory variables, and (iii) interaction terms. This section also serves to underscore the meaning of the adjective <em>linear</em> in the phrase “linear regression”; the regression function is linear in the parameters but may be a highly nonlinear function of the explanatory variables.</p>
<div id="binary-variables" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Binary Variables<a href="C3BasicMLR.html#binary-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Categorical variables provide a numerical label for measurements of observations that fall in distinct groups, or <em>categories</em>. Because of the grouping, categorical variables are discrete and generally take on a finite number of values. We begin our discussion with a categorical variable that can take on one of only two values, a <em>binary</em> variable. Further discussion of categorical variables is the topic of Chapter 4.</p>
<hr />
<p><strong>Example: Term Life Insurance - Continued.</strong> We now consider the marital status of the survey respondent. In the Survey of Consumer Finances, respondents can select among several options describing their marital status including “married,” “living with a partner,” “divorced,” and so on. Marital status is not measured continuously but rather takes on values that fall into distinct groups. In this chapter, we group survey respondents according to whether or not they are single, defined to include those who are separated, divorced, widowed, never married, and are not married nor living with a partner. Chapter 4 will present a more complete analysis of marital status by including additional categories.</p>
<p>The binary variable SINGLE is defined to be one if the survey respondent is single and 0 otherwise. The variable SINGLE is also known as an <em>indicator</em> variable because it indicates whether or not the respondent is single. Another name for this important type of variable is a <em>dummy</em> variable. We could use 0 and 100, or 20 and 36, or any other distinct values. However, 0 and 1 are convenient for the interpretation of the parameter values, discussed below. To streamline the discussion, we now present a model using only LNINCOME and SINGLE as explanatory variables.</p>
<p>For our sample of <span class="math inline">\(n = 275\)</span> households, 57 are single and the other 218 are not. To see the relationships among LNFACE, LNINCOME, and SINGLE, Figure <a href="C3BasicMLR.html#fig:LinesLetterPlot">3.5</a> introduces a <em>letter plot</em> of LNFACE versus LNINCOME, with SINGLE as the code variable. We can see that Figure <a href="C3BasicMLR.html#fig:LinesLetterPlot">3.5</a> is a scatter plot of LNFACE versus LNINCOME, using 50 randomly selected households from our sample of 275 (for clarity of the graph). However, instead of using the same plotting symbol for each observation, we have coded the symbols so that we can easily understand the behavior of a third variable, SINGLE. In other applications, you may elect to use other plotting symbols such as <span class="math inline">\(\clubsuit\)</span>, <span class="math inline">\(\heartsuit\)</span>, <span class="math inline">\(\spadesuit\)</span>, and so on, or use different colors, to encode additional information. For this application, the letter codes “S” for single and “o” for other were selected because they remind the reader of the nature of the coding scheme. Regardless of the coding scheme, the important point is that a letter plot is a useful device for graphically portraying three or more variables in two dimensions. The main restriction is that the additional information must be categorized, such as with binary variables, to make the coding scheme work.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:LinesLetterPlot"></span>
<img src="RegressionMarkdown_files/figure-html/LinesLetterPlot-1.png" alt="Letter plot of LNFACE versus LNINCOME, with the letter code ‘S’ for single and ‘o’ for other. The fitted regression lines have been superimposed. The lower line is for single and the upper line is for other." width="100%" />
<p class="caption">
Figure 3.5: <strong>Letter plot of LNFACE versus LNINCOME, with the letter code ‘S’ for single and ‘o’ for other.</strong> The fitted regression lines have been superimposed. The lower line is for single and the upper line is for other.
</p>
</div>
<p>Figure <a href="C3BasicMLR.html#fig:LinesLetterPlot">3.5</a> suggests that LNFACE is lower for those single than others for a given level of income. Thus, we now consider a regression model, <code>LNFACE = β_0 + β_1 LNINCOME + β_2 SINGLE + ϵ</code>. The regression function can be written as:</p>
<p><span class="math display">\[
\text{E } y =
\begin{cases}
    \beta_0 + \beta_1 \text{LNINCOME} &amp; \text{for other respondents} \\
    \beta_0 + \beta_2 + \beta_1 \text{LNINCOME} &amp; \text{for single respondents}
\end{cases}
\]</span></p>
<p>The interpretation of the model coefficients differs from the continuous variable case. For continuous variables such as LNINCOME, we interpret <span class="math inline">\(\beta_1\)</span> as the expected change in <span class="math inline">\(y\)</span> per unit change of logarithmic income, holding other variables fixed. For binary variables such as SINGLE, we interpret <span class="math inline">\(\beta_2\)</span> as the expected increase in <span class="math inline">\(y\)</span> when going from the base level of SINGLE (=0) to the alternative level. Thus, although we have one model for both marital statuses, we can interpret the model using two regression equations, one for each type of marital status. By writing a separate equation for each marital status, we have been able to simplify a complicated multiple regression equation. Sometimes, you will find it easier to communicate a series of simple relationships compared to a single, complex relationship.</p>
<p>Although the interpretation for binary explanatory variables differs from the continuous, the ordinary least squares estimation method remains valid. To illustrate, the fitted version of the above model is</p>
<p><span class="math display">\[
\small{
\begin{array}{cclll}
  \widehat{LNFACE} &amp; = &amp; 5.09   &amp;  + 0.634 \text{LNINCOME} &amp; - 0.800 \text{SINGLE} .\\
  \text{std error}    &amp;   &amp; (0.89) &amp; ~~(0.078) &amp; ~(0.248) \\
\end{array}
}
\]</span></p>
<p>To interpret <span class="math inline">\(b_2 = -0.800\)</span>, we say that we expect the logarithmic face to be smaller by 0.80 for a survey respondent who is single compared to the other category. This assumes that other things, such as income, remain unchanged. For a graphical interpretation, the two fitted regression lines are superimposed in Figure <a href="C3BasicMLR.html#fig:LinesLetterPlot">3.5</a>.</p>
<hr />
</div>
<div id="transforming-explanatory-variables" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Transforming Explanatory Variables<a href="C3BasicMLR.html#transforming-explanatory-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Regression models have the ability to represent complex, <em>nonlinear</em> relationships between the expected response and the explanatory variables. For example, early regression texts, such as Plackett (1960, Chapter 6), devote an entire chapter of material to polynomial regression,</p>
<p><span class="math display" id="eq:eq312">\[
\text{E } y  =  \beta_0 + \beta_1 x + \beta_2 x^2 + \ldots + \beta_p x^p.
\tag{3.12}
\]</span></p>
<p>Here, the idea is that a <span class="math inline">\(p\)</span>th order polynomial in <span class="math inline">\(x\)</span> can be used to approximate general, unknown nonlinear functions of <span class="math inline">\(x\)</span>.</p>
<p>The modern-day treatment of polynomial regression does not require an entire chapter because the model in equation <a href="C3BasicMLR.html#eq:eq312">(3.12)</a> can be expressed as a special case of the linear regression model. That is, with the regression function in equation <a href="C3BasicMLR.html#eq:eq35">(3.5)</a>, <span class="math inline">\(\text{E } y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k\)</span>, we can choose <span class="math inline">\(k = p\)</span> and <span class="math inline">\(x_1 =x\)</span>, <span class="math inline">\(x_2 = x^2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(x_p = x^p\)</span>. Thus, with these choices of explanatory variables, we can model a highly nonlinear function of <span class="math inline">\(x\)</span>.</p>
<p>We are not restricted to powers of <span class="math inline">\(x\)</span> in our choice of transformations. For example, the model <span class="math inline">\(\text{E } y = \beta_0 + \beta_1 \ln x\)</span>, provides another way to represent a gently sloping curve in <span class="math inline">\(x\)</span>. This model can be written as a special case of the basic linear regression model using <span class="math inline">\(x^{\ast} = \ln x\)</span> as the transformed version of <span class="math inline">\(x\)</span>.</p>
<p>Transformations of explanatory variables need not be smooth functions. To illustrate, in some applications, it is useful to categorize a continuous explanatory variable. For example, suppose that <span class="math inline">\(x\)</span> represents the number of years of education, ranging from 0 to 17. If we are relying on information self-reported by our sample of senior citizens, there may be a substantial amount of error in the measurement of <span class="math inline">\(x\)</span>. We could elect to use a less informative, but more reliable, transform of <span class="math inline">\(x\)</span> such as <span class="math inline">\(x^{\ast}\)</span>, a binary variable for finishing 13 years of school (finishing high school). Formally, we would code <span class="math inline">\(x^{\ast} = 1\)</span> if <span class="math inline">\(x \geq 13\)</span> and <span class="math inline">\(x^{\ast} = 0\)</span> if <span class="math inline">\(x &lt; 13\)</span>.</p>
<p>Thus, there are several ways that nonlinear functions of the explanatory variables can be used in the regression model. An example of a nonlinear regression model is <span class="math inline">\(y = \beta_0 + \exp (\beta_1 x) + \varepsilon.\)</span> These typically arise in science applications of regressions where there are fundamental scientific principles guiding the complex model development.</p>
</div>
<div id="interaction-terms" class="section level3 hasAnchor" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Interaction Terms<a href="C3BasicMLR.html#interaction-terms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have so far discussed how explanatory variables, say <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, affect the mean response in an additive fashion, that is, <span class="math inline">\(\mathrm{E}~y = \beta_0 + \beta_1 x_1 + \beta_2 x_2\)</span>. Here, we expect <span class="math inline">\(y\)</span> to increase by <span class="math inline">\(\beta_1\)</span> per unit increase in <span class="math inline">\(x_1\)</span>, with <span class="math inline">\(x_2\)</span> held fixed. What if the marginal rate of increase of <span class="math inline">\(\mathrm{E}~y\)</span> differs for high values of <span class="math inline">\(x_2\)</span> when compared to low values of <span class="math inline">\(x_2\)</span>? One way to represent this is to create an <em>interaction variable</em> <span class="math inline">\(x_3 = x_1 \times x_2\)</span> and consider the model <span class="math inline">\(\mathrm{E}~y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3\)</span>.</p>
<p>With this model, the change in the expected <span class="math inline">\(y\)</span> per unit change in <span class="math inline">\(x_1\)</span> now depends on <span class="math inline">\(x_2\)</span>. Formally, we can assess small changes in the regression function as:</p>
<p><span class="math display">\[
\frac{\partial \mathrm{E}~y}{\partial x_1} = \frac{\partial}{\partial x_1} \left(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 \right) = \beta_1 + \beta_3 x_2 .
\]</span></p>
<p>In this way, we may allow for more complicated functions of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. Figure <a href="C3BasicMLR.html#fig:interaction">3.6</a> illustrates this complex structure. From this figure and the above calculations, we see that the partial changes of <span class="math inline">\(\mathrm{E}~y\)</span> due to movement of <span class="math inline">\(x_1\)</span> depend on the value of <span class="math inline">\(x_2\)</span>. In this way, we say that the partial changes due to each variable are not unrelated but rather “move together.”</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:interaction"></span>
<img src="RegressionMarkdown_files/figure-html/interaction-1.png" alt="Plot of \(\mathrm{E}~y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2\) versus \(x_1\) and \(x_2\)." width="60%" />
<p class="caption">
Figure 3.6: <strong>Plot of <span class="math inline">\(\mathrm{E}~y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2\)</span> versus <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>.</strong>
</p>
</div>
<p>More generally, an interaction term is a variable that is created as a nonlinear function of two or more explanatory variables. These special terms, even though permitting us to explore a rich family of nonlinear functions, can be cast as special cases of the linear regression model. To do this, we simply create the variable of interest and treat this new term as another explanatory variable. Of course, not every variable that we create will be useful. In some instances, the created variable will be so similar to variables already in our model that it will provide us with no new information. Fortunately, we can use <span class="math inline">\(t\)</span>-tests to check whether the new variable is useful. Further, Chapter 4 will introduce a test to decide whether a group of variables is useful.</p>
<p>The function that we use to create an interaction variable must be more than just a linear combination of other explanatory variables. For example, if we use <span class="math inline">\(x_3 = x_1 + x_2\)</span>, we will not be able to estimate all of the parameters. Chapter 5 will introduce some techniques to help avoid situations when one variable is a linear combination of the others.</p>
<p>To give you some exposure to the wide variety of potential applications of special explanatory variables, we now present a series of short examples.</p>
<hr />
<p><strong>Example: Term Life Insurance - Continued.</strong> How do we interpret the interaction of a binary variable with a continuous variable? To illustrate, consider a Term Life regression model,</p>
<p><span class="math display">\[
\mathrm{LNFACE} = \beta_0 + \beta_1 \mathrm{LNINCOME} + \beta_2 \mathrm{SINGLE} + \beta_3 \mathrm{LNINCOME*SINGLE} + \varepsilon .
\]</span></p>
<p>In this model, we have created a third explanatory variable through the interaction of LNINCOME and SINGLE. The regression function can be written as:</p>
<p><span class="math display">\[
\mathrm{E}~y = \begin{cases}
        \beta_0 + \beta_1  \mathrm{LNINCOME}, &amp; \text{for other respondents}, \\
        \beta_0 + \beta_2 + (\beta_1 + \beta_3)  \mathrm{LNINCOME}, &amp; \text{for single respondents}.
\end{cases}
\]</span></p>
<p>Thus, through this single model with four parameters, we can create two separate regression lines, one for those single and one for others. Figure <a href="C3BasicMLR.html#fig:letterinteract">3.7</a> shows the two fitted regression lines for our data.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:letterinteract"></span>
<img src="RegressionMarkdown_files/figure-html/letterinteract-1.png" alt="Letter plot of LNFACE versus LNINCOME, with the letter code S for single and o for other. The fitted regression lines have been superimposed. The lower line is for single and the upper line is for other." width="100%" />
<p class="caption">
Figure 3.7: <strong>Letter plot of LNFACE versus LNINCOME, with the letter code <code>S</code> for single and <code>o</code> for other.</strong> The fitted regression lines have been superimposed. The lower line is for single and the upper line is for other.
</p>
</div>
<hr />
<p><strong>Example: Life Insurance Company Expenses.</strong> In a well-developed life insurance industry, minimizing expenses is critical for a company’s competitive position. Segal (2002) analyzed annual accounting data from over 100 firms for the period 1995-1998, inclusive, using a database from the National Association of Insurance Commissioners (NAIC) and other reported information. Segal modeled overall company expenses as a function of firm outputs and the price of inputs. The outputs consist of insurance production, measured by <span class="math inline">\(x_1\)</span> through <span class="math inline">\(x_5\)</span>, described in <a href="C3BasicMLR.html#Table310">Table 3.10</a>. Segal also considered the square of each output, as well as an interaction term with a dummy/binary variable <span class="math inline">\(D\)</span> that indicates whether or not the firm uses a branch company to distribute its products. (In a branch company, field managers are company employees, not independent agents.)</p>
<p><a id=Table310></a></p>
<p><span id="Table310">Table 3.10</span>.<strong>Twenty-Three Regression Coefficients from an Expense Cost Model</strong></p>
<p><span class="math display">\[
\small{
\begin{array}{l|rrrr}
\hline
&amp;  \text{Variable} &amp;  &amp;\text{Variable}&amp;\text{ Squared} \\
  &amp;  \text{Baseline} &amp;  \text{Interaction}  &amp;  \text{Baseline} &amp;  \text{Interaction} \\
  &amp;   &amp;   \text{with}~~   &amp;   &amp;   \text{with}~~  \\
\text{Variable} &amp; (D=0) &amp;  (D=1) &amp; (D=0) &amp;  (D=1)\\
\hline \text{Number of Life Policies Issued } (x_1)   &amp;  -0.454 &amp; 0.152 &amp; 0.032 &amp; -0.007 \\
\text{Amount of Term Life Insurance Sold } (x_2)  &amp; 0.112 &amp; -0.206 &amp; 0.002 &amp; 0.005 \\
\text{Amount of Whole Life Insurance Sold } (x_3) &amp; -0.184 &amp; 0.173 &amp; 0.008 &amp; -0.007 \\
\text{Total Annuity Considerations } (x_4)        &amp; 0.098 &amp; -0.169 &amp; -0.003 &amp; 0.009 \\
\text{Total Accident and Health Premiums } (x_5)  &amp;-0.171 &amp; 0.014 &amp; 0.010 &amp; 0.002 \\
\text{Intercept } &amp; 7.726 &amp; &amp; &amp; \\
\text{Price of Labor (PL)} &amp; 0.553 &amp; &amp; &amp; \\
\text{Price of Capital (PC)} &amp; 0.102 &amp; &amp; &amp; \\
\hline
\end{array}
\\
\begin{array}{l}
\textit{Note: } x_1 \text{ through } x_5 \text{ are in logarithmic units}.\\
\textit{Source: Segal (2002)}
\end{array}
}
\]</span></p>
<p>For the price inputs, the price of labor (<span class="math inline">\(PL\)</span>) is defined to be the total cost of employees and agents divided by their number, in logarithmic units. The price of capital (<span class="math inline">\(PC\)</span>) is approximated by the ratio of capital expense to the number of employees and agents, also in logarithmic units. The price of materials consists of expenses other than labor and capital divided by the number of policies sold and terminated during the year. It does not appear directly as an explanatory variable. Rather, Segal took the dependent variable (<span class="math inline">\(y\)</span>) to be total company expenses divided by the price of materials, again in logarithmic units.</p>
<p>With these variable definitions, Segal estimated the following regression function:</p>
<p><span class="math display">\[
\mathrm{E~}y=\beta_0 + \sum_{j=1}^5 \left( \beta_j x_j + \beta_{j+5} D x_j + \beta_{j+10} x_j^2 + \beta_{j+15}D x_j^2  \right) + \beta_{21} PL + \beta_{22} PC.
\]</span></p>
<p>The parameter estimates appear in <a href="C3BasicMLR.html#Table310">Table 3.10</a>. For example, the marginal change in <span class="math inline">\(\mathrm{E}~y\)</span> per unit change in <span class="math inline">\(x_1\)</span> is:</p>
<p><span class="math display">\[
\frac{\partial ~ \mathrm{E}~y}{\partial x_1}= \beta_1 + \beta_{6} D + 2 \beta_{11} x_1 + 2 \beta_{16}D x_1,
\]</span></p>
<p>which is estimated as $ -0.454 + 0.152 D + (0.064 - 0.014 D) x_1$. For these data, the median number of policies issued was <span class="math inline">\(x_1=15,944\)</span>. At this value of <span class="math inline">\(x_1\)</span>, the estimated marginal change is $ -0.454 + 0.152 D + (0.064 - 0.014 D) (15944) = 0.165 + 0.017 D,$ or 0.165 for baseline <span class="math inline">\((D=0)\)</span> and 0.182 for branch <span class="math inline">\((D=1)\)</span> companies.</p>
<p>These estimates are elasticities, as defined in Section 3.2.2. To interpret these coefficients further, let <span class="math inline">\(COST\)</span> represent total general company expenses and <span class="math inline">\(NUMPOL\)</span> represent the number of life policies issued. Then, for branch <span class="math inline">\((D=1)\)</span> companies, we have:</p>
<p><span class="math display">\[
0.182 \approx \frac{\partial y }{\partial x_1 } = \frac{\partial ~ \mathrm{ln}~COST}{\partial ~ \mathrm{ln}~NUMPOL}= \frac{ \frac{\partial ~ COST}{\partial ~NUMPOL}} {\frac{COST}{NUMPOL}},
\]</span></p>
<p>or <span class="math inline">\(\frac{\partial ~ COST}{\partial ~NUMPOL} \approx 0.182 \frac{COST}{NUMPOL}\)</span>. The median cost is $15,992,000, so the marginal cost per policy at these median values is <span class="math inline">\(0.182 \times (15992000/15944) = \$182.55\)</span>.</p>
<hr />
<div id="special-case-curvilinear-response-functions" class="section level4 unnumbered hasAnchor">
<h4>Special Case: Curvilinear Response Functions<a href="C3BasicMLR.html#special-case-curvilinear-response-functions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can expand the polynomial functions of an explanatory variable to include several explanatory variables. For example, the expected response, or <em>response function</em>, for a second-order model with two explanatory variables is:</p>
<p><span class="math display">\[
\mathrm{E} y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{11} x_1^2 + \beta_{22} x_2^2 + \beta_{12} x_1 x_2.
\]</span></p>
<p>Figure <a href="C3BasicMLR.html#fig:Curvilinear">3.8</a> illustrates this response function. Similarly, the response function for a second-order model with three explanatory variables is:</p>
<p><span class="math display">\[
\mathrm{E} y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_{11} x_1^2 + \beta_{22} x_2^2 + \beta_{33} x_3^2 + \beta_{12} x_1 x_2 + \beta_{13} x_1 x_3 + \beta_{23} x_2 x_3.
\]</span></p>
<p>When there is more than one explanatory variable, third and higher-order models are rarely used in applications.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Curvilinear"></span>
<img src="RegressionMarkdown_files/figure-html/Curvilinear-1.png" alt="Plot of \(\mathrm{E}~y = \beta_0 + \beta_1~x_1 + \beta_2~x_2 + \beta_{11}~x_1^2 + \beta_{22}~x_2^2 + \beta_{12}~x_1~x_2\) versus \(x_1\) and \(x_2\)." width="60%" />
<p class="caption">
Figure 3.8: <strong>Plot of <span class="math inline">\(\mathrm{E}~y = \beta_0 + \beta_1~x_1 + \beta_2~x_2 + \beta_{11}~x_1^2 + \beta_{22}~x_2^2 + \beta_{12}~x_1~x_2\)</span> versus <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>.</strong>
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Fig38.Hide" href="javascript:togglecode('toggleCode.Fig38.Hide','displayCode.Fig38.Hide');"><i><strong>R Code to Produce Figure 3.8</strong></i></a>
</h5>
<div id="toggleCode.Fig38.Hide" style="display: none">
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="C3BasicMLR.html#cb40-1" tabindex="-1"></a><span class="co">#  FIGURE 3.8</span></span>
<span id="cb40-2"><a href="C3BasicMLR.html#cb40-2" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">3</span>, <span class="dv">16</span>, <span class="at">length=</span><span class="dv">15</span>)</span>
<span id="cb40-3"><a href="C3BasicMLR.html#cb40-3" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">5</span>, <span class="dv">15</span>, <span class="at">length=</span><span class="dv">15</span>)</span>
<span id="cb40-4"><a href="C3BasicMLR.html#cb40-4" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(X1,X2) {y <span class="ot">&lt;-</span> <span class="dv">50</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>X1 <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>X2 <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>(X1<span class="dv">-10</span>)<span class="sc">*</span>(X2<span class="dv">-10</span>)<span class="sc">-</span> .<span class="dv">4</span><span class="sc">*</span>(X1<span class="dv">-10</span>)<span class="sc">^</span><span class="dv">2</span><span class="fl">+.4</span><span class="sc">*</span>(X2<span class="dv">-10</span>)<span class="sc">^</span><span class="dv">2</span>}</span>
<span id="cb40-5"><a href="C3BasicMLR.html#cb40-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">outer</span>(X1, X2, f)</span>
<span id="cb40-6"><a href="C3BasicMLR.html#cb40-6" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">2.1</span>,<span class="dv">0</span>,.<span class="dv">1</span>))</span>
<span id="cb40-7"><a href="C3BasicMLR.html#cb40-7" tabindex="-1"></a><span class="fu">persp</span>(X1, X2, y, <span class="at">theta =</span> <span class="dv">30</span>, <span class="at">phi =</span> <span class="dv">30</span>, <span class="at">expand =</span> <span class="fl">0.5</span>, </span>
<span id="cb40-8"><a href="C3BasicMLR.html#cb40-8" tabindex="-1"></a>   <span class="at">ticktype=</span><span class="st">&quot;detailed&quot;</span>)</span></code></pre></div>
</div>
<hr />
</div>
<div id="special-case-nonlinear-functions-of-a-continuous-variable" class="section level4 unnumbered hasAnchor">
<h4>Special Case: Nonlinear Functions of a Continuous Variable<a href="C3BasicMLR.html#special-case-nonlinear-functions-of-a-continuous-variable" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In some applications, we expect the response to have some abrupt changes in behavior at certain values of an explanatory variable, even if the variable is continuous. For example, suppose that we are trying to model an individual’s charitable contributions (<span class="math inline">\(y\)</span>) in terms of their wages (<span class="math inline">\(x\)</span>). For 2007 data, a simple model we might entertain is given in Figure <a href="C3BasicMLR.html#fig:Charity">3.9</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Charity"></span>
<img src="RegressionMarkdown_files/figure-html/Charity-1.png" alt="The marginal change in \(\mathrm{E}~y\) is lower below $97,500. The parameter \(\beta_2\) represents the difference in the slopes." width="60%" />
<p class="caption">
Figure 3.9: <strong>The marginal change in <span class="math inline">\(\mathrm{E}~y\)</span> is lower below $97,500.</strong> The parameter <span class="math inline">\(\beta_2\)</span> represents the difference in the slopes.
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Fig39.Hide" href="javascript:togglecode('toggleCode.Fig39.Hide','displayCode.Fig39.Hide');"><i><strong>R Code to Produce Figure 3.9</strong></i></a>
</h5>
<div id="toggleCode.Fig39.Hide" style="display: none">
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="C3BasicMLR.html#cb41-1" tabindex="-1"></a><span class="co">#  FIGURE 3.9</span></span>
<span id="cb41-2"><a href="C3BasicMLR.html#cb41-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">90000</span>,<span class="dv">105000</span>,<span class="dv">20</span>)</span>
<span id="cb41-3"><a href="C3BasicMLR.html#cb41-3" tabindex="-1"></a>Ey <span class="ot">&lt;-</span> <span class="dv">20</span> <span class="sc">+</span> <span class="dv">1</span><span class="sc">*</span>x <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>(x<span class="dv">-97500</span>)<span class="sc">*</span>(x<span class="sc">&gt;</span><span class="dv">97500</span>)</span>
<span id="cb41-4"><a href="C3BasicMLR.html#cb41-4" tabindex="-1"></a></span>
<span id="cb41-5"><a href="C3BasicMLR.html#cb41-5" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">4.1</span>,<span class="fl">3.1</span>,.<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">cex=</span><span class="fl">1.3</span>)</span>
<span id="cb41-6"><a href="C3BasicMLR.html#cb41-6" tabindex="-1"></a><span class="fu">plot</span>(x,Ey, <span class="at">type=</span><span class="st">&quot;l&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>,  <span class="at">las=</span><span class="dv">1</span>)</span>
<span id="cb41-7"><a href="C3BasicMLR.html#cb41-7" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;E y&quot;</span>, <span class="at">side=</span><span class="dv">2</span>,<span class="at">las=</span><span class="dv">1</span>, <span class="at">line=</span><span class="fl">1.5</span>,<span class="at">cex=</span><span class="fl">1.1</span>)</span>
<span id="cb41-8"><a href="C3BasicMLR.html#cb41-8" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">102000</span>,<span class="dv">102000</span>,<span class="fu">expression</span>(beta[<span class="dv">1</span>]<span class="sc">+</span>beta[<span class="dv">2</span>]),<span class="at">cex=</span><span class="fl">1.1</span>)</span>
<span id="cb41-9"><a href="C3BasicMLR.html#cb41-9" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">94000</span>,<span class="dv">92000</span>,<span class="fu">expression</span>(beta[<span class="dv">1</span>]),<span class="at">cex=</span><span class="fl">1.1</span>)</span></code></pre></div>
</div>
<p>A rationale for this model is that, in 2007, individuals paid 7.65% of their income for Social Security taxes up to 97,500. No Social Security taxes are excised on wages in excess of 97,500. Thus, one theory is that, for wages in excess of $97,500, individuals have more disposable income per dollar and thus should be more willing to make charitable contributions.</p>
<p>To model this relationship, define the binary variable <span class="math inline">\(z\)</span> to be zero if <span class="math inline">\(x &lt; 97,500\)</span> and to be one if <span class="math inline">\(x \ge 97,500\)</span>. Define the regression function to be <span class="math inline">\(\mathrm{E}~y = \beta_0 + \beta_1 x + \beta_2 z (x - 97,500)\)</span>. This can be written as:</p>
<p><span class="math display">\[
\mathrm{E}~y = \begin{cases}
\beta_0 + \beta_1  x &amp; x &lt; 97,500 \\
\beta_0 - \beta_2(97,500) + (\beta_1+\beta_2) x &amp; x \geq 97,500
\end{cases}
\]</span></p>
<p>To estimate this model, we would run a regression of <span class="math inline">\(y\)</span> on two explanatory variables, <span class="math inline">\(x_1 = x\)</span> and <span class="math inline">\(x_2 = z \times (x - 97,500)\)</span>. If <span class="math inline">\(\beta_2 &gt; 0\)</span>, then the marginal rate of charitable contributions is higher for incomes exceeding $97,500.</p>
<p>Figure <a href="C3BasicMLR.html#fig:Charity">3.9</a> illustrates this relationship, known as <em>piecewise linear regression</em> or sometimes a “broken stick” model. The sharp break in Figure <a href="C3BasicMLR.html#fig:Charity">3.9</a> at <span class="math inline">\(x = 97,500\)</span> is called a “kink.” We have linear relationships above and below the kinks and have used a binary variable to put the two pieces together. We are not restricted to one kink. For example, suppose that we wish to do a historical study of Federal taxable income for 1992 single filers. Then, there were three tax brackets: the marginal tax rate below 21,450 was 15%, above 51,900 was 31%, and in between was 28%. For this example, we would use two kinks, at 21,450 and 51,900.</p>
<p>Further, piecewise linear regression is not restricted to continuous response functions. For example, suppose that we are studying the commissions paid to stockbrokers (<span class="math inline">\(y\)</span>) in terms of the number of shares purchased by a client (<span class="math inline">\(x\)</span>). We might expect to see the relationship illustrated in Figure <a href="C3BasicMLR.html#fig:Break">3.10</a>. Here, the discontinuity at <span class="math inline">\(x = 100\)</span> reflects the administrative expenses of trading in odd lots, as trades of less than 100 shares are called. The lower marginal cost for trades in excess of 100 shares simply reflects the economies of scale for doing business in larger volumes. A regression model of this is <span class="math inline">\(\mathrm{E}~y = \beta_0 + \beta_1 x + \beta_2 z + \beta_3 z x\)</span> where <span class="math inline">\(z = 0\)</span> if <span class="math inline">\(x &lt; 100\)</span> and <span class="math inline">\(z = 1\)</span> if <span class="math inline">\(x \geq 100\)</span>. The regression function depicted in Figure <a href="C3BasicMLR.html#fig:Break">3.10</a> is:</p>
<p><span class="math display">\[
\mathrm{E}~y = \begin{cases}
\beta_0 + \beta_1  x_1 &amp; x &lt; 100 \\
\beta_0 + \beta_2 + (\beta_1+\beta_3) x_1 &amp; x \geq 100
\end{cases}
\]</span></p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Break"></span>
<img src="RegressionMarkdown_files/figure-html/Break-1.png" alt="Plot of expected commissions (\(\mathrm{E}~y\)) versus number of shares traded (\(x\)). The break at \(x=100\) reflects savings in administrative expenses. The lower slope for \(x \ge 100\) reflects economies of scales in expenses." width="60%" />
<p class="caption">
Figure 3.10: <strong>Plot of expected commissions (<span class="math inline">\(\mathrm{E}~y\)</span>) versus number of shares traded (<span class="math inline">\(x\)</span>)</strong>. The break at <span class="math inline">\(x=100\)</span> reflects savings in administrative expenses. The lower slope for <span class="math inline">\(x \ge 100\)</span> reflects economies of scales in expenses.
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Fig310.Hide" href="javascript:togglecode('toggleCode.Fig310.Hide','displayCode.Fig310.Hide');"><i><strong>R Code to Produce Figure 3.10</strong></i></a>
</h5>
<div id="toggleCode.Fig310.Hide" style="display: none">
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="C3BasicMLR.html#cb42-1" tabindex="-1"></a><span class="co">#  FIGURE 3.10</span></span>
<span id="cb42-2"><a href="C3BasicMLR.html#cb42-2" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">4.1</span>,<span class="fl">4.5</span>,.<span class="dv">1</span>,.<span class="dv">1</span>))</span>
<span id="cb42-3"><a href="C3BasicMLR.html#cb42-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">50</span>,<span class="dv">150</span>,.<span class="dv">1</span>)</span>
<span id="cb42-4"><a href="C3BasicMLR.html#cb42-4" tabindex="-1"></a>Ey <span class="ot">&lt;-</span> <span class="dv">20</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">-</span> <span class="dv">1</span><span class="sc">*</span>(x<span class="dv">-100</span>)<span class="sc">*</span>(x<span class="sc">&gt;</span><span class="dv">100</span>) <span class="sc">-</span> <span class="dv">20</span><span class="sc">*</span>(x<span class="sc">&gt;</span><span class="dv">100</span>)</span>
<span id="cb42-5"><a href="C3BasicMLR.html#cb42-5" tabindex="-1"></a><span class="fu">plot</span>(x,Ey, <span class="at">type=</span><span class="st">&quot;p&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">font.lab=</span><span class="dv">1</span>, <span class="at">cex.lab=</span><span class="fl">1.1</span>, <span class="at">cex=</span>.<span class="dv">25</span>, <span class="at">las=</span><span class="dv">1</span>)</span>
<span id="cb42-6"><a href="C3BasicMLR.html#cb42-6" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;E y&quot;</span>, <span class="at">side=</span><span class="dv">2</span>,<span class="at">las=</span><span class="dv">1</span>, <span class="at">line=</span><span class="fl">2.8</span>,<span class="at">cex=</span><span class="fl">1.1</span>)</span></code></pre></div>
</div>
</div>
</div>
</div>
<div id="further-reading-and-references-2" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Further Reading and References<a href="C3BasicMLR.html#further-reading-and-references-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For proofs of the Chapter 3 results, we refer the reader to
Goldberger (1991). Nonlinear regression models are discussed in, for
example, Bates and Watts (1988).</p>
<p>Chapter 3 has introduced the fundamentals of multiple linear
regression. Chapter 4 will widen the scope by introducing
categorical variables and statistical inference methods for handling
several coefficients simultaneously. Chapter 5 will introduce
techniques to help you pick appropriate variables in a multiple
linear regression model. Chapter 6 is a synthesis chapter,
discussing model interpretation, variable selection and data
collection.</p>
<p><strong>Chapter References</strong></p>
<ul>
<li>Bates, Douglas M. and Watts, D. G. (1988). <em>Nonlinear Regression Analysis and its Applications</em>. John Wiley &amp; Sons, New York.</li>
<li>Lemaire, Jean (2002). Why do females live longer than males? <em>North American Actuarial Journal</em>, 6(4), 21-37.</li>
<li>Goldberger, Arthur (1991). <em>A Course in Econometrics</em>. Harvard University Press, Cambridge.</li>
<li>Plackett, R.L. (1960). <em>Regression Analysis</em>. Clarendon Press, Oxford, England.</li>
<li>Segal, Dan (2002). An economic analysis of life insurance company expenses. <em>North American Actuarial Journal</em>, 6(4), 81-94.</li>
</ul>
</div>
<div id="exercises-2" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Exercises<a href="C3BasicMLR.html#exercises-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>3.1. Consider a fictitious data set of <span class="math inline">\(n = 100\)</span> observations with <span class="math inline">\(s_y = 100\)</span>. We run a regression with three explanatory variables to get <span class="math inline">\(s = 50\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Calculate the adjusted coefficient of determination, <span class="math inline">\(R_a^2\)</span>.</li>
<li>Complete the ANOVA table.</li>
</ol>
<p><span class="math display">\[
\small{
    \begin{array}{|l|l|c|l|}
    \hline
    \text{ANOVA Table} \\ \hline
    \text{Source} &amp; \text{Sum of Squares} &amp; \text{df} &amp; \text{Mean Square} \\ \hline
    \text{Regression} &amp; &amp; &amp; \\
    \text{Error} &amp; &amp; &amp; \\
    \text{Total} &amp; &amp; &amp; \\ \hline
    \end{array}
}
\]</span>
c. Calculate the (unadjusted) coefficient of determination, <span class="math inline">\(R^2\)</span>.</p>
<p>3.2. Consider a fictitious data set of <span class="math inline">\(n = 100\)</span> observations with <span class="math inline">\(s_y = 80\)</span>. We run a regression with three explanatory variables to get <span class="math inline">\(s = 50\)</span>. We also get</p>
<p><span class="math display">\[
\small{
\left(\mathbf{X^{\prime} X} \right)^{-1} = \begin{pmatrix}
    100 &amp; 20 &amp; 20 &amp; 20 \\
    20 &amp; 90 &amp; 30 &amp; 40 \\
    20 &amp; 30 &amp; 80 &amp; 50 \\
    20 &amp; 40 &amp; 50 &amp; 70 \\
\end{pmatrix}.
}
\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Determine the standard error of <span class="math inline">\(b_3\)</span>, <span class="math inline">\(se(b_3)\)</span>.</li>
<li>Determine the estimated covariance between <span class="math inline">\(b_2\)</span> and <span class="math inline">\(b_3\)</span>.</li>
<li>Determine the estimated correlation between <span class="math inline">\(b_2\)</span> and <span class="math inline">\(b_3\)</span>.</li>
<li>Determine the estimated variance of <span class="math inline">\(4b_2 + 3b_3\)</span>.</li>
</ol>
<p>3.3. Consider the following small fictitious data set. You will be fitting a regression model to <span class="math inline">\(y\)</span> using two explanatory variables, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>.</p>
<p><span class="math display">\[
\small{
    \begin{array}{c|cccc}
    \hline
    i &amp; 1 &amp; 2 &amp; 3 &amp; 4 \\ \hline
    x_{i,1} &amp; -1 &amp; 2 &amp; 4 &amp; 6 \\
    x_{i,2} &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\
    y_i &amp; 0 &amp; 1 &amp; 5 &amp; 8 \\ \hline
    \end{array}
}
\]</span></p>
<p>From the fitted regression model, we have <span class="math inline">\(s = 1.373\)</span> and
<span class="math display">\[
\small{
    \mathbf{b} = \begin{pmatrix}
    0.1538 \\
    0.6923 \\
    2.8846
    \end{pmatrix}
    ~~~\text{and}~~~
    \left(\mathbf{X^{\prime} X} \right)^{-1} = \begin{pmatrix}
    0.53846 &amp; -0.07692 &amp; -0.15385 \\
    -0.07692 &amp; 0.15385 &amp; -0.69231 \\
    -0.15385 &amp; -0.69231 &amp; 4.11538 \\
    \end{pmatrix}.
}
\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Write down the vector of dependent variables, <span class="math inline">\(\mathbf{y}\)</span>, and the matrix of explanatory variables, <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li>Determine the numerical value for <span class="math inline">\(\widehat{y}_3\)</span>, the fitted value for the third observation.</li>
<li>Determine the numerical value for <span class="math inline">\(se(b_2)\)</span>.</li>
<li>Determine the numerical value for <span class="math inline">\(t(b_1)\)</span>.</li>
</ol>
<p>3.4. <strong>Wisconsin Lottery.</strong> Section 2.1 described a sample of <span class="math inline">\(n=50\)</span> geographic areas (zip codes) containing sales data on the Wisconsin state lottery (<span class="math inline">\(y = \text{SALES}\)</span>). In that section, sales were analyzed using a basic linear regression model with <span class="math inline">\(x = \text{POP}\)</span>, the area population, as the explanatory variable. This exercise extends that analysis by introducing additional explanatory variables given in Table 3.11.</p>
<p><span class="math display">\[
    \text{Table 3.11: Lottery, economic and demographic characteristics of fifty Wisconsin ZIP codes}
\]</span></p>
<p><span class="math display">\[
\small{
\begin{array}{ll}
    \hline
    \textbf{Lottery characteristics} \\ \hline
    \text{SALES} &amp; \text{Online lottery sales to individual consumers} \\ \hline
    \textbf{Economic and } \\
     ~~~~~\textbf{demographic characteristics} \\\hline
    \text{PERPERHH} &amp; \text{Persons per household} \\
    \text{MEDSCHYR} &amp; \text{Median years of schooling} \\
    \text{MEDHVL} &amp; \text{Median home value in \$1000s for owner-occupied homes} \\
    \text{PRCRENT} &amp; \text{Percent of housing that is renter occupied} \\
    \text{PRC55P} &amp; \text{Percent of population that is 55 or older} \\
    \text{HHMEDAGE} &amp; \text{Household median age} \\
    \text{MEDINC} &amp; \text{Estimated median household income, in \$1000s} \\
    \text{POP} &amp; \text{Population} \\ \hline
\end{array}
}
\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Produce a table of summary statistics for all variables. One zip code (observation 11, zip = 53211, Shorewood Wisconsin, a suburb of Milwaukee) appears to have unusually large values of MEDSCHYR and MEDHVL. For this observation, how many standard deviations is the value of MEDSCHYR above the mean? For this observation, how many standard deviations is the value of MEDHVL above the mean?</li>
<li>Produce a table of correlations. What three variables are most highly correlated with SALES?</li>
<li>Produce a scatter plot matrix of all explanatory variables and SALES. In the plot of MEDSCHYR versus SALES, describe the position of observation 11.</li>
<li>Fit a linear model of SALES on all eight explanatory variables. Summarize the fit of this model by citing the residual standard deviation, <span class="math inline">\(s\)</span>, the coefficient of determination, <span class="math inline">\(R^2\)</span> and its adjusted version, <span class="math inline">\(R_a^2\)</span>.</li>
<li>Based on your part (d) model fit, is MEDSCHYR a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion and your decision-making rule.</li>
<li>Now fit a more parsimonious model, using SALES as the dependent variable and MEDSCHYR, MEDHVL and POP as explanatory variables. Summarize the fit of this model by citing the residual standard deviation, <span class="math inline">\(s\)</span>, the coefficient of determination, <span class="math inline">\(R^2\)</span> and its adjusted version, <span class="math inline">\(R_a^2\)</span>. How do these values compare to the model fit in part (d)?</li>
<li>Note that the sign of the regression coefficient associated with MEDSCHYR is negative. To help interpret this coefficient, compute the corresponding partial correlation coefficient. What is the interpretation of this coefficient?</li>
<li>To get further insights into the relation between MEDSCHYR and SALES, produce an added variable plot controlling for the effects of MEDHVL and POP. Check that the correlation associated with this plot agrees with your answer in part (g).</li>
<li>Re-run the regression in part (f), after removing observation 11. Cite the basic summary statistics from this regression. For this model fit, is MEDSCHYR a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion and your decision-making rule.</li>
<li>Re-run the regression in part (f), after removing observation 9. Cite the basic summary statistics from this regression.</li>
</ol>
<p>3.5. <strong>Insurance Company Expenses</strong>. This exercise considers insurance company data from the NAIC and described in Exercise 1.</p>
<p>Table <a href="C3BasicMLR.html#tab:NAICVariables">3.10</a> describes several variables that can be used to explain expenses. As with Segal’s (2002) study of life insurers, firm “outputs” consist of premiums written (for property and casualty, these are subdivided into personal and commercial lines) as well as losses (subdivided into short and long tail lines). ASSETS and CASH are commonly used measures of the size of a company. GROUP, STOCK, and MUTUAL describe the organizational structure. Firm “inputs” were gathered from the Bureau of Labor Statistics (BLS, from the Occupational Employee Statistics program). STAFFWAGE is calculated as the average wage in the state where the insurance company is headquartered. AGENTWAGE is calculated as the weighted average of annual wages of the brokerage industry, weighted by the percentage of gross premium written in each state.</p>
<table class=" lightable-classic table table-striped table-condensed" style="font-size: 12px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:NAICVariables">Table 3.10: </span><strong>Insurer Expense Variables</strong>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:left;">
Description
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
EXPENSES
</td>
<td style="text-align:left;width: 1.5cm; width: 12cm; ">
Total expenses incurred, in millions of dollars
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
LONGLOSS
</td>
<td style="text-align:left;width: 1.5cm; width: 12cm; ">
Losses incurred for long tail lines, in millions of dollars
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
SHORTLOSS
</td>
<td style="text-align:left;width: 1.5cm; width: 12cm; ">
Losses incurred for short tail lines, in millions of dollars
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
GPWPERSONAL
</td>
<td style="text-align:left;width: 1.5cm; width: 12cm; ">
Gross premium written for personal lines, in millions of dollars
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
GPWCOMM
</td>
<td style="text-align:left;width: 1.5cm; width: 12cm; ">
Gross premium written for commercial lines, in millions of dollars
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
ASSETS
</td>
<td style="text-align:left;width: 1.5cm; width: 12cm; ">
Net admitted assets, in millions of dollars
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
CASH
</td>
<td style="text-align:left;width: 1.5cm; width: 12cm; ">
Cash and invested assets, in millions of dollars
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
GROUP
</td>
<td style="text-align:left;width: 1.5cm; width: 12cm; ">
Indicates if the company is affiliated
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
STOCK
</td>
<td style="text-align:left;width: 1.5cm; width: 12cm; ">
Indicates if the company is a stock company
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
MUTUAL
</td>
<td style="text-align:left;width: 1.5cm; width: 12cm; ">
Indicates if the company is a mutual company
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
STAFFWAGE
</td>
<td style="text-align:left;width: 1.5cm; width: 12cm; ">
Annual average wage of the insurer’s administrative staff, in thousands of dollars
</td>
</tr>
<tr>
<td style="text-align:left;width: 2.5cm; border-right:1px solid;width: 4cm; ">
AGENTWAGE
</td>
<td style="text-align:left;width: 1.5cm; width: 12cm; ">
Annual average wage of the insurance agent, in thousands of dollars
</td>
</tr>
</tbody>
</table>
<p>A preliminary inspection of the data showed that many firms did not report any insurance losses incurred in 2005. For this exercise, we consider the 384 companies with some losses in the file <code>NAICExpense.csv</code>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Summary Statistics <br>
Produce summary statistics of the response variable and the (non-binary) explanatory variables. Note the pattern of skewness for each variable. Note that many variables have negative values. <br></p></li>
<li><p>Transform Variables <br>
Transform each non-binary variable through the modified logarithm transform, <span class="math inline">\(\ln(1+x)\)</span>. Produce summary statistics of these modified non-binary explanatory variables. Let LNEXPENSES (<span class="math inline">\(= \ln(1+\text{EXPENSES})\)</span>) denote the modified expense variable. <br>
For subsequent analysis, use only the modified variables described in part (b). <br></p></li>
<li><p>Correlation Table <br>
Produce a table of correlations for the non-binary variables. What three variables are most highly correlated with LNEXPENSES? <br></p></li>
<li><p>Boxplot of LNEXPENSES by GROUP <br>
Provide a boxplot of LNEXPENSES by level of GROUP. Which level of group has higher expenses? <br></p></li>
<li><p>Linear Model on All Variables <br>
Fit a linear model of LNEXPENSES on all eleven explanatory variables. Summarize the fit of this model by citing the residual standard deviation, <span class="math inline">\(s\)</span>, the coefficient of determination, <span class="math inline">\(R^2\)</span>, and its adjusted version, <span class="math inline">\(R^2_a\)</span>. <br></p></li>
<li><p>Reduced Model <br>
Fit a linear model of LNEXPENSES on a reduced model using eight explanatory variables, dropping LNCASH, STOCK, and MUTUAL. For the explanatory variables, include ASSETS, GROUP, both versions of losses, and gross premiums, as well as the two BLS variables. <br></p>
<p><strong>f(i).</strong> Summarize the fit of this model by citing <span class="math inline">\(s\)</span>, <span class="math inline">\(R^2\)</span>, and <span class="math inline">\(R^2_a\)</span>. <br>
<strong>f(ii).</strong> Interpret the coefficient associated with commercial lines gross premiums on the logarithmic scale. <br>
<strong>f(iii).</strong> Suppose that GPWCOMM increases by $1, how much do we expect EXPENSES to increase? Use your answer in part f(ii) and median values of GPWCOMM and EXPENSES for this question. <br></p></li>
<li><p>Quadratic Terms <br>
Square each of the two loss and the two gross premium variables. Fit a linear model of LNEXPENSES on a reduced model using twelve explanatory variables, the eight variables in part (f) and the four additional squared terms just created. <br>
<strong>g(i).</strong> Summarize the fit of this model by citing <span class="math inline">\(s\)</span>, <span class="math inline">\(R^2\)</span>, and <span class="math inline">\(R^2_a\)</span>. <br>
<strong>g(ii).</strong> Do the quadratic variables appear to be useful explanatory variables? <br></p></li>
<li><p>Excluding BLS Variables <br>
Now omit the two BLS variables, so you are fitting a model of LNEXPENSES on ASSETS, GROUP, both versions of losses, and gross premiums, as well as quadratic terms. Summarize the fit of this model by citing <span class="math inline">\(s\)</span>, <span class="math inline">\(R^2\)</span>, and <span class="math inline">\(R^2_a\)</span>. Comment on the number of observations used to fit this model compared to part (f). <br></p></li>
<li><p>Interaction Terms <br>
Drop the quadratic terms in part (g) and add interaction terms with the dummy variable GROUP. Thus, there are now eleven variables: ASSETS, GROUP, both versions of losses and gross premiums, as well as interactions of GROUP with ASSETS and both versions of losses and gross premiums. <br>
<strong>i(i).</strong> Summarize the fit of this model by citing <span class="math inline">\(s\)</span>, <span class="math inline">\(R^2\)</span>, and <span class="math inline">\(R^2_a\)</span>. <br>
<strong>i(ii).</strong> Suppose that GPWCOMM increases by $1, how much do we expect EXPENSES to increase for GROUP=0 companies? Use the median values of GPWCOMM and EXPENSES of GROUP=0 companies for this question. <br>
<strong>i(iii).</strong> Suppose that GPWCOMM increases by $1, how much do we expect EXPENSES to increase for GROUP=1 companies? Use the median values of GPWCOMM and EXPENSES of GROUP=1 companies for this question.</p></li>
</ol>
<p>3.6 <strong>National Life Expectancies</strong>. We continue the analysis begun in Exercises 1 and 2. Now fit a regression model on LIFEEXP using three explanatory variables: FERTILITY, PUBLICEDUCATION, and lnHEALTH (the natural logarithmic transform of PRIVATEHEALTH).</p>
<ol style="list-style-type: lower-alpha">
<li>Interpretation of Public Education Coefficient <br>
Interpret the regression coefficient associated with PUBLICEDUCATION. <br></li>
<li>Interpretation of Health Expenditures Coefficient <br>
Interpret the regression coefficient associated with HEALTH expenditures without using the logarithmic scale for expenditures. <br></li>
<li>Statistical Significance of PUBLICEDUCATION <br>
Based on the model fit, is PUBLICEDUCATION a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. <br></li>
<li>Added Variable Plot <br>
The negative sign of the PUBLICEDUCATION coefficient is surprising, given that the sign of the correlation between PUBLICEDUCATION and LIFEEXP is positive and intuition suggests a positive relation. To check this result, an added variable plot appears in Figure <a href="C3BasicMLR.html#fig:UNLIFE2">3.11</a>. <br>
<strong>d(i).</strong> For an added variable plot, describe its purpose and a method for producing it. <br>
<strong>d(ii).</strong> Calculate the correlation corresponding to the added variable plot that appears in Figure <a href="C3BasicMLR.html#fig:UNLIFE2">3.11</a>. <br></li>
</ol>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:UNLIFE2"></span>
<img src="RegressionMarkdown_files/figure-html/UNLIFE2-1.png" alt="Added variable plot of PUBLICEDUCATION versus LIFEEXP, controlling for FERTILITY and lnHEALTH" width="60%" />
<p class="caption">
Figure 3.11: <strong>Added variable plot of PUBLICEDUCATION versus LIFEEXP, controlling for FERTILITY and lnHEALTH</strong>
</p>
</div>
<h5 style="text-align: center;">
<a id="displayCode.Fig311.Hide" href="javascript:togglecode('toggleCode.Fig311.Hide','displayCode.Fig311.Hide');"><i><strong>R Code to Produce Figure 3.11</strong></i></a>
</h5>
<div id="toggleCode.Fig311.Hide" style="display: none">
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="C3BasicMLR.html#cb43-1" tabindex="-1"></a><span class="co">#  FIGURE 3.11</span></span>
<span id="cb43-2"><a href="C3BasicMLR.html#cb43-2" tabindex="-1"></a>LifeExp <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;CSVData/UNLifeExpectancy.csv&quot;</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb43-3"><a href="C3BasicMLR.html#cb43-3" tabindex="-1"></a><span class="co">#  REMOVE MISSING LIFEEXPs</span></span>
<span id="cb43-4"><a href="C3BasicMLR.html#cb43-4" tabindex="-1"></a>LifeExp3 <span class="ot">&lt;-</span> <span class="fu">subset</span>(LifeExp, <span class="sc">!</span><span class="fu">is.na</span>(LIFEEXP) )   </span>
<span id="cb43-5"><a href="C3BasicMLR.html#cb43-5" tabindex="-1"></a>varLife <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;FERTILITY&quot;</span>,<span class="st">&quot;PUBLICEDUCATION&quot;</span>, <span class="st">&quot;REGION&quot;</span>,<span class="st">&quot;COUNTRY&quot;</span>,<span class="st">&quot;LIFEEXP&quot;</span>, <span class="st">&quot;HEALTHEXPEND&quot;</span>)</span>
<span id="cb43-6"><a href="C3BasicMLR.html#cb43-6" tabindex="-1"></a>LifeExp4 <span class="ot">&lt;-</span> LifeExp3[varLife]</span>
<span id="cb43-7"><a href="C3BasicMLR.html#cb43-7" tabindex="-1"></a>LifeExp4<span class="sc">$</span>lnHEALTH <span class="ot">&lt;-</span> <span class="fu">log</span>(LifeExp4<span class="sc">$</span>HEALTHEXPEND)</span>
<span id="cb43-8"><a href="C3BasicMLR.html#cb43-8" tabindex="-1"></a>LifeExp4.good <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(LifeExp4) </span>
<span id="cb43-9"><a href="C3BasicMLR.html#cb43-9" tabindex="-1"></a></span>
<span id="cb43-10"><a href="C3BasicMLR.html#cb43-10" tabindex="-1"></a><span class="co">#  ADDED VARIABLE PLOT</span></span>
<span id="cb43-11"><a href="C3BasicMLR.html#cb43-11" tabindex="-1"></a>model4a <span class="ot">&lt;-</span> <span class="fu">lm</span>(LIFEEXP <span class="sc">~</span> FERTILITY<span class="sc">+</span>lnHEALTH, <span class="at">data=</span>LifeExp4.good)</span>
<span id="cb43-12"><a href="C3BasicMLR.html#cb43-12" tabindex="-1"></a>model4b <span class="ot">&lt;-</span> <span class="fu">lm</span>(PUBLICEDUCATION <span class="sc">~</span> FERTILITY<span class="sc">+</span>lnHEALTH, <span class="at">data=</span>LifeExp4.good)</span>
<span id="cb43-13"><a href="C3BasicMLR.html#cb43-13" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">residuals</span>(model4b),<span class="fu">residuals</span>(model4a),</span>
<span id="cb43-14"><a href="C3BasicMLR.html#cb43-14" tabindex="-1"></a><span class="at">xlab=</span><span class="st">&quot;residuals(PUBLICEDUCATION)&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;residuals(LIFEEXP)&quot;</span>)</span></code></pre></div>
</div>

<!-- # Chap 1 -->
<!-- # Chap 2 -->
<!-- # Chap 3 -->
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="C2BasicLR.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="C4MLRANOVA.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
