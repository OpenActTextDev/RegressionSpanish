
# Basic Regression

# Multiple Linear Regression - I

*Chapter Preview*. This chapter introduces linear
regression in the case of several explanatory variables, known as
\emph{multiple linear regression}. Many basic linear regression
concepts extend directly, including goodness of fit measures such as
$R^2$ and inference using $t$-statistics. Multiple linear regression
models provide a framework for summarizing highly complex,
multivariate data. Because this framework requires only linearity in
the parameters, we are able to fit models that are nonlinear
functions of the explanatory variables, thus providing a wide scope
of potential applications.

## Method of Least Squares {#S3:LSMethod}
\index{least squares!method}

Chapter 2 dealt with the problem of a response depending on a single
explanatory variable. We now extend the focus of that chapter and study how
a response may depend on several explanatory variables.

\linejed

\empexjed{TermLife}
\index{datasets!term lifeinsurance}
\index{actuarial \& financial terms and concepts!demand}
\ecaptionjed{Demand for Term Life Insurance}

**Example: Term Life Insurance.**} Like all firms, life insurance companies continually seek new ways to deliver products to the market. Those involved in
product development wish to know ``who buys insurance and how much
do they buy?'' In economics, this is known as \emph{demand} side of
a market for products. Analysts can readily get information on
characteristics of current customers through company databases.
Potential customers, those that do not have insurance with the
company, are often the main focus for expanding market share.

In this example, we examine the Survey of Consumer Finances (SCF), a
nationally representative sample that contains extensive information
on assets, liabilities, income, and demographic characteristics of
those sampled (potential U.S. customers). We study a random sample
of 500 households with positive incomes that were interviewed in the
2004 survey. We initially consider the subset of $n=275$ families
that purchased term life insurance. We wish to address the second
portion of the demand question and determine family characteristics
that influence the amount of insurance purchased. Chapter 11 will
consider the first portion, whether or not a household purchases
insurance, through models where the response is a binary random
variable.

For term life insurance, the quantity of insurance is measured by
the policy FACE, the amount that the company will pay in the event
of the death of the named insured. Characteristics that will turn
out to be important include annual INCOME, the number of years of
EDUCATION of the survey respondent and the number of household
members, NUMHH.


\linejed\index{symbols!$k$, number of explanatory variables}

In general, we will consider data sets where there are $k$
explanatory variables and one response variable in a sample of size
$n$. That is, the data consist of:

\begin{center}
\begin{equation*}
\left\{
\begin{tabular}{c}
$x_{11},x_{12},\ldots,x_{1k},y_1$ \\
$x_{21},x_{22},\ldots,x_{2k},y_2$ \\
\\
$x_{n1},x_{n2},\ldots,x_{nk},y_n$%
\end{tabular}%
\right\} .
\end{equation*}
\end{center}

\noindent The $i$th observation corresponds to the $i$th row,
consisting of $(x_{i1},x_{i2},\ldots,x_{ik},y_i)$. For this general
case, we take $k+1$ measurements on each entity. For the insurance
demand example, $k=3$ and the data
consists of $(x_{11},x_{12},x_{13}, y_1)$, \ldots , $%
(x_{275,1},x_{275,2},x_{275,3},y_{275})$. That is, we use four
measurements from each of the $n=275$ households.

### Summarizing the Data {-}

\marginparjed{Begin the data analysis by examining each variable in
isolation of the others.}

We begin the data analysis by examining each variable in isolation
of the others. Table \ref{T3:FaceSumStats} provides basic summary
statistics of the four variables. For FACE and INCOME, we see that
the mean is much greater than the median, suggesting that the
distribution is skewed to the right. Histograms (not reported here)
show that this is the case. It will turn out to be useful to also
consider their logarithmic transforms, LNFACE and LNINCOME,
respectively, that are also reported in Table \ref{T3:FaceSumStats}.

\begin{table}[h]
\caption{\label{T3:FaceSumStats} Term Life Summary Statistics}
\begin{center}
\begin{tabular}{lrrrrr}
\hline
                     &  & & Standard &  &  \\
Variable & Mean & Median & Deviation & Minimum & Maximum \\
\hline
      FACE &    747,581 &    150,000 &  1,674,362 &        800 & 14,000,000 \\
    INCOME &    208,975 &     65,000 &    824,010 &        260 & 10,000,000 \\
 EDUCATION &     14.524 &     16.000 &      2.549 &      2.000 &     17.000 \\
     NUMHH &      2.960 &      3.000 &      1.493 &      1.000 &      9.000 \\
    LNFACE &     11.990 &     11.918 &      1.871 &      6.685 &     16.455 \\
  LNINCOME &     11.149 &     11.082 &      1.295 &      5.561 &     16.118 \\
\hline
\end{tabular}\end{center}\end{table}

The next step is to measure the relationship between each $x$ on
$y$, beginning with the scatter plots in Figure
\ref{F3:TermLifeTwoPlots}. The left-hand panel is a plot of FACE
versus INCOME; with this panel, we see a large clustering in the
lower left-hand corner corresponding to households that have both
small incomes and face amounts of insurance. Both variables have
skewed distributions and their joint effect is highly nonlinear. The
right-hand panel presents the same variables but using logarithmic
transforms. Here, we see a relationship that can be more readily
approximated with a line.


\begin{figure}[htp]
  \begin{center}
 %     \includegraphics[width=2.5in,height=5in,angle=270]{Chapter3/F3TermLifeTwoPlotsB.ps}
    \includegraphics[width=.8\textwidth]{Chapter3/F3TermLifeTwoPlots.eps}
    \caption{\label{F3:TermLifeTwoPlots} \small  Income versus Face Amount of Term Life Insurance. The
left-panel is a plot of face versus income, showing a highly
nonlinear pattern. In the right-hand panel, face versus income is in
natural logarithmic units, suggesting a linear (although variable)
pattern.}
  \end{center}
\end{figure}

The Term Life data are \emph{multivariate} in the sense that several
measurements are taken on each household. It is difficult to produce
a graph of observations in three or more dimensions on a
two-dimensional platform, such as a piece of paper, that is not
confusing, misleading or both. To summarize graphically multivariate
data in regression applications, consider using a \emph{scatterplot
matrix} such as in Figure \ref{F3:TermLifeSMatrix}. Each square of
this figure represents a simple plot of one variable versus another.
For each square, the row variable gives the units of the vertical
axis and the column variable gives the units of the horizontal axis.
The matrix is sometimes called a \emph{half scatterplot matrix}
because only the lower left-hand elements are presented.

\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=1\textwidth]{Chapter3/F3TermLifeSMatrix.eps}
    \caption{\label{F3:TermLifeSMatrix} \small  Scatterplot
matrix of four variables. Each square is a scatter plot.}
  \end{center}
\end{figure}\index{plots!scatterplot matrix}\index{plots!half scatterplot matrix}

The scatterplot matrix can be numerically summarized using a
correlation matrix. Each correlation in Table \ref{T3:Corr}
corresponds to a square of the scatterplot matrix in Figure
\ref{F3:TermLifeSMatrix}. Analysts often present tables of
correlations because they are easy to interpret. However, remember
that a correlation coefficient merely measures the extent of linear
relationships. Thus, a table of correlations provides a sense of
linear relationships but may miss a nonlinear relationship that can
be revealed in a scatterplot matrix.




\begin{table}[h]
\caption{\label{T3:Corr} Term Life Correlations}
\begin{tabular}{lccc}
\hline
          &  NUMHH    & EDUCATION & LNINCOME  \\
EDUCATION & -0.064~   \\
LNINCOME  & 0.179     & 0.343   \\
LNFACE    & 0.288     & 0.383   & 0.482     \\

\hline
\end{tabular}\end{table}

The scatterplot matrix and corresponding correlation matrix are
useful devices for summarizing multivariate data. They are easy to
produce and to interpret. Still, each device captures only
relationships between pairs of variables and cannot quantify
relationships among several variables.

\subsubsection*{Method of Least Squares}

Consider the question: ``Can knowledge of education, household size
and income help us understand the demand for insurance?'' The
correlations in Table \ref{T3:Corr} and the graphs in Figures
\ref{F3:TermLifeTwoPlots} and \ref{F3:TermLifeSMatrix} suggest that
each variable, EDUCATION, NUMHH and LNINCOME, may be a useful
explanatory variable of LNFACE when taken individually. It seems
reasonable to investigate the \emph{joint} effect of these variables
on a response.

The geometric concept of a \emph{plane} is used to explore the
linear relationship between a response and several explanatory
variables. Recall that a plane extends the concept of a line to more
than two dimensions. A plane may be defined through an algebraic
equation such as
\begin{equation*}
y = b_0 + b_1 x_1 + \ldots + b_k x_k.
\end{equation*}
This equation defines a plane in $k+1$ dimensions. Figure
\ref{F3:3DPlane} shows a plane in three dimensions. For this figure,
there is one response variable, LNFACE, and two explanatory
variables, EDUCATION and LNINCOME (NUMHH is held fixed). It is
difficult to graph more than three dimensions in a meaningful way.

\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=.5\textwidth]{Chapter3/F33DPlane.eps}
    \caption{\label{F3:3DPlane} \small  An example of a three-dimensional plane.}
  \end{center}
\end{figure}

We need a way to determine a plane based on the data. The difficulty
is that in most regression analysis applications, the number of
observations, $n$, far exceeds the number of observations required
to fit a plane, $k+1$. Thus, it is generally not possible to find a
single plane that passes through all $n$ observations. As in Chapter
2, we use the \emph{method of least squares} to determine a plane
from the data.

The method of least squares is based on determining the values of $
b_0^{\ast},b_1^{\ast},\ldots,b_k^{\ast}$ that minimize the quantity
\begin{equation}\label{E3:SSBasic}
SS(b_0^{\ast},b_1^{\ast},\ldots,b_k^{\ast})=\sum_{i=1}^{n}\left(
y_i-\left(
b_0^{\ast}+b_1^{\ast}x_{i1}+\ldots+b_k^{\ast}x_{ik}\right) \right)
^2.
\end{equation}
We drop the asterisk, or star, notation and use $b_0, b_1, \ldots,
b_k$ to denote the best values, known as the \emph{least squares
estimates}. With the least squares estimates, define the \emph{least
squares, or fitted, regression plane} as
\begin{equation*}
\widehat{y} = b_0 + b_1 x_1 + \ldots + b_k x_k.
\end{equation*}\index{least
squares!regression plane}\index{symbols!$b_0, b_1, \ldots, b_k$,
least squares regression coefficients}

The least squares estimates are determined by minimizing
$SS(b_0^{\ast},b_1^{\ast},\ldots,b_k^{\ast})$. It is difficult to
write down the resulting least squares estimators using a simple
formula unless one resorts to matrix notation. Because of their
importance in applied statistical models, an explicit formula for
the estimators is provided below. However, these formulas have been
programmed into a wide variety of statistical and spreadsheet
software packages. The fact that these packages are readily
available allows data analysts to concentrate on the ideas of the
estimation procedure instead of focusing on the details of the
calculation procedures.

As an example, a regression plane was fit to the Term Life data
where three explanatory variables, $x_1$ for EDUCATION, $x_2$ for
NUMHH and $x_3$ for LNINCOME, were used. The resulting fitted
regression plane is

\begin{equation}\label{E3:TermRegression}
\widehat{y} = 2.584 + 0.206 x_1 + 0.306 x_2 + 0.494 x_3.
\end{equation}


\subsubsection*{Matrix Notation}\index{symbols!$\mathbf{y}$, vector of dependent
variables}\index{symbols!$\mathbf{X}$, matrix of explanatory
variables}

Assume that the data are of the form
$(x_{i0},x_{i1},\ldots,x_{ik},y_i)$, where $i = 1, \ldots, n$. Here,
the variable $x_{i0}$ is associated with the ``intercept'' term. That
is, in most applications, we assume that $x_{i0}$ is identically
equal to 1 and thus need not be explicitly represented. However,
there are important applications where this is not the case and
thus, to express the model in general notation, it is included here.
The data are represented in matrix notation using


\begin{equation*}
\mathbf{y}=\left(
\begin{array}{l}
y_1 \\
y_2 \\
\multicolumn{1}{c}{\vdots} \\
y_n%
\end{array}%
\right) ~~~\mathrm{and}~~~\mathbf{X}=\left(
\begin{array}{cccc}
x_{10} & x_{11} & \cdots & x_{1k} \\
x_{20} & x_{21} & \cdots & x_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n0} & x_{n1} & \cdots & x_{nk}%
\end{array}%
\right) .
\end{equation*}%
Here, \textbf{y} is the $n\times 1$ vector of responses and
\textbf{X} is the $n\times (k+1)$ matrix of explanatory variables.
We use the matrix algebra convention that lower and upper case bold
letters represent vectors and matrices, respectively. (If you need
to brush up on matrices, review Section 2.11.)

\linejed

\textbf{Example: Term Life Insurance - Continued.} Recall that $y$
represents the logarithmic face, $x_1$ for years of education, $x_2$
for number of household members and $x_3$ for logarithmic income.
Thus, there are $k=3$ explanatory variables and $n=275$ households.
The vector of responses and the matrix of explanatory variables are:

\scalefont{0.80}
\begin{equation*}
\mathbf{y}=\left(
\begin{array}{l}
y_1 \\
y_2 \\
\multicolumn{1}{c}{\vdots} \\
y_{275}%
\end{array}%
\right) =\left(
\begin{array}{c}
9.904 \\
11.775 \\
\vdots \\
9.210
\end{array}%
\right) ~~~\mathrm{and}~~~\mathbf{X}=\left(
\begin{array}{cccc}
1 & x_{11} & x_{12} & x_{13} \\
1 & x_{21} & x_{22} & x_{23}\\
\vdots & \vdots & \vdots &\vdots\\
1 & x_{275,1} & x_{275,2} & x_{275,3}%
\end{array}%
\right) =\left(
\begin{array}{cccc}
1 & 16 & 3  & 10.669\\
1 & 9 & 3 & 9.393\\
\vdots & \vdots & \vdots &\vdots\\
1 & 12 & 1 & 10.545%
\end{array}%
\right) .
\end{equation*}
\scalefont{1.25}


\noindent For example, for the first observation in the data set,
the dependent variable is $y_1$=9.904 (corresponding to
$\textrm{exp}(9.904)= \$ 20,000$), for a survey respondent with 16
years of education living in a household with 3 people with
logarithmic income of 10.669 ($\exp (10.669)= \$ 43,000)$.

\linejed

Under the least squares estimation principle, our goal is to choose
the coefficients $b_0^{\ast},b_1^{\ast},\ldots,b_k^{\ast}$ to
minimize the sum of squares function
$SS(b_0^{\ast},b_1^{\ast},\ldots,b_k^{\ast})$. Using calculus, we
return to equation (\ref{E3:SSBasic}), take partial derivatives with
respect to each coefficient and set these quantities equal to zero:
\begin{equation*}
\frac{\partial }{\partial
b_j^{\ast}}SS(b_0^{\ast},b_1^{\ast},\ldots,b_k^{\ast})=\sum_{i=1}^{n}\left(
-2x_{ij}\right) \left( y_i-\left(
b_0^{\ast}+b_1^{\ast}x_{i1}+\ldots+b_k^{\ast}x_{ik}\right) \right)
=0,~~~\mathrm{for}~~j=0,1,\ldots .,k.
\end{equation*}
This is a system of $k+1$ equations and $k+1$ unknowns that can be readily
solved using matrix notation, as follows.

We may express the vector of parameters to be minimized as
$\mathbf{b}^{\ast}=(b_0^{\ast},b_1^{\ast},\ldots,b_k^{\ast})^{\prime}$.
Using this,
the sum of squares can be written as $SS(\mathbf{b}^{\ast})=(\mathbf{y-Xb}%
^{\ast})^{\prime}(\mathbf{y-Xb}^{\ast}).$ Thus, in matrix form, the
solution to the minimization problem can be expressed as $(\partial
/\partial \mathbf{b}^{\ast})SS(\mathbf{b}^{\ast})=\mathbf{0}$. This
solution satisfies the \emph{normal equations}
\begin{equation}\label{E3:NormalEquations}
\mathbf{X^{\prime}Xb}=\mathbf{X}^{\prime}\mathbf{y}.
\end{equation}
Here, the asterisk notation (*) has been dropped to denote the fact that $%
\mathbf{b}=(b_0,b_1,\ldots,b_k)^{\prime}$ represents the best vector
of values in the
sense of minimizing $SS(\mathbf{b}^{\ast})$ over all choices of $\mathbf{b}%
^{\ast}$.\index{symbols!$\mathbf{b}$, vector of regression
coefficients}\index{normal equations}

The least squares estimator $\mathbf{b}$ need not be unique.
However, assuming that the explanatory variables are not linear
combinations of one another, we have that $\mathbf{X^{\prime}X}$ is
invertible. In this case, we can write the unique solution as
\begin{equation}\label{E3:LSEstimates}
\mathbf{b}=\left( \mathbf{X^{\prime}X}\right) ^{-1}\mathbf{X}^{\prime}%
\mathbf{y}.
\end{equation}

\noindent To illustrate, for the Term Life example, equation
(\ref{E3:TermRegression}) yields
\begin{equation*}
\mathbf{b} = \left(
\begin{array}{c}
b_0 \\ b_1 \\ b_2 \\ b_3 \\
\end{array}
\right) = \left(
\begin{array}{c}
2.584 \\ 0.206 \\ 0.306 \\ 0.494 \\
\end{array}
\right).
\end{equation*}

\section{Linear Regression Model and Properties of Estimators}

In the previous section, we learned how to use the method of least
squares to fit a regression plane with a data set. This section
describes the assumptions underpinning the regression model and some
of the resulting properties of the regression coefficient
estimators. With the model and the fitted data, we will be able to
draw inferences about the sample data set to a larger population.
Moreover, we will later use these regression model assumptions to
help us improve the model specification in Chapter 5.

\subsection{Regression Function}

Most of the assumptions of the multiple linear regression model will
carry over directly from the basic linear regression model
assumptions introduced in Section 2.2. The primary difference is
that we now summarize the relationship between the response and the
explanatory variables through the \emph{regression
function}\index{regression function}

\begin{equation}\label{E3:MLRegressionFct}
\mathrm{E~}y=\beta_0 x_0+\beta_1 x_1+\ldots+\beta_k x_k,
\end{equation}%
that is linear in the parameters $\beta_0,\ldots ,\beta_k$.
Henceforth, we will use $x_0=1$ for the variable associated with the
parameter $\beta_0;$ this is the default in most statistical
packages and most applications of regression include the intercept
term $\beta_0$. The intercept is the expected value of $y$ when all
of the explanatory variables are equal to zero. Although rarely of
interest, the term $\beta_0$ serves to set the height of the fitted
regression plane.

\marginparjed{Interpret $\beta_j$ to be the expected change in $y$
per unit change in $x_j$ assuming all other explanatory variables
are held fixed.}\index{symbols!$\beta_j$, regression coefficient
associated with $x_j$}

In contrast, the other betas are typically important parameters from
a regression study. To help interpret them, we initially assume that
$x_j$ varies continuously and is not related to the other
explanatory variables. Then, we can interpret $\beta_j$ as the
expected change in $y$ per unit change in $x_j$ \emph{assuming all
other explanatory variables are held fixed}. That is, from calculus,
you will recognize that $\beta_j$ can be interpreted as a partial
derivative. Specifically, using equation (\ref{E3:MLRegressionFct}),
we have that
\begin{equation*}
\beta_j=\frac{\partial }{\partial x_j}\mathrm{E}~y.
\end{equation*}


\subsection{Regression Coefficient
Interpretation}\index{transformations!logarithmic}

Let us examine the regression coefficient estimates from the Term
Life Insurance example and focus initially on the \emph{sign} of the
coefficients. For example, from equation (\ref{E3:TermRegression}),
the coefficient associated with NUMHH is $b_2 = 0.306>0$. If we
consider two households that have the same income and the same level
of education, then the larger household (in terms of NUMHH) is
expected to demand \textit{more} term life insurance under the
regression model. This is a sensible interpretation, larger
households have more dependents for which term life insurance can
provide needed financial assets in the event of the untimely death
of a breadwinner. The positive coefficient associated with income
($b_3 = 0.494$) is also plausible; households with larger incomes
have more disposable dollars to purchase insurance. The positive
sign associated with EDUCATION ($b_1 = 0.206)$ is also reasonable,
more education suggests that respondents are more aware of their
insurance needs, other things being equal.

You will also need to interpret the \emph{amount} of the regression
coefficient. Consider first the EDUCATION coefficient. Using
equation (\ref{E3:TermRegression}), fitted values of
$\widehat{LNFACE}$ were calculated by allowing EDUCATION to vary and
keeping NUMHH and LNINCOME fixed at the sample averages. The results
are:
\begin{center}
\scalefont{0.9}
\begin{tabular}{lrrrr}
\hline \multicolumn{5}{c}{\textit{Effects of Small Changes in Education}} \\
\hline
 EDUCATION                &         14 &       14.1 &       14.2 &       14.3 \\
 $\widehat{LNFACE}$       &     11.883 &     11.904 &     11.924 &     11.945 \\
   $\widehat{FACE}$       &    144,803 &    147,817 &    150,893 &    154,034 \\
$\widehat{FACE}$ \% Change&            &      2.081 &      2.081 &      2.081 \\
\hline
\end{tabular}
\scalefont{1.1111}
\end{center}

\noindent As EDUCATION increases,  $\widehat{LNFACE}$ increases.
Further, the amount of $\widehat{LNFACE}$ increase is a constant
0.0206. This comes directly from equation (\ref{E3:TermRegression});
as EDUCATION increases by 0.1 years, we expect the demand for
insurance to increase by 0.0206 logarithmic dollars, holding NUMHH
and LNINCOME fixed. This interpretation is correct but most product
development directors are not overly fond of logarithmic dollars. To
return to dollars, fitted face values can be calculated through
exponentiation as $ \widehat{FACE}=\textrm{exp}(\widehat{LNFACE})$.
Moreover, the percentage change can be computed; for example,
$100*(147,817/144,803 - 1) \approx 2.08\% $. This provides another
interpretation of the regression coefficient; as EDUCATION increases
by 0.1 years, we expect the demand for insurance to increase by
2.08\%. This is a simple consequence of calculus using $ \partial
\textrm{ln} ~y /
\partial x  = \left(\partial y / \partial x \right) / y$; that is, a
small change in the logarithmic value of $y$ equals a small change
in $y$ as a proportion of $y$. It is because of this calculus result
that we use natural logs instead of common logs in regression
analysis. Because this table uses a discrete change in EDUCATION,
the 2.08\% differs slightly from the continuous result $0.206 \times
(\mathrm{change~in~EDUCATION}) = 2.06\%$. However, this proximity is
usually regarded as suitable for interpretation purposes.

Continuing this logic, consider small changes in logarithmic income.

\begin{center}
\scalefont{0.9}
\begin{tabular}{lrrrr}
\hline \multicolumn{5}{c}{\textit{Effects of Small Changes in Logarithmic Income}} \\
 \hline
  LNINCOME &         11 &       11.1 &       11.2 &       11.3 \\
    INCOME &     59,874 &     66,171 &     73,130 &     80,822 \\
INCOME \% Change  &            &      10.52 &      10.52 &      10.52 \\
\hline
 $\widehat{LNFACE}$ &     11.957 &     12.006 &     12.055 &     12.105 \\
 $\widehat{FACE}$ &    155,831 &    163,722 &    172,013 &    180,724  \\
$\widehat{FACE}$ \% Change &            &       5.06 &       5.06 &       5.06 \\
\hline
$\widehat{FACE}$ \% Change / INCOME \% Change &            &       0.482 &      0.482 &      0.482 \\
\hline
\end{tabular}
\scalefont{1.1111}
\end{center}

\index{actuarial \& financial terms and concepts!elasticity}


\noindent We can use the same logic to interpret the LNINCOME
coefficient in equation (\ref{E3:TermRegression}). As logarithmic
income increases by 0.1 units, we expect the demand for insurance to
increase by 5.06\%. This takes care of logarithmic units in the $y$
but not the $x$. We can use the same logic to say that as
logarithmic income increases by 0.1 units, INCOME increases by
10.52\%. Thus, a 10.52\% change in INCOME corresponds to a 5.06\%
change in FACE. Summarizing, we say that, holding NUMHH and
EDUCATION fixed, we expect that a 1\% increase in INCOME is
associated with a 0.482\% increase in $\widehat{FACE}$ (as before,
this is close to the parameter estimate $b_3 = 0.494$). The
coefficient associated with income is known as an \emph{elasticity}
in economics. In economics, elasticity is the ratio of the percent
change in one variable to the percent change in another variable.
Mathematically, we summarize this as
\begin{equation*}
\frac{\partial \textrm{ln} ~y}{\partial \textrm{ln} ~x} =
\left(\frac{\partial ~y}{y}\right)/\left(\frac{\partial
~x}{x}\right).
\end{equation*}
