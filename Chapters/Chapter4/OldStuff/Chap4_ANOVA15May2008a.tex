\setcounter{chapter}{3}

\chapter{Categorial Explanatory Variables}


{\small \textit{Chapter Preview}. \textit{Categorical variables}
allow us to group observations into distinct categories. This
chapter shows how to incorporate categorical variables into
regression functions using binary variables, thus considerably
widening the scope of potential applications for regression
analysis. Statistical inference for several coefficients is
introduced in this chapter to allow analysts to make decisions about
categorical variables (as well as other important applications).
Categorical explanatory variables also provide the basis of
\textit{ANOVA} models, representations which are equivalent to
regression in some circumstances that permit easier interpretation
and analysis.}

\section{The Role of Binary Variables}

\textit{Categorical variables} provide labels for observations to
denote membership in distinct groups, or categories. A binary
variable is a special case of a categorical variable. To illustrate,
a binary variable may tell us whether or not someone has health
insurance. A categorical variable could tell us whether someone has
(i) private individual health insurance, (ii) private group
insurance, (iii) public insurance or (iv) no health insurance.

For categorical variables, there may or may not be an ordering of
the groups. For health insurance, it is difficult to say which is
``larger,'' private individual versus public health insurance (such
as Medicare). However, for education, we may group individuals from
a dataset into ``low,'' ``intermediate'' and ``high'' years of
education. In this case, there is an ordering among groups; this
ordering may or may not provide information about the dependent
variable. \textit{Factor} is another term used for a (unordered)
categorical explanatory variable.

The most direct way of handling categorical variables in regression
is through the use of binary variables. A categorical variable with
$c$ levels can be represented using $c$ binary variables, one for
each category. For example, from a categorical education variable,
we could code $c$=3 binary variables: (1) a variable to indicate low
education, (2) one to indicate intermediate education and (3) one to
indicate high education. These binary variables are often known as
\emph{dummy variables}. In regression analysis with an intercept
term, we use only $c$-1 of these binary variables. The remaining
variable enters implicitly through the intercept term.

Through the use of binary variables, we do not make use of the
ordering of categories within a factor. Because no assumption is
made regarding the ordering of the categories, for the model fit it
does not matter which variable is dropped with regard to the fit of
the model. However, it does matter for the interpretation of the
regression coefficients. Consider the following example.

\linejed

\bigskip

\textbf{Example - Car Prices.} Motor Trend's \textit{1993 New Car
Buyer's Guide} provides information on 173 new cars, including the
price, horsepower and the type of car. Here, we consider LNPRICE,
the natural logarithm of the car price, as the response variable of
interest. We use HP, the car's horsepower, as a continuous
explanatory variable. Presumably, consumers are willing to pay more
for more powerful cars, and HP is a standard industry measure of a
car's power. We also consider CARCLASS, the car class, where there
are $c=5$ different types of cars. The variable CARCLASS is
categorical, where 0 means Convertible, 1 means Coupe, 2 means
Hatchback, 4 means Sedan and 5 means Mini-Van.

We begin by summarizing each continuous variable in Table
\ref{T4:SumStatsCarPrice}. The next step is to display the
distribution of the continuous variables. Figure
\ref{F4:CarPriceBox} shows the box plot for logarithmic car prices.
Here the box captures the middle 50\% of the data and the so-called
``whiskers'' capture the middle 80\%. This figure shows that the
dependent variable is approximately symmetric (which is not true of
the price before taking logarithms).

\scalefont{0.9}  \begin{center}  \begin{table}[h]
\caption{\label{T4:SumStatsCarPrice} Summary Statistics of Each
Continuous Variable}
\begin{tabular}{lcccccc}
\hline
& Number & Mean & Median & Standard & Minimum & Maximum \\
&  &  &  & deviation &  &  \\ \hline
LNPRICE & $173$ & $9.80$ & $9.70$ & $0.60$ & $8.81$ & $11.612$ \\
HP & $173$ & $147$ & $134$ & $60$ & $55$ & $400$ \\ \hline
\end{tabular}

\end{table}  \end{center}  \scalefont{1.1111}



To summarize the categorical variable and its relation to the
response variable, Table \ref{T4:CarPricebyType} provides summary
statistics by car type.

\scalefont{0.9}  \begin{center}  \begin{table}[h]
\caption{\label{T4:CarPricebyType} Summary Statistics of Logarithmic
Price By Car Type}
\begin{tabular}{lcccc}
\hline
& CARCLASS & Number & Mean & Standard \\
&  &  &  & deviation \\ \hline
Convertible & $0$ & $10$ & $10.46$ & $0.77$ \\
Coupe & $1$ & $46$ & $9.89$ & $0.67$ \\
Hatchback & $2$ & $19$ & $9.29$ & $0.47$ \\
Sedan & $4$ & $79$ & $9.83$ & $0.53$ \\
Mini-Van & $5$ & $19$ & $9.63$ & $0.23$ \\
All &  & $173$ & $9.80$ & $0.60$ \\ \hline
\end{tabular}
\end{table}  \end{center}  \scalefont{1.1111}

\noindent Here, we see that most observations are sedans and coupes
and that these car types have similar average prices. The mini-vans
are priced similarly to the sedans and coupes, yet have relatively
little variation about the average price. The convertibles display
the highest average price and the highest variability of prices.
Many of these observations can also be seen in Figure
\ref{F4:CarPriceType} which is a box plot of logarithmic price by
car type. By using the box and whiskers to capture the majority of
the data, the viewer can get a quick sense of the distribution.

\begin{figure}[htp]
    \includegraphics[width=1\textwidth,angle=270,scale=0.45]{Chapter4/Fig4_1.ps}
    $~~~$
    \includegraphics[width=1\textwidth,angle=270,scale=0.45]{Chapter4/Fig4_2.ps}    \hfill
      \parbox[t]{2.5in}{\caption{\label{F4:CarPriceBox} \small  Box plot of car price in logarithmic units. \textit{Source: Motor Trend's
1993 New Car Buyer's Guide}}} \hfill
        \parbox[t]{2.5in}{ \caption{\label{F4:CarPriceType} \small  Box plot of Logarithmic price by car type}}
\end{figure}


Both Table \ref{T4:CarPricebyType} and Figure \ref{F4:CarPriceType}
show that the type of car seems important for explaining price. Is
this also true of horsepower? Figure \ref{F4:CarPriceLetter} shows
the answer to be a resounding ``Yes!"\ by exhibiting a strong
relationship between LNPRICE and HP. The correlation coefficient
turns out to be 87.2\%.

Also in Figure \ref{F4:CarPriceLetter} you will notice that letter
coding was used to plot symbols. In this way, we are able to look at
the three variables simultaneously. Unfortunately, for this
application, adding the letter coding produced little additional
information. The letter coding does show that the high priced cars
are convertibles and coupes. You should be aware of the potential
for using more sophisticated graphing techniques such as letter
plots and also realize that they do not always succeed.

\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=1\textwidth,angle=270,scale=0.75]{Chapter4/Fig4_3.ps}
    \caption{\label{F4:CarPriceLetter} \small  Letter plot of logarithmic prices versus horsepower. Here, the letter codes
are `C' for convertible, `K' for coupe, `H' for hatchback, `S' for
sedan and `M' for mini-van.}
  \end{center}
\end{figure}


Are the continuous and categorical variables jointly important
determinants of response? To answer this, a regression was run using
LNPRICE as the response, HP as an explanatory variable and four
binary variables of the car class. Here, we define C to be a binary
variable for convertibles so that $C=1$ if the car is a convertible
and $C=0$ for the other car types. Similarly, define the binary
variables K for coupe, H for hatchback, S for sedan and M for
mini-van.

Display 4.3 summarizes the results of a regression run using HP, C,
K, H and S as explanatory variables. From the ANOVA Table, we see
that the independent variables explain a good deal of the price
variability. For
example, the proportion of variability explained is $%
R^{2}=48.0304/62.1070=77.3\%$ . Further, the $t$-ratios for HP show
that it is a significant explanatory variable because
$t(b_{HP})=21.2$ is very large.

\scalefont{0.9}  \begin{center}  \begin{table}[h]
\begin{tabular}{cccc}
\multicolumn{2}{l}{ANOVA Table} &  &  \\ \hline
Source & Sum of & \textit{df} & Mean \\
& Squares &  & Square \\ \hline
\multicolumn{1}{l}{Regression} & $48.0304$ & $5$ & $9.6061$ \\
\multicolumn{1}{l}{Error} & $14.0765$ & $167$ & $0.0843$ \\
\multicolumn{1}{l}{Total} & $62.1070$ & $172$ &  \\ \hline
\end{tabular}%
\begin{tabular}{cccc}
\multicolumn{2}{l}{Coefficient Estimates} &  &  \\ \hline
Explanatory & Coefficient & Standard & t-ratio \\
Variable &  & Error &  \\ \hline
\multicolumn{1}{l}{Constant} & \multicolumn{1}{l}{$8.55326$} &
\multicolumn{1}{l}{$0.08383$} & \multicolumn{1}{l}{$102.04$} \\
\multicolumn{1}{l}{HP} & \multicolumn{1}{l}{$0.008379$} &
\multicolumn{1}{l}{$0.0003954$} & \multicolumn{1}{l}{$21.2$} \\
\multicolumn{1}{l}{C} & \multicolumn{1}{l}{$0.124$} & \multicolumn{1}{l}{$%
0.118$} & \multicolumn{1}{l}{$1.05$} \\
\multicolumn{1}{l}{K} & \multicolumn{1}{l}{$0.033$} & \multicolumn{1}{l}{$%
0.080$} & \multicolumn{1}{l}{$0.41$} \\
\multicolumn{1}{l}{H} & \multicolumn{1}{l}{$-0.180$} & \multicolumn{1}{l}{$%
0.094$} & \multicolumn{1}{l}{$-1.90$} \\
\multicolumn{1}{l}{S} & \multicolumn{1}{l}{$0.042$} & \multicolumn{1}{l}{$%
0.745$} & \multicolumn{1}{l}{$0.57$} \\ \hline
\end{tabular}

\caption{\label{T4:ANOVACarPrice} ANOVA table and coefficient
estimates for Example 4.1}

\end{table}  \end{center}  \scalefont{1.1111}

From Table \ref{T4:ANOVACarPrice}, we see that the fitted regression
equation is

\begin{equation*}
\widehat{y}=8.55326+0.008379HP+0.124C+0.033K-0.18H+0.042S.
\end{equation*}

Thus, for example, for a convertible with HP = 200, we would predict
the logarithmic price to be

\begin{equation*}
\widehat{y}%
=8.55326+0.008379(200)+0.124(1)+0.033(0)-0.18(0)+0.042(0)=10.35306,
\end{equation*}

\noindent which corresponds to $e^{10.35306}$ = \$31,353. If,
however, the car were a mini-van with HP = 200, we would predict the
logarithmic price to be

\begin{equation*}
\widehat{y}%
=8.55326+0.008379(200)+0.124(0)+0.033(0)-0.18(0)+0.042(0)=10.22906.
\end{equation*}

\noindent The difference between these two estimates is 0.124, the
coefficient associated with convertibles. Thus, we may interpret
$b_{C}=0.124$ to be the estimated expected price difference between
a convertible and a mini-van.

For the model fit in the ANOVA table, it does not matter which
variable is dropped with regard to the fit of the model. However, it
does matter for the interpretation of the regression coefficients.
To illustrate, the regression model was re-run with HP as a
continuous explanatory variable and C, K, S and M as binary
explanatory variables. The analysis of variance table is the same as
given in Table \ref{T4:ANOVACarPrice}. The coefficient estimates are
given in Table 4.4. Unlike Table \ref{T4:ANOVACarPrice}, we see that
almost all of the $t$-ratios of the binary variables are now
statistically significant, in that they exceed two in absolute
value. Does this mean that the car type is now much more important
by retaining these four  binary  variables?

No, the $t$-ratios in Table 4.4 are for comparing each of the
variables with the omitted hatchback variable. The significant
$t$-ratios mean that each car type is priced significantly higher
than the hatchback. This is to be expected from our preliminary
examination of the data in Table \ref{T4:CarPricebyType}, that
indicates that hatchbacks were the least expensive type of car.
Further, Table \ref{T4:CarPricebyType} also shows that mini-vans are
close to being in the middle of the price range. Thus, when we
examined the summary of the regression fit in Table
\ref{T4:ANOVACarPrice}, we saw that some car types were more highly
priced, some lower, but none were significantly different than the
mini-van type.

\begin{table}[h]
\scalefont{0.9} \caption{\label{T4:CarPriceRegression} Coefficient
Estimates of a Regression of Car Price}
\begin{tabular}{lccc}
\hline Explanatory variable & Coefficient & Standard error &
$t$-ratio
\\ \hline
Constant & $8.37338$ & $0.07950$ &$104.3$ \\
HP & $0.008379$ & $0.0003954$ & $21.2$ \\
C  & $0.304$ & $ 0.121$ & $2.53$ \\
K  & $0.219$ & $0.082$ & $2.62$ \\
S  & $0.222$ & $0.076$ & $2.94$ \\
M  & $0.180$ & $ 0.094$ & $1.90$ \\
 \hline
\end{tabular}

{\small All binary variables are significantly different from the
hatchbacks, \\
 the omitted binary variable}

\linetjed
 \scalefont{1.1111}
\end{table}




\section{Statistical Inference for Several Coefficients}

In many applications, it is useful to examine several regression
coefficients at the same time. For example, we have already seen the
regression function in equation (3.5) expressed as a linear
combination of regression coefficients. As another example, when
assessing the effect of a categorical variable with $c$ levels, we
need to say something jointly about the $c-1$ binary variables that
enter the regression equation. To do this, Section 4.2.1 introduces
the idea of \emph{sets} of regression coefficients using matrix
algebra. Section 4.2.2 shows applications in the context of
hypothesis testing and Section 4.2.3 presents other inference
applications.


\subsection{Sets of Regression Coefficients}

Recall that our regression coefficients are specified by
$\boldsymbol \beta =\left( \beta _{0}, \beta _{1}, \dots,\beta
_{k}\right) ^{\prime },$ a $(k+1)\times 1$ vector. It will be
convenient to express linear combinations of the regression
coefficients using the notation $\mathbf{C} \boldsymbol \beta,$
where \textbf{C} is a $p\times (k+1)$ matrix that is user-specified
(depending on the application). To demonstrate the broad variety of
applications in which sets of regression coefficients can be used,
we now present a series of special cases.

Some applications involve estimating $\mathbf{C} \boldsymbol \beta$.
Others involve testing whether $\mathbf{C} \boldsymbol \beta$ equals
a specific known value (denoted as \textbf{d}). We call
$H_{0}:\mathbf{C \boldsymbol \beta =d}$ the \emph{general linear
hypothesis}.

\textbf{Special Case 1 - One Regression Coefficient}. In Section
3.4, we investigated the importance of a single coefficient, say
$\beta_j.$ We may express this coefficient as $\mathbf{C}
\boldsymbol \beta$ by choosing $p=1$ and \textbf{C} to be a $1\times
(k+1$) vector with a one in the $(j+1)st$ column and zeros
otherwise. These choices result in
\begin{equation*}
\mathbf{C \boldsymbol \beta =}\left( 0~...~0~1~0~...~0\right) \left(
\begin{array}{c}
\beta _{0} \\
\vdots  \\
\beta _{k}%
\end{array}
\right) =\beta_j.
\end{equation*}

\textbf{Special Case 2 - Regression Function}. Here, we choose $p=1$
and \textbf{C} to be a $1\times (k+1$) vector representing the
transpose of a set of explanatory variables. These choices result in

\begin{equation*}
\mathbf{C \boldsymbol \beta =}\left(x_0,x_1, \ldots, x_k \right)
\left(
\begin{array}{c}
\beta _0 \\
\vdots  \\
\beta _k
\end{array}
\right) = \beta _0 x_0 + \beta_1 x_1 +\ldots + \beta_k x_k =
\mathrm{E} ~y.
\end{equation*}


\textbf{Special Case 3 - Linear Combination of Regression
Coefficients}. When $p=1$, we use the convention that lower-case
bold letters are vectors and let $\mathbf{C = c^{\prime}}=
\left(c_0, \ldots, c_k \right)^{\prime}$. In this case, $\mathbf{C}
\boldsymbol \beta$ is a generic linear combination of regression
coefficients

\begin{equation*}
\mathbf{C} \boldsymbol \beta =\mathbf{c}^{\prime} \boldsymbol \beta
= c_0 \beta_0 + \ldots + c_k \beta_k.
\end{equation*}

\bigskip

\textbf{Special Case 4 - Testing Equality of Regression
Coefficients}. Suppose that the interest is in testing $H_{0}:
\beta_1 = \beta_2.$ For this purpose, let $p=1$,
$\mathbf{c}^{\prime}= \left(0,1, -1, 0, \ldots, 0\right),$ and
\textbf{d}=0. With these choices, we have

\begin{equation*}
\mathbf{C \boldsymbol \beta = c^{\prime} \boldsymbol \beta}=
\left(0,1, -1, 0, \ldots, 0\right) \left(
\begin{array}{c}
\beta _0 \\
\vdots  \\
\beta _k
\end{array}
\right) =\beta_1 - \beta_2 = 0,
\end{equation*}

\noindent so that $H_{0}: \beta_1 = \beta_2.$


\textbf{Special Case 5 - Adequacy of the Model}. It is customary in
regression analysis to present a test of whether or not \emph{any}
of the explanatory variables are useful for explaining the response.
Formally, this is a test of the null hypothesis $H_{0}:\beta
_{1}=\beta _{2}=...=\beta _{k}=0$. Note that, as a convention, one
does not test whether or not the intercept
is zero. To test this using the general linear hypothesis, we choose $p=k$, $%
\mathbf{d=}\left( 0~...~0\right) ^{\prime }$ to be a $k\times 1$
vector of zeros and $\mathbf{C}$ to be a $k\times (k+1)$ matrix such
that
\begin{equation*}
\mathbf{C \boldsymbol \beta =}\left(
\begin{array}{ccccc}
0 & 1 & 0 & \cdots  & 0 \\
0 & 0 & 1 & \cdots  & 0 \\
\vdots  & \vdots  & \vdots  & \ddots  & \vdots  \\
0 & 0 & 0 & \cdots  & 1%
\end{array}%
\right) \left(
\begin{array}{c}
\beta _{0} \\
\vdots  \\
\beta _{k}%
\end{array}%
\right) =\left(
\begin{array}{c}
\beta _{1} \\
\vdots  \\
\beta _{k}%
\end{array}%
\right)  =\left(
\begin{array}{c}
0 \\
\vdots  \\
0
\end{array}%
\right) =\mathbf{d}.
\end{equation*}


\textbf{Special Case 6 - Testing Portions of the Model.} Suppose
that we are interested in comparing a \emph{full} regression
function

\begin{equation*}
\mathrm{E~}y=\beta _{0}+\beta _{1}x_{1}...+\beta _{k}x_{k}+\beta
_{k+1}x_{k+1}+...+\beta _{k+p}x_{k+p}
\end{equation*}%
to a \emph{reduced} regression function,%
\begin{equation*}
\mathrm{E~}y=\beta _{0}+\beta _{1}x_{1}...+\beta _{k}x_{k}.
\end{equation*}%
Beginning with the full regression, we see that if the null
hypothesis $H_{0}:\beta _{k+1}=...=\beta _{k+p}=0$ holds, then we
arrive at the reduced regression. To illustrate, the variables
$x_{k+1}, \ldots, x_{k+p}$ may refer to several binary variables
representing a categorial variable and our interest is in whether
the categorial variable is important. To test the importance of the
categorical variable, we want to see whether the binary variables
$x_{k+1}, \ldots, x_{k+p}$ \emph{jointly} affect the dependent
variables.

To test this using the general linear hypothesis, we choose
$\mathbf{d}$ and
$\mathbf{C}$ such that%
\begin{equation*}
\mathbf{C\boldsymbol \beta =}\left(
\begin{array}{ccccccc}
0 & \cdots  & 0 & 1 & 0 & \cdots  & 0 \\
0 & \cdots  & 0 & 0 & 1 & \cdots  & 0 \\
\vdots  & \vdots  & \vdots  & \vdots  & \vdots  & \ddots  & \vdots  \\
0 & \cdots  & 0 & 0 & 0 & \cdots  & 1%
\end{array}%
\right) \left(
\begin{array}{c}
\beta _{0} \\
\vdots  \\
\beta _{k} \\
\beta _{k+1} \\
\vdots  \\
\beta _{k+p}%
\end{array}%
\right) =\left(
\begin{array}{c}
\beta _{k+1} \\
\vdots  \\
\beta _{k+p}%
\end{array}%
\right) =\left(
\begin{array}{c}
0 \\
\vdots  \\
0
\end{array}%
\right) =\mathbf{d}.
\end{equation*}

The additional variables do not need to be the last $p$ in your
regression run. Dropping $x_{k+1},...,x_{k+p}$ is for notational
convenience only. From a list of $k+p$ variables
$x_{1},...,x_{k+p}$, you may drop any $p$ that you deem appropriate.


\subsection{The General Linear Hypothesis}

To recap, the general linear hypothesis can be stated as
$H_{0}:\mathbf{C \boldsymbol \beta =d}$. Here, $\mathbf{C}$ is a
$p\times (k+1)$ matrix, $\mathbf{d}$ is a $p\times 1$ vector and
both $\mathbf{C}$ and $\mathbf{d}$ are user specified and depend on
the application at hand. Although $k+1$ is the number of regression
coefficients, $p$ is the number of restrictions under $H_{0}$ on
these coefficients. (For those readers with knowledge of advanced
matrix algebra, $p$ is the rank of $\mathbf{C}$.) This null
hypothesis is tested against the alternative $H_{a}:\mathbf{C
\boldsymbol \beta \neq d}$. This may be obvious, but we do require
$p \leq k+1$ because we can not test more constraints than free
parameters.

To understand the basis for the testing procedure, we first recall
some of the basic properties of the regression coefficient
estimators described in Section 3.3. Now, however, our goal is to
understand properties of the linear combinations of regression
coefficients specified by $\mathbf{C\boldsymbol \beta } $. An
obvious estimator of this quantity is $\mathbf{Cb}$. It is easy to
see that $\mathbf{Cb}$ is an unbiased estimator of
$\mathbf{C\boldsymbol \beta }$, because $
\mathrm{E~}\mathbf{Cb=C}\mathrm{E~}\mathbf{b=C\boldsymbol \beta }$.
Moreover, the
variance is $\mathrm{Var}\left( \mathbf{Cb}\right) \mathbf{=C}\mathrm{Var}%
\left( \mathbf{b}\right) \mathbf{C}^{\prime }=\sigma
^{2}\mathbf{C}\left( \mathbf{XX}\right) ^{-1}\mathbf{C}^{\prime }$.
To assess the difference between $\mathbf{d}$, the hypothesized
value of $\mathbf{C \boldsymbol \beta }$, and its
estimated value, $\mathbf{Cb}$, we use the following statistic%
\begin{equation}
F-ratio=\frac{(\mathbf{Cb-d)}^{\prime }\left( \mathbf{C}\left( \mathbf{XX}%
\right) ^{-1}\mathbf{C}^{\prime }\right) ^{-1}(\mathbf{Cb-d)}}{ps_{full}^{2}}%
.  \label{E4:GenLinHypF-ratio}
\end{equation}%
Here, $s_{full}^{2}$ is the mean square error from the full
regression model. Using the theory of linear models, it can be
checked that the statistic $F$-ratio has an $F$-distribution with
numerator degrees of freedom $df_{1}=p$ and denominator degrees of
freedom $df_{2}=n-(k+1)$ (see Goldberger, 1991, for a proof). Both
the statistic and the theoretical distribution are named for R. A.
Fisher, a renowned scientist and statistician who did much to
advance statistics as a science in the early half of the twentieth
century.

Like the normal and the $t$-distribution, the $F$-distribution is a
continuous distribution. The $F$-distribution is the sampling
distribution for the $F$-ratio and is proportional to the ratio of
two sum of squares, each of which is positive or zero. Thus, unlike
the normal distribution and the $t$-distribution, the
$F$-distribution takes on only nonnegative values. Recall that the
$t$-distribution is indexed by a single degree of freedom parameter.
The $F$-distribution is indexed by two degree of freedom
parameters: one for the numerator, $df_{1}$, and one for the denominator, $%
df_{2}$.

The test statistic in equation (\ref{E4:GenLinHypF-ratio}) is
complex in form. Fortunately, there is an alternative that is
simpler to implement and to interpret; this alternative is based on
the \emph{extra sum of squares principle}.

\bigskip

\boxedjed

\textit{Procedure for Testing the General Linear Hypothesis}.

\begin{enumerate}
\item Run the full regression and get the error sum of squares and mean
square error, which we label as $(Error~SS)_{full}$ and
$s_{full}^{2}$, respectively.

\item Consider the model assuming the null hypothesis is true. Run a
regression with this model and get the error sum of squares, which we label $%
(Error~SS)_{reduced}$.

\item Calculate
\begin{equation}\label{E4:FratioErrSumSquares}
F-ratio=\frac{(Error~SS)_{reduced}-(Error~SS)_{full}}{ps_{full}^{2}}.
\end{equation}

\item Reject the null hypothesis in favor of the alternative if the $F$
-ratio exceeds an $F$-value. The $F$-value is a percentile from the
$F$-distribution with $df_{1}=p$ and $df_{2}=n-(k+1)$ degrees of
freedom. The percentile is one minus the significance level of the
test. Following our notation with the $t$-distribution, we denote
this percentile as $F_{p,n-(k+1),1-\alpha }$, where $\alpha$ is the
significance level.
\end{enumerate}
\end{boxedminipage}

\bigskip

To understand the extra sum of squares principle, recall that the
error sum of squares for the full model is determined to be the
minimum value of
\begin{equation*}
SS(b_{0}^{\ast },...,b_k^{\ast })=\sum_{i=1}^{n}\left( y_{i}-\left(
b_{0}^{\ast }+...+b_k^{\ast }x_{i,k}\right) \right) ^{2}.
\end{equation*}

\noindent Here, $SS(b_{0}^{\ast },...,b_{k}^{\ast })$ is a function
of $b_{0}^{\ast },...,b_{k}^{\ast }$ and $(Error~SS)_{full}$ is the
minimum over all
possible values of $b_{0}^{\ast },...,b_{k}^{\ast }$. Similarly, $%
(Error~SS)_{reduced}$ is the minimum error sum of squares under the
constraints in the null hypothesis. Because there are fewer
possibilities under the null hypothesis, we have that

\begin{equation}\label{E4:DropErrorSS}
(Error~SS)_{full}\leq (Error~SS)_{reduced}.
\end{equation}

To illustrate, consider our first special case where $H_0 : \beta_j
= 0$. In this case, the difference between the full and reduced
models amounts to dropping a variable. A consequence of equation
(\ref{E4:DropErrorSS}) is that, when adding variables to a
regression model, the error sum of squares never goes up (and, in
fact, usually goes down). Thus, adding variables to a regression
model always increases $R^2,$ the coefficient of determination.

How large a decrease in the error sum of squares is statistically
significant? Intuitively, one can view the $F$-ratio as the
difference in the error sum of squares divided by the number of
constraints, $((Error~SS)_{reduced}-(Error~SS)_{full})/p,$ and then
rescaled by the best estimate of the variance term, the $s^{2},$
from the full model. Under the null hypothesis, this statistic
follows an $F$-distribution and we may compare the test statistic to
this distribution to see if it is unusually large.

Using the relationship $Regression~SS=Total~SS-Error~SS$, we can
re-express the difference in the error sum of squares as
\scalefont{0.9}
\begin{equation*}
(Error~SS)_{reduced}-(Error~SS)_{full}=(Regression~SS)_{full}-(Regression~SS)_{reduced}.
\end{equation*} \scalefont{1.1111}
This difference is known as a \emph{Type III Sum of Squares}. When
testing the importance of a set of explanatory variables,
$x_{k+1},...,x_{k+p},$ in the presence of $x_{1},...,x_{k}$, you
will find that many statistical software packages compute this
quantity directly in a single regression run. The advantage of this
is it allows the analyst to perform an $F$-test using a single
regression run, instead of two regression runs as in our four-step
procedure described above.

\linejed

\textbf{Example.} Before discussing the logic and the implications
of the $F$-test, let's illustrate the use of it. Consider our
taxpayer example, described in Example 3.3. This illustrates our
Special Case 3 of the general linear hypothesis -- testing portions
of the model. Suppose that we are examining a 1990 sample of $n=65$
returns prepared by a local branch office of a national tax
preparation service. We are working with the full regression model
in equation (3.9) and wish to compare it to the reduced regression
model in equation (3.10). Thus, the number of variables that we
consider dropping is $p=3$.

\begin{enumerate}
\item We begin by running the full regression and get $%
(Error~SS)_{full}=401.61$ and $s_{full}^{2}$ = 7.046.

\item We next run the reduced regression model to get $%
(Error~SS)_{reduced}=504.88.$

\item We calculate the test statistic
\begin{equation*}
F-ratio=\frac{504.88-401.61}{3(7.046)}=4.886.
\end{equation*}

\item Using a 5\% level of significance, it turns out that the 95th
percentile from an $F$-distribution with $df_{1}=3$ and $df_{2}=57$
is approximately $F$-value $\approx $ 2.766. Thus, we reject the
null hypothesis $H_{0}:\beta _{14}=\beta _{24}=\beta _{34}=0$. This
suggests that it is important to have separate regression functions
for married and single filers.
\end{enumerate}

To illustrate the test for the adequacy of the model, consider the
data summarized in the ANOVA Table \ref{T3:Decisions} and assume a
significance level at 5 percent. From Table \ref{T3:Decisions}, the
$F$-ratio is 0.2874 / 0.0102 = 28.18. With $df_1$ = 2 and $df_2$ =
33, we have that the $F$-value is approximately 3.30. This leads us
to reject the notion that the MILES and FOOTAGE variables are not
useful in understanding rent per square foot, reaffirming what we
learned in the graphical and correlation analysis. Any other result
would be surprising.

\linejed

\subsubsection*{Some Special Cases}

The general linear hypothesis test is available whenever you can
express one model as a subset of another. For this reason, it useful
to think of it as a device for comparing ``smaller'' to ``larger''
models. However, the smaller model must be a subset of the larger
model. For example, the general linear
hypothesis test cannot be used to compare the regression functions $\mathrm{%
E~}y=\beta _{0}+\beta _{7}x_{7}$ versus $\mathrm{E~}y=\beta
_{0}+\beta _{1}x_{1}+\beta _{2}x_{2}+\beta _{3}x_{3}+\beta
_{4}x_{4}$. This is because the former, smaller function is not a
subset of the latter, larger function.

The general linear hypothesis can be used in many instances,
although its use is not always necessary. For example, suppose that
we wish to test $H_{0}:\beta _{k}=0$. We can already seen that this
null hypothesis can be examined using the $t$-ratio test. In this
special case, it turns out that $(t-ratio)^{2}=F-ratio$. Thus, these
tests are equivalent
for testing $H_{0}:\beta _{k}=0$ versus $H_{a}:\beta _{k}\neq 0$. The $F$%
-test has the advantage that it works for more than one predictor
whereas the $t$-test has the advantage that one can consider
one-sided alternatives. Thus, both tests are considered useful.

Dividing the numerator and denominator of equation (\ref
{E4:FratioErrSumSquares}) by $Total~SS$, the test statistic can also
be written
as:%
\begin{equation}
F-ratio=\frac{\left( R_{full}^{2}-R_{reduced}^{2}\right) /p}{\left(
1-R_{full}^{2}\right) /(n-(k+1))}.  \label{FratioRsquare}
\end{equation}%
The interpretation of this expression is that the $F$-ratio measures
the drop in the coefficient of determination, $R^{2}$.

The expression in equation (\ref{E4:FratioErrSumSquares}) is
particularly useful for testing the adequacy of the model, our
Special Case 5. In this case, $p=0$, and the regression sum of
squares under the reduced model is
zero. Thus, we have%
\begin{equation*}
F-ratio=\frac{\left( (Regression~SS)_{full}\right) /k}{s_{full}^{2}}=\frac{%
(Regression~MS)_{full}}{(Error~SS)_{full}}.
\end{equation*}

\noindent This test statistic is a regular feature of the ANOVA
table for many statistical packages.

Again, dividing by $Total~SS$, we may write
\begin{equation*}
F-ratio=\frac{R^{2}}{1-R^{2}}\frac{n-(k+1)}{k}.
\end{equation*}%
Because both $F$-ratio and $R^{2}$ are measures of model fit, it
seems intuitively plausible that they be related in some fashion. A
consequence of this relationship is the fact that as $R^{2}$
increases, so does the $F$-ratio and vice versa. The $F$-ratio is
used because its sampling distribution is known under a null
hypothesis so we can make statements about statistical significance.
The $R^{2}$ measure is used because of the easy interpretations
associated with it.

\subsection{Estimating and Predicting Several Coefficients}

\subsubsection*{Estimating Linear Combinations of Regression Coefficients}

In some applications, the main interest is to estimate a linear
combination of regression coefficients. To illustrate, recall in
Example 3.6 that we developed a regression function for an
individual's charitable contributions ($y$) in terms of their wages
($x$). In this function, there was an abrupt discontinuity at
$x=55,500$. To model this, we defined the binary variable $z$ to be
zero if  $x<55,500$ and to be one if $x\geq 55,500$ and the
regression function $\mathrm{E~}y=\beta _{0}+\beta _{1}x+\beta
_{2}z(x-55,500)$. Thus, the marginal expected change in
contributions per dollar wage change for wages in excess of 55,500
is $\partial \left( \mathrm{E~}y\right) /\partial x=\beta _{1}+\beta
_{2}$.

To estimate $\beta _{1}+\beta _{2}$, a reasonable estimator is
$b_{1}+b_{2}$ which is readily available from standard regression
software. In addition, we would also like to compute standard errors
for $b_{1}+b_{2}$ to be used,
for example, in determining a confidence interval for $\beta _{1}+\beta _{2}$%
. However, $b_{1}$ and $b_{2}$ are typically correlated so that the
calculation of the standard error of $b_{1}+b_{2}$ requires
estimation of the covariance between $b_{1}$ and $b_{2}$.

Estimating $\beta_1 + \beta_2$ is an example of our Special Case 3
that considers linear combinations of regression coefficients of the
form $\mathbf{c}^{\prime }  \boldsymbol \beta=c_{0}\beta
_{0}+c_{1}\beta _{1}+...+c_{k}\beta _{k}$. For our charitable
contribution's example, we would choose $c_{1}=c_{2}=1$ and other
$c$'s equal to zero.

To estimate $\mathbf{c}^{\prime } \boldsymbol \beta $, we replace
the vector of
parameters by the vector of estimators and use $\mathbf{c}^{\prime }\mathbf{b%
}$. To assess the reliability of this estimator, as in Section
4.2.2, we have that $\mathrm{Var}\left( \mathbf{c}^{\prime
}\mathbf{b}\right) =\sigma ^{2} \mathbf{c}^{\prime
}(\mathbf{X^{\prime }X)}^{-1}\mathbf{c}$. Thus, we may define the
estimated standard deviation, or standard error, of $\mathbf{c}
^{\prime }\mathbf{b}$ to be
\begin{equation*}
se\left( \mathbf{c}^{\prime }\mathbf{b}\right) =s\sqrt{\mathbf{c}^{\prime }(%
\mathbf{X^{\prime }X)}^{-1}\mathbf{c}}.
\end{equation*}%
With this quantity, a $100(1-\alpha ) \%$ confidence interval for
$\mathbf{c}^{\prime } \boldsymbol \beta$ is
\begin{equation}  \label{E4:ConfIntLinCombination}
\mathbf{c}^{\prime }\mathbf{b}\pm t_{n-(k+1),1-\alpha /2} ~se(\mathbf{c}%
^{\prime }\mathbf{b}).
\end{equation}

The confidence interval in equation (\ref{E4:ConfIntLinCombination})
is valid under Assumptions F1-F5; see Goldberger (1991) for a proof.
If we choose $\mathbf{c}$ to have a ``1'' in the $(j+1)st$ row and
zeros otherwise, then $\mathbf{c}^{\prime }\boldsymbol \beta =\beta
_{j}$, $\mathbf{c}^{\prime }\mathbf{b=}b_{j}$ and
\begin{equation*}
se(b_{j})=s\sqrt{%
(j+1)st~diagonal~element~of~(\mathbf{X^{\prime }X)}^{-1}}.
\end{equation*} Thus, (\ref
{E4:ConfIntLinCombination}) generalizes the confidence interval for
individual
regression coefficients introduced in Section 3.4's equation (\ref{ConfIntb1}%
).

Another important application of equation
(\ref{E4:ConfIntLinCombination}) is the choice of $\mathbf{c}$
corresponding to a set of explanatory variables of interest, say,
$\mathbf{x}^{\ast }=\left( 1,x_{1}^{\ast },x_{2}^{\ast
},...,x_{k}^{\ast }\right) ^{\prime }$. These may correspond to an
observation within the data set or to a point outside the available
data. The parameter of interest, $\mathbf{c}^{\prime} \boldsymbol
\beta = \mathbf{x}^{\ast \prime} \boldsymbol \beta $, is the
expected response or the
regression function at that point. Then, $\mathbf{\mathbf{x}^{\ast }}%
^{\prime }\mathbf{b}$ provides a point estimator and equation (\ref%
{E4:ConfIntLinCombination}) provides the corresponding confidence
interval.

\subsubsection*{Prediction Intervals}

Prediction is an inferential goal that is closely related to
estimating the regression function at a point. Suppose that, when
considering charitable contributions, we know an individual's wages
(and thus whether wages are in excess of 55,500) and wish to predict
the amount of charitable
contributions. In general, we assume that the set of explanatory variables $%
\mathbf{x}^{\ast }$ is known and wish to predict the corresponding response $%
y^{\ast }$. This new response follows the assumptions as described
in
Section 3.2. Specifically, the expected response is $\mathrm{E~}y^{\ast }=%
\mathbf{\mathbf{x}^{\ast }}^{\prime }\boldsymbol \beta $, $\mathbf{\mathbf{x}%
^{\ast }}$ is nonstochastic, $\mathrm{Var~}y^{\ast }=\sigma ^{2}$,
$y^{\ast } $ is independent of $\{y_{1},...,y_{n}\}$ and is normally
distributed. Under these assumptions, a 100(1-$\alpha $)\%
prediction interval for $y^{\ast }$
is%
\begin{equation}\label{E4:PredictionInterval}
\mathbf{\mathbf{x}^{\ast }}^{\prime }\mathbf{b}\pm
t_{n-(k+1),1-\alpha /2} ~s
\sqrt{1+\mathbf{x}^{\ast }{}^{\prime }(\mathbf{X^{\prime }X)}^{-1}\mathbf{x}%
^{\ast }}.
\end{equation}%
Equation (\ref{E4:PredictionInterval}) generalizes the prediction
interval for introduced in Section 2.4.


\section{One Factor ANOVA Model}

To establish notation for the one factor ANOVA model, we now
consider the following example.

\linejed

\textbf{Example. Hospital Charges.}\ecaptionjed{Hospital Charges} We
now study the impact of various predictors on hospital charges in
the state of Wisconsin. Identifying predictors of hospital charges
can provide direction for hospitals, government, insurers and
consumers in controlling these factors that in turn leads to better
control of hospital costs. The data for the year 1989 were obtained
from the Office of Health Care Information, Wisconsin's Department
of Health and Human Services. Cross sectional data are used, which
details the 20 diagnosis related group (DRG) discharge costs for
hospitals in the state of Wisconsin, broken down into nine major
health service areas and three types of payer (Fee for service, HMO,
and other). Even though there
are 540 potential DRG, area and payer combinations $(20\times 9\times 3=540)$%
, only 526 combinations were actually realized in the 1989 data set. Other
predictor variables included the logarithm of the total number of discharges
(NO DSCHG) and total number of hospital beds (NUM BEDS) for each
combination. The response variable is the logarithm of total hospital
charges per number of discharges (CHG\_NUM).

As before, we use the symbol $y$ to denote the response variable.
Not surprisingly, it turns out that the diagnosis-related group
(DRG) is an important determinant of costs. In this section, we
focus our analysis on this categorical variable. Thus, we use the
notation $y_{ij}$ to mean the $i$th observation of the $j$th DRG.
For this data set, $j$ may be 1, 2, \ldots , or 20. For the $j$th
DRG, we assume there are $n_{j}$ observations. There are
$n=n_{1}+n_{2}+\ldots +n_{c}$ observations. The data are:

\begin{center}
\begin{tabular}{cccccc}
Data for DRG 1 & \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  & $y_{11}$ & $y_{21}$ & $%
\ldots $ & $y_{n1,1}$ \\
Data for DRG 2 &  & $y_{12}$ & $y_{22}$ & $\ldots $ & $y_{n2,1}$ \\
. &  & $.$ & $.$ & $...$ & $.$ \\
Data for DRG $c$ &  & $y_{1c}$ & $y_{2c}$ & $\ldots $ & $y_{nc,c}$%
\end{tabular}
\end{center}

\noindent where $c=20$ is the number of levels of the DRG factor.
Because we do not assume an ordering of the levels, any system of
ordering of the DRGs is fine. Because each level of a factor can be
arranged in a single row (or column), another term for this type of
data is a one way classification. Thus, a \textit{one way model} is
another term for a one factor model.

\subsubsection*{Summarizing the Data: Hospital Charges Case Study}

An important summary measure of each level of the factor is the sample
average. We use

\begin{equation*}
\overline{y}_{j}=\frac{1}{n_{j}}\sum_{i=1}^{n_{j}}y_{ij}
\end{equation*}

\noindent to denote the average from the $j$th DRG.

To get an idea of cost by level of the factor, Figure 4.4 is a
scatter plot of $\{y_{ij}\}$ versus $\{\overline{y}_{j}\}$. This
plot illustrates several features of the data. These are:

1. First, it is clear that the average cost varies by type of DRG. For
example, it turns out that \textit{angina pectoris}, chest pains, normal
newborns and chemotherapy are relatively inexpensive diagnosis-related
groups. On the other hand, major joint and limb reattachment and psychoses
are expensive DRGs.

2. We see that the variability is about the same for each DRG. Note
that we have controlled for the frequency by working on a per
discharge basis. Further, working in logarithmic units evens out the
variability.

3. As emphasized by Levin, Sarlin and Webne-Behrman (1989), when the
horizontal and vertical axes are on the same scale, the data are
centered about a 45 degree line. This aids in interpreting the
graph. In particular, the scatter plot makes it easy to identify the
outlier for the group with average cost about 8.4. For this
particular combination of medical treatment, health service area and
type of payer, there were only two patients discharged in 1989,
compared to an average of 509 discharges. Thus, although unusual,
this point represents a relatively small amount of information about
hospital costs and should not have an undue influence in driving the
model selection.

\linejed

\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=1\textwidth,angle=270,scale=0.75]{Chapter4/Fig4_4.ps}
    \caption{\label{F4:DRGObsFits} \small  Scatter plot of responses versus average response over diagnosis-related
group (DRG). \textit{Source: Wisconsin Department of Health and
Human Services}.}
  \end{center}
\end{figure}



\subsubsection*{Model Assumptions and Analysis}

In this section, the mean $\mu_j$ is allowed to vary by the level of
the factor, denoted by \textit{j}. We can express this model as

\begin{equation*}
y_{ij}=\mu _{j}+ \varepsilon_{ij}\ \ \ \ \ \ i=1,\ldots ,n_{j},\ \ \
\ \ j=1,\ldots ,c.
\end{equation*}

This is short-hand notation for $n_{1}+n_{2}+\ldots +n_{c}=n$
equations, one for each observation. The random errors $\{
\varepsilon_{ij} \}$ are assumed to be a random sample from an
unknown population of errors. Because we assume the expected value
of each error is zero, we have E$\,y_{ij}=\mu _{j}$. Thus, we
interpret $\mu _{j}$ to be the expected value of the response
$y_{ij}$. Similarly, because we assume that the random errors have
variance $\sigma ^{2}$, we have Var$\,y_{ij}=\sigma ^{2}$. Thus, we
interpret $\sigma ^{2}$ to be the true, unknown variance of the
response. This variance is assumed to be common over all factor
levels.

To estimate the parameters $\{\mu _{j}\}$, as with regression we use
the \textit{method of least squares}, introduced in Section 3.1.
That is, let $\hat{\mu}_j$ be a candidate estimate of $\mu_{j}$. The
quantity

\begin{equation*}
SS(\hat{\mu}_{1},\ldots
,\hat{\mu}_{c})=\sum_{j=1}^{c}\sum_{i=1}^{n_{j}}(y_{ij}-\hat{\mu}_{j})^{2}
\end{equation*}%
represents the sum of squared deviations of the responses from these
candidate estimates. From straight-forward algebra, the value of
$\hat{\mu}_j$ that minimizes this sum of squares is $\bar{y}_{j}$.
Thus, $\bar{y}_{j}$ is the \textit{least squares estimate }of $\mu
_{j}$.

To understand how reliable the estimates are, we can partition the
variability as in the regression case, presented in Sections 3.3 and 4.3.
The minimum sum of squared deviations is called the \textit{error sum of
squares} and is defined to be

\begin{equation*}
\text{Error SS}=SS(\bar{y}_{1},\ldots ,\bar{y}_{c})=\sum_{j=1}^{c}%
\sum_{i=1}^{n_{j}}(y_{ij}-\bar{y}_{j})^{2}
\end{equation*}%
The total variation in the data set is summarized by the \textit{total sum
of squares}, Total $SS=\sum_{j=1}^{c}\sum_{i=1}^{n_{j}}(y_{ij}-\bar{y})^{2}$%
. The difference, called the \textit{factor sum of} \textit{squares}, can be
expressed as:

\begin{equation*}
\text{Factor SS = Total SS -- Error SS}
\end{equation*}

\begin{equation*}
=\sum_{j=1}^{c}\sum_{i=1}^{n_{j}}(y_{ij}-\bar{y})^{2}-\sum_{j=1}^{c}%
\sum_{i=1}^{n_{j}}(y_{ij}-\bar{y}_{j})^{2}=\sum_{j=1}^{c}\sum_{i=1}^{n_{j}}(%
\bar{y}_{j}-\bar{y})^{2}
\end{equation*}

\begin{equation*}
=\sum_{j=1}^{c}n_{j}(\bar{y}_{j}-\bar{y})^{2}
\end{equation*}

The last two equalities follow from algebra manipulation. The Factor
SS plays the same role as the Regression SS in Chapters 2 and 3. The
variability decomposition is summarized in the following analysis of
variance (ANOVA) table.

\scalefont{0.9}  \begin{center}  \begin{table}[h]
\caption{\label{T4:ANOVAOneFactor} ANOVA Table for One Factor Model}
\begin{tabular}{cccc}
\hline
Source & Sum of Square & \textit{df} & Mean Square \\ \hline
\multicolumn{1}{l}{Factor} & \multicolumn{1}{l}{Factor SS} &
\multicolumn{1}{l}{$c-1$} & \multicolumn{1}{l}{Factor MS} \\
\multicolumn{1}{l}{Error} & \multicolumn{1}{l}{Error SS} &
\multicolumn{1}{l}{$n-c$} & \multicolumn{1}{l}{Error MS} \\
\multicolumn{1}{l}{Total} & \multicolumn{1}{l}{Total SS} &
\multicolumn{1}{l}{$n-1$} & \multicolumn{1}{l}{} \\ \hline
\end{tabular}

\end{table}  \end{center}  \scalefont{1.1111}

The conventions for this table are the same as in the regression case. That
is, the mean squares (MS) column is defined by the sum of squares (SS)
column divided by the degrees of freedom (\textit{df}) column. Thus, $Factor$
$MS\equiv (Factor$ $SS)/(c-1)$ and $Error$ $MS\equiv (Error$ $SS)/(n-c)$. We
use

\begin{equation*}
s^{2}=\text{Error MS}=\frac{\sum_{j=1}^{c}\sum_{i=1}^{n_{j}}\hat{e}_{ij}^{2}%
}{n-c}
\end{equation*}%
to be our estimate of $\sigma ^{2}$, where $\hat{e}_{ij}=y_{ij}-\bar{y}_{j}$
is the residual. The variability in the ANOVA table is often summarized by $%
R^{2}=(Factor$ $SS)/(Total$ $SS)$, the coefficient of determination, or its
adjusted version, $R_{a}^{2}=1-s^{2}/s_{y}^{2}$, where $s_{y}^{2}=(Total$ $%
SS)/(n-1)$.


To make a formal decision as to whether the differences among
machines are real, we introduce a test of hypothesis in the one
factor model framework. The null, or working, hypothesis, is no
difference among the levels of the factors, denoted by H$_{0}$: $\mu
_{1}=\mu _{2}=\ldots =\mu _{c}$. This notation states that the null
hypothesis is equality of the means. The alternative hypothesis is
that at least some of the means differ from one another. As in
regression, we examine the test statistic \textit{F}-ratio = (Factor
MS)/(Error MS). The procedure is to reject the null hypothesis in
favor of the alternative if $F$-ratio $>$ $F$-value. Here, $F$-value
is a percentile from the \textit{F}-distribution with $df_{1}=c-1$
and $df_{2}=n-c$ degrees of freedom. The percentile is one minus
significance level of the test.

To interpret this test, recall that under H$_{0}$, we have equality of the
means so that all means $\mu _{j}$ are equal to one another and are equal
to, say, $\mu $. The sample averages are approximations to the true means.
Thus, under H$_{0}$, we expect the sample means to be close to one number, $%
\bar{y}$. To examine their separation, we look at squared differences, $(%
\bar{y}_{j}-\bar{y})^{2}$. To give levels with more observations greater
weight and look at all separations together, we examine

\begin{equation*}
\sum_{j=1}^{c}n_{j}(\bar{y}_{j}-\bar{y})^{2}=\text{Factor SS}.
\end{equation*}

The larger that Factor SS is, the less likely we will be to believe in the
null hypothesis H$_{0}$. Dividing Factor SS by $(c-1)$ and by $s^{2}$ =
Error MS is the right standardization so that we can compare to the
reference distribution, the \textit{F}-distribution.


As another example, consider the Hospital Charges example. From
Figure \ref{F4:DRGObsFits} , it seems clear that costs differ by
DRG. To make a formal statement using our test of hypothesis
machinery, some straightforward calculations yield:

\scalefont{0.9}  \begin{center}  \begin{table}[h]
\caption{\label{T4:HospChgANOVA} ANOVA Table for Hospital Charges}
\begin{tabular}{cccc}
\hline
Source & Sum of Squares & \textit{df} & Mean Square \\ \hline
DRG & \multicolumn{1}{r}{$260.09$} & \multicolumn{1}{r}{$19$} &
\multicolumn{1}{r}{$13.69$} \\
Error & \multicolumn{1}{r}{$36.54$} & \multicolumn{1}{r}{$506$} &
\multicolumn{1}{r}{$0.0722$} \\
Total & \multicolumn{1}{r}{$296.63$} & \multicolumn{1}{r}{$525$} &
\multicolumn{1}{r}{} \\ \hline
\end{tabular}

\end{table}  \end{center}  \scalefont{1.1111}

\noindent From this table, we note that DRGs have explained $R^{2}=260.09/296.63=0.877$%
, or 87.7\%, of the variability. The ``typical'' error is $s=($Error MS$%
)^{1/2}=0.27$. To conduct the test of the null hypothesis, H$_{0}$: $\mu
_{1}=\mu _{2}=\ldots =\mu _{20}$, we have $F$-ratio $=13.69/0.0722=189.6$.
From the F-table, with $df_{1}=19$, $df_{2}=$ infinity and, at the 5\% level
of significance, we have \textit{F}-value $=1.590$. Because \textit{F}-ratio
> \textit{F}-value, we reject the null hypothesis in favor of
the alternative, that there is some difference among costs of different
Diagnosis Related Groups.

Although comforting, this hypothesis test does not really tell us
anything that is not clearly evident in Figure \ref{F4:DRGObsFits} .
To supplement this information, it is useful to give estimates, and
ranges of reliability, of the cost summary measures. To this end, we
use $\bar{y}_{j}$ as our \textit{point estimate} of the parameter
$\mu _{j}$. To provide a range of reliability, the corresponding
interval estimate is

\begin{equation*}
\bar{y}_{j}\pm (t-value)\frac{s}{\sqrt{n_{j}}}.
\end{equation*}

\noindent Here, the \textit{t}-value is a percentile from the
\textit{t}-distribution with $n-c$ degrees. The percentile is 1 --
(1 -- confidence level) / 2.

To illustrate, we consider costs for the psychoses DRG, the highest cost of
the medical treatment groups. This was the $j=$ 10th DRG, and we have $\bar{y%
}_{10}=9.3267$ and $n_{10}=26$. Thus, a 95\% confidence interval for $\mu
_{10}$ is

\begin{equation*}
9.3267\pm \ (1.96)(0.27)/(26)^{1/2}=9.3267\pm \ 0.1038,\text{ or }%
(9.2229,9.4305).
\end{equation*}

Note that these estimates are in natural logarithmic units. In dollars per
discharge, our point estimate is $e^{9.3267}=\$11,234$ and our 95\%
confidence interval is $(e^{9.229},e^{9.4305})$, or (\$10,188, \$12,463).

\subsubsection*{Link with Regression and Reparameterization}

An important feature of the one factor ANOVA tests of hypotheses and
confidence intervals is the ease of computation. Although the sum of
squares appear complex, it is important to note that \emph{no matrix
calculations are required}. Rather, all of the calculations can be
done through averages and sums of squares. This been an important
consideration historically, before the age of readily available
desktop computing. Further, it also provides for direct
interpretation of the results.

In this subsection, we show how a one factor ANOVA model can be
rewritten as a regression model. Using the regression formulation,
we already have introduced many of the important statistical
inference results. For example, with this rewriting we will be able
to show that the test of equality of means is a special case of the
regression test of model adequacy. Thus, the justifications of the
tests and intervals estimates done in the regression case need not
be repeated in the ANOVA context. Further, additional inference
techniques in Chapters 5 and 6 will be available for both the
regression and ANOVA models.

To this end, for a categorical variable with $c$ levels, define $c$
binary variables, $x_{1},$ $x_{2},\ldots ,x_{c}$. Here, $x_{j}$
indicates whether or not an observation falls in the $j$th level.
With these variables, we can rewrite our one factor ANOVA model $
y=\mu _{j}+ \varepsilon$ as
\begin{equation}\label{E4:OneFactor}
y=\mu _{1}x_{1}+\mu _{2}x_{2}+\ldots +\mu _{c}x_{c}+\varepsilon.
\end{equation}
The regression model in equation (\ref{E4:OneFactor}) includes $c$
independent variables but does not include an intercept term, $\beta
_{0}$. To include an intercept term, define $\tau _{j}=\mu _{j}-\mu
$, where $\mu $ is an, as yet, unspecified parameter. Because each
observation must fall into one of the $c$ categories, we have
$x_{1}+x_{2}+\ldots +x_{c}=1$ for each observation. Thus, using $\mu
_{j} = \tau_j + \mu $ in equation (\ref{E4:OneFactor}), we have

\begin{equation}\label{E4:OneFactorTau}
y=\mu +\tau _{1}x_{1}+\tau _{2}x_{2}+\ldots +\tau
_{c}x_{c}+\varepsilon.
\end{equation}

\noindent Thus, we have re-written the model into what appears to be
our usual regression format, as in equation (\ref{E4:OneFactorTau}).

We use the $\tau $ in lieu of $\beta $ for historical reasons. ANOVA
models were invented by R.A. Fisher in connection with agricultural
experiments. Here, the typical set-up is to apply several
\textit{treatments} to plots of land in order to quantify crop yield
responses. Thus, the Greek ``t", $\tau ,$ suggests the word
treatment, another term used to described levels of the factor of
interest.

A simpler version of equation (\ref{E4:OneFactorTau}) can be given
when we identify the level of the factor. That is, if we know an
observation falls in the $j$th level, then only $x_{j}$ is one and
the other $x$'s are 0. Thus, a simpler expression for equation
(\ref{E4:OneFactorTau}) is

\begin{equation*}
y_{ij}=\mu +\tau_{j} + \varepsilon_{ij}.
\end{equation*}

Comparing equations (\ref{E4:OneFactor}) and
(\ref{E4:OneFactorTau}), we see that the number of parameters
has increased by one. That is, in equation (\ref{E4:OneFactor}), there are $c$ parameters, $%
\mu _{1},\ldots ,\mu _{c}$, even though in equation
(\ref{E4:OneFactorTau}) there are $c+1$ parameters, $\mu $ and $\tau
_{1},\ldots ,\tau _{c}$. The model in equation
(\ref{E4:OneFactorTau}) is said to be \textit{overparameterized}. To
make these two expressions equivalent, we now present two ways of
\textit{restricting} the movement of the parameters in
(\ref{E4:OneFactorTau}).

The first type of restriction, usually done in the regression context, is to
require one of the $\tau $'s to be zero. This amounts to \textit{dropping}
one of the explanatory variables. For example, we might use

\begin{equation}  \label{E4:OneFactorTauDrop}
y=\mu +\tau _{1}x_{1}+\tau _{2}x_{2}+\ldots +\tau
_{c-1}x_{c-1}+\varepsilon,
\end{equation}%
dropping $x_{c}$. With this formulation, it is easy to fit the model
in equation (\ref{E4:OneFactorTauDrop}) using regression statistical
software routines because one only needs to run the regression with
$c-1$ explanatory variables. However, one needs to be careful with
the interpretation of parameters. To equate the models in
(\ref{E4:OneFactor}) and (\ref{E4:OneFactorTau}), we need to define
$\mu \equiv \mu _{c}$ and $\tau _{j}=\mu _{j}-\mu _{c}$ for
$j=1,2,\ldots ,c-1$. That is, the regression intercept term is the
mean level of the category dropped, and each regression coefficient
is the difference between a mean level and the mean level dropped.
It is not necessary to drop the last level c, and indeed, one could
drop any level. However, the interpretation of the parameters does
depend on the variable dropped. With this restriction, the fitted
values are
$\hat{\mu}=\hat{\mu}_{c}=\bar{y}_{c}$ and $\hat{\tau}_{j}=\hat{\mu}_{j}-\hat{%
\mu}_{c}=\bar{y}_{j}-\bar{y}_{c}$. Recall that the carat $(\symbol{94})$, or
"hat", stands for an estimated, or fitted, value.

The second type of restriction, from the ANOVA context, is to interpret $\mu
$ as a mean for the entire population. To this end, the usual requirement is
$\mu \equiv (1/n)\sum_{j=1}^{c}n_{j}\mu _{j}$, that is, $\mu $ is a weighted
average of means. With this definition, we interpret $\tau _{j}=\mu _{j}-\mu
$ as treatment differences between a mean level and the population mean.
Another way of expressing this restriction is $\sum_{j=1}^{c}n_{j}\tau
_{j}=0 $, that is, the (weighted) sum of treatment differences is zero. The
disadvantage of this restriction is that it is not readily implementable
with a regression routine, and a special routine is needed. The advantage is
that there is a symmetry in the definitions of the parameters. There is no
need to worry about which variable is being dropped from the equation, an
important consideration. With this restriction, the fitted values are

\begin{equation*}
\hat{\mu}=(1/n)\sum_{j=1}^{c}n_{j}\hat{\mu}_{j}=(1/n)\sum_{j=1}^{c}n_{j}\bar{%
y}_{j}=\bar{y}\text{ \ \ and \ }\hat{\tau}_{j}=\hat{\mu}_{j}-\hat{\mu}=\bar{y%
}_{j}-\bar{y}.
\end{equation*}

\section{Combining Categorical and Continuous Explanatory Variables}

There are several ways to combine categorical and continuous
explanatory variables. We initially present the case of only one
categorical and one continuous variable. We then briefly present the
general case, called the \textit{general linear model}. When
combining categorical and continuous variable models, we use the
terminology \emph{factor} for the categorical variable and
\emph{covariate} for the continuous variable.

\subsubsection*{Combining a Factor and Covariate}

Let us begin with the simplest models that use a factor and a
covariate. In Section 4.3, we introduced the one factor model:

\begin{equation*}
y_{ij}=\mu _{j} + \varepsilon_{ij}\qquad i=1,\ldots ,n_{j},\text{
}j=1,\ldots ,c.
\end{equation*}

In Chapter 2, we introduced basic linear regression in terms of one
continuous variable, or covariate, using:

\begin{equation*}
y_{ij}=\beta _{0}+\beta _{1}x_{ij} + \varepsilon_{ij}.
\end{equation*}

\noindent To summarize different approaches for combining these
variables, Table \ref{T4:OneFactorCovariate} describes several
models that could be used to represent combinations of a factor and
covariate.

\scalefont{0.9} \begin{center}  \begin{table}[h]
\caption{\label{T4:OneFactorCovariate}  Several Models that
Represent Combinations of One Factor and One Covariate}
\begin{tabular}{ll}
\hline Model Description & Notation \\ \hline One factor ANOVA (no
covariate model) &
$y_{ij}=\mu _{j}+\varepsilon_{ij}$ \\
Regression with constant intercept and slope (no factor
model) & $y_{ij}=\beta _{0}+\beta _{1}x_{ij}+\varepsilon_{ij}$ \\
Regression with variable intercept and constant slope &
$y_{ij}=\beta _{0j}+\beta _{1}x_{ij}+\varepsilon_{ij}$ \\
~~~(analysis of covariance model) &  \\
Regression with constant intercept and variable slope &
$y_{ij}=\beta _{0}+\beta _{1j}x_{ij}+\varepsilon_{ij}$ \\
Regression with variable intercept and slope &
$y_{ij}=\beta _{0j}+\beta _{1j}x_{ij}+\varepsilon_{ij}$ \\
\hline
\end{tabular}
\end{table}  \end{center}  \scalefont{1.1111}

We can interpret the regression with variable intercept and constant
slope to be an additive model, because we are adding the factor
effect, $\beta _{0j}$, to the covariate effect, $\beta _{1}x_{ij}$.
Note that one could also use the notation, $\mu _{j}$, in lieu of
$\beta _{0,j}$ to suggest the presence of a factor effect. This is
also know as an \emph{analysis of covariance (ANCOVA) model}. The
regression with variable intercept and slope can be thought of as an
\emph{interaction model}. Here, both the intercept, $\beta _{0j}$,
and slope, $\beta _{1,j}$, may vary by level of the factor. In this
sense, we interpret the factor and covariate to be ``interacting.''
The model with constant intercept and variable slope is typically
not used in practice; it is included here for completeness. With
this model, the factor and covariate interact only through the
variable slope. Figures \ref{F4:TheoryVarIntConSlope},
\ref{F4:TheoryConIntVarSlope} and \ref{F4:TheoryVarIntVarSlope}
illustrate the expected responses of these models.



\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=1\textwidth,angle=0,scale=0.45]{Chapter4/Fig4_5A.ps}
    \includegraphics[width=1\textwidth,angle=0,scale=0.45]{Chapter4/Fig4_6A.ps} \hfill
     \parbox[t]{2.5in}{\caption{\label{F4:TheoryVarIntConSlope} \small  Plot of the expected response versus the covariate for the regression model
with variable intercept and constant slope.}} \hfill
    \parbox[t]{2.5in}{\caption{\label{F4:TheoryConIntVarSlope} \small  Plot of the expected response versus the covariate for the regression model
with constant intercept and variable slope.}}
  \end{center}
\end{figure}


\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=1\textwidth,angle=0,scale=0.5]{Chapter4/Fig4_7A.ps}
    \caption{\label{F4:TheoryVarIntVarSlope} \small  Plot of the expected response versus the covariate for the regression model
with variable intercept and variable slope.}
  \end{center}
\end{figure}

For each model presented in Table \ref{T4:OneFactorCovariate},
parameter estimates can be calculated using the method of least
squares. As usual, this means writing the expected response, E
$y_{ij}$, as a function of known variables and unknown parameters.
Then, for candidate estimates of the parameters, an error sum of
squares can be calculated and minimized over all candidate
estimates. For the regression model with variable intercept and
constant slope, the least squares estimates can be expressed
compactly as:

\begin{equation*}
b_{1}=\frac{\sum_{j=1}^{c}\sum_{i=1}^{n_{j}}(x_{ij}-\bar{x}_{j})(y_{ij}-\bar{%
y}_{j})}{\sum_{j=1}^{c}\sum_{i=1}^{n_{j}}(x_{ij}-\bar{x}_{j})^{2}}
\end{equation*}

\noindent and $b_{0,j}=\bar{y}_{j}-b_{1}\bar{x}_{j}$. Similarly, the
least squares estimates for the regression model with variable
intercept and slope can be expressed as:

\begin{equation*}
b_{1,j}=\frac{\sum_{i=1}^{n_{j}}(x_{ij}-\bar{x}_{j})(y_{ij}-\bar{y}_{j})}{%
\sum_{i=1}^{n_{j}}(x_{ij}-\bar{x}_{j})^{2}}
\end{equation*}

\noindent and $b_{0,j}=\bar{y}_{j}-b_{1}\bar{x}_{j}$. With these
parameter estimates, fitted values may be calculated.

For each model, the error sum of squares is defined as the sum of
squared deviations between the observation and the corresponding
fitted values, that is,

\begin{equation*}
\text{Error
SS}=\sum_{j=1}^{c}\sum_{i=1}^{n_{j}}(y_{ij}-\hat{y}_{ij})^{2}.
\end{equation*}

\noindent Fitted values are defined to be the expected response with
the unknown parameters replaced by their least squares estimates.
For example, for the regression model with variable intercept and
constant slope the fitted values are
$\hat{y}_{ij}=b_{0,j}+b_{1}x_{ij}$.

\linejed

\textbf{Example. Hospital Charges - Continued.} To illustrate, we
now consider the Hospital Charges case study introduced in Section
4.3. To streamline the presentation, we now consider only costs
associated with three diagnostic related groups (DRGs), DRG \#209,
DRG \#391 and DRG \#430.

The covariate, $x$, is the natural logarithm of the number of
discharges. In ideal settings, hospitals with more patients enjoy
lower costs due to economies of scale. In non-ideal settings,
hospitals may not have excess capacity and thus, hospitals with more
patients have higher costs. One purpose of this analysis is to
investigate the relationship between hospital costs and hospital
utilization.

Recall that our measure of hospital charges is the logarithm of
costs per discharge $(y)$. The scatter plot in Figure
\ref{F4:CostperNumber} gives a preliminary idea of the relationship
between $y$ and $x$. We note that there appears to be a negative
relationship between $y$ and $x$.

The negative relationship between $y$ and $x$ suggested by Figure
\ref{F4:CostperNumber} is misleading and is induced by an
\textit{omitted variable}, the category of the cost (DRG). To see
the joint effect of the categorical variable DRG and the continuous
variable $x$, in Figure \ref{F4:DRGbyNumber} is a plot of $y$ versus
$x$ where the plotting symbols are codes for the level of the
categorical variable. From this plot, we see that the level of cost
varies by level of the factor DRG. Moreover, for each level of DRG,
the slope between $y$ and $x$ is either zero or positive. The slopes
are not negative, as suggested by Figure \ref{F4:CostperNumber}.

\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=1\textwidth,angle=0,scale=0.45]{Chapter4/Fig4_8A.ps}
    \includegraphics[width=1\textwidth,angle=0,scale=0.45]{Chapter4/Fig4_9A.ps} \hfill
    \parbox[t]{2.5in}{\caption{\label{F4:CostperNumber} \small  Plot of natural logarithm of cost per discharge versus natural
logarithm of the number of discharges.}} \hfill
    \parbox[t]{2.5in}{\caption{\label{F4:DRGbyNumber} \small  Letter plot of natural logarithm of cost per discharge versus natural
logarithm of the number of discharges by DRG. Here, A is for DRG
\#209, B is for DRG \#391 and C is for DRG \#430.}}
  \end{center}
\end{figure}


\scalefont{0.7}  \begin{center}  \begin{table}[h]
\caption{\label{T4:DRGModels} Degree of Freedom and Error Sum of
Squares of Several Models to Represent One Factor and One Covariate
for the DRG Example}
\begin{tabular}{lccrcc}
\hline & Model & Error & Error  &  & Error  \\
& degrees & degrees
& Sum & $R^2$  & Mean \\
Model Description & of freedom & of freedom & of Squares & (\%) &
Square
\\ \hline
One factor ANOVA & 2 &
76 & \multicolumn{1}{r}{$9.396$} & \multicolumn{1}{r}{$%
93.3$} & \multicolumn{1}{r}{$0.124$} \\
Regression with constant intercept & 1 & 77 &
\multicolumn{1}{r}{$115.059$} &
\multicolumn{1}{r}{$18.2$} & \multicolumn{1}{r}{$1.222$} \\
~~~and slope & & & & &  \\
Regression with variable intercept &3 & 75 &
\multicolumn{1}{r}{$7.482$} &
\multicolumn{1}{r}{$94.7$} & \multicolumn{1}{r}{$0.100$} \\
~~~and constant slope &  & &  &  &  \\
Regression with constant intercept & 3 & 75 &
\multicolumn{1}{r}{$14.048$} &
\multicolumn{1}{r}{$90.0$} & \multicolumn{1}{r}{$0.187$} \\
~~~and variable slope &  &  &  &  & \\
Regression with variable intercept & 5 & 73 &
\multicolumn{1}{r}{$5.458$} &
\multicolumn{1}{r}{$96.1$} & \multicolumn{1}{r}{$0.075$} \\
~~~and slope &  & & &  &  \\
\hline
\end{tabular}

\end{table}  \end{center} \scalefont{1.4286}


Each of the five models defined in Table \ref{T4:OneFactorCovariate}
was fit to this subset of the Hospital case study. The summary
statistics are in Table \ref{T4:DRGModels}. For this data set, there
are $n=79$ observations and $c=3$ levels of the DRG factor. For each
model, the model degrees of freedom is the number of model
parameters minus one. The error degrees of freedom is the number of
observations minus the number of model parameters.


Using binary variables, each of the models in Table
\ref{T4:OneFactorCovariate} can be written in a regression format.
As we have seen in Section 4.2, when a model can be written as a
subset of another, larger model, we have formal testing procedures
available to decide which model is more appropriate. To illustrate
this testing procedure with our DRG example, from Table
\ref{T4:DRGModels} and the associated plots, it seems clear that the
DRG factor is important. Further, a $t$-test, not presented here,
shows that the
covariate $x$ is important. Thus, let's compare the full model E $%
y_{ij}=\beta _{0,j}+\beta _{1,j}x$ to the reduced model E
$y_{ij}=\beta _{0,j}+\beta _{1}x$. In other words, is there a
different slope for each DRG?

Using the notation from Section 4.2, we call the variable intercept
and slope the full model. Under the null hypothesis, $H_0:
\beta_{1,1}=\beta_{1,2}=\beta_{1,3}$, we get the variable intercept,
constant slope model. Thus, using the $F$-ratio in equation
(\ref{E4:FratioErrSumSquares}), we have

\begin{equation*}
F\text{-ratio}=\frac{(Error~SS)_{reduced}-(Error~SS)_{full}}{ps_{full}^{2}}=\frac{{7.482-5.458}}{2(0.075)}=13.535.
\end{equation*}

\noindent The 95th percentile from the $F$-distribution with
$df_1=p=2$ and $df_2=(df)_{full}=73$ is approximately 3.13. Thus,
this test leads us to reject the null hypothesis and declare the
alternative, the regression model with variable intercept and
variable slope, to be valid.

\linejed

\subsubsection*{General Linear Model}

In Section 4.1, we saw that we only use $c-1$ indicator variables to
represent a categorical variable with $c$ levels. Similarly, in
Section 4.3 we saw that the one factor ANOVA model could be
expressed as a regression model with $c$ indicator variables.
However, if we had attempted to estimate the model in equation
(\ref{E4:OneFactorTau}), the method of least squares would not have
arrived at a unique set of regression coefficient estimates. The
reason is that, in equation (\ref{E4:OneFactorTau}), each
explanatory variable can be expressed as a
linear combination of the others. For example, observe that $%
x_{c}=1-(x_{1}+x_{2}+\ldots \ +x_{c-1})$.

The fact that parameter estimates are not unique is a drawback, but
not an
overwhelming one. In fact, we now introduce the \textit{general linear model}%
,

\begin{equation} \label{E4:GeneralLinearModel}
y=\beta _{0}+\beta _{1}x_{1}+\ldots \ +\beta _{k}x_{k} +
\varepsilon,
\end{equation}

\noindent where $\{\varepsilon_{i}\}$ is a random sample from an
unknown population with mean zero. We follow standard terminology
and view the linear regression model as a special case of the
general linear model. To distinguish the two sets of models, we
assume that the explanatory variables are not linear combinations of
one another in the linear regression model context. This restriction
is not made in the general linear model case. To illustrate, the
models in equations (\ref{E4:OneFactorTau}) and (4.13) are examples
of general linear models that are not regression models.

In the linear regression model case, the assumption that the
explanatory variables are not linear combinations of one another
means that we can compute unique estimates of the regression
coefficients using the method of least squares. In the general
linear model case, the parameter estimates need not be unique.
However, an important feature of the general linear model is that
the resulting fitted values turn out to be unique, using the method
of least squares.

Specifically, suppose that we are considering the model in equation
(4.15) and, using the method of least squares, our regression
coefficient estimates are $b_{0}^{o},b_{1}^{o},\ldots ,b_{k}^{o}$.
This set of regression coefficients estimates minimizes our error
sum of squares, but there are other sets of coefficients that also
minimize the error sum of squares. The fitted values are computed as
$\hat{y}_{i}=b_{0}^{o}+b_{1}^{o}x_{i1}+\ldots \ +b_{k}^{o}x_{ik}$.
It can be shown that the resulting fitted values are unique, in the
sense that any set of coefficients that minimize the error sum of
squares produce the same fitted values.

Thus, for a set of data and a specified general linear model, fitted
values are unique. Because residuals are computed as observed
responses minus fitted values, we have that the residuals are
unique. Because residuals are unique, we have the error sums of
squares are unique. Thus, it seems reasonable, and is true, that we
can use the general test of hypotheses described in Section 4.2 to
decide whether collections of explanatory variables are important.

To summarize, for general linear models, parameter estimates are not
unique and thus not meaningful. An important part of regression
models is the interpretation of regression coefficients. This
interpretation is not necessarily available in the general linear
model context. However, for general linear models, we may still
discuss the important of an individual variable or collection of
variables through partial \textit{F}-tests. Further, fitted values,
and the corresponding exercise of prediction, works in the general
linear model context. The advantage of the general linear model
context is that we need not worry about the type of restrictions to
impose on the parameters. Although not the subject of this text,
this advantage is particularly important in complicated experimental
designs used in the life sciences. Searle (1987) is one reference
for these designs and for further details of the general linear
model. The reader will find that general linear model estimation
routines are widely available in statistical software packages
available on the market today.


\section{Two Factor ANOVA Model}

Suppose that we now wish to consider two categorical explanatory
variables, or factors. We first establish some notation in the
context of the following example.

\linejed

\textbf{Example: Machine Run Times.} In this example, the response
of interest is the length of time it takes a machine to complete a
certain benchmark test. The explanatory variables considered are the
type of machine and the type of person operating the machine. For
convenience, think of the operator as either experienced or
inexperienced (a rookie). We refer to the operator as Factor 1 and
the type of machine as Factor 2, although these designations are
interchangeable. Table \ref{T4:MachineTimes} presents the sample
data.

\scalefont{0.9}  \begin{center}  \begin{table}[h]
\caption{\label{T4:MachineTimes} Hypothetical Run Times of Three
Machines by Two Operators}
\begin{tabular}{cccc}
\hline & \multicolumn{2}{c}{Operator (Factor 1)} &  \\ \cline{2-3}
Machine (Factor 2) & Rookie & Experienced & Machine Averages \\
\hline 1 & \multicolumn{1}{r}{$14,12$} & \multicolumn{1}{r}{$10,12$}
&
\multicolumn{1}{l}{$\ \ \ \ \bar{y}_{\cdot 1\cdot }=12$} \\
2 & \multicolumn{1}{r}{$15,16$} & \multicolumn{1}{r}{$9,12$} &
\multicolumn{1}{l}{$\ \ \ \ \bar{y}_{\cdot 2\cdot }=13$} \\
3 & \multicolumn{1}{r}{$8,10$} & \multicolumn{1}{r}{$7,7$} &
\multicolumn{1}{l}{$\ \ \ \ \bar{y}_{\cdot 3\cdot }=8$} \\ \hline
Operator Average & $\bar{y}_{1..}=12.5$ & $\bar{y}_{2..}=9.5$ & $\bar{y}%
_{3..}=11$ \\ \hline
\end{tabular}

\end{table}  \end{center}  \scalefont{1.1111}


Extending the notation introduced in Section 4.3, we use $y_{ijk}$
to denote the $k$th observation of the $i$th operator of the $j$th
machine. We suppose that there are $K=2$ observations for each
combination of the $I=2$ types of operators and $c=3$ types of
machines. Thus, there are $n=IcK=2(3)2=12$ observations in total.

As in Section 4.3, an important issue that can be addressed with
this data is whether the (population) mean run times differ among
machine types. To this end, we define the average for each machine,
$j=1,2,3,$ using $ \bar{y}_{\cdot j\cdot
}=\frac{1}{IK}\sum_{i=1}^{I}\sum_{k=1}^{K}y_{ijk}\text{ }.$ Here,
the notation $\{\cdot j\cdot \}$ in the subscript means sum over $%
i=1,\ldots ,I$, leave $j$ fixed, and sum over $k=1,\ldots ,K$. One
goal of this section is to explain part of the unknown variability
in terms of the type of operator. To this end, we can define the
average for each operator, $i=1,2$, using $\bar{y}_{i\cdot \cdot
}=\frac{1}{cK}\sum_{j=1}^{c}\sum_{k=1}^{K}y_{ijk}\text{ }.$
 It is also convenient for subsequent analyses to define
the average over each combination of operator and machine
$\bar{y}_{ij\cdot }=\frac{1}{K}\sum_{k=1}^{K}y_{ijk}. $

Table \ref{T4:MachineTimes} shows that there may be a difference
between the two types of operators as well as the three types of
machines. Are the differences in sample mean run times due to
sampling variability or are they due to differences in population
mean run times? How does accounting for the type of operator help
understand the performance of different machine types? To respond to
these and related questions, we now introduce two models of
variability.

\linejed

\subsubsection*{Model Assumptions and Analysis - Additive Model}

If we wish to put two categorical, or attribute, variables together
in one model, there are two basic approaches. These are called
\textit{additive} and \textit{interaction} models, respectively. To
illustrate these two approaches, we begin with the simpler additive
model.

Using the one factor formulation in equation
(\ref{E4:OneFactorTau}), we interpret the parameter $\mu $ to be the
population mean and $\tau _{j}$ to be the difference due to the
$j$th level of Factor 2. Similarly, we introduce the parameter
$\beta _{i}$ to be interpreted as the difference due to the $i$th
level of Factor 1. With these parameters, the two factor additive
model is

\begin{equation}  \label{E4:TwoFactorTau}
y_{ijk}=\mu +\beta _{i}+\tau _{j}+\varepsilon_{ijk}
\end{equation}%
where $i=1,\ldots ,I,j=1,\ldots ,c,$ and $k=1,\ldots ,K$. The
errors, $ \{\varepsilon_{ijk}\}$, are assumed to be random,
independent draws from a common population with mean zero and
variance $\sigma ^{2}$.

As with the one factor model, we again need to impose certain
restrictions on the factor differences. So that all levels of each
factor are treated in the same fashion, we require

\begin{equation}\label{E4:Restrictions}
\beta_1 + \beta_2 + \ldots + \beta_I = 0 ~ ~\mathrm{and}~ ~\tau_1 +
\tau_2 + \ldots + \tau_c = 0.
\end{equation}

Note that in this section we do not use the number of observations
in our restrictions as we did in Section 4.3. This is because of the
fact that in this section the data are assumed to be
\textit{balanced}. That is, for each combination of levels of
Factors 1 and 2, we assume that there are an equal number,
\textit{K}, of observations available. This assumption is made
primarily in order to simplify the presentation. It is possible to
present the formulas where the number of observations may vary by
combinations of levels (see, for example, Searle, 1987). Instead, in
this chapter, we handle unbalanced data a special case of the
general linear model, introduced in Section 4.5.

The least squares parameter estimates are determined by minimizing
the sum of squares

\begin{equation*}
SS(\hat{\mu},\hat{\beta}_{1},\ldots ,\hat{\beta}_{I},
\hat{\tau}_{1},\ldots
,\hat{\tau}_{c})=\sum_{i=1}^{I}\sum_{j=1}^{c}\sum_{k=1}^{K}(y_{ijk}-(\hat{\mu}+
\hat{\beta}_{i}+\hat{\tau}_{j}))^{2}.
\end{equation*}

\noindent Here, $\hat{\mu},\hat{\beta}_{1},\ldots
,\hat{\beta}_{I},\hat{\tau}_{1},\ldots ,\hat{\tau}_{c}$ are
candidate estimates of $\mu ,\beta _{1},\ldots ,\beta _{I},\tau
_{1},\ldots ,\tau _{c}$. Minimizing this sum of squares subject to
the restrictions in equation (\ref{E4:Restrictions}), the least
squares estimates are

\begin{equation}  \label{E4:TwoFactorEstimates}
\hat{\mu}=\bar{y}_{\cdot \cdot \cdot },\text{ \ \ }\hat{\beta}_{i}=\bar{y}%
_{i\cdot \cdot }-\bar{y}_{\cdot \cdot \cdot },\text{ \ and }\hat{\tau}_{j}=%
\bar{y}_{\cdot j\cdot }-\bar{y}_{\cdot \cdot \cdot }\text{ .}
\end{equation}

Thus, the variability still unaccounted for, after the introduction
of the
parameters $\mu $, $\beta $ and $\tau $, is summarized by%
\begin{eqnarray}\label{E4:TwoFactorErrorSS}
\text{Error SS} &=&SS(\hat{\mu},\hat{\beta}_{1},\ldots ,\hat{\beta}_{I},\hat{%
\tau}_{1},\ldots ,\hat{\tau}_{c})=\sum_{i=1}^{I}\sum_{j=1}^{c}%
\sum_{k=1}^{K}(y_{ijk}-(\hat{\mu}+\hat{\beta}_{i}+\hat{\tau}_{j}))^{2}.
\notag \\
&=&\sum_{i=1}^{I}\sum_{j=1}^{c}\sum_{k=1}^{K}(y_{ijk}-\bar{y}_{i\cdot
\cdot }-\bar{y}_{\cdot j\cdot }+\bar{y}_{\cdot \cdot \cdot })^{2}
\end{eqnarray}

To account for each source of the variability, consider the
decomposition

\begin{equation}  \label{E9:TwoFactorDecomp}
y_{ijk}-\bar{y}_{\cdot \cdot \cdot }=(\bar{y}_{\cdot i\cdot
}-\bar{y}_{\cdot \cdot \cdot })+(\bar{y}_{\cdot j\cdot
}-\bar{y}_{\cdot \cdot \cdot })+(y_{ijk}-\bar{y}_{i\cdot \cdot
}-\bar{y}_{\cdot j\cdot }+\bar{y}_{\cdot \cdot \cdot }).
\end{equation}

\begin{center}
(1) \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2)\qquad\ \ \ \ \ \ \ \ (3) \ \ \ \
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (4) \ \ \ \ \ \ \ \ \
\end{center}

Interpret this equation as (1) the total deviation equals (2) the
deviation explained by Factor 1 plus (3) the deviation explained by
Factor 2 plus (4) the unexplained deviation. Squaring each side of
equation (\ref{E9:TwoFactorDecomp}) and summing over all
observations yields

\begin{equation*}
\text{Total SS = Factor 1 SS + Factor 2 SS + Error SS}.
\end{equation*}

\noindent Here, Error SS is defined in equation
(\ref{E4:TwoFactorErrorSS}) and, with equation
(\ref{E4:TwoFactorEstimates}),

\begin{subequations}
\begin{eqnarray}
\text{Total SS} &=&\sum_{i=1}^{I}\sum_{j=1}^{c}\sum_{k=1}^{K}(y_{ijk}-\bar{y}%
_{\cdot \cdot \cdot })^{2},   \label{E4:TwoFactorTotalSS}\\
\text{Factor 1 SS } &=&cK\sum_{i=1}^{I}(\bar{y}_{j\cdot \cdot }-\bar{y}%
_{\cdot \cdot \cdot })^{2}=cK\sum_{i=1}^{I}\hat{\beta}_{i}^{2}  \notag \\
\text{Factor 2 SS} &=&IK\sum_{j=1}^{c}(\bar{y}_{\cdot j\cdot }-\bar{y}%
_{\cdot \cdot \cdot })^{2}=IK\sum_{j=1}^{c}\hat{\tau}_{j}^{2}.
\notag
\end{eqnarray}
\end{subequations}

\noindent The variability decomposition is summarized in the
following analysis of variance (ANOVA) table.

\begin{center}
ANOVA Table for Two Factor Additive Model
\begin{tabular}{lccc}
\hline Source & Sum of Squares & $\mathit{df}$ & Mean Square \\
\hline
Factor 1 & Factor 1 SS & $I-1$ & Factor 1 MS \\
Factor 2 & Factor 2 SS & $c-1$ & Factor 2 MS \\
Error & Error SS & $n-(I+c-1)$ & Error MS \\
Total & Total SS & $n-1$ &  \\ \hline
\end{tabular}
\end{center}

Again, the mean squares (MS) column is defined by the sum of squares
(SS) column divided by the degrees of freedom (\textit{df}) column.
Thus, Factor 1 MS $\equiv $ (Factor 1 SS)/$(I-1)$, Factor 2 MS
$\equiv $ (Factor 2 SS)/$(c-1)$ and Error MS $\equiv $ (Error
SS)/$(n-(I+c-1))$. To understand the degrees of freedom column for
the errors, first note that there are $1+I+c$ parameters, one for
$\mu $, $I$ for $\beta $ and $c$ for $\tau $. However, there are two
restrictions on $\{\beta _{i}\}$ and $\{\tau _{j}\}$, resulting in
$I+c-1$ free parameters. Thus, the error degrees of freedom follows
the same rule as all regression models, the number of observations,
$n$, minus the number of (free) parameters, $I+c-1$.

To illustrate, Table \ref{T4:ANOVATwoFactor} presents results for
the data in Table \ref{T4:MachineTimes}.

\scalefont{0.9}  \begin{center}  \begin{table}[h]
\caption{\label{T4:ANOVATwoFactor} ANOVA Table for Two Factor
Additive Model of Hypothetical Run Times}

\begin{tabular}{lccc}
\hline Source & Sum of Squares & \textit{df} & Mean Square \\
\hline Operator (Factor 1) & 27  & 1  & 27 \\
Machine (Factor 2)         & 56  & 2  & 28 \\
Error                      & 17  & 8  & 2.12 \\
Total                      & 100 & 11 &  \\ \hline
\end{tabular}

\end{table}  \end{center}  \scalefont{1.1111}

As before, tests of hypotheses allows us to test formally for
differences among levels of each factor. For example, the notation
H$_{0}$: $\beta _{1}=\ldots =\beta _{I}=0$ stands for the null
hypothesis: all Factor 1 level mean differences are equal to zero.
In other words, this is the hypothesis that there is no difference
among levels of Factor 1. The alternative hypothesis is that at
least some of the means differ from one another. For this test, we
examine the \textit{F}-ratio=(Factor 1 MS)/(Error MS). The null
hypothesis is rejected in favor of the alternative if
$F-ratio>F-value. $ Here, the $F$-value is a percentile from the
$F$-distribution with $df_{1}=I-1$ and $df_{2}=n-(I+c-1)$ degrees of
freedom. The percentile is one - significance level. In our machine
example, with $df_{1}=1$ and $df_{2}=8$, at the 5\% significance
level, we have \textit{F}-value=5.318 from a table of the
$F$-distribution. From Table \ref{T4:ANOVATwoFactor}, we have
$\mathit{F}$-ratio $=27/2.12=12.74$. Because $12.74=F$-ratio
$>F$-value $=5.318$, we reject the null hypothesis and conclude that
there is a real difference between types of operators. This result
reinforces our examination of the data in Table
\ref{T4:MachineTimes}.

The test of hypothesis for differences among levels of Factor 2 is
similar. To summarize, consider the Table \ref{T4:TwoFactorTests}.


\scalefont{0.8}  \begin{center}  \begin{table}[h]
\caption{\label{T4:TwoFactorTests} Test of Hypothesis of Differences
Among Levels for Two Factor Additive Model}

\begin{tabular}{ccccc}
\hline & Null & Alternative & Test statistic & Degree of Freedom to
use \\
Factor & hypothesis & hypothesis & ($F$-ratio) & with the $F$-value \\
\hline 1 & H$_{0}$: $\beta _{1}=\ldots$ & ~~~H$_{a}$: At least one  & (Factor 1 MS)/ & $df_{1}=I-1,$  \\
&  $~~~~=\beta _{I}=0$ & $
~~~~\beta \neq 0$ & (Error MS) &  $df_{2}=n-(I+c-1)$\\
2 & H$_{0}$: $\tau _{1}=\ldots $  & ~~~H$_{a}$: At least one  & (Factor 2 MS)/ & $df_{1}=c-1,$  \\
& $~~~~=\tau _{c}=0$ &  ~~~~$\tau \neq 0$& (Error MS) &
$df_{2}=n-(I+c-1)$ \\ \hline
\end{tabular}

\end{table}  \end{center}  \scalefont{1.25}

For example, to test differences among types of machines, we hypothesize H$%
_{0}$: $\tau _{1}=\tau _{2}=\tau _{3}=0$. To perform the test, we
first calculate \textit{F}-ratio $=$ (Factor 2 MS)/(Error MS) =
28/2.12 = 13.21. From the \textit{F}-table with $df_{1}=2$ and
$df_{2}=8$, at the 5\% significance level, we have \textit{F}-value
= 4.459. Because $13.21>4.459$, we reject the null hypothesis that
there is no difference among machines.


\subsubsection*{Model Assumptions and Analysis - Interaction Model}

For the two factor additive model, we assumed that we could simply
add together the impact of each variable, together with a population
mean, to form the expected response. However, it may be that reality
is better represented by examining more complicated interactions
between the two factors. For example, in our hypothetical machine
example, it may be that experienced operators run certain types of
machines much faster than inexperienced operators even though, for
other types of machines, experienced operators post only marginally
faster run times.

To accommodate potential interactions, we use the model

\begin{equation}\label{E$:TwoFactorInteraction}
y_{ijk}=\mu _{ij}+\varepsilon_{ijk}.
\end{equation}

\noindent Here, $\mu_{ij}$ represents the mean response for the
$i$th level of Factor 1 and the $j$th level of Factor 2. As with
equation (\ref{E4:OneFactor}), we would like to rewrite this model
into interpretable components. To this end, define

\begin{equation*}
\text{(a) }\mu =\frac{1}{Ic}\sum_{i=1}^{I}\sum_{j=1}^{c}\mu
_{ij}\text{, \ \ (b) }\beta _{i}=(\frac{1}{c}\sum_{j=1}^{c}\mu
_{ij})-\mu \text{,}
\end{equation*}

\begin{equation*}
\text{(c) }\tau _{j}=(\frac{1}{I}\sum_{i=1}^{I}\mu _{ij})-\mu
\text{,\ \ \ (d) }(\beta \tau )_{ij}=\mu _{ij}-\beta _{i}-\tau
_{j}-\mu .
\end{equation*}

\noindent As with the additive model, $\mu $ represents the overall
mean, $\beta _{i}$ represents Factor 1 differences and $\tau _{j}$
represents Factor 2 differences. We use the term $(\beta \tau
)_{ij}$ to represent the interaction between the two factors.

By substituting the expression for $(\beta \tau )_{ij}$ into
(\ref{E$:TwoFactorInteraction}), we get

\begin{equation} \label{E4:TwoFactorInteractionComponents}
y_{ijk}=\mu +\beta _{i}+\tau _{j}+(\beta \tau
)_{ij}+\varepsilon_{ijk}.
\end{equation}%
\qquad

When comparing equations (\ref{E$:TwoFactorInteraction}) and
(\ref{E4:TwoFactorInteractionComponents}), we see that there are
$Ic$ linear parameters in equation (\ref{E$:TwoFactorInteraction})
even though there are $1+I+c+Ic$ parameters in equation
(\ref{E4:TwoFactorInteractionComponents}). As before, certain
restrictions need to be imposed on the parameters in equation
(\ref{E4:TwoFactorInteractionComponents}) so that these models are
equivalent. The restrictions adopted here are:

\begin{equation*}
\sum_{i=1}^{I}\beta _{i}=0,\text{ }\sum_{j=1}^{c}\tau _{j}=0,\text{ }%
\sum_{i=1}^{I}(\beta \tau )_{ij}=0\,,\text{ \ for each }j,
\end{equation*}

\begin{equation*}
\text{and }\sum_{j=1}^{c}(\beta \tau )_{ij}=0,\text{ \ for each }i.
\end{equation*}

\noindent These restrictions impose $I+c+1$ constraints, so that
there are $Ic$ free parameters in each expression.

Parameter estimation and partitioning the variability of the
interaction model parallel the development of the additive model.
Thus, only a brief outline is presented here. The least squares
estimates of $\mu $, $\beta _{i} $ and $\tau _{j}$ are the same as
presented in equation (\ref{E4:TwoFactorEstimates}). The least
squares estimate of $(\beta \tau )_{ij}$ turns out to be $\bar{y}_{ij\cdot }-%
\bar{y}_{i\cdot \cdot }-\bar{y}_{\cdot j\cdot }+\bar{y}_{\cdot \cdot \cdot }$%
. Partitioning the variability yields:

\begin{center}
Total SS = Factor 1 SS + Factor 2 SS + Interaction SS + Error SS.
\end{center}

Here, Total SS, Factor 1 SS and Factor 2 SS are defined in equation
(\ref{E4:TwoFactorTotalSS}) and

\begin{equation*}
\text{Interaction SS}=K\sum_{i=1}^{I}\sum_{j=1}^{c}(\bar{y}_{ij\cdot }-\bar{y%
}_{i\cdot \cdot }-\bar{y}_{\cdot j\cdot }+\bar{y}_{\cdot \cdot \cdot
})^{2}
\end{equation*}

\begin{equation*}
\text{and \ \  Error SS}=\sum_{i=1}^{I}\sum_{j=1}^{c}\sum_{k=1}^{K}(y_{ijk}-%
\bar{y}_{ij\cdot })^{2}.
\end{equation*}

These results can be summarized in the following analysis of
variance table.

\scalefont{0.9}
\begin{center}
\begin{tabular}{cccc}
\multicolumn{4}{c}{\textbf{\ }ANOVA Table for Two Factor Interaction Model} \\

\hline Source & Sum of Squares & \textit{df} & Mean Square \\ \hline
\multicolumn{1}{l}{Factor 1} & \multicolumn{1}{l}{Factor 1 SS} &
$I-1$ &
\multicolumn{1}{l}{Factor 1 MS} \\
\multicolumn{1}{l}{Factor 2} & \multicolumn{1}{l}{Factor 2 SS} &
$c-1$ &
\multicolumn{1}{l}{Factor 2 MS} \\
\multicolumn{1}{l}{Interaction} & \multicolumn{1}{l}{Interaction SS} & $%
(I-1)(c-1)$ & \multicolumn{1}{l}{Interaction MS} \\
\multicolumn{1}{l}{Error} & \multicolumn{1}{l}{Error SS} & $n-Ic$ &
\multicolumn{1}{l}{Error MS} \\
\multicolumn{1}{l}{Total} & \multicolumn{1}{l}{Total SS} & $n-1$ &
\multicolumn{1}{l}{} \\ \hline
\end{tabular}
\end{center}
\scalefont{1.1111}

\noindent We remark that the degrees of freedom for the unexplained
variability, the Error Sum of Squares, is the number of
observations, \textit{n}, minus the number of free parameters,
\textit{Ic}.

From the error degrees of freedom, we see that it is necessary to
have more than one observation for each combination of the two
factors. That is, \textit{K} must be greater than one. If \textit{K}
equals one, then the number of observations, $n=IcK$, equals the
number of parameters. In this case, the data fits the model
perfectly, there is no error, and there are no degrees of freedom
available for the error sum of squares. This is not the case in the
additive model where we may have $K=1$. This is because the error
degrees of freedom, $n-(I+c+1)=IcK-(I+c+1)$, can be greater than
zero even if $K=1$.

To test whether or not the interaction terms are important, we hypothesize H$%
_{0}$: all $(\beta \tau )_{ij}$'s $=0$ versus the alternative hypothesis H$%
_{a}$: at least one $(\beta \tau )_{ij}\neq 0$. This null hypothesis
is rejected in favor of the alternative if $F$-ratio = (Interaction
MS)/(Error MS) $>$ $F$-value, where the $F$-value is a (1 -
significance level) percentile from the $F$-distribution with
$df_{1}=(I-1)(c-1)$ and $df_{2}=n-Ic$ degrees of freedom. To
illustrate this test, consider the machine run data presented in
Table \ref{T4:MachineTimes}. Table \ref{T4:TwoFactorANOVARunTimes}
presents the analysis of variance for the two factor interaction
model fit of this example.

\scalefont{0.9}  \begin{center}  \begin{table}[h]
\caption{\label{T4:TwoFactorANOVARunTimes} ANOVA Table for Two
Factor Interaction Model of Hypothetical Run Times}
\begin{tabular}{lccc}
\hline Source & Sum of Squares & \textit{df} & Mean Square \\ \hline
Operator (Factor 1)      & 27  & 1  & 27 \\
Machine (Factor 2)       & 56  & 2  & 28 \\
Interaction              & 6   & 2  & 3 \\
Error                    & 11  & 6  & 1.83 \\
Total                    & 100 & 11 &\\ \hline
\end{tabular}
\end{table}  \end{center}  \scalefont{1.1111}

To test for the presence of significant interaction terms, we
compute
$F$-ratio $=3/1.83=1.64$. From an $F$-table with $df_{1}=2$ and $%
df_{2}=6$ degrees of freedom, at the 5\% level of significance we
have $F$-value $=5.143$. Thus, we cannot reject the null hypothesis
that the interaction effects are significantly different from zero.

It is also possible to test the hypothesis of no differences among
factor levels using the interaction model. One would simply use the
procedures outlined in Table \ref{T4:TwoFactorTests} for the
additive model but using the interaction model error mean squares
and degrees of freedom. However, the interpretation of this
decision-making procedure is not clear. Under the interaction model,
the terms $(\beta \tau )_{ij}$ represent the interaction, or joint
effect, of the $i$th level of Factor 1 and the $j$th level of Factor
2. With terms of this type present, it is difficult to interpret the
decision that either Factor 1 or Factor 2 is not important.

Deciding whether or not a factor is important may be the main goal
of the data analysis. One way to address this is to test first
whether or not the interaction terms are important. If not, as in
the machine example above, the analyst can then represent the data
using the additive model where the importance of a factor can be
tested. It is important to note that with this procedure, we are
fitting two models to the data and that the usual caveats apply.

\subsubsection*{Link with Regression}

In this subsection, we show how to connect the two factor ANOVA
models to a regression model using binary variables. To this end,
for the \textit{I} levels of Factor 1, define $x_{1,i}$ to be a one
if the observation falls in the $i$th level of Factor one and is
zero otherwise. Similarly, define $x_{2,j}$ to be an indicator of
the $j$th level of Factor 2. With this notation, we can re-express
the two factor additive model in equation (\ref{E4:TwoFactorTau}) as

\begin{equation} \label{E4:TwoFactorRegression}
y=\mu +\sum_{i=1}^{I}\beta _{i}x_{1,i}+\sum_{j=1}^{c}\tau
_{j}x_{2,j} + \varepsilon.
\end{equation}

For example, for an observation falling in the third level of Factor
1 and the fourth level of Factor 2, we have $x_{1,3}=1$, $x_{2,4}=1$
and all other $x$'s $=0$. Thus, equation
(\ref{E4:TwoFactorRegression}) reduces to $y_{23,k}=\mu +\beta
_{3}+\tau _{4}+\varepsilon_{34,k}$, as in equation
(\ref{E4:TwoFactorInteractionComponents}).

As with equation (\ref{E4:TwoFactorInteractionComponents}), certain
restriction must be applied to the parameters. In the ANOVA models,
the restriction is that the sum over levels of the parameters is
zero. For regression routines, it is more straightforward to drop an
explanatory indicator variable from each factor. Dropping the last
variable of each factor yields

\begin{equation*}
y\equiv \mu +\sum_{i=1}^{I-1}\beta _{i}x_{1,i}+\sum_{j=1}^{c-1}\tau
_{j}x_{2,j} + \varepsilon.
\end{equation*}

\noindent Here, we interpret $\mu$ to be the mean response for the
$I$th level of Factor 1 and the $c$th level of Factor 2. The
parameter $\beta _{i}$ is interpreted to be the difference in mean
responses between the $i$th and the $I$th levels of Factor 1.
Similarly, the parameter $\tau _{j}$ is interpreted to be the
difference in mean responses between the $j$th and the $c$th levels
of Factor 2. Thus, for the model fit, it does not matter which
variables are dropped from the equation. However, it does matter
when interpreting the parameters, and their resulting estimates.

The case of the two factor interaction model is similar. We can
rewrite equation (\ref{E4:TwoFactorRegression}) as

\begin{equation} \label{E4:TwoFactorRegressionInteract}
y=\mu +\sum_{i=1}^{I}\beta _{i}x1,i+\sum_{j=1}^{c}\tau
_{j}x_{2,j}+\sum_{i=1}^{I}\sum_{j=1}^{c}(\beta \tau
)_{ij}x_{1,i}x_{2,j}+ \varepsilon.
\end{equation}

\noindent Dropping one indicator variable from each factor yields
the analogous regression model

\begin{equation}  \label{E4:TwoFactorRegressionDrop}
y=\mu +\sum_{i=1}^{I-1}\beta _{i}x_{1,i}+\sum_{j=1}^{c-1}\tau
_{j}x_{2,j}+\sum_{i=1}^{I-1}\sum_{j=1}^{c-1}(\beta \tau
)_{ij}x_{1,i}x_{2,j} + \varepsilon.
\end{equation}

Again, equation (\ref{E4:TwoFactorRegressionInteract}) must be
estimated using restricted parameters. Equation
(\ref{E4:TwoFactorRegressionDrop}) provides an equivalent
formulation without the need to restrict the parameters. To
illustrate equation (\ref{E4:TwoFactorRegressionDrop}), consider our
machine example with $I=2$ and $c=3$. In this case, equation
(\ref{E4:TwoFactorRegressionDrop}) reduces to:

\begin{equation*}
y=\mu +\beta _{1}x_{1,1}+\tau _{1}x_{2,1}+\tau _{2}x_{2,2}+(\beta
\tau )_{11}x_{1,1}x_{2,1}+(\beta \tau )_{12}x_{1,1}x_{2,2}+e.
\end{equation*}

\noindent This is a multiple linear regression model with five
independent variables.
