

## Appendix A2. Matrix Algebra {-}

### Basic Definitions {-}

*   *Matrix* - a rectangular array of numbers arranged in rows and columns (the plural of matrix is matrices).

*   *Dimension* of the matrix - the number of rows and columns of the matrix.

*   Consider a matrix $\mathbf{A}$ that has dimension $m \times k$. Let $a_{ij}$ be the symbol for the number in the $i$th row and $j$th column of $\mathbf{A}$. In general, we work with matrices of the form
$$
\mathbf{A} = \left(
\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1k} \\
a_{21} & a_{22} & \cdots & a_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mk}
\end{array}
\right) .
$$

*   *Vector* - a (column) vector is a matrix containing only one row ($m=1$).

*   *Row vector* - a matrix containing only one column ($k=1$).

*   *Transpose* - transpose of a matrix $\mathbf{A}$ is defined by interchanging the rows and columns and is denoted by $\mathbf{A}^{\prime}$ (or $\mathbf{A}^{\mathrm{T}}$). Thus, if $\mathbf{A}$ has dimension $m \times k$, then $\mathbf{A}^{\prime}$ has dimension $k \times m$.

*   *Square matrix* - a matrix where the number of rows equals the number of columns, that is, $m=k$.

*   *Diagonal element* -- the number in the $r$th row and column of a square matrix, $r=1,2,\ldots$

*   *Diagonal matrix* - a square matrix where all non-diagonal numbers are equal to zero.

*   *Identity matrix* - a diagonal matrix where all the diagonal elements are equal to one and is denoted by $\mathbf{I}$.

*   *Symmetric matrix* - a square matrix $\mathbf{A}$ such that the matrix remains unchanged if we interchange the roles of the rows and columns, that is, if $\mathbf{A} = \mathbf{A}^{\prime}$. Note that a diagonal matrix is a symmetric matrix.

### Review of Basic Operations {-}

*   *Scalar multiplication*. Let $c$ be a real number, called a *scalar* (a $1 \times 1$ matrix). Multiplying a scalar $c$ by a matrix $\mathbf{A}$ is denoted by $c \mathbf{A}$ and defined by
$$
c\mathbf{A} = \left(
\begin{array}{cccc}
ca_{11} & ca_{12} & \cdots & ca_{1k} \\
ca_{21} & ca_{22} & \cdots & ca_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
ca_{m1} & ca_{m2} & \cdots & ca_{mk}
\end{array}
\right) .
$$

*   *Matrix addition and subtraction*. Let $\mathbf{A}$ and $\mathbf{B}$ be matrices, each with dimension $m \times k$. Use $a_{ij}$ and $b_{ij}$ to denote the numbers in the $i$th row and $j$th column of $\mathbf{A}$ and $\mathbf{B}$, respectively. Then, the matrix $\mathbf{C} = \mathbf{A} + \mathbf{B}$ is defined to be the matrix with the number $(a_{ij} + b_{ij})$ to denote the number in the $i$th row and $j$th column. Similarly, the matrix $\mathbf{C} = \mathbf{A} - \mathbf{B}$ is defined to be the matrix with the number $(a_{ij} - b_{ij})$ to denote the numbers in the $i$th row and $j$th column.

*   *Matrix multiplication*. If $\mathbf{A}$ is a matrix of dimension $m \times c$ and $\mathbf{B}$ is a matrix of dimension $c \times k$, then $\mathbf{C} = \mathbf{A} \mathbf{B}$ is a matrix of dimension $m \times k$. The number in the $i$th row and $j$th column of $\mathbf{C}$ is $\sum_{s=1}^c a_{is} b_{sj}$.

*   *Determinant* - a function of a square matrix, denoted by $\mathrm{det}(\mathbf{A})$, or $|\mathbf{A}|$. For a $1 \times 1$ matrix, the determinant is $\mathrm{det}(\mathbf{A}) = a_{11}$. To define determinants for larger matrices, we need two additional concepts. Let $\mathbf{A}_{rs}$ be the $(m-1) \times (m-1)$ submatrix of $\mathbf{A}$ defined by removing the $r$th row and $s$th column. Recursively, define $\mathrm{det}(\mathbf{A}) = \sum_{s=1}^m (-1)^{r+s} a_{rs} \mathrm{det}(\mathbf{A}_{rs})$, for any $r=1,\ldots,m$. For example, for $m=2$, we have $\mathrm{det}(\mathbf{A}) = a_{11}a_{22} - a_{12}a_{21}$.

*   *Matrix inverse*. In matrix algebra, there is no concept of division. Instead, we extend the concept of reciprocals of real numbers. To begin, suppose that $\mathbf{A}$ is a square matrix of dimension $m \times m$ such that $\mathrm{det}(\mathbf{A}) \neq 0$. Further, let $\mathbf{I}$ be the $m \times m$ identity matrix. If there exists a $m \times m$ matrix $\mathbf{B}$ such that $\mathbf{AB = I = BA}$, then $\mathbf{B}$ is called the *inverse* of $\mathbf{A}$ and is written as $\mathbf{B} = \mathbf{A}^{-1}$.

### Further Definitions {-}

*   *Linearly dependent vectors* -- a set of vectors $\mathbf{c}_{1},\ldots,\mathbf{c}_{k}$ is said to be linearly dependent if one of the vectors in the set can be written as a linear combination of the others.

*   *Linearly independent vectors* -- a set of vectors $\mathbf{c}_{1},\ldots,\mathbf{c}_{k}$ is said to be linearly independent if they are not linearly dependent. Specifically, a set of vectors $\mathbf{c}_{1},\ldots,\mathbf{c}_{k}$ is said to be linearly independent if and only if the only solution of the equation $x_{1}\mathbf{c}_{1} + \ldots + x_{k}\mathbf{c}_{k} = 0$ is $x_{1} = \ldots = x_{k} = 0$.

*   *Rank of a matrix* -- the largest number of linearly independent columns (or rows) of a matrix.

*   *Singular matrix* -- a square matrix $\mathbf{A}$ such that $\mathrm{det}(\mathbf{A}) = 0$.

*   *Non-singular matrix* -- a square matrix $\mathbf{A}$ such that $\mathrm{det}(\mathbf{A}) \neq 0$.

*   *Positive definite matrix* -- a symmetric square matrix $\mathbf{A}$ such that $\mathbf{x}^{\prime}\mathbf{Ax} > 0$ for $\mathbf{x} \neq 0$.

*   *Non-negative definite matrix* -- a symmetric square matrix $\mathbf{A}$ such that $\mathbf{x}^{\prime}\mathbf{Ax} \geq 0$ for $\mathbf{x} \neq 0$.

*   *Orthogonal* -- two matrices $\mathbf{A}$ and $\mathbf{B}$ are orthogonal if $\mathbf{A}^{\prime}\mathbf{B} = 0$, a zero matrix.

*   *Idempotent* -- a square matrix such that $\mathbf{AA = A}$.

*   *Trace* -- the sum of all diagonal elements of a square matrix.

*   *Eigenvalues* -- the solutions of the $n$th degree polynomial $\mathrm{det}(\mathbf{A} - \lambda \mathbf{I}) = 0$. Also known as *characteristic roots* and *latent roots*.

*   *Eigenvector* -- a vector $\mathbf{x}$ such that $\mathbf{Ax} = \lambda \mathbf{x}$, where $\lambda$ is an eigenvalue of $\mathbf{A}$. Also known as a *characteristic vector* and *latent vector*.

*   *Generalized inverse* - of a matrix $\mathbf{A}$ is a matrix $\mathbf{B}$ such that $\mathbf{ABA = A}$. We use the notation $\mathbf{A}^{-}$ to denote the generalized inverse of $\mathbf{A}$. In the case that $\mathbf{A}$ is invertible, then $\mathbf{A}^{-}$ is unique and equals $\mathbf{A}^{-1}$. Although there are several definitions of generalized inverses, the above definition suffices for our purposes. See Searle (1987) for further discussion of alternative definitions of generalized inverses.

*   *Gradient vector* -- a vector of partial derivatives. If $\mathrm{f}(.)$ is a function of the vector $\mathbf{x} = (x_1,\ldots,x_m)^{\prime}$, then the gradient vector is $\partial \mathrm{f}(\mathbf{x})/\partial \mathbf{x}$. The $i$th row of the gradient vector is $\partial \mathrm{f}(\mathbf{x})/\partial x_i$.

*   *Hessian matrix* -- a matrix of second derivatives. If $\mathrm{f}(.)$ is a function of the vector $\mathbf{x} = (x_1,\ldots,x_m)^{\prime}$, then the Hessian matrix is $\partial^2 \mathrm{f}(\mathbf{x})/\partial \mathbf{x}\partial \mathbf{x}^{\prime}$. The element in the $i$th row and $j$th column of the Hessian matrix is $\partial^{2}\mathrm{f}(\mathbf{x})/\partial x_i\partial x_j$.
