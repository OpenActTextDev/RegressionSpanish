
\chapter{Matrix Algebra}

\vspace{-1.5in}

\section{Basic Definitions}

\quad \textbullet\ \emph{Matrix} - a rectangular array of numbers
arranged in rows and columns (the plural of matrix is matrices).

\textbullet\ \emph{Dimension} of the matrix - the number of rows and columns
of the matrix..

\textbullet\ Consider a matrix $\mathbf{A}$ that has dimension
$m\times k$. Let $a_{ij}$ be the symbol for the number in the $i$th
row and $j$th column of $\mathbf{A}$. In general, we work with
matrices of the form
\begin{equation*}
\mathbf{A=}\left(
\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1k} \\
a_{21} & a_{22} & \cdots & a_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mk}
\end{array}
\right) .
\end{equation*}

\textbullet\ \emph{Vector} - a (column) vector is a matrix containing only
one row ($m=1$).

\textbullet\ \emph{Row vector} - a matrix containing only one column ($k=1$).

\textbullet\ \emph{Transpose} - transpose of a matrix $\mathbf{A}$
is defined by interchanging the rows and columns and is denoted by
$\mathbf{A}^{\prime}$ (or $\mathbf{A}^{\mathrm{T}}$). Thus, if
$\mathbf{A}$ has dimension $m\times k$, then $\mathbf{A}^{\prime}$
has dimension $k\times m$.

\textbullet\ \emph{Square matrix} - a matrix where the number of rows equals
the number of columns, that is, $m=k$.

\textbullet\ \emph{Diagonal element} -- the number in the $r$th row and
column of a square matrix, $r=1,2,\ldots $

\textbullet\ \emph{Diagonal matrix} - a square matrix where all non-diagonal
numbers are equal to zero.

\textbullet\ \emph{Identity matrix} - a diagonal matrix where all the
diagonal elements are equal to one and is denoted by $\mathbf{I}$.

\textbullet\ \emph{Symmetric matrix} - a square matrix $\mathbf{A}$
such that the matrix remains unchanged if we interchange the roles
of the rows and columns, that is, if $\mathbf{A=A}^{\prime}$. Note
that a diagonal matrix is a symmetric matrix.

\section{Review of Basic Operations}

\quad \textbullet\ \emph{Scalar multiplication}. Let $c$ be a real
number, called a \emph{scalar }(a $1 \times 1$ matrix). Multiplying
a scalar $ c$ by a matrix $\mathbf{A}$ is denoted by $c \mathbf{A}$
and defined by
\begin{equation*}
c\mathbf{A=}\left(
\begin{array}{cccc}
ca_{11} & ca_{12} & \cdots & ca_{1k} \\
ca_{21} & ca_{22} & \cdots & ca_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
ca_{m1} & ca_{m2} & \cdots & ca_{mk}
\end{array}
\right) .
\end{equation*}

\textbullet\ \emph{Matrix addition and subtraction}. Let
$\mathbf{A}$ and $ \mathbf{B}$ be matrices, each with dimension
$m\times k$. Use $a_{ij}$ and $ b_{ij}$ to denote the numbers in the
$i$th row and $j$th column of $\mathbf{A }$ and B, respectively.
Then, the matrix $\mathbf{C}=\mathbf{A}+\mathbf{B}$ is defined to be
the matrix with the number $(a_{ij}+b_{ij})$ to denote the number in
the $i$th row and $j$th column. Similarly, the matrix $\mathbf{C}=
\mathbf{A}-\mathbf{B}$ is defined to be the matrix with the number
$(a_{ij}-b_{ij})$ to denote the numbers in the $i$th row and $j$th
column.

\textbullet\ \emph{Matrix multiplication}. If $\mathbf{A}$ is a
matrix of dimension $m\times c$ and B is a matrix of dimension
$c\times k$, then $ \mathbf{C}$ = $\mathbf{A}$ $\mathbf{B}$ is a
matrix of dimension $m\times k$ . The number in the $i$th row and
$j$th column of $\mathbf{C}$ is $ \sum_{s=1}^c a_{is} b_{sj}.$

\textbullet\ \emph{Determinant} - a function of a square matrix,
denoted by $ \mathrm{det}(\mathbf{A})$, or $|\mathbf{A}|$. For a
$1\times 1$ matrix, the determinant is
$\mathrm{det}(\mathbf{A})=a_{11}$. To define determinants for larger
matrices, we need two additional concepts. Let $\mathbf{A}_{rs}$ be
the $(m-1)\times (m-1)$ submatrix of $\mathbf{A}$ defined be
removing the $r$ th row and $s$th column. Recursively, define
$\mathrm{det}(\mathbf{A}) = \sum_{s=1}^m (-1)^{r+s} a_{rs}
\mathbf{A}_{rs}$, for any $r=1,\ldots ,m$. For example, for $m=2$,
we have $\mathrm{det}(\mathbf{A)}$ = $ a_{11}a_{22}-a_{12}a_{21}$.

\textbullet\ \emph{Matrix inverse}. In matrix algebra, there is no
concept of division. Instead, we extend the concept of reciprocals
of real numbers. To begin, suppose that $\mathbf{A}$ is a square
matrix of dimension $m\times m$ such that
$\mathrm{det}(\mathbf{A})\neq 0$. Further, let $\mathbf{I}$ be the
$m\times m$ identity matrix. If there exists a $m\times m$ matrix $
\mathbf{B}$ such that $\mathbf{AB=I=BA}$, then $\mathbf{B}$ is
called the \emph{inverse} of $\mathbf{A}$ and is written as
$\mathbf{B}=\mathbf{A}^{-1}$ .\index{matrix algebra!matrix inverse}

\section{Further Definitions}

\quad \quad \textbullet\ \emph{Linearly dependent vectors} -- a set of
vectors $\mathbf{c}_{1},\ldots ,\mathbf{c}_{k}$ is said to be linearly
dependent if one of the vectors in the set can be written as a linear
combination of the others.

\textbullet\ \emph{Linearly independent vectors} -- a set of vectors
$ \mathbf{c}_{1},\ldots ,\mathbf{c}_{k}$ is said to be linearly
independent if they are not linearly dependent. Specifically, a set
of vectors $\mathbf{c} _{1},\ldots ,\mathbf{c}_{k}$ is said to be
linearly independent if and only if the only solution of the
equation $x_{1}\mathbf{c}_{1}+\ldots +x_{k} \mathbf{c}_{k}=0$ is
$x_{1}\mathbf{=}\ldots =x_{k}=0$ .

\textbullet\ \emph{Rank of a matrix} -- the largest number of linearly
independent columns (or rows) of a matrix.

\textbullet\ \emph{Singular matrix} -- a square matrix $\mathbf{A}$ such
that $\mathrm{det}(\mathbf{A})=0$.

\textbullet \emph{\ Non-singular matrix }-- a square matrix
$\mathbf{A}$ such that $\mathrm{det}(\mathbf{A})\neq 0$.

\textbullet\ \emph{Positive definite matrix} -- a symmetric square
matrix $ \mathbf{A}$ such that $\mathbf{x}^{\prime}\mathbf{Ax>}0$
for $\mathbf{x\neq 0}$.

\textbullet\ \emph{Non-negative definite matrix} -- a symmetric
square matrix $\mathbf{A}$ such that
$\mathbf{x}^{\prime}\mathbf{Ax\geq }0$ for $ \mathbf{x\neq 0}$.

\textbullet\ \emph{Orthogonal} -- two matrices $\mathbf{A}$ and
$\mathbf{B}$ are orthogonal if $\mathbf{A}^{\prime}\mathbf{B=0}$, a
zero matrix.\index{matrix algebra!orthogonal matrices}

\textbullet \emph{\ Idempotent }-- a square matrix such that $\mathbf{AA=A}$.

\textbullet\ \emph{Trace} -- the sum of all diagonal elements of a square
matrix.

\textbullet\ \emph{Eigenvalues} -- the solutions of the $n$th degree
polynomial $\mathrm{det}(\mathbf{A-}\lambda \mathbf{I})=0$. Also
known as \emph{characteristic roots} and \emph{latent
roots}.\index{matrix algebra!eigenvalue}

\textbullet\ \emph{Eigenvector }-- a vector $\mathbf{x}$ such that
$\mathbf{ Ax=}\lambda \mathbf{x}$, where $\lambda$ is an eigenvalue
of $\mathbf{A}$. Also known as a \emph{characteristic vector} and
\emph{latent vector}.

\textbullet\ \emph{Generalized inverse} - of a matrix $\mathbf{A}$
is a matrix $\mathbf{B}$ such that $\mathbf{ABA=A.}$ We use the
notation $\mathbf{ A}^{-}$ to denote the generalized inverse of
$\mathbf{A}$. In the case that $ \mathbf{A}$ is invertible, then
$\mathbf{A}^{-}$ is unique and equals $ \mathbf{A}^{-1}$. Although
there are several definitions of generalized inverses, the above
definition suffices for our purposes. See Searle (1987) for further
discussion of alternative definitions of generalized
inverses.\index{matrix algebra!generalized inverse}

\textbullet\ \emph{Gradient vector} -- a vector of partial
derivatives. If $ \mathrm{f}(.)$ is a function of the vector
$\mathbf{x}=(x_1,\ldots ,x_m)^{\prime}$, then the gradient vector is
$\partial \mathrm{f}(\mathbf{ x)}/\partial \mathbf{x}$. The $i$th
row of the gradient vector is is $\partial \mathrm{f}
(\mathbf{x)}/\partial x_i$.

\textbullet\ \emph{Hessian matrix} -- a matrix of second
derivatives. If $ \mathrm{f}(.)$ is a function of the vector
$\mathbf{x}=(x_1,\ldots ,x_m)^{\prime}$, then the Hessian matrix is
$\partial^2 \mathrm{f}( \mathbf{x)}/\partial \mathbf{x}\partial
\mathbf{x}^{\mathbf{\prime}}$. The element in the $i$th row and
$j$th column of the Hessian matrix is $\partial
^{2}\mathrm{f}(\mathbf{x)}/\partial x_i\partial x_j$.\index{matrix
algebra!Hessian}
