\setcounter{chapter}{4}

\chapter{Model Selection}

{\small \textit{Chapter Preview}. This chapter describes tools and
techniques to help you select a regression model, beginning with an
iterative model selection process. In applications with many
potential explanatory variables, automatic variable selection
procedures are available that will help you quickly evaluate many
models. Nonetheless, automatic procedures have serious limitations
including the inability to account properly for nonlinearities such
as the impact of unusual points; this chapter expands upon the
Chapter 2 discussion of unusual points. It also describes
collinearity, a common feature of regression data where explanatory
variables are linearly related to one another. Other topics needed
for model selection, including heteroscedasticity and out-of-sample
validation, are also introduced.}

\section{An Iterative Approach to Data Analysis and
Modeling}\label{S5:Iterative}

In our introduction of basic linear regression in Chapter 2, we
examined the data graphically, hypothesized a model structure, and
compared the data to a candidate model in order to formulate an
improved model. Box (1980) describes this as an \emph{iterative
process} which is shown in Figure \ref{F5:Iterative}.

 \begin{figure}[htp]
    \includegraphics[width=1\textwidth,angle=0,scale=1]{Chapter5/FigJed5_1.ps}
    \caption{\label{F5:Iterative} \small The iterative model specification process.}
\end{figure}

\marginparjed{Diagnostic checking reflects symptoms of mistakes made
in previous specification steps and provides ways to correct these
mistakes.}

This iterative process provides a useful recipe for approaching the
task of specifying a model to represent a set of data. The first
step, the model formulation stage, is accomplished by examining the
data graphically and using prior knowledge of relationships, such as
from economic theory. The second step in the iteration is based on
the assumptions of the specified model. These assumptions must be
consistent with the data to make valid use of the model. The third
step, \emph{diagnostic checking}, is also known as data and model
criticism; the data and model must be consistent with one another
before additional inferences can be made. Diagnostic checking is an
important part of the model formulation; it can reveal mistakes made
in previous steps and provide ways to correct these mistakes.

The iterative process also emphasizes the skills you need to make
regression analysis work. First, you need a willingness to summarize
information numerically and portray this information graphically.
Second, it is important to develop an understanding of model
properties. You should understand how a theoretical model behaves in
order to match a set of data to it. Third, theoretical properties of
the model are also important for inferring general relationships
based on the behavior of the data.


\section{Automatic Variable Selection
Procedures}\label{S5:Automatic}

Business and economics relationships are complicated, there are
generally many variables that could serve as useful predictors of
the response. In searching for a suitable model, there is a large
number of potential models that are based on linear combinations of
explanatory variables and an infinite number that are based on
nonlinear combinations. To search among models based on linear
combinations, several automatic procedures are available to select
variables to be included in the model. These automatic procedures
are easy to use and will suggest one or more models that you can
explore in further detail.

To illustrate how large is the potential number of linear models,
suppose that there are only four variables, $x_{1},$ $x_{2},$
$x_{3}$ and $x_{4}$, under consideration for fitting a model to $y$.
Without any consideration of multiplication or other nonlinear
combinations of explanatory variables, how many possible models are
there? Table \ref{T5:NumberModels} shows that the answer is 16.

\begin{table}[h]
\scalefont{0.9}

\caption{\label{T5:NumberModels} Sixteen Possible Models}
\begin{tabular}{llll}
\hline
E $y=\beta _{0}$ &  &  & 1 model with no independent \\
&  &  & \ \ variables \\
E $y=\beta _{0}+\beta _{1}x_{i},$ & $i=$ & $1,2,3,4$ & 4 models with one
independent \\
&  &  & \ \ variable \\
E $y=\beta _{0}+\beta _{1}x_{i}+\beta _{2}x_{j},$ & $(i,j)=$ & $%
(1,2),(1,3),(1,4),$ & 6 models with two independent \\
&  & $(2,3),(2,4),(3,4)$ & \ \ variables \\
E $y=\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{j}$ & $(i,j,k)=$ & $%
(1,2,3),(1,2,4),$ & 4 models with three independent \\
$\ \ \ \ \ \ \ \ +\beta _{3}x_{k},$ &  & $(1,3,4),(2,3,4)$ & \ \ variables
\\
E $y=\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}$ &  &  & 1 model with all
independent \\
$\ \ \ \ \ \ \ \ +\beta _{3}x_{3}+\beta _{4}x_{4}$ &  &  & \ \ variables \\
\hline
\end{tabular}
\scalefont{1.1111}
\end{table}


\noindent Now suppose there were only three explanatory variables
under consideration. Use the same logic to verify that there are
eight possible models. Extrapolating from these two examples, how
many linear models will there be if there are ten explanatory
variables? The answer is 1,024, which is quite a few. In general,
the answer is $2^k$, where $k$ is the number of explanatory
variables. For example, $2^3$ is 8, $2^4$ is 16, and so on.

In any case, for a moderately large number of explanatory variables,
there are many potential models that are based on linear
combinations of explanatory variables. We would like a procedure to
search quickly through these potential models to give us more time
to think about other interesting aspects of model selection. One
procedure for bringing explanatory variables into the model is
\textit{stepwise regression}. This procedure employs a series of
\textit{t}-tests to check the ``significance'' of explanatory
variables entered into, or deleted from, the model. The following is
a description of the basic algorithm.

\bigskip

\boxedjed
\subsubsection*{Stepwise Regression Algorithm}

Suppose that the analyst has identified one variable as the
response, $y$, and $k$ potential explanatory variables, $x_1, x_2,
\ldots, x_k$.
\begin{enumerate}

\item Consider all possible regressions using one explanatory variable.
For each of the $k$ regressions, compute $t(b_1)$, the $t$-ratio for
the slope. Choose that variable with the largest $t$-ratio. If the
\textit{t}-ratio does not exceed a pre-specified $t$-value (such as
two), then do not choose any variables and halt the procedure.

\item Add a variable to the model from the previous step. The variable to enter
is the one that makes the largest significant contribution. To
determine the
size of contribution, use the absolute value of the variable's \textit{t}%
-ratio. To enter, the $t$-ratio must exceed a specified $t$-value in
absolute value.

\item Delete a variable to the model from the previous step. The variable to be
removed is the one that makes the smallest contribution. To
determine the size of contribution, use the absolute value of the
variable's $t$-ratio. To be removed, the $t$-ratio must be less than
a specified $t$-value in absolute value.

\item Repeat steps (ii) and (iii) until all possible additions and deletions are
performed.
\end{enumerate}

\end{boxedminipage}

\bigskip


\noindent When implementing this routine, some statistical software
packages use an \textit{F-}test in lieu of $t$-tests. Recall, when
only one variable is being considered, that ($t$-ratio)$^{2}$ =
\textit{F-}ratio and thus these procedures are equivalent.

This algorithm is useful in that it quickly searches through a number of
candidate models. However, there are several drawbacks:
\begin{enumerate}

\item The procedure ``snoops'' through a large number of models and may
fit the data ``too well.''

\item There is no guarantee that the selected model is the best. The algorithm
does not consider models that are based on nonlinear combinations of
explanatory variables. It also ignores the presence of outliers and
high leverage points.

  \item In addition, the algorithm does not even search all $2^{k}$ possible
linear regressions.

  \item The algorithm uses one criterion, a \textit{t}-ratio, and does not
consider other criteria such as s, $R^{2}$, R, and so on.

  \item There is a sequence of significance tests involved. Thus, the
significance level that determines the \textit{t}-value is not
meaningful.

  \item By considering each variable separately, the algorithm does not take into
account the joint effect of explanatory variables.

  \item Purely automatic procedures may not take into account an investigator's
special knowledge.

\end{enumerate}

Simpler variants of the algorithm are available. An advantage of
these variants is that they are easier to explain and
computationally simpler (important for large data sets). These
include:

\begin{itemize}
\item Forward selection. Add one variable at a time without trying
to delete variables.

\item Backwards selection. Start with the full model and delete one variable at
a time without trying to add variables.

\end{itemize}

Many of the criticisms of the basic stepwise regression algorithm
can be addressed with modern computing software that is now widely
available. We now consider each drawback, in reverse order. To
respond to drawback number (vii), many statistical software routines
have options for forcing variables into a model equation. In this
way, if other evidence indicates that one or more variables should
be included in the model, then the investigator can force the
inclusion of these variables.

For drawback number (vi), in Section \ref{S5:Suppressor} on
\textit{suppressor variables}, we will provide examples of variables
that do not have important individual effects but are important when
considered jointly. These combinations of variables may not be
detected with the basic algorithm but will be detected with the
backwards selection algorithm. Because the backwards procedure
starts with all variables, it will detect, and retain, variables
that are jointly important.

Drawback number (v) is really a suggestion about the way to use
stepwise regression. Bendel and Afifi (1977) suggested using a
cut-off smaller than you ordinarily might. For example, in lieu of
using \textit{t}-value = 2 corresponding approximately to a 5\%
significance level, consider using \textit{t}-value = 1.645
corresponding approximately to a 10\% significance level. In this
way, there is less chance of screening out variables that may be
important. A lower bound, but still a good choice for exploratory
work, is a cut-off as small as \textit{t}-value = 1. This choice is
motivated by an algebraic result: when a variable enters a model,
$s$ will decrease if the \textit{t}-ratio exceeds one in absolute
value.

\marginparjed{When a variable enters a model, $s$ will decrease if
the \textit{t}-ratio exceeds one in absolute value.}

To address drawbacks number (iii) and (iv), we now introduce the \textit{%
best regressions }routine. Best regressions is a useful algorithm that is
now widely available in statistical software packages. The best regression
algorithm searches over all possible combinations of explanatory variables,
unlike stepwise regression, that adds and deletes one variable at a time.
For example, suppose that there are four possible explanatory variables, $%
x_{1}$, $x_{2}$, $x_{3}$ and $x_{4}$, and the user would like to know what
is the best two variable model. The best regression algorithm searches over
all six models of the form E $y$ = $\beta _{0}$ + $\beta _{1}$ $x_{i}$ + $%
\beta _{2}$ $x_{j}$. Typically, a best regression routine recommends
one or two models for each $p$ coefficient model, where \textit{p}
is a number that is user specified. Because it has specified the
number of coefficients to enter the model, it does not matter which of the criteria we use: $R^{2}$, $%
R $, or $s$.

The best regression algorithm performs its search by a clever use of
the algebraic fact that, when a variable is added to the model, the
error sum of squares does not increase. Because of this fact,
certain combinations of variables included in the model need not be
computed. An important drawback of this algorithm is that it can
take a considerable amount of time when the number of variables
considered is large.

Users of regression do not always appreciate the depth of drawback
number (i), \textit{data-snooping}. Data-snooping occurs when the
analyst fits a great number of models to a data set. We will address
the problem of data-snooping in Section \ref{S5:ModelValidation} on
model validation. Here, we illustrate the effect of data-snooping in
stepwise regression.

\linejed

\textbf{Example. Data-Snooping in Stepwise Regression.} The idea of
this illustration is due to Rencher and Pun (1980). Consider $n=100$
observations of $y$ and fifty explanatory variables, $x_1, x_2,
\ldots,x_{50}$. The data we consider here were simulated using
independent standard normal random variates. Because the variables
were simulated independently, we are working under the null
hypothesis of no relation between the response and the explanatory
variables, that is, H$_{0}$: $\beta _{1}=\beta _{2}=$ $...$ $=\beta
_{50}=0$. Indeed, when the model with all
fifty explanatory variables was fit, it turns out that $s=1.142$, $%
R^{2}=46.2\%$ and \textit{F-}ratio = (Regression MS) / (Error MS) =
0.84. Using an \textit{F-}distribution with $df_{1}=50$ and
$df_{2}=49$, the 95th percentile is 1.604. In fact, 0.84 is the 27th
percentile of this distribution, indicating that the
\textit{p-}value is 0.73. Thus, as expected, the data are in
congruence with $H_0$.

Next, a stepwise regression with \textit{t}-value = 2 was performed. Two
variables were retained by this procedure, yielding a model with $s=1.05$, $%
R^{2}=9.5\%$ and \textit{F-}ratio = 5.09. For an
\textit{F-}distribution with $df_{1}=2$ and $df_{2}=97$, the 95th
percentile is \textit{F-}value = 3.09. This indicates that the two
variables are statistically significant predictors of $y$. At first
glance, this result is surprising. The data were generated so
that $y$ is unrelated to the explanatory variables. However, because \textit{%
F-}ratio $>$ \textit{F-}value, the \textit{F-}test indicates that
two explanatory variables are significantly related to $y$. The
reason is that stepwise regression has performed many hypothesis
tests on the data. For example, in Step 1, fifty tests were
performed to find significant variables. Recall that a 5\% level
means that we expect to make roughly one mistake in 20. Thus, with
fifty tests, we expect to find $50(0.05)=2.5$ ``significant''
variables, even under the null hypothesis of no relationship between
$y$ and the explanatory variables.

To continue, a stepwise regression with \textit{t-}value = 1.645 was
performed. Six variables were retained by this procedure, yielding a
model with $s=0.99$, $R^{2}=22.9\%$ and \textit{F-}ratio = 4.61. As
before, an \textit{F-}test indicates a significant relationship
between the response and these six explanatory variables.

\marginparjed{When explanatory variables are selected using the
data, \textit{t-}ratios and \textit{F-}ratios will be too large,
thus overstating the importance of variables in the model.}

To summarize, using simulation we constructed a data set so that the
explanatory variables have no relationship with the response.
However, when using stepwise regression to examine the data, we
``found'' seemingly significant relationships between the response
and certain subsets of the explanatory variables. This example
illustrates a general caveat in model selection: when explanatory
variables are selected using the data, \textit{t-}ratios and
\textit{F-}ratios will be too large, thus overstating the importance
of variables in the model.

\linejed

\newpage

\marginparjed{A model suggested by automatic variable selection
procedures should be subject to the same careful diagnostic checking
procedures as a model arrived at by any other means.}

Stepwise regression and best regressions are examples of
\textit{automatic variable selection procedures}. In your modeling
work, you will find these procedures to be useful because they can
quickly search through several candidate models. However, these
procedures do ignore nonlinear alternatives as well as the effect of
outliers and high leverage points. The main point of the procedures
is to mechanize certain routine tasks. This automatic selection
approach can be extended and indeed, there are a number of so-called
``expert systems'' available in the market. For example, algorithms
are available that ``automatically'' handle unusual points such as
outliers and high leverage points. A model suggested by automatic
variable selection procedures should be subject to the same careful
diagnostic checking procedures as a model arrived at by any other
means.

\section{Residual Analysis}\label{S5:ResidualAnalysis}

Recall the role of a residual in the linear regression model
introduced in Section 2.6. A residual is a response minus the
corresponding fitted value under the model. Because the model
summarizes the linear effect of several explanatory variables, we
may think of a residual as a response controlled for values of the
explanatory variables. If the model is an adequate representation of
the data, then residuals should closely approximate random errors.
Random errors are used to represent the natural variation in the
model; they represent the result of an unpredictable mechanism.
Thus, to the extent that residuals resemble random errors, there
should be no discernible patterns in the residuals. Patterns in the
residuals indicate the presence of additional information that we
hope to incorporate into the model. A lack of patterns in the
residuals indicates that the model seems to account for the primary
relationships in the data.

\marginparjed{Patterns in the residuals indicate the presence of
additional information that we hope to incorporate into the model. A
lack of patterns in the residuals indicates that the model seems to
account for the primary relationships in the data.}

\subsection{Residuals}\label{S5:Residuals}

There are at least four types of patterns that can be uncovered
through the residual analysis. In this section, we discuss the first
two; residuals that are unusual and those that are related to other
explanatory variables. We then introduce the third type, residuals
that display a heteroscedastic pattern, in Section
\ref{S5:Heteroscedasticity}. In our study of time series data that
begins in Chapter 7, we will introduce the fourth type, residuals
that display patterns through time.

When examining residuals, it is usually easier to work with a \textit{%
standardized residual}, a residual that has been rescaled to be
dimensionless. We generally work with standardized residuals because
we achieve some carry-over of experience from one data set to
another and may thus focus on relationships of interest. By using
standardized residuals, we can train ourselves to look at a variety
of residual plots and immediately recognize an unusual point when
working in standard units.

There are a number of ways of defining a standardized residual.
Using $e_i = y_{i}-\hat{y}_{i}$ as the $i$th residual, here are
three commonly used definitions:

\begin{equation} \label{E5:StdResid}
\text{(a) }\frac{e_i}{s},\text{ \ \ \ (b) }\frac{e_i}{s\sqrt{%
1-h_{ii}}}, \text { \  \   \   }\text{(c)
}\frac{e_i}{s_{(i)}\sqrt{1-h_{ii}}}\text{\ }.
\end{equation}

\noindent Here, $h_{ii}$ is the $i$th leverage. It is calculated
based on values of the explanatory variables and will be defined in
Section \ref{S5:Leverage}. Recall that $ s $ is the residual
standard deviation (defined below equation 2.6). Similarly, define
$s_{(i)}$ to be the residual standard deviation when running a
regression after having deleted the $i$th observation.

Now, the first definition in (a) is simple and easy to explain. An
easy calculation shows that the sample standard deviation of the
residuals is approximately $s$ (one reason that $s$ is often
referred to as the residual standard deviation). Thus, it seems
reasonable to standardize residuals by dividing by $s$.

The second choice presented in (b), although more complex, is more
precise. Using theory from mathematical statistics, it turns out
that the variance of the $i$th residual is
\begin{equation*}
\mathrm{Var}(e_i)=\sigma ^{2}(1-h_{ii}).
\end{equation*}
Note that this variance is smaller than the variance of the error
term, Var $(\varepsilon_i)=\sigma ^{2}$. Now, we can replace $\sigma
$ by its estimate, $s$. Then, this result leads to using the
quantity $s(1-h_{ii})^{1/2}$ as an estimated standard deviation, or
standard error, for $e_i$. Thus, we define the standard error of
$e_i$ to be
\begin{equation*}
se(e_i)=s \sqrt{1-h_{ii}}.
\end{equation*}
Following the conventions introduced in Section 2.6, in this text we use $%
e_i/se(e_i)$ to be our \textit{standardized residual}.

The third choice presented in (c) is a modification of (b) and is
known as a \textit{studentized residual}. As emphasized in Section
\ref{S5:ResidualsOutliers}, one important use of residuals is to
identify unusually large responses. Now, suppose that the $i$th
response is unusually large and that this is measured through its
residual. This unusually large residual will also cause the value of
$s$ to be large. Because the large effect appears in both the
numerator and denominator, the standardized residual may not detect
this unusual response. However, this large response will not inflate
$s_{(i)}$ because it is constructed after having deleted the $i$th
observation. Thus, when using studentized residuals we get a better
measure of observations that have unusually large residuals. By
omitting this observation from the estimate of $\sigma $, the size
of the observation affects only the numerator $e_i$ and not the
denominator $s_{(i)}$.

As another advantage, studentized residuals follow a
\textit{t-}distribution with $n-(k+1)$ degrees of freedom, assuming
the errors are normally distributed (assumption E5). This knowledge
of the precise distribution helps us assess the degree of model fit,
and is particularly useful in small samples. It is this relationship
with the ``Student's'' \textit{t-}distribution that suggests the
name ``studentized'' residuals.

\subsection{Using Residuals to Identify
Outliers}\label{S5:ResidualsOutliers}

\marginparjed{A good rule of thumb is that an observation is an
outlier if the standardized residual exceeds two in absolute value.}

One important role of residual analysis is to identify outliers. An
outlier is an observation that is not well fit by the model; these
are observations where the residual is unusually large. A good rule
of thumb that is used by many statistical packages is that an
observation is marked as an outlier if the standardized residual
exceeds two in absolute value. To the extent that the distribution
of standardized residuals mimics the standard normal curve, we
expect about only one in 20 observations, or 95\%, to exceed two in
absolute value and very few observations to exceed three.

Outliers provide a signal that an observation should be investigated
to understand special causes associated with this point. An outlier
is an observation that seems unusual with respect to the rest of the
data set. It is often the case that the reason for this atypical
behavior may be uncovered after additional investigation. Indeed,
this may be the primary purpose of the regression analysis of a data
set.

Consider a simple example of so-called \textit{performance
analysis}. Suppose we have available a sample of $n$ salespeople and
are trying to understand each person's second-year sales based on
their first-year sales. To a certain extent, we expect that higher
first-year sales are associated with higher second-year sales. High
sales may be due to a salesperson's natural ability, ambition, good
territory, and so on. First-year sales may be thought of as a proxy
variable that summarizes these factors. We expect variation in sales
performance both cross-sectionally and across years. It is
interesting when one salesperson performs unusually well (or poorly)
in the second year compared to their first-year performance.
Residuals provide a formal mechanism for evaluating second-year
sales after controlling for the effects of first-year sales.

There are a number of options available for handling outliers.

\bigskip

\boxedjed

\textit{Options for Handling Outliers}
\begin{itemize}
\item  Include the observation in
the usual summary statistics but comment on its effects. An outlier
may be large but not so large as to skew the results of the entire
analysis. If no special causes for this unusual observation can be
determined, then this observation may simply reflect the variability
in the data.

\item  Delete the observation from the data set. The observation may be
determined to be unrepresentative of the population from which the
sample is drawn. If this is the case, then there may be little
information contained in the observation that can be used to make
general statements about the population. This option means that we
would omit the observation from the regression summary statistics
and discuss it in our report as a separate case.

\item  Create a binary variable to indicate the presence of an outlier. If one or several
special causes have been identified to explain an outlier, then
these causes could be introduced into the modeling procedure
formally by introducing a variable to indicate the presence (or
absence) of these causes. This approach is similar to point deletion
but allows the outlier to be formally included in the model
formulation so that, if additional observations arise that are
affected by the same causes, then they can be handled on an
automatic basis.

\end{itemize}

\end{boxedminipage}

\bigskip

\subsection{Using Residuals to Select Explanatory
Variables}\label{S5:ResidualsExplanatory}

Another important role of residual analysis is to help identify
additional explanatory variables that may be used to improve the
formulation of the model. If we have specified the model correctly,
then residuals should resemble random errors and contain no
discernible patterns. Thus, when comparing residuals to explanatory
variables, we do not expect any relationships. If we do detect a
relationship, then this suggests the need to control for this
additional variable. This can be accomplished by introducing the
additional variable into the regression model.

Relationships between residuals and explanatory variables can be
quickly established using correlation statistics. However, if an
explanatory variable is already included in the regression model,
then the correlation between the residuals and an explanatory
variable will be zero, by a result from matrix algebra. It is a good
idea to reinforce this correlation with a scatter plot. Not only
will a plot of residuals versus explanatory variables reinforce
graphically the correlation statistic, it will also serve to detect
potential nonlinear relationships. For example, a quadratic
relationship can be detected using a scatter plot, not a correlation
statistic.

If you detect a relationship between the residuals from a
preliminary model fit and an additional explanatory variable, then
introducing this additional variable will not always improve your
model specification. The reason is that the additional variable may
be linearly related to the variables that are already in the model.
If you would like a guarantee that adding an additional variable
will improve your model, then construct an added variable plot.

To summarize, after a preliminary model fit, you should:
\begin{itemize}
\item Calculate summary statistics and display the distribution of
(standardized) residuals to identify outliers.

\item  Calculate the correlation between the (standardized) residuals and
additional explanatory variables to search for linear relationships.

\item  Create scatter plots between the (standardized) residuals and additional
explanatory variables to search for nonlinear relationships.
\end{itemize}

\linejed

\textbf{Example. Stock Liquidity.} An investor's decision to
purchase a stock is generally made with a number of criteria in
mind. First, investors usually look for a high expected return. A
second criterion is the riskiness of a stock which can be measured
through the variability of the returns. Third, many investors are
concerned with the length of time that they are committing their
capital with the purchase of a security. Many income stocks, such as
utilities, regularly return portions of capital investments in the
form of dividends. Other stocks, particularly growth stocks, return
nothing until the sale of the security. Thus, the average length of
investment in a security is another criterion. Fourth, investors are
concerned with the ability to sell the stock at any time convenient
to the investor. We refer to this fourth criterion as the
\textit{liquidity }of the stock. The more liquid is the stock, the
easier it is to sell. To measure the liquidity, in this study we use
the number of shares traded on an exchange over a specified period
of time (called the VOLUME). We are interested in studying the
relationship between the volume and other financial characteristics
of a stock.

We begin this study with 126 companies whose options were traded on December
3, 1984. The stock data were obtained from Francis Emory Fitch, Inc. for the
period from December 3, 1984 to February 28, 1985. For the trading activity
variables, we examine
\begin{itemize}
\item the three months total trading volume (VOLUME, in millions of shares),
\item the three months total number of transactions (NTRAN), and
\item the average time between transactions (AVGT, measured in minutes).
\end{itemize}
\noindent For the firm size variables, we use the
\begin{itemize}
\item opening stock price on January 2, 1985 (PRICE),
\item  the number of outstanding shares on December 31, 1984 (SHARE, in millions of shares), and
\item  the market equity value (VALUE, in billions of dollars) obtained by taking the product of PRICE and SHARE.
\end{itemize}
\noindent Finally, for the financial leverage, we examine the
debt-to-equity ratio (DEB\_EQ) obtained from the Compustat
Industrial Tape and the Moody's manual. The data in SHARE are
obtained from the Center for Research in Security Prices (CRSP)
monthly tape.

After examining some preliminary summary statistics of the data,
three companies were deleted because they either had an unusually
large volume or high price. They are Teledyne and Capital Cities
Communication, whose prices were more than four times the average
price of the remaining companies, and American Telephone and
Telegraph, whose total volume was more than seven times than the
average total volume of the remaining companies. Based on additional
investigation, the details of which are not presented here, these
companies were deleted because they seemed to represent special
circumstances that we would not wish to model. Table
\ref{T5:LiquidSumStats} summarizes the descriptive statistics based
on the remaining $n=123$ companies. For example, from Table
\ref{T5:LiquidSumStats} we see that the average time between
transactions is about five minutes and this time ranges from a
minimum of less than a minute to a maximum of about 20 minutes.

\begin{table}[h]
\scalefont{0.9}

\caption{\label{T5:LiquidSumStats} Summary Statistics of the Stock
Liquidity Variables}

\begin{tabular}{cccccc}
\hline
& Mean & Median & Standard & Minimum & Maximum \\
&  &  & deviation &  &  \\ \hline
\multicolumn{1}{l}{VOLUME} & \multicolumn{1}{r}{$13.423$} &
\multicolumn{1}{r}{$11.556$} & \multicolumn{1}{r}{$10.632$} &
\multicolumn{1}{r}{$0.658$} & \multicolumn{1}{r}{$64.572$} \\
\multicolumn{1}{l}{AVGT} & \multicolumn{1}{r}{$5.441$} & \multicolumn{1}{r}{$%
4.284$} & \multicolumn{1}{r}{$3.853$} & \multicolumn{1}{r}{$0.590$} &
\multicolumn{1}{r}{$20.772$} \\
\multicolumn{1}{l}{NTRAN} & \multicolumn{1}{r}{$6436$} & \multicolumn{1}{r}{$%
5071$} & \multicolumn{1}{r}{$5310$} & \multicolumn{1}{r}{$999$} &
\multicolumn{1}{r}{$36420$} \\
\multicolumn{1}{l}{PRICE} & \multicolumn{1}{r}{$38.80$} & \multicolumn{1}{r}{%
$34.37$} & \multicolumn{1}{r}{$21.37$} & \multicolumn{1}{r}{$9.12$} &
\multicolumn{1}{r}{$122.37$} \\
\multicolumn{1}{l}{SHARE} & \multicolumn{1}{r}{$94.7$} & \multicolumn{1}{r}{$%
53.8$} & \multicolumn{1}{r}{$115.1$} & \multicolumn{1}{r}{$6.7$} &
\multicolumn{1}{r}{$783.1$} \\
\multicolumn{1}{l}{VALUE} & \multicolumn{1}{r}{$4.116$} & \multicolumn{1}{r}{%
$2.065$} & \multicolumn{1}{r}{$8.157$} & \multicolumn{1}{r}{$0.115$} &
\multicolumn{1}{r}{$75.437$} \\
\multicolumn{1}{l}{DEB\_EQ} & \multicolumn{1}{r}{$2.697$} &
\multicolumn{1}{r}{$1.105$} & \multicolumn{1}{r}{$6.509$} &
\multicolumn{1}{r}{$0.185$} & \multicolumn{1}{r}{$53.628$} \\ \hline
\end{tabular}

\textit{Source: Francis Emory Fitch, Inc., Standard \& Poor's
Compustat, and University of Chicago's Center for Research on
Security Prices.}

\scalefont{1.1111}
\end{table}

Table \ref{T5:LiquidCorr} reports the correlation coefficients and
Figure \ref{F5:LiquidPlot} provides the corresponding scatterplot
matrix. If you have a background in finance, you will find it
interesting to note that the financial leverage, measured by
DEB\_EQ, does not seem to be related to the other variables. From
the scatterplot and correlation matrix, we see a strong relationship
between VOLUME and the size of the firm as measured by SHARE and
VALUE. Further, the three trading activity variables, VOLUME, AVGT
and NTRAN, are all highly related to one another.

\begin{table}[h]
\scalefont{0.9}

\caption{\label{T5:LiquidCorr} Correlation Matrix of the Stock
Liquidity}
\begin{tabular}{lrrrrrrr}
\hline & AVGT & NTRAN & PRICE & SHARE & VALUE & DEB\_EQ \\ \hline
\multicolumn{1}{l}{NTRAN} & $-0.668$ &  &  &  &  &  \\
\multicolumn{1}{l}{PRICE} & $-0.128$ & $0.190$ &  &  &  &  \\
\multicolumn{1}{l}{SHARE} & $-0.429$ & $0.817$ & $0.177$ &  &  &  \\
\multicolumn{1}{l}{VALUE} & $-0.318$ & $0.760$ & $0.457$ & $0.829$ &  &  \\
\multicolumn{1}{l}{DEB\_EQ} & $0.094$ & $-0.092$ & $-0.038$ & $-0.077$ & $%
-0.077$ &  \\
\multicolumn{1}{l}{VOLUME} & $-0.674$ & $0.913$ & $0.168$ & $0.773$ & $0.702$
& $-0.052$ \\ \hline
\end{tabular}
\scalefont{1.1111}
\end{table}

Figure \ref{F5:LiquidPlot} shows that the variable AVGT is inversely
related to VOLUME and NTRAN is inversely related to AVGT. In fact,
it turned out the correlation between the average time between
transactions and the reciprocal of the number of transactions was
$99.98\%$! This is not so surprising when one thinks about how AVGT
might be calculated. For example, on the New York Stock Exchange,
the market is open from 10:00 A.M. to 4:00 P.M. For each stock on a
particular day, the average time between transactions times the
number of transactions is nearly equal to 360 minutes (= 6 hours).
Thus, except for rounding errors because transactions are only
recorded to the nearest minute, there is a perfect linear
relationship between AVGT and the reciprocal of NTRAN.


\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=1\textwidth,angle=270]{Chapter5/Fig5_1.ps}
    \caption{\label{F5:LiquidPlot} \small  Scatterplot matrix for
stock liquidity variables. The number of transactions variable
(NTRAN) appears to be strongly related to the VOLUME of shares
traded, and inversely related to AVGT.}
  \end{center}
\end{figure}


To begin to understand the liquidity measure VOLUME, we first fit a
regression model using NTRAN as an explanatory variable. The fitted
regression model is:

\scalefont{0.9}
\begin{center}
\begin{tabular}{lccl}
  VOLUME & = & 1.65 & +0.00183 NTRAN \\
  std errors &  & (0.0018)  & (0.000074) \\
\end{tabular}
\end{center}
\scalefont{1.1111}

\noindent with $R^{2}=83.4\%$ and $s=4.35$. Note that the
\textit{t-}ratio for the slope associated with NTRAN is
$t(b_{1})=b_{1}/se(b_{1})=0.00183/0.000074=24.7$, indicating strong
statistical significance. Residuals were computed using this
estimated model. To see if the residuals are related to the other
explanatory variables, below is a table of correlations.


\begin{table}[h]
\scalefont{0.9}

\caption{\label{T5:LiquidResidCorr1} First Table of Correlations }
\begin{tabular}{cccccc}
 \hline
& AVGT & PRICE & SHARE & VALUE & DEB\_EQ \\
RESID & -0.155 & -0.017 & 0.055 & 0.007 & 0.078 \\ \hline
\end{tabular}

{\small \textit{Note:} The residuals were created from a regression
of VOLUME on NTRAN.} \scalefont{1.1111}
\end{table}

The correlation between the residual and AVGT and the scatter plot (not
given here) indicates that there may be some information in the variable
AVGT in the residual. Thus, it seems sensible to use AVGT directly in the
regression model. Remember that we are interpreting the residual as the
value of VOLUME having controlled for the effect of NTRAN.

We next fit a regression model using NTRAN and AVGT as an explanatory
variables. The fitted regression model is:

\scalefont{0.9}
\begin{center}
\begin{tabular}{lccll}
  VOLUME     & = & 4.41   & -0.322 AVGT & +0.00167 NTRAN \\
  std errors &   & (1.30) & (0.135)     & (0.000098)     \\
\end{tabular}
\end{center}
\scalefont{1.1111}


\noindent with $R^{2}=84.2\%$ and $s=4.26$. Based on the
\textit{t-}ratio for AVGT, $t(b_{AVGT})=$ $(-0.322)/0.135$ $=-2.39$,
it seems as if AVGT is a useful explanatory variable in the model.
Note also that $s$ has decreased, indicating that $R_a^2$ has
increased.

Table \ref{T5:LiquidResidCorr2} provides correlations between the
model residuals and other potential explanatory variables and
indicates that there does not seem to be much additional information
in the explanatory variables. This is reaffirmed by the
corresponding table of scatter plots in Figure
\ref{F5:LiquidResidPlot}. The histograms in Figure
\ref{F5:LiquidResidPlot} suggest that although the distribution of
the residuals is fairly symmetric, the distribution of each
explanatory variable is skewed. Because of this, transformations of
the explanatory variables were explored. This line of thought
provided no real improvements and thus the details are not provided
here.

\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=1\textwidth,angle=270,scale=0.75]{Chapter5/Fig5_2.ps}
    \caption{\label{F5:LiquidResidPlot} \small  Scatterplot matrix of the
residuals from the regression of VOLUME on NTRAN and AVGT on the
vertical axis and the remaining predictor variables on the
horizontal axes.}
  \end{center}
\end{figure}



\begin{table}[h]
\scalefont{0.9}

\caption{\label{T5:LiquidResidCorr2} Second Table of Correlations }
\begin{tabular}{ccccc}
\hline
& PRICE & SHARE & VALUE & DEB\_EQ \\
RESID & -0.015 & 0.096 & 0.071 & 0.089 \\ \hline
\end{tabular}

{\small \textit{Note:} The residuals were created from a regression
of VOLUME on NTRAN and AVGT.} \scalefont{1.1111} \scalefont{1.1111}
\end{table}

\linejed

\section{Influential Points}

Influential points are observations that may have a disproportionate
effect on the overall regression fit. The reason for this
disproportionate effect is that regression coefficients estimates
can be shown to be weighted sums of responses (see Section 3.2.4).
To illustrate this, recall the example from Section 2.6 that shows
how one observation in twenty can reduce the $R^{2}$ from 90\% to
10\% (point C). An unusual set of explanatory variables or an
unusual response (given a set of explanatory variables) can mean
that a single observation has a major impact on the regression fit.
Of course, simply because an observation is influential does not
mean that it is incorrect or that its impact on the model is
misleading. As analysts, we would simply like to know whether our
fitted model is sensitive to mild changes such as the removal of a
single point so that we feel comfortable generalizing our results
from the sample to a larger population.

To assess influence, we think of observations as being unusual
responses, given a set of explanatory variables, or having an
unusual set of explanatory variables. We have already seen in
Section \ref{S5:ResidualAnalysis} how to assess unusual responses
using residuals. This section focuses on unusual sets of explanatory
variables.

We introduced this topic in Section 2.6 where we called an
observation having an unusual explanatory variable a ``high leverage
point.'' In multiple linear regression with many explanatory
variables, one can still get a feel for influential observations by
examining summary statistics (such as minima and maxima) for each
explanatory variable.


With more than one explanatory variable, determining whether an
observation is a high leverage point is not straightforward. For
example, it is possible for an observation to be ``not unusual'' for
any single variable and yet still be unusual in the space of
explanatory variables. Consider the fictitious data set represented
in Figure \ref{F5:Ellipsoid}. The point marked in the upper right
hand corner is unusual. However, it is not unusual when examining
the histogram of either $x_{1}$ or $x_{2}$. It is only unusual when
the explanatory variables are considered jointly.

For two explanatory variables, this is apparent when examining the
data graphically. Because it is difficult to examine graphically
data having more than two explanatory variables, Section
\ref{S5:Leverage} resorts to a numerical procedure for assessing
leverage.


\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=1\textwidth,angle=270,scale=0.25]{Chapter5/Fig5_3.ps}
    \caption{\label{F5:Ellipsoid} \small  The ellipsoid
represents most of the data. The arrow marks an unusual point.}
  \end{center}
\end{figure}



\subsection{Leverage}\label{S5:Leverage}

To define the concept of leverage in multiple linear regression, we
begin with a result from matrix algebra. It can be shown that the
fitted values can be expressed as a linear combination of responses.
Thus, we have
\begin{equation*}
\hat{y}_{i}=h_{i1}y_{1}+h_{i2}y_{2}+...+h_{ii}y_{i}+...+h_{in}y_{n}.
\end{equation*}
The values $h_{ij}$ are calculated using only the values of the
explanatory variables. From this expression, we see that the larger
is $h_{ii}$, the larger is the effect that the $i$th response
$(y_i)$ has on the corresponding fitted value $(\hat{y}_i)$. Thus,
we call $h_{ii}$ to be the \textit{leverage }for the $i$th
observation. Because the values $h_{ii}$ are calculated based on the
explanatory variables, the values of the response variable do not
affect the calculation of leverages.

Large leverage values indicate that an observation may exhibit a
disproportionate effect on the fit, essentially because it is
distant from the other observations (when looking at the space of
explanatory variables). How large is large? Some guidelines are
available from matrix algebra, where we have that
\begin{equation*}
\frac{1}{n}\leq h_{ii}\leq 1
\end{equation*}
and
\begin{equation*}
\bar{h}=\frac{1}{n}\sum_{i=1}^{n}h_{ii}=\frac{k+1}{n}.
\end{equation*}
Thus, each leverage is bounded by $n^{-1}$ and $1$ and the average
leverage equals the number of regression coefficients divided by the
number of observations. From these and related arguments, we use a
widely adopted convention and declare an observation to be a
\textit{high leverage point} if the leverage exceeds three times the
average, that is, if $h_{ii}>3(k+1)/n $.

Having identified high leverage points, as with outliers it is
important for the analyst to search for special causes that may have
produced these unusual points. To illustrate, in Section 2.7 we
identified the 1987 market crash as the reason behind the high
leverage point. Further, high leverage points are often due to
clerical errors in coding the data, which may or may not be easy to
rectify. In general, the options for dealing with high leverage
points are similar to those available for dealing with outliers.
\bigskip

\boxedjed

\emph{Options for Handling High Leverage Points}

\begin{enumerate}

\item Include the observation in the summary statistics but comment on its
effect. For example, an observation may barely exceed a cut-off and its
effect may not be important in the overall analysis.

\item  Delete the observation from the data set. Again, the basic rationale for
this action is that the observation is deemed not representative of
some larger population. An intermediate course of action between (i)
and (ii) is to present the analysis both with and without the high
leverage point. In this way the impact of the point is fully
demonstrated and the reader of your analysis may decide which option
is more appropriate.

\item  Choose another variable to represent the information. In some instances,
another explanatory variables will be available to serve as a
replacement. For example, in an apartment rents example, we could
use the number of bedrooms to replace a square footage variable as a
measure of apartment size. Although an apartment's square footage
may be unusually large causing it to be a high leverage point, it
may have one, two or three bedrooms, depending on the sample
examined.

\item  Use a nonlinear transformation of an explanatory variable. To illustrate, with our Stock Liquidity
example in Section \ref{S5:ResidualsExplanatory}, we can transform
the debt-to-equity DEB\_EQ continuous variable into a variable that
indicates the presence of ``high'' debt-to-equity. For example, we
might code DE\_IND $=1$ if DEB\_EQ $>5$ and DE\_IND $=0$ if DEB\_EQ
$\leq 5$. With this recoding, we still retain information on the
financial leverage of a company without allowing the large values of
DEB\_EQ drive the regression fit.
\end{enumerate}
\end{boxedminipage}
\bigskip


Some analysts use ``robust'' estimation methodologies as an
alternative to least squares estimation. The basic idea of these
techniques is to reduce the effect of any particular observation.
These techniques are useful in reducing the effect of both outliers
and high leverage points. This tactic may be viewed as intermediate
between one extreme procedure, ignoring the effect of unusual
points, and another extreme, giving unusual points full credibility
by deleting them from the data set. The word \textit{robust }is
meant to suggest that these estimation methodologies are ``healthy''
even when attacked by an occasional bad observation (a germ). We
have seen that this is not true for least squares estimation.

\subsection{Cook's Distance}

To quantify how influential a point is, a measure that considers
both the response and explanatory variables is \textit{Cook's
Distance}. This distance, $D_{i}$, is defined as

\begin{eqnarray}
D_{i} &=&\frac{\sum_{j=1}^{n}(\hat{y}_{j}-\hat{y}_{j(i)})^{2}}{(k+1)s^{2}}
\label{2} \\
&=&\left(\frac{e_i}{se(e_i)}\right)^{2}\frac{h_{ii}}{(k+1)(1-h_{ii})}.
\nonumber
\end{eqnarray}

\noindent The first expression provides a definition. Here,
$\hat{y}_{j(i)}$ is the prediction of the $j$th observation,
computed leaving the $i$th observation out of the regression fit. To
measure the impact of the $i$th observation, we compare the fitted
values with and without the $i$th observation. Each difference is
then squared and summed over all observations to summarize the
impact.

The second equation provides another interpretation of the distance
$D_{i}$. The first part, $(e_i/se(e_i))^{2}$, is the square of the
$i$th standardized residual. The second part,
$h_{ii}/((k+1)(1-h_{ii}))$, is attributable solely to the leverage.
Thus, the distance $D_{i}$ is composed of a measure for outliers
times a measure for leverage. In this way, Cook's distance accounts
for both the response and explanatory variables.

To get an idea of the expected size of $D_{i}$ for a point that is not
unusual, recall that we expect the standardized residuals to be about one
and the leverage $h_{ii}$ to be about $(k+1)/n$. Thus, we anticipate that $%
D_{i}$ should be about $1/n$. Another rule of thumb is to compare $D_{i}$ to
an \textit{F-}distribution with $df_{1}=k+1$ and $df_{2}=n-(k+1)$ degrees of
freedom. Values of $D_{i}$ that are large compared to this distribution
merit attention.

\linejed

\textbf{Example. The Effect of Outliers and High Leverage Points -
Continued.} To illustrate, we return to our example in Section 2.6.
In this example, we considered 19 ``good,'' or base, points plus
each of the three types of unusual points, labeled A, B and C. Table
\ref{T5:Outliers} summarizes the calculations.

\begin{table}[h]
\scalefont{0.9}

\caption{\label{T5:Outliers} Measures of Three Types of Unusual
Points}
\begin{tabular}{cccc}
\hline
& Standardized residual & Leverage & Cook's distance \\
Observation & $e/se(e)$ & $h$ & $D$ \\ \hline
A & 4.00 & .067 & .577 \\
 B & .77 & .550 & .363 \\
C & -4.01 & .550 & 9.832 \\ \hline
\end{tabular}
\scalefont{1.1111}
\end{table}

As noted in Section 2.6, from the standardized residual column we
see that both points A and C are outliers. To judge the size of the
leverages, because there are $n=20$ points, the leverages are
bounded by 0.05 and 1.00 with the average leverage being
$\bar{h}=2/20=0.10$. Using 0.3 ($ = 3 \times  \bar{h}$) as a
cut-off, both points B and C are high leverage points. Note that
their values are the same. This is because, from Figure 2.7, the
values of the explanatory variables are the same and only the
response variable has been changed. The column for Cook's distance
captures both types of unusual behavior. Because the typical value
of $D_{i}$ is $1/n$ or 0.05, Cook's distance provides one statistic
to alert us to the fact that each point is unusual in one respect or
another. In particular, point C has a very large $D_{i}$, reflecting
the fact that it is both an outlier and a high leverage point. The
95th percentile of an \textit{F-}distribution with $df_{1}=2$ and
$df_{2}=18$ is 3.555. The fact that point C has a value of $D_{i}$
that well exceeds this cut-off indicates the substantial influence
of this point.

\linejed

\section{Collinearity}

\subsection{What is Collinearity?}

\textit{Collinearity}, or \textit{multicollinearity}, occurs when
one explanatory variable is, or nearly is, a linear combination of
the other explanatory variables. Intuitively, it is useful to think
of the explanatory variables as being highly correlated with one
another as an indication of collinearity. With collinear data, the
explanatory variables may provide little additional information over
and above the information provided by the other explanatory
variables. The issues are: Is collinearity important? If so, how
does it affect our model fit and how do we detect it? To address the
first question, consider a somewhat pathological example.

\linejed

\textbf{Example. Perfectly Correlated explanatory variables.} Joe
Finance was asked to fit the model E $y=\beta _{0}+\beta
_{1}x_{1}+\beta _{2}x_{2}$ to a data set. His resulting fitted model
was $\hat{y}=-87+x_{1}+18x_{2}.$  The data set under consideration
is:

\scalefont{0.9}
\begin{center}
\begin{tabular}{ccccc}
\hline
$i$ & 1 & 2 & 3 & 4 \\
$y_{i}$ & 23 & 83 & 63 & 103 \\
$x_{i1}$ & 2 & 8 & 6 & 10 \\
$x_{i2}$ & 6 & 9 & 8 & 10 \\ \hline
\end{tabular}%
\end{center}
\scalefont{1.1111}

Joe checked the fit for each observation. Joe was very happy because
he fit the data perfectly! For example, for the third observation
the fitted value is $\hat{y}_{3}=-87+6+18(8)=63$, which is equal to
the third response, $y_{3} $. Because the response equals the fitted
value, the residual is zero. You may check that this is true of each
observation and thus the $R^{2}$ turned out to be $100\%$.

However, Jane Actuary came along and fit the model
$\hat{y}=-7+9x_{1}+2x_{2}.$ Jane performed the same careful checks
that Joe did and also got a perfect fit ($R^2 = 1)$. Who is right?

The answer is both and neither one. There are, in fact, an infinite
number of fits. This is due to the perfect relationship
$x_{2}=5+x_{1}/2$ between the two explanatory variables.

\linejed

This example illustrates some important facts about collinearity.
\bigskip

\boxedjed

\textit{Collinearity Facts}
\begin{itemize}
\item Collinearity neither precludes us from
getting good fits nor from making predictions of new observations.
Note that in the above example we got perfect fits.

\item  Estimates of error variances and, therefore, tests of model adequacy, are
still reliable.

\item  In cases of serious collinearity, standard errors of individual
regression coefficients are larger than cases where, other things
equal, serious collinearity does not exist. With large standard
errors, individual regression coefficients may not be meaningful.
Further, because a large standard error means that the corresponding
\textit{t-}ratio is small, it is difficult to detect the importance
of a variable.
\end{itemize}
\end{boxedminipage}
\bigskip


To detect collinearity, begin with a matrix of correlation
coefficients of the explanatory variables. This matrix is simple to
create, easy to interpret and  quickly captures linear relationships
between pairs of variables. A scatterplot matrix provides a visual
reinforcement of the summary statistics in the correlation matrix.


\subsection{Variance Inflation Factors}

Correlation and scatterplot matrices capture only relationships
between pairs of variables. To capture more complex relationships
among several variables, we introduce the \textit{variance inflation
factor (VIF)}. To define a \textit{VIF}, suppose that the set of
explanatory variables is labeled $x_{1},x_{2},...,x_{k}$. Now, run
the regression using $x_{j}$ as
the ``response'' and the other $x$'s $%
(x_{1},x_{2},...,x_{j-1},x_{j+1},...,x_{k})$ as the explanatory
variables. Denote the coefficient of determination from this
regression by $R_j^2$. We interpret $R_j=\sqrt{R_j^2}$ as the
multiple correlation coefficient between $x_{j}$ and linear
combinations of the other $x$'s. From this coefficient of
determination, we define the variance inflation factor

\begin{equation*}
VIF_{j}=\frac{1}{1-R_{j}^{2}},\text{ \ \ \ for \ }j=1,2,...,k.
\end{equation*}
A larger $R_j^2$ results in a larger $VIF_{j}$; this means greater
collinearity between $x_{j}$ and the other $x$'s. Now, $R_j^2$ alone
is enough to capture the linear relationship of interest. However,
we use $VIF_{j}$ in lieu of $R_j^2$ as our measure for collinearity
because of the algebraic relationship:

\begin{equation} \label{E5:SEsAndVIFs}
se(b_{j}) = s \frac{\sqrt{VIF_{j}}}{s_{x_{j}}\sqrt{n-1}}
\end{equation}

\noindent Here, $se(b_{j})$ and $s$ are standard errors and residual
standard deviation from a full regression fit of $y$ on
$x_{1},...,x_{k}$. Further, $s_{x_j} = \sqrt{(n-1)^{-1}
\sum_{i=1}^{n}(x_{ij}-\bar{x}_{j})^{2} }$ is the sample standard
deviation of the $j$th variable $x_{j}$.

Thus, a larger $VIF_{j}$ results in a larger standard error
associated with the $j$th slope, $b_{j}$. Recall that $se(b_{j})$ is
$s$ times the square root of the $(j+1)$st diagonal element of
$(\mathbf{X^{\prime} X})^{-1}$. The idea is that when collinearity
occurs, the matrix $\mathbf{X^{\prime}X}$ has properties similar to
the number zero. When we attempt to calculate the inverse of
$\mathbf{X^{\prime} X}$, this is analogous to dividing by zero for
scalar numbers. As a rule of thumb, when $VIF_{j}$ exceeds 10 (which
is equivalent to $R_{j}^{2}>90\%$), we say that severe collinearity
exists. This may signal is a need for action.

\marginparjed{A commonly used rule of thumb is that $VIF_j > 10$ is
a signal that severe collinearity exists.}

\linejed

\textbf{Example. Stock Liquidity - Continued.} As an example,
consider a regression of VOLUME on PRICE, SHARE and VALUE. Unlike
the explanatory variables considered in Section
\ref{S5:ResidualsExplanatory}, these three explanatory variables are
not measures of trading activity. From a regression fit, we have
$R^{2}=61\%$ and $s=6.72$. The statistics associated with the
regression coefficients are in Table \ref{T5:LiquidRegression}.


\begin{table}[h]
\scalefont{0.9}
\caption{\label{T5:LiquidRegression} Statistics from
a Regression of VOLUME on PRICE, SHARE and VALUE}

\begin{tabular}{crrrrr}
\hline
$x_j$ & $s_{x_j}$ & $b_j$ & $se(b_j)$ & $t(b_j)$ & $VIF_j$ \\
\hline PRICE& 21.37 & -0.022 & 0.035&
-0.63& 1.5 \\
SHARE & 115.1 & 0.054 & 0.010 &
5.19 & 3.8 \\
VALUE & 8.157 & 0.313 & 0.162 & 1.94 & 4.7
\\ \hline
\end{tabular}
\scalefont{1.1111}
\end{table}

You may check that the relationship in equation
(\ref{E5:SEsAndVIFs}) is valid for each of the explanatory variables
in Table \ref{T5:LiquidRegression}. Because each $VIF$ statistic is
less than ten, there is little reason to suspect severe
collinearity. This is interesting because you may recall that there
is a perfect relationship between PRICE, SHARE and VALUE in that we
defined the market value to be VALUE = PRICE $\times $ SHARE.
However, the relationship is multiplicative, and hence is nonlinear.
Because the variables are not linearly related, it is valid to enter
all three into the regression model.

\linejed

For collinearity, we are only interested in detecting linear trends,
so nonlinear relationships between variables are not an issue here.
For example, we have seen that it is sometimes useful to retain both
an explanatory variable $(x)$ and its square $(x^{2})$, despite the
fact that there is a perfect (nonlinear) relationship between the
two. Still, we must check that nonlinear relationships are not
approximately linear over the sampling region. Even though the
relationship is theoretically nonlinear, if it is close to linear
for our available sample, then problems of collinearity might arise.
Figure \ref{F5:Nearlinear} illustrates this situation.


\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=1\textwidth,angle=270,scale=0.25]{Chapter5/Fig5_4.ps}
    \caption{\label{F5:Nearlinear} \small  The relationship
between $x_{1}$ and $x_{2}$ is nonlinear. However, over the region
sampled, the variables have close to a linear relationship.}
  \end{center}
\end{figure}



What can we do in the presence of collinearity? One option is to center each
variable, by subtracting its average and dividing by its standard deviation.
For example, create a new variable $x_{ij}^{\ast }=(x_{ij}-\bar{x}%
_{j})/s_{x_{j}}$ . Occasionally, one variable appears as millions of units
and another variable appears as fractions of units. Compared to the first
mentioned variable, the second mentioned variable is close to a constant
column of zeroes, at least if one uses single-precision (eight significant
digits) arithmetic. If this is true, then the second variable looks very
much like a linear shift of the constant column of ones corresponding to the
intercept. This is a problem even using double-precision arithmetic because,
with the least squares operations, we are implicitly squaring numbers that
can make these columns appear even more similar.

This problem is simply a computational one and is easy to rectify. Simply
recode the variables so that the units are of similar order of magnitude.
Some data analysts automatically center all variables to avoid these
problems. This is a legitimate approach because regression techniques search
for linear relationships; scale and location shifts do not affect linear
relationships.

Another option is to simply not explicitly account for collinearity in the
analysis but to discuss some of its implications when interpreting the
results of the regression analysis. This approach is probably the most
commonly adopted one. It is a fact of life that, when dealing with business
and economic data, collinearity does tend to exist among variables. Because
the data tends to be observational in lieu of experimental in nature, there
is little that the analyst can do to avoid this situation.

\marginparjed{When severe collinearity exists, often the only option
is to remove one or more variables from the regression equation.}

In the best-case situation, an auxiliary variable that provides
similar information and that eases the collinearity problem, is
available to replace a variable. Similar to our discussion of high
leverage points, a transformed version of the explanatory variable
may also be a useful substitute. In some situations, such an ideal
replacement is not available and we are forced to remove one or more
variables. Deciding which variables to remove is a difficult choice.
Sometimes automatic variables selection techniques, described in
Section \ref{S5:Automatic}, can help determine an overall suitable
model choice. When deciding among variables, often the choice will
be dictated by the investigator's judgement as to which is the most
relevant set of variables.

\subsection{Collinearity and Leverage}

Measures of collinearity and leverage share common characteristics, and yet
are designed to capture different aspects of a data set. Both are useful for
data and model criticism; they are applied after a preliminary model fit
with the objective of improving model specification. Further, both are
calculated using only the explanatory variables; values of the responses do
not enter into either calculation.

Our measure of collinearity, the variance inflation factor, is designed to
help us with model criticism. It is a measure calculated for each
explanatory variable, designed to explain the relationship with other
explanatory variables.

The leverage statistic is designed to help us with data criticism. It is a
measure calculated for each observation to help us explain how unusual an
observation is with respect to other observations.

Collinearity may be masked, or induced, by high leverage points, as
pointed out by Mason and Gunst (1985) and Hadi (1988). Figures
\ref{F5:CollMask} and \ref{F5:CollInduce} provide illustrations of
each case. These simple examples underscore an important point; data
criticism and model criticism are not separate exercises.


\begin{figure}[htp]
    \includegraphics[width=1\textwidth,angle=270,scale=0.45]{Chapter5/Fig5_5.ps}
    $~~~$
    \includegraphics[width=1\textwidth,angle=270,scale=0.45]{Chapter5/Fig5_6.ps}    \hfill
      \parbox[t]{2.5in}{\caption{\label{F5:CollMask} \small  With the exception of
the marked point, $x_1$ and $x_2$ are highly linearly related.}}
\hfill
        \parbox[t]{2.5in}{ \caption{\label{F5:CollInduce} \small  The highly linear relationship
between $x_1$ and $x_2$ is primarily due to the marked point.}}
\end{figure}


The examples in Figures \ref{F5:CollMask} and \ref{F5:CollInduce}
also help us to see one way in which high leverage points may affect
standard errors of regression coefficients. Recall, in Section
\ref{S5:Leverage}, we saw that high leverage points may affect the
model fitted values. In Figures \ref{F5:CollMask} and
\ref{F5:CollInduce}, we see that high leverage points affect
collinearity. Thus, from equation (\ref{E5:SEsAndVIFs}), we have
that high leverage points can also affect our standard errors of
regression coefficients.

\subsection{Suppressor Variables}\label{S5:Suppressor}

As we have seen, severe collinearity can seriously inflate standard
errors of regression coefficients. Because we rely on these standard
errors for judging the usefulness of explanatory variables, our
model selection procedures and inferences may be deficient in the
presence of severe collinearity. Despite these drawbacks, mild
collinearity in a data set should not be viewed as a deficiency of
the data set; it is simply an attribute of the available explanatory
variables.

Even if one explanatory variable is nearly a linear combination of
the others, that does not necessarily mean that the information that
it provides is redundant. To illustrate, we now consider a
\textit{suppressor variable}, an explanatory variable that increases
the importance of other explanatory variables when included in the
model.

\linejed

\textbf{Example. Suppressor Variable.} Figure \ref{F5:Suppress}
shows a scatterplot matrix of a hypothetical data set of fifty
observations. This data set contains a response and two explanatory
variables. Table \ref{T5:Suppress} provides the corresponding matrix
of correlation coefficients. Here, we see that the two explanatory
variables are highly correlated. Now recall, for regression with one
explanatory variable, that the correlation coefficient squared is
the coefficient of determination. Thus, using Table
\ref{T5:Suppress}, for a regression of $y$ on $x_{1}$, the
coefficient of determination is $(0.188)^{2}=3.5\%$. Similarly, for
a regression of $y$ on $x_{2}$, the coefficient of
determination is $(-0.022)^{2}=0.04\%$. However, for a regression of $y$ on $%
x_{1}$ and $x_{2}$, the coefficient of determination turns out to be a
surprisingly high $80.7\%$. The interpretation is that individually, both $%
x_{1}$ and $x_{2}$ have little impact on $y$. However, when taken
jointly, the two explanatory variables have a significant effect on
$y$. Although Table \ref{T5:Suppress} shows that $x_{1}$ and $x_{2}$
are strongly linearly related, this relationship does not mean that
$x_{1}$ and $x_{2}$ provide the same information. In fact, in this
example the two variables complement one another.

\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=1\textwidth,angle=270,scale=0.5]{Chapter5/Fig5_7.ps}
    \caption{\label{F5:Suppress} \small  Scatterplot matrix of a
response and two explanatory variable for the suppressor variable
example.}
  \end{center}
\end{figure}


\begin{table}[h]
\scalefont{0.9}

\caption{\label{T5:Suppress} Correlation Matrix for the Suppressor
Example Corresponding to Figure \ref{F5:Suppress}}

\begin{tabular}{ccc}
\hline
& $x_{1}$ & $x_{2}$ \\
\multicolumn{1}{l}{$x_{2}$} & $0.972$ &  \\
\multicolumn{1}{l}{$y$} & $0.188$ & $-0.022$ \\ \hline
\end{tabular}
\linetjed \scalefont{1.1111}
\end{table}

\section{Selection Criteria}


\subsection{Goodness of Fit}

How well does the model fit the data? Criteria that measure the
proximity of the fitted model and realized data are known as
\emph{goodness of fit} statistics. We introduced most of the basic
criteria in Chapters 2 and 3. These criteria include the coefficient
of determination $(R^{2})$, an adjusted version $(R_{a}^{2})$, the
size of the typical error $(s)$, and $t$-ratios for each regression
coefficient.

This subsection introduces an additional goodness of fit measure,
the $C_p$ statistic. To define this statistic, assume that we have
available $k$ explanatory variables $x_{1},...,x_{k}$ and run a
regression to get $s_{full}^{2}$ as the mean square error. Now,
suppose that we are considering using only $p-1$ explanatory
variables so that there are $p$ regression coefficients. With these
$p-1$ explanatory variables, we run a regression to get the error
sum of squares $(Error~SS)_p$. Thus, we are in the position to
define
\begin{equation*}
C_{p}=\frac{(Error\text{ }SS)_p}{s_{full}^{2}}-(n-2p).
\end{equation*}
The choice of $p$ may vary from $1$ to $k+1$. For example, in the case where
$p=k+1$, all of the variables are included. In this case, we have

\begin{center}
\begin{eqnarray*}
C_{k+1} &=&\frac{(Error\text{ }SS)_{k+1}}{s_{full}^{2}}-(n-2(k+1)) \\
&=&(n-(k+1))\frac{(Error\text{ }MS)_{k+1}}{s_{full}^{2}}-(n-2(k+1)) \\
&=&(n-(k+1))-(n-2(k+1))=k+1,
\end{eqnarray*}
\end{center}

\noindent because $(Error$ $MS)_{k+1}=s_{full}^{2}$.

In general, if the model with $p$ regression coefficients is correct, then
we expect $C_{p}$ to be close to $p$. The idea is that $s_{full}^{2}$ should
be close to $\sigma ^{2}$ and, if the model is correct, then $(Error$ $%
MS)_{p}$ should also be close to $\sigma ^{2}$. Thus,

\begin{center}
\begin{eqnarray*}
C_{p} &=&(n-p)\frac{(Error\text{ }MS)_{p}}{s_{full}^{2}}-(n-2p) \\
&\approx &(n-p)\frac{\sigma ^{2}}{\sigma ^{2}}-(n-2p)=p.
\end{eqnarray*}
\end{center}

As a selection criterion, we choose the model with a ``small''
$C_{p}$ coefficient, where small is taken to be relative to $p$. In
general, models with smaller values of $C_{p}$ are more desirable.

The $C_{p}$ statistic measures the candidate model's mean square error
relative to a full model mean square error. In general, we prefer models
with a small $C_{p}$ coefficient such that $C_{p}\approx p$. It may be,
however, that the full model is poorly specified and that the resulting mean
square error is inflated. In such cases, the value of $C_{p}$ can be
negative. This is not to say that the model with the smallest $C_{p}$ is
poor; it merely states that the full model is poorly specified.

\subsection{Model Validation}\label{S5:ModelValidation}

Model validation is the process of confirming that our proposed
model is appropriate, especially in light of the purposes of the
investigation. Recall the iterative model formulation selection
process described in Section \ref{S5:Iterative}. An important
criticism of this iterative process is that it is guilty of
\emph{data-snooping}, that is, fitting a great number of models to a
single set of data. As we saw in Section \ref{S5:Automatic} on
data-snooping in stepwise regression, by looking at a large number
of models we may actually overfit the data and understate the
natural variation in our representation.

We can respond to this criticism by using a technique called
\textit{out-of-sample} \textit{validation}. The ideal situation is
to have available two sets of data, one for model development and
one for model validation. We initially develop one, or several,
models on a first data set. The models developed from the first set
of data are called our \emph{candidate} models. Then, the relative
performance of the candidate models could be measured on a second
set of data. In this way, the data used to validate the model is
unaffected by the procedures used to formulate the model.

Unfortunately, rarely will two sets of data be available to the
investigator. However, we can implement the out-of\textit{-}sample
validation process by splitting the data set into two subsamples. We
call these the \textit{model development }and \textit{validation
subsamples}, respectively. To see how the data-splitting process
works in the linear regression context, consider the following
procedure.

\bigskip
\boxedjed

\textit{Out-of-sample Validation Procedure}

\begin{enumerate}
\item Begin with a sample size of $n$ and divide it into two
subsamples, called the model development and validation subsamples.
Let $n_{1}$ and $n_{2}$ denote the size of each subsample. In
cross-sectional regression, do this split using a random sampling
mechanism. Use the notation $i=1,...,n_1$ to
represent observations from the model development subsample and $%
i=n_{1}+1,...,n_{1}+n_{2}=n$ for the observations from the
validation subsample. Figure \ref{F5:ModelValidation} illustrates
this procedure.

\item  Using the model development subsample, fit a candidate model to the data
set $i=1,...,n_{1}$.

\item  Using the model created in Step (ii) and the explanatory variables
from the validation subsample, ``predict'' the dependent variables
in the validation subsample, $\hat{y}_i$, where
$i=n_{1}+1,...,n_{1}+n_{2}$. (To get these predictions, you may need
to transform the dependent variables back to the original scale.)

\item Assess the proximity of the predictions to the held-out data.
One measure is the \textit{sum of squared prediction errors}

\begin{equation}\label{E5:SSPE}
SSPE=\sum_{i=n_{1}+1}^{n_{1}+n_{2}}(y_{i}-\hat{y}_{i})^{2}
\end{equation}


Repeat Steps (ii) through (iv) for each candidate model. Choose the
model with the smallest \textit{SSPE}.

\end{enumerate}
\end{boxedminipage}
\bigskip


\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=1\textwidth,angle=270,scale=0.5]{Chapter5/Fig5_8.ps}
    \caption{\label{F5:ModelValidation} \small  For model validation, a
data set of size $n$ is randomly split into two subsamples.}
  \end{center}
\end{figure}


There are a number of criticisms of the \textit{SSPE}. First, it is clear
that it takes a considerable amount of time and effort to calculate this
statistic for each of several candidate models. However, as with many
statistical techniques, this is merely a matter of having specialized
statistical software available to perform the steps described above. Second,
because the statistic itself is based on a random subset of the sample, its
value will vary from analyst to analyst. This objection could be overcome by
using the first $n_{1}$ observations from the sample. In most applications
this is not done in case there is a lurking relationship in the order of the
observations. Third, and perhaps most important, is the fact that the choice
of the relative subset sizes, $n_{1}$ and $n_{2}$, is not clear. Various
researchers recommend different proportions for the allocation. Snee (1977)
suggests that data-splitting not be done unless the sample size is
moderately large, specifically, $n\geq 2(k+1)+20$. The guidelines of Picard
and Berk (1990) show that the greater the number of parameters to be
estimated, the greater the proportion of observations needed for the model
development subsample. As a rule of thumb, for data sets with 100 or fewer
observations, use about 25-35\% of the sample for out-of-sample validation.
For data sets with 500 or more observations, use 50\% of the sample for
out-of-sample validation.

Because of these criticisms, several variants of the basic out-of-sample
validation process are used by analysts. Although there is no theoretically
best procedure, it is widely agreed that model validation is an important
part of confirming the usefulness of a model.

\subsection{PRESS Statistic}

For small sample sizes, an attractive validation statistic is
\textit{PRESS}, the \textit{Predicted Residual Sum of Squares}. To
define the statistic, consider the following procedure where we
suppose that a candidate model is available.

\bigskip
\boxedjed

\textit{PRESS Validation Procedure}

\begin{enumerate}
\item From the full sample, omit the $i$th point and use the remaining
$n-1$ observations to compute regression coefficients.

\item Use the regression coefficients computed in step one and the explanatory
variables for the $i$th observation to compute the predicted response, $\hat{y}%
_{(i)}$. This part of the procedure is similar to the calculation of
the \textit{SSPE} statistic with $n_{1}=n-1$ and $n_{2}=1$.

\item Now, repeat (i) and (ii) for $i=1,...,n$. Summarizing, define
\begin{equation}\label{E5:PressDef}
PRESS=\sum_{i=1}^{n}(y_{i}-\hat{y}_{(i)})^{2}.
\end{equation}

\end{enumerate}

\noindent As with \textit{SSPE}, this statistic is calculated for
each of several competing models. Under this criterion, we choose
the model with the smallest \textit{PRESS}.

\end{boxedminipage}
\bigskip

Based on this definition, the statistic seems very computationally
intensive in that it requires $n$ regression fits to evaluate it.
However, matrix algebra can be used to establish that

\begin{equation} \label{E5:StudentResid}
y_{i}-\hat{y}_{(i)}=\frac{e_i}{1-h_{ii}}.
\end{equation}

\noindent Here, $e_i$ and $h_{ii}$ represent the $i$th residual and
leverage from the regression fit using the complete data set. This
yields

\begin{equation}\label{E5:Press}
PRESS=\sum_{i=1}^{n}\left(\frac{e_i}{1-h_{ii}}\right)^{2},
\end{equation}
which is a much easier computational formula. Thus, the
\textit{PRESS} statistic is less computationally intensive that
\textit{SSPE}.

Another important advantage of this statistic, when compared to
\textit{SSPE}, is that we do not need to make an arbitrary choice as
to our relative subset sizes split. Indeed, because we are
performing an ``out-of-sample'' validation for each observation, it
can be argued that this procedure is more efficient, an especially
important consideration when the sample size is small (say, less
than 50 observations).

Because the model is re-fit for each point deleted, \textit{PRESS}
does not enjoy the appearance of independence between the estimation
and prediction aspects, unlike \textit{SSPE}. Further, out-of-sample
validation is a general principle that is useful in a number of
circumstances, including cross-sectional regression and time series.
Although computationally attractive, the sample re-use principle
that the \textit{PRESS} statistic is based on is not as well
understood for model selection purposes.

%\subsection{Bootstrapping}

%need text here

\section{Heteroscedasticity}\label{S5:Heteroscedasticity}

In most regression applications, the goal is to understand
determinants of the regression function $\mathrm{E~}y_i =
\mathbf{x}_i^{\prime} \boldsymbol \beta =\mu_i$. Our ability to
understand the mean is strongly influenced by the amount of spread
from the mean that we quantify using the variance
$\mathrm{E}\left(y_i-\mu_i\right)^2$. In some applications, such as
when I measure my weight on a scale in the morning, there is
relatively little variability; repeated measurements yield almost
the same result. In other applications, such as the time it takes me
to fly to New York, repeated measurements yield substantial
variability and are fraught with inherent uncertainty.

The amount of uncertainty can also vary on a case-by-case basis. We
denote the case of ``varying variability'' with the notation
$\sigma_i^2=\mathrm{E}\left(y_i-\mu_i\right)^2$. When the
variability varies by observation, this is known as
\emph{heteroscedasticity} for ``different scatter.''  In contrast,
the usual assumption of common variability (assumption E3/F3 in
Section 3.2) is called \textit{homoscedasticity} which stands for
``same scatter.''

Our estimation strategies depends on the extent of
heteroscedasticity. For datasets with only a mild amount of
heteroscedasticity, one can use least squares to estimate the
regression coefficients, perhaps combined with an adjustment for the
standard errors (described in Section \ref{S5:HeteroStdErrors}).
This is because least squares estimators are unbiased even in the
presence of heteroscedasticity (see Property 1 in Section 3.2).

However, with heteroscedastic dependent variables, the Gauss-Markov
theorem no longer applies and so the least squares estimators are
not guaranteed to be optimal. In cases of severe heteroscedasticity,
alternative estimators are used, the most common being those based
on transformations of the dependent variable, as will be described
in Section \ref{S5:Transformations}.

\subsection{Detecting Heteroscedasticity}

To decide a strategy for handling potential heteroscedasticity, we
must first assess, or detect, its presence.

To detect heteroscedasticity graphically, a good idea is to perform
a preliminary regression fit of the data and plot the residuals
versus the fitted values. To illustrate, Figure
\ref{F5:HeteroRegress} is a plot of a fictitious data set with one
explanatory variable where the scatter increases as the explanatory
variable increases. A least squares regression was performed -
residuals and fitted values were computed. Figure
\ref{F5:HeteroResid} is an example of a plot of residuals versus
fitted values. The preliminary regression fit removes many of the
major patterns in the data and leaves the eye free to concentrate on
other patterns that may influence the fit. We plot residuals versus
fitted values because the fitted values are an approximation of the
expected value of the response and, in many situations, the
variability grows with the expected response.

\marginparjed{To detect heteroscedasticity, plot the residuals
versus the fitted values.}

\begin{figure}[htp]
    \includegraphics[width=1\textwidth,angle=270,scale=0.45]{Chapter5/Fig5_9.ps}
    $~~~$
    \includegraphics[width=1\textwidth,angle=270,scale=0.45]{Chapter5/Fig5_10.ps}    \hfill
      \parbox[t]{2.5in}{\caption{\label{F5:HeteroRegress} \small  The shaded area
represents the data. The line is the true regression line.}} \hfill
        \parbox[t]{2.5in}{ \caption{\label{F5:HeteroResid} \small  Residuals plotted
versus the fitted values for the data in Figure
\ref{F5:HeteroRegress}.}}
\end{figure}

More formal tests of heteroscedasticity are also available in the
regression literature. To illustrate, let us consider a test due to
Breusch and Pagan (1980). Specifically, this test examines the
alternative hypothesis $H_a$: $\mathrm{Var~} y_i = \sigma^2 +
\mathbf{w}_i^{\prime} \boldsymbol \gamma $, where $\mathbf{w}_i$ is
a known vector of variables and $\boldsymbol \gamma$ is a
$p$-dimensional vector of parameters. Thus, the null hypothesis is
$H_0:~ \boldsymbol \gamma = \mathbf{0}$ is equivalent to
homoscedasticity,  $\mathrm{Var~} y_i = \sigma^2.$

\bigskip

\boxedjed

\textit{Procedure to Test for Heteroscedasticity}
\begin{enumerate}
  \item Fit a regression model and calculate the model residuals, ${e_i}$.
  \item Calculate squared standardized residuals, $e_i^{\ast 2}=e_i^2/s^2$ .
  \item  Fit a
regression model of $e_i^{\ast 2}$ on $\mathbf{w}_i$.
\item The test statistic is $LM = (Regress~SS_w)/2$, where $Regress~SS_w$ is the regression sum of squares from the
model fit in step (iii).
\item Reject the null hypothesis if $LM$ exceeds a
percentile from a chi-square distribution with $p$ degrees of
freedom. The percentile is one minus the significance level of the
test.
\end{enumerate}

\end{boxedminipage}

\bigskip

\noindent Here, we use $LM$ to denote the test statistic because
Breusch and Pagan derived it as a Lagrange multiplier statistic; see
Breusch and Pagan (1980) for more details.


\subsection{Heteroscedasticity Consistent Standard
Errors}\label{S5:HeteroStdErrors}

For datasets with only mild heteroscedasticity, a sensible strategy
is to employ least squares estimators of the regression coefficients
and to adjust the calculation of standard errors to account for the
heteroscedasticity.

From the Section 3.2 on properties, we saw that least square
regression coefficients could be written as $ \mathbf{b} =
\sum_{i=1}^n \mathbf{w}_i y_i $, where $\mathbf{w}_i =\left(
\mathbf{X}^{\prime}\mathbf{X}\right)^{-1} \mathbf{x}_i$. Thus, with
$\sigma_i^2 = \mathrm{Var~} y_i$, we have
\begin{equation}\label{E5:HeterVariances}
\mathrm{Var~}\mathbf{b} = \sum_{i=1}^n \mathbf{w}_i
\mathbf{w}_i^{\prime} \sigma_i^2  \\
=\left( \mathbf{X}^{\prime}\mathbf{X}\right)^{-1} \left(
\sum_{i=1}^n \sigma_i^2 \mathbf{x}_i \mathbf{x}_i^{\prime} \right)
\left( \mathbf{X}^{\prime}\mathbf{X}\right)^{-1}.
\end{equation}
This quantity is known except for $\sigma_i^2$. We can compute
residuals using the least square regression coefficients as $e_i =
y_i - \mathbf{x}_i^{\prime} \mathbf{b}$. With these, we may define
the \emph{empirical}, or \emph{robust}, estimate of the variance
covariance matrix as
\begin{equation*}
\widehat{\mathrm{Var~}\mathbf{b}} =\left(
\mathbf{X}^{\prime}\mathbf{X}\right)^{-1} \left( \sum_{i=1}^n e_i^2
\mathbf{x}_i \mathbf{x}_i^{\prime} \right) \left(
\mathbf{X}^{\prime}\mathbf{X}\right)^{-1}.
\end{equation*}
The corresponding ``heteroscedasticity-consistent" standard errors
are
\begin{equation}\label{E5:RobustSEs}
se_r(b_j) = \sqrt{(j+1)^{st} ~diagonal~
element~of~\widehat{\mathrm{Var~}\mathbf{b}}}.
\end{equation}
The logic behind this estimator is that each squared residual,
$e_i^2$ may be a poor estimate of $\sigma_i^2$. However, our
interest is estimating a (weighted) sum of variances in equation
(\ref{E5:HeterVariances}); estimating the sum is a much easier task
than estimating any individual variance estimate.

Robust, or heteroscedasticity-consistent, standard errors are widely
available in statistical software packages. Here, you will also see
alternative definitions of residuals employed, as in Section
\ref{S5:Residuals}. If your statistical package offers options, the
robust estimator using studentized residuals is generally preferred.


\subsection{Transformations}\label{S5:Transformations}

The least squares estimators are less useful for datasets with
severe heteroscedasticity. One strategy is to use a mild variation
of least squares estimation by weighting observations. The idea is
that, when minimizing the sum of squared errors using
heteroscedastic data, the expected variability of some observations
is smaller than others. Intuitively, it seems reasonable that the
smaller the variability of the response, the more reliable that
response and the greater weight that it should receive in the
minimization procedure. We will introduce a technique, called
\textit{weighted least squares}, in Chapter 14 that accounts for
this ``varying variability.''

A simpler, and widely used, device that we introduced in Section 1.3
is to transform the dependent variable, typically with a logarithmic
transformation of the form $y^{\ast} = \mathrm{ln~}y$. As we saw in
Section 1.3, transformations can serve to ``shrink'' spread out data
and symmetrize a distribution. Through a change of scale, a
transformation also changes the variability, potentially altering a
heteroscedastic dataset into a homoscedastic one. This is both a
strength and limitation of the transformation approach - a
transformation simultaneously affects both the distribution and the
heteroscedasticity.

\marginparjed{The transformation of the dependent variable affects
both the skewness of the distribution and the heteroscedasticity.}

Power transformations, such as the logarithmic transform, are most
useful when the variability of the data grows with the mean. In this
case, the transform will serve to ``shrink'' the data to a scale
that appears to be homoscedastic. Conversely, because
transformations are monotonic functions, they will not help with
patterns of variability that are non-monotonic. Further, if your
data is reasonably symmetric but heteroscedastic, a transformation
will not be useful because any choice that mitigates the
heteroscedasticity will skew the distribution.



\section{Further Reading and References}

Long and Ervin (2000) gather compelling evidence for the use of
alternative \newline heteroscedasticity-consistent estimators of
standard errors that have better finite sample performance than the
classic versions. The large sample properties of empirical
estimators have been established by Eicker (1967), Huber (1967) and
White (1980) in the linear regression case. For the linear
regression case, MacKinnon and White (1985) suggest alternatives
that provide superior small-sample properties. For small samples,
the evidence is based on (1) the biasedness of the estimators, (2)
their motivation as jackknife estimators and (3) their performance
in simulation studies.

See Carroll and Ruppert (1988) for further discussions of
transformations in regression.

\bigskip

\textbf{Chapter References}
\begin{multicols}{2}
\scalefont{0.9}

Bendel, R. B. and Afifi, A. A. (1977). Comparison of stopping rules
in forward ``stepwise'' regression. \textit{Journal of the American
Statistical Association} 72, 46-53.

Box, George E. P. (1980). Sampling and Bayes inference in scientific
modeling and robustness (with discussion). \textit{Journal of the
Royal Statistical Society}, Series A, 143, 383-430.

Breusch, T. S. and A. R. Pagan (1980).  The Lagrange multiplier test
and its applications to model specification in econometrics.
\textit{Review of Economic Studies}, 47, 239-53.

Carroll, Raymond J. and David Ruppert (1988). \textit{Transformation
and Weighting in Regression}, Chapman-Hall.

Eicker, F. (1967), Limit theorems for regressions with unequal and
dependent errors.  \textit{Proceedings of the Fifth Berkeley
Symposium on Mathematical Statistics and Probability} 1, LeCam, L.
M. and J. Neyman, editors, University of California Press, pp,
59-82.

Hadi, A. S. (1988). Diagnosing collinearity-influential
observations. \textit{Computational Statistics and Data Analysis} 7,
143-159.

Huber, P. J. (1967). The behaviour of maximum likelihood estimators
under non-standard conditions. \textit{Proceedings of the Fifth
Berkeley Symposium on Mathematical Statistics and Probability} 1,
LeCam, L. M. and Neyman, J. editors, University of California Press,
pp, 221-33.

Long, J.S. and L.H. Ervin (2000). Using heteroscedasticity
consistent standard errors in the linear regression model.
\textit{American Statistician} 54, 217-224.

MacKinnon, J.G. and H. White (1985). Some heteroskedasticity
consistent covariance matrix estimators with improved finite sample
properties. \textit{Journal of Econometrics} 29, 53-57.

Mason, R. L. and Gunst, R. F. (1985). Outlier-induced
collinearities. \textit{Technometrics} 27, 401-407.

Picard, R. R. and Berk, K. N. (1990). Data splitting. \textit{The
American Statistician} 44, 140-147.

Rencher, A. C. and Pun, F. C. (1980). Inflation of R2 in best subset
regression. \textit{Technometrics} 22, 49-53.

Snee, R. D. (1977). Validation of regression models. Methods and
examples. \textit{Technometrics} 19, 415-428.

\scalefont{1.1111}

\end{multicols}
